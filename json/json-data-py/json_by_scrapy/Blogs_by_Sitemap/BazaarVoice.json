[
{"website": "BazaarVoice", "title": "Platform API release notes, version 5.3", "author": ["Craig Gilchrist"], "link": "https://blog.developer.bazaarvoice.com/2012/08/03/platform-api-release-notes-version-5-3/", "abstract": "We are pleased to announce that the following functionality has been developed for version 5.3: More detailed information on each of these items is listed below. For complete documentation, refer to the Platform API documentation, version 5.3 . Hosted authentication – email Hosted email authentication can be used during submission to confirm the identity of a content submitter. When submitting content for the first time, a user receives an email containing a link. When the link is clicked, the user is directed to a landing page that calls back to the API to confirm their identity. This call results in the generation of an encrypted user token that can be used in subsequent submission calls. Depending on your configuration, the submitter’s content might not be accepted until the confirmation call is submitted. In order to use this feature, you must have hosted authentication enabled for your submission process. If you need more information, read the “Bazaarvoice hosted authentication reference guide” and the submission method documentation for details on the required parameters. Feedback submission for comments Feedback submission for review comments and story comments is now supported in addition to the existing support for feedback submission on reviews, questions, answers, and stories. For complete documentation, see the Feedback Submission method page. RatingDistribution and SecondaryRatingsAverages added to review statistics New RatingDistribution and SecondaryRatingsAverages blocks have been added to the ReviewStatistics block. You can now see the distribution of ratings for each product, which allows you to construct a rating histogram. You can also see the average rating of your secondary rating dimensions for reviews in relation to products and authors. Time zone changed to UTC The API now returns all time data using UTC (+00:00) to avoid the confusion of multiple time zones. The date format has not changed. Error codes added to form errors The API response has been updated to return error codes in addition to the existing error message for all form errors. A complete listing of the error codes can be found in each submission method. Syndication attribution on reviews All reviews have an “isSyndicated” field set to true or false. If the review is syndicated, a SyndicationSource block is displayed with details of where the review is being syndicated from. Syndicated content can only be returned if the API key is configured to show syndicated content.", "date": "2012-08-03"},
{"website": "BazaarVoice", "title": "5 Ways to Improve Your Mobile Submission Form", "author": ["Alex Medearis"], "link": "https://blog.developer.bazaarvoice.com/2012/09/13/5-ways-to-improve-your-mobile-submission-form/", "abstract": "Here at Bazaarvoice, we’re constantly focused on improving the user experience for our products. From the initial email invitation, to the submission form, to the way in which reviews are presented, we want to make sure that our interfaces are as flexible and intuitive as possible. Part of my job on the mobile team at Bazaarvoice is to make sure that our products reflect best practices when displayed on mobile devices. In reality, that means running hands-on user tests, A/B testing different designs, and gathering detailed information about the way in which users interact with our products. Recently, we ran a test with Buckle , one of our partner clients, to experiment with various mobile-friendly submission forms. What follows are some of the takeaways from those experiments. 1. Handle Landscape Mode Gracefully It is important that users are able to navigate forms easily while in landscape mode. It becomes particularly important to support landscape for form fields that solicit text input. We found that mobile users will, on average, input about 20% fewer words in their reviews than desktop users, so the last thing we want to do is to make it even more difficult to enter text. Many users prefer to type in landscape mode as it provides for a larger keyboard. 2. Make Interactions Easy Generally, a desktop user with a mouse can interact much more precisely than a mobile user with a clumsy finger. Therefore, it is important to make sure that elements are large enough to be either clicked or tapped. Apple recommends that tappable elements be at least 44×44 pixels . In our experimental form, we intentionally oversized our radio buttons, selection drop-downs and sliders to make the form easier to interact with and to prevent form errors. Additionally, mobile devices provide a number of different keyboard layouts for different types of inputs. For instance, an input type of “email” might surface the @ symbol to make it more readily accessible. In order to take advantage of the various keyboard layouts, be sure to properly specify the input type on your form elements. 3. Snappy Over Flashy The first version of our experimental form involved a heavy amount of JavaScript to do things like alpha animations and transforms. While our animations generally ran smoothly on a desktop, they became sluggish on the iPhone and lower end Android devices. When designing for mobile, be sure to prioritize function over flashiness. Slick animations can greatly improve the usability and “wow” factor of a site, but they should be used sparingly. If necessary, use hardware-accelerated transforms to minimize sluggishness. 4. Choose The Most Efficient Form Path Overall, our goal is to allow the user to complete our form in the quickest, simplest manner possible. In our testing, we found that a surprising number of users preferred to navigate and exit form elements via the “done” button rather than using the next/previous buttons. This has several interesting consequences. First, short forms are better than tall forms. While some users “tab” through fields, most users scroll. By minimizing the vertical spacing between elements, users do not need to scroll as far to get to the next field. Second, for most users, the interaction with a select element will involve 3 clicks: open, select, and done. Therefore, if a user is selecting between just a few options, it is better to use oversized radio buttons than select elements. 5. Provide Instant Feedback If a user submits an invalid form value such as a malformed email address, provide a clear error message that instructs the user how to fix the error. If possible, provide an error near the offending field. Additionally, once the form field becomes valid, notify the user immediately rather than requiring the user to submit the form again. For our experimental form, we used the JQuery validation library , which makes basic form validation dead simple. Since it is all client side, it makes validation snappy as well. Our tests are ongoing, so be on the lookout for more updates soon. Until then, hopefully these insights will be valuable to others as the Internet becomes more mobile-friendly.", "date": "2012-09-13"},
{"website": "BazaarVoice", "title": "Platform API release notes, version 5.4", "author": ["Craig Gilchrist"], "link": "https://blog.developer.bazaarvoice.com/2013/01/14/platform-api-release-notes-version-5-4/", "abstract": "We are pleased to announce that the following functionality has been developed for version 5.4: More detailed information on each of these items is listed below. For complete documentation, refer to the Platform API documentation, version 5.4 . Submission forms pre-filled for non-anonymous users When submitting content, the values of all known submission fields are now returned in the submission response fields. This only affects submissions where the user is not anonymous and the user/userid parameter is provided with the GET request. Full text search on all UGC and on includes The following content types were added to the existing search capabilities: All content is now searchable. For a list of all the fields that are searched for any given content type, see the API Basics page. Product family queries When filtering by product id, all content from that product’s product family is also returned by default. There is a new excludeFamily parameter that you can set to not return product family content. For examples and full documentation, see the Product Display method page. Photo upload accepts URLs The uploadphoto endpoint now accepts HTTP URLs of images in addition to locally stored photos from the client side. For examples and full documentation, see the Photo Submission method page. Brightcove Smart Player Javascript integration Brightcove videos can be loaded in a variety of ways. The information necessary to load these videos in the browser is now returned in the Videos block of the response elements. See the API Basics page for details on the new response items that were added to support Brightcove videos. Story rating field exposed in the response The story display response has a new block called “StoryRating” that contains two fields: Special product attributes exposed in the response The product display response has new fields for each of the following five product attributes: New filtering capabilities The following new filters are available: For more information, see the appropriate method’s documentation.", "date": "2013-01-14"},
{"website": "BazaarVoice", "title": "SELECT developers FROM Bazaarvoice UNION SELECT flags FROM Stripe_CTF;", "author": ["Jay Paz"], "link": "https://blog.developer.bazaarvoice.com/2012/09/03/select-developers-from-bazaarvoice-union-select-flags-from-stripe_ctf/", "abstract": "Stripe ( https://stripe.com/ ) held their second capture the flag event, this time the CTF was dedicated to web-based vulnerabilities and exploits. As a new Security Engineer here at BV the timing of this was perfect. It allowed me to use it as a vehicle for awareness and to ramp up curiosity, interest and even excitement for web application security (webappsec). Let’s start with some definitions to get us all on the same page. Capture the flag is a traditional outdoor game where two teams each have a flag and the objective is to capture the other team’s flag, located at the team’s base. In computer security the flag is a piece of data stored somewhere on a vulnerable computer and the goal is to use tools and know how to “hack” the machine to find the flag. Webappsec is the acronym for Web Application Security a branch of Information Security that deals specifically with security of websites and web applications. In my opinion there is more to it than just that. As we see from the OWASP Top 10 ( https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project ) and the emerging DevOps movement, web developers are doing more than just coding. They are now configuring full stacks to support their applications. So, webappsec needs to encompass more than just the application. It has to deal with developer and management education, configuration management, networking security, OS hardening, just to name a few areas, and of course not to forget the code. Now that we know what CTF and webappsec are lets move on to the details of the event. The CTF started on Wednesday, August 22nd, 2PM CDT today at 2PM CDT and ran for a week until Wednesday, August 29th, 2012 2PM CDT. One week == Seven Days == 168 hours and, for me at least, this was not enough. This capture the flag included 9 levels (0-8) to conquer before arriving at the actual flag. Each level was progressively more difficult and when completed provided you with the password for the next level. The CTF was very challenging, yet no security specific training/knowledge was needed in order to participate. In my opinion the minimum requirements to participate and find it fun and enjoyable was some knowledge of web application programming and a willingness to learn and research possible vulnerabilities and potential exploits. In total about 15 or so Bazaarvoice cohorts tried their hand at the CTF. Four actually captured the flag! When did they have the time? Well, we used our 20% time while at work and personal time when away from work. Late nights, lunch hours, weekend, etc.…believe me once you get started you are hooked. You end up thinking about it all day. You think about the level you are on, how to solve it and whether the previous levels offer up any hints or possible staging areas for potential exploits. If you are anything like me, first a developer at heart then a security professional, this type of activity is a great way to test out your chops and have some fun while learning new things and potentially bringing some of the lessons back to your teams. This is the perfect avenue for awareness – code wins after all! Here are quotes from some of the BV participants: I got to 8 last night. I don’t think I’ll get it in the next 4 hours, but it was a fun challenge. – Cory Made it to level 8 last night, but not until after midnight. Don’t think I’ll be finishing, but it was fun all the same. Level 8 is really interesting, and I look forward to hearing how folks finished it. – RC I’ve been having a lot of fun with it. – Jeremy Overall everyone that tried it had a good time with it no matter how far they actually got. All in all a total of four BVers completed the CTF and captured the final flag! Congratulations to the following BVers for successfully capturing the flag: For their last CTF Stripe made available the solutions and a working VM with each level. I hope they do the same for this one; this will give everyone the opportunity to learn and see what vulnerabilities were found and how they were exploited in order to complete each level and eventually capture the flag. For now I leave you with a quote which I think embodies web application security, its education and use in the development community: Secure programming is a mind-set. It may start with a week or two of training, but it will require constant reinforcement. – Karl Keller, President of IS Power As with many things here at Bazaarvoice, the education and growth of our developers and their skills often take on a fun and enjoyable approach.", "date": "2012-09-03"},
{"website": "BazaarVoice", "title": "Hello, World: Thoughts from an Undergrad Intern", "author": ["Matt Leibowitz"], "link": "https://blog.developer.bazaarvoice.com/2013/05/29/hello-world-thoughts-from-an-undergrad-intern/", "abstract": "Last summer I was given an internship opportunity in the R&D department of Bazaarvoice as a software engineer. Having only finished my freshman year in college, I had no idea what to expect from a tech company of this size. I would have never guessed that on my first day I would be handed a macbook pro with better specs than any computer I had used before. I certainly didn’t anticipate anything like the All Hands conference downtown, or the Seven Year anniversary party. And although I suspected I would pick up a few new skills during my employment, I could have never imagined the breadth of my learning over the past few months. My first week was spent setting up my development environment, and performing the dev onboarding — writing a game of tic-tac-toe in GWT. For those of you who do not know, GWT is the Google Web Toolkit, which Wikipedia identifies as “an open source set of tools that allows web developers to create and maintain complex JavaScript front-end applications in Java.” Virtually all of my programming experience is in Java, so I was grateful that I would be able to apply that experience. However, all of the programs I had written before had been completely text based. None of my previous projects involved a graphical interface, even one as simple as tic-tac-toe. I recall each day of my first week following the same pattern — I arrived in the morning feeling overwhelmed at the alien concept I was supposed to learn that day; by the late afternoon I had struggled through enough tutorials and examples to feel that I had a decent understanding of the new skill, and before leaving the office I would begin to look at the next subject to learn, and would feel overwhelmed all over again. Overall I feel that my first week was analogous to any of the computer science classes I have taken in school, albeit at an accelerated pace. I would write some code, and once I felt comfortable with my progress, I would show it to my mentor, who would point out the things I had done correctly, and offer advice for improvement in the areas where it was needed. Like any assignment for school, the tic-tac-toe project was exclusively for my own benefit; no one was going to use my code for anything, but there is no better teacher than practice. The project served its purpose, because by the end of my first week I was developing code for the product. Development was a totally new experience for me. All of my previous programming involved starting from scratch, and at completion, every line of code was my own. At Bazaarvoice, I was a developer jumping into a code base thousands of lines deep, and there were a dozen other developers constantly shaping it. It was an awesome experience working as a member of a team, rather than working on a project on my own. As a team member I not only gained experience in programming, but also in the team dynamics of software development. I feel that I was able to be a boon to my team, having contributed code in the development of several features. Because I was writing code for features that the team was familiar with, it was very easy to get help from the other members of the team. I was only one degree of separation away from the developer who had the answer to one of my many questions; if the first person I asked did not know the answer, they could immediately direct me to someone who did. This high availability of assistance was a key factor in my ability to put useful code into the product. The code review system — in which every line of code was looked at by at least one other developer before entering the product’s codebase — ensured that my code conformed to the proper coding practices. Because of this, I not only improved the functionality of my code, but also the style. My education went beyond the software itself. I attended all of the team meetings, which taught me about the “real life” side of software development. I attended meetings which showed me how the developers made sure that the product they were producing was in line with the needs of the customers. Every week involved multiple meetings to estimate the time it would take to implement various features, so that the team’s progress could be projected into the future,  and we could know how long it would take for the product to have certain degrees of functionality. The meetings were one of the most informative aspects of my internship, because they addressed issues that I had never even considered as a CS student. This was my first internship, so while I can’t offer a personal comparison between working on a personal project and working as a team member, I can say that last summer was an enriching opportunity, which not only expanded my computer science knowledge, but taught me first hand what it means to work as a software engineer, and in my mind, that’s the most valuable thing I can get out of an internship. This summer I have returned to Bazaarvoice, and this time I will be working on an independent project from the ground up. I am excited to see how this internship compares and contrasts to my first.", "date": "2013-05-29"},
{"website": "BazaarVoice", "title": "Interns and graduates – Keys to job search success", "author": ["Chris Norris"], "link": "https://blog.developer.bazaarvoice.com/2012/09/28/interns-and-graduates-keys-to-job-search-success/", "abstract": "Bazaarvoice R&D had a great year of intensive university recruiting with 12 interns joining our teams last summer and working side-by-side with the developers on our products. We have further expanded the program this year to accommodate two co-op positions for students from the University of Waterloo. The influx of fresh ideas and additional energy from these students has been great for the whole organization! For many students, looking for an internship or graduate employment may be their first time experiencing the interview process and creating resumes, and I’d like to offer some advice for those of you in this position. These guidelines are intended to help you think about how to present your capabilities in the best possible light, and in my experience, apply to tech interviews at most companies. What we’re looking for For new graduate and internship positions, it often surprises students that tech companies are, in general, less focused on them knowing specific technologies or languages. They are more focused on determining whether you have: It is generally expected that you have confidence in at least one programming language, with a solid grip on its syntax and common usage. It is also helpful for you to demonstrate an understanding of object-oriented concepts and design but, again, this can be independent of any specific language. Resumes Your resume is a critical chance to sell the reader on your abilities. While it can be uncomfortable to ‘toot your own horn’ it is important that you use your resume to try to differentiate yourself from the sea of other candidates. A dry list of courses or projects is unlikely to do this, so it really is worth investing a lot of thought in expressing on the resume what was particularly interesting, important, or impressive about what you did. Interview technique While an interview can be nerve-wracking, interviewers love to see people do well and are there to be supportive. Over my career I’ve found that these rules of thumb apply well in all technical interview/application processes, and hopefully they are useful guidance for students out there. Any other advice from readers?", "date": "2012-09-28"},
{"website": "BazaarVoice", "title": "You’ve lost that startup feeling…", "author": ["Scott Bonneau"], "link": "https://blog.developer.bazaarvoice.com/2012/03/08/youve-lost-that-startup-feeling/", "abstract": "Huge opportunity for impact. A sense of ownership. Collaboration with people across every function in the organization. An understanding of the big picture and a real opportunity to shape the future. These are some of the best and most exciting qualities of working at a software start-up. In my personal experience, working at both a number of start-ups as well as in my nearly 4 years at Google, environments with these qualities attract some of the highest-quality software engineers, and subsequently allows them to be maximally effective once they’re on board. Now that Bazaarvoice is six years old and technically a “big company” (in fact the best big company to work for in Austin ), one of the biggest challenges we face from an engineering team culture standpoint is maintaining these qualities through growth of both the team and the scope of business. All too often in technology companies, especially those that experience fast success and growth, the culture is sacrificed at the altar of the business. This can be deadly to companies that thrive in large part on the strength of their culture, and highlights an important and often overlooked question about organizational culture: Is your culture scalable? As VP of Engineering, a big part of my job is shepherding our culture and making sure that we create the kind of environment that allows us to attract and retain the best engineering talent. This brings us to the main topic of this blog post: Bazaarvoice engineering’s approach to building a scalable engineering culture and organization. The central question I try to tackle is this: how can I build an engineering organization and culture that scales to hundreds of engineers where every engineer experiences, on a daily basis, those key qualities of a start-up? To answer this question, we focus on four major areas. 1. Team Size This one seems obvious, but it never ceases to amaze me how many organizations miss the boat on this one. The bigger the team is, the less significant each individual is going to feel. The math is simple: it’s hard to feel like there’s a big opportunity for impact when you’re one of fifteen people on a team as compared to when you’re one of three people on a team. Also often overlooked is how much easier it is to hide an underperformer on a larger team than on a smaller team, and having to work with (and make up for) underperforming teammates can be toxic to an engineering culture. In my experience, some of the most effective and high-powered engineering teams were 3- and 4-person teams working on Really Big Problems. I like to create environments populated by small teams, led by high-potential people, tackling problems that on their face are just slightly bigger than the team should, on paper, be able to handle. When done properly, this creates a sense of urgency and creates an opportunity for growth and self-discovery for the team members. (When done improperly, it makes people feel like you’re asking the impossible, which is obviously very demotivating, and it’s a fine line to walk between the two. This highlights how important it is that, as a leader, you know your team well and are very in tune with their state of mind.) 2. Cross-Functional Structure Every organization has to choose how it’s going to fundamentally structure itself: functionally or cross-functionally (sort of like choosing your clustered index in SQL Server). I believe in a functional organization structure in the sense that every engineer reports to an engineering manager and on up through a VP of Engineering or CTO, for individual performance management and career development reasons. However, on a daily basis, I think the organization should operate logically and physically as if it were cross-functionally structured. What I mean by this is that I believe we should have teams that work together daily consist of members from different functions (e.g. engineering, product management, design, operations, etc), that those teams should by physically co-located (i.e. sit together), and that the success of the team as a whole should be weighed more heavily than individual success. The reason for this is simple: I feel that every engineer should have both visibility into why they’re building what they’re building, as well as having input into what gets built and how it gets built. Engineers, by and large, are creative people and they have a lot to bring to the table when it comes to creative problem solving. That stuff shouldn’t be solely the domain of the product managers, it should be a highly collaborative process. It’s similarly important that the engineers get visibility into how their products are being used when out in the wild. All of these inputs are hugely valuable to making sure the engineering team builds the right product for the market. As an important side benefit, exposure to these other areas gives the engineers themselves an opportunity o learn, makes them smarter and stronger and more likely to make a good decision when faced with uncertainty in the future. 3. Power! Unlimited Power! (Not really, I just wanted an excuse to link to this video .) Seriously though, this bullet would be better titled: “Uncapped Potential”, or “Roles, Not Titles”. Every engineer ultimately wants to feel like they’re in control of their own destiny, and I want to create an environment where that’s largely true. This boils down to one major question: can an engineer with talent, potential and ambition take the ball and run with it as far as her talent will let her? If not, what’s getting in her (or his) way? Maybe it’s an overbearing management structure, or lack of visibility into how the product is used in the field, or how it’s sold. Whatever it is, find it and get it out of the way as quickly as you can. We want to create an environment in which engineers with talent, potential and ambition can be as effective as possible, and nothing is more frustrating than an artificial barrier. One simple organization structure change that helps this along is organizing teams around roles, not titles. First, some definitions. In my opinion, a job “title” should do one thing and one thing only: define the expectations by which someone’s work should be judged, vis-a-vis the organization’s overall values. For instance, the job title of “senior software engineer” may entail the expectation that an individual be capable of owning moderate-to-large components of a big system and operating in a self-directed manner. Notably, it says nothing about the function of that individual, or his relationship to his peers. A job “role”, on the other hand, is all about the function of the individual in some specific context. For instance, a technical lead (TL) for a team may be responsible for the technical delivery of a project, including the day-to-day work assignments for the engineers on that team. Notably, it says nothing about the title or level of the individual. Every team can and should have a TL, regardless of whether the TL is the only individual on the team or if the team has dozens of members (in which case there may be multiple TLs). What this means, as an extreme example, is that you can have situations where you’ve got a more junior engineer that is the TL of a team that includes much more senior engineers. Under the right circumstances, this can be exactly what you want (I’ve seen it work out spectacularly well on multiple occasions in my career). More importantly though, is that you want your organization and culture to support this kind of flexibility because it provides an opportunity for high-potential engineers, regardless of seniority, to take ownership over something and stretch themselves. 4. A Voice Heard The last bullet point is in many ways the most important, and really runs as an undercurrent to each of the others. Ultimately, the engineers on the team need to feel like they have a voice, and that that voice will be heard when they’ve got something to say. Whether that means having a voice in the design of the product, the technology direction of the team, the company’s core values, or anything else, engineers are ultimately people, and they want to feel like their opinion matters. As team leaders, it’s our job to make sure both that they feel this way and that it is actually true! Conclusion As always with these types of philosophical discussions, your mileage may vary, and there are always exceptions to the rules. However, I’ve seen these approaches and techniques be successful repeatedly throughout my career, perhaps most notably during my time at Google, and I believe that they are generally applicable. What are your thoughts? I’d love to hear them and discuss them in the comments section below. Like the main 4 constraints you are focusing for building up a healthy orgainzation culture giving efficient productivity. Also would like to add to give balanced amount of feedback including criticism and appreciation must be given to each and every employee so that they should feel that there work is being observed.", "date": "2012-03-08"},
{"website": "BazaarVoice", "title": "Bazaarvoice Platform API Tutorial", "author": ["Dustin Mihalik"], "link": "https://blog.developer.bazaarvoice.com/2012/04/04/bazaarvoice-platform-api-tutorial/", "abstract": "We recently released our new Bazaarvoice Platform API. This is a new RESTful API that allows access to much more data and provides responses in XML and JSON. We are really excited to see the types of applications our clients will be building on the API. For a quick introduction to the API, we created the API Tutorial (you must be logged in to view) that walks through creating a Javascript-based widget for displaying Bazaarvoice content on any webpage. The tutorial explores the JSONP response format of the API and how it can be used with Javascript templates to inject content onto the page. You can read the API tutorial or go grab the code from Github and start checking out the new API.", "date": "2012-04-04"},
{"website": "BazaarVoice", "title": "Mashery-powered Bazaarvoice Developer portal is LIVE!", "author": ["Craig Gilchrist"], "link": "https://blog.developer.bazaarvoice.com/2013/02/28/mashery-powered-bazaarvoice-developer-portal-is-live/", "abstract": "Welcome to the Mashery-powered Bazaarvoice Developer portal. We strive to give you the tools you need to develop cutting-edge applications on the Bazaarvoice platform. Some changes you’ll notice: Note that none of your existing API keys are affected by this transition. They will continue to work without interruption. Thanks for your support of the Bazaarvoice platform.", "date": "2013-02-28"},
{"website": "BazaarVoice", "title": "Third-Party Front-end Performance: Part 1", "author": ["Alex Sexton"], "link": "https://blog.developer.bazaarvoice.com/2012/04/11/third-party-front-end-performance-part-1/", "abstract": "Bazaarvoice is a third-party application provider. We have a growing number of applications running on our own domain, but our core business is providing user-generated content applications and widgets that are hosted by us, but run on our clients’ webpages. Scaling an application platform of our size certainly has its challenges at the data layer. We use all the coolest noSQL tools and post up big request per seconds numbers every month, but an oft forgotten part of the story lives long after all of that is cached and delivered: On the front end. Steve Souders, the guy who (literally) wrote the book on web performance, estimates that 80% of average page load times actually occur after the markup is completely downloaded. That means a 50% speed up in your front-end code is going to mean a lot more than a 50% speed up in your backend code. This is important stuff! I’d like to go through some of our front-end performance considerations and explain the optimizations that we take, or the ones we’d like to start taking soon. I recently read that “the history of web development is just increasingly difficult ways of concatenating strings together.” So we’ll start at that point: We’ve got everything ready to go on the server and now we have to put it on client’s site and make it work. From this point, we’ll identify three distinct areas of the front-end performance: This post will cover The Network and the other two will be posted as continued parts. The Network The network is by far the most commonly discussed area of optimization on the client. That’s for good reason, though, as it’s often the bottleneck. There is really no great ‘secret’ to making this fast, though. The smaller your payload is and the fewer amount of files that you request, the faster your page will generally be. There are lots of articles on reducing image sizes, and spriting css, and you should read those, but I wanted to focus on some less commonly talked about techniques that are especially important in third party applications. The only non-obvious part of this equation to me (in regards to 3rd party code) is the caching as well as the cacheability of your resources. There are a few unique problems when doing this as a third party, but more or less, the higher percentage of cacheable static resources that make up your app, the more opportunity you have to make the network problem a lot less scary/slow. There are whole books and companies who specialize in this stuff, but we’ll go through at a high level. — Obligatory Wu-tang Cache Rules Everything Around Me reference. — Edge caching One of the easiest wins is using a CDN that implements what’s known as ‘edge-caching.’ This is the practice of a putting a distributed system of cache servers around the world (or country, etc) and serving content from the geographically closest server to each request. This not only decreases load on individual servers, but it also vastly improves latency of requests. Note that edge-caching is still not quite cached on a user’s computer, but still on a server in the CDN. Even if you have amazing cacheability in your app, you can still benefit from edge-caching for the initial load of the application (when the cache isn’t primed). Bazaarvoice does this with almost all of its content, even relatively dynamic content. We are one of the largest users of the Akamai CDN, and well over 90% of our total traffic actually just hits Akamai servers and never makes it back to our origin servers. You may be familiar with something like Amazon S3 to serve your static files. S3 is not distributed by default, but adding on the CloudFront option to your Amazon S3 buckets will achieve many of the same results (assuming you serve from the CloudFront URLs). Maximum Cacheability The content in most web apps changes frequently. However, the application really only changes when new code is written and pushed (read: hopefully far less frequently). In an ideal world, everything except for the data would be cached for a length of time (in the end-user’s browser). The larger the percentage of your application that you can statically generate, the more you can cache. This seems pretty easy at a glance, but can actually get a bit hairy in a third party environment. Due to the fact that the application is included via JavaScript, the markup for the page is not generally served in the initial request and is injected afterwards instead. This differs from normal webapps that can send their rendered markup along with the initial page request. Dynamic Content Currently, Bazaarvoice’s template system integrates with our Java backend, so after the static resources are loaded, we make a request for the rendered content of the page we’re on. This comes back to us as a string, which is saved to a javascript variable. Exactly how this works is unimportant, but the key is that this entire additional request is not cacheable (at least for any significant amount of time). Since the data in the rendered templates changes, we not only have to send the fresh data on each request, we also have to send up all the templates each time, multiplied times the number of times they are invoked. Given a significant amount of data, this can add quite a bit of weight to these requests. GZip can help quite a bit here with repeated data, but if you also factor in all the escaping characters to save these as strings in JavaScript, you can see where this could reduce performance by increasing the file size of noncacheable requests. For the most part, in practice, we don’t serve so much data in an individual request to be effected by this problem on an alarming level, but it’s always nice to optimize further, and we’d love to save the money on bandwidth costs. In our simple example above, we had the added duplication of a single set of <li> tags. The solution to only sending that once and caching it after the initial load is client-side templating. If we are able to include our templates in our cached, static resources, you only need to request them once. So even though the library to render them has some cost, it usually pays off quickly (often still on the initial load). We’ll get into the actual performance and responsiveness of the app in a later post, but note that in production, these templates can actually be ‘pre-compiled’ into javascript functions that run very quickly (more or less as quickly as string concatenation can take you). Also in production, the templates and data are much larger, so this reduction pays off at scale. In a third-party application that can cache its templates alongside the application code, the data (usually JSON) is the only unique and the only non-cached bytes going over the wire after the initial load. This is great. I’ll leave optimizing data payloads to a different blog post, but just make sure you only grab the minimum amount of data that you need. For extreme performance (and SEO to boot!), a third party tool could also integrate the markup injection on the server-side. The markup would go in the initial payload and be instantly visible upon page load (great perceived performance). You would have to write your JavaScript in a way that could bind to pre-rendered templates, but it’s all possible. This is usually a more difficult thing to get clients to agree to. You end up duplicating the markup in loops, like in our first example, but you also save an http request. Since the client can cache the request for every end user on their app server this ends up being by far the fastest integration in practice. This is a good complement for a high-performance third-party system, even if it can be tough to get clients to agree to. Data Caching too! If you want to go even further (and I very much encourage it), you may want to look into caching your data too. This wouldn’t do much for new page loads, but it could really help for common widgets across pages, or reloads/revisits of the same page. Anything that doesn’t take a significant amount of code that can reduce origin requests is probably going to be worth it eventually at scale. Consider a “Most Recently Reviewed Products” widget that goes across every page on a site. This widget would list 5 products or so. The recency of the products in the list is important, but the exact accuracy is likely not important within a few minute period (unless you’re an unusually high-review-volume webpage). Unfortunately, though, people usually visit quite a few pages in that same time period. As a third-party, you’re more than likely relying on JSONP to request this information for the widget. Each subsequent page load is highly cached (static resources) because you’re doing all the things we’ve mentioned previously, but you’re forced to re-request this redundant data each time. I know what you’re thinking… “Since JSONP injects a script tag under the hood, it is just a GET request for a “javascript file” that can be cached by the browser.” — you While this is technically correct, it’s usually difficult to actually realize in the real-world for a couple of reasons. At Bazaarvoice, and in many (read: most of the ones I’ve worked with) APIs, all requests are sent with a ‘no-cache’ header. This stops all caching from occuring. If you do have an API that allows browser caching you’re one step closer, but still usually have another issue. Most people are using jQuery or other AJAX libraries in order to make JSONP requests to the server. Unfortunately for fans of API caching, these libraries almost always bust your cache, either intentionally, or unintentionally. This code: actually results in a request to: And every time you request it, those last two numbers will change. Even if you set the cache option to false – you still end up with unique callback values in each request. This will still bust your cache. There’s a touch too much code to include here, but the solution is to set cache to false as well as override the jsonpCallback with a value that stays the same. However, you must then manage callback collisions yourself. That has some significant code behind it. If you successfully implement all that, this means that, as long as edge cases don’t arise, the url will be the same each time, and the jsonp requests would be cache-hits for the cache TTL of the api server. Another approach that we are in the process of testing and implementing at Bazaarvoice is utilizing localStorage and sessionStorage . There is not a lot of detail needed, but in browsers that support these DOM Storage utilities, you can write logic to check them for data before making JSONP requests, and invalidate the data after a given time. Make sure you clean up old data when you invalidate, because space is limited, but this is a nice solution, because the fallback is just the natural case. Currently I’d suggest using sessionStorage with an appropriate TTL on your data. That way it would get cleaned up automatically at the end of the ‘session.’ This solution seems a lot more elegant to me than api cache-headers and JSONP overrides. Now, after the first request for our data, each subsequent page load has a cache-hit (for our allotted interval) and saves a request from occurring. FASTER! BONUS TIP!! If you have very real-time data, don’t count out this method. One very slick ‘realtime-y’ touch is to have visibly updating data. In our example, even if we wanted to make an uncached request each time, we could instantly display the old data on page load from sessionStorage , wait until all the critical page elements were fully loaded, as to not block their speed, and then load in the new data. If the data was updated, you can put in a smooth update transition to let the user know that your app is awesome and real time. Best of both worlds! Cache-busting, or “how long do I cache?” “But Alex! How long do I cache my resources? If I cache them too long, then when I update my app, everyone will have different files!” — you, again. Very insightful of you. In Bazaarvoice’s current system, each file has a relatively low cache max-age (it varies, but usually between no-cache and ~2 hours). This means that we can be sure that all the files that people have after 2 hours of pushing a change will be the most current one. This actually ends up being fairly practical. For the most part, people are done browsing a site well before their cache runs out. Then the next time they just have to get it again one time. But we can do better! 😀 We’re working on a new system for caching that affords us more update control, and much higher cache-hit rates. Win-win! The general idea is to have a single “bootstrap” JavaScript file that we refer to as a “scout” file. This file is usually unique for each our clients, and kept as small as we can keep it. It’s usually included via a script tag (partially for ease of implementation), but could be injected asynchronously, as well. ** Including it as a script tag (synchronously) has the added benefit of caching the dns lookup for the domain in a single request, which should actually speed up subsequent requests in many browsers. This scout file should have a very low cache max-age — in our case, somewhere in the ballpark of 0 to 5 minutes — depending on the need for control. EVERYTHING ELSE IS CACHED FOREVER. In this “scout” file, we kick off our real application by asynchronously injecting our built core javascript file. Concatenation of this main application file and its dependencies is well understood and necessary, so I’m not covering it in this post, but it is essential to good performance. The trick is that at build time, we embed a version number into our scout file. Then the asynchronous inject function uses this version number as part of the request for the file. This can be done by putting each build in a folder with its version as the name of the folder, or it can just be done via url-rewrites. I would encourage you to not use the query string (get params) in order to do cache busting, as VPNs and other old browser situations can sometimes ruin the cache, and get you into weird situations. The affect of this setup is that we have a single, very small file that is constantly looking for updates (hence: “scout file”). It generally is still cached for the average ‘time on site’ of a consumer on one of our clients’ pages, but not much longer. However, if the version of the build doesn’t change, then all the requested scripts after that will be cached forever, or until the version changes again. If you don’t push a new build, a visitor can still have cached content from years prior and only have to download the “scout” file, but at any point in time, if you run a build, their cache is busted and they get all the new files within the amount of time the scout file was cached for. This obviously becomes more complex when you have multiple scripts loading. Bazaarvoice uses the require.js library, and its baseUrl configuration option is more or less the only place we end up having to update to make this work for us. It may be more complicated in more hand-rolled solutions, but still very worth it. The numbers in the end are generally very much in your favor when taking average browsing habits into account. You end up with a few small non-cached hits, but a much better long tail of cacheability of the large application files — with the added benefit of quickly being able to force an update in the event of an emergency. Conclusion We glazed over the reducing file-size and concatenation section in half of a paragraph, but I’d like to reiterate that this is just not optional if you care about performance. Your mechanism for doing this may vary, and we don’t mind that, as long as it works for you. I would however like to point out that you should be constantly testing the performance of your app and identifying your bottlenecks specifically. So often I see people byte-shaving licenses at the tops of their libraries while simultaneously sending improper cache-headers. The key to tackling performance is always tackling the bottleneck first. Then you’ll have a new bottle neck to tackle. You can measure network performance of all these techniques in your browser. Google Chrome (and other webkit based browsers) as well as the Firebug extension on Firefox can give you great insight in to the latency, time-to-load, and file size of your files (in the Network tab). You can export this data as a .har file and then import it into a variety of other evaluation tools if you need more info. At Bazaarvoice we’re tackling our performance issues so we can help give the best experience for our clients’ users. Remember: after you have a responsible amount of http requests and total file size, cache as much as you can, and then cache more. It’s as simple as that. Next Stay tuned for information on Parsing and evaluation, and application responsiveness.", "date": "2012-04-11"},
{"website": "BazaarVoice", "title": "MongoDB Arrays and Atomicity", "author": ["Michael Norman"], "link": "https://blog.developer.bazaarvoice.com/2012/04/18/mongodb-arrays-and-atomicity/", "abstract": "Over time, even technologies that are tried and true begin to show their age. This is especially true for data stores as the shear amount of data explodes and site traffic increases. Because of this, we are continually working with new technologies to determine whether they have a place in our primary stack. To that end, we began using MongoDB for one of our new internal systems several months ago. Its quick setup and ease of use make it a great data store for starting a new project that’s constantly in flux. Its robustness and scalability make it a worthy contender for our primary data store, if it should prove capable of handling our use cases. Being traditionally a MySQL shop, our team is used to working with databases in a certain way. When we write code that interacts with the DB, we use transactions and locks rather liberally. Therefore using MongoDB is a rather significant paradigm shift for our developers. Though MongoDB doesn’t have true ACID compliance , it does support atomic updates to an individual object. And because a “single” object in Mongo can actually be quite complex, this is sufficient for many operations. For example, for the following complex object, setting values, incrementing numbers, adding and removing items from an array, can all be done at the same time atomically: Even setting values of child objects can be done atomically: In fact, individual items in an array can be changed: However, one thing you may notice is that the code above to modify a specific item in the array requires us to know the specific index of the item in the array we wish to modify. That seems simple, because we can pull down the object, find the index of the item, and then change it. The difficulty with this is that other operations on an array do not necessarily guarantee that the order of the elements will be the same. For example, if an item in an array is removed and then added later, it will always be appended to the array. The only way to keep an array in a particular order is to $set it (replacing all the objects in it), which for a large array may not have the best performance. This problem is best demonstrated with the following race conditions: One solution to this problem is to $pull a particular item from the array, change it, and then $push it back onto the array. In order to avoid the race condition above, we decided that this was a safer solution. After all, these two operations should be doable atomically. So, what’s the problem? The issue is that MongoDB doesn’t allow multiple operations on the same property in the same update call. This means that the two operations must happen in two individually atomic operations. Therefore it’s possible for the following to occur: That is bad enough, but it can be dealt with by updating an object only if it is in the correct state. Unfortunately, the following can happen as well: That looks very odd to the reader, because at one moment it would see the item, then the next read would lose it, and then it would come back. To an end user that’s bad enough, but to another system that is taking actions based on the data, the results can be inconsistent at best, and deleterious at worst. What we really want is the ability to modify an item (or items) in an array by query, effectively $set with $pull semantics: Since that’s not yet supported by MongoDB, we decided to use a map. This makes a lot of sense for our use case since the structure happened to be a map in Java. Because MongoDB uses JSON-like structures, the map is an object just like any other, and the keys are simply the property names of the object. This means that individual elements can be accessed and updated from the map: This seems like a rather elegant solution, so why didn’t we start out this way? One pretty major problem: if you don’t know all of the possible keys to the map, there is no good way to index the fields for faster querying. Indexes to child objects in MongoDB are created using dot notation, just like the query syntax. This works really well with arrays because MongoDB supports indexing fields of objects in arrays just like any other child object field. But for a map, the key is part of the path to the child field, so each key is another index: For our purposes, we are actually able to index every key individually, and treat the map as an object with known properties. The sinister part of all of this is that you won’t normally run into any problems while doing typical testing of the system. And in the case of the pull/read/push diagram above, even typical load testing wouldn’t necessarily exhibit the problem unless the reader is checking every response and looking for inconsistencies between subsequent reads. In a complex system, where data is expected to change constantly, this can be a difficult bug to notice, and even more difficult to track down the root cause. There are ways to make MongoDB work really well as the primary store, especially since there are so many ways to update individual records atomically . However, working with MongoDB and other NoSQL solutions requires a shift in how developers think about their data storage and the way they write their query/update logic. So far it’s been a joy to use, and getting better all the time as we learn to avoid these types of problems in the future.", "date": "2012-04-18"},
{"website": "BazaarVoice", "title": "Technical Talk: How Bazaarvoice is using Cassandra and Elastic Search", "author": ["RC Johnson"], "link": "https://blog.developer.bazaarvoice.com/2013/06/18/technical-talk-how-bazaarvoice-is-using-cassandra-and-elastic-search/", "abstract": "In late May the Bazaarvoice team was delighted to speak again as a part of Database Month in New York City, and we were excited to speak about our recent work with Cassandra & Elastic Search. We discussed our goals for replacing the Bazaarvoice data infrastructure, as well as our hopes for the new system, and then we dove into the internal details of how we’re using Cassandra and Elastic Search to handle the scale needed by the myriad Bazaarvoice applications. We had a lot of fun at the talk as well as answering questions for quite some time aftewards, and we’re always excited to talk about this even more.", "date": "2013-06-18"},
{"website": "BazaarVoice", "title": "Scaling on Mysql: Sometimes you’ve gotta break a few eggs", "author": ["RC Johnson"], "link": "https://blog.developer.bazaarvoice.com/2012/05/03/scaling-on-mysql-sometimes-youve-gotta-break-a-few-eggs/", "abstract": "We recently delivered this presentation titled “How to Scale Big on MySQL? Break a Few Rules!” as part of Database Week here in New York City. The presentation is a lighthearted, and informative take on how Bazaarvoice Engineering has been able to take MySQL to billions of requests per month. The slides and video are available over at LeadIt.us . In the presentation we cover denormalization, query planning, partitioning, MySQL replication, InfoBright’s take on a data storage, and thinking beyond the RDBMS. Overall the presentation is a little over an hour, and is littered with great questions. I had a great time delivering the presentation, and I got a lot of very good feedback so I hope it proves useful for you as well.", "date": "2012-05-03"},
{"website": "BazaarVoice", "title": "Platform API Release Notes, Version 5.2", "author": ["Craig Gilchrist"], "link": "https://blog.developer.bazaarvoice.com/2012/06/01/platform-api-release-notes-version-5-2/", "abstract": "We are pleased to announce that the following functionality has been developed for version 5.2: More detailed information on each of these items is listed below. For complete documentation, refer to the Platform API Documentation, version 5.2 . Helpfulness and inappropriate feedback Submission of helpfulness votes and inappropriate feedback can now be done through the API. The response for the user-generated content has also been updated to display the actual inappropriate feedback and total vote and feedback counts. In order for inappropriate feedback to be populated, the API key must be updated. Implicit default ContentLocale filter removed There is no longer any implicit ContentLocale filter if none is specified as an argument. If no filter is provided, all content will be returned, regardless of what locale the content is in. There is a default locale defined for every API key. Prior to version 5.2, if the locale parameter was used, it caused an implied ContentLocale filter to be used. Note that version 5.2 does not change the behavior of explicitly supplied ContentLocale filters. In addition, you can now ask for labels in any locale and specify a different content locale. Therefore, if you request a locale of en_US and a ContentLocale of fr_FR, you get English labels and French content. Product and category attributes Products and categories now have a new attributes field populated. This field contains a map of attributes provided to Bazaarvoice from a product feed import. Hosted video submission and display Video elements of all content now contain URLs that can be used to embed the video into an HTML page. Bazaarvoice provides boilerplate HTML tags for use with embedding these videos. For more information, see the API Basics page. Inline ratings data A new method has been created to provide a quick way to access inline ratings data for products. For complete documentation, see the Statistics Display method page.", "date": "2012-06-01"},
{"website": "BazaarVoice", "title": "Ember.js — the framework formerly known as SproutCore 2.0 (Amber.js)", "author": ["Michael Norman"], "link": "https://blog.developer.bazaarvoice.com/2012/03/21/ember-js-the-framework-formerly-known-as-sproutcore-2-0-amber-js/", "abstract": "Update: It looks like there was a conflict with the name Amber with another JavaScript project, so the SproutCore folks have graciously decided to change their name to Ember.js so as not to be in conflict. Check out the details at Yehuda’s blog . The following is the original blog post: Yehuda Katz has just officially announced the rebranding of SproutCore 2.0 to Amber.js. This is refreshing for all the reasons he mentions in his post, especially when attempting to learn new things about the framework. It gets confused with SproutCore 1.0 quite a bit, and I can imagine trying to find answers about Amber.js will be a much easier experience! Bazaarvoice’s newest product, Customer Intelligence , has been using SproutCore 2.0 since well before its release, and one of the primary developers on that team, Daniel Marcotte, spent a fair bit of time over several months working with the nascent framework and helping to get Beta 2 out the door. Look for an upcoming post where Daniel goes into much more detail on that process.", "date": "2012-03-21"},
{"website": "BazaarVoice", "title": "The Tools We Use to Innovate in Bazaarvoice Labs (Part 1)", "author": ["Jon Loyens"], "link": "https://blog.developer.bazaarvoice.com/2011/12/19/the-tools-we-use-to-innovate-in-bazaarvoice-labs-part-1/", "abstract": "Hi everyone! This is my first post to the Bazaarvoice Developer blog and I’d like to take this opportunity to shed some light on some of the tools Bazaarvoice Labs has recently found very useful in creating the pilots and prototypes that ultimately morph into new products and features on the Bazaarvoice platform. Before I talk about our toolset though, I’d like to give you a quick rundown on what Bazaarvoice Labs is, our process and why it’s important for us to be flexible in our toolset choices. Bazaarvoice Labs is the new product research and development group at Bazaarvoice with emphasis on the new and research . We are actually a team of engineers that report to our Product Management team (rather than through the engineering group) that help our Product Managers realize their wildest (and potentially most game-changing) ideas. Every quarter we evaluate and prioritize new ideas proposed by our Product Management team, customers and Bazaarvoicers around the company in order to research and create prototypes. The ideas we prioritize highest are those that come with big hairy assumptions but could change our business if they work. By building prototypes we’re able to suss out where the trouble might lie if we were to introduce the new product or feature to our entire customer base. We currently have over one thousand of the world’s biggest brands hosting their user-generated content in our platform and many large services organization to boot. The introduction of even a small new feature can have very large consequences to our organization. So on the risky stuff, we like to know where the gotchas lie. Some of the products spawned out of this process include BrandAnswers and Ratings and Reviews for Facebook (part of our SocialConnect Suite ). In order to build a prototype, we assign an engineer to work directly with a Product Manager or Product Designer. These two work together in an agile manner (agile with a little-a, not a capital A) in order to create a tangible prototype that demonstrates the Product Manager’s idea unencumbered by writing lots of requirements or unnecessary process. It’s this one-to-one relationship that makes this process hum, gives the creative process a kick in the pants and really lets these ideas properly gestate. Once more people get involved in the project, due to network effect, managing the project gets exponentially harder with each person you add and the need for process increases as a way to mitigate risk. By imposing a one-to-one structure for our prototyping teams we strip away any unnecessary obstacles to creativity and give real creative ownership to our Product Managers, Designers and Engineers. In a way, these teams become entrepreneurial cofounders as they attempt to prove their ideas. Additionally, by artificially constraining the initial project team to one engineer, the team is focused on building out the Minimally Viable Product needed to prove their assumptions and build a business case before further investment is required. Another nice side effect of this style of working is that it allows the engineer working on the project to choose their own tool-chain for each new project. Since they’re working alone on a project, there’s no need to constrain the tool choices to lowest common denominator of what every team member might already know or be familiar with. Of course, it’s up to the engineer’s discretion to reuse code or tools that may already be in use at Bazaarvoice but that choice ultimately lies with the engineer and the engineer knows to optimize around speed of creation vs. other organizational considerations. One nice side effect of an engineer being able to choose a new tool-chain with every project is that, in addition to proving business and product ideas, emerging technologies can be realistically evaluated and, where appropriate, integrated into our core engineering stack (this happened with requireJS which has become an integral part of how we deploy Javascript on our customer’s sites). Sometimes simply building a prototype may not answer the questions that we have around the viability of the product and we need to take further steps to answer the questions we might have. For example, we needed to answer the following question for Ratings and Reviews for Facebook: “Will people be willing to read and write Product reviews inside a Facebook app?” In this case, a prototype isn’t enough. We needed to progress to the next phase of the process and actually pilot the application we had built with a couple of customers. For this reason, the “prototypes” we build need to be more robust than what you might initially think. Yes, we’re building concept cars in Labs but our concept cars actually need to run. We generally launch these pilots with three to five customers and generally won’t internationalize them. These restrictions keep us agile and help make sure we don’t have to build too much customizability into the pilots. Even though these pilots will only be launched with a handful of customers, some of these will be placed in some very high profile, high traffic places (some getting over 100,000 hits per day). Examples of running pilots right now include Nikon’s Ask and Answer for Facebook , Travelocity’s Social Connect Discovery Pilot (see the “Traveler Reviews” link) and TurboTax’s People Like You review search tool. Of course, when we launch pilots we track the data and rapidly move to improve the product and build out a suitable business case for productization. In the pilot phase the engineers are free to launch new code whenever they choose and must play the roles of UX, server side and operations engineer. By being chief cook and bottle washer on these projects, it frees the owning engineer (there’s still only one per project) to push releases as frequently as necessary to build that business case and observe how changes affect the project’s KPIs. So what tools do we use to build software in Labs? Let’s review the two phases of our projects momentarily: The tools we chose to build with in Bazaarvoice Labs must support two phases of a project: Because our development cycles are so short at Bazaarvoice, projects must also be able to transition between the prototype and pilot phase seamlessly. The tools we select must therefore support the requirements mentioned above. Generally we can divide our tool-chain into a few broad categories: In my next blog post, I’m going to step through each of these categories and talk about a couple of tools that we use and the projects that we’ve used them in. This will not be an exhaustive list since we’re always evaluating new tools but it should give you some insight into the how and why we pick the tools that we do.", "date": "2011-12-19"},
{"website": "BazaarVoice", "title": "Root Cause Analysis for Hadoop Applications", "author": ["Parth Shah"], "link": "https://blog.developer.bazaarvoice.com/2019/08/07/root-cause-analysis-for-hadoop-applications/", "abstract": "Parth Shah and Thai Bui One of the reasons why Hadoop jobs are hard to operate is their inability to provide clear, actionable error diagnostic messages for users. This stems from the fact that Hadoop consists of many interrelated components. When a component fails or behaves poorly, the failure will be cascaded to its dependent components which causes a job to fail. This blog post is an attempt to help to solve that problem by created a user-friendly, self-serving and actionable Hadoop diagnostic system. Due to its complex nature, the project was split into multiple components. First, we prototyped a diagnostic tool to help debug Hadoop application failure by providing a clear root cause analysis and save engineering time. Second we purposely inflict failures on a cluster (via a method called chaos testing) and collected the data to understand how certain log messages map to errors. Finally, we examined regular expression as well as natural language processing (NLP) techniques to automatically provide root cause analysis in production. To document this, we organized the blog in the following sections: Bazaarvoice provides an internal portal tool for managing analytics applications by the end-users. The following screenshot demonstrates an example message of a “Job failures due to missing data”. This message is caught by using a simple regular expression of the stack trace. A regular expression works because there is only one way a job could fail due to missing data. Partitioning is a strategy of dividing a table into related parts based on date, components or other categories. Using a partition, it is easy and faster to query a portion of data. However, the partition a job is querying can sometimes not be available which causes a job to fail by our design. The dashboard below calculates metrics and keeps track of jobs that failed due to an unavailable partition. The dashboard classifies failed jobs as either a partition failure (late/missing data) or an unknown failure (Hadoop application failure). Our diagnostic tool attempts to find the root cause of the unknown failures since a late or missing data is an easy problem to solve. Since our clusters are powered by Apache Ambari, we leveraged and enhanced Ambari Logsearch Logfeeder to ship logs of relevant services directly to S3 and partitioned the data by as shown in the raw_log of the Directory Tree Diagram below. However, as the dataset got bigger, partitioning was not enough to efficiently query the data. To speed up read performance and to iterate faster, the data was later converted into ORC format. The chaos data generation process makes up a bulk of this project as well is the most important. It stems from the process of chaos testing to experiment on software and infrastructure in order to understand the systems ability to withstand unexpected or turbulent conditions. This concept of testing was first introduced by Netflix in 2011. The following pseudocode explains how it works. We followed the same process to generate chaos data on different types of injected failures such as: While there are certainly more errors the model is capable of learning and classifying, for the purpose of prototyping we kept the type of failures to 2-3 categories. In this section, we explore 2 types of error classification methods, a simple regex and supervised learning. There are many ways to analyze text for different patterns. One of these ways is called Regular Expression Matching (regex). Regex is “special strings representing a pattern to be matched in search operation”. One use of regex is finding a keyword like “partition”. When a job fails due to missing partitions its error message usually looks something like this “ Not all partitions are available even after 16 attempts “. The regex for this specific case would look like \\W*((?i)partition(?-i))\\W* . A regex log parser could easily be able to identify this error and do what is necessary to fix the problem. However, the power of regex is very limiting when it comes to analyzing complex log messages and stack traces. When we find a new error, we would have to manually hardcode a new regular expression to pattern match the new error type which is very error-prone and tedious in the long run. Since the Hadoop eco-system includes many components, there are many combinations of log messages that can be produced; a simple regex parser would be difficult to work here because it can not process general similarities between different sentences. This is where natural language processing helps. The idea behind this solution is to use natural language processing to process log messages and supervised learning to learn and classify errors. This model should work well with log data vs. normal language due since machine logs are more structured and low in entropy. You can think of entropy as a measure of how unstructured or random a set of data is. The English language tends to have high entropy relative to machine logs due to its sometimes illogical sentence structure. On the other hand, machine-generated log data is very repetitive which makes it easier to model and classify. Our model required pre-processing of logs which are called tokenization. Tokenization is the process of taking a set of text and breaking it up into its individual words or tokens. Next, the set of tokens were used to model their relationship in high dimensional space using Word2Vec. Word2Vec is a widely popular model used for learning vector representation of words called, “word embeddings” ( word2vec ). Finally, we use the vector representation to train a simple logistic regression classifier using the chaos data generated earlier. The diagram below shows a similar training processing from Experience Report: Log Mining using Natural Language Processing and Application to Anomaly Detection by Christophe Bertero, Matthieu Roy, Carla Sauvanaud and Gilles Tredan. In contrast to the diagram above, since we only trained the classifier on error logs, it was not possible to classify a normal system as it should produce no error logs. Instead, we trained the data on different types of stressed systems. Using the dataset generated by chaos testing, we were able to identify the root cause for each of the error message for each failed job. This allowed us to make our training dataset as shown below. We split our dataset into a simple 70% training and 30% for testing. Subsequently, each log message was tokenized to create a word vector. All the tokens were inputted into a pre-trained word2vec model which mapped out each word in a vector space, creating a word embedding. Each line was represented as a list of vectors and an average of all the vectors in a log message represents the feature vector in high dimensional space. We then inputted each feature vector and its labels into a classification algorithm such as logistic regression or random forest to create a model that could predict the root cause error from a logline. However, since failure often contains multiple error log messages, it would not be logical to simply reach a root cause conclusion from just a single line of the log from a long error log. A simple way to tackle this problem would be to window the log and input the individual lines from the window into the model and output the final error as the most common error outputted by all the lines in a window. This is a very naive way of breaking apart long error log so more research would have to be done to process error logs without losing the valuable insight of their dependent messages. Below are a few examples of error log messages and their tokenized version. The model accurately predicted the root cause of failed job with 99.3% accuracy in our test dataset. At an initial glance, the metrics calculated on the test data look very promising. However, we still need to evaluate its efficiency in production to get a more accurate picture. The initial success in this proof of concept warrants further experimentation, testing, and investigation for using NLP to classify errors. In order to implement this tool in production, data engineers will have to automate certain aspects of the data aggregation and model building pipeline A machine learning model is only as good as its data. For this tool to be robust and accurate, any time engineers encounter a new error or an error that the model does not know about, they should report their belief of the root cause so the corresponding errors logs get tagged with the specific root cause error. For instance, when a job fails for a similar reason, the model will be able to diagnose and classify its error type. This can be done through a simple API, form, Hive query that takes in the job id and its assumed root_cause. The idea behind is this that by creating a form, we could manual label log messages and errors in case the classifier fails to correctly diagnose the true problem. The self-reported error should take the form of the chaos error to ensure the existing pipeline will work. The chaos tests should run on a regular basis in order to keep the model up to date. This could be done by creating a Jenkins job that automatically runs on a regular cadence. Often times, running a stress test will make certain nodes to be unhealthy. It causes our automation to be unable to SSH into the nodes again to stop the stress test such as when the stress test is interfering with connectivity to the node (see the error below). This can be solved by creating a time limit for the stress test, so the script does not have to ssh again once the job has finished. In the long run, as the self-reported errors grow, the model should rely on chaos test generated test less. Chaos tests produce logs for very extreme situations which could be not typical for a normal production environment.", "date": "2019-08-07"},
{"website": "BazaarVoice", "title": "Hackathon 2019", "author": ["John Banning"], "link": "https://blog.developer.bazaarvoice.com/2019/05/21/hackathon-2019/", "abstract": "This year’s Bazzarvoice Hackathon coincided with our annual all hands meeting in Austin. Our global offices took time to work on projects that focused on innovation, social integrations, and improved efficiencies. Teams across our departments participated This included: R&D, Product, Customer Services, and Knowledge Base. Hackathon teams took two days to work on their projects. The following day, teams present their outcomes in a science fair setting while the company voted on the projects.  The top 10 teams then then went on to present to the entire company. Thanks to all who participated and especially those who organized the various activities. We hope to see many of these projects become new product enhancements.", "date": "2019-05-21"},
{"website": "BazaarVoice", "title": "Response API Demo App", "author": ["Brijendra Nag"], "link": "https://blog.developer.bazaarvoice.com/2019/04/29/response-api-demo-app/", "abstract": "Are you looking to develop your own application on top of the Bazaarvoice Response API ? Well, we got something for you. The Response API Demo App is a simple Node-React application which demonstrates how to use Response API in conjunction with our 3-legged OAuth2 API. It is recommended to go through the Developer Portal and read about OAuth2 API and the Response API before diving into the application architecture below. This application was bootstrapped with Create React App and consists of two separate components – the front-end client side in React and a back-end server side in NodeJS. Let’s talk more about the back-end architecture first. Almost all of the server-side logic is contained in server.js . Using the Express framework for NodeJS, this file defines the following endpoints: This application uses express-session for storing and maintaining user sessions using browser cookies which is important for being able to maintain multiple users without storing state on your backend. However, this implementation of user sessions is not suitable for production applications and you should probably be maintaining user sessions using a combination of cookies and session storage. The server side credentials and configurations are supposed to be stored in server/server-config.json which is then picked up by the Node server. Note that these credentials are confidential and should not be exposed to the client side in any way. Coming to the front-end architecture, we are using the React library and React Semantic UI for the UI Components. The client side does a bunch of stuff ranging from querying a review from the Conversations API and then using that Review ID to obtain/add/modify/delete the corresponding responses to that review using the Response API . Following components make up the core of the front-end: You might have noticed a department field on a response which appears as a drop-down menu. These menu options have been hard-coded in the demo application and can be modified in the departmentFormOptions.js file. Further, client.js is an utility file that presents all the commonly used API calls as simple functions accessible to all components on the client side. The client side configurations are stored in response-demo/client/src/utils/config.js file. This is built into the the project when the front-end client is packaged before running. These configurations are accessible to the browser so you shouldn’t store anything confidential here. Apart from that, the application follows a pretty standard architecture and you can read more about it here . On the deployment side of things, there is a Dockerfile which builds the client artifacts, sets up the server and gets the application up and running on your local environment. Follow these instructions to get it up and running locally. The app can also be deployed to Flynn by making changes to the redirect URI in configurations and also making sure that your Flynn Redirect URI is added to the list of allowed Redirect URIs for your app credentials.", "date": "2019-04-29"},
{"website": "BazaarVoice", "title": "Vger Lets You Boldly Go . . .", "author": ["patrick.sullivan"], "link": "https://blog.developer.bazaarvoice.com/2019/01/18/vger-lets-you-boldly-go/", "abstract": "Are you working on an agile team? Odds are high that you probably are. Whether you do Scrum/Kanban/lean/extreme, you are all about getting work done with the least resistance possible. Heck, if you are still on Waterfall, you care about that. But how well are you doing? Do you know? Is that something a developer or a lead should even worry about or is a SEP? That’s a trick question. If your team is being held accountable and there is a gap between their expectations and your delivery, by the transitive property, you should worry about some basic lean metrics. Here at Bazaarvoice, we are agile and overwhelmingly leverage kanban. Kanban emphasizes the disciplines of flow and continuous improvement. In an effort to make data-driven decisions about our improvements, we needed an easy way to get the relevant data. With just JIRA and GitHub alone, access to the right data has a significant barrier to entry. So, like any enterprising group of engineers, we built an app for that. Some of us had recently gone through an excellent lean metric forecasting workshop with Troy Magennis from Focused Objective . In his training he presented the idea of displaying a quadrant of lean metrics in order to force a narrative for a teams behavior, and to avoid overdriving on a single metric. This really resonated with me and seemed like good paradigm for the app we wanted to build. And thus, Vger was born. We went with a simple quadrant view with very bookmarkable url parameters. We made it simple to for teams to self-service by giving them an interface to make their own “Vger team” and add whatever “Vger boards” they need. Essentially, if you can make a JQL query and give it a board in JIRA, Vger can graph the metrics for it. In the display, we provide a great deal of flexibility by letting teams configure date ranges for the dashboard, work types to be displayed, and the JIRA board columns to be considered as working/non-working. Now the barrier to entry for lean metrics is down to “can you open a browser.” Not too shabby. We show the following in the quadrant view: 1. Throughput – The number of completed tickets per week. 2. Variation – the variation (standard deviation/mean) for the Throughput. 3. Backlog Growth – the tickets opened versus closed. 4. Lead Times – The lead times for the completed tickets. This also provides a detailed view by Jira board column to see where you spend most of your time. We at Bazaarvoice are conservative gamblers, so you’ll see the throughput and lead time quadrants show the 50%, 80%, and 90% likelihood (the inverse of percentile). We do this because relying on the average or mean is not your friend. Who want’s to bet on a coin toss? Not us. We like to be right say, eight out of ten times. Later, we were asked to show throughput by quarter to help with quarterly goal planning. We created a side-car page for this. It shows Throughput by quarter: We also built a scatterplot for lead times so outliers could be investigated: This view has zoomable regions and each point lets you click through to the corresponding JIRA ticket. So that’s nice. From day one, we chose to show the same Quadrant for GitHub Pull Requests. Note that we show rejected and merged lines in the PR Volume quadrant. We also support overlaying your git tags on both PR and JIRA ticket data. Pretty sweet! Vger lets you download throughput data from the Quadrant and Quarterly views. You can also download lead time from the Quarterly view too. This lets teams and individuals perform their own visualizations and investigations on these very useful lean metrics. Vger was built with three use cases in mind: Teams should have easy access to these key lean metrics in their retros. We recommend that they start off viewing the quadrant and seeing if they agree with the narrative the retro facilitator presents. They should also consider the results of any improvement experiments they tried. Did the new behavior make throughput go up as they hoped it would? It the new behavior reduce time spent in code review? Did it reduce the number open bugs? etc. Certainly not everything in a retro should be mercilessly data-driven, but it is a key element to a culture of continuous improvement. Team managers commonly speak to how their teams are progressing. These discussions should be data-driven, and most importantly it should be driven by the same data the team has access to (and hopefully retros to). It should also be presented in a common format that still provides for some customization. NOTE: You should avoid comparing team to team in Vger or a similar visualization. In most situations, that way leads to futility, confusion, and frustration. Lean forecasting is beyond the scope of this post however, Troy Magennis has a fine take on it . My short two cents on the matter is: a reasonably functioning team with even a little bit of run time should never be asked “how long will it take?” Drop that low value ritual and do the high value task of decomposing the work, then forecast with historical data. Conveniently, you can download this historical data from Vger you used in your spreadsheet of choice. I happen to like monte carlo simulations myself. You’ll note I used the term “lean metrics” throughout. I wanted to avoid any knee-jerk “kanban vs scrum vs ‘how we do things'”reaction. These metrics apply no matter what methodology you consciously (or unconsciously) use for the flow of work through your team. It was built for feature development teams in mind, but we had good success when our client implementation team started using it as an early adopter. It allowed them to have a clear view into their lead time details and ferret out how much time was really spent waiting on clients to perform an action on their end. We open sourced it here , so help yourself. This presented as “it worked for us”-ware and is not as finely polished as it could be, so it has some caveats. It is a very simple serverless app. We use JIRA and GitHub , so only those tools are currently supported. If you use similar, give Vger a try! If your fingers are itching to contribute, here’s some ideas:", "date": "2019-01-18"},
{"website": "BazaarVoice", "title": "Still Looking Good While Testing: Automated Testing With a Visual Regression Service Part II", "author": ["Gary Spillman"], "link": "https://blog.developer.bazaarvoice.com/2018/12/12/still-looking-good-while-testing-automated-testing-with-a-visual-regression-service-part-ii/", "abstract": "If you’ve followed our blog regularly, you’ve probably read our post on using visual regression testing tools and services to better test your applications’ front end look and feel.  If not, take a few minutes to read through our previous post on this topic. Now that you’re up to speed, let’s take what we did in our previous post to the next level. In this post, we’re going to show how you can alter your test configuration and code to test mobile browsers through a given testing service ( browserstack ) as well as get better reporting out of our tests natively and through options available to us in Jenkins. If you run your test multiple times across several browsers with the visual regression service  using default settings, you may notice a handful of exception images cropping up in the diff directory which look nearly identical. Browser fight!!! If you happen to be testing across several browsers (using Browserstack or a similar service) which potentially have these browsers hosted in real or virtual environments with widely differing screen resolutions, this may impact how the visual-regression service performs its image diff comparison. To alleviate this, either ensure your various browser hosts are able to display the browser in the same resolution or add a slight increase in your test’s mismatch tolerance. As mentioned in our previous post, you can do this by editing the values under the ‘visual regression service’ object in your project’s wdio.conf file.  For example: Setting the mismatch tolerance value to 0.25 in the above snipped would allow the regression service a 25% margin of error when checking screen shots against any reference images it has captured. Also mentioned in the previous post, one of the drawbacks to given examples of using the visual-regression service in our tests is that there is little feedback being returned other than the output of our image comparison. Having fun with that lack of feedback? However, with a bit of extra code, we can make usable assertion statements in our test executions once a screen comparison event is generated. The key is the checkElement() method which is really a WebdriverIO method that is enhanced by the visual-regression service.  This method returns an object that contains some meta data about the check being requested for the provided argument. We can assign the method call to a new variable, which, once we ‘deserialize’ the object into something readable (e.g. a JSON string) we can then leverage Chai or some other framework to use assertions to make our tests more descriptive. Here’s an example: In the above code snippet, near the end of the test, we are calling ‘checkElement()’ to do a visual comparison of the contents of the given web element selector, then converting the object returned by ‘checkElement()’ to a string.  Afterward, we are asserting there is some text/string content within the stringified object our comparison returned. In the case of the text assertion, we would want to assert a successful match message is contained within the returned object.  This is because the ‘checkElement()’ method, while it may return data that indicates a test failure, on its own will not trigger an exception that would appropriately fail our test should an image comparison mismatch occur. Oooh – someone downloaded the pancake app! Combining WebdriverIO’s framework along with the visual-regression service and a browser test service like Browserstack, we can create tests that run against real browsers.  To do this, we will need to make some changes to our WebdriverIO config.  Try the following: Next, we need to edit the contents of the wdio.mobile.conf.js script to run tests only against mobile devices.  The reason for adding a whole, new test config and script declaration is that due to the way mobile devices behave, there are some settings to declare for mobile browser testing with WebdriverIO and Browserstack which are incompatible with running tests against desktop browsers. Edit the code block at the top of the mobile config file, changing it to the following: Note that we’ve removed the declarations for height and width dimensions.  As Browserstack allows us to test on actual mobile devices, defining viewport constraints are not only unnecessary, but it will result in our tests failing to execute (the height and width dimension object can’t be passed to webdriver as part of a configuration for a real, mobile device). Next, update the visual-regression service’s orientation configuration near the bottom of the mobile config files to the following: orientations: ['portrait', 'landscape'], Since applications using responsive design can sometime break when moving from one orientation to another on mobile devices, we’ll want to run out tests in both orientation modes. Stating this behavior in our configuration above will trigger our tests to automatically switch orientations and retest. Finally, we’ll need to update our browser capability settings in this config file: In the code above, the ‘realMobile’ descriptor is necessary to run tests against modern mobile devices through Browserstack.  For more information on this, see Browserstack’s documentation . Once your change is saved, try running your tests on mobile by doing: npm run test:mobile Examine the image outputs of your tests and compare them to the results from your desktop test run.  Now you should be able to run tests for your app’s UI across a wide range of browsers (mobile and desktop). You could include the project we’ve put together so far into the codebase of your app (just copy the dev dependencies from your package.json your specs and configs into the existing project). Another option is to use this as a standalone testing tool to apply some base screen-shot based verification of your app. In the following example, we’ll go through the steps you can to set up a project like this as a resource to use in Jenkins to test a given application and send the results to members of your team. Consider the value of being able to generate a series of targeted screen shots of elements of your app per build and send them to team members like a product manager or UX designer.  This isn’t necessarily ‘heavy lifting’ from a dev ops standpoint but automating any sort of feedback for your app will trend toward delivering a better app, faster. Assuming you have your project hosted in Github, you have administrative access to Jenkins and that your app is hosted in an environment Jenkins can access (your organization’s Amazon S3 bucket, hosted from a Docker image, etc.): * Create a new Jenkins task within the same space the job that builds and ships your web app (should one exist).  Assign it a name. * Go to the source control configuration portion of the job.  Enter the info for your repository for the test app you’ve built. * Within the build settings of the job, choose the build periodically option and configure your job’s frequency. Ideally, you would set this job to trigger once the task that builds and ‘ships’ your app completes successfully (thus, executing your tests every time a new version of the app is published). Alternatively, to run periodically based on a given time, then enter something like ‘* 23 * * *’ into Jenkins’ cron configuration field to set your job to run once every day (in this case, at 11 PM, relative to your server’s configuration). * Create a build step and for your task and choose the shell script option.  You can embed your npm test script execution here. From the list of post build options, choose the ‘archive artifacts’ option.  In the available text field, enter the path to the generated screen shots you wish to capture.  Note that this path may differ depending on how your Jenkins server is configured.  If you’re having trouble pinpointing the exact path to your artifacts, echo the ‘pwd’ command in your shell script step to have the job list your working directory in the job’s console output then work from there. * Lastly, choose from the post build options menu the advanced email notification option. Enter an email or list of email addresses you wish to contact once this job completes.  Fill out your email settings (subject line, body, etc.) and be sure to enter the path to the screen resources you will wish to attach to a given email. Save your changes and you are ready to go! This job will run on a regular basis, execute a given set of UI specific tests and send screen capture information to inquiring team members who would want to know. You can create more nuanced runs but using Jenkins’ clone feature to copy this job, altering to run your mobile-specific tests, diversifying your test runs. If you’re looking to dive further into visual regression testing, there are several code examples online worth reviewing. Additionally, there are other visual regression testing services worth investigating such as Percy.IO , Screener.IO and Fluxguard .", "date": "2018-12-12"},
{"website": "BazaarVoice", "title": "Looking Good While Testing: Automated Testing With a Visual Regression Service", "author": ["Gary Spillman"], "link": "https://blog.developer.bazaarvoice.com/2018/10/04/looking-good-while-testing-automated-testing-with-a-visual-regression-service/", "abstract": "A lot of (virtual) ink has been spilled on this blog about automated testing (no, really ). This post is another in a series of dives into different automated testing tools and how you can use them to deliver a better, higher-quality web application. Here, we’re going to focus on tools and services specific to ‘visual regression testing ‘ – specifically cross-browser visual testing of an application front end. By visual regression testing, we mean regression testing applied specifically to an app’s appearance may have changed across browsers or over time as opposed to functional behavior. One of the most common starting points in testing a web app is to simply fire up a given browser, navigate to the app in your testing environment, and note any discrepancy in appearance (“oh look, the login button is upside down. Who committed this!?”). A spicy take on how to enforce code quality – we won’t be going here. The strength of visual regression testing is that you’re testing the application against a very humanistic set of conditions (how does the application look to the end user versus how it should look). The drawback is that doing this is generally time consuming and tedious.  But that’s what we have automation for! How our burger renders in Chrome vs how it renders in IE 11… A wise person once said, ‘In software automation, there is no such thing as a method call for “if != ugly, return true”‘. For the most part – this statement is true. There really isn’t a ‘silver bullet’ for the software automation problem of fully automating testing the appearance of your web application across a given browser support matrix.  At least not with some caveats. The methods and tools for doing so can run afoul of at least one of the following: Sure, you can support delicate tools. Just keep in mind the total cost. We’re going to show how you can quickly set up a set of tools using WebdriverIO , some simple JavaScript test code and the visual-regression-service module to create snapshots of your app front end and perform a test against its look and feel. Assuming you already have a web app ready for testing in your choice of environment (hopefully not in production) and that you are familiar with NodeJS, let’s get down to writing our testing solution: This meme is so old, I can’t even… 1. From the command line, create a new project directory and do the following in order: 2. Once you’ve finished installing your modules, you’ll need to configure your instance of WebdriverIO. You can do this manually by creating the file ‘wdio.conf.js’ and placing it in your project root (refer to the WebdriverIO developers guide on what to include in your configuration file) or you can use the wdio automated configuration script. 3. To quickly configure your tools, kick off the automated configuration script by running ‘npm run wdio’ from your project root directory.  During the configuration process, be sure to select the following (or include this in your wdio.conf.js file if you’re setting things up manually): Note that in this case, we won’t install the selenium standalone service or any local testing binaries like Chromedriver . The purpose of this exercise is to quickly package together some tools with a very small footprint that can handle some high-level regression testing of any given web app front end. Once you have completed the configuration script, you should have a wdio.conf.js file in your project configured to use WebdriverIO and the visual-regression service. Next, we need to create a test. First, make a directory within your project’s main source called tests/. Within that directory, create a file called homepage.js. Set the contents of the file to the following: That’s it. Within the single test function, we are calling a method from the visual-regression service, ‘checkElement()’. In our code, we are providing the ID, ‘header’ as an argument but you should replace this argument with the ID or CSS selector of a container element on the page you wish to check. When executed, WebdriverIO will open the root URL path it is provided for our web application then will execute its check element comparison operation. This will generate a series of reference screen shots of the application. The regression service will then generate screen shots of the app in each browser it is configured to test with and provide a delta between these screens and the reference image(s). You may have a need to articulate part of your web application before you wish to execute your visual regression test. You also may need to execute the checkElement() function multiple times with multiple arguments to fully vet your app front end’s look and feel in this manner. Fortunately, since we are simply inheriting the visual regression service’s operations through WebdriverIO, we can combine WebriverIO-based method calls within our tests to manipulate and verify our application: One of several factors that can add to the fragility of a visual test like this is attempting to account for minor changes in your visual elements. This can be a lot to bite off and chew at once. Attempting to check the content of a large and heavily populated container (e.g. the body tag) is likely going to contain so many possible variations across browsers that your test will always throw an exception. Conversely, attempting to narrow your tests’ focus to something very marginal (e.g. selecting a single instance of a single button) may never be touched by code implemented by front end developers and thus, you may be missing crucial changes to your app UI. This is waaay too much to test at once. The visual-regression service’s magic is in that it allows you to target testing to specific areas of a given web page or path within your app – based on web selectors that can be parsed by Webdriver. And this is too little… Ideally, you should be choosing web selectors with a scope of content that is not too large nor too small but in between. A test that focuses on comparing content of a specific div tag that contains 3-4 widgets will likely deliver much more value than one that focuses on the selector of a single button or a div that contains 30 widgets or assorted web elements. Alternately, some of your app front end may be generated by templating or scaffolding that never received updates and is siloed away from code that receives frequent changes from your team. In this case, marshalling tests around these aspects may result in a lot of misspent time. But this is just about right! Choose your area of focus accordingly. Before we run our tests, let’s make a few updates to our config file to make sure we are ready to roll with our initial homepage verification script. First, we will need to add some helper functions to facilitate screenshot management. At the very top of the config file, add the following code block: This function will be utilized to build the paths for our various screen shots we will be taking during the test. As stated previously, we are leveraging Browserstack with this example to minimize the amount of code we need to ship (given we would like to pull this project in as a resource in a Jenkins task) while allowing us greater flexibility in which browsers we can test with. To do this, we need to make sure a few changes in our config file are in place. Note that if you are using a different browser provisioning services ( SauceLabs , Webdriver’s grid implementation), see WebdriverIO’s online documentation for how to set you wdio configuration for your respective service here). Open your wdio.conf.js file and make sure this block of code is present: This allows us to pass our browser stack authentication information into our wdio script via the command line. Next, let’s set up which browsers we wish to test with. This is also done within our wdio config file under the ‘capabilities’ object. Here’s an example: While we are here, be sure you have set up your config file to specifically point to where you wish to have your screen shots copied to. The visual-regression service will want to know the paths to 4 types of screenshots it will generate and manage: Yup… Too many screens References : This directory will contain the reference images the visual-regression service will generate on its initial run. This will be what our subsequent screen shots will be compared against. Screens: This directory will contain the screen shots generated per browser type/view by tests. Errors: If a given test fails, an image will be captured of the app at the point of failure and stored here. Diffs: If there is a given comparison performed by the visual-regression service between an element from a browser execution and the reference images which results in a discrepancy, a ‘heat-map’ image of the difference will be capture and stored here. Consider the content of this directory to be your test exceptions. Fuzzy… Not Fozzy Finally, before kicking off our tests, we need to enable our visual-regression service instance within our wdio.conf.js file. This is done by adding a block of code to our config file that instructs the service on how to behave. Here is an example of the code block taken from the WebdriverIO developer guide: Place this code block within the ‘services’ object in your file and edit it as needed. Pay attention to the following attributes and adjust them based on your testing needs: ‘viewports’: This is a JSON object that provides width/height pairs to test the application at. This is very handy if you have an app that has specific responsive design constraints. For each pair, the test will be executed per browser – resizing the browser for each set of dimensions. ‘orientations’: This allows you to configure the tests to execute using portrait and/or landscape view if you happen to be testing in a mobile browser (default orientation is portrait). ‘viewportChangePause’: This value pauses the test in milliseconds at each point the service is instructed to change viewport sizes. You may need to throttle this depending on app performance across browsers. ‘mismatchTolerance’: Arguably the most important setting there. This floating-point value defines the ‘fuzzy factor’ which the service will use to determine at what point a visual difference between references and screen shots should fail. The default value of 0.10 indicates that a diff will be generated if a given screen shot differs, per pixel from the reference by 10% or more. The greater the value the greater the tolerance. Once you’ve finished modifying your config file, lets execute a test. Provided your config file is set to point to the root of where your test files are located within the project, edit your package.json file and modify the ‘test’ descriptor in the scripts portion of the file. Set it to the following: ‘./node_modules/.bin/wdio /wdio.desktop.conf.js’ To run your test, from the command line, do the following: ‘BSTACK_USERNAME= BSTACK_KEY= npm run test — –baseUrl=’ Now, just sit back and wait for the test results to roll in. If this is the first time you are executing these tests, the visual-regression service can fail while trying to capture initial references for various browsers via Browserstack. You may need to increase your test’s global timeout initially on the first run or simply re-run your tests in this case. If you’re used to your standard JUnit or Jest-style test execution output, you won’t necessarily similar test output here. If there is a functional error present during a test (an object you are attempting to inspect isn’t available on screen) a standard Webdriver-based exception will be generated. However, outside of that, your tests will pass – even if a discrepancy is visually detected. However, examine your screen shot folder structure we mentioned earlier. Note the number of files that have been generated. Open a few of them to view what has been capture through IE 11 vs. Chrome while testing through Browserstack. Note that the files have names appended to them descriptive of the browser and viewport dimensions they correspond to. Example of a screen shot from a specific browser Make note if the ‘Diff’ directory has been generated. If so, examine its contents. These are your test results – specifically, your test failures. Example of a diff’ed image There are plenty of other options to explore with this basic set of tooling we’ve set up here. However, we’re going to pause here and bask in the awesomeness of being able to perform this level of browser testing with just 5-10 lines of code. This post really just scratches the surface of what you can do with a set of visual regression test tools.  There are many more options to use these tools such as enabling mobile testing, improving error handling and mating this with your build tools and services. We hope to cover these topics in a bit more depth in a later post.   For now, if you’re looking for additional reading, feel free to check out a few other related posts on visual regression testing here , here and here .", "date": "2018-10-04"},
{"website": "BazaarVoice", "title": "Auditing your Web App for Accessibility with Lighthouse", "author": ["Gary Spillman"], "link": "https://blog.developer.bazaarvoice.com/2018/06/05/auditing-your-web-app-for-accessibility-with-lighthouse/", "abstract": "If you’ve followed our blog for some time, you’ve likely encountered posts detailing how to engage in various kinds of software testing, from performance to data-driven to security and more. This post continues that trend with a focus on testing your site for accessibility. If you are unfamiliar with the concept, accessibility, within the realm of web applications is a term that covers a wide range of concerns. The primary focus is codifying and helping to reinforce design standards that ensure the web is accessible to all – regardless of people’s ability to consume and interact with it. This is a very large topic and its definition is beyond the scope of this blog post’s intent. If you’re looking for a starting point however, our friends at W3C.org can help you with that: https://www.w3.org/standards/webdesign/accessibility If you want to cut to the chase, there are two major accessibility standards you should be aware of (and are generally considered the path toward compliance). Both of these are supported and covered by the W3C as international standards and can be reviewed in full here: You can also visit this link for some related coding examples. Why should everyone be concerned with accessibility design issues?  The short answer can be found from this quite from Sir Tim Berners-Lee, via the W3C: “The power of the Web is in its universality. Access by everyone regardless of disability is an essential aspect.” A more complex answer is that, as the web reaches more and more global ubiquity, we as software creators need to take into account the range of needs to access the web from all users. This is simply fair play as web technology has matured and become evermore essential to our daily lives. There are also legal ramifications for not taking accessibility into account. Here are some examples: If you’ve closely followed the design principles of WCAG 2.0 , then your web application front end should be compliant with most modern accessibility software such as screen readers. However it’s difficult to verify your app’s end-user experience in this regard without actually sitting down with a browser, your app, some third-party tools and applying some old-fashioned manual testing. The challenge with this – you’re likely already thinking – is that this is time consuming and labor intensive. If you are on a small software team or have a very tight deadline (or both), this level of manual testing might be a hard (but necessary) pill for your product team to swallow. There are 3rd party services that can perform this level of testing for you. A quick google search can get you started down this path. However, if hiring a 3rd party is either logistically or financially out of reach, there are plenty of software tools that can help take the sting out of accessibility verification. As of this blog post, Google’s Chrome browser currently accounts for about 1/2 of all worldwide browser usage on the web. Chrome however is not only popular, it also has a lot of helpful tools under the hood to aid in web development. In fact, you can use your basic, desktop version of Chrome to perform an accessibility audit of a given site you happen to have open in your browser. To try this trick out, simply do the following: Look what we found under the hood! The audit tool in this case will scan the contents of the site at the current URL you have open for markup patterns that support accessibility standards (e.g. proper element naming conventions, css settings, etc.) and flag patterns within the markup on the page that violate these standards. These of course are based on the WCAG and AIRA conventions mentioned at the beginning of this post. This is a powerful but very easy to access and run set of tools for web developers (you can perform several kinds of audits, not just ones for accessibility). So easy in fact, you might be thinking that there must be a way to program these tools (and you would be right). Once you started the audit within Chrome, you might have noticed a message stating that something called Lighthouse was starting up, prior to the audit being executed. Lighthouse in fact, is the engine that performs the audit. It’s also available as a library dependency or even its own stand-alone project. Lighthouse can be incorporated into your project front end or build cycle in several ways. One of those is to use the Lighthouse as a standalone analyzation tool which you can use to verify the front end of your web app post deployment (think of this as another set of functional tests with a different, specific kind of output – in this case, an accessibility report. You can get Lighthouse running locally via command line quickly by doing the following (assuming you have NodeJS installed): 1. Open a new terminal and do, npm install -g lighthouse 2. Once the latest version of lighthouse is installed, do, lighthouse \"url of choice\" -GA —output html —output-path ./report.html Once the above command executes and finishes, you can open your Lighthouse report.html locally in your web browser: Lighthouse Report Drill down into the different segments of the report to see what best practices you are following for accessibilityy (or performance, SEO or other web standards) and where you should target to improve your app. Jenkins configuration for Lighthouse To include this report within your CI build process, just add the above Node command line statements to a shell script attached to your job of choice and make sure to archive the results (if configuring this in a Jenkins task, your job config will probably look something like this:) By default, the report Lighthouse provides is expansive and may take a while to read through. Fortunately, there are several options to move this form of testing further upstream. In this case, we are going to look at a small, open source project called Lightcrawler . Lightcrawler allows you to call specific audit aspects of Lighthouse from within your NodeJS project. The benefit of doing this is that you can more specifically target what you are auditing and perform said audits before you ship your project through to the rest of your CI process. You can install Lightcrawler into your project by simply doing npm install —save-dev lightcrawler . Next, given your Node app is configured with a script that will generate and run the app locally, simply add this to your “scripts” portion of your app’s package.json file: ”audit:a11y”: \"./node_modules/.bin/lightcrawler --url=http://localhost:8080 --config lighthouse-config.json\", With Lightcrawler you can configure an accessibility audit in a variety of ways that can meet your project needs. For the purpose of this post, we will configure Lightcrawler to run only tests classified within an accessibility audit, and to run all tests of that type currently available. Here is our configuration file: You can copy the JSON above and save it as lightcrawler.conf.js within your project directory and direct your script to the config file’s path. Test out your newly added audit script but running npm run audit:a11y . If everything is in working order, you should receive a command line output detailing the number of audits executed by Lightcrawler and error output for any given audit tests that may have failed (and more importantly, why). Lightcrawler Console Output This may look similar to linting report output from something like Prettier or ESLint .  Similar to the way Prettier or ESLint are used in pre-commit hooks within your project, you could add a Lightcrawler script to your pre-commit hook in order to better safeguard your application code from possible accessibility violations. This should give you some ideas as to how you can reduce the overall effort required to deliver your apps’ web front end while still maintaining accessibility.  By no means are the handful of tips presented here deep enough to fully cover everything under the accessibility umbrella.  That task will require significant time and effort but, in keeping with the web’s initial intent of inclusivity and connectivity, one we as software developers should strive toward.", "date": "2018-06-05"},
{"website": "BazaarVoice", "title": "Getting Started with Dependency Security and NodeJS", "author": ["Gary Spillman"], "link": "https://blog.developer.bazaarvoice.com/2018/02/27/getting-started-with-dependency-security-and-nodejs/", "abstract": "Internet security is a topic that receives more attention every day.  If you’re reading this article in early 2018, issues like Meltdown , Specter and the Equifax breach are no doubt fresh in your mind. Cybersecurity is a massive concern and can seem overwhelming.  Where do you start?  Where do you go?  What do you do if you’re a small application team with limited resources?  How can you better engineer your projects with security in mind? Tackling the full gestalt of this topic, including OWASP and more is beyond the scope of this article. However, if you’re involved in a small front end team, and if you leverage services like AWS, have gone through the OWASP checklist and are wondering, ‘what now?’, this article is for you (especially if you are developing in NodeJS, which we’ll focus on in this article). Let’s Talk About Co(de) Dependency: We’re talking about claiming dependents right? One way we can quickly impact the security of our applications is through better dependency management.  Whether you develop in Python, JavaScript, Ruby or even compiled languages like Java and C#, library packages and modules are part of your workflow. Sure, it’s 3 rd party code but that doesn’t mean it can’t have a major impact on your project (everyone should remember the Leftpad ordeal from just a few years ago). As it turns out, your app’s dependencies can be your app’s greatest vulnerability. Managing Code you Didn’t Write: Below, we’ll outline some steps you can take to tackle at least one facet of secure development – dependency management.  In this article, we’ll cover how you can implement a handful of useful tools into a standard NodeJS project to protect, remediate and warn against potential security issues you may introduced into your application via its dependency stack. We’ve all had to deal with other people’s problematic code at one time or another Using Dependency-Check: Even if you’re not using NodeJS, if you are building any project that inherits one or more libraries, you should have some form of dependency checking in place for that project. For NodeJS apps, Dependency-Check is the easiest, lowest-hanging fruit you can likely reach for to better secure your development process. Dependency-Check is a command-line tool that can scan your project and warn you of any modules being included in your app’s manifest but not actually being utilized in any functioning code (e.g. modules listed in your package.json file but never ‘required’ in any class within your app).  After all, why import code that you do not require? Installation is a snap.  From your project directory you can: If you have any unused packages, Dependency-Check will warn you via a notice such as: Armed with a list of any unused packages in hand, you can prune your application before pushing it to production.  This tool can be triggered within your CI process per commit to master or even incorporated into your project’s pre-commit hooks to scan your applications further upstream. RetireJS: ‘What you require, you must retire’ as the saying goes – at least as it does when it comes to RetireJS .  Retire is a suite of tools that can be used for a variety of application types to help identify dependencies your app has incorporated that have known security vulnerabilities. Retire is suited for JavaScript based projects in general, not just the NodeJS framework. For the purpose of this article, and since we are dealing primarily with the command line, we’ll be working with the CLI portion of Retire . then from the root of your project just do: If you are scanning a NodeJS project’s dependencies only, you can narrow the scan by using the following arguments: By default, the tool will return something like the following, provided you have vulnerabilities RetireJS can identify: Retire scans your app and compares packages used within it with an updated online database of packages/package versions that have been identified by the security world as having exploitable code. Note the scan output above identified that the version of jquery we have imported via the ICanHandlebarz module has a bug that is exploitable by hackers (note the link to CVE-2011-4969 – the exploit in question). Armed with this list, we can identify any unnecessary dependencies within our app, as well as any exploits that could be utilized given the code we are currently importing and using. There are a multitude of package scanning utilities for NodeJS apps out on the web. Among them, NSP (Node Security Platform) is arguably the most popular. Where the previously covered Retire is a dependency scanner suited for JavaScript projects in general, NSP , as the name implies, is specifically designed for Node applications. The premise is identical: this command line tool can scan your project’s package manifest and identify any commonly known web exploits that can be leveraged given the 3 rd party packages you have included in your app. Utilizing NSP and Retire may sound redundant but, much like a diagnosing a serious condition via medical professional, it’s often worth seeking a second opinion.  It’s also equally easy to get up and running with NSP: Running the above within the root of your node application will generate a report in your chosen output format Again, wiring this up into a CI job should be straightforward.  You can even perform back-to-back scans using both Retire and NSP. Snyk: Yes, this is yet another dependency scanner – and like NSP, another one that specifically NodeJS oriented but with an added twist: Snyk . Snyk home page With Retire and NSP, we can quickly and freely generate a list of vulnerabilities our app is capable of having leveraged against it.  However, what of remediation?  What if you have a massive Node project that you may not be able to patch or collate dependency issues on quickly?  This is where Snyk comes in handy. Snyk can generate detailed and presentable reports (perfect for team members who may not be elbow deep in your project’s code).  The service also provides other features such as automated email notifications and monitoring for your app’s dependency issues. Typical Snyk report Cost: Now, these features sound great (they also sound expensive).  Snyk is a free service, depending on your app’s size.  For small projects or open source apps, Snyk is essentially free.  For teams with larger needs, you will want to consult their current pricing . To install and get running with Snyk, first visit https://snyk.io and register for a user account (if you have a private or open source project in github or bitbucket, you’ll want to register your Snyk account with your code management tool’s account. Next, from your project root in the command line console: Read through the console prompt.  Once you receive a success message, your project is now ready to report to your Snyk account.  Next: Once Snyk is finished you will be presented with a URL will direct you to a report containing all vulnerable dependency information Snyk was able to find regarding your project. The report is handy in that it not only identifies vulnerabilities and the packages they’re associated with but also steps for remediation (e.g. – update a given package to a specific version, remove it, etc.). Snyk has a few more tricks up its sleeve as well.  If you wish to dive headlong into securing your application. Simply run: This will rescan your application in an interactive wizard mode that will walk you through each vulnerable package and prompt you for actions to take for each vulnerable package. Note, it is not recommended to use Snyk’s wizard mode when dealing with large scale applications due to it being very time consuming. It will look like this: Snyk monitor output Additionally, you can utilize Snyk’s monitoring feature which, in addition to scanning your application, will send email notifications to you or your team when there are updates to vulnerable packages associated with your project. Of course we’re going to arrange these tools into some form a CI instance.  And why not?  Given we have a series of easy-to-implement command line tools, adding these to part of our project’s build process should be straight forward. Below is an example of a shell script we added as a build step in a Jenkins job to install and run each of these scan tools as well as output some of their responses to artifacts our job can archive: Note – depending on how you stage your application’s build lifecycle in your CI service, you may want to break the above script up into separate scripts within the job or separate jobs entirely. For example, you may want to scan your app using Dependency-Check for every commit but save your scan of your application using NSP or Snyk for only when the application is built nightly or deployed to your staging environment.  In which case, you can divide this script accordingly. Note on Snyk: In order to use Snyk in your CI instance, you will need to reference your Snyk authentication key when executing this tool (see the API key reference in the script). This key can be passed as a static string to the job.  Your auth key for Snyk can be obtained by logging in at Snyk.io and retrieving your API key from the account settings page after clicking on the My Account link from Snyk’s main menu. This is only the tip of the iceberg when it comes to security and web application development.  Hopefully, this article has given you some ideas and hits of where to start in writing more secure software.  For more info on how to secure your Node-based apps, here’s some additional reading to check out:", "date": "2018-02-27"},
{"website": "BazaarVoice", "title": "Creating a Realtime Reactive App for a collaborative domain", "author": ["Ady Young"], "link": "https://blog.developer.bazaarvoice.com/2018/02/06/creating-a-realtime-reactive-app-for-a-collaborative-domain/", "abstract": "Sampling is a Bazaarvoice product that allows consumers to join communities and claim a limited amount of free products. In return consumers provide honest & authentic product reviews for the products they sample. Products are released to consumers for reviews at the same time. This causes a rush to claim these products. This is an example of a collaborative domain problem, where many users are trying to act on the same data (as discussed in Eric Evens book Domain-Driven design ). Bazaarvoice ran a 2 day hackathon twice a year. Employees are free to use this time to explore any technologies or ideas they are interested in. From our hackathon events Bazaarvoice has developed significant new features and products like our advertising platform and our personalization capabilities. For the Bazaarvoice 2017.2 hackathon , the Belfast team demonstrated a solution to this collaborative domain problem using near real-time state synchronisation. Bazaarvoice uses React + Redux for our front end web development. These libraries use the concepts of unidirectional data flows and immutable state management. These mean there is always one source of truth, the store, and there is no confusion about how to mutate the application state. Typically, we use the side effect library redux-thunk to synchronise state between server and client via HTTP API calls. The problem here is that, the synchronisation one way, it is not reactive. The client can tell the server to mutate state, but not vice versa. In a collaborative domain where the data is changing all the time, near-real time synchronisation is critical to ensure a good UX. To solve this we decided to use Google’s Firebase platform. This solution provided many features that work seamlessly together, such as OAuth authentication, CDN hosting and Realtime DB. One important thing to note about Firebase, it’s a backend as a service, there was no backend code in this project. The Realtime Database provides a pub/sub model on nodes of the database, this allows clients to be always up-to-date with the latest state. With Firebase Realtime DB there is an important concept not to be overlooked, data can only be accessed by it’s key (point query). You can think of the database as a cloud-hosted JSON tree. Unlike a SQL database, there are no tables or records. When you add data to the JSON tree, it becomes a node in the existing JSON structure with an associated key ( Reference ) During the hackathon demo we demonstrated updating the app’s style and content via the administration portal, this would allow clients to style the app to suite their branding. These updates were pushed in real time to 50+ client devices from Belfast to Austin (4,608 miles away). The code to achieve this state synchronisation across clients was deceptively easy! Given the nature of react, once a style config update was received, every device just ‘reacted’. In the demo, we added 40 products to the live campaign. This pushed 40 products to the admin screen and to the 50+ mobile app. Participants were then instructed to claim items. All members were authenticated via OAuth providers (Facebook, Github or Gmail). To my surprise the live demo went without a hitch. I’m pleased to add…. my team won the hackathon for the ‘Technical’ category. Firebase was a pleasure to work with, everything worked as expected and it performed brilliantly in the live demo….even on their free tier. The patterns used in Firebase are a little unconventional for the more orthodox engineers, but if your objective is rapid development, Firebase is unparalleled by any other platform. Firebase Realtime Database produced a great UX for running Sampling campaigns. While Firebase will not be used in the production product, it provided great context for discussions on the benefits and possibilities of realtime data synchronisation. Some years ago, I would have maintained that web development was the wild west of software engineering; solutions were being developed without discipline and were lacking a solid framework to build on. It just didn’t seem to have any of the characteristics I would associate with good engineering practices. Fast forward to today, we now have a wealth of tools, libraries and techniques that make web development feel sane. In recent years front-end developers have embraced concepts like unidirectional dataflows, immutability, pure functions (no hidden side-effects), asynchronous coding and concurrency over threading. Im curious to see if these same concepts gain popularity in backend development as Node.js continues to grow as backend language.", "date": "2018-02-06"},
{"website": "BazaarVoice", "title": "Event Stream Modeling", "author": ["Edwin Wise"], "link": "https://blog.developer.bazaarvoice.com/2018/01/02/event-stream-modeling/", "abstract": "Recently, during a holiday lull, I decided to look at another way of modeling event stream data (for the purposes of anomaly detection). I’ve dabbled with (simplistic) event stream models before but this time I decided to take a deeper look at Twitter’s anomaly detection algorithm [1], which in turn is based (more or less) on a 1990 paper on seasonal-trend decomposition [2]. To round it all off, and because of my own personal preference for on-line algorithms with minimal storage and processing requirements, I decided to blend it all with my favorite on-line fading-window statistics math.  On-line algorithms operate with minimal state and no history, and are good choices for real-time systems or systems with a surplus of information flowing through them (https://en.wikipedia.org/wiki/Online_algorithm) The high-level overview of the problem is this: we have massive amounts of data in the form of web interaction events flowing into our system (half a billion page views on Black Friday, for example), and it would be nice to know when something goes wrong with those event streams. In order to recognize when something is wrong we have to be able to model the event stream to know what is right . A typical event stream consists of several signal components:  a long-term trend (slow increase or decrease in activity over a long time scale, e.g. year over year), cyclic activity (regular change in behavior over a smaller time scale, e.g. 24 hours), noise (so much noise), and possible events of interest (which can look a lot like noise). This blog post looks at extracting the basic trend, cycle, and noise components from a signal. Note: The signals processed in this experiment are based on real historical data, but are not actual, current event counts. The big payback of having a workable model of your system is that when you compare actual system behavior against expected behavior, you can quickly identify problems — significant spikes or drops in activity, for example.  These could be from DDOS attacks, broken client code, broken infrastructure, and so forth — all of which are better to detect sooner than later. Having accurate models can also let you synthesize test data in a more realistic manner. A first approximation of modeling the event stream comes in the form of a simple average across the data, hour-by-hour: The blue line is some fairly well-behaved sample data (events per hour) and the green lines are averages across different time windows. Note how the fastest 1-day average takes on the shape of the event stream but is phase-shifted; it doesn’t give us anything useful to test against. The slowest 21-day average gives us a decent trend line for the data, so if we wanted to do a rough de-trending of the signal we could subtract this mean from the raw signal.  Twitter reported better results using the median value for de-trending but since I’m not planning on de-trending the signal in this exploration, and median calculations don’t fit into my lightweight on-line philosophy, I’m going to stay with the mean as trend. While the fast 1-day average takes on the shape of the signal, it is phase shifted and is not a good model of the signal’s cyclic nature. The STL seasonal-trend technique(see [2]) models the cyclic component of the signal (“seasonal” in the paper, but our “season” is a 24-hour span) by creating multiple averages of the signal, one per cycle sub-series.  What this means for our event data is that there will be one average for event counts from 1:00am, another average for 2:00am, and so forth, for a total of 24 running averages: The blue graph in Figure 2a shows the raw data, with red triangles at “hour 0″… these are all averaged together to get the “hour 0” averages in the green graph in Figure 2b.  The green graph is the history of all 24 hourly sub-cycle averages.  It is in-phase with and closely resembles the raw data, which is easier to see in Figure 2c where the signal and cyclic averages are super-imposed. A side effect of using a moving-window average is that the trend information is automatically incorporated into the cyclic averages, so while we can calculate an overall average, we don’t necessarily need to. The noise component is the signal minus the cyclic model and the seasonal trend. Since our online cyclic model incorporates trending already, we can simple subtract the two signals from Figure 2c to give us the noise component, shown in Figure 3: Now that we have a basic model in place we can look at some refinements. One problem with the basic model is that a huge spike in event counts can unduly distort the underlying cyclic model. A mild version of this can be seen in Figure 4a, where the event spike is clearly reflected in the model after the spike: By taking a page out of the audio signal compression handbook, we can squeeze our data to better fit within a “standard of deviation”. There are many different ways to compress a signal, but the one illustrated here is arc-tangent compression that limits the signal to PI/2 times the limit factor (in this case, 4 times the standard deviation of the signal trend): There is still a spike in the signal but its impact on the cyclic model is greatly reduced, so post-event comparisons are made against a more accurate baseline model. Now we have a signal, a cyclic model of the signal, and some statistical information about the signal such as its standard deviation along the hourly trend or for each sub-cycle. Throwing it all into a pot and mixing, we choose error boundaries that are a combination of the signal model and the sub-cycle standard deviation and note any point where the raw signal exceeds the expected bounds: You may note in Figure 5 how the lower bound goes negative.  This is silly – you can’t have negative event counts. Also, for a weak signal where the lower bound tends to be negative, it would be hard to notice when the signal fails completely – as it’s still in bounds. It can be useful to enforce an event floor so we can detect the failure of a weak signal. This floor is arbitrarily chosen to be 3% of the cyclic model (though it could also be a hard constant): Playing with data is a ton of fun, so all of the Python code that generated these examples (plus a range of sample data) can be found in GitHub here: https://github.com/bazaarvoice/event_monitor_test For your reading pleasure: [1] Automatic Anomaly Detection in the Cloud Via Statistical Learning [2] STL: A Seasonal-Trend Decomposition Procedure Based on Loess", "date": "2018-01-02"},
{"website": "BazaarVoice", "title": "Testing for Application Front End Performance with Web Page Test", "author": ["Gary Spillman"], "link": "https://blog.developer.bazaarvoice.com/2017/09/15/testing-for-application-front-end-performance-with-web-page-test/", "abstract": "‘Taco-meter’ – a device used to measure how quickly you can obtain tacos from your current location If you’ve followed Bazaarvoice’s R&D blog, you’ve probably read some of our posts on web application performance testing with tools like Jmeter here and here . In this post, we’ll continue our dive into web app performance, this time, focusing on testing front end applications. Application UI testing in general can be challenging as the cost of doing so (in terms of both time, labor and money) can become quite expensive. In our previous posts highlighted above, we touched on testing RESTful APIs – which revolves around sending a set of requests to a target, measuring their response times and checking for errors. For front-end testing, you could do the same – automate requesting the application’s front-end resources and then measuring their collective response times.  However, there are additional considerations for front end testing that we must account for – such as cross-browser issues, line speed constraints (i.e. broadband vs. a mobile 3G) as well as client side loading and execution (especially important for JavaScript). Tools like Jmeter can address some of these concerns, though you may be limited in what can be measured.  Plugins for Jmeter that allow Webdriver API access exist but can be difficult to configure, especially in a CI environment (read through these threads from Stack Overflow on getting headless Webdriver to work in Jenkins for example). There are 3 rd party services available that are specialized for UI-focused load tests (e.g. Blazemeter ) but these options can sometimes be expensive.  This may be too much for some teams in terms of time and money – especially if you’re looking for a solution for introductory, exploratory testing as opposed to something larger in scale. Webpagetest.org is an open source project backed by multiple partners such as Google, Akamai and Fastly, among others.  You can check out the project’s github page here . Webpagetest.org is a publicly available service.  Its main goal is to provide performance information for web applications, allowing designers and engineers to measure and improve application speed. Let’s check out Webpagetest.org’s public facing page – note the URL form field.  We can try out the service’s basic reporting features by pasting the URL of a website or application into the field and submitting the form.  In this case, let’s test the performance of Bazaarvoice’s home page. Before submitting the form however, note some of the additional options made available to us: Webpagetest.org’s initial submission form This service allows us to configure our tests to simulate user requests from a slow, mobile, 3G device in the US to a desktop client with a fiber optic connection in Singapore (just to name a few). Go ahead and submit the test form.  Once the test resolves, you’ll first notice the waterfall snapshot Webpagetest.org returns.  This is a reading of the application’s resources (images, JavaScript execution, external API calls, the works).  It should look very familiar if you have debugged a web site using a browser’s developer tools console. Webpagetest.org’s waterfall report This waterfall readout gives us a visual timeline of all the assets our web page must load for it to be active for the end user, in the order they are loaded, while recording the time taken to load and/or execute each element. From here, we can determine which resources which have the longest load times, the largest physical foot prints and give us a clue as to how we might better optimize the application or web page. Webpagetest.org’s content breakdown chart In addition to our waterfall chart, our test results can provide any more data points, including but not limited to: Armed with this information from our test execution, you can begin to see where we might decide to make adjustment to our site to increase client-side performance. Webpagetest.org’s web UI provides some great, easy-to-use features.  However, we can automated these tests using Webpagetest.org’s API. In our next couple of steps, we’ll demonstrate how to do just that using Jenkins, some command line tools and Webpagetest.org’s public API, which you can read more about here . There are a few limitations or considerations regarding using Webpagetest.org and its API: If test result confidentiality or scalability is a major concern, you may want to skip to the end of this article where we discuss using private instances of Webpagetest.org’s service. As mentioned above, we need an API key from Webpagetest.org. Just visit https://www.webpagetest.org/getkey.php and submit the form made available there.  You’ll then receive an email containing your API key. Once you have your API key – lets set up a test using as CI service.  For this, like in our other test tutorials, we’ll be using Jenkins. For this walkthrough, we’ll use the following: First, let’s get the Jenkins job up and running and make sure we can use it to reach our primary testing tool. Do the following: Now that the job is created, let’s set a few basic parameters: Specifying a NodeJs version for Jenkins to use Next, let’s create a temporary build step to verify our job can reach Webpagetest.org Once the job has executed, click on the build icon and select the console output option.  Review the console output captured by Jenkins.  If you see HTML output captured at the end of the job, congratulations, your Jenkins instance can talk to Webpagetest.org (and its API). Now we are ready to configure our actual job. The next steps will walk you through creating a test script.  The purpose of this script is as follows: Click on the configure link for your job and scroll to the shell execution window.  Remove your curl command and copy the following into the window: If you know that your Jenkins environments comes deployed with wget already installed, you can skip the last npm install command above. Once this portion of the script executes, we’ll now have a command line tool to execute tests against Webpagetest’s API.  This wrapper makes it somewhat easier to work with the API, reducing the amount of syntax we’ll need for this script. Next in our script, we can declare this command that will kick off our test: This script block calls the API, passing it some variables (which we’ll get to in a minute) then saves the results to a JSON file. Next, we use the command line tools cat and jq to sort our response file, assigning the testId key value from that file to a shell variable.  We then use some string manipulation to get rid of a few extraneous quotation marks in our variable. Finally, we print the testId variable’s contents to the Jenkins console log.  The testId variable in this case should now be set to a unique string ID assigned to our test case by the API.  All future API requests for this test will require this ID. Before we go further, let’s address the 3 variables we mentioned above.  Using shell variables in the confines of the Jenkins task will give us a bit more flexibility in how we can execute our tests.  To finish setting this part up, do the following: Save changes to your job and try running it.  See if it passes.  If you view the console output form the run, there’s probably not much you’ll find useful.  We have more work to do. Your site parameter should look like this Your runs parameter should look like this And your key parameter should looks like this The next portion of our script is probably the trickiest part of this exercise.  When executing a test with the Webpagetest.org web form, you probably noticed that as your test run executes, your test is placed into a queue. Cue the Jeopardy! theme… Depending current API usage, we won’t be able to tell how long we may be waiting in line for our test execution to finish.  Nor will we know exactly how long a given test will take. To account for that, next we’ll have to modify our script to poll the API for our test’s status until our test is complete.  Luckily, Webpagetest.org’s API provides a ‘status’ endpoint we can utilize for this. Add this to your script: Using our API wrapper for webpage test and jq, we’re posting a request to the API’s status endpoint, with our test ID variable passed as an argument.  The response then is saved to the file, status.json. Next, we use cat and jq to filter the content of status.json, returning the contents of the ‘testsCompleted’ (which is a child of the ‘data’ node) within that file.  This gets assigned to a new variable – testStatus. Just like going to the DMV The response returned when polling the status endpoint contains a key called ‘status’ with possible values set to ‘In progress’ or ‘Completed’.  This seems like the perfect argument we would want to check to monitor our test status.However, in practice, this key-value pair can be set prematurely – returning a value of ‘Completed’ when not necessarily every run within our test set has finished.  This can result in our script attempting to retrieve an unfinished test result – which would be partial or empty. After some trial and error, it turns out that reading the ‘testsCompleted’ key from the status response is a more accurate read as to the status of our tests.  This key contains an integer value equal to the number of test runs you specified that have completed execution. Add this to your script: This loop compares the number of tests completed with the number of runs we specified via our Jenkins string variable, $RUNS. While those two variables are not equal, we request our test’s status, overwrite the status.json file with our new response, filter that file again with jq and assign our updated ‘testsCompleted’ value to our shell variable. Note that the 10 second sleep command within the loop is optional.  We added that as to not overload the API with requests every second for our test’s status. Once the condition is met, we know that the number of tests runs we requested should be the same number completed. We can now move on. Once we have passed our while loop, we can assume that our test has completed.  Now we simply need to retrieve our results: webpagetest har $testId -o results.har webpagetest results $testId -o results.json Webpagetest’s API provides multiple options to retrieve result information for any given test.  In this case, we are asking the API to give us the test results via HAR format . Note – if you wish to deliver your test results in HAR format, you will need a 3 rd party application or service to view .har files.  There are several options available including stand-alone applications such as Charles Proxy and Har Viewer . Additionally, we are going to retrieve the test results from our run using the API’s results endpoint and assign this to a JSON file – results.json. Now would be a great time to save the changes we’ve made to your test. Next, we want to retrieve the web application’s waterfall image – similar to what was returned when executing our test via the Webpagetest.org web UI. To do so, add this to your script: Since we’ve already retrieved our test results and saved them to the results.json file, we simply need to filter this file for a key-value pair that contains the URL for where our waterfall snapshot resides online. In the results.json file, the main parent that contains all test run information is the ‘data’ node.  Within that node, our results are divided up amongst each test run.  Here we will further filter our results.json object based on the 1 st test run. Within that test run’s ‘firstView’ node, we have an images node that contains our waterfall image. We then assign the value of that node to another shell variable (and then use some string manipulation to trim off some unnecessary leading and ending quotation characters). Finally, as we installed the nwget module at the beginning of this script, we invoke it, passing the URL of our waterfall image as an argument and the option to output the result to a PNG file. Upon each execution, we wish to save our results files so that we can build a collection of historical data as we continue to build and test our application.  Doing this within Jenkins is easy Just click on the ‘Add Post Build Action’ button within the Jenkins job configuration menu and select ‘Archive the artifacts’ from the menu. In the text field provided for this new step, enter ‘*.har, results.json, waterfall.png’ and click save. Now, when you run your test job – once the script succeeds, it will save each instance of our retrieved HAR, waterfall image and results.json file to each respective Jenkins run. Once you save your job configuration, click the ‘Build with Parameters’ button and try running it.  Your artifacts should be appended to each job run once it completes. Collecting your results in Jenkins Next, we’re going to talk about result filtering.  The results.json provided initially by the API is exhaustive in size.  Go ahead and scroll through that file saved from your last successful test run (you will probably be scrolling for a while).  There’s a mountain of data there but let’s see if we can fish some information out of the pile. The next JSON-relative trick we’re going to show you is how you can filter the results.json from Webpagetest.org down to a specific object you wish to measure statistics for. The structure of the results.json lists every web element (from .js libraries to images, to .css files) called during the test and enumerates some statistics about it (e.g. time to load, size, etc.).  What if your goal is to monitor and measure statistics about a very specific element within your application (say, a custom .js library you ship with your app)? That’s one way of finding it… For that purpose, we’ll use cat and js again to filter our results down to one, specific file/element’s results.  Try adding something like this to your script: This is similar to how we filtered our results to obtain our waterfall image above.  In this case, each test result from Webpagetest.org contains a JSON array called ‘requests’.  Each item in the array is delineated by the URL for each relative web element. Our script command above parses the contests of results.json, pulling the ‘results’ array out of the initial test run, then filtering that array based on the ‘url’ key, provided that key matches the string we provided to jq’s ‘contains’ method. These filtered results are then output to the stats.json file.  This file will contain the specific test result statistics for your specific web element. Add stats.json to the list of artifacts you wish to archive per test run.  Now save and run your test again.  Note, you may need to experiment with the arguments passed to JQ’s contains method to filter your results based on your specific needs. At this point, we should have a Jenkins job that contains a script that will allow us to execute tests against Webpagetest.org’s public API and retrieve and archive test results in a set of files and formats. This can be handy in of itself but what if you have team or organization members who need access to some or all of this data but do not have access to Jenkins (for security purposes or otherwise)? Some options to expose this data to other teams could be: There’s quite a few ideas to expand upon for this kind of testing and reporting.  We’ll cover some of these in a future blog post so stay tuned for that. If you find this method of performance testing useful but are feeling a bit limited by Webpagetest.org’s restrictions on their public API (or the fact that all test results are made public), it is worth mentioning that if you’re needing something more robust or confidential, you can host your own, private instance of the free, open-source API (see Webpagetest.org’s github project for documentation ). Additionally, Webpagetest.org also has pre-built versions of their app available as Amazon EC2 AMIs for use by anyone with AWS access.  You can find more information on their free AMIs here. Additionally, here’s the script we’ve put together through this post in its entirety: We hope this article has been helpful in demonstrating how some free tools and services can be used to quickly stand up performance testing for your web applications.  Please check back with our R&D blog soon for future articles on this and other performance related topics.", "date": "2017-09-15"},
{"website": "BazaarVoice", "title": "Maintaining Test Data with the “someObject” Test Structure", "author": ["Seth Hubbell"], "link": "https://blog.developer.bazaarvoice.com/2017/09/05/maintaining-test-data-with-the-someobject-test-structure/", "abstract": "Language: Scala TestTool: Scalatest When systems become reasonably complex, tests must manage cumbersome amounts of data. A test case that may test a small bit of functionality may start to require large amounts of domain knowledge about the system being tested. This is often done through the mock data used to set up the test. Maintenance of this data becomes cumbersome, monotonous and can feel Sisyphean. To solve these problems we created “someObject”, a modular system that allows us to maintain data in only one location while providing the flexibility to create specific data for our tests. To start this post, we’re going to build a system without the “someObject” test structure to provide context for its use. (To skip to the “someObject” structure, jump to here! ). Suppose we are building a service that reports on advertising campaigns. We may create a class that describes an advertising campaign and call it `Campaign`. Now we are going to store this campaign in a database, and we need to write some integration tests to make sure this operation is performed properly. A test that confirms a campaign is stored properly might look like this: BlogIntro.scala Now we add the ability to update some values for this campaign in the database, and we need to test this new piece of functionality. That test might look something like this: BlogUpdateFunction.scala But here we see the duplication of test boilerplate code in `campaignToStore`. We don’t want to have to copy over `campaignToStore` into every test, so we might want to abstract that out to be used all over the suite. BlogAbstractToSuite.scala Now we can use the same data in every test! Suppose we now write a function to fetch all the campaigns that are stored in the database. We might need a test that involves uniqueness in the data we store, such as the following example: BlogTestWithUniqueTestData.scala In the example, we re-used the campaign we abstracted out earlier for conciseness, but this makes this test unclear that `anotherCampaignToStore` is unique. What if someone else comes in and changes `campaignToStore` and it happens to match data from `anotherCampaignToStore`? This test would then become flakey and nobody likes flakey tests. We might decide to just make all data used in this test local to this test, but then we will need to maintain the test data in both this test, and `MyCampaignTestObjects`. Suppose now that there is a new design constraint on how campaigns can be stored in the database. Now all client names must be lowercased in all campaigns: BlogNewConstraint.scala Now we start to see the issue with maintaining test data across the whole suite that we’ve been constructing. We need to find every mock campaign that is used in our suite and ensure that its client field data is lowercased. Realistically, many of our tests, (specifically in this example, the `fetchAllCampaigns` test) don’t care about the client field of their campaign, and so we shouldn’t need to care about the client field value while setting up our mock test data. Because this example is small, it’s not cumbersome to directly update the value to satisfy the new constraint. Now let us Imagine a large set of suites, each containing hundreds of unique test cases. Suddenly this single suite requires a large amount of work to refactor one field across each test case. Nobody wants to do that monotonous maintenance. To address this, our team adopted the “someObject” structure to minimize this data maintenance within our tests. When designing this test structure, we wanted to make our test data extendable for use anywhere it is needed. We used Scala’s `trait` to mix in necessary functions to provide test objects to the objects inside our tests, such as the `MyCampaignTestObjects` object above: Now we can revisit our `fetchAllCampaigns` test example. BlogBasicSomeObject.scala Inside this test, we’ve set up two unique campaigns, by calling the `someCampaign` method from our test data trait. This populates the returned campaign with dummy data that we don’t care about. All we need out of this method is “some campaign” with “some data”. Now, instead of obscuring the intent of the test case by setting up cumbersome, overly-expressive mock data, we can simply override the implicitly available mock objects with only the necessary data . For the unique campaigns needed in the `fetchAllCampaigns` test, we only only really care about each campaign’s identifier. We don’t update the name, client, startDate, etc. because this test doesn’t care about any of that data. We only need to care that the campaigns are unique for our database. Under this test structure, when we receive the design change about the client names being lowercased, we don’t need to update our `fetchAllCampaigns`. Let’s provide another example that our team encountered. Campaigns inside our database now need to also store the amount of money spent on each ad campaign. We’re adding a new column to our database, and changing the database schema. Now, every test that has a campaign involved needs to be updated to include a new field; but under the “someObject” structure we only need to add two lines and all existing tests should be working fine again: BlogPostSchemaChange.scala The purpose of the “someObject” structure is to minimize data maintenance within tests. We want to ensure that we’re disciplined about only setting data that the tests need to care about. There are cases where data might seem necessary for what we are testing, but we can abstract that data away to de-couple the test’s reliance on hard coded values. For example, suppose we have a function that returns the sum of all the `totalAdSpend` across our database. To test this function, we might write a test like this: BlogPostNonBehaviorTest.scala While this test does work, and it utilizes this “someObject” structure, it still forces data management at the test level. `sumAllSpend` doesn’t really care about any one campaign’s `totalAdSpend` value. It only cares that we add all of the `totalAdSpend` values up correctly. We could instead write our test to assert on this behavior instead of doing the math ourselves and taking on the responsibility of managing more data. BlogPostBehaviorTest.scala With this test, we don’t care what campaigns sales were, we don’t care how many campaigns are stored, and we don’t care about any constant value. This test will return the sum of all campaign’s `totalAdSpend` value that we store in the database. In this introductory blog post, we explored the someObject testing structure in scala, but this concept is not intended to be language specific. Scala makes this concept easy to implement through the use of Default Parameter Values but in a future post I’ll show how it can be implemented through the Builder Pattern in a language like Java. Another unexplored “someObject” concept is the granularity of control in setting default data. This post introduces the “global” and test specific setting of default data, but doesn’t explore how to set test suite level data for our test objects, and the cases in which that might be useful. I’ll discuss that the future post as well. BlogIntro.scala BlogUpdateFunction.scala BlogAbstractToSuite.scala BlogTestWithUniqueTestData.scala BlogNewConstraint.scala BlogBasicSomeObject.scala BlogPostSchemaChange.scala BlogPostNonBehaviorTest.scala BlogPostBehaviorTest.scala", "date": "2017-09-05"},
{"website": "BazaarVoice", "title": "Publishing Load Test Results with Jmeter, Jenkins and S3", "author": ["Gary Spillman"], "link": "https://blog.developer.bazaarvoice.com/2017/06/05/publishing-load-test-results-with-jmeter-jenkins-and-s3/", "abstract": "A while ago, I published a blog post that presented a tutorial overview of how to use Jmeter for load testing a typical RESTful API. This post builds upon that original post with handy information on some updated reporting features of Jmeter as well a quick dive into how you can better propagate your load results in a continuous build environment (ala Jenkins). if you’re completely new to Jmeter, please read my previous blog post, linked above before proceeding. For those of you experienced with Jmeter (hopefully you already have some tests you’re wanting to deploy somewhere rather than let them live contentedly on your local machine), get ready because you’re gonna love this It’s truly amazing the gifs you can find on the web. The previous load test tutorial covered setting up a load test from scratch using an early version of Jmeter.  One of the benefits of using Jmeter, as outlined in the original post was that it has been around for quite a long time, is well known, stable and free.  However, one of the major cons in using Jmeter is that its’ well, old – and has gone for a long while without major updates – especially when it comes to report generation. Well, welcome to Jmeter 3!  It still comes with that same, great, vintage 90s UI but now with a hoard a bug fixes, stability upgrades and – wait for it – native HTML report generation, wooo!  Check out version 3’s documentation here: So, let’s get down to it!  You can find the latest build of Jmeter 3 here: https://jmeter.apache.org/download_jmeter.cgi Steps to setting up Jmeter 3 locally are similar to the installation steps covered in my original blog post.  Pre-installation requirements are the same: Functionally, Jmeter 3 appears identical to older versions of the application but there are a lot of changes under the hood.  I recommend unarchiving and installing Jmeter 3 in its own directory, separate from any previous version of Jmeter you may have previously installed. If you’re following along with this tutorial and wish to create a new load test from scratch, please follow the test setup steps in the original post’s tutorial however, be sure to SKIP the 3 rd party report and listener portion of the test setup (guess what, you won’t be needing those plugins anymore). If you have an existing test you wish to simply fire up in Jmeter 3, your mileage will vary in this case.  Some of the 3 rd party listeners and other tools you may have used in your setup may be incompatible with the newer version of Jmeter.  If you have any custom listeners embedded in your test I recommend doing the following: From this point, you won’t really need any of your original report listeners configured in your test, however, for debugging purposes, I do recommend keeping a simple response listener in your test – one for successful API responses, another separate for failed responses – just to help with debugging any issues your test might run into. In a few cases, I’ve found despite removing any 3 rd party plugins that some tests have compatibility issues with Jmeter 3.  Sadly, the easiest course of action is to rebuild the test from scratch.  Such is life (sigh). Such is life… Given you have your initial test configured for Jmeter 3, let’s start up the app and view it in the new UI (though it looks like the old UI).  Once you start Jmeter froim the command line, you’ll see a message like this in your terminal: You don’t say!?  If you’re a long-time user of earlier version of Jmeter, you’re probably thinking, “Well, no duh!” right? Open your load test in the UI and give it a look-over to make sure its configured the way you intend.  It should be minimal simple thread group, your HTTP samplers, your results tree report and any CSV data resources you might need. Your test may look something like this Next, save your test and close Jmeter (we’re now done with the Jmeter UI). Now, we are going to use Jmeter’s command line interface to execute the load test and generate the report for the test. Do the following: Open your CLI terminal and navigate to your Jmeter 3 bin directory.  Execute the following command: Be sure to set the ‘test JMX file’ argument to the path where your load test’s .JMX file resides, relative to the Jmeter bin directory. The test log file should be an arbitrary log file path/name that currently does not exist. The output folder path is where your report will be written to.  This must be an empty directory. If you configured your test to run for a set period, just sit back and wait.  The report will be generated in the output directory you provided once the test execution finishes. If you configured it to run in perpetuity – you will need to formally stop the test before report generation will execute (simply killing your terminal session won’t help you here). To stop your Jmeter test from the command line, simply open a new terminal window and enter one of the following commands: Control + . – This stops Jmeter, halting any active listeners. Control + , – This sends a shutdown command to Jmeter – it will wait until all active threads have finished their current tasks before closing shop (this is the recommended method to use unless you’re impatient or there is an emergency – like, oh you forgot you pointed your test at production). Are you really going to argue with Chuck? Once your test has gracefully exited, navigate to the output folder you specified and open your newly minted report in your web browser of choice. Jmeter 3 HTML report Pretty sweet huh? If you haven’t seen Jmeter’s dashboard report before, you’re probably thinking, “golly, there’s a lot of stuff here” – and there is.  The dashboard report contains a pretty wide variety of data on your test execution.  Enumerating through all of that here would be exhausting however – if you’re interested in all the gory details of Jmeter’s dashboard metrics, check out this in-depth explanation here: https://jmeter.apache.org/usermanual/generating-dashboard.html So now you have this fancy load test report but hoarding all this metric goodness on your local development workstation is no fun for the rest of your team.  But that’s OK because if you have access to a continuous integration build environment (ala – Jenkins), the rest of this blog post will walk you through taking your Jmeter test and throwing it into a Jenkins build that allows you to not only run your test remotely but also save and propagate your results to the rest of your agile team. First things first, you’re going to need to take your Jmeter test, get it off your local machine and into some form of web-accessible delivery.  If you have AWS access, you could place your .JMX files and resources into a bucket and be done with it.  In our case, I recommend creating a private Github repository for your tests and committing your Jmeter working directory, tests and resource files to that repository.  There’s a couple of reasons for this: That said, before tossing your tests into Github’s gaping maw, here’s an important note regarding information security: Depending on your type of load test, you may have some API or other service access keys stored in your test.  You don’t want to check in API or other access keys into your repository (even if it is marked private).  But don’t just take my word for it: https://softwareengineering.stackexchange.com/questions/205606/strategy-for-keeping-secret-info-such-as-api-keys-out-of-source-control If you’ve already committed your tests to your repository and you still have some software keys in there and are thinking, “Oh boy… What now!?”, here’s a link to some handy notes on how to extract some sensitive values from your test or code using some handy features of Git: https://gist.github.com/derzorngottes/3b57edc1f996dddcab25 Jenkins is one of the most widely available CI build tools in use today.  As such, we’re going to cover how to set up a Jenkins task to run your load tests.  If you’re using some other build system (e.g. Team City) this walk-through won’t be as pertinent but essentially, the tasks will be the same. Assuming you already have a Jenkins build server of some sort available to your organization – let’s log into it.  If you don’t have access to Jenkins, talk to your friendly neighborhood dev ops engineer.  Note, that for this walkthrough, you’re going to need some level of administrator access to set up everything we need to execute our tests. Do the following: If your test is going to loop, create another string parameter named, “loops” and set its default, numeric value. If you have an API key (or keys) you need for your test to execute, as mentioned above, and have already removed said keys from your test (substituting them for a placeholder value) you can create additional string parameters here (naming them key1, key2, etc.).  You can then set your API key(s) to the default value field.  Granted, you are propagating your API key here but this is a much better practice in terms of security than leaving them in a potentially publicly accessible repository as your Jenkins instance should be private and protected. While still in the Jenkins job configuration screen, under the Source Code Management section, select “Git” and enter your repository’s URL alias into the Repository URL field. If this is your first time doing this, Jenkins expects a specific format for your Github URL strings.  It should look something like this: If you’re using a publicly accessible Github repository, skip the next step, enter your preferred branch name (or */master if you don’t need a specific branch). Next, select the appropriate credentials option from the relative drop down menu to allow Jenkins to properly introduce itself to your Github repo (if you’re having trouble with this, ask your dev ops engineer or Jenkins server admin for help – and maybe do something nice for them while you’re at it like get them a thank you card or some cookies or something, because at this point, you’d be out of luck without them, right?). You can kick off your tests manually however, it’s probably better to set up timer or other method to handle this for you.  Try the following: Under the Build Triggers heading, check the Build periodically option Enter your time range you wish the build to trigger at (this will be in Cron notation – which is tricky if you’re seeing this for the first time). This article does a decent job explain Cron’s formatting: http://www.nncron.ru/help/EN/working/cron-format.htm Now, click on the Add build step option and from the dropdown menu, select ‘Execute shell’. Here, we will define the command we will use to execute the Jmeter test once Jenkins pulls the test resources from Github. This will be like the shell command we used to run Jmeter locally as described above. Now we need to make sure we’re saving our generated report somewhere so the rest of our team can review it.  Jenkins has a Jmeter performance plugin that can archive our results.  Also, we could just have Jenkins archive the output directory Jmeter generates the report in (though handing all the recursive directories it will build might be a pain). Given you have an AWS account, you have the options to sync your reports to an AWS bucket and host them like a static web page.  This method would be beneficial if you need to expose your load test results to team members or other parties that do not or should not have access to your build systems (e.g. product manager, support team members, etc.). If you don’t have access to AWS, aren’t sure you have access to AWS or may have issues with your build server communicating to AWS, once again, now is a fine time to make friends with (*cough* bribe *cough*) your friendly neighborhood dev ops engineer. Let’s say you’ve taken care of that and you and your dev ops guru are best buddies now.  Next, you’ll need to further configure your Jenkins job by adding another Execute shell build step.  Make sure this one is ordered sequentially after your Jmeter test execution step. In the Execution shell, simply add a command to change to the output directory your report will be in (defined in your test execution command) and use the AWS sync command to send your HTML report to AWS: Save your Jenkins job.  You’re done (with Jenkins at least). Now, in the above step, we are assuming you have a bucket already created in AWS where you can sync your data to.  You’re also going to need access to modify how the bucket works in terms of its permissions. By default, your AWS bucket is out there on the web, you can read from it, write from it, but your other team members will have trouble reading the HTML report from that location.  We need to configure the bucket so that it will display its contents like a static web page.  Here’s how: Log into your AWS account and access your specified bucket. Click on the bucket and select the Properties option Click the Static website hosting panel Enable the static hosting option Specify your index.html file location (this should be the relative path to the main HTML file for your uploaded report). Save your changes and boom – now you’re done. Save and run your job.  If your account permissions are all set up correctly, your test should execute and the test results will be uploaded to the specified AWS bucket. From here, simply access your AWS bucket using the following styled web link: https://s3.amazonaws.com/myBucketRoot/myBucketName Your test results should now be viewable just as if they were a static web page by anyone on your team. Now that you have your load tests residing in a Github repository, feel free to leverage Github’s branching features to allow you to manage multiple iterations of your load tests: Commit different variations of the same branch to new branches Use Jenkin’s copy feature to copy your existing job, and just point it to your different branches to allow you to execute multiple test types. Use Jmeter’s UI locally to copy/edit your existing tests to make additional iterations.  Save these to your repo and commit them just like you would any other coding project. Again, use Jenkin’s copy feature to create variations of your test run jobs – this time, just edit your shell command to execute your desired .JMX test file per a given execution. Thanks for reading and I hope this give you further ideas and insight into how you can leverage Jmeter to further improve your software projects through load testing.", "date": "2017-06-05"},
{"website": "BazaarVoice", "title": "Why the value of hackathons goes beyond free pizza Categories: Culture", "author": ["Dan Heberden"], "link": "https://blog.developer.bazaarvoice.com/2017/05/11/why-the-value-of-hackathons-goes-beyond-free-pizza-categories-culture/", "abstract": "About six months ago, we shared Why We Hackathon . At Bazaarvoice, we host a company-wide hackathon twice a year, and our next one kicks off this week. My previous post primarily focused on the people and company culture aspects of running a hackathon. In lieu of writing “Synergizing Innovation With Disruptive Hackathons”, this time around I’d like to  to share the real value we’ve seen from our hackathons, as well as some ideas on how to realize that value with your own hackathon. If you participate in a hackathon at Bazaarvoice, it is likely that what you create will somehow touch our clients. Many of the products and features we offer today got their start from a hackathon project. Curations , our social media curation platform, started from one of our public hackathons. Bazaarvoice engineers working with engineers from FeedMagnet, one of the companies invited to participate, created the initial prototype of displaying reviews and social content together. Since then, we’ve put in years of work to build a system that can collect, manage, and display content at tremendous scale, but it all started with that original hackathon project. Recently, we’ve seen many hackathon projects focused on helping consumers find the perfect product. Most of these took shape as various forms of personalization, like product recommendations, which we’re now offering as a limited-availability product. A different version of this technology is a new capability we’ve been experimenting with called Post Interaction Notification. In short, instead of showing products it thinks someone is interested in, it asks for reviews about products they’ve purchased. You guessed it — also a past hackathon project. Our successful projects aren’t limited to display-specific projects. Our last hackathon saw projects that used different machine learning technologies to improve our ability to identify relationships across products, shopper profiles, and related consumer-generated content (CGC). For example, one project solved the problem of widely varied product titles and descriptions in our product catalog. If searching for ‘60” Flatscreen TV’, our product matching services would previously only return results that exactly matched those search terms. Now, thanks to the hackathon project, our system understands what 60”, flatscreen,  and TV actually mean . It can then find similar, but semantically different, items like “60 inch Flatscreen Television” or “60in. LCD TV”. This capability dramatically improves our personalization and syndication services by finding the same products across thousands of varied product catalogs. While not every hackathon project grows into a full-fledged solution, they continually temper and sharpen the direction of our offerings and often improve our clients’ CGC programs. Projects of this variety range from seemingly simple improvements to complex, long term initiatives. One project adored by both our clients and employees (we use these tools too!) was the ability to hop between different client accounts available to the logged-in user in the administration portal; simple, but a big time and frustration saver. Speaking of administration tools, we’ve been building a new platform to build and deliver consistent tooling across our capabilities. Many of the last hackathon’s projects focused on providing actionable insight into the health of implemented Bazaarvoice capabilities on client’s websites, as well as supporting aspects like product-catalog imports and email-based review solicitation. These projects have shaped the solutions we’re building to proactively communicate the technical health of a client’s CGC program. Normally, you’ll read “just give people pizza, beer, and games for a few days” as the recipe for a successful hackathon. In my opinion, those things are fantastic additions to any hackathon. However, it’s the people that make a hackathon great. To create success, empower people to find creative solutions to real problems. We treat product development as a company-wide exercise. Having this sort of inclusive culture means individuals building, supporting,marketing, and selling our solutions are actively involved with client feedback, product feedback, and market opportunity across all of our functions. Providing a few days to create solutions to the problems seen over past months unleashes incredible ideas and creativity. Secondly, this isn’t a top-down driven event. We involve a handful of hackathon participants and their peers to help plan and execute our hackathons. They make sure every activity, piece of swag, and communication is planned and ready to grow and sustain a big list of happy attendees. After all, the primary focus of the event to create space and time to be creative, have fun, and build stronger relationships. Logistically, we made a change last hackathon that greatly improved the involvement from employees who weren’t participating on a hackathon team.It can be a bit challenging to stay focused with 50 teams demoing projects on stage one after the other, let alone remembering what you liked most in the final voting. Instead, we tried a science-fair style demo to promote walking around, engaging with various teams, and “investing” in the most promising projects using tickets. This boosted participation from those that aren’t interested in hacking, but who are interested in seeing and voting in the end results. This month’s hackathon we’ve got trifold panels, glue sticks, and markers to really get into the science fair spirit. It’s not about buzzwords and forced team bonding. If you’re looking to run a hackathon, focus on the people — both those on hackathon teams and those who just want to spectate. If you’re looking to participate in a hackathon, try to find a real problem to solve. Regardless of your role, you’ll be pleasantly surprised at the creative, brilliant output. And at the very least, you’ll get some free pizza.", "date": "2017-05-11"},
{"website": "BazaarVoice", "title": "Database Migration", "author": ["Seth Hubbell"], "link": "https://blog.developer.bazaarvoice.com/2017/05/24/database-migration/", "abstract": "(Always One More Thing…) Who Are We? The Ad Management team here at Bazaarvoice grew out of an incubator team. The goal of our incubator is to quickly iterate on ideas, producing prototypes and “proof of concept” projects to be iterated on if they validate a customer need. The project of interest here generates reports based on aggregations of event data gathered from several other teams at the company. As our project gained traction, it grew in size and scope, eventually leading to the need to revisit some of the design decisions made in the prototyping phase.  Specifically, we found the original database system we chose, EmoDB, to not meet our needs as our requirements evolved. Why Migrate? When this project was started, it began as a prototype designed to get the project rolling as quickly and easily as possible. The initial team chose EmoDB since they were familiar with the in-house technology from their other projects and it fit our initial needs. As the project gained traction, and we had more data to operate against, we encountered scalability issues, initially resolved with caching and some refactoring. We found that we were querying EmoDB as if it were a typical relational database, when it’s not actually designed for that use case. (Emodb is an eventually consistent json blob store with a change notification databus that spans multiple AWS AZs and Regions. EmoDB powers many of our solutions at Bazaarvoice and is now open-source and available at: https://github.com/bazaarvoice/emodb We chose to switch to MySql to leverage the Relational Data Model for rolling up aggregations of data we collect and calculate.  We ran into problems previously when we  retrieved whole documents to perform aggregations on our data, leading us to decide that a technology that is optimized for relational models would suit the project much better. How to Migrate? Since our project already had trained users by the time we wanted to migrate database systems, we needed to design our migration with a no-down-time approach;  “seamlessly” changing out the back-end implementation for our users. We also made these transitions configurable, such that we wouldn’t need to make one large switch to master from the new system, but we could choose which services were ready to be cut over to the new data back-end. The following image is our design document that describes how we planned our migration. On the left is our origin code base named “legacy”. On the right was the proposed design for our new service stack for the migration. Inserted into the middle is the “Service Facade” where we intended to run our quality assurance against live data between the legacy technology stack and our new technology stack. How to Maintain Data Consistency? Depending on the size of the data that is being diffed and migrated between databases, it can be expensive to run the necessary migrations. Our solution was both writing specific tasks that backfilled data or directly migrating data sets to the new data source. This allowed us to smoke test that our services are working, without expending large amounts of time or money finding bugs along the way. As our confidence grew in our custom tooling and services, we would backfill and migrate larger chunks of data, until we had migrated everything necessary to master from our new service. What is a Service Facade? The service facade layer is responsible for executing the respective operations out of the legacy and new stacks. This is where we placed our diffing logic to compare the results returned from Emo and Mysql for the same operation. The facade returns data from the pre-defined configured stack. This meant that certain areas of the application could be sourcing from Mysql, while other areas, that we weren’t confident in, continued to source from Emo. For example, our CampaignRoiReportBuilderServiceFacade written in Scala looked something like this: The original resource classes will be modified to call from the new facade layer, but no other functionality should change. If constructed properly, the facade layer will act in the same way as the original service because the facade mimics the public functions available in the original service class. These duplicated functions will call to the methods from the legacy service class as well as the new service. With the responses from both the legacy and new services, the facade layer can make an assessment on the differences between the two service stacks. To report our differences such that we could be notified during API usage, we would log them out to our log management and monitoring system. How Did We Capture Mismatches? Logging was a large concern of ours. We knew that there would be many differences per call while we were debugging our new service stack. As an example, on one call, we reported 2000+ differences. We wanted to compose all differences into one log per call in a meaningful way. For this, we wrote custom diff tooling that would return differences in the data as sets of MismatchedField classes. This templated class will hold the values returned from both the legacy service (legacyValue), and the new stack’s service (newValue), as well as some meaningful tag with which to identify where this mismatch came from (name). We would then compose all mismatches for any given call into a single log through our custom diff tool. Every function within our custom diff tool returns Set[MismatchedField[Any]]. We can then compose each set into a single set of differences such that we can use only one log call to write out the whole set of differences in one log entry. An Interesting Finding: One of the most interesting findings we had through this migration weren’t bugs that came in constructing our new service stack for the new database, but that we found bugs in our original database stack. One take-away from this was to make sure to investigate any mismatches found down to the source data. We found during the code migration process that some of our legacy functionality was written incorrectly. As an example, in our legacy code we were storing some aggregated data in sets, unintentionally masking duplicate data. When re-implementing these same aggregations for our new service stack, they were correctly implemented as a list, producing a mismatch in the data. Through our investigation, instead of simply matching the data to how our legacy service worked, we went back to our origin data, and ran the calculations manually through the Scala REPL. In doing so, we found that the new service was correct, where our legacy code was wrong. Fortunately, the bug within our legacy code was a simple fix. We implemented the fix within the legacy code and our mismatch disappeared. Other Take-aways: An important team take-away was to be very upfront and declarative about the work that the migration would require. Our investigation into the migration not only involved setting up a new technology stack for MySql, but also changing our build tool from Maven to SBT , introducing a Flyway + Jooq plugin to enforce type safety throughout the migration, designing a new data model (which was ultimately the driving factor for doing the migration in the first place), as well as upgrading our code up to the newest scala version to leverage all of the previous changes. Ultimately, we severely underestimated, and under-ticketed the work necessary to start our migration. It is also important to keep in mind that every team is different and has different needs. When having conversations about database migrations, take the time to do a proper risk assessment for the work ahead.  Keep these conversations going during the migration as well. As a team, we ended up prioritizing new feature requests and non-migration related bugs because the migration felt orthogonal to our production environment. A further takeaway is that we could have saved ourselves a lot of time if we had more realistically assessed our users. In retrospect, the users of these reports were internal and would have been more lenient with smaller service outages, which would have allowed us to leverage our configurable services to migrate much sooner. At the expense of stability, we believe that we could have had a quicker migration by forcing ourselves to fix problems forward, instead of maintaining our legacy code for as long as we did.  Still, most scenarios don’t have this luxury and we hope the façade based approach is of help to you.", "date": "2017-05-24"},
{"website": "BazaarVoice", "title": "Context and Higher Order Components: Two Immediately Applicable Topics from the Advanced React Workshop", "author": ["Andrew Terranova"], "link": "https://blog.developer.bazaarvoice.com/2017/04/26/context-higher-order-components-two-immediately-applicable-topics-from-the-advanced-react-workshop/", "abstract": "Thanks to Bazaarvoice I recently attended an “Advanced React Workshop” put on by React Training and taught by Ryan Florence, one of the creators of React Router . Ryan was an engaging teacher and the workshop was filled with memorable quotes. Here are some highlights: The training took place over two days in East Austin and I learned a ton of stuff that I was able to immediately put to use in my day-to-day projects. Let’s dive right in. In addition to the familiar props and state , React components also have access to a magical thing called context . Context is for when your application has a hierarchical structure and needs to pass data from a parent component down to another level of child component (child, grandchild, great grandchild, etc.). As the React documentation puts it, “In some cases, you want to pass data through the component tree without having to pass the props down manually at every level. You can do this directly in React with the powerful “context” API.” Before we move on to the context API itself, let’s make it a little more clear why you would want to use it. Consider the previous example of a parent node with a child, grandchild, and great grandchild. In order for the parent to pass a property (via props ) down to the great-grandchild , it would first need to pass the property through child and grandchild . Note that there is no other reason for child and grandchild to care about that property. As you can imagine, the desire to eliminate this kind of cruft increases as your project grows in size and complexity. context helps by passing properties through the tree automatically and any component in the subtree can access them. Components (like parent in the example above) define a static childContextTypes object that defines the properties that they will put on context . childContextTypes is very similar to the propTypes object that we’re already familiar with. Additionally, components that are putting properties on context must actually provide their values, and do so via the getChildContext component member function. This function should return an object with the same structure as defined in childContextTypes . For example: To continue the example, your great-grandchild component now must consume this property. It can do so by defining a static contextTypes object that describes the context properties it expects, and can then pull the properties off of this.context directly. Great use cases for context include localization, theming, etc. Basically any time you have things you don’t want to have to pass down through a bunch of components that don’t need to know about them. In fact, React Redux uses context to make its store available to all of your project’s components – imagine how cumbersome it would be to pass the store to every single component! Name collisions is a common pitfall. Consider the case where a component that has both a parent and grand parent component that provide a rating property on context . The suggested workaround in this case is to name your context after your component: Then your child component could differentiate the two: this.context.parentComponent.rating versus this.context.grandParentComponent.rating . A post about context would not be complete without pointing out that the very first section in React’s documentation about context is titled “Why Not To Use Context”, and it includes dire warnings like: “If you want your application to be stable, don’t use context.” , and “It [Context] is an experimental API and it is likely to break in future releases of React.” Ryan did point out to us however, that even though the documentation warns that the Context API is likely to change, it is one of the only React APIs that has stayed constant 😛 In software, a higher order function is a function that returns another function. Consider: Similarly, a Higher Order Component is a function that returns a (React) Component. Higher Order Components are great for reducing redundancy across components, and can also be used to “wrap up” a lot of complexity and encapsulate it, which allows the rest of your components to stay simple and straightforward. If you were familiar with the old React mixins , Higher Order Components replace a lot of what mixins did. Context and Higher Order Components are just two topics I was able to immediately integrate into my React projects here at Bazaarvoice. Hopefully you will find them useful too!", "date": "2017-04-26"},
{"website": "BazaarVoice", "title": "My Hacktoberfest", "author": ["Andrew Terranova"], "link": "https://blog.developer.bazaarvoice.com/2017/02/21/my-hacktoberfest/", "abstract": "This past October I participated in an awesome Open Source event called “Hacktoberfest”, sponsored by Digital Ocean and GitHub. Hacktoberfest is a month-long celebration of Open Source where developers are encouraged to contribute to the community. Participation is easy: Further, if you opened four pull requests in Open Source repositories between October 1st and October 31st you would win a cool Hacktoberfest t-shirt and other swag. Maintainers of Open Source projects (including some here at BV) were asked to tag open issues with “Hacktoberfest” if they wanted help with that issue during the event. GitHub provides the ability to search issues based tags, so it was really easy to find cool projects and issues to work on. I personally started off small, helping one team track down a bug with their JSON files, and another finish a database for movies used by their front-end application (similar to IMDB ). Next I found a Hacktoberfest issue in the the New York Times’s kyt repository. Kyt is a build, test and development tool for advanced JavaScript apps. I ended up helping them fix a bug in one of their setup scripts. Then came my Hacktoberfest pièce de résistance. In my 20% time here at Bazaarvoice I had been playing around with browser extensions / add-ons, specifically in an effort to make implementing our products easier for our clients. So when I saw that Mozilla and the Mozilla Developer Network (MDN) were asking for someone to create a browser extension for them, I was immediately interested. They noticed that a popular type of extension being authored was what they were calling a “replacement” add-on, something that would replace words or phrases in a web page with alternate words, or images, etc. In their Web Extensions Examples repository, they were looking for an example of such an add-on that they could turn into a “How to Write your First Add-On” tutorial. Thus their two main requirements were: Seeing as how readability and performance are two of the main things that we check for in every code review here at Bazaarvoice, this was right up my alley! I was so excited that I stayed up all weekend to finish the project: @supernova_at Hacking together open source late at night on Friday, October… 14th? It’s almost too sp00ky to miss! — DigitalOcean (@digitalocean) October 15, 2016 I submitted my pull request, worked with the developers at Mozilla, and was so proud when my Emoji Substitution contribution was merged into their repository. What a rush! As we traded Hacktoberfest-themed emoji (???? and ???? were my favorites), fixed bugs, and fleshed out their projects, it was really cool to lend my expertise and experience the gratitude of all the teams I worked with – this is what Open Source is all about! I had a great time participating in Hacktoberfest this year and will definitely do it again next year. You should join me!", "date": "2017-02-21"},
{"website": "BazaarVoice", "title": "I want to be a UX Designer. Where do I start?", "author": ["Litsa Litsa"], "link": "https://blog.developer.bazaarvoice.com/2017/01/27/i-want-to-be-a-ux-designer-where-do-i-start/", "abstract": "So many folks are wonder what they need to do to make a career of User Experience Design. As someone who interviewed many designers before, I’d say the only gate between you and a career in UX that really matters is your portfolio. Tech moves too fast and is too competitive to worry about tenure and experience and degrees. If you can bring it, you’re in! That doesn’t mean school is a waste of time, though. Some of the best UX Design candidates I’ve interviewed came from Carnegie Mellon. We have a UX Research intern from the University of Texas on staff right now, and I’m blown away by her knowledge and talent. A good academic program can help you skip a lot of trial-by-fire and learning things the painful way. But most of all, a good academic program can feed you projects to use as samples in your portfolio. But goodness, choose your school carefully! I’ve also felt so bad for another candidate whose professors obviously had no idea what they were talking about. Okay, so that portfolio… what should it demonstrate? What sorts of samples should it include? Well, that depends on what sort of UX Designer you want to be. Below is a list of to-dos, but before you jump into your project, I strongly suggest forming a little product team. Your product team can be your your knitting circle, your best friend and next-best-friend, a fellow UX-hopeful. It doesn’t really matter so long as your team is comprised of humans. I make this suggestion because I’ve observed that many UX students actually have projects under their belt, but they are mostly homework assignments they did solo. So they are going through the motions of producing journey maps, etc., but without really knowing why. So then they imagine to themselves that these deliverables are instructions. This is how UX Designers instruct engineers on what to do. Nope. The truth is, deliverables like journey maps and persona charts and wireframes help other people instruct us. In real life, you’ll work with a team of engineers, and those folks must have opportunites to influence the design; otherwise, they won’t believe in it. And they won’t put their heart and soul into building it. And your mockups will look great, and the final product will be a mess of excuses. So, if you can demonstrate to a hiring manager that you know how to collaborate, dang. You are ahead of the pack. So round up your jackass friends, come up with a fun team name, and… Demonstrate product discovery. Demonstrate collaboration. Demonstrate ideation. Demonstrate collaboration. Demonstrate prototyping skill. Demonstrate collaboration. Demonstrate that you are paying attention. Demonstrate collaboration. Whew! That’s a lot of work! I know. At the very least, school buys you time to do all this stuff. And it’s totally okay to focus on just UX Research or just Visual Design and bill yourself as a specialist. Anyway, if you honestly enjoy UX Design, it will feel like playing. And remember to give your brain breaks once in a while. Go outside and ride your bike; it’ll help you keep your creative energy high. Hope that helps, and good luck! This article was originally published on Medium, “ How to I break into UX Design? “", "date": "2017-01-27"},
{"website": "BazaarVoice", "title": "As a software engineer, how do I change my career to DevOps?", "author": ["jona fenocchi"], "link": "https://blog.developer.bazaarvoice.com/2017/01/23/as-a-software-engineer-how-do-i-change-my-career-to-devops/", "abstract": "At Bazaarvoice, we’re big fans of cloud. Real big. We’re also fans of DevOps. There’s been a lot of discussion over the past several years about “What is DevOps?” Usually, this term is used to describe Systems Engineers and Site Reliability Engineers (sometimes called Infrastructure Engineers, Systems Engineers, Operations Engineers or, in the most unfortunate case, Systems Administrators, which is an entirely different job!). This is not what DevOps means, but in the context of career development, it carries the connotation of a “modern” Systems or Site Reliability Engineer. There’s a lot of great literature about what a DevOps engineer is. I encourage you to read this interview of Google’s VP of Engineering , as well as Hixson and Beyer’s excellent paper on Systems Engineering and its distinction among Software, Systems and Site Reliability engineers. Although DevOps engineering goes beyond these technical descriptions, I’ll save that exegesis for another time. (Write me if you want to hear it, though!) Many companies claim to hire or employ DevOps engineers. Few actually do. Bazaarvoice does. Google does, too, although they’re very hipster about it (they called it Site Reliability Engineering before the term DevOps landed on the scene, so they don’t call it DevOps because they had it before it was cool, or something). I don’t know about other companies because I haven’t worked at them (well, I haven’t worked at Google either, but they are pretty vocal about their engineering philosophies, so I’ll take them at their word). But there’s a lot of industry buzzwordium with little substance. This isn’t a jab at other companies (but really, Bazaarvoice is way cooler), it’s just a side-effect of assigning job titles based on pop culture. If you’re really a DevOps engineer, then you already know all of this, and you probably filter out a lot of this nonsense on a daily basis. But we’re here to answer a specific question: If I’m already a software engineer, how do I become a DevOps engineer? So, you’re a developer and you want to get in on the ops action. Boy, are you in for a surprise! This isn’t about installing Arch Linux and learning to write Perl. There’s a place for that kind of thing (a very small, dark place in a very distant corner of the universe), but that isn’t inherently what DevOps means. Let’s consider a few of the properties and responsibilities of DevOps engineering. A DevOps engineer: Phew! That’s a lot. Turns out, almost all of these skills are directly applicable to software engineering. The only difference is the breadth of domain, but a good software engineer will grow his breadth of domain expertise into operations naturally anyway! A DevOps engineer just starts his growth from a different side of the engineering career map. Let’s stop and think for a moment about some things DevOps engineer is not . These details are critically important! A DevOps engineer is not: A career shift Here are a few things you should do to begin positioning yourself as a DevOps engineer. Do I need to have deep operations experience to become a good devops engineer? I’ve asked myself the same question. I come from a development background myself and only had less than a year of experience dealing with operations (part time) before becoming a DevOps engineer. (Granted, I had a referral vouching for me, which weighed in my favor.) Despite my less-than-stellar CS/algorithm skills (based on my complete lack of formal computer science education), I’ve had enough experience writing software that I could apply these concepts to systems in a similar fashion. That is, in the same way a software engineer needs to understand at what point his application code should be abstracted in support of future changes (i.e., refactored into smaller components), a DevOps engineer needs to understand at what point a component of his infrastructure should be abstracted in support of future changes (i.e., rebuilding all of his servers and rearchitecting infrastructure that’s already in production, despite the potential risk, in order to solve a problem, like allowing the architecture to scale to business needs). At its core, a good engineer is just as good whether he’s writing software or deploying it. Understanding complex systems and their interactions is a critical skill. But all of these are important for software engineers, whether you’re writing application code or not! I hope this post helps you in your endeavor to become a DevOps engineer, or at least understand what it means to be a DevOps engineer at Bazaarvoice (as I said before, it may mean something totally different at other companies!). You should get your feet wet with some of the things we do. If it gets you tingly and excited, then come work with me at Bazaarvoice: http://www.bazaarvoice.com/careers/research-and-development/ . This article was originally posted as an answer on Quora . Due to surprising popularity , I’ve updated the article and posted it here.", "date": "2017-01-23"},
{"website": "BazaarVoice", "title": "Cross-Platform Mobile SDK Testing", "author": ["Tim Kelly"], "link": "https://blog.developer.bazaarvoice.com/2017/01/03/cross-platform-mobile-sdk-testing/", "abstract": "This Bazaarvoice blog entry is co-authored by Tanvir Pathan as part of a Bazaarvoice internship project on the Bazaarvoice Mobile Team. Automated testing of native mobile applications has long been a pain point in the world of mobile app development. If you are creating and distributing apps or open source SDKs across two or more major platforms (Android and iOS in our case), you can easily find yourself duplicating efforts to test the same source and business logic across different technology stacks. For example, if you have experienced developers and testers using Xcode for iOS apps, they may tend to automate testing with Instruments Automation, where Android developers and testers may automate with Espresso or UIAutomator . This becomes an expensive proposition for development and maintenance of unit tests, which can be costly as your test coverage increases. Test strategy can also vary depending on the type of mobile app development your shop pursues: native, hybrid, cross-compile, mobile web. Hence, the selection of test tools will vary depending on how you build and deploy apps. In this blog post, we’ll detail a novel solution to cross-platform testing of our native SDKs, along with some background of other mobile tool offerings. Our solution focuses on cross-platform open source mobile SDK testing utilizing Cordova to wrap our SDKs in a generic JavaScript interface, and Calabash to drive our cross-platform behavioral tests. If you want to check out the full solution, the Cordova plugin and description on how to execute Calabash can be found in our Github repository. Non-mobile app developers typically don’t actually know the difference between a web app, native, or hybrid app. If you work in any business that supports some kind of mobile solution (and you probably wouldn’t be reading this if you didn’t) it’s really important to understand some fundamental differences. It’s very easy to just throw out the word “mobile” in conversation and not realize there’s multiple parts to this elephant! The table below presents four general categories of mobile application development. Keep these categories in mind when talking about “mobile” in general and don’t fall in the trap of the blind men and the elephant. When developing for native mobile, developers will typically write unit tests to check individual pieces of functionality and business logic, perhaps even employ certain mocking techniques to test networking and user interface capabilities without the need for a full application. However, when it comes to full system testing of full applications and SDKs, making the right selection can be a tough process. However, if cross-platform testing is your objective and you want to write all your tests in one common scripting language, the options narrow quickly. While there are platform-specific UI automation frameworks for Android ( Robotium , UiAutomation ) and iOS ( Instruments Automation , Keep It Functional , EarlGrey ), there currently only two (that we are aware of) that allow us to test cross-platform with a common script. Making the decision to use Cordova and Calabash was fairly easy. First we already distribute our BBVSD via frameworks and libraries for iOS and Android. Second we know some of our customers are creating hybrid applications with Corodva. So we immediately thought: what if we could create a test environment that not only tests our SDK deliverables, but also provides our clients with an easy avenue to integrate Bazaarvoice services into their own Cordova app. Win! Win! As well, because we already use Cucumber extensively at Bazaarvoice, we decided to leverage our already strong in-house expertise and utilize an automation framework that is internally familiar. Another great thing about using Calabash at Bazaarvoice is that we already have an internal framework developed on top of Cucumber. Because Calabash layers on top of Cucumber, the paradigm and philosophy of writing human readable test cases still applies. The test cases utilize Behavior Driven Development modeling tools to add meaning to your mobile app testing. Let’s say you are creating the same app for multiple platforms. Typically, you would have to write completely different sets of code to run similar tests. With Calabash, this is not the case. You write one set of code tests and make slight adjustments depending on the platforms in question and you are done!  Best of all, in addition to Calabash being free, the test cases are super easy to write as a developer and even easier to read for others who may be interested in checking out the health of the project. Needless to say, Calabash provides a lot of benefits for cross platform testing. Lets take a look at an example test case from the BVSDK Cordova Plugin project. Let’s go through a simple scenario based on the following app screens shots from the iOS simulator. Say you wanted to count the number of products that were recommended by our Product Recommendations API. If you were doing it manually you would go through the following steps: Now how would we code this? Calabash has two essential components: one feature file and one ruby file. The feature file is where you write out the tests and the ruby file is used primarily to make custom functions if needed (although most of what you need comes right out of the box). So returning to our problem, writing out the test case, you simply write down those exact steps in the feature file: That’s really all there is to it. Of course, tests can be also written to be more platform specific when needed. We use Travis CI for all our public repos at Bazaarvoice. It’s awesome. But, we have to support multiple build tools on different virtual machines. Configuring all these build machines with custom tools sounds and build scripts really scary! Freak out! The really slick thing about Travis CI is that you can test multiple configuration, variations, permutations, salutations, etc, etc, etc, by building a matrix in Travis’ Config file (.travis.yml). For our testing, since XCode only runs on OS X and it’s the only way to build for iOS, we must have an OS X image. For the Android Studio and Gradle build tools, we build against Linux. In addition there’s some common tooling we can install for each build machine. The result is that we can use two different VMs for testing each platform, with just one set of tests. Note in the test result below, the build jobs are defined by the environment variables defined in the Travis config file. The .travis.yml script looks like this, where we build a matrix with environment variables and platforms: So what if I want to try out the BVSDK Cordova plugin? If you want more info or checkout the source code for the plugin and unit tests, just head over to our Cordova Github repo. There’s plenty of info in the README for running the examples and unit tests. If you are building a Cordova-based application and want to see other things added just let us know , or better yet submit a pull request and we’ll be happy to review it!", "date": "2017-01-03"},
{"website": "BazaarVoice", "title": "Augment your pattern library with page types", "author": ["Litsa Litsa"], "link": "https://blog.developer.bazaarvoice.com/2016/12/16/augment-your-pattern-library-with-page-types/", "abstract": "P attern libraries sometimes fall short of helping enterprise teams build different products the same way. These palettes of components (toolbars, pop-ins) and patterns (searching, navigating) can be assembled into any number of UIs, leading to too many right answers. While the public pattern libraries like Google Material must accommodate countless unimagined applications, our private libraries can serve us better. We have special insight into our own users’ workflows. A page type is a layout and set of patterns packaged together according to the workflow they support. If your pattern library is a basket of ingredients, your page types are the recipes. These starting points are immensely helpful in a few ways: Many pattern libraries, especially the older ones like Yahoo Design Pattern Library , take a bottom-up approach. Documentation starts with the component, noting its general purpose, but focusing primarily on its interactive states. A handful of examples show the component used in various contexts. It is up to designers to imagine how this information relates to their own projects. Page types are top-down: in this workflow, these components are used in this way . The example below shows two Object Editor pages with different interpretations of “Toolbar.” The top applies Google Material’s definition of Toolbar: Toolbar actions appear above the view affected by their actions. The bottom applies a workflow-specific definition of Toolbar: If the object is edited indirectly and previewed, configuration actions and preview actions are separated into panel and toolbar, respectively. In this way, the user is not led to believe temporary preview modes (like zoom) are saved with their configuration. It takes a lot of design thinking to work through how components could best serve a workflow. It is highly valuable, then, to document your best solutions. Page type documentation prescribes the layout, component arrangement, and interactive patterns used to achieve a desired workflow. Here’s list of page types common to enterprise applications: In application design, the layout and use of components on any given page create a workflow that serve the page’s central purpose. If the central purpose of your application page is not singular, your design is probably overcomplicated. Before you invent a new page type, reconsider your application architecture. For example, the central purpose of a document editor is to edit a document. If the page is well-designed, its layout maximizes the editable area, and its buttons and tools all relate to editing. Notice that Google did not smash document editing, management, and publishing into one page. Sometimes different workflows serve the same central purpose. For example, direct editing and configuration are very different workflows even though their central purpose, Object Editing, is the same. In cases like these, it’s appropriate to offer variations. While page types promote focused design, discipline should not compromise usability. In the examples above, direct editing is the easiest way to work with text. Configuration is the easiest way for non-technical users to change XML values, build an email template, or add filters to a photo, etc. It’s important to differentiate between workflow and content presentation : tables emphasize data, lists emphasize titles, cards emphasize media, etc. If the workflow is the same, one page type can house various content types in their optimal formats. In the Object Manager examples below, relevant activities—finding, filtering, selecting, applying actions, etc.—are the same and can be accessed the same way. Similarly, let workflow define your page types, not content presentation or layout. “Table page” is not a good page type because your users do not want to table. Page type templates accelerate design and development projects by advancing their starting points. They also simplify the information architecture design process by providing constraint: Each screen maps to a page type. Therefore, each screen represents a workflow with singular purpose. Adhering to this principle influences decisions around how to group functionality into various pages. An IA design that uses your company’s set of established page types as illustrations is more tangible to stakeholders. Page types are not new; website designers—especially those who use template-based CMS’s like Joomla—have been using them all along. They are essential to Information Architecture. We application designers have been somewhat distracted. Pattern libraries—especially when incorporated into UI development environments like Storybook —are incredibly useful. However, only we know what we want to help our users do. Our own private pattern libraries can be far more workflow-aware than the public libraries from which we draw our inspiration. What are your page types? This post was originally published on Medium.com", "date": "2016-12-16"},
{"website": "BazaarVoice", "title": "Ah, But Do You Have a Flag?", "author": ["patrick.sullivan"], "link": "https://blog.developer.bazaarvoice.com/2016/10/27/ah-but-do-you-have-a-flag/", "abstract": "Hey you there, did you know that forty percent of all data breaches are due to web application vulnerabilities ? That means the very software your team is building is likely to be the vector to getting your data pwnd. Still feeling skeptical? You should google Heartland’s 2008 breach , eBay’s XSS vulnerability , or Time Warner’s password leak . I’ll wait. Done? Pretty scary, isn’t it? Great, But How Do You Get Your Developers Thinking About Security? The discipline of bullet-proofing your code against application vulnerabilities is called Secure Coding . You want this fancy secure coding to up your AppSec game, but what if your R&D organization lacks the skills? You hired smart people, they can learn it, but they need to want to.  They need to feel it. So how do you get your team stoked on security awareness (besides telling them to stop writing their passwords on post-it notes)? The first thing you do is put together a rip-roaring slide deck with the top ten security flaws and a snazzy background and get them to read the heck out of it.  Developers love slide decks. Hmm, That didn’t work. If only there were a better way, more engaging way.  And there is. Did you learn to code just by reading about Java? No way. You started working on coding examples to get the hang of it, right?  Maybe you even got your code katas or koans on so you could motivate yourself. Why not do the same to cultivate some security awareness love? AppSec enthusiasts commonly compete in Capture The Flag contests. No, not this . Not this either. There are a couple of CTF formats out there, but the Jeopardy format is the one that best suits the needs of introductory training. This format is made up of a ladder of increasingly difficult puzzles. The ladder works like this: You might be thinking, “This is great advice but how do I get me one of those CTF contests?” We thought the same thing. We didn’t have time to wait for someone else to put together a competition, and we wanted to make inroads on secure coding training in a more controlled environment. What to do? What We Did Some of us had competed in the Stripe 2.0 CTF like, 37.5 computer years ago (that’s roughly 4 years ago in people years). Fortunately, the good people at Stripe open-sourced those very same web app puzzles. Yea! But they had languished untouched in the backwaters of github. Boo! We needed… After some studious digital archeology in the form of ancient version management, we resuscitated the puzzles. Once we had the puzzles in hand, we used veewee to roll a VirtualBox (VBox) compatible VM with some scripting magic to auto-generate the loot values. In this VM, each puzzle was set up to run sandboxed away from the casual user, but still gave them access to the source code. How Can You Do the Same? If you’ve read this far, you might be ready to introduce some CTF-based training to your organization. You might still be thinking, “This is great and all, but how do I get me one of those CTFs?” Scroll no further, true believer. We have open-sourced all the material you need to conduct your own CTF training right here in this very github: https://github.com/bazaarvoice/stripe-ctf-2-vm The instructions to roll the VMs can be found here . The slide deck needed for the training sessions can be found here . It’s like today is your birthday! In the next post, I will explain how we developed an introductory secure coding training session around this vm and provide advice on how you can do it too. Now, go get that flag!", "date": "2016-10-27"},
{"website": "BazaarVoice", "title": "Three Takeaways from CSSConf 2016", "author": ["Andrew Terranova"], "link": "https://blog.developer.bazaarvoice.com/2016/10/17/three-takeaways-from-cssconf-2016/", "abstract": "This year Bazaarvoice sponsored CSSConf 2016 in beautiful Boston, MA, USA and I was able to attend! Here are my three top takeaways from CSSConf 2016: A little over a year ago, our application team wasn’t sure how “stable” Flexbox or its spec were: there was already an old syntax, a new syntax, and a weird IE10 “tweener” syntax . The layout advantages Flexbox brought were strong enough (*cough* vertical centering) that we decided to move forward with it and prefix all the things. Now browser support is so good that if you can drop IE8 and work around some known IE11 bugs, there is no reason not to use Flexbox in your designs right now. A great reference I keep going back to for Flexbox is this css-tricks guide . Here are some other tips and tricks from the conference: For more information, I recommend CSS4 Grid: True Layout Finally Arrives by Jen Kramer and It’s Time To Ditch The Grid System by Emily Hayman . The talk Stop Thinking In Pixels by Keith Grant was particularly enlightening. The basic premise was not to micromanage your CSS: Without fully understanding what CSS is doing for us, we try to push through it to control exactly what is going on in the browser. Driving this point home, Keith recommended to stop thinking in pixels because The pixels don’t matter. Let the browser do it. You should instead be thinking in terms of the em and rem . Tools that simply convert px to em aren’t the answer either — you’re still thinking in terms of pixels and therefore missing out on some important benefits. Instead, learn from something like type scale and approach measurements with a fresh perspective. I recommend watching the talk in full, but a quick cheatsheet follows: When in doubt, use em . To summarize, Ems are the most powerful when you fully embrace them. In this day and age we are all used to thinking in terms of “apps”. But the trinity of HTML, CSS, and JS was not conceived in this day and age. Two great quotes I wrote down from Component-Based Style Reuse by Pete Hunt are CSS is great for documents, maybe not 2016 Apps and If you sat down and created styling in 2016, you would not come up with CSS Our newest applications are written in React , which encourages developers to think of things in terms of components — pieces of UI that are reusable in different contexts. The C ascading part of CSS interferes with that, however: depending on the context your component is dropped into, it may look drastically different across usages. When that is not what you want, Pete’s ideas center around reusing components , not CSS classes. As you can imagine, this idea is largely controversial in a conference with a name like CSSConf, but I will continue to keep my eye on it. Pete’s thought leadership on this topic inspires me to challenge norms and dare to envision things differently. After all, if we’re constantly fighting with our tool (CSS), that tool may not be right for the job. Thanks for reading! For a full list of talks and slides from the conference, check out https://2016.cssconf.com/#videos .", "date": "2016-10-17"},
{"website": "BazaarVoice", "title": "The one thing you need that will make you and your team successful", "author": ["Bryan Chagoly"], "link": "https://blog.developer.bazaarvoice.com/2016/08/17/the-one-thing-you-need-that-will-make-you-and-your-team-successful/", "abstract": "Over the last 20 years of my career, I have worked with a lot of different people and lot of different teams. Some were very successful, and some were not. I am always trying to understand what makes successful people tick, and what I can do differently to be more successful. The one thing that I have found that consistently is the determining factor as to whether a person or team will be successful is their energy. Energy takes a lot of different forms and states. There is high energy and low energy, positive energy and negative energy. Everything in the universe is energy. Everything has a force and a pull and a gravity. Every person has an energy. Some people call it a “spark”, or “Spirit”, or “vibration”. If you pay close enough attention, you can see it, and sometimes you can even feel it.  Have you ever walked into a room where a group of people had been discussing something serious, and you can feel the negative energy in the room. Have you ever worked with a very successful charismatic leader who just seemed to attract winners? Why is that? What is that? The most successful people and teams that I have encountered or had the privilege to work with have very positive and high energy. Everyone on the team is excited and passionate to be working there. They love what they do, they love working with the people on their team, they know they are going to win, they know they are making a positive difference in the world, and they have fun winning. That kind of positive high energy feeds and builds on itself. The more positive high energy people you get together, the better the team will be. It’s like waves in an ocean oscillating at the same frequency. They multiply the goodness.  These are the people who see the future, and the solutions and the answers and choose not to focus on the past or the problems. How does your energy affect others around you? How does other’s energy affect you? How does your boss’s energy affect you? The opposite of positive high energy, is negative low energy. It manifests itself in people who fixate on the past and the problems. They are the people who think that it can’t be done. They are the “We’ve tried that before and it didn’t work” people. They are the naysayers. They typically start all new requests for change with “No!”. They are always bickering or blaming others or finger pointing or micromanaging. There are many possible reasons for a person’s negativity. It could be their FUD (Fear Uncertainty and Doubt) about the situation. It could be their fear of failure, or their fear of embarrassment, or their fear of looking stupid in front of their peers, or their insecurity in their position. The truth is that your emotional state is affected by those around you.  Just like positive energy builds on itself, so does negative energy. It’s like a rotten apple, or a cancer. It grows and infects others who are near it. I have seen one negative energy person bring down an entire team. And as with any cancer, it has to be cut out quickly. It’s easy to say, but hard to do. I have seen too many good managers who I respected wait way too long letting a negative energy person fester in the team. The reality is that you have the power to choose how you exert your own energy.  You have the power to choose who you want to work with. Not everyone is positive high energy all the time. We all have our good days and bad days, but you can choose to be passionate and excited and have a high positive energy, or you can be the opposite. So how can you get started growing your positive high energy? Tell me what part of our story you want to hear next. How do you build a team and culture that enables you to execute on your vision?  Follow me on twitter @bchagoly and @bazaarvoicedev to be the first to read new related posts and to join the conversation.", "date": "2016-08-17"},
{"website": "BazaarVoice", "title": "Intern Demo Day", "author": ["John Banning"], "link": "https://blog.developer.bazaarvoice.com/2016/08/10/intern-demo-day/", "abstract": "As the summer comes to an end, so do the internships for numerous university students here at Bazaarvoice. This past week, the interns were given an opportunity to present a summary of their accomplishments. This afternoon of presentations, known as the Bazaarvoice “Intern Demo Day”, highlighted the various achievements throughout the company, not just in the R&D department. The following is a short summary of the great work our interns complete this summer as well as some images from the “Intern Demo Day”. CHASE PORTER: My project, which I have named “The Great (red)Shift”, is intended to improve data accessibility for computed aggregated counts of various canonical events written to HBase. To do this I designed a data warehouse in Amazon Redshift that I loaded with transformed aggregated counts extracted from the tables in HBase. This makes the counts readily SQL query-able in an incredibly fast system whereas before they had to be computed with performance heavy queries from Raw Logs generated by Cookie Monster. The biggest block for this project was in processing the data from HBase which was stored as serialized bytes and needed to be handled uniquely for different types of canonical events (i.e. pageviews, impressions, features) to translate into a readable form for Redshift. BEN DEVORE: My product is web crawler written in node.js that scrapes clients’ webpages for product data in order to build their product feeds for them. For many of Bazaarvoice’s smaller clients, building and maintaining their product feed is a significant obstacle in the onboarding process. This tool aims to clear that obstacle by taking this task out of there hands. STONEY MCCRARY: So I have been fortunate enough to get to work on several different pieces in curations but I am going to talk on what I have been hammering on for the last couple of weeks. More and more of our high volume clients are receiving millions of hits a day and this has caused performance to become a higher priority problem for them. In response to this, we are focusing our efforts on building a new display with performance in mind. Performance for the display centers around only providing the minimal amount of data needed and supply the rest as necessary. The piece I will be showing is the display carousel and how it dynamically loads and dumps the data to allow for faster loading and to keep browser memory low. ZESHAN ANWAR: Eagle is a dashboard built for our Incubator team. With so many moving parts, it was important we had a summarized ‘birds-eye’ view of the team in one place. Eagle was initially meant to be an aggregation of all our Jenkin builds; a single view of all our jobs across our different Jenkins environments. However, it grew to also include JIRA and GitHub statistics. My other project was optimizing our UI tests by having them run concurrently. Our old UI tests were extremely slow, and by running them in parallel we drastically reduced test times. BRENDON KELLEY: Testing Framework: This summer my project was to help build out a new testing framework for Curations. The current automation tests used for Curations is Saladhands. Before my internship, there wasn’t much if any automation tests for the submission/direct upload capability of Curations. I worked on creating tests and a CI environment for submission in a new testing framework called Intern. One of tests includes a language translation test using mongoDB as an endpoint to store the various languages’ text. Intern is a javascript based testing framework which will allow developers to contribute to writing tests since Curations is mostly javascript. I’ve also worked on updating and creating new console tests in this framework. The foundation built this summer in Intern will enable the ability to further contribute to the framework. KRYSTINA DIAO: My main project for the summer was to analyze and report the effectiveness of the implementation of the new Connections Knowledgebase. Through Salesforce, I collected and analyzed the number of cases, time spent on each case, etc. After drawing my conclusions, I decided to present my findings via data visualization methods (JavaScript’s C3 and D3 libraries) and provide actionable insights on how this information can be leveraged. This information is valuable in that it can be used for future product KB decisions, as well as understanding how much time, manpower, and money is saved by having a KB. MARKO SAVIC: Over the summer, I was a part of the SEO Team. I managed to create a tool on pagesManaged and keywordsManaged feed for every Spotlights client. Generated feeds will be consumed by SeoClarity tools on a daily basis. This helps in identifying search rank gains on the specified keywords and pages where Spotlights are present. The SeoClarity reporting will help in proving out Spotlights value and eventually lead to Spotlights renewal/upticks.Also, I created algorithm tweaks on the PINS (Post Interaction Notification System) Generator that take into account product activeness, product relevancy and review count, and use them to ask the user to write reviews on the most relevant products. TREVOR NELLIGAN: Here is a description of my project: I worked on the Aperture Component library and many of the projects it supports. Aperture is build in React, and its purpose is to be used as an internal Bazaarvoice tool for constructing web pages. Using Aperture, anyone at Bazaarvoice can easily create a functional, intuitive, Bazaarvoice themed webpage, all with the building blocks Aperture provides. Using the Aperture library, I helped the construction of numerous pages for the curations beta console. I personally built the interface for a new client-facing template builder, which will allow clients to create curations templates quickly and easily without having to go through an implementation engineer and a long process, as was the case previously. I also supplied custom Aperture components for several projects, like the content curation beta page. RAMIE RAUFDEEN: The mixer is a component of our product recommendations engine which differentiates shoppers, and optimizes recommendations for them. This is primarily derived from their shopping behavior – in real time. Prior to the mixer, product recommendations were aggregated from multiple sources, using the same algorithm for every shopper. Shoppers are now categorized based off of a set of rules (using the shopper’s profile data), each of the rules map to a plan (which you can think of as an ‘algorithm’). A plan defines how recommendations should be mixed from each of the sources. For example, if source B has proven to have a higher conversion rate for ‘heavy-shoppers’, the plan for ‘heavy-shopper’ would give a higher weighting to source B. We can now target specific types of shoppers when it comes to product recommendations. This also sets the groundwork for a more granular machine learning implementation in the future. We want to thank all the interns who spent time with us this summer and wish you the best back at school. We look forward to hearing about all the great things you all develop in the future. If you are interested in an internship at Bazaarvoice, please contact kindall.heye@bazaarvoice.com.", "date": "2016-08-10"},
{"website": "BazaarVoice", "title": "HackerX event hosted at Bazaarvoice", "author": ["John Banning"], "link": "https://blog.developer.bazaarvoice.com/2016/07/26/hackerx-event-hosted-at-bazaarvoice/", "abstract": "The Bazaarvoice headquarters hosted the July 20th HackerX event in Austin, Texas. The event featured not only Bazaarvoice , but also included Facebook, Amazon, and Indeed.  70+ engineers participated in onsite interviews and networking. HackerX commented that “this was one of the most successful events” they have ever seen. Gary Allison , Executive Vice President of Engineering, kicked off the event with a compelling message about Bazaarvoice and why this is an awesome place to work. HackerX started in 2012 with invite-only, face-to-face recruiting events that connect tech talent with some of the world’s most innovative companies. Currently, they operate over 100+ events in 40+ cities, 15+ countries annually. See www.hackerx.org for additional information.", "date": "2016-07-26"},
{"website": "BazaarVoice", "title": "Don’t Look Back in Anger: Retrospectives, Software Development and How Your Team Can Improve", "author": ["Gary Spillman"], "link": "https://blog.developer.bazaarvoice.com/2016/07/29/dont-look-back-in-anger-retrospectives-software-development-and-how-your-team-can-improve/", "abstract": "Retrospective – This term can elicit a negative response in people in the software development industry (verbally and physically).  After all, it is a bit of a loaded term.  Looking back can be painful especially since that usually means looking back at mistakes, missteps and decisions we might want to take back. I have worked for over a decade in software development and information technology fields and during that time I’ve been involved in many meetings you could label as ‘retrospective’.  Some of these have been great, others terrible.  Some were over-long sessions of moaning and wailing resulting in a great deal of sound and fury, signifying little while others have been productive discussions that led to positive, foundational change in the way a team operated. Looking back at all of that, I’ve realized a few common truths to the process of retrospectives and how it relates to building software: For the past five months, my team has been employing a process to facilitate retrospectives based around the three bullet points above to foment positive change in the way our team works.  This blog post will detail how we do this.  This is not to say, “you should perform retrospectives and you should do them this exact way ”, because every software team works differently.  This however is to say, “here’s an example of how the retrospective process was done with success”.  Maybe it could work for you or give you ideas on how you can form your own process that works best for your team. OK, let’s get the grisly stuff out of the way first:  Your team needs to improve.  And that is OK – because no team is perfect and no matter how well we execute on our goals, there’s always room to improve.  In fact, if your team is not at some regular interval assessing or reassessing how you design software, you’re probably ‘doing it wrong’ because (can you smell the theme I’m cooking up here yet?) – there’s always room to improve.  If you’re exploring or actively participating management concepts like Kanban/continuous improvement, this process is essential. OK – you want to improve your process and are ready to discuss how that can be done.  How does the rest of your team feel about that?  You can drag a horse to water and you can drag your team mates into a meeting but you can’t make either drink (unless you’re looking to rack up some workplace violations and future sensitivity training). This is not how we define team work. It’s important to get a consensus from the entire team in question before proceeding with a retrospective.  This will actually be easier than some might think.  After all – in software engineering, we’re all trying to solve complex problems.  This is just another problem that needs figuring out. Before going further, you’re going to need a few things: The tough stuff is out of the way at this point.  You and your team have decided to fine tune your process and have met in a room together to do so (the 2 boxes of Shipley’s donuts you brought with you is barely a tangent in this case). When we started performing retrospectives regularly on the Shopper Marketing team, we started our discussion by talking about our recent successes as a team. In this case, we as a team, one by one went around the room and listed one thing during the past period (sprint, release cycle, week, microfortnight, etc. – the interval doesn’t matter, whatever works for your team) and offered 1 thing we felt the team did well.  Doing this has a couple of functions: Note that, especially with larger teams, sometimes one’s perception of how well something might have gone might be very different from someone else’s.  If Jane, your engineering head for your web service says she thought the release of the bulk processor was on time and under budget but Bill, your UI lead contends that it in fact wasn’t on time, wasn’t under budget and wiped out an entire village and made some kids cry, then this is the point where you should pause the retro and have a brief discussion about the matter and possibly a follow-up later.  This leads us to our next step… Now that you’re full of happy thoughts about what you’ve done well as a team (and full of donuts) its time to discuss change.  This process is the same as the previous step only this time, each team member must call out one thing they think the team could have done better. Note that this isn’t about what was bad, or what sucked, or so-and-so is a doofus and has bad breath – but what can the team do better .  In order to keep the discussion productive and expedient, each person is limited to 1 item and that item has to be something the team has direct agency over.  You also might find it handy to nominate a moderator for this point of discussion. Here’s an example of a real issue that was brought up in a retrospective at a previous company I worked for and the right and wrong way to frame it when it comes to focusing on ‘your team’s agency’. “ Marketing and sales sold our client a bunch of features that our product doesn’t do and we had to work 90+ hours this week just to deliver it” -Or – “We should work and communicate closer with marketing and sales so they understand our product features as well as the effort required in developing said features” While the first statement may be true it’s the second one that actually encapsulates the problem in a way the team has a manner of dealing with.  I found that focusing strongly on the positive aspects of a retrospective discussion helps some teams to self-align toward reaching toward the kind of perspective found in the latter statement above than the former. Other teams I found realized the need to appoint a moderator to help keep the discussion focused.  Its important to figure out what sort of need your team has in this regard early in the retrospective process.  This might seem tough at first (and emotionally charged) and that’s OK.  What’s important is to keep a positive frame of mind to work through this discussion. As previously stated, if there appears to be a major disconnect between team members regarding issues that could have been done better, this is a good time to discuss that and hopefully iron out any misunderstandings or make plans to do so soon after the retrospective. Whoever is taking notes, I hope you’re getting all of this because we’re about to tie it all together. By now, you and your team will have come up with a good deal of input on what the team did well and what could be improved.  You might be surprised but having done this numerous times, often a very clear pattern to what the team though went well and could have improved on appears and does so quickly. As a team, go over the list of items to potentially improve on and note which ones are the strongest, common denominator.  Pick 1 or 2 of these and this will provide some focus for the team for the next period of work (sprint, cycle, dog year, etc.).  These can be process-oriented intangibles or even technical issues.  Here are some examples: “ Our front end engineers were idle during much of the sprint waiting on services to be updated” – Or – “ We spend 1-2 hours doing process Y manually every week and we should probably automate it ” The first item has to do with improving the team’s process of how it interacts with itself.  The other is a clear, intangible need that can be solved using a technical solution. Once the team has identified 1 or 2 high-valued issues they feel need to be addressed, its time to do just that. This is the most important step.  It’s one thing to identify a problem to be solved, another to actually act on it.  Hold a discussion as to what potential solutions to your issues might be.  The time keeper at this point should be keeping an eye on the clock to help the team keep the meeting moving forward in a productive manner.  At this step, try and keep the following in mind: Once you’ve brainstormed on solutions to how you and your team can make improvements, it’s time to really act.  In this case, you should have some ideas that can be transformed into action items ( you get a Jira ticket and you get a Jira ticket and…).  The retro should conclude with the team being able to break down a solution for improving into 1 or more workable tasks and then assigning them to team members for the next sprint, cycle or galactic year. The key to making this all work is that last part.  What comes out of the retrospective should be treated as any other item of work your team normally commits to in the confines of your regular working period (sprint, release, Mayan lunar month, etc.) with one or more team members responsible for delivering the solution, whether technical or otherwise. Now you’re ready to move forward with improving as a team and dive into that post-donut food coma. Whether you use this process or some other framework to run a retrospective, repeating that process is very important.  To what degree or frequency your team does that is for the team to decide.  Some teams I’ve worked with performed retrospectives after every sprint, others only after major releases and some just whenever the team felt it was necessary.  The key is for the team to decide on that interval and after your initial retrospective, be sure to devote some time at the beginning of subsequent retrospectives to review your action items from the previous session (did you complete them and were the issues related to them resolved?). Hopefully this post has given you an idea how your team can not only perform retrospectives but also improve your team’s working process and even look forward to looking back. And on that note…", "date": "2016-07-29"},
{"website": "BazaarVoice", "title": "Why we do weekly demos", "author": ["Bryan Chagoly"], "link": "https://blog.developer.bazaarvoice.com/2016/06/28/why-we-do-weekly-demos/", "abstract": "If you are part of an agile, or lean, or kanban development team, you probably do or have done demos at one point. Some people call them “end of sprint” demos.  Some people call them “stakeholder” demos. We are pretty informal and irreverent about it at Bazaarvoice, and we just call them “demos” because giving them too formal of a name, or process will defeat the purpose. Demos are an amazingly valuable part of the development process, and I highly recommend that your team start doing them weekly. There are a lot of reasons why not to do demos. If you are doing demos because of any (or all) of these reasons, you are probably doing it wrong. (Oh, did I mention that this is my personal opinionated viewpoint?) Before you can do effective demos that help accelerate your development, your delivery, your quality and your culture, you need to set the stage for success. So how do you do that? At Bazaarvoice, we believe in hiring smart, passionate owners that we can trust to do the right thing for the company, for our customers, and most importantly for our consumers. If everyone is bought into the vision and mission that you are all working to accomplish together, and if everyone is engaged and proactively helping to find creative solutions to those problems, then it is a lot easier to have productive weekly demos. You want to have an environment where everyone can speak without judgment, and where they know that their random idea will be heard and appreciated and not shot down. If everyone is coming from this place of openness, transparency and trust, then you can get some great demos. We believe that demos are part of the creative, collaborative and iterative process of developing amazing software solutions. The point of the demo is it be a time when everyone on the team (and stakeholders) can get together and celebrate incremental progress. It’s a way to bring out all the current work out into the light and to discuss it. What’s good, what still needs work, what did we learn, what do we need to change, and how could we make it even better. It lets us course correct earlier before we fly into the mountain. And It gives others awareness of what’s going on and brings the “aha” moments. It’s about learning, and questioning, and celebrating. Everyone who demos gets applause. All work that gets done is important and should be celebrated. All work and everyone means Engineers, QA, UX, Product Managers, Marketing, Documentation, Sales, Management, and anyone who completed something that helped move the cause forward. I like to do demos at 4pm on Fridays. Ouch right?! Well that’s kind of the point. Doesn’t everyone just want to check out early for the weekend? Nope… Well let’s hope not. If people are leaving early and complaining, then pause and go back and re-read the “Prep Work” section. We actually use that time because it’s the end of the week. It gives us the most time during the week to get things done, and it gives us free space after the meeting if we need to run long for a brainstorming session or to discuss things more deeply.  We have 14 engineers/UX/QA on our team, and we schedule demos for 30 minutes. If we finish early, or not, we usually stick around for beers and games anyway until 6-7pm because we just like hanging out with each other. Imagine that. So isn’t that great? You get to hang out with your friends, show off your accomplishments, and leave work for the week feeling pride in what was accomplished both personally and by the entire team. You are hopefully excited about the future, and probably brainstorming new ideas over the weekend from what you learned from the demos. We do planning for our week on Monday mornings. That part is important too and it’s the ying to the demo’s yang. People get into the office on Monday morning and are fresh and ready to go and hungry to know what they can pick up next, so it’s the perfect time to set the stage for the week. What are our top priorities for the week? What do we absolutely have to get done by the end of the week (aka by demos on Friday afternoon)? Planning is a great venue to ensure everyone is in sync with the vision and Product priorities. Product shares upcoming business epics, UX walks us through new mockups and user findings, Engineering discusses new platform capabilities that we should consider, and QA reminds us of areas we need to harder.  It helps form this continuous cycle of planning and validation.  It’s a bit odd, because we are very Kanban and flow oriented, but we have found that having some timeboxes around planning and demo’ing gives the team a sense of closure and accomplishment. It’s important to occasionally step back and assess the progress.  We have found this sets a really good and sustainable cadence for a productive team. Tell me what part of our story you want to hear next. How do you build a team and culture that enables you to execute on your vision?  Follow me on twitter @bchagoly and @bazaarvoicedev to be the first to read new related posts and to join the conversation.", "date": "2016-06-28"},
{"website": "BazaarVoice", "title": "Injecting Applications onto Third-Party Webpages Made Easy", "author": ["Andrew Terranova"], "link": "https://blog.developer.bazaarvoice.com/2016/07/15/injecting-applications-onto-third-party-webpages-made-easy/", "abstract": "Bazaarvoice’s Small Web App Technologies (SWAT) team is pleased to announce that we are open sourcing swat-proxy – a tool to inject applications onto third-party webpages. In third-party web application development it is difficult to be certain how our applications will look and behave on a client’s webpage until they are implemented. Any number of things could interfere – including other third-party applications! Delivering applications that don’t work can obviously have a severe negative impact on both our clients and us. One solution is to inject – or proxy – our applications onto the client’s web page. This way we can ensure they work correctly – before they go into production. I wrote swat-proxy to do exactly that, acting as a man-in-the-middle between browser and web server. As the browser requests web pages from the server, swat-proxy intercepts the response and proxies our application into it. The browser renders the web page as if it contained our application all along – exactly simulating the client having implemented it. Now we can be certain how our applications will look and behave. Other tools exist to accomplish this task, but none are as front-end developer-friendly as swat-proxy: it is written entirely in Javascript – plugging in nicely to our existing workflows – and uses familiar CSS selectors to target DOM elements when injecting content. It is run locally using NodeJS and is very easy to use. We have found swat-proxy to be incredibly useful when rapidly iterating on prototypes and ensuring the behavior of our applications before they are released to production – we hope you do too! We are releasing it to the larger world as open source, under the Apache 2.0 license. Please download it, try it out, and let us know what you think (in comments below, or as issues or pull requests on Github ).", "date": "2016-07-15"},
{"website": "BazaarVoice", "title": "How to seamlessly move 300 Million shoppers to a highly scalable architecture, part 2", "author": ["Gary Allison"], "link": "https://blog.developer.bazaarvoice.com/2016/06/20/how-to-seamlessly-move-300-million-shoppers-to-a-highly-scalable-architecture-part-2/", "abstract": "As Engineers, we often like nice clean solutions that don’t carry along what we like to call technical debt.  Technical debt literally is stuff that we have to go back to fix/rewrite later or that requires significant ongoing maintenance effort.  In a perfect world, we fire up the the new platform and move all the traffic over.  If you find that perfect world, please send an Uber for me. Add to this the scale of traffic we serve at Bazaarvoice, and it’s obvious it would take time to harden the new system. The secret to how we pulled this off lies in the architecture choices to break apart the challenge into two parts: frontend and backend.  While we reengineered the front-end into the the new javascript solution, there were still thousands of customers using the template-based front end.  So, we took the original server side rendering code and turned it into a service talking to our new Polloi service.  This enabled us to handle request from client sites exactly like the Classic original system. Also, we created a service improved upon the original API but was compatible from a specific version forward.  We chose to not try to be compatible for all version for all time, as all APIs go through evolution and deprecation.  We naturally chose the version that was compatible with the new Javascript front end.  With these choices made, we could independently decide when and how to move clients to the new backend architecture irrespective of the front-end service they were using. A simplified view of this architecture looks like this: With the above in place, we can switch a Javascript client to use the new version of the API through just changing the endpoint of the API key.  For a template-based client, we can change the endpoint to the new referring service through a configuration in our CDN Akamai. Testing for compatibility is a lot of work, though not particularly difficult. API compatibility is pretty straight forward, which testing whether a template page renders correctly is a little more involved especially since those pages can be highly customized.  We found the most effective way to accomplish the later since it was a one time event was with manual inspection to be sure that the pages rendered exactly the same on our QA clusters as they did in the production classic system. Success we found early on was based on moving cohorts of customers together to the new system. At first we would move a few at a time, making absolutely sure the pages rendered correctly, monitoring system performance, and looking for any anomalies.  If we saw a problem, we could move them back quickly through reversing the change in Akamai. At first much of this was also manual, so in parallel, we had to build up tooling to handle the switching of customers, which even included working with Akamai to enhance their API so we could automate changes in the CDN. From moving a few clients at a time, we progressed to moving over 10s of clients at a time. Through a tremendous engineering effort, in parallel we improved the scalability of our ElasticSearch clusters and other systems which allowed us to move 100s of clients at a time, then 500 clients at time. As of this writing, we’ve moved over 5,000 sites and 100% of our display traffic is now being served from our new architecture. More than just serving the same traffic as before, we have been able to move over display traffic for new services like our Curations product that takes in and processes millions of tweets, Instagram posts, and other social media feeds.  That our new architecture could handle without change this additional, large-scale use case is a testimony to innovative engineering and determination by our team over the last 2+ years. Our largest future opportunities are enabled because we’ve successfully been able to realize this architectural transformation. In addition to rearchitecting the service to scale, we also had to rearchitect our team. As we set out on this journey to rebuild our solution into a scalable, cloud based service oriented architecture, we had to reconsider the very way our teams are put together.  We reimagined our team structure to include all the ingredients the team needs to go fast.  This meant a big investment in devops – engineers that focus on new architectures, deployment, monitoring, scalability, and performance in the cloud. A critical part of this was a cultural transformation where the service is completely owned by the team, from understanding the requirements, to code, to automated test, to deployment, to 24×7 operation.  This means building out a complete monitoring and alerting infrastructure and that the on-call duty rotated through all members of the team.  The result is the team becomes 100% aligned around the success of the service and there is no “wall” to throw anything over – the commitment and ownership stays with the team. For this team architecture to succeed, the critical element is to ensure the team has all the skills and team players needed to succeed.  This means platform services to support the team, strong product and program managers, talented QA automation engineers that can build on a common automation platform, gifted technical writers, and of course highly talented developers.  These teams are built to learn fast, build fast, and deploy fast, completely independent of other teams. Supporting the service-oriented teams, a key element is our Platform Infrastructure team we created to provide a common set of cloud services to support all our teams.  Platform Infrastructure is responsible for the virtual private cloud (VPC) supporting the new services running in amazon web services. This team handles the overall concerns of security, network, service discovery, and other common services within the VPC. They also set up a set of best practices, such as ensuring all cloud instances are tagged with the name of the team that started them. To ensure the best practices are followed, the platform infrastructure team created “Beavers” (a play on words describing a engineer at Bazaarvoice, a “BVer”). An idea borrowed from Netflix’s chaos monkeys, these are automated processes that run and examine our cloud environment in real time to ensure the best practices are followed.  For example, the “Conformity Beaver” runs regularly and checks to make sure all instances and buckets are tagged with team names. If it finds one that is not, it infers the owner and emails team aliases of the problem.  If not corrected, Conformity Beaver can terminate the instance.  This is just one example of the many Beavers we have created to help maintain consistency in a world where we have turned teams lose to move as quickly as possible. An additional key common capability created by the Platform Infrastructure team is our Badger monitoring services. Badger enables teams to easily plug in a common healthcheck monitoring capability and can automatically discover nodes as they are started in the cloud. This service enables teams to easily implement these healthcheck that is captured in a common place and escalated through a notification system in the event of a service degradation. The Black Friday and Holiday shopping season of 2015 was one of the smoothest ever in the history of Bazaarvoice while serving record traffic. From Black Friday to Cyber Monday, we saw over 300 million visitors.  At peak on Black Friday, we were seeing over 97,000 requests per second as we served up over 2.6 billion review impressions, a 20% increase over the year before.  There have been years of hard work and innovation that preceded this success and it is a testimony to what our new architecture is capable of delivering. A few ingredients we’ve found to be important to successfully pull off a large scale rearchitecture such as described here: One piece of advice: don’t be too critical of yourself along the way; celebrate each step of the reachitecture journey. As software engineers, we are driven to see things “complete”, wrapped up nice and neat, finished with a pretty bow. When replacing an existing system of significant complexity, this ideal is a trap because in reality you will never be complete.  It has taken us over 3 years of hard work to reach this point, and there are more things we are in the process of moving to newer architectures. Once we complete the things in front of us now, there will be more steps to take since we live in an ever evolving landscape. It is important to remember that we can never truly be complete as there will be new technologies, new architectures that deliver more capabilities to your customers, faster, and at a lower cost. Its a journey. Perhaps that is the reason many companies can never seem to get started. They understandably want to know “When will it be done?” “What is it going to cost?”, and the unpopular answers are of course, never and more than you could imagine.  The solution to this puzzle is to identify and articulate the business value to be delivered as a step in the larger design of a software platform transformation.  Trouble is of course, you may only realistically be able to design the first few steps of your platform rearchitecture, leaving a lot of technical uncertainty ahead. Get comfortable with it and embrace it as a journey.  Engineer solid solutions in a service oriented way with clear interfaces and your customers will be happy never knowing they were switched to the next generation of your service. authored by Gary Allison", "date": "2016-06-20"},
{"website": "BazaarVoice", "title": "How to seamlessly move 300 Million shoppers to a highly scalable architecture, part 1", "author": ["Gary Allison"], "link": "https://blog.developer.bazaarvoice.com/2016/06/10/how-to-seamlessly-move-300-million-shoppers-to-a-highly-scalable-architecture-part-1/", "abstract": "At Bazaarvoice, we’ve pulled off an incredible feat, one that is such an enormous task that I’ve seen other companies hesitate to take on. We’ve learned a lot along the way and I wanted to share some of these experiences and lessons in hopes they may benefit others facing similar decisions. Our original Product Ratings and Review service served us well for many years, though eventually encountered severe scalability challenges. Several aspects we wanted to change: a monolithic Java code base, fragile custom deployment, and server-side rendering. Creative use of tenant partitioning, data sharding and horizontal read scaling of our MySQL/Solr based architecture allowed us to scale well beyond our initial expectations. We’ve documented how we have accomplished this scaling on our developer blog in several past posts if you’d like to understand more. Still, time marches on and our clients have grown significantly in number and content over the years. New use cases have come along since the original design: emphasis on the mobile user and responsive design, accessibility, the emphasis on a growing network of consumer generated content flowing between brands and retailers, and the taking on of new social content that can come in floods from Twitter, Instagram, Facebook, etc. As you can imagine, since the product ratings and reviews in our system are displayed on thousands of retailer and brand websites around the world, the read traffic from review display far outweighs the write traffic from new reviews being created. So, the addition of clusters of Solr servers that are highly optimized for fast queries was a great scalability addition to our solution. A highly simplified diagram of our classic architecture: However, in addition to fast review display when a consumer visited a product page, another challenge started emerging out of our growing network of clients. This network is comprised of Brands like Adidas and Samsung who collect reviews on their websites from consumers who purchased the product and then want to “syndicate” those reviews to a set of retailer ecommerce sites where shoppers can benefit from them. Aside from the challenges of product matching which are very interesting, under the MySQL architecture this could mean the reviews could be copied over and over throughout this network. This approach worked for several years, but it was clear we needed a plan for the future. As we grew, so did the challenge of an expanding volume of data in the master databases to serve across an expanding network of clients. This, together with the need to deliver more front-end web capability to our customers, drove us to what I hope you will find is a fascinating story of rearchitecture. One of the first things we decided to tackle was to start moving analytics and reporting off the existing platform so that we could deliver new insights to our clients showing how reviews are used by shoppers in their purchase decisions. This choice also enabled us to decouple the architecture and spin up parallel teams to speed delivery. To deliver these capabilities, we adopted big data architectures based on Hadoop and HBase to be able to assimilate hundreds of millions of web visits into analytics that would paint the full shopper journey picture for our clients. By running map reduce over the large set of review traffic and purchase data, we are able to give our clients insight into these shopper behaviors and help our clients better understand the return on investment they receive from consumer generated content. As we built out this big data architecture, we also saw the opportunity to offload reporting from the review display engine. Now, all our new reporting and insight efforts are built off this data and we are actively working to move existing reporting functionality to this big data architecture. On the front end, flexibility and mobile was a huge driver in our rearchitecture. Our original template-driven, server-side rendering can provide flexibility, but that ultimate flexibility is only required in a small number of use cases. For the vast majority, a client-side rendering via javascript with behavior that can be configured through a simple UI would yield a better mobile-enabled shopping experience that’s easier for clients to control. We made the call early on not to try to force migration of clients from one front end technology to another. For one thing, it’s not practical for a first version of a product to be 100% feature function capable to the predecessor. For another, there was just simply no reason to make clients choose. Instead, as clients redesigned their sites and as new clients were onboard, they opt’ed in to the new front end technology. We attracted some of the top javascript talent in the country to this ambitious undertaking. There are some very interesting details of the architecture we built that have been described on our developer blog and that are available as open source projects on in our bazaarvoice github organization. Look for the post describing our Scoutfile architecture in March of 2015. The BV team is committed to giving back to the Open Source community and we hope this innovation helps you in your rearchitecure journey. On the backend, we took inspiration from both Google and Netflix. It was clear that we needed to build an elastic, scalable, reliable, cloud-based data store and query layer. We needed to reorganize our engineering team into autonomous service oriented teams that could move faster. We needed to hire and build new skills in new technologies. We needed to be able to roll this out as transparently as possible to our clients while serving live shopping traffic so no one knows its happening at all. Needless to say, we had our work cut out for us. For the foundation of our new architecture, we chose Cassandra, an Open Source NoSQL data solution based on influence of ideas from Google and their BigTable architecture. Cassandra had been battle hardened at Netflix and was a great solution for a cloud resilient, reliable storage engine. On this foundation we built a service we call Emo, originally intended for sentiment analysis. As we made progress towards delivery, we began to understand the full potential of Cassandra and its NoSQL based architecture as our primary display storage. With Emo, we have solved the potential data consistency issues of Cassandra and guarantee ACID database operations. We can also seamlessly replicate and coordinate a consistent view of all the rating and review data across AWS availability zones worldwide, providing a scalable and resilient way to serve billions of shoppers. We can also be selective in the data that replicates for example from the European Union (EU) so that we can provide assurances of privacy for EU based clients. In addition to this consistency capability, Emo provides a databus that allows any Bazaarvoice service to listen for the kinds of changes the service particularly needs, perfect for a new service oriented architecture. For example, a service can listen for the event of a review passing moderation which would mean that it should now be visible to shoppers. While Emo/Cassandra gave us many advantages, its NoSQL query capability is limited to what Cassandra’s key-value paradigm. We learned from our experience with Solr that having a flexible, scalable query layer on top of the master datastore resulted in significant performance advantages for calculating on-demand results of what to display during a shopper visit. This query layer naturally had to provide the distributed advantages to match Emo/Cassandra. We chose ElasticSearch for our architecture and implemented a flexible rules engine we call Polloi to abstract the indexing and aggregation complexities away from engineers on teams that would use this service. Polloi hooks up to the Emo databus and provides near real time visibility to changes flowing into Cassandra. The rest of the monolithic code base was reimplemented into services as part of our service oriented architecture. Since your code is a direct reflection of the team, as we took on this challenge we formed autonomous teams that owned everything full cycle from initial conception to operation in production. We built the teams with all the skills needed for success: product owners, developers, QA engineers, UX designers (for front end), DevOps engineers, and tech writers. We built services that managed the product catalog, UI Configuration, syndication edges, content moderation, review feeds, and many more. We have many of these rearchitected services now in production and serving live traffic. Some examples include services that perform the real time calculation of what Brands are syndicating consumer generated content to which Retailers, services that process client product catalog feeds for 100s of millions of products, new API services, and much more. To make all of the above more interesting, we also created this service-oriented architecture to leverage the full power of Amazon’s AWS cloud. It was clear we had the uncommon opportunity to build the platform from the ground up to run in the cloud with monitoring, elastic resiliency, and security capabilities that were unavailable in previous data center environments. With AWS, we can take advantage of new hardware platforms with a push of a button, create multi datacenter failover capabilities, and use new capabilities like elastic MapReduce to deliver big data analytics to our clients. We build auto-scaling groups that allow our services to automatically add compute capacity as client traffic demands grow. We can do all of this with a highly skilled team that focuses on delivering customer value instead of hardware procurement, configuration, deployment, and maintenance. So now after two plus years of hard work, we have a modern, scalable service-oriented solution that can mirror exactly the original monolithic service. But more importantly, we have a production hardened new platform that we will scale horizontally for the next 10 years of growth. We can now deliver new services much more quickly leveraging the platform investment that we have made and deliver customer value at scale faster than ever before. So how did we actually move 300 million shoppers without them even knowing?  We’ll take a look at this in an upcoming post! authored by Gary Allison", "date": "2016-06-10"},
{"website": "BazaarVoice", "title": "Quick and Easy Web Service Load Testing with JMeter", "author": ["Gary Spillman"], "link": "https://blog.developer.bazaarvoice.com/2016/05/19/quick-and-easy-web-service-load-testing-with-jmeter/", "abstract": "Somewhere between the disciplines of Dev Operations, Database Management, Software Design and Testing, there’s a Venn diagram where at its crunchy, peanut-butter filled center lies the discipline of performance testing. Which is to say, professional performance testers have a very specific set of skills (that they have acquired over many years) that make them a nightmare for non-performing web services. This helps them to answer the following question from our clients: “Just what is the level of performance we can expect from your service”? What if your project team doesn’t have access to anyone with a background in perf testing? What if you suddenly find yourself needing to answer the above question but don’t know where to begin? Scary, right? Bazaarvoice’s Shopper Marketing team recently found itself in this exact situation (they weren’t being hunted by Liam Neeson so they had that going for them though). The point of this article is not to bootstrap you, the reader, into a perf-testing, dev-ops version of the dude from Taken. Instead, it’s to show how a small team can quickly (and cheaply) performance test a web service in order to insure they can meet a client’s needs. If you’ve never been involved in any sort of performance testing for web services before, there are essentially two different tracks performance testing can begin from: Targeted Testing – you have a pre-defined level of service/latency that you need to reach. Generally, you already have an understanding of what your service’s current performance baseline is. Exploratory Testing – The current performance baseline isn’t really known. Here, the goal is to find out at what point and how quickly performance degrades. Typically, with small-team oriented projects, you’ll find often that the team starts with the latter path in order to then progress to the former – as was the case with Shopper Marketing’s efforts here. We have a RESTful web API (built in Java) which handles requests for shopper profile information stored and sorted across multiple types of data stores. This API will service a JavaScipt based front end widget deployed to a client’s home page to display product data. The client’s home page receives approximately 20 simultaneous unique views per second on average. Can our API service the client at that level? To test this, we constructed a load test in JMeter that would do the following: So why are we conducting our test using JMeter? Isn’t that thing two days older than dirt? Well, for one, JMeter is free. We could just leave it at that but wait, there’s more: JMeter is a load testing tool that has been around for many years. It isn’t just developed and maintained by the same group responsible for Apache Web Server, JMeter is a modified version of Apache itself.  It specializes not only in sending and receiving HTTP requests (you know, like a web server) but with monitoring and reporting tools available for it as well as a wealth of plugins. Sure, there are better (cough! more expensive cough!) tools out there that specialize in load testing but in our case, we needed to determine metrics quickly and with a tool that could be easily set up and re-run numerous times (heavy emphasis on quick and cheap). Performance testing is a very repetitive process. You will be executing tests, reporting findings, then modifying your service in order to improve performance – followed by a lot of washing, rinsing and repeating to further refine performance. Whatever load testing tool you choose, make sure it is one that allows you to quickly and easily modify, re-run and re-report findings as you will be living your own form of dev-ops Groundhog Day when you take on this endeavor. But enough memes – lets get down to how we programmed Jmeter to show us pretty performance graphs! You can download JMeter from the Apache Software Foundation here: http://jmeter.apache.org/download_jmeter.cgi Note – Jmeter requires Java 6 or above (you are using Java 8+ right?) and you should have your Java HOME environment variables set up on your local environment (or wherever you plan on deploying and executing your load tests from). Latest Java download: http://java.com/en/download/ Setting up your local Java environment: https://docs.oracle.com/cd/E19182-01/820-7851/inst_cli_jdk_javahome_t/ Once the JMeter binary package is downloaded and unzipped to your test machine, start JMeter by running ./jmeter from the command line within the application’s bin/ directory. Regardless of what load testing tool you prefer to use; its technical merits will always be tied to its reporting capability. JMeter’s default reporting capabilities are pretty limited. However, there are a wealth of plugins to augment this. Before going any further you will want to install JMeter Plugins Extras and JMeter Plugins Extras Lib in order to get the results you’ll want from JMeter. http://jmeter-plugins.org/downloads/all/ Unarchive the contents of these files and place them in the lib/ext directory within your JMeter installation. Once finished, re-start JMeter. Note – you can install, update and manage plugins for JMeter using the JMeter plugin manager. This feature is in Beta so your mileage may vary. More on the JMeter plugin manager here: http://jmeter-plugins.org/wiki/PluginsManager/ For those new to JMeter, setting up a test is rather simple but there’s a little bit of jargon to explain. A basic JMeter test consists of the following: • Test Plan – A collection of all of the elements that make up your load test • Thread Group – Controls the number of threads and their ramp-up/ramp-down time. Think of each thread as a unique visitor to your site/request to your service. • Listeners – These will be attached to your thread group and will generate your reports • Config Elements – These contain a variety of base information required to execute the test – mainly domain, IP or port information related to the service’s location. Optionally, some config elements can be used to handle situations like having to authenticate through LDAP during tests. • Samplers – These elements are used to generate and handle data as well as options and arguments during test (e.g. request payloads and arguments). 1. Click on your test plan and assign it a name and check the Run Teardown option 2. Right click on the test plan and select Add > Threads > Thread Group 3. Enter a name for the thread group (e.g. load test 1) a. Set the number of threads option to the maximum desired number of requests you want to field         to the API per second (simultaneously) b. Set the ramp-up period option to the number of seconds you wish the test to take before it                 reaches the maximum number of threads set above (e.g. setting thread count to 100 and the             ramp-up to 60 will start the test with 1 thread and add an additional thread per second. After 1           minute, the test will be at a maximum of 100 concurrent requests per second). c. Set the Loop option for the number of cycles of maximum requests you wish the test to repeat           once it reaches is maximum number of threads. Once this loop finishes, the test will end. d. Check the forever option if you wish the test to continue to execute at its max thread count                 indefinitely. Note – this will require you to manually shut the test down. 4. Right click on the Thread Group and select Add > Config Element > HTTP Request Defaults 5. Set the Server Name or IP Address (and optionally the Port) fields to the domain/IP/port your             service can be reached at (e.g. http://my.network.bazaarvoice.com ) Now we’re ready to program our test – using the options in the HTTP Request element, we’ll construct what we want each request per each thread to contain. 1. Right click on the thread group and select Add > Sampler > HTTP Request 2. In the HTTP Request config panel, set the implementation to HTTPClient 4 3. Set your protocol (http or https) and your method type (in this place, GET) 4. Set the path option to the endpoint you wish to send your request to – do not include any HTTP         arguments (e.g. /path/sub-path1/sub-path2/endpoint) 5. Next, we’ll configure each and every HTTP argument we need to pass within our request. 6. Do this by clicking into the first line of the send parameters table. 7. Enter your first argument name into the name field, the value into the value field, click the include       equals option and, if need be, click the encode option if your argument value needs to be HTTP         encoded. 8. Click Add and repeat this process for each key-value pair you need to send with your request Now would be a good time to save your test! Next, we need to add listeners (JMeter-speak for report generators) in order to report our findings during and after the load test. Right click on the thread group and select Add > Listeners > and then pick your choice of listener. The choice of test listeners is quite deep, especially if you installed the reporting add ons as noted above. You can configure whatever listeners you feel you need, though here some you may want to add to your test: View Results Tree – This listener will tabulate each request response it receives during the test as well as collect its response type, headers and content. I highly recommend configuring two of these listeners and assigning 1 for successes and 1 for failures. This will help sort your response types, allow you to debug your tests in case of authentication errors, malformed requests or troubleshoot issues if your API should suddenly start returning 500 errors. Response Times Vs Threads – If you’re configuring your test to ramp up its load over time, this listener will provide a chart which you can use to measure the responsiveness of your API over time as the request load is increased. Again, I recommend configuring multiple instances of this listener – one to record requests and another to record error latency if you choose to use this listener. Response Times Over Time – If your test is being configured to provide a constant load over a period of time, this listener can help chart performance of the API based on a steady load over time. It’s helpful in helping to spot issues such as inadequate load balancing or rate limiting of your requests depending on if your service architecture is really aggressive when it comes to that aspect (cough, cough – load balancers – cough). Now would be another great time to save your progress. OK – the moment you’ve been waiting for (be honest). Let’s kick this pig and bask in the glory of our performant API! Click the big, green button to start the test. Note on the upper right hand side of the JMeter UI, you’ll have an indicator showing the number of threads currently running (out of the total max to ramp up to) as well as an indicator of any warnings or errors being thrown). Click on the Results Tree listener to view a table of your responses. If you’re seeing errors, click on an error instance in the results tree to view the error type, body and content. Once you’re ready to stop a test, click the big, red, X icon to shut the test down. You’re probably thinking, “Hey, that was easy and oh look! Our test results are coming in and they look pretty good. There’s got to be more to it than this”. …And you would be right. Remember that comment about load balancers above? Well, in most modern web service architectures, you’ll encounter some form of load balancing whether it’s part of the web server’s features or an intermediary. In our case, Mashery would have cached our static request after a few seconds at maximum load. After that, we weren’t even talking to the API directly, rather, Mashery simply sent us the cached response for the request. Our results in Jmeter may have looked good but it was lying to us. Fortunately, JMeter allows us to inject some form of randomness into our requests to circumvent this issue. One way of accomplishing this is to invoke a randomized ID into your HTTP arguments – especially if your API accepts a random, serialized load ID as an argument. Here’s how you can do that: 1. Right click on the thread group and select Add > Config Elements > Random Variable 2. On the Random Variable config screen, set a value for in the variable name field (e.g.                         my_random_id) 3. Set a minimum and maximum value to define the range your random variable will take (e.g. 1000       and 9999) 4. Set the Per Thread option to true (this will ensure a random value will be set for each thread. 5. Next, we’ll need to click on the HTTP Sampler and include our newly added random variable to         our test. Let’s assume our API accepts an argument called ‘loadId’ which corresponds to a                 random, 5-digit number. 6. In this case, click on the Send Parameters table and add a new key value pair with the name set       to ‘loadId’ and the value set to ‘{$my_random_id}’ (or whatever you’ve named your variable in the       config element screen. One of the requirements of our load test request is that we must provide a specific profile ID that relates to a profile to be returned by the API. For our purposes, we exported a list of existing IDs (over 90,000) from the Cassandra database our API reads from and writes to, imported that into our JMeter test and instructed the HTTP Request sampler to randomly grab an ID and include it as the argument for every individual request. We configured this by doing the following: 1. Right click on the thread group and select Add > Config Element > CSV Data Set Config 2. In the CSV data set config options, set the file name option to the path to your CSV file that                 contains your working data 3. In the variable name field, provide a name for which the test sampler will refer to each instance of       your data as (e.g. myRandomID) 4. Enter ‘,’ into the delimiter option field 5. Set the Recycle on EoF to true, Stop on EoF to false and Sharing Mode to All Threads This last set of options will ensure that if the test cycles through all elements in your CSV (which it will use for each and every thread) it will simply start back at the top of the list. Next, click on your HTTP Sampler. Here you will need to add a bash script style variable to the sampler in order for it to automatically pull the shared data variable from your CSV config element (e.g. if you named your variable in the CSV config element to “myRandomID” you need to inject the value {$myRandomID} into the listener somewhere. This will depend on the nature of your API. In our case, we simply appended this to our API endpoint, setting the ID variable to be called between the API domain/endpoint call and the HTTP arguments in the URI. Yup – good time to save your game – I mean test. After that… We’ve gone over how to build and run a performance test but once the test has concluded and you have gathered results, you need to understand what you’re looking at. To view the results of a particular listener, just click on it in the JMeter UI. The Results Tree reports are self-explanatory but what about the other reports? In particular, lets look at the Threads Over Time listener. Here is the graph output for this listener from our initial performance test: This listener was configured to only measure time per successful request in order to obtain more focused results. In the graph you can see that over time, there was a great deal of variance with the majority of requests taking around 1.6 seconds to resolve. Note the highest and lowest points on the graph – these are the outlining deviations for test results as opposed to the concentrated area of red (the average time per request). Generally speaking, the tighter the graph, the more consistent the API’s performance and of course, the lower the average number, the faster the performance. Large spikes with pronounced peaks and valleys usually indicate there is an issue with the service’s load balancing features or something “mechanical” getting in the way of the test. Long periods of plateauing are another indicator to watch for. These may indicate some form of rate limiting or timeout. Now you’re ready to send off your newly minted beast of a load test to go show that MCP who’s boss . Before you go and press that button – some advice. JMeter is a great tool and all – especially for the price you pay but it is old and not necessarily the most high-performance perf testing tool out there (oh the irony). When launching tests off your local machine or a server – keep in mind that each thread you configure for your test will be another thread your CPU will need to handle. You can quickly and easily create a test that will test your local environments to its limit. Doing so can, at times, crash you test (and dump your results into the ether – engage sad panda face). Start with a small-to-medium performance load and build up from there. When stopping a test, manually or automatically, you might notice a sudden uptick in errors and latency at the very end of the test (and possibly at the beginning as well). This is normal behavior – when a test is started and stopped you can experience some level of thread abandonment (which JMeter will record as an error because those last requests never receive proper responses). These errors can be ignored when viewing results. Basically, the test results are kind of like a loaf of bread – no one wants the ends. JMeter is basically a multi-threaded web request generator and manager. The traffic patterns it generates can resemble those seen during a DoS attack – especially for very large tests. If there are any internal or external web security policies in place within the network you’re testing, be careful as to not set these off (i.e. just because you can run a test on a production server that sends 400,000 simultaneous requests to a google web service – which then gets your whole office IP range banned from said service – doesn’t mean you should and no, the author of this piece has absolutely no knowledge of any similar event ever happening, ever…). The above performance graph was from the very first performance test against our internal Shopper Marketing recommendations API. Utilizing the test, its results and monitoring tools like DataDog we were able to find where we needed to improve our service from both the code base as well as hosting environment to reach our performance goal. After several repeated tests along with re-provisioning new Elastic Search clusters and a lot of code refactoring, we eventually arrived at the following test result: From and average response rate of 1.6 seconds to 100 milliseconds is a pretty big leap in performance. Ultimately, our client was pretty happy with our answer. This is by no means an exhaustive method of load testing but merely a way of doing quick and easy exploratory testing that delivered a good deal of value for our team. Have fun testing!", "date": "2016-05-19"},
{"website": "BazaarVoice", "title": "Conversations API Deprecation for Versions 5.2 and 5.3", "author": ["John Banning"], "link": "https://blog.developer.bazaarvoice.com/2016/04/27/conversations-api-deprecation-for-versions-5-2-and-5-3/", "abstract": "Today we are announcing an important change to our Conversations API service: By deprecating older versions of our API service, we can refocus our energies on the current and future API services, which we feel offer the most benefits to our customers. Please visit our Upgrade Guide to learn more about the Conversations API, our API versioning, and the steps necessary to support the upgrade. We understand that this change will require effort on your part. Bazaarvoice is committed to making this transition easy for you. We are prepared to assist you in a number of ways: In summary, on April 30, 2017, Conversations API versions released before 5.4 will no longer be available. Applications and websites using versions before 5.4 will no longer function properly after April 30, 2017. If your custom application or website is making API calls to Conversations API versions 5.2 or 5.3 you will need to upgrade to the current Conversations API (5.4). Applications using Conversations API versions 5.4 and later will continue to receive uninterrupted API service. If you have any questions about this notice, please submit a case in Spark . We will periodically update this blog and our developer Twitter feed ( @BazaarvoiceDev ) as we move closer to the change of service date. Visit the following page Coversations API 2017 Deprecation to learn more. Thank you for your partnership, Chris Kauffman Manager, Product Management", "date": "2016-04-27"},
{"website": "BazaarVoice", "title": "What does a data scientist do?", "author": ["Matthew McClain"], "link": "https://blog.developer.bazaarvoice.com/2016/03/28/what-does-a-data-scientist-do/", "abstract": "More and more companies and industries are grappling with the challenges of extracting value from large amounts of data. Data scientists, the people whose job it is to overcome these challenges, are becoming more prominent, yet what it is they do, and how they’re different than software engineers, is still a mystery to a lot of people. The goal of this article is to explain one of the most important tools that data scientists use: machine learning (ML). The bottom-line is: using ML is slow, costly, and error-prone, but it allows companies to achieve business objectives that are unattainable any other way. Just like a software engineer, the goal of a data scientist is to develop programs that perform business functions. In software engineering, the engineer writes a program that encodes all of the “rules” for what the program is supposed to do, based on the requirements. For example, take the task of returning all of the product reviews for a given product ID. The rules here include things like how to make sure the product ID is valid (and what to do when it’s not), how to query a database of reviews with the product ID, and how to format the reviews to be returned. A reasonably skilled software engineer can easily write a program that encodes all of these rules. However, there are many problems where it is not feasible for anyone to write down all of the rules required for a software program to perform some task. Sometimes this is because the rules are simply not known, and other times it’s because there are way too many rules. Several good example of the latter type come from natural language processing (NLP), like the problem of predicting the sentiment of movie reviews (i.e. did the reviewer like the movie or not?). Like nearly all NLP problems, this is something that a human could do reasonably well, but it doesn’t mean that a person could easily write down a set of rules for how they made their decisions. If you had to, you’d probably start by listing key words or phrases that would indicate the reviewer’s sentiment, like “great”, “liked”, “smash hit”, “boring” and “terrible”, and use their appearance in a review to judge the sentiment. This will only work so far, because language is much more complex than that. You’d need additional rules to get around the fact that many of the key words can mean different things in different contexts, more rules to cover all of they ways that you can express sentiment without using a key word, and even more rules to detect when sentiment gets flipped by a negating phrase. Add to this the fact that people don’t actually write in perfect English, and you’d end up with a seemingly endless list of rules and an impossible software engineering problem. ML has completely different approach: instead of writing out the rules in a top-down fashion, ML attempts to infer the rules in a bottom-up way, from data. For the problem of predicting the sentiment of movie reviews, you’d take a set of movie reviews with the actual sentiment of each, and feed them into a ML program. This ML program then literally outputs another program that takes in a movie review and outputs a prediction of its sentiment (with some expected accuracy). One reason that ML can work (not perfectly) on NLP problems is because where a human would have a very hard time creating millions and millions of rules, a computer has no such limitation. Of course, there are a few catches to using ML. First, the data is not always cheap. Sentiment analysis of movie reviews has been popular topic only because many movie reviews online come with ratings (usually on a scale of 1 to 5), which tell you the answer — the term for this in ML is “ground truth”. For many problems, the ground truth is not readily available and can be very costly to figure it out. A recent Kaggle competition on this topic used a dataset of 50,000 movie reviews — imagine needing to read every single one of these to determine the ground truth sentiment. Another catch, which I’ve already mentioned, is that ML will rarely produce a program with perfect accuracy. This is because for most real-world problems, it’s impossible to provide the ML program with all of the relevant information. For NLP problems, humans have a wealth of knowledge about what words mean, while computers do not. Many other real-world problems involve predicting human behavior, but it’s impossible to observe everything that’s going on in people’s heads. While ML algorithms are designed to make the most out of limited information, they’re only viable as a solution when the business objectives can tolerate some amount of error. ML solutions are also slow to develop, even more so than standard software engineering solutions. As mentioned earlier, ML solutions need to work with limited information, which means that it’s impossible to know whether ML will meet the business’s requirements beforehand. Effective data scientists will make educated guesses about the data, ML algorithms, and algorithm parameters that are most likely to succeed based on the problem, but experimentation is always required. This can mean multiple iterations refining the data, algorithms, and parameters before a definitive answer can be reached about whether an ML solution will work. Last, ML solutions in production are costly to maintain. Their performance needs to be continuously monitored, because their performance can change over time as the characteristics of the data they’re analyzing changes. In the case of predicting the sentiment of movie reviews, just changes in writing style could drop the accuracy significantly. Processes and instrumentation are required to evaluate a statistically significant sample of the solution’s predictions, and take actions to improve performance whenever it drops such as creating a new program using newer data. I hope this de-mystifies some of what data scientists do, and explains one of the important tools they use to extract value from large amounts of data.", "date": "2016-03-28"},
{"website": "BazaarVoice", "title": "Holiday season preparation", "author": ["John Banning"], "link": "https://blog.developer.bazaarvoice.com/2015/12/24/holiday-season-preparation/", "abstract": "Preparing for the Holiday season is a year round task for all of us here at Bazaarvoice.  This year we saw many retailers extending their seasonal in-store specials to their websites as well. We also saw retailers going as far as closing physical stores on Thanksgiving (Nordstrom, Costco, Home Depot, etc.) and Black Friday (REI).  Regardless of which of the above strategies were taken,  the one common theme amongst retailers  was the increase in online sales. This trend is not new. Online sales are catching up to in stores sales ( http://www.usnews.com/news/business/articles/2015/11/28/black-friday-store-sales-fall-as-americans-buy-more-online ) over the holiday season.  Along with the demand in online sales was the increase in demand on the Bazaarvoice network. So here are just a few of the metrics that the Bazaarvoice network saw in the busiest week of online shopping: As soon as the online traffic settles from the peak levels, the R&D team begins preparing for the next  year’s Holiday Season.  First by looking back at the numbers and how we did as a team through various retrospectives. Taking inventory of what went well and what we can improve upon for the next year. Before you know it the team gets together in June to being preparations for the next years efforts. I want to touch on just a few of the key areas the team focused on this past year to prepare for a successful Holiday Season: One key improvement this year was client communication both between R&D and other internal teams as well as externally to clients. This was identified as an area we could improve from last year.  Internally a response communication plan was developed. This plan makes sure that key representatives in R&D and support teams were on call at all times and everyone understands escalation paths and procedures should an issue occur. It was then the responsibility of the  on call representative to communicate any needs with the different engineers and client support staff. The on call period lasted from November 24th to Tuesday December the 1st. A small focused team was identified for creation and delivery of all client communication.  As early as August, “Holiday Preparedness” communications were delivered to clients informing them of our service level objectives. Monthly client communications followed containing load target calculations, freeze plans, disaster recover preparations, as well as instructions on how to contact Bazaarvoice in the event of an issue as well as how we would communicate current status of our network during this critical holiday season. Internally there was also an increased emphasis on the creation and evaluation of runbooks. Runbooks are ‘play by play’ instructions which engineers should carry out for different scenarios. The collection of procedures and operations were vital in the teams disaster recovery planning. To improve our operational excellence, we needed to ensure our teams were conducting exploratory disaster scenario testing to know for certain how our apps/service behaved and improve our Dev Ops code, monitoring/alerting, runbooks, etc.  Documenting the procedures was completed in the late summer.  That quickly moved into evaluating our assumptions and correcting where necessary. All teams were responsible for: Sign off was required for all test plans and results shared amongst the teams.  We also executed a full set of Disaster Recovery scenarios and performed additional Green Flag fire drills to ensure all systems and personnel were prepared for any contingencies during the holiday season. For an added layer of insurance, we pre scaled our environment ahead of the anticipated holiday load profile.  Analysis of 3 years of previous holiday traffic showed a predictable increase of approximately 2.5x the highest load average over the past 10 months. For this holiday season we tested at 4x the highest load average over that time period to ensure we were covered. The load test allowed the team to test beyond expected target traffic profile to ensure all systems would execute above expected levels. Load testing initially was isolated per each system.  Conducting tests in such environment helped quickly identify any failure points. As satisfactory results were obtain, complexities were introduced by running systems in tandem. This simulated a environments more representative of what would be encountered in the holiday season. One benefit experienced through this testing was the identification and evaluation of other key metrics to ensure the systems are operating and performing successfully. Also, a predictive model was created to evaluate our expected results.  The accuracy of the daily model was within 5% of the expected daily results and overall, for the 2015 season, was within 3%. This new model will be a essential tool when preparing for the next holiday season. Once again, we locked down the code prior to the holiday season. Limiting the number of ‘moving parts’ and throughly testing the code in place increased our confidence that we would not experience any major issues.  As the image below demonstrates, two critical time periods were identified: As you can see the critical times coincide with the times we see increased online traffic. A substantial amount of the work was all completed in the months prior to Black Friday and Cyber Monday. The team’s coordinated efforts prior to the holiday season ensured that our client’s online operations ran smoothly.  Over half of the year was spent ensuring performance and scalability for these critical times in the holiday season.  Data, as far back as three years, was also used to predict web traffic forecasts and ensure we would scale appropriately. This metric perspective also provided new insightful models to be used in future year’s forecasts. The preparation paid off, and Bazaarvoice was able to handle 8.398 Billion impressions over Black Friday thru Cyber Monday (11/27-11/30), a new record for the our network.", "date": "2015-12-24"},
{"website": "BazaarVoice", "title": "BVIO 2015 Summary and Presentations", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2015/09/04/bv-io-2015/", "abstract": "Every year Bazaarvoice R&D throws BVIO, an internal technical conference followed by a two-day hackathon. These conferences are an opportunity for us to focus on unlocking the power of our network, data, APIs, and platforms as well as have some fun in the process. We invite keynote speakers from within BV, from companies who use our data in inspiring ways, and from companies who are successfully using big data to solve cool problems. After a full day of learning we engage in an intense, two-day hackathon to create new applications, visualizations, and insights into our extensive our data. Continue reading for pictures of the event and videos of the presentations. This year we held the conference at the palatial Omni Barton Creek Resort in one of their well-appointed ballrooms. Participants arrived around 9am (some of us a little later). After breakfast, provided by Bazaarvoice, we got started with the speakers followed by lunch, also provided by Bazaarvoice, followed by more speakers. After the speakers came a “pitchfest” during which our Product team presented hackathon ideas and participants started forming teams and brainstorming. Finally it was time for 48 hours of hacking, eating, and gaming (not necessarily in that order) culminating in project presentations and prizes. Venkat Gopalan Director of Architecture & Devops @ Sephora.com Venkat presented on the work Sephora is doing around serving relevant, targeted content to their consumers in both the mobile and in-store space. It was a fascinating speech and we love to see our how our clients are innovating with us. Unfortunately due to technical difficulties we don’t have a recording 🙁 John Roesler & Fahd Siddiqui Bazaarvoice Engineers This talk was about the overarching design of Bazaarvoice’s innovative data architecture. According to them there are aspects to it that may seem unexpected at first glance (especially not coming from a big data background), but are actually surprisingly powerful. The first innovation is the separation of storage and query, and the second is choosing a knowledge-base-inspired data model. By making these two choices, we guarantee that our data infrastructure will be robust and durable. Ian Clarke Co-Founder and CTO at OneSpot Ian has built and manages a team of world-class software engineers as well as data scientists at OneSpot™s. In his presentation he discusses how he applied machine learning and game theory to architect a sophisticated realtime bidding engine for OneSpot™ capable of predicting the behavior of tens of thousands of people per second. Jeff Nun Amazon Solutions Architect In his presentation Jeff discusses the history of Amazon Machine Learning and the Lambda architecture, how Amazon uses it and you can use it. This isn’t just a presentation; Ian walks us through the AWS UI for building and training a model. Thanks to Sharon Hasting, Dan Heberden, and the presenters for contributing to this post.", "date": "2015-09-04"},
{"website": "BazaarVoice", "title": "Partner Integrations: Do’s and Don’ts", "author": ["David Gee", "David Gee"], "link": "https://blog.developer.bazaarvoice.com/2015/05/22/partner-integrations-dos-and-donts/", "abstract": "In this blog post David Gee , a Senior Product Manager on our Product team, discusses challenges to building and maintaining technical partnerships between organizations as well as provides advice on how to overcome those challenges. Every company comes to a point, early or late, where it realizes that it must partner with other companies to drive value in the market.  Partnerships always start with conversations, handshakes, and NDAs.  At some point, unlocking the value of partnership may hinge upon establishing a formal integration between the two companies.  These integrations constitute a technical “bridge” between companies.  They can unlock otherwise inaccessible value, allow for one company to OEM the other, and/or can accelerate work that otherwise is “re-invented” each time the companies engage each other. Integrations can be amazing vehicles to create value that only comes from combining capabilities from separate entities, while simultaneously allowing each entity to focus on what each one does best.  They can be the perfect manifestation of the all too often promised “complimentary” value.  Integrations can offer consistency, repeatability, and reduced friction in the activities involved in unlocking that value. Unfortunately, integrations are often approached in manner in which all parties involved are not setup for success.  Why? Integrations aren’t just some “code.” They are product. They require an organized effort to build, including technical staff and non-technical staff to build (engineers, architects, project manager, product manager, partnership manager).  They require support, assigned owners, subject matter experts, marketing, documentation, and proper roadmap vision.  Integrations demand the same attention and focus that any first class “product” requires. Integrations require both more and different types of communication. Because the value of the integration is typically not front-and-center to the core value of each org, there must be additional effort to communicate the existence of the integration and the value it brings within each org.  Sales, onboarding support, post-live support organizations all need ways to communicate with the other integrated party (who calls who when something stops working?).  The two product organizations must communicate ahead of any changes to dependent technologies such as APIs.  A classic communication gap happens when one entity changes their APIs and doesn’t let the other party know soon enough or not at all.  Problems are only discovered when something breaks. Integrations are usually birthed by the wrong part of the org. The challenge with integrations is that the impetus to create them usually originates from one or both of the company’s business development/partnerships team – a group that typically has little appreciation for the discipline of product management.  Their priority is on “relationships” that historically focus on non-technical efforts. Additionally, the ADD-like attention span of most partnerships teams results in a great desire to create an “integration” with a partner for marketing and sales-driven reasons, but very little attention, effort, and commitment to the long-term requirements of a properly supported product.  It is quite easy to just stop communicating with a partner who is no longer deemed valuable, but such an about-face cannot be made when an integration is in place with paying customers.  Most often, partnerships orgs do not have technical resources within their structure, but rather rely on borrowing technical resources from wherever they can be found (“Hey, I have a partner company who just trying to do this thing, and they have a quick technical question…”).  This is a great approach for proof-of-concept initiatives, but certainly not for something that companies/customers must trust to deliver value.  The product organizations at each company must be responsible for bringing an integration to life. Regardless of whether the product org has enough resources to service an integration like a first-class product citizen, at least the owner will have an understanding of what is and isn’t getting handled properly and can mitigate the potentially negative outcomes that arise from under-served products. Correctly structured incentives are crucial to the short and long-term success of integrations. There must be something in it for all concerned parties.  Direct compensation and rev share are two good options.  You should be cautious of such benefits as “stickiness” (as in, the assumption that giving an integration free-of-charge to an existing customer makes that customer less likely to debook your core service) or the halo effect associated with integrating with a company (i.e. “Do you know we’re integrated with Facebook?”).  Many integrations have been built on the promise of return.  Once that promise begins to fade (from any one or more of the parties), so does the motivation of the affected party to keep up their end of the technical bargain.  The technology world’s version of “he’s just not that into you (anymore).”  Once an integration is no longer properly attended to from one party, the integration becomes a liability.  It’s not enough for the bridge to be secured to just one side of the river. People love to build stuff.  But, they hate to support it. There must be something in it for the platform to properly prioritize integration maintenance efforts.  Be wary of agreements that lack commitments, SLAs, etc. (often termed that they will do any needed work within the bounds of “best efforts”) as these agreements allow the company responsible for the integration (“code”) to elect to not invest in the support and roadmap development, should their interest wane.  If the agreement lacks commitments, then the partnership will likely as well.  They will acknowledge the maintenance effort, but it will always get pushed to the next dev cycle.  Which leads us to… The Challenge of Opportunity Cost The assumption here is that these companies contemplating an integration are predominantly product organizations.  Their company mandate is to bring products to market at scale.  This is dramatically different than a service organization who essentially trades dollars for hours. This means that the cost of the technical/engineering effort at a product organization is different than that of a service organization.  Not in that engineers get paid more at product organizations, but rather the additional opportunity cost of engineering effort at a product organization often introduces an impossibly high hurdle rate for putting those engineers on non-core “integration work.”    Even just the existence of opportunity cost, albeit uncalculated, is all that a dissenting product or engineering leader needs to de-prioritize seemingly less important “integration work” that doesn’t deliver core value. One innovative approach to solve this dilemma is to use outsourced engineering resources from a service organization to avoid the challenges that comes with opportunity cost.  It makes good business sense: let your in-house engineering staff concentrate on doing things that drive core value at scale.  The downside of this approach is that there is a very clear and visible cost (hrs * hourly rate) that is attached to all effort associated with the integration.  A similar cost analysis is rarely thought about when utilizing internal resources, so the integration product manager should be prepared.  Getting things done is always more expensive than you thought. Of course, another solution is to simply make integration work of the same perceived class of value as that of the core product org’s core solution.  However, as we describe above, this can be a big challenge. The technical approach must be at the convergence of correctly structured incentives and technical viability. How open or closed a platform is can dictate how an integration can be executed.  The associated partnership incentive structure can dictate how an integration should be executed.  The resulting integration will result from the intersection of these two perspectives. Closed platforms force the work on that platform.  Open platforms allow for more options – either or both entities, possibly even a third-party, can contribute to the integration development. Let’s look at a few scenarios. Scenario 1: B is a “closed” platform “Closed” here means that the platform does not allow for integration (read: code) to be hosted by that platform and that the platform does not have externally accessible APIs to utilize from outside the platform.  The closed platform may have internally accessible APIs, but those do an external party little good. Closed platforms force that platform to do the integration work.  Thus, there must be incentives for the closed platform to both build and support the integration long-term.  The effort to build the integration is often simply the result of the opportunistic convergence of both parties being sold on (at least) the promise of value and some available engineering capacity within the close platform.  Without the proper incentives for platform B, this becomes a classic example of the issue of the Challenge of Opportunity Cost, discussed above.  The engineer who had some free time to build the integration is suddenly no longer available to fix a bug or build a new feature.  There must be motivation in some form to continue to maintain the integrity of the integration. Scenario 2: B is open Open platforms present more options. In scenario 2, B is no longer the only entity who can develop the integration.  A, B, or a third-party org can build the integration.  There are more alternative incentive structures as well.  Since the engineering effort can be executed by a non-B entity, there doesn’t need to be much in it for B (there can be, but it is not near the necessity).  There will certainly need to be knowledge of the B platform (documentation, sandboxes, API keys, deployment directions, etc.) on the part of the developing entity, but this effort on the part of B has a much lower hurdle rate than that which is required to get something into B’s engineering roadmap.  Typically, B will have some form of partner “program” by where such assets and knowledge are available for a predetermined fee.  Even in absence of such a program, the needs are significantly less than if the development effort required engineers from platform B to do the build work. Scenario 3: Middle-ware Solution Scenario 3 is just a derivative of Scenario 2.  Options are abundant.  A, B, or a third-party can build the integration.  In most cases, any of those entities can bring the integration to market.  A major decision will be how and where to host the middle-ware solution and how to provide production-ready support, specifically beyond the initial build phase (which can just leverage cloud hosting services like Amazon, etc. to quickly get up and running).  The trade-off is that such a middle-man solution removes any challenges that come with the need to host the integration within the B platform, which can range from simple plug-n-play effort to per-instance customizations required for each integration incarnation. Incentive options are very similar to Scenario 2.  One exception is that there is a clear opportunity for a third-party to bring the integration to market with an associated price tag. Summary Integrations are powerful and often hugely valuable, but their success is directly tied the ability to structure them for the long-term. Integrations are a special kind of “product” requiring different types of communication and can benefit from the use of outsourced resources to execute and maintain. A successful integration is the result of a technical and non-technical relationship that is structured in a way that provides benefit to both parties that can adequately compensate for the often underestimated level of involvement required across both organizations.", "date": "2015-05-22"},
{"website": "BazaarVoice", "title": "Automating a Git Rebase Workflow", "author": ["Lon Ingram"], "link": "https://blog.developer.bazaarvoice.com/2015/05/01/automating-a-git-rebase-workflow/", "abstract": "When I started on the Firebird team at Bazaarvoice, I was happy to learn that they host their code on GitHub and review and land changes via pull requests. I was less happy to learn that they merged pull requests with the big green button . I was able to convince the team to try out a new, rebase-oriented, workflow that keeps the mainline branch linear and clean. While the new workflow was a hit with the team, it was much more complicated than just clicking a button, so I automated the workflow with a simple git extension, git land , which we have released as an open source tool. The big green button is the “Merge pull request” button that GitHub provides to merge pull requests. Clicking it prompts the user to enter a commit message (or accept a default provided by GitHub) and then confirm the merge. When the user confirms the merge, the pull request branch is merged using the –no-ff option, which always creates a merge commit. Finally, GitHub closes the pull request. For example, given a master branch like this: An example master branch …and a feature branch that diverges from the second commit: A feature branch started from the second commit …this is the result of doing a –no-ff merge: The result of merging the examples with the –no-ff option. Note that the third commit on master is interleaved with the merge commit and the feature branch commits. Merging with the big green button is frowned upon by many; for detailed discussions of why this is, see Isaac Z. Schlueter and Benjamin Sandofsky . In addition to the problems with merge commits that Isaac and Benjamin point out, the big green button has another downside: it merges the pull request without an opportunity to squash commits or otherwise clean up the branch. This causes a couple of problems. First, because only the pull request author can clean up the PR branch, merging often became a tedious and drawn out process as reviewers cajoled the author to update their branch to a state that would keep `master`’s history relatively clean. Worse, sometimes messy pull requests were hastily or mistakenly merged. As a result, the team was encouraged to keep their pull requests squashed into one or two clean commits at all times. This solved one problem, but introduced another: when an author responds to comments by pushing up a new version of the pull request, the latest changes are squashed together into one or two commits. As a result, reviewers had to hunt through the entire diff to ensure that their comments were fully addressed. After some lively discussion, the team adopted a new workflow centered on fast-forward merging squashed and rebased pull request branches. Developers create topic branches and pull requests as before, but when updating their pull request, they never squash commits. This preserves detailed history of the changes the author makes in response to review feedback. When the PR is ready to be merged, the merger interactively rebases it on the latest master, squashes it down to one or two commits, and does a fast-forward merge. The result is a clean, linear, and atomic history for `master`. The result of merging the example feature branch into master using the described workflow. One hiccup is that GitHub can’t easily tell that the rebased and squashed commit contains the changes in the pull request, so it doesn’t close the PR automatically. Fortunately, GitHub will close pull requests that contain special keywords . So, the merger has a final task: adding “[closes #<PR number>]” to one of the squashed commit’s message. The biggest downside to the new workflow is that it transformed merging a PR from a simple operation (pushing a button) to a somewhat tricky multi-step process: This process was too lengthy and error-prone to be reliable unless automated. To address this problem, I created a simple git extension: git-land . The Firebird team has been using this tool for a little over a year with very few problems. In fact, it has spread to other teams at Bazaarvoice. We are excited to release it as an open source tool for the public to use.", "date": "2015-05-01"},
{"website": "BazaarVoice", "title": "Front End Application Testing with Image Recognition", "author": ["Gary Spillman"], "link": "https://blog.developer.bazaarvoice.com/2015/04/24/front-end-application-testing-with-image-recognition/", "abstract": "One of the many challenges of software testing has always been cross-browser testing. Despite the web’s overall move to more standards compliant browser platforms, we still struggle with the fact that sometimes certain CSS values or certain JavaScript operations don’t translate well in some browsers (cough, cough IE 8). In this post, I’m going to show how the Curations team has upgraded their existing automation tools to allow for us to automate spot checking the visual display of the Curations front end across multiple browsers in order to save us time while helping to build a better product for our clients. The Curations front end is a highly configurable product that allows our clients to implement the display of moderated UGC made available through the API from a Curations instance. This flexibility combined with BV’s browser support guidelines means there are a very large number ways Curations content can be rendered on the web. Initially, rather than attempt to test ‘all the things’, we’ve codified a set of possible configurations that represent general usage patterns of how Curations is implemented. Functionally, we can test that content can be retrieved and displayed however, when it comes whether that the end result has the right look-n-feel in Chrome, Firefox and other browsers, our testing of this is largely manual (and time consuming). How can we better automate this process without sacrificing consistency or stability in testing? Sikuli is an open-source Java-based application and API that allows users to automate web, mobile and OS applications across multiple platforms using image recognition.   It’s platform based and not browser specific, so it enables us to circumvent limitations with screen capture and compare features in other automation tools like Webdriver. Imagine writing a test script that starts with clicking the home button within an iOS simulator, simply by providing the script a .png of the home button itself. That’s what Sikuli can do. You can read more about Sikuli here . You can check out their project here on github . Sikuli provides two different products for your automation needs – their stand-alone scripting engine and their API. For our purposes, we’re interested in the Sikuli API with the goal to implement it within our existing Saladhands test framework, which uses both Webdriver and Cucumber. Assuming you have Java 1.6 or greater installed on your workstation, from Sikuli.org’s download page, follow the link to their standalone setup JAR http://www.sikuli.org/download.html Download the JAR file and place it in your local workstation’s home directory, then open it. Here, you’ll be prompted by the installer to select an installation type. Select option 3 if wish to use Sikuli in your Java or Jython project as well as have access to its command line options. Select option 4 if you only plan on using Sikuli within the scope of your Java or Jython project. Once the installation is complete, you should have a sikuli.jar file in your working directory. You will want to add this to your collection of external JARs for your installed JRE. For example, if you’re using Eclipse, go to Preferences > Java > Installed JREs , select your JRE version, click Edit and add Sikuli.jar to the collection. Alternately, if you are using Maven to build your project, you can add Sikuli’s API to your project by adding the following to your POM.XML file: Clean then build your project and now you’re ready to roll. Ultimately, we wanted a method we could control using Cucumber that allows us to articulate a web application using Webdriver that could take a screen shot of a web application (in this case, an instance of Curations) and compare it to a static screen shot of specific web elements (e.g. Ratings and Review stars within the Curations display). This test method would then make an assumption that either we could find a match to the static screen element within the live web application or have TestNG throw an exception (test failure) if no match could be found. First, now that we have the ability to use Sikuli, we created a new helper class that instantiates an object from their API so we can compare screen output. Once we import the Sikuli API, we create a simple class with a single class method. In this case, screenMatch is going to accept a path within the Java project relative to a static image we are going to compare against the live browser window. True or false will be returned depending on if we have a match or not. The main object type Sikuli wants to handle everything with is ScreenRegion. In this case, we are instantiating a new screen region relative to the entire desktop screen area of whatever OS our project will run on. Without passing any arguments to DesktopScreenRegion(), we will be defining the region’s dimension as the entire viewable area of our screen. Sikuli allows you to define a fuzzing factor (if you’ve ever used ImageMagick, this should be a familiar concept). Essentially, rather than defining a 1:1 exact match, you can define a minimal acceptable percentage you wish your screen comparison to match. For Sikuli, you can define this within a range from 0.1 to 1 (ie 10% match up to 100% match). Here we are defining a default minimum match (or fuzz factor) of 90%. Additionally, we load in from a set of properties in Saladhand’s test.properties file a value which, if present can override the default 90% match – should we wish to increase or decrease the severity of test criteria. Now that we know what fuzzing percentage we want to test with, we use target’s setMinScore method to set that property. This is where the magic happens. We create a new screen region called found. We then define that using fullScreen’s find method, providing the path to the image file we will use as comparison (target). What happens here is that Sikuli will take the provided image (target) and attempt to locate any instance within the current visible screen that matches target, within the lower bound of the fuzzing percentage we set and up to a full, 100% match. The find method either returns a new screen region object, or returns nothing.   Thus, if we are unable to find a match to the file relative to target, found will remain undefined (null). So in this case, we simply return false if found is null (no match) or true of found is assigned a new screen region (we had a match). To completely incorporate this behavior into our test framework, we write a simple cucumber step definition that allows us to call our Sikuli helper method, and provide a local image file as an argument for which to compare it against the current, active screen. Here’s what the cucumber step looks like: We’re referring to the image file via regex. The step definition makes an assertion using TestNG that the value returned from our instance of SikuliHelper’s screen match method is true (Success!!!). If not, TestNG throws an exception and our test will be marked as having failed. Finally, since we already have cucumber steps that let us invoke and direct Webdriver to a live site, we can write a test that looks like the following: In this case, the image we are attempting to find is a portion of the nav element on BV’s home page: This is not a full-stop solution to cross browser UI testing. Instead, we want to use Sikuli and tools like it to reduce overall manual testing as much as possible (as reasonably as possible) by giving the option to pre-warn product development teams of UI discrepancies. This can help us make better decisions on how to organize and allocate testing resources – manual and otherwise. There are caveats to using Sikuli. The most explicit caveat is that tests designed with it cannot run heedlessly – the test tool requires a real, actual screen to capture and manipulate. Obviously, the other possible drawback is the required maintenance of local image files you will need to check into your automation project as test artifacts. How deep you will be able to go with this type of testing may be tempered by how large of a file collection you will be able to reasonably maintain or deploy. Despite that, Sikuli seems to have a large number of powerful features, not limited to being able to provide some level of mobile device testing. Check out the project repository and documentation to see how you might be able to incorporate similar automation code into your project today.", "date": "2015-04-24"},
{"website": "BazaarVoice", "title": "Predictively Scaling EC2 Instances with Custom CloudWatch Metrics", "author": ["Timothy Maxwell"], "link": "https://blog.developer.bazaarvoice.com/2015/04/17/predictively-scaling-ec2-instances-with-custom-cloudwatch-metrics/", "abstract": "One of the chief promises of the cloud is fast scalability, but what good is snappy scalability without load prediction to match? How many teams out there are still manually switching group sizes when load spikes? If you would like to make your Amazon EC2 scaling more predictive, less reactive and hopefully less expensive it is my intention to help you with this article. Problem 1: AWS EC2 Autoscaling Groups can only scale in response to metrics in CloudWatch and most of the default metrics are not sufficient for predictive scaling. For instance, by looking at the CloudWatch Namespaces reference page we can see that Amazon SQS queues, EC2 Instances and many other Amazon services post metrics to CloudWatch by default. From SQS you get things like NumberOfMessagesSent and SentMessageSize. EC2 Instances post metrics like CPUUtilization and DiskReadOps. These metrics are helpful for monitoring. You could also use them to reactively scale your service. The downside is that by the time you notice that you are using too much CPU or sending too few messages, you’re often too late. EC2 instances take time to start up and instances are billed by the hour, so you’re either starting to get a backlog of work while starting up or you might shut down too late to take advantage of an approaching hour boundary and get charged for a mostly unused instance hour. More predictive scaling would start up the instances before the load became business critical or it would shut down instances when it becomes clear they are not going to be needed instead of when their workload drops to zero. Problem 2: AWS CloudWatch default metrics are only published every 5 minutes. In five minutes a lot can happen, with more granular metrics you could learn about your scaling needs quite a bit faster. Our team has instances that take about 10 minutes to come online, so 5 minutes can make a lot of difference to our responsiveness to changing load. Solution 1 & 2: Publish your own CloudWatch metrics Custom metrics can overcome both of these limitations, you can publish metrics related to your service’s needs and you can publish them much more often. For example, one of our services runs on EC2 instances and processes messages off an SQS queue. The load profile can vary over time; some messages can be handled very quickly and some take significantly more time. It’s not sufficient to simply look at the number of messages in the queue as the average processing speed can vary between 2 and 60 messages per second depending on the data. We prefer that all our messages be handled within 2 hours of being received. With this in mind I’ll describe the metric we publish to easily scale our EC2 instances. The metric we publish is called ApproximateSecondsToCompleteQueue. A scheduled executor on our primary instance runs every 15 seconds to calculate and publish it. In our CloudFormation template we have a parameter calledDesiredSecondsToCompleteQueue and by default we have it set to 2 hours (7200 seconds). In the Auto Scaling Group we have a scale up action triggered by an Alarm that checks whether DesiredSecondsToCompleteQueue is less than ApproximateSecondsToCompleteQueue. Visualizing the Outcome What’s a cloud blog without some graphs? Here’s what our load and scaling looks like after implementing this custom metric and scaling. Each of the colors in the middle graph represents a service instance. The bottom graph is in minutes for readability. Note that our instances terminate themselves when there is nothing left to do. I hope this blog has shown you that it’s quite easy to publish your own CloudWatch metrics and scale your EC2 AutoScalingGroups accordingly.", "date": "2015-04-17"},
{"website": "BazaarVoice", "title": "Conversations API Deprecation for Versions 4.9, 5.0 and 5.1, and Custom Domains", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2015/04/09/conversations-api-deprecation-for-versions-4-9-5-0-and-5-1-and-custom-domains/", "abstract": "This blog post only applies to the Conversations API and does not apply to any other Bazaarvoice product. You are able to identify the Bazaarvoice Conversations API by the following: Code related to the Bazaarvoice Hosted Display does not need modification. It can be identified by the following: Still unsure if this applies to you? Learn more. Today we are announcing two important changes to our Conversations API services: Both of these changes will go into effect on April 30, 2016. Our newer APIs and universal domain system offer you important advantages in both features and performance. In order to best serve our customers, Bazaarvoice is focusing its API efforts on the latest, highest performing API services. By deprecating older versions, we can refocus our energies on the current and future API services, which we feel offer the most benefits to our customers. Please visit our Upgrade Guide to learn more about the Conversations API, our API versioning, and the steps necessary to support the upgrade. We understand that this news may be surprising. This is your first notification of this change. In the months and weeks ahead, we will continue to remind you that this change is coming. We also understand that this change will require effort on your part. Bazaarvoice is committed to making this transition easy for you. We are prepared to assist you in a number of ways: In summary, on April 30, 2016, Conversations API versions released before 5.2 will no longer be available. Applications and websites using versions before 5.2 will no longer function properly after April 30, 2016. In addition, all Conversations API calls, regardless of version, made to a custom domain will no longer respond. Applications and websites using custom domains (such as “ReviewStarsInc.ugc.bazaarvoice.com”) will no longer function properly after April 30, 2016. If your application or website is making API calls to Conversations API versions 4.9, 5.0 and 5.1 you will need to upgrade to the current Conversations API (5.4) and use the universal domain (“api.bazaarvoice.com.”). Applications using Conversations API versions 5.2 and later (5.2, 5.3, 5.4) with the universal domain will continue to receive uninterrupted API service. If you have any questions about this notice, please submit a case in Spark . We will periodically update this blog and our developer Twitter feed ( @BazaarvoiceDev ) as we move closer to the change of service date. Thank you for your partnership, Chris Kauffman Sr. Product Manager", "date": "2015-04-09"},
{"website": "BazaarVoice", "title": "Upgrading Dropwizard 0.6 to 0.7", "author": ["Timothy Maxwell"], "link": "https://blog.developer.bazaarvoice.com/2015/04/10/upgrading-dropwizard-0-6-to-0-7/", "abstract": "At Bazaarvoice we use Dropwizard for a lot of our java based SOA services. Recently I upgraded our Dropwizard dependency from 0.6 to the newer 0.7 version on a few different services. Based on this experience I have some observations that might help any other developers attempting to do the same thing. Package Name Change The first change to look at is the new package naming. The new io.dropwizard package replaces com.yammer.dropwizard . If you are using codahale’s metrics library as well, you’ll need to change com.yammer.metrics to com.codahale.metrics . I found that this was a good place to start the migration: if you remove the old dependencies from your pom.xml you can start to track down all the places in your code that will need attention (if you’re using a sufficiently nosy IDE). Class Name Change aka: where did my Services go? Something you may notice quickly is that the Service interface is gone, it has been moved to a new name: Application. Configuration Changes The Configuration object hierarchy and yaml organization has also changed. The http section in yaml has moved to server with significant working differences. Here’s an old http configuration: and here is a new server configuration: There are at least two major things to notice here: Speaking of appender-based logging, the logging configuration has changed as well. Here is an old logging configuration: and here is a new one: Now that you can configure a list of logback appenders, you can write your own or get one from a library. Previously this kind of logging configuration was not possible without significant hacking. Environment Changes The whole environment API has been re-designed for more logical access to different components. Rather than just making calls to methods on the environment object, there are now six component specific environment objects to access. AdminEnvironment extends ServletEnvironment since it’s just the admin servlet context. By treating the environment as a collection of libraries rather than a Dropwizard monolith, fine-grained control over several configurations is now possible and the underlying components are easier to interact with. Here is a short rundown of the changes: Lifecycle Environment Several common methods were moved to the lifecycle environment, and the build pattern for Executor services has changed. 0.6: 0.7: Other Miscellaneous Environment Changes Here are a few more common environment configuration methods that have changed: 0.6 0.7 Object Mapper Access It can be useful to access the objectMapper for configuration and testing purposes. 0.6 0.7 HttpConfiguration This has changed a lot, it is much more configurable and not quite as simple as before. 0.6 0.7 Test Changes The functionality provided by extending ResourceTest has been moved to ResourceTestRule. 0.6 0.7 Dependency Changes Dropwizard 0.7 has new dependencies that might affect your project. I’ll go over some of the big ones that I ran into during my migrations. Guava Guava 18.0 has a few API changes: Metrics Metric 3.0.2 is a pretty big revision to the old version, there is no longer a static Metrics object available as the default registry. Now MetricRegistries are instantiated objects that need to be managed by your application. Dropwizard 0.7 handles this by giving you a place to put the default registry for your application: bootstrap.getMetricRegistry(). Compatible library version changes These libraries changed versions but required no other code changes. Some of them are changed to match Dropwizard dependencies, but are not directly used in Dropwizard. Jackson 2.3.3 Jersey 1.18.1 Coursera Metrics-Datadog 1.0.2 Jetty 9.0.7.v20131107 Apache Curator 2.4.2 Amazon AWS SDK 1.9.21 Future Concerns Dropwizard 0.8 The newest version of Dropwizard is now 0.8, once it is proven stable we’ll start migrating. Hopefully I’ll find time to write another post when that happens. Thank You For Reading I hope this article helps.", "date": "2015-04-10"},
{"website": "BazaarVoice", "title": "Scoutfile: A module for generating a client-side JS app loader", "author": ["Rebecca Murphey"], "link": "https://blog.developer.bazaarvoice.com/2015/03/05/scoutfile-a-module-for-generating-a-client-side-js-app-loader/", "abstract": "A couple of years ago, my former colleague Alex Sexton wrote about the techniques that we use at Bazaarvoice to deploy client-side JavaScript applications and then load those applications in a browser. Alex went into great detail, and it’s a good, if long, read. The core idea, though, is pretty simple: an application is bootstrapped by a “scout” file that lives at a URL that never changes, and that has a very short TTL. Its job is to load other static resources with long TTLs that live at versioned URLs — that is, URLs that change with each new deployment of the application. This strategy balances two concerns: the bulk of application resources become highly cacheable, while still being easy to update. In order for a scout file to perform its duty, it needs to load JavaScript, load CSS, and host the config that says which JS and CSS to load. Depending on the application, other functionality might be useful: the ability to detect old IE; the ability to detect DOM ready; the ability to queue calls to the application’s methods, so they can be invoked for real when the core application resources arrive. At Bazaarvoice, we’ve been building a lot of new client-side applications lately — internal and external — and we’ve realized two things: one, it’s very silly for each application to reinvent this particular wheel; two, there’s nothing especially top secret about this wheel that would prevent us from sharing it with others. To that end, I’m happy to release scoutfile as an NPM module that you can use in your projects to generate a scout file. It’s a project that Lon Ingram and I worked on, and it provides both a Grunt task and a Node interface for creating a scout file for your application. With scoutfile , your JavaScript application can specify the common functionality required in your scout file — for example, the ability to load JS, load CSS, and detect old IE. Then, you provide any code that is unique to your application that should be included in your scout file. The scoutfile module uses Webpack under the hood, which means you can use loaders like json! and css! for common tasks. The most basic usage is to npm install scoutfile , then create a scout file in your application. In your scout file, you specify the functionality you need from scoutfile : Next, you can generate your scout file using a simple Node script: The README contains a lot more details, including how to use flags to differentiate production vs. development builds; how to configure the Grunt task; how to configure the “namespace” that is occupied on window (a necessary evil if you want to queue calls before your main application renders); and more. There are also several open issues to improve or add functionality. You can check out the developer README if you’re interested in contributing.", "date": "2015-03-05"},
{"website": "BazaarVoice", "title": "Automated Product Matching, Part II: Guidelines", "author": ["Tony Cassandra"], "link": "https://blog.developer.bazaarvoice.com/2015/03/26/automated-product-matching-part-ii-guidelines/", "abstract": "This post continues the discussion from Automated Product Matching, Part I: Challenges . With each design iteration, I gradually came to appreciate how important it was to have an overall matching system that was well designed. The quality of the matching algorithm did not matter if its output was going to be impacted by failures in other parts of the system. The mistake of focusing on the algorithmic design first, and the system design second means that you wind up with an interesting technical talk, but you have not really solved the problem for the end users and/or the business. An example from my past experience might help give flavor for why the system view matters just as much as the algorithmic view. In one of the earlier systems I worked on, after having successfully defined how to build a set of “canonical” products which would be used to match against all our incoming data, and having created a reasonably good matching algorithm, we were happy that we could now continually process and match all of our data each day and at scale. The problem was solved, but only in a static sense. We chose to ignore how new products would get into the canonical set. As time went on, this became more and more of a problem, until we finally had to address this omission. This was about the time when iPads first hit the market and the lack of freshness became glaringly obvious to anyone looking at iPads on our web site. There was nothing algorithmically challenging about solving this: we knew how to create canonical products, but the code we built did not support adding new canonical products easily. Although the guts of the algorithmic logic could be re-used, the vast majority of the code that comprised the system we built around it needed to be redesigned. A little forethought here would have saved many months of additional work, not to mention all the bad user experiences that we were delivering due to our lack of matching newer products. “A good algorithm in a bad system is indistinguishable from a bad algorithm.” The difficulty of matching products ranges from easy to impossible. I recommend starting with an algorithm that focuses on matching the easiest things first and building a system around that. This allows you to start working on the important system issues sooner and get some form of product matching working faster. From a system design perspective, the product data needs to find its way to the matching algorithm, you will need a data model and data storage for the matches, you also need some access layer for the matches and you likely need to have some system for evaluating and managing the product matches. The matching logic itself will be a very small percentage of the code you have to write and there are plenty of challenges in all these other areas. There are important lessons to be learned just in putting the system together, and even the simplest matching logic will lead to a greater understanding of how to build the appropriate data models you will need. “For the human makers of things, the incompleteness and inconsistencies of our ideas become clear only during implementation.” The topic name of “automated matching” implies that people will not be involved. Combine this with engineers who are conditioned to build systems that remove the rote, manual work from tasks and there is the risk of being completely blind to a few important questions. Most fundamentally, you should ask whether you really need automated matching and whether it will be the most cost-effective solution. This is going to be determined by the scale of your problem. If your matching needs are on the order of only thousands of products, there are crowd-source solutions that make manual matching a very viable option. If your scale is on the order of millions, manual matching is not out of the question, though it may take some time and money to get through all the work. Once you get into the range of tens of millions, you likely have little choice but to use some form of automated matching. Another option is a hybrid approach that uses algorithms to generate candidate matches and has people assigned to accept or reject the matches. This puts less pressure on the accuracy requirements of your algorithms and makes the people process more efficient, so it can be viewed as an optimization of a manual matching system. An approach that scales slightly better is to automatically match the easy products and defer the harder ones to manual matching or verification. The other question about human involvement depends on how the quality of the matching system will be measured. Building training and/or evaluation data sets will likely require some human input and tools to support this work. Considering how feedback will be used is important because it can have an impact on the matching system and algorithm designs. Evaluation will likely need to be an ongoing process, so make sure consideration is given to the longer term human resource requirements. “It ain’t what you don’t know that gets you into trouble. It’s what you know for sure that just ain’t so.” — Mark Twain Simply put: it is not possible for a single algorithm to do equally well matching all types of products. It is possible to use the same class of algorithm on some parts of the product category space, but you will need to parameterize the algorithm with category-specific information. Here are a couple examples to illustrate this point. Consider the color attribute of a product. For a product like a dishwasher, color is a secondary characteristic and would be unimportant for the purpose of review content. For some types of products, say a computer monitor, the color might not even matter for price comparisons. For a $50 savings, many people are not going to care if their monitor is black or silver. On the other hand, for products like cosmetics, the color is the most essential feature of the product. If your algorithm universally treats all attributes the same while matching, regardless of the type of product it is, then it will necessarily perform poorly in some categories. To get better accuracy you will have to invest in human domain expertise in either encoding domain-specific information, or training algorithms for each product category. If you have ever taken a hard look at camera products, there is a lot of cryptic symbology in the lens specifications and the other camera accessories. Without encoding knowledge from a domain expert, it is not going to be possible to match these types of products well. There’s no silver bullet. You can decide to allocate your time to one set of categories over another, but you should expect limited accuracy in the areas you have not invested in. Another example lies in the contrast between consumer electronics and books. The product titles for consumer electronics are descriptive in that they contain a list of product features. With a rich enough title, there are enough features to yield relatively high confidence in matches. However, titles for books are arbitrary words and phrases chosen by the author and may give you little understanding of the contents. Similarity between book titles is not correlated with the similarity of their content. “Do not mistake knowing something for knowing everything.” String-based matching algorithms may suffice depending on your targets for accuracy and coverage, but there is a hard limit on how well they will perform without imparting semantics to the strings. Not all words in product titles are created equal, so it helps to do something that is akin to part of speech tagging (e.g., The product “noun” is much more important than a product’s adjective, such as its color). Showing two different dishwashers as being the same might be a data error, but it is a characteristically different user experience than showing a dishwasher and a shoe as being the same. A string comparison algorithm might match the shoe to the dishwasher because it had the same color plus a few other strings in common, but no understanding that the mismatch of nouns “shoe” and “dishwasher” should trump anything else that might be indicating that they are similar. You will need more than just adjectives and nouns though. There are many different types of adjectives used to describe products. There are colors, materials, dimensions, quantities, shapes, patterns, etc. and depending on the types of product, these may or may not matter in how you want to define product equivalence. It is also true that just because two strings are different, it is not necessarily the case that they are referring to two different concepts. If you do not encode the knowledge that “loafer” and “shoe” are semantically similar, even though they have no string similarity, you will be limited in matching the variations that different data sources will provide. For more accurate results, it is important to semantically tokenize the strings so that your algorithms can work on a normalized, conceptual view of the products. Some algorithmic technique might be helpful in dealing with these word synonyms, but if the domain vocabulary is restricted, it may even be feasible to manually curate the important variations and their parts of speech. Whether algorithmic or hand curated, you will need to encode this domain knowledge so that it is dependent on the product’s context. The string “apple” may be referring to a popular brand, a deciduous fruit or the scent of a hair care product. Category and peripheral information about the product will be needed to disambiguate “apple” and similar strings. “Algorithms are for people who don’t know how to buy RAM.” Product titles are not amenable to generic natural language processing (NLP) solutions. Product titles are not well-formed sentences and have their own structure that often varies by the person or company that crafted them. Thinking that product matching can be solved with some off-the-shelf NLP techniques is a mistake. There are some NLP techniques that can be applied, but they have to be carefully tailored to work in this domain. Consider the relative importance of word order between product titles for consumer electronics and for books. For electronics, the title word order does not really matter: “LCD TV 55 inch Sony” is not semantically different from “Sony 55 inch LCD TV”. Yet if you change the order of two words in a book’s title, you now have something completely different. “The Book of the Moon” and “The Moon Book” are two completely different books. Product descriptions offer the best opportunity for the use of NLP techniques, since they tend to be natural language descriptions. Unfortunately, all sorts of peripheral concepts are included in the descriptions and this makes it hard to use them for product matching. It is also true that the descriptions for similar, but not identical products tend to look very similar. The best use of descriptions is in helping to determine the product’s category, which can help with matching, but do not expect that it will provide a strong signal for matching. “If you find a solution and become attached to it, the solution may become your next problem.” Neither the input product data nor your matching algorithm will be 100% accurate. You need to make sure your algorithms are not rigidly expecting consistent data. This includes being able to compensate and/or correct bad data when it is detected. This is easier said than done, especially because we have all been conditioned to prefer more elegant and/or understandable code. Software code can look quite poetic when you do not have to litter it with constant sanity checks for edge cases and all the required exception handling this leads to. Unfortunately, the real world of crawl and feed data is not very elegant, nor will your algorithms produce flawless results. This assumption about imperfect data should not be limited to the technical side of the product. I believe it is critically important that product designers work closely with the algorithmic designers to understand the characteristics of the data and the nature of the errors since this can be critical in designing a good user experience. Different algorithmic choices can result in different types of errors, and only by working together can the trade-offs be evaluated and good choices made which will influence how the users will perceive the product matching accuracy. As a simple example of how designers and engineers can work together to make a better solution, suppose the engineers build a matching algorithm that outputs some measure of confidence. In isolation, the engineers will have to find the right threshold to balance accuracy and coverage, then declare matches for those above the threshold. In this scenario, the user interface designer only knows whether or not there are matches, so the interface is designed with the wording that says “matching products”. If these products are on the lower end of the confidence range, and they are bad matches, it will be a bad user experience. Alternatively, if the designers are aware that there is a spectrum of match confidence, they could agree to expose those confidence values and instead of having to declare “matching products”, when the confidence is lower, they might opt to use softer wording like “similar products”, maybe even positioning them differently on the page. A user will not be quite as disappointed in the matching if they were only promised “similar” products. “There are two ways to write error-free programs; only the third one works.” — Alan J. Perlis Suppose you have built your matching system, and an evaluation system to go along with it, then find out the accuracy rate is 95%. Assuming your system is giving reasonably good coverage, and in the presence of bad and missing data, this is definitely an impressive achievement. But what if within that 5% of the errors lies the current most popular products? If you weight error frequency by number of page views, the effective accuracy rate is going to be much, much lower. All the people viewing those mismatched product are not going to be impressed with your algorithm. Even without considering weighting by page views, consider a situation where you display 20 products at a time on a page. With a 5% error rate, on average every page you show contains an error. Defined differently, this means your error rate is not 5% but 100%. Matching algorithms will not be perfect, and even near perfect algorithms will need help. This help usually comes in the form of providing tools that allow human intervention to influence the overall quality or to influence the algorithmic output. When you are making an error on a highly visible product, someone should be able to be able to quickly override the algorithmic results to fix the problem. “Williams and Holland’s Law: If enough data is collected, anything may be proven by statistical methods.” There is no shortage of other product matching topics to discuss and interesting details to dive into. These first two blog posts have tried to capture some of the higher-level considerations. Future articles will provide more detailed examinations of these topics and some of the approaches we have taken in Bazaarvoice’s product matching systems.", "date": "2015-03-26"},
{"website": "BazaarVoice", "title": "Automated Product Matching, Part I: Challenges", "author": ["Tony Cassandra"], "link": "https://blog.developer.bazaarvoice.com/2015/03/12/automated-product-matching-part-i-challenges/", "abstract": "Bazaarvoice’s flagship product is a platform for our clients to accept, display and manage consumer generated content (CGC) on their web sites. CGC includes reviews, ratings, images, videos, social network content, etc. Over the last few years, syndicating CGC from one site to another has become increasingly important to our customers. When a user submits a television review on Samsung’s branded web site, it benefits Samsung, Target and the consumer when that review can be shown on Target’s retail web site. Before syndicating CGC became important to Bazaarvoice, our content could be isolated for each individual client. There was never any need for us to consider the question of whether our clients had any overlap in their product catalogs. With syndication, it is now vital for us to be able to match products across all our clients’ catalogs. The product matching problem is not unique to Bazaarvoice. Shopping comparison engines, travel aggregators and ticket brokers are among the other domains that require comprehensive and scalable automated matching. This is a common enough problem that there are even a number of companies trying to grow a business based on providing product matching as a service. I have helped design and build product matching systems five different times across two different domains and will share some of what I have learned about the characteristics of the problem and its solutions. This article will not be about specific algorithms or technologies, but guidelines and requirements that are needed when considering how to design a technical solution. This will address not just the algorithmic challenges, but also the equally important issues with designing product matching as a system. Blog posts are best kept to a modest length, and I have many more thoughts to share on this topic than would be polite to include in a single article, so I have divided this discussion into two parts. This blog post is about the characteristics that make this an interesting and challenging problem. The second posting will focus on guidelines to follow when designing a product matching system. The focus here will be on retail product matching, since that is where my direct experience lies. I am sure that there are additional lessons to be learned in other domains, but I think many of these insights may be more broadly applicable. “If at first you don’t succeed, you must be a programmer.” Product matching is one of those problems that initially seems straightforward, but whose complexity is revealed only after having immersed oneself in it. Even the most enlightened product manager is not going to have the time to spell out, in detail, how to deal with every nuance that arises. Understanding problems in depth, and filling in the large numbers of unspecified items with reasonably good solutions is why many software engineers are well paid. It is also what makes our jobs more interesting than most, since it allows us to invoke our problem solving and design skills, which we generally prefer to rote execution of tasks. I am not proposing that the engineers should fill in all the details without consulting the product managers and designers, I only mean that the engineers should expect the initial requirements will need to be refined. Ideally both will work to fill in the gaps, but the engineers should expect they will be the ones uncovering and explaining the gaps. “I have yet to see any problem, however complicated, which, when you looked at it in the right way, did not become still more complicated.” — Poul Anderson Language is inherently imprecise. The same word can refer to completely different concepts at different times and yet it causes no confusion when the people conversing share the same contextual information. On the other hand, software engineers creating a data model have to explicitly enumerate, encode, and give names to all the concepts in the system. This is a fundamental difference between how the engineers and others view the problem and can be a source of frustration when engineers begin to inject questions into the requirements process such as: “What is a product?”. Those that are not accustomed to diving into the concepts underlying their use of a word can often feel like this is a time-wasting, philosophical discussion. I’ve run across 8 distinct concepts where the word “product” has been used. The most basic difference lies between those “things” that you are trying to match and the “thing” you are using as the basis of the match. Suppose you get a data feed from Acme, Inc. which includes a thing called an “Acme Giant Rubber Band” and that you also crawled the Kwik-E-Mart web site, which yielded a thing called an “Acme Giant Green Rubber Band”. You then ask the question, are these the same “product”? Here we have an abstract notion of a specific rubber band in our mind and we are asking the question of whether these specific items from those two data sources match this concept. Now let us also suppose that the “Acme Giant Rubber Band” item in the Acme data feed has listed 6 different UPC values, which correspond to 6 different colors they manufacturer for the product. This means that the “thing” in the feed is really a set of individual items, while the “Acme Giant Green Rubber Band” we saw on the Kwik-E-Mart web site just is a single item. These two items are similar, but not identical product-related concepts. With just this simple example, there are 3 different concepts floating around, yet for each of them the “product” is often the word people will use. For most domains, when you really start to explore the data model that is required, more than three product-related concepts will likely be needed. Software designers must carefully consider how many different “product” concepts they need to model and those helping to define the requirements should appreciate the importance of, and invest time in understanding the differences between the concepts. The importance of getting this data model correct from the start cannot be stressed enough. “If names are not correct, then language is not in accord with the truth of things. If language is not in accord with the truth of things, then affairs cannot be carried out successfully.” — Confucius You should start with the most basic of questions: What is a “match”? My experience working on product matching in different domains and varying use cases is that there is not a single definition of product equality that applies everywhere. For those that have never given product matching much thought beyond their intuition, this might seem like an odd statement: two products are either the same or they are not, right? By way of example, here is an illustration of why different use cases require different notions of equality. Suppose you are shopping for some 9-volt batteries and you are interested in seeing which brands tend to last longer based on direct user experience. You do a search, you navigate through some web site and then will likely need to make a choice at some point: are you looking to buy the 2-pack, the 4-pack or the 8-pack? Having to make a quantity choice at this point may be premature, but you usually have to make this choice to get at the review content. However, the information you are looking for, and likely the bulk of the review content, is independent of the size of the box in which it is packaged. Requiring a quantity choice to get at review content may just be a bad user experience, but regardless of that, you certainly would not want to miss out on relevant review content simply because you had chosen the wrong quantity at this point in your research. The conclusion here is that reviews posted to the web page for the 2-pack and reviews posted for the page of an 8-pack should probably not be fragmented. Therefore, for the purposes of review content, these two products, which would have different UPC and/or EAN values, should be considered equivalent. Now suppose you have made your decision on the brand of battery to buy and now you are looking for the best price on an 8-pack. For a price comparison, you most definitely do not want to be comparing the 2-pack prices along with its 8-pack equivalent. Here, for price comparisons, these two products should definitely not be considered equivalent. Understanding that product equivalence varies by context is not only important for designing algorithms and software systems, but has a lot of implications for creating better user experiences. For the companies looking to offer product matching as a service, the flexibility they offer in tailoring the definition of equality for their clients will be an important factor in how broadly applicable their solutions will be. “It is more important to know where you are going than to get there quickly. Do not mistake activity for achievement.” — Isocrates If all the products you need to match have been assigned a globally unique identifier, such as a UPC, EAN or ISBN, and you have access to that data, and the data can be trusted, then product matching could be trivial. However, not all products get assigned such a number and for those that do, you do not always have access to those values. As discussed, it is also true that a “match” cannot always be defined simply by the equality of unique identifiers. Those that crawl the web for product data tend to think that a structured data feed is the answer to getting better data. However, the companies that create product feeds vary greatly in their competency. Even when competent, they may build their feed from one system’s database, while more useful information may be stored in another system. Further, the competitive business landscape can result in companies wanting to deliberately suppress or obfuscate identifying information. You also have the ubiquitous issues of software bugs and data entry errors to contend with. All these realities add up to the fact that data feeds are not a panacea for product matching. So while we have the web crawling folks wishing for feed data, we simultaneously have the feed processing folks wishing for crawled data to fill in their gaps. The first piece of advice for building a product matching system is to assume you will need to accept data from a variety of data sources. The ability to fill in data gaps with alternative sources will allow you to get the best of both worlds. This also means you may not only be trying to match products between different sites, but you may need to match products within the same site and merge the data from different sources to form a single view of a product at a site. I know of one very large shopping comparison site that did not design for this case and found themselves without the ability to support particular types of new business opportunities. “If you think the problem is bad now, just wait until we’ve solved it.” — Arthur Kasspe The specific algorithms and technologies one chooses for an automated product matching system should not be the primary focus. It is very tempting for us information scientists and engineers to dive right into the algorithmic and technical solutions. After all, this is predominantly what universities have trained us to focus on and, in some sense, is the more interesting part of the problem. You can choose almost any one of a host of algorithms and get some form of product matching fairly quickly. Depending on your specific quality requirements, a simple system may be enough, but if there are higher expectations for a matching system, you will need a lot more than just a fancy algorithm. When more than simple matching is needed, it will not be the algorithm you use, but how you use the algorithm that will matter. This means really understanding the characteristics of the problem in the context of your domain. It is also important not to define the problem too narrowly. There are a bunch of seemingly tangential issues in product matching that are very easy to put into the bucket of “we can deal with that later”, but which turn out to be very hard to deal with after the fact. It is how well you handle all of these practical details that will most influence the overall success of the project. Choosing a simplistic data model is an example where it may seem like a good starting approach. However, this will wind up being so deeply ingrained in the software, that it will become nearly impossible to change. You wind up with either serious capability limitations or a series of kludges that both complicate your software and lead to unintended side effects. I learned this from experience. “A doctor can bury his mistakes but an architect can only advise his clients to plant vines.” — Frank Lloyd Wright This posting covers some of the important characteristics of the product matching problem. In the sequel, there will be some more specific guidelines for building matching systems.", "date": "2015-03-12"},
{"website": "BazaarVoice", "title": "Full Consistency Lag for Eventually Consistent Systems", "author": ["Fahd Siddiqui"], "link": "https://blog.developer.bazaarvoice.com/2015/03/23/full-consistency-lag-for-eventually-consistent-systems/", "abstract": "A distributed data system consisting of several nodes is said to be fully consistent when all nodes have the same state of the data they own. So, if record A is in State S on one node, then we know that it is in the same state in all its replicas and data centers. Full Consistency sounds great. The catch is the CAP theorem that states that its impossible for a distributed system to simultaneously guarantee consistency (C), availability (A), and partition tolerance (P). At Bazaarvoice, we have sacrificed full consistency to get an AP system and contend with an eventually consistent data store. One way to define eventual consistency is that there is a point in time in the past before which the system is fully consistent (full consistency timestamp, or FCT). The duration between FCT and now is called the Full Consistency Lag (FCL). An eventually consistent system may never be in a fully consistent state given a massive write throughput. However, what we really want to know deterministically is the last time before which we can be assured that all updates were fully consistent on all nodes. So, in the figure above, in the inconsistent state, we would like to know that everything up to Δ2 has been replicated fully, and is fully consistent. Before we get down to the nitty-gritty of this metric, I would like to take a detour to set up the context of why it is so important for us to know the full consistency lag of our distributed system. At Bazaarvoice, we employ an eventually consistent system of record that is designed to span multiple data centers, using multi-master conflict resolution. It relies on Apache Cassandra for persistence and cross-data-center replication. One of the salient properties of our system of record is immutable updates. That essentially means that a row in our data store is simply a sequence of immutable updates, or deltas. A delta can be a creation of a new document, an addition, modification, or removal of a property on the document, or even a deletion of the entire document. For example, a document is stored in the following manner in Cassandra, where each delta is a column of the row. Δ1 { “rating”: 4, “text”: “I like it.”} Δ2 { .., “status”: “APPROVED” } Δ3 { .., “client”: “Walmart” } So, when a document is requested, the reader process resolves all the above deltas (Δ1 + Δ2 + Δ3) in that order, and produces the following document: { “rating”: 4, “text”: “I like it.”, “status”: “APPROVED”, “client”: “Walmart”, “~version”: 3 } Note that these deltas are stored as key-value pairs with the key as Time UUID. Cassandra would thus always present them in increasing order of insertion, making sure the last-write-wins property. Storing the rows in this manner allows us massive non-blocking global writes. Writes to the same row from different data centers across the globe would eventually achieve a consistent state without making any cross-data center calls. This point alone warrants a separate blog post, but it will have to suffice for now. To recap, rows are nothing but a sequence of deltas. Writers simply append these deltas to the row, without caring about the existing state of the row. When a row is read, these deltas are resolved in ascending order and produce a json document. There is one problem with this: over time rows will accrue a lot of updates causing the row to become really wide. The writes will still be OK, but the reads can become too slow as the system tries to consolidate all those deltas into one document. This is where compaction helps. As the name suggests, compaction resolves several deltas, and replaces them with one “compacted” delta. Any subsequent reads will only see a compaction record, and the read slowness issue is resolved. Great. However, there is a major challenge that comes with compaction in a multi-datacenter cluster. When is it ok to compact rows on a local node in a data center? Specifically, what if an older delta arrives after we are done compacting? If we arbitrarily decide to compact rows every five minutes, then we run the risk of losing deltas that may be in flight from a different data center. To solve this issue, we need to figure out what deltas are fully consistent on all nodes and only compact those deltas, which basically is to say, “Find time (t) in the past, before which all deltas are available on all nodes”. This t, or full consistency timestamp , assures us that no deltas will ever arrive with a time UUID before this timestamp. Thus, everything before the full consistency timestamp can be compacted without any fear of data loss. There is just one issue. This metric is absent in out of the box AP systems such as Cassandra. To me, this is a vital metric for an AP system. It would be rare to find a business use case in which permanent inconsistency is tolerable. Although Cassandra doesn’t provide the full consistency lag, we can still compute it in the following way: T f = Time no hints were found on any node rpc_timeout = Maximum timeout in cassandra that nodes will use when communicating with each other. FCT = Full Consistency Timestamp FCL = Full Consistency Lag FCT = T f – rpc_timeout FCL = T now – FCT The concept of Hinted Handoffs was introduced in Amazon’s dynamo paper as a way of handling failure. This is what Cassandra leverages for fault-tolerant replication. Basically, if a write is made to a replica node that is down, then Cassandra will write a “hint” to the coordinator node and try again in a configured amount of time. We exploit this feature of Cassandra to get us our full consistency lag. The main idea is to poll all the nodes to see if they have any pending hints for other nodes. The time when they all report zero (T f ) is when we know that there are no failed writes, and the only pending writes are those that are in flight. So, subtracting the cassandra timeout (rpc_timeout) will give us our full consistency lag. Now, that we have our full consistency lag, this metric can be used to alert the appropriate people when the cluster is lagging too far behind. Finally, you would want to graph this metric for monitoring. Note that in the above graph we artificially added a 5 minute lag to our rpc_timeout value to avoid excessively frequent compactions. We periodically poll for full consistency every 300 seconds (or 5 minutes). You should tweak this value according to your needs. For our settings above, the expected lag is 5 minutes, but you can see it spike at 10 minutes. All that really says is there was one time when we checked and found a few hints. The next time we checked (after 5 minutes in our case) all hints were taken care of. You can now set an alert in your system that should wake people up if this lag violates a given threshold–perhaps several hours–something that makes sense for your business.", "date": "2015-03-23"},
{"website": "BazaarVoice", "title": "Analyzing our global shopper network (part one)", "author": ["Trey Perry"], "link": "https://blog.developer.bazaarvoice.com/2015/02/20/analyzing-our-global-shopper-network-part-one/", "abstract": "Every holiday season, the virtual doors of your favorite retailer are blown open by a torrent of shoppers who are eager to find the best deal, whether they’re looking for a Turbo Man action figure or a ludicrously discounted 4K flat screen. This series focuses on our Big Data analytics platform, which is used to learn more about how people interact with our network. Within the Reporting & Analytics group, we use Big Data analytics to help some of the world’s largest brands and retailers understand how to most effectively serve their customers, as well as provide those customers with the information they need to make informed buying decisions. The amount of clickstream traffic we see during the holidays – over 45,000 events per second, produced by 500 million monthly unique visitors from around the world – is tremendous. In fact, if we reserved a seat at the Louisiana Superdome for each collected analytics event, we would fill it up in about 1.67 seconds. And, if we wanted to give each of our monthly visitors their own seat in a classic Beetle, we’d need about 4.64 times the total number produced between 1938 and 2003. That’s somewhere in the neighborhood of a hundred million cars! Fortunately for us, we live in the era of Big Data and high scalability. Our platform, which is based on the principles outlined in Nathan Marz’s Lambda architecture design, addresses the requirements of ad-hoc, near real-time, and batch applications. Before we could analyze any data, however, we needed a way to reliably collect it. That’s where our in-house event collection service, which we named “Cookie Monster,” came into the picture. When investigating how clients would send events to us, our engineers knew that the payload had to fit within the query string of an HTTP GET request. They settled upon a lightweight serialization format called Rison , which expresses JSON data structures, but is designed to support URI encoding semantics. (Our Rison plugin for Jackson , which we leverage to handle the processing of Rison-encoded events, is available on GitHub.) In addition, we decided to implement support for client-side batching logic, which would allow a web browser to send multiple events within the payload of a single request. By sending fewer requests, we reduced the amount of HTTP transaction overhead, which minimized the amount of infrastructure required to support a massive network audience. Meanwhile, as their browsers would only need to send one request, end-users also saw a performance uptick. Because the service itself needed a strong foundation, we chose the ubiquitous Dropwizard framework, which accelerated development by providing the basic ingredients needed to create a maintainable, scalable, and performant web service. Dropwizard glues together Jetty (a high-performance web server), Jersey (a framework for REST -ful web services), and Jackson (a JSON processor). Perhaps most importantly, we used the Disruptor library ‘s ring buffer implementation to facilitate very fast inter-thread messaging. When a new event arrives, it is submitted to the EventQueue by the EventCollector. Two event handler classes, which listen for ring events, ensure that the event is delivered properly. The first event handler acts as a producer for Kafka , publishing the event to the appropriate topic. (Part two of this series will discuss Kafka in further detail.) The second is a “fan out” logging sink, which mods specific event metadata and delivers the corresponding event to the appropriate logger. At the top of every hour, the previous hour’s batch logs are delivered to S3, and then consumed by downstream processes. When building Cookie Monster, we knew that our service would need to maintain as little state as possible, and accommodate the volatility of cloud infrastructure. Because EC2 is built on low-cost, commodity hardware, we knew that we couldn’t “cheat” with sophisticated hardware RAID – everything would run on machines that were naturally prone to failure. In our case, we deemed those trade-offs acceptable, as our design goals for a distributed system aligned perfectly with the intent of EC2 auto-scaling groups. Even though the service was designed for EC2, there were a few hiccups along the way, and we’ve learned many valuable lessons. For example, the Elastic Load Balancer, which distributes HTTP requests to instances within the Auto Scaling group, must be “pre-warmed” before accepting a large volume of traffic. Although that’s by design, it means that good communication with AWS prior to deployment must be a crucial part of our process. Also, Cookie Monster was designed prior to the availability of EBS optimized instances and provisioned IOPS, which allow for more consistent performance of an I/O-bound process when using EBS volumes. Even in today’s world, where both of those features could be enabled, ephemeral (i.e. host-local) volumes remain a fiscally compelling – if brittle – alternative for transient storage. ( AWS generally discourages the use of ephemeral storage where data loss is a concern, as they are prone to failure.) Ultimately, our choice to deploy into EC2 paid off, and it allowed us to scale the service to stratospheric heights without a dedicated operations team. Today, Cookie Monster remains an integral service within our Big Data analytics platform, successfully collecting and delivering many billions of events from all around the world.", "date": "2015-02-20"},
{"website": "BazaarVoice", "title": "Theo Schlossnagle, The Rise of the Machines and Data PTSD", "author": ["Bryan Chagoly"], "link": "https://blog.developer.bazaarvoice.com/2014/06/14/theo-schlossnagle-rise-machines-data-ptsd/", "abstract": "Check out Bazaarvoice IO 2014 Technical Conference keynote speaker, Theo Schlossnagle, @postwait , CEO of Circonus discussing The Rise of the Machines and Data PTSD", "date": "2014-06-14"},
{"website": "BazaarVoice", "title": "Otis Gospodnetic, The Open Source Search Evolution", "author": ["Bryan Chagoly"], "link": "https://blog.developer.bazaarvoice.com/2014/06/14/otis-gospodnetic-open-source-search-evolution/", "abstract": "Check out Bazaarvoice IO 2014 Technical Conference keynote speaker Otis Gospodnetic, @otisg , Founder of Sematext , Committer on Lucene, Solr, Nutch, Mahout, OpenRelevance, and author of Lucene in Action discussing the Open Source Search Evolution", "date": "2014-06-14"},
{"website": "BazaarVoice", "title": "Bob Metcalfe, Metcalfe’s Law After 40 Years of Ethernet", "author": ["Bryan Chagoly"], "link": "https://blog.developer.bazaarvoice.com/2014/06/14/bob-metcalfe-metcalfes-law-40-years-ethernet/", "abstract": "Check out Bazaarvoice IO 2014 Technical Conference Keynote speaker Bob Metcalfe, @BobMetcalfe , Professor of Innovation at the University of Texas, discussing Metcalfe’s Law After 40 Years of Ethernet", "date": "2014-06-14"},
{"website": "BazaarVoice", "title": "Greg Brockman, How We Scaled Stripe from 4 to 94 Employees", "author": ["Bryan Chagoly"], "link": "https://blog.developer.bazaarvoice.com/2014/06/14/greg-brockman-scaled-stripe-4-94-employees/", "abstract": "Check out Bazaarvoice IO 2014 Technical Conference keynote speaker Greg Brockman, @thegdb , CTO of Stripe and designer of the Stripe CTF contests discussing how they scaled Stripe from 4 to 94 employees", "date": "2014-06-14"},
{"website": "BazaarVoice", "title": "Open sourcing cloudformation-ruby-dsl", "author": ["Nathaniel Eliot"], "link": "https://blog.developer.bazaarvoice.com/2014/04/11/cloudformation-ruby-dsl/", "abstract": "Cloudformation is a powerful tool for building large, coordinated clusters of AWS resources. It has a sophisticated API, capable of supporting many different enterprise use-cases and scaling to thousands of stacks and resources. However, there is a downside: the JSON interface for specifying a stack can be cumbersome to manipulate, especially as your organization grows and code reuse becomes more necessary. To address this and other concerns, Bazaarvoice engineers have built cloudformation-ruby-dsl , which turns your static Cloudformation JSON into dynamic, refactorable Ruby code. Ruby DSL for creating Cloudformation templates https://github.com/bazaarvoice/cloudformation-ruby-dsl 76 forks. 206 stars. 22 open issues. Recent commits: The DSL closely mimics the structure of the underlying API, but with enough syntactic sugar to make building Cloudformation stacks less painful. We use cloudformation-ruby-dsl in many projects across Bazaarvoice. Now that it’s proven its value, and gained some degree of maturity, we are releasing it to the larger world as open source, under the Apache 2.0 license. It is still an earlier stage project, and may undergo some further refactoring prior to it’s v1.0 release, but we don’t anticipate major API changes. Please download it, try it out, and let us know what you think (in comments below, or as issues or pull request on Github). A big thanks to Shawn Smith , Dave Barcelo , Morgan Fletcher , Csongor Gyuricza , Igor Polishchuk , Nathaniel Eliot , Jona Fenocchi , and Tony Cui , for all their contributions to the code base.", "date": "2014-04-11"},
{"website": "BazaarVoice", "title": "Output from bv.io", "author": ["Scotty Loewen Jr"], "link": "https://blog.developer.bazaarvoice.com/2014/04/11/output-bv-io/", "abstract": "Looks like everyone had a blast at bv.io this year! Thank yous go out to the conference speakers and hackathon participants for making this year outstanding. Here are some tweets and images from the conference: RT @bazaarbrett : Hackathon is kicking off, very glad to be here! #bvhackathon pic.twitter.com/q8dnfqlQxh — Bazaarvoice (@Bazaarvoice) April 2, 2014 BV.IO 2014 is on! #bvhackathon pic.twitter.com/OkRnOsoGo3 — Benton Porter (@bentonporter) April 2, 2014 Me and @BobMetcalfe in front of his presentation notes at @Bazaarvoice #bvhackathon – awesome job, Bob! pic.twitter.com/uKjQ4WZr2y — Brett Hurt (@databrett) April 2, 2014 The first internet with @BobMetcalfe at #bvhackathon pic.twitter.com/LOsvDb4dpT — Gary Allison (@garyallison) April 2, 2014 Bob Metcalf (see: Metcalf's Law http://t.co/TPvDWdPm0U ) dropping knowledge of early networks at #BVIO2014 — Ari Paparo (@aripap) April 2, 2014 Getting schooled on search indexes by @otisg at #bvio2014 #bvhackathon @BazaarvoiceDev pic.twitter.com/I1snZhZiFn — Bryan Chagoly (@bchagoly) April 2, 2014 Awesome! RT @bchagoly : Our conference is #1 trending topic in Austin. #bvhackathon #bvio2014 http://t.co/Ofocqzj2LL pic.twitter.com/RgmLbj5BSW — BV Dev Relations (@BazaarvoiceDev) April 2, 2014 The Rise of the Machines and Data PTSD — Theo Schlossnagle @postwait @ImageThink #bvio2014 pic.twitter.com/eGYxIn7PN8 — Ann Enders (@AEnders) April 2, 2014 BVIO2014 http://t.co/BzO6yPjK9n #storify #bvhackathon #bvio2014 — Theo Schlossnagle (@postwait) April 4, 2014 @theuntergeek from ElasticSearch on the economies of scale #bvio2014 pic.twitter.com/LtzdYJk8bU — Ann Enders (@AEnders) April 2, 2014 Time to learn about the BV Data Infrastructure w/ Fahd Siddiqui & John Roesler! #bvhackathon #bvio2014 #ImageThink pic.twitter.com/hTYMpLZEtx — Sheila Hippert (@sheilahippert) April 2, 2014 Breakout sessions going well at #BVIO2014 ! Here's @danheberden discusssing rapid prototyping w/ Ember #bvhackathon pic.twitter.com/FlhnoOGt6N — Sheila Hippert (@sheilahippert) April 2, 2014 @postwait chilling in a beanbag while watching @danheberden discussing rapid prototyping with Ember #bvhackathon pic.twitter.com/SYJ8nlhwhH — Ann Enders (@AEnders) April 2, 2014 . @brianarn is showing us the power of the chrome console and debugger at the #bvhackathon pic.twitter.com/LaLu5F2phK — pavel_lishin (@pavel_lishin) April 3, 2014 You only get to pick one top priority…hiring awesome people @thegdb #bvio2014 #bvhackathon @BazaarvoiceDev pic.twitter.com/8S5FLAXPxk — Bryan Chagoly (@bchagoly) April 2, 2014 #bvio2014 #bvhackathon our awesome @UWaterloo co-ops! pic.twitter.com/I1MHznxiVk — BV Dev Relations (@BazaarvoiceDev) April 2, 2014 Still hacking away at (almost) 4am! #bvhackathon #bvio2014 @danheberden @CEEJ86 @otf63 pic.twitter.com/DD1b2DxpkK — Sheila Hippert (@sheilahippert) April 4, 2014 #iamprogramming #bvhackathon pic.twitter.com/OoIHhjUHgU — Victor Trac 💉💉⌛ (@victortrac) April 4, 2014 #bvhackathon black ops competition pic.twitter.com/Cu0NLShZmQ — BV Dev Relations (@BazaarvoiceDev) April 4, 2014 https://twitter.com/bvrecruits/status/451813466941059072 And the demos have begun! #bvhackathon #bvio2014 pic.twitter.com/zc1CqnrQDm — BV Dev Relations (@BazaarvoiceDev) April 4, 2014 Hackathon fun at #bvhackathon ! pic.twitter.com/Uu15wHiTs2 — Gary Allison (@garyallison) April 4, 2014 . @uwaterloo “ @sheilahippert : Our Waterloo interns are such badasses. #killinit #bvhackathon #bvio2014 pic.twitter.com/ePEEOkgD4r ” — jonloyens (@jonloyens) April 4, 2014 This is how you win a prize… Solo. #bvhackathon #bvh2014 pic.twitter.com/DPRKLIWZK3 — Qingqing Ouyang (@qqouyang) April 4, 2014 Most Entertaining!! #bvhackathon #bvh2014 pic.twitter.com/LTRG8Eu7o3 — Qingqing Ouyang (@qqouyang) April 4, 2014 RT @bchagoly : Awesome #bvhackathon great demos. Congrats winners @BazaarvoiceDev pic.twitter.com/1xjCQYDtre — Bazaarvoice (@Bazaarvoice) April 4, 2014 Closet to Reality!! #bvhackathon #bvh2014 pic.twitter.com/rZxmj5q7XI — Qingqing Ouyang (@qqouyang) April 4, 2014 People's Choice Award winners. Congrats!! #bvhackathon #bvh2014 pic.twitter.com/8DDN3lOfvY — Qingqing Ouyang (@qqouyang) April 4, 2014 Very cool product comparison ideas using bigram analysis at #bvhackathon pic.twitter.com/HdaHepi03W — Joseph Poirier (@joepeartree) April 4, 2014 CrushMeNot… #bvhackathon #bvh2014 pic.twitter.com/J1VqlFbzw3 — Qingqing Ouyang (@qqouyang) April 4, 2014 https://twitter.com/bvrecruits/status/452169125419184128 lots of Firebird engineers among the prize-winners at the #bvhackathon today. love my team. — Rebecca Murphey (@rmurphey) April 5, 2014 Had an amazing time at #bvio2014 – such a blast. Tons of fun to help so many teams. Thanks for a great time, @Bazaarvoice ! — Brian Sinclair (@brianarn) April 4, 2014 . @chestondev recaps last week's @Bazaarvoicedev #bvhackathon ! Feat. @mashery winning hack w/ @BBYOpen . W00t! http://t.co/NJ5RgtETUX — Mashery Developers (@MasheryDev) April 9, 2014", "date": "2014-04-11"},
{"website": "BazaarVoice", "title": "BV I/O: Nick Bailey – Cassandra", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2013/10/17/bv-io-nick-bailey-cassandra/", "abstract": "Every year Bazaarvoice holds an internal technical conference for our engineers. Each conference has a theme and as a part of these conferences we invite noted experts in fields related to the theme to give presentations. The latest conference was themed “unlocking the power of our data.” You can read more about it here . Nick Bailey is a software developer for datastax , the company that develops commercially supported, enterprise-ready solutions based on the open source Apache Cassandra database. In his BV I/O talk he introduces Cassandra, discusses several useful approaches to data modeling and presents a couple real world use-cases.", "date": "2013-10-17"},
{"website": "BazaarVoice", "title": "Jolt Command Line Interface", "author": ["Sam Kinard"], "link": "https://blog.developer.bazaarvoice.com/2013/10/11/jolt-command-line-interface/", "abstract": "Some of y’all may have caught our previous blog post announcing the release of our Java JSON transformation library, Jolt. Jolt is a powerful tool that can accomplish a variety of useful transformations on JSON data, and even chain multiple transformations together. Jolt has additional functionality that is useful for working with JSON including the ability to intelligently diff JSON documents and sort JSON documents. Users of Jolt can now transform, diff and sort JSON via the command line using the Jolt CLI. The CLI even allows you to string multiple commands together via standard in: The Jolt CLI supports the following sub-commands: The Jolt Diffy sub-command is an excellent way to compare JSON documents at the command line. It gives you a lot of friendly human-readable output, or you can have it run silently and examine the exit code to determine if any differences were found. Have you ever tried using diff to detect differences in a JSON document? Due to the nature of JSON data the regular diff command can sometimes be inadequate. For Example,  consider the following two JSON documents: diff1.json diff2.json Running the diff command from the command line returns the following: This really isn’t helpful. Since the example data is in the form of a map, then two documents are essentially equal. However, because the entries are ordered differently, diff detects differences. Diffy ignores the ordering of map entries: Diffy does a recursive tree walk to find differences throughout the JSON document, so it can detect differences N levels deep. diff3.json diff4.json Diffy does flag differences in array ordering. Consider the following two JSON documents: array1.json array2.json Diffy detects the differences in the array: If for some reason you are crazy (like some of us at Bazaarvoice) and you want to ignore array order, you can use the -a flag for those occasions. The Jolt Transform sub-command allows you to perform transforms on JSON documents provided via standard in or file. Transform also takes a spec, which is a JSON document that contains one or more Jolt specs to indicate what transformations should be done on the document. Transform has the option to produce the results with or without pretty print formatting. You can read more about Jolt transforms here . The Jolt Sort sub-command will sort JSON input. The sort order is standard alphabetical ascending, with a special case for “~” prefixed keys to be bumped to the top. This can be useful for debugging when you need to manually inspect the contents of a JSON document. Sort has the option to produce the results with or without pretty print formatting. That does it for today. Hopefully you have an idea of what the Jolt CLI does in broad strokes. If you’re curious about Jolt, you can read much more about it here .", "date": "2013-10-11"},
{"website": "BazaarVoice", "title": "BV I/O: Peter Wang – Architecting for Data", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2013/10/03/bv-io-peter-wang-architecting-for-data/", "abstract": "Every year Bazaarvoice holds an internal technical conference for our engineers. Each conference has a theme and as a part of these conferences we invite noted experts in fields related to the theme to give presentations. The latest conference was themed “unlocking the power of our data.” You can read more about it here . In this presentation Peter Wang , co-founder and president of Continuum Analytics , discusses data analysis, the challenges presented by big data, and opportunities technology provides to overcome those challenges. He also discusses the importance of performance and visualization as well as advances the concept of “engineering on principle” which he demonstrates by discussing the design of the A-10 Thunderbolt and SAGE computerized command and control center for United States air defense. Peter ends his talk by discussing the Python programming language and its suitability for data analysis tasks.  The full talk is below.", "date": "2013-10-03"},
{"website": "BazaarVoice", "title": "BV I/O: Dr. Jason Baldridge – Scaling Models for Text Analysis", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2013/09/26/bv-io-dr-jason-baldridge-scaling-models-for-text-analysis/", "abstract": "Every year Bazaarvoice holds an internal technical conference for our engineers. Each conference has a theme and as a part of these conferences we invite noted experts in fields related to the theme to give presentations. The latest conferences was themed “unlocking the power of our data.” You can read more about it here . The following video is of Dr. Jason Baldridge , currently an associate professor in the Linguistics Dept. at University of Texas and co-founder of People Pattern . Dr. Baldridge presented on the subject of text analysis. During his hour long talk he identified the desirable traits of a good text analysis function and focused on the problems of performing text categorization tasks given different amounts of labeled data. Big thanks to Dr. Baldridge for his informative presentation. The full talk is below:", "date": "2013-09-26"},
{"website": "BazaarVoice", "title": "Bazaarvoice developer portal moving to Mashery effective 3/1/13", "author": ["Craig Gilchrist"], "link": "https://blog.developer.bazaarvoice.com/2013/02/08/bazaarvoice-developer-portal-moving-to-mashery-effective-3113/", "abstract": "We are pleased to announce that on March 1, 2013, we are moving our developer portal hosting to Mashery. What does this mean to you, our developer community: We know that the increased security and faster key generation will enhance your development experience. As always, thank you for developing on the Bazaarvoice platform. We look forward to seeing your applications.", "date": "2013-02-08"},
{"website": "BazaarVoice", "title": "Conversations API Inspector", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2013/09/18/conversations-api-inspector/", "abstract": "Imagine you’re a developer working for Widgets n’More. The marketing team just came up with a new cross platform social media promotion. It’s going to involve collecting user generated content in the form of ratings and reviews. As luck would have it you remember your friend on the Ecom Team had mentioned working with a company called Bazaarvoice last year. Widgets n’More partnered with Bazaarvoice specifically for the purpose of collecting and displaying reviews. In short order you find yourself at developer.bazaarvoice.com where you start reading the Conversations API documentation . There are a lot of fields and parameters and content types. What’s more, some of them appear to be customizable. They offer custom rating fields, tags, context data questions, and additional free text fields. One company might have “rating_value” while another might be using “rating_quality”. It’s also not immediately clear how any those should be displayed in a webform. The fields can even have customizable properties like min and max length. So, you call your friend hoping she’ll be able to shed some light on the situation. She explains that Bazaarvoice can even configure fields based on content type, like reviews, questions or answers, and make different custom fields available depending on the parent category of a product. Unfortunately it’s been so long since the initial Bazaarvoice implementation that she doesn’t remember what was set up. If only there was an easy way for you to see exactly what fields are available taking all those factors into account… The Conversations API Inspector was created with the above scenario in mind. It is a web based app that shows what fields can be submitted to the Bazaarvoice platform using the Conversations API for any API key + content type + ID combination. With the Conversations API Inspector our imaginary developer would be able to see what fields are available, how they must be submitted in an HTTP request, meta-data about each field and much more The Conversations API Inspector is ready to use and publicly available at http://api-inspector.bazaarvoice.com/ . It is well documented at our Developer Portal, so instead of repeating that here I’ll leave you with some screenshots. Even without the Conversations API Inspector all would not have been lost for our imaginary developer. He could have used the API itself to determine what fields are available. In fact this is exactly how the Conversations API Inspector does it. Of course, the Inspector provides a much more user friendly and interactive GUI than the raw JSON or XML returned by the Conversations API. You can read more about how the inspector works at the documentation under the heading “How it works”.", "date": "2013-09-18"},
{"website": "BazaarVoice", "title": "Using the Bazaarvoice Conversations API to build a UGC template", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2013/09/11/using-the-bazaarvoice-conversations-api-to-build-a-ugc-template/", "abstract": "(This post is by Devin Carr , one of our Summer 2013 interns.) Working as a Developer Advocate intern on the Bazaarvoice Developer Relations team has been a great learning opportunity. At the beginning of the Summer I discussed with my mentors, Chas Peacock and Frederick Feibel, what I wanted to learn while interning. We decided on front-end web-development because it is an area in which I had minimal experience. For the rest of the Summer I spent my time creating a simple review display template using the Bazaarvoice Conversations API and some really cool front-end frameworks & libraries. I also researched other ratings and reviews implementations to learn about common industry best practices and incorporated them into what I am calling my LightReviewDisplay Template . Coming from a background limited to Java, I quickly discovered that Javascript is quite a different programming language. I took a few days to learn basic Javascript and HTML from Codecademy to get a grasp of the syntax and functions. Using that knowledge I created the first version of the LightReviewDisplay. I put all of the Javascript and dynamic HTML in one large file that handled loading the reviews and displaying them on the page. During my first code review Chas and Frederick suggested my code was lacking in structure and layout best practices, such as including an Object Oriented Programing model. Instead of most of my code being in just a few functions they recommended I implement RequireJS , self described as a “JavaScript file and module loader”, to encapsulate my code within a maintainable structure. They also suggested that I use Mustache , a HTML template library. Using these two libraries I could divide my code into multiple modules and avoid having large amounts of HTML embedded within my Javascript. The second version that I made was starting to look more like a modern and modular Javascript project. Chas and Frederick agreed that the increased readability of my refactored project was an improvement, but they felt it could be further improved by imposing more structure. They suggested I research some MVC (Model, View, Controller) frameworks. I looked into Backbone, KnockoutJS, Angular, and Stapes, all very versatile with separated modules for the model (data), the view (page), and the controller (user input). I ended up choosing Stapes because it is lightweight and is a very minimal framework that allows lots of customization. At this point the project was functional but the user experience needed improvement. I decided to do some research into how a product review page should actually look. I inspected several retailers to see what was common about their layouts and what would be the best way to integrate reviews onto a product page. They all followed a two part system: a summary section at the top of the page and a more detailed section further down. The summary section shows the product’s price, description and a review ratings summary. It typically had the average rating for the product, the total number of reviews, and a rating breakdown per-star. Then towards the bottom of the product page, there was a review module that held all of the consumer reviews. I combined what I had learned about layout with Twitter Bootstrap resulting in an efficient and usable template for displaying UGC. My Summer project was a great way to get started with Javascript and get familiar with the Bazaarvoice Conversations API . Throughout my internship I learned a lot about being a software engineer, expanded my CS knowledge and got some firsthand experience working amongst a small team of developers. This internship also gave me the opportunity to learn with the latest front-end libraries, architectural patterns, and work with really great developers. A special thank you goes out to Chas and Frederick for taking time out of their days to assist me with any problem I came across. More about the LightReviewDisplay Template: Bazaarvoice Inspiration Gallery: https://developer.bazaarvoice.com/inspiration_gallery/lightreviewdisplay Source: https://github.com/DevinCarr/LightReviewDisplay", "date": "2013-09-11"},
{"website": "BazaarVoice", "title": "Making of the Bazaarvoice SDK for Windows Phone 8", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2013/08/23/making-of-the-bazaarvoie-sdk-for-windows-phone-8/", "abstract": "Hi, my name is Ralph Pina, I am a Summer ’13 intern and UT-Austin Computer Science student. During this summer I had the privilege of working with another intern, Devin Carr, on Bazaarvoice’s .NET SDK for Windows Phone 8 and Windows 8 Store apps. Our goal was to provide convenient access to our Conversations API and make the app development experience even better. We want our customers’ developers to spend more time figuring out how to make better use of the data in our network and innovating on ways to increase engagement with their brands, products, and services. We currently provide SDKs for many other platforms including iOS , Android , so moving to cover the third largest mobile platform was a natural extension. As for Windows 8 Store apps, they are not as numerous or popular as traditional Windows Desktop apps, but as the install base for Windows 8+ devices grows, and developers get more accustomed to working with a touch interface on all their devices, their numbers should increase. This will provide an opportunity for first movers to grab a spot in million (billions?) of user’s Start Screen. We’ve got your back. We tried to implement best practices in our development. Below are some of the technical challenges we experienced: So you say there are better ways of doing this, or you want a specific implementation for your enterprise to use across various apps/brands/divisions? You are in luck! Our .NET SDK is open sourced and can be found on GitHub: https://github.com/bazaarvoice/bv-.net-sdk . So head over, hack it, maybe even submit a pull request to lay your claim to glory. While you are in GitHub, browse all the other awesome repos we’ve open sourced over the years. Below I have included some screenshots of a couple of sample apps that demonstrate how to use the .NET SDK to submit and display sample data from our API. Cheers, Ralph Pina", "date": "2013-08-23"},
{"website": "BazaarVoice", "title": "Bazaarvoice SDK for Windows Phone 8 has been Open Sourced!", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2013/08/21/bazaarvoice-sdk-for-windows-phone-8-has-been-open-sourced/", "abstract": "The Bazaarvoice Mobile Team is happy to announce our newest mobile SDK for Windows Phone 8 . It is a .NET SDK that supports Windows Phone 8 as well as Windows 8 Store apps. This will add to our list of current open-source mobile SDKs for iOS , Android and Appcelerator Titanium . The SDK will allow developers to more quickly build applications for Windows Phone 8 and Windows 8 Store that use the Bazaarvoice API. While the code is the same for both platforms, they each need their own compiled DLL to be used in their perspective Visual Basic projects. As Windows Phone 8 and Windows 8 Store apps gain more traction in the marketplace, we hope the Bazaarvoice SDK will prove a valuable resource to developers using our API. Learn more at our Windows Phone 8 SDK home page , check out our repo in GitHub, and contribute! The SDK was developed by two summer interns: Devin Carr from Texas A&M and Ralph Pina from UT-Austin. Go horns! In the next few days we’ll publish a more in-depth post about how the Windows Phone 8 SDK was built, some of design decisions and challenges.", "date": "2013-08-21"},
{"website": "BazaarVoice", "title": "BV I/O: Adrian Cockroft", "author": ["Chas Peacock"], "link": "https://blog.developer.bazaarvoice.com/2013/08/29/bv-io-adrian-cockroft/", "abstract": "As part of our internal BV I/O conference we’ve previously profiled on the blog, we had Adrian Cockroft , Cloud Architect at Netflix , come give us an overview of a lot of Netflix’s architecture as well as information on their multitude of open source projects and the ways Netflix is engaging the community to contribute. He also talked about some of his personal sources of inspiration when it’s come to things his teams have developed at Netflix. Big thanks to Adrian for taking the time out to come visit with us in Austin.  The full talk is below:", "date": "2013-08-29"},
{"website": "BazaarVoice", "title": "Maintenance Notice: Bazaarvoice Developer Portal", "author": ["Lee Goolsbee"], "link": "https://blog.developer.bazaarvoice.com/2013/08/12/maintenance-notice-bazaarvoice-developer-portal/", "abstract": "We’ll soon be giving our Developer Portal some love in the way of functionality and style updates.  To facilitate this, we’ll be taking the portal offline for approximately 6 hours on Thursday, August 15th; the estimated times are listed in a few timezones below: Bookmarks to existing documentation will work through at least part of the downtime, but please excuse any styling inconsistencies as we work through the update. If you need something quickly, tweet us @BazaarvoiceDev and we will do our best to get you the assistance you need. Thanks for your patience!", "date": "2013-08-12"},
{"website": "BazaarVoice", "title": "Summer 2013 Intern Science Fair", "author": ["Frederick Feibel"], "link": "https://blog.developer.bazaarvoice.com/2013/08/08/summer-2013-intern-science-fair/", "abstract": "Every year at Bazaarvoice we bring on a new class of Summer interns and put them to work creating innovative (and educational) projects. At the beginning of the Summer interns choose from a list of projects and teams that interest them. From there they are embedded in a team where they spend the rest of the Summer working on their chosen project with help from seasoned professional mentors. At the end of the Summer they set up demos and we invite the whole company to visit with them and see what they have accomplished. Read on to learn about our interns and their fantastic and innovative projects. Below are some of the 2013 intern science fair projects in the creators own words: Devin Carr School : Texas A&M – Field of study : Electrical Engineering I built a product review template using the Bazaarvoice Platform API as well as some of the latest front-end libraries and architectural patterns. It provides a best practice and simplistic model to assist potential/existing clients and their developers with a base to develop an efficient product page integrated with Bazaarvoice Ratings & Reviews. Lewis Ren School : UC Berkeley – Field of study : Electrical Engineering & Computer Science Genie is a product recommendation tool based on user clustering. The idea is that you want to give every user the possibility of a product within the network as a recommendation; however, be able to rank them in such a way that is most relevant to the specific user. By clustering users based on modularity we can determine how big of an impact a specific user will have on the rest of the network. For example, a high modularity node that makes a decision will have a stronger impact on its neighbors, but will propagate out slower and have a small to negligible effect on outer nodes. Similarly, a low modularity node that makes a decision will have a lesser impact, but will radiate outwards much more quickly. Matt Leibowitz School : University of Dallas – Field of study : Physics / Computer Science My project is called Flynn, which is a web page for accessing documents out our data store. Flynn allows developers to bypass a fairly long and involved process (up to ten minutes of looking up documentation and performing curl requests) with a quick and simple web interface. Perry Shuman My project is a bookmarklet built to provide debug information about a page with injected BV content: information that could help a client fix implementation issues that they might have, without having to know the inner workings of our code. It also allows previously hidden errors to be displayed to a client, giving them a first line of information to debug broken pages. Ralph Pina School : UT Austin – Field of study : Computer Science “Over this past summer I interned with Bazaarvoice’s mobile team. In its effort to make it easier to integrate BV’s services into every platform and outlet their clients use it has build and distributed mobile SDKs (Software Development Kits) to display and submit data to the BV API. I worked to improve the Android SDK, and along with another intern, Devin Carr, wrote a new Windows Phone 8 SDK. I also wrote 2 Windows Phone 8 sample applications to demonstrate how to implement the SDK in an app. Afterwards, we opened sourced all our mobile SDKs under the permissive Apache 2.0 license.” Rishi Bajekal School : University of Illinois at Urbana-Champaign – Field of study : Computer Science Built a RESTful web service for serving photos from the data stack and worked on the Conversations 2013 API as part of the that team. Ryan Morris School : McCombs School of Business at the University of Texas at Austin – Field of study : Management Information Systems My project’s theme was predictive modeling and focused around using a client’s web metrics, review count, and average rating to predict orders for products. To create the predictive model, I pulled the client’s visits, orders, and revenue for a certain time period from their web analytics account. I then pulled the client’s number of approved reviews and average rating for each product from the Bazaarvoice database using mySQL. Finally, I ran the data in the statistical program R to come up with a predictive model. With this model, I was able to analyze the effects that varying review count and average rating had on predicted orders for a product. Schukey Shan School : University of Waterloo – Field of study : Software Engineering My project was the brand search summary, which provides both an API and a UI to summarize data we collect on brands. For example, if we search for a brand “Acme”, the API looks for all the brand names that contain the word “Acme”, creates a composite of the matched brand values, and returns the count for pageviews, ugc mpressions, and unique visitors for each client of each brand value in the composite. This would be useful as an overview of how brand values look like over the network. Steven Cropp School : Rensselaer Polytechnic Institute – Field of study : Computer Systems Engineering and Computer Science This Summer I worked on an xml file validation service. Users can set up “rules” that dictate how xml files must be formed, and then apply those rules to our clients xml feeds to generate user friendly reports that outline what is wrong with the xml, and some details about how to fix any issues. This should help smooth out the process our clients go through when importing feeds, saving the support team and our clients time. Thomas Poepping School : Carnegie Mellon University – Field of study : Computer Science 1. A tool to inject spreadsheets of UGC into the system. 2. An addition to the web app that generates a payroll report for all moderators, streamlining the payroll process. 3. A pilot WordPress plugin that moderates WordPress comments, then allows administrators to act on rejected comments.", "date": "2013-08-08"},
{"website": "BazaarVoice", "title": "Project Lassie: who let the dog out!", "author": ["Cory Thomas"], "link": "https://blog.developer.bazaarvoice.com/2013/08/14/project-lassie-who-let-the-dog-out/", "abstract": "The Bazaarvoice Platform Infrastructure Team recently open sourced project Lassie. Lassie is a Java library that can manipulate the new DataDog screenboards . The Lassie library can create, get, update, and delete the DataDog screenboards via the REST API. We use DataDog across various teams to collect metrics at both a system-wide and application level to give our teams a clearer view of what’s happening across all environments. The project was developed by a Bazaarvoice summer intern, Mykal Thomas. Mykal is a senior at Georgia Tech. Check out the Github for more information: https://github.com/bazaarvoice/lassie Documentation for DataDog’s Screenboard API: http://docs.datadoghq.com/api/screenboards/", "date": "2013-08-14"},
{"website": "BazaarVoice", "title": "BV I/O: Imagination unlocked", "author": ["Bryan Chagoly"], "link": "https://blog.developer.bazaarvoice.com/2013/07/31/bv-io-imagination-unlocked/", "abstract": "What do you get when you lock 100+ engineers, product managers, designers and other techies in a building for 2 days and ask them to come up with new and creative ways to “unlock the power of our data”? Well, I could tell you, but then I would have to… yeah that’s top secret awesome product roadmap stuff now. (and even redacted) As an extension to our BV.IO internal tech conference that I recently blogged about, we held an engineering wide Hackathon for everyone in our technical community to go nuts with our data and try to come up with some of the next big ideas for Bazaarvoice. We had over 100 folks participate, form teams of 3, and after 2 days, we had 31 really cool prototypes that they demo’d to the entire company. It was such a great experience to see so many smart and passionate people singularly focused on innovation and building some cool new ideas and value for Bazaarvoice. Here is a quick summary of how things went down: Tuesday = BV.IO tech conference & speakers, present Hackathon ideas Wednesday = Form teams & brainstorm Thursday = Hackathon & XBOX Call of Duty: Black Ops 2 tourney Friday = Pancake breakfast, Hackathon, Demos, Prizes Saturday = Sleep We started the hackathon as a continuation of the BV.IO event at the Alamo Drafthouse, where we had anyone with a proposal come on stage and try and sell the idea. Think about it like trying to sell your idea to a group of engineers to come work with you on your startup idea. After we heard all the interesting ideas, everyone went off and self formed teams, and started brainstorming. On Thursday, we kicked off the Hackathon, and it was eerily quiet in engineering. Everyone split off into small teams and was heads down coding or held up in a breakout room whiteboarding designs. We had tons of food and snacks brought in for breakfast, lunch and dinner to keep everyone energized, and by the end of the day everyone had made some amazing progress and were ready to blow off some steam…and by “blow off”, I mean blow up, and by “steam”, I mean COD:BO2. We set up 8 portable flat screen monitors and 8 Xbox 360s, and we got our game on. It was so simple and so much fun, I don’t know why we hadn’t done this earlier. It was a huge hit, and we are thinking about how we can keep that set up all the time. Everyone self rated their skill level, and we balanced teams for a round robin Call of Duty: Black Ops 2 tournament. Friday morning, all the managers got together to show our appreciation for the team with pancakes and bacon. I really think there is no better way to show your appreciation than with bacon. “That’s way too much bacon”‘, said no one ever… Coding continued throughout the day, and at 3pm it was pencils down, and time for demos. The company filled the All Hands, and it was rapid fire through 31 demos. We even had a really slick Google Glass hackathon project. The energy was awesome, the ideas were awesome, and the conversations it inspired across the entire company was awesome. After all the teams had demo’d, the company voted, and winners across several categories were selected. We had a few cool prizes for the winners like iPad Minis, Parrot Drones, Rokus, cash money and of course totally custom Lego trophies. If you haven’t done a full on Hackathon at your company in a while, I highly recommend it. Every time we do it here, I am amazed by the creativity, the innovative ideas and solutions that are created in such a small time. And the ripple effect that happens from that continues for months as the business internalizes the ideas and roadmaps and direction start to change based on those ideas. The key is to not let the ideas die on the vine. Champion them, advocate them, and push them forward, and you too can change the world one authentic conversation at a time. This hackathon is CTO approved.", "date": "2013-07-31"},
{"website": "BazaarVoice", "title": "SQL, NoSQL,… What’s now? New SQL", "author": ["Igor Polishchuk"], "link": "https://blog.developer.bazaarvoice.com/2013/07/11/sql-nosql-whats-now-new-sql/", "abstract": "It has not been long since the holy war between SQL and NoSQL database technologies faded, and now we see a new contender, NewSQL , rising. What is it? Will it cause another round of the war? Recently at Bazaarvoice we hosted an informational session on VoltDB , one of the better known NewSQL solutions, with several engineers and technical managers from Austin and San Francisco offices participating in that session.  The question we needed answered: what is VoltDB and why it might be an interesting datastore technology for us? In short, it may be very good for real-time and near real time analytics, where SQL and ACID compliance are desirable. Personalized ad-serving, marketing campaign real-time effectiveness measuring, and electronic trading systems are some of the reference applications that VoltDB provides. VoltDB is an in-memory database, which makes it extremely fast. However, this is just a small portion of the story. Besides residing in memory, VoltDB has a few performance improving architectural solutions based on research by well known database technologists, including the famous Michael Stonebraker , who was involved in the creation of Ingres, Postgres, and Vertica. The creators of VoltDB wanted to preserve all the good features of a traditional RDBMS like SQL, ACID compliance, and data integrity, but they also wanted to drastically improve performance and scalability. All the modern commercial and open source RDBMS are built on the same principles, which were created more than 40 years ago for the era of small memory and slow disks. The researchers analyzed the bottlenecks of a traditional RDBMS and found that at high load about 88% of the server capacity is wasted on the traditional RDBMS overheads and only about 12% of the capacity is used for doing  actual useful work. VoltDB’s architectural solutions eliminate the traditional RDBMS overheads: These architectural solutions allow combining all the advantages of a traditional RDBMS with scalability features usually associated only with NoSQL databases: automatic sharding across a shared-nothing cluster, eliminating many overheads, automatic replication and log-less durability for high availability. With these features, VoltDB claims to be one of the fastest databases on the market today. VoltDB’s impressive performance is illustrated by the results of the TPC-C-like benchmark below, in which VoltDB and a well known OLTP DBMS were compared running the same test on identical hardware (Dell R610, 2x 2.66Ghz Quad-Core Xeon 5550 with 12x 4GB (48GB) DDR3-1333 Registered ECC DIMMs, 3x 72GB 15K RPM 2.5in Enterprise SAS 6GBPS Drives): So, will the rising NewSQL technology cause another religious database war? Probably not. VoltDB positions their database as a niche database doing a few things really well. It doesn’t try to be a one size fits all database, and VoltDB’s philosophy is “an organization should use a few datastore technologies, using each for the case where it plays the best.” For example, you cannot use VoltDB if your data does not fit into the combined memory of your cluster.  Long leaving transactions are also not supported on VoltDB. Hopefully, if some team has a need for a very fast but consistent, ACID and SQL compliant database for a new project, they would consider VoltDB as as an option.", "date": "2013-07-11"},
{"website": "BazaarVoice", "title": "BV I/O: Unlocking the power of our data", "author": ["Bryan Chagoly"], "link": "https://blog.developer.bazaarvoice.com/2013/07/01/bv-io/", "abstract": "At Bazaarvoice, we strongly believe that our people are our most important asset. We hire extremely smart and passionate people, let them loose on complex problems, and watch all the amazing things they create. We had another opportunity to see that innovation engine in full effect last week at our internal technical conference and 2 day hackathon. Every year we hold an internal technical conference for our engineers and technical community. If you are lucky enough to have been at Bazaarvoice, you remember our conference last year called BV.JS which was all about front end UI and javascript, and in years’ past we did Science Fairs. Last year at BV.JS we were focused on redesigning our consumer facing ratings and reviews product (Conversations) so we gathered some amazing javascript gurus such as Paul Irish ( @paul_irish ), Rebecca Murphey ( @rmurphey ), Andrew Dupont ( @andrewdupont ), Alex Sexton ( @SlexAxton ) and Garann Means ( @garannm ) to school us on all the latest in javascript. This year our event was called BV.IO and we are focused on “unlocking the power of our data”, so we asked some great minds in big data analytics and data visualization to come inspire our engineering team. The event kicked off with a day at the Alamo Drafthouse . Bazaarvoice is powered by tacos, so of course there were tons of breakfast tacos to get us ready for a fun filled day of learning and mind opening presentations, and a colorful pants competition, but I digress and will get to that in a minute. First up was Adrian Cockcroft (‪ @adrianco‬ ), cloud architect from Netflix. We are big fans of Netflix’s architecture and we use and have added to several of their open source packages . Some of the projects we use are Curator, Priam and Astyanax. Adrian gave us an update on some of the new advancements in Netflix’s architecture and scale as well as details on their new open source projects. Netflix is also running an open source competition called NetflixOSS and they have some cool prizes for the best contributions to their projects. The competition is open until September 15, 2013, so get coding. Jason Baldridge (‪ @jasonbaldridge‬ ), Ph.D. and associate professor in Computational Linguistics at the University of Texas, presented on scaling models for text analysis. He shared some really interesting insights into things that can be done with geotagged, temporal, and toponym data. Nick Bailey (‪ @nickmbailey‬ ), an engineer at DataStax, presented on Cassandra best practices, new features, and some interesting real world use cases. And Peter Wong (‪ @pwang‬ ), Co-founder and President of Continuum Analytics, gave a really entertaining talk about planning and architecting for big data as well as some interesting python packages for data analysis and visualization. Ok, and now back to the most important aspect of the day, the Colorful Pants Competition. Qingqing, one of our amazing directors of engineering, organized this hilarious competition. Can you guess who was the winner? We really enjoyed all the speakers, and we know that you will too, so we will be sharing their presentations on this blog in the coming days and weeks. Check back regularly for the videos.", "date": "2013-07-01"},
{"website": "BazaarVoice", "title": "Jolt released to the world", "author": ["Milo Simpson"], "link": "https://blog.developer.bazaarvoice.com/2013/06/28/jolt-released-to-the-world/", "abstract": "We are pleased to announce a new open source contribution, a Java based JSON to JSON transformation tool named Jolt . Jolt grew out of a BV Platform API project to migrate the backend from Solr/MySql to Cassandra/ElasticSearch.  As such, we were going to be doing a lot of data transformations from the new ElasticSearch JSON format to the BV Platform API JSON format. Prior to Jolt, there were 3 general strategies for doing JSON to JSON transforms : Those options were rather unpalatable, so we went with option “4”, write reusable custom code. The key insight was that there are actually separable concerns when doing a transform, and that part of the reason the XSLT or template approaches are unpalatable, is that they force you to deal with them all together. Jolt tackles each separate concern individually : The code is now available at Github , and jar artifacts are now being published to Maven central.", "date": "2013-06-28"},
{"website": "BazaarVoice", "title": "How Bazaarvoice Weathered The AWS Storm", "author": ["Ernest Mueller"], "link": "https://blog.developer.bazaarvoice.com/2013/06/22/how-bazaarvoice-weathered-the-aws-storm/", "abstract": "Greetings all! In the world of SaaS, wiser men than I have referred to Operations as the “Secret Sauce” that distinguishes you from your competition. As manager of one of our DevOps teams, I wanted to talk to you about how Bazaarvoice uses the cloud and how we engineer our systems for maximum reliability. You may have heard about the AWS Storm and the Leapocalypse , two events that made the weekend of June 29th last year a sad one for many Internet companies. Electrical storms in the Northeast knocked out one of Amazon Web Service’s availability zones in their US East region Friday night, knocking many services off the air (Netflix, Mozilla, Pinterest, Heroku, LinkedIn, Foursquare, Yelp, Instagram, Reddit, and many more). Then on Saturday a “leap second” caused Java virtual machines across the planet to freak out and peg CPUs. Guess what two technologies we use heavily here at Bazaarvoice? That’s right, Amazon Web Services and Java. Here’s a great graph from alerting service PagerDuty showing the impact these two events had across the Internet: But here’s the Keynote graph we use to continually monitor our customer facing services for the same time period: It’s really five different graphs for a set of our major customers overlaid, but it’s hard to tell because they are all flatlined on top of each other. That’s right – we had 100% availability on all our properties for the entire crisis period. Were we “untouched?” By no means. We lost 77 servers, 37 of which were production servers, during this time. But by architecting for resilience, we have constructed a system to avoid customer impact even when an outage hits our systems like a shotgun blast. As you know from previous blog posts, we’re a big Solr and mySQL shop. We shard our customers into distinct “clusters” for scalability (we’re up to seven). But then each cluster is mirrored into the AWS East region, AWS West region, and Rackspace. Inside each region, we make use of multiple availability zones and levels of load balancing (haproxy in the cloud, F5 in Rackspace). Here’s the view inside one region (1/3 of a cluster): Then we use Neustar GTM for DNS-based traffic balancing across all three parts of the cluster in AWS East, AWS West, and Rackspace. This means we can lose zones within a region, or even a full region, without downtime – though in a case like this, we definitely had to expand our capacity in AWS West when AWS East went down so that we wouldn’t have performance issues, and we did have engineers working over the weekend to clean up the “debris” left over from the outage. We are working on engineering our clusters to dynamically scale up and clean up behind themselves to avoid that manual work. But what about the data, you ask? Well, the other key to this setup is that we use a message queue for all writes instead of writing synchronously to the database. This gives us a huge amount of flexibility in how and where we process them – we use a master/slave relationship for each cluster where the mySQL and Solr are mastered out of Rackspace, but with that architecture if Rackspace were completely down all that does is delay content submission – nothing is lost and the user experience is still good. This architecture also allows us to scale quickly and gives us a lot of control over shifting traffic around to meet specific challenges (like Black Friday). More on that in a later post, but I hope this gives you some insight into how we make our service as reliable as we can!", "date": "2013-06-22"},
{"website": "BazaarVoice", "title": "Stop, collaborate and innovate: Engineering a company science fair", "author": ["RC Johnson"], "link": "https://blog.developer.bazaarvoice.com/2011/08/31/stop-collaborate-and-innovate-engineering-a-company-science-fair/", "abstract": "Many people are aware of Google’s “ 20 percent time ,” and the number of innovations that have been produced because of it (Gmail, Google News, and AdSense to name a few). Several other companies have tried this before and after Google; one of the most famous examples of side-project innovation is 3M’s Post it Note. Software development companies are now emulating this model, holding hack-a-thons where flurries of innovation occur.At Bazaarvoice we’ve been holding regular Science Fairs every few months for well over the last year to allow our Engineering team to stretch their creative muscles. The format is simple; we schedule the two-day Science Fairs at times that are not likely to have a lot of conflicts with the regular release cycle, typically over a Thursday and a Friday. When the engineers arrive on Thursday, instead of working on the newest features in our evergreen platform, they get to choose what they want to work on. Teams of engineers (and designers) from across our Implementation, Development, Operations, and Support teams work together all day Thursday—often late into the night—and half of the day Friday to bring their ideas to life. After lunch on Friday the teams present their projects to a panel of judges and finalists, and winners are announced. These types of events are an amazing addition to the company’s culture, and really help to drive new innovations into our products. Engineers walk away from the two days of coding with a new sense of empowerment, realizing just how much they can accomplish in such a short period of time. Additionally, many of the projects stretch the bounds of what our products can do already, showing us new areas for efficiency improvements, incredible new user experiences, or just plain fun new ways of looking at the same data. Having done this for a year, we’ve learned a few things along the way about how to run a successful Science Fair. Here are my top three tips. 1. Get thematic We’ve learned that providing some structure to the projects helps to give people direction. We’ve published a list of themes for the last two Science Fairs, and that has helped not only with the task of judging all of the projects by dividing it into manageable chunks, but has also been a way to help engineers discover projects that they are passionate about. 2. Don’t be afraid to show it off Second, it is critical that participants be able to see as many of the projects as possible at the conclusion of the Science Fair. We still have room to improve on this, but including a Science Fair Open House the Monday after the fair provides everyone with a chance to see what their peers were able to accomplish. 3. Level set Finally, it is worthwhile noting that you should not expect to get new features or products out of every event. It is much more valuable to keep the process very open and to allow everyone to enjoy themselves rather than feeling like they have to produce the next great thing.", "date": "2011-08-31"},
{"website": "BazaarVoice", "title": "Making sure pages still look good when Facebook inevitably goes down", "author": ["Alex Sexton"], "link": "https://blog.developer.bazaarvoice.com/2011/09/13/making-sure-pages-still-look-good-when-facebook-inevitably-goes-down/", "abstract": "I’m a technical guy, so this is a mostly technical post, but hopefully that’s why you’re reading it. At Bazaarvoice, especially in Labs, we do a lot of work with the Facebook APIs. This can be challenging, because Facebook isn’t exactly known for it’s great uptime or documentation, but it’s also very rewarding to be able to pull in rich social information so easily. One of the original Facebook-related things that we did across a large group of clients (at least since I’ve been here) was to add the Facebook “Like” button to client’s product and category pages. The integration that Facebook gives is simple, and it seems easy enough, but there’s quite a lot of work involved in safely integrating the like button while still considering uptime, page performance and meta-data. Most specifically, we quickly ran into a problem with Facebook uptime. Since the like button is essentially implemented via a cross-domain iframe, there is little information you can gather, and little you can do with JavaScript to try to watch things unfold. Things have certainly gotten better since the early days of the Like Button. Load times are better, and the entire Facebook infrastructure feels just a little more stable, though you might say that we felt that way as well the first time Facebook went down and left ugly error messages in the middle of many of our high-profile clients’ pages. It was actually fairly interesting going around the internet the day that Facebook was down for a long period of time. I believe the issue ended up being a configuration failure, and Akamai was returning server errors, though I’m not sure they ever officially said anything. I do know that instead of Like Buttons showing up on pages, boxes containing big, bold, “service unavailable” messages were being injected all over the place. This occurred on big and small sites alike, and there seemed to be nothing we could do about it. Well, as a company that serves as high quality, and as high quantity of customers as we do, it behooved us to try and figure out a way to let something like this fail more gracefully. So a meeting was born. I’m sorry to say to all the meeting-haters that a solution was also born that day too. One that I’m quite fond of. We went through some of the code for the asynchronous injection of the FB Like Button. It seemed as if a URL was built, an iframe was injected, and then a size was chosen. Even though Facebook was down, people using the Asynchronous ‘injection’ version of the Like Button were still mostly affected. This is because this file is highly cached, and very popular. As long as someone had visited any site with Facebook integration (there are a few…), the script was likely cached and running regardless of the actual server status. Then, when the script finally decided to load in actual data, the iframe that it injected was filled with the resulting error. This meant that we had to rely on things that we knew didn’t have to phone home to Facebook, or rather, that if something odd happened after phoning home to Facebook, then we’d know we’re on the right track. We took a look at the events that the Facebook SDK would fire and noticed that there was an event that happened directly after the iframe was injected into the site. There was also code that followed it to determine the correct height for the widget. So all we had to was set the initial height to 0 after the iframe was injected, and then allow the FB to set the correct height afterwards. This worked great. If the inner-page of the FB Like iFrame never loaded the javascript that determined the height of itself (like when the server returns error messages instead of markup and code), then it could never relay the correct information to resize its height to the correct proportion. This meant that we could effectively hide the Like Button until we could guarantee that the server had responded with working code. Here’s the snippet we used: Again, this stops the Like button from ever appearing if the Facebook servers are down. It’s a nice solution for a problem that rarely happens, but is important to handle well when it does. I’d encourage you to consider not only third party security, but also third party uptime when talking to providers. Facebook, while extremely useful, has had a tendency to go under as soon as you think they won’t again. Your clients don’t end up caring why there are error messages on their pages, so it’s the duty of the league of implementors to tackle problems like this one, and make sure that none of their clients have issues.", "date": "2011-09-13"},
{"website": "BazaarVoice", "title": "What I learned from the Etsy CTO-turned-CEO", "author": ["Joah Spearman"], "link": "https://blog.developer.bazaarvoice.com/2011/09/22/what-i-learned-from-the-etsy-cto-turned-ceo/", "abstract": "I’ll be the first to tell you that my background isn’t at all in engineering or software development. I’ve spent much of the last eight years in business advisory, communications consulting and operations roles. Still, one of the best things about working at Bazaarvoice has been working with and learning from highly-talented technology leaders. Mike Svatek, our chief product officer, Scott Bonneau, our VP of Engineering, and Jon Loyens, who leads BV Labs, are just a few of the people I’ve had the opportunity to work with in my first four months here. But it’s not just the management team here at Bazaarvoice that has taught me plenty about the R&D functions of a tech company; last month I attended an event in New York (hosted by First Round Capital) where the CTO-turned-CEO of Etsy, Chad Dickerson, spoke about building a world-class dev organization. Going from CTO to CEO is not a traditional route even in today’s technology startup environment, but Chad stands out for making the transition amidst Etsy’s rapid growth. Under his watch, the company’s dev team grew from 20 to 80 engineers in around a year’s time and has seen page views go from 200 million to one billion per month. Talk about hyper-growth! And what exactly did I learn from Chad’s talk? Well, for starters, he’s a big-time fan of Peter Drucker, one of the top business minds of the last century, who is credited with having shaped much of today’s common management theory and executive MBA programs. On several occasions, Chad quoted Drucker, including one of his most famous lines: “Culture eats strategy for breakfast.” The culture Chad spoke of began with continuous deployment. He was very much in opposition of traditional “releases”, QA, lengthy and multi-layered “sign off” processes and having a single individual tasked with being the official release engineer. He cited each as reasons for delays in innovation and improvements to features that customers want more and more as the company grows, stating, “as you get bigger, the demands for new features goes up.” He shared that one of the job duties for every new engineer is to release code on their very first day on the job and agrees with Clay Shirky that, “process is an embedded reaction to prior stupidity.” However, whether or not Etsy’s development policies and practices are directly related to the systems we have in place at Bazaarvoice wasn’t as important to me as the way Chad spoke about promoting an engineer-friendly, technology-driven culture. After all, our business models are very different. The culture piece, though, was of particular interest because here at Bazaarvoice we very much embrace many of the same concepts Chad spoke of when referring to Etsy. At one point, Chad was asked a question about recruiting and he said, flat out, “Do what it takes to hire super stars.” Well, funny he should say that because every week it seems, we’re bringing on first-rate talent for our dev team, from proven technology business leaders to kick-ass coders. The culture here, the culture that enables us to be the perennial “Best Place to Work in Austin”, starts with people and top-notch talent. It also involves optimizing for developer happiness, something Chad mentioned, by finding ways to give our people the satisfaction of finishing a job. Drucker is the originator of that last statement, too. As a self-described risk-taker, I loved what Chad had to say about this quality, again quoting Drucker: “People who don’t take risks generally make about two big mistakes a year. People who do take risks generally make about two big mistakes a year.” Chad said, “It’s better to be aggressive and make mistakes than be tentative and make mistakes.” Blameless post mortems, Chad said, was the key to creating an environment where small production changes are made daily will certainly lead to minor mistakes, but that small corrections typically address these issues. The tendency to pull back and slow things down was not their M.O., but rather to “roll forward” to progress. I’m a big fan of that mindset. One of the last things Chad spoke about was doing things to make engineers heroes within the business, which is something customarily reserved for sales guys who bring in the revenue. He said that one of his 2011 goals as CEO is to have every engineer to blog for Etsy, significantly contribute to an open source project or speak at a conference to continue the company’s generosity-filled culture. After having the privilege of hearing from Chad for two hours, I became doubly enamored with the technology leaders and the team they’re managing here at Bazaarvoice because many of these principles, even if not pulled directly from a Peter Drucker book, are applied in our business. These were the four keys to dev team success listed by Chad. I haven’t had many sleepless nights since joining Bazaarvoice four months ago, and I’m comfortable saying the main reason is because the other three keys are top of mind every day.", "date": "2011-09-22"},
{"website": "BazaarVoice", "title": "Why Columns are Cool", "author": ["Alex Pinkin"], "link": "https://blog.developer.bazaarvoice.com/2011/10/04/why-columns-are-cool/", "abstract": "Near real time analytics on a large data set = hard! About two years ago we found ourselves adding a lot of analytical features to the Bazaarvoice Workbench, our client facing analytics product. We were implementing various new dashboards, reports, and alerts. Dashboards needed to be near real time, so we were forced to either find a way to run required SQL queries in real time, or pre-compute required data. As the main data store was MySQL, we found it impossible to aggregate large amounts of data on the fly in real time, so we were forced to pre-compute a lot of aggregates. Pre-computation happens to be error prone, so we eventually went looking for other solutions which would not require us to do as much pre-calculation. As we looked at the queries which we were running to pre-compute aggregate data, we spotted a common theme, i.e. most of the queries filtered and aggregated data. In the select clause of the query there would normally be a small number of columns with most values being aggregates such as counts, sums and averages. So we went brainstorming what type of database would potentially be a better fit for such queries compared to a traditional row oriented relational DBMS. We looked at a number of different data stores which could work for the problem we had. One of the alternatives considered was Apache SOLR which is a great NoSQL search server we had been using for a long time already with great success. SOLR supports filtering and faceting which allows us to implement most things we were doing with MySQL using SQL. However, there is quite a bit of dev work which needs to go into indexing data in SOLR and querying it. Enter the world of column oriented databases Imagine a database which stores the data by column rather than by row. For example, we might want to store the following employee data in a table: ID, Name, Department, Salary 1, Chuck Norris, DEV, 10000 2, Gate Bills, SALES, 20000 3, Bart Simpson, HR, 15000 Traditional row oriented databases would store column values for each row sequentially, hence the data is stored like this: 1, Chuck Norris, DEV, 10000 2, Gate Bills, SALES, 20000 3, Bart Simpson, HR, 15000 A column oriented database serializes values of each column together: 1,2,3 Chuck Norris, Gate Bills, Bart Simpson DEV, SALES, HR 10000, 20000, 15000 This approach has some advantages over row oriented databases: Getting to know Infobright One of the column oriented databases we evaluated is Infobright which is an open source database built on MySQL by changing the storage engine to be column oriented. Of course a significant change to storage layout also means that Infobright had to replace MySQL’s query execution engine with a custom engine capable of taking advantage of the columnar storage model. So, let’s jump straight to test results we got when evaluating Infobright using a data set with 100MM records in the main fact table. As you can see, the average query execution time for analytical queries was 20x faster than MySQL’s. Additionally, the disk footprint was over 10x smaller compared to MySQL due to data compression. Also, we found that Infobright supported most of the SQL syntax we were already using in our queries including outer joins, sub-queries and unions. We found that a little tweaking was still required to make some queries perform well. The benchmark numbers were impressive enough that we ended up using Infobright to implement near real time analytics in one of our products. It does a great job at calculating aggregates on the fly so we no longer needed to maintain as many aggregate tables (for instance, daily, and monthly aggregates). Infobright – the secret sauce What is the secret sauce in Infobright? First, its column oriented storage model which leads to smaller disk I/O. Second, its “knowledge grid” which is aggregate data Infobright calculates during data loading. Data is stored in 65K Data Packs. Data Pack nodes in the knowledge grid contain a set of statistics about the data that is stored in each of the Data Packs. For instance, Infobright can pre-calculate min, max, and avg value for each column in the pack during the load, as well as keep track of distinct values for columns with low cardinality. Such metadata can really help when executing a query since it’s possible to ignore data packs which have no data matching filter criteria. If a data pack can be ignored, there is no penalty associated with decompressing the data pack. Compared to our MySQL implementation, Infobright eliminated the need to create and manage indexes, as well as to partition tables. Infobright – Limitations Infobright Community Edition is the open source version of Infobright. Unfortunetly, it does not support DML (inserts, updates, and deletes), so the only way to get data loaded is bulk loads using “LOAD DATA INFILE …” command. It’s still possible to append data to the table, however there is no way to update or delete existing data without having to re-load the table. For some types of data (such as log files or call detail records), this is not a significant issue since data is rarely edited or deleted. However, for other projects this limitation may be a show stopper unless the data set is small enough when it can be periodically re-loaded into Infobright. Infobright chose to take the SMP (Symmetric MultiProcessing) approach, so there is no built-in support for MPP (Massively Parallel Processing). This certainly limits the size of the databases for which query performance would be acceptable. However, it’s possible to shard the data across multiple Infobright instances and then aggregate the results. Shard-Query is an open source project which makes it possible to query a data set partitioned across many database instances, and our test results confirm that this approach works really well. Summary Column oriented databases proved to be a great fit for analytical workloads. We managed to eliminate the need to maintain a large number of aggregate tables by migrating our data warehouse from MySQL to Infobright. This in turn let us support near real time visualizations in our analytics products. Thanks, this is useful information about column-oriented databases.", "date": "2011-10-04"},
{"website": "BazaarVoice", "title": "Can’t get into your favorite conference? No problem. Just create your own!", "author": ["Brian Showers"], "link": "https://blog.developer.bazaarvoice.com/2011/10/20/cant-get-into-your-favorite-conference-no-problem-just-create-your-own/", "abstract": "One of the best things about working in Austin is the sheer number of great software companies in town. From web-based start-ups to established enterprise software giants, there is huge diversity in the locally available business models, technology stacks, market sectors, and experience levels. It’s a good bet that no matter what technology problem that you’re facing, a few Austin companies have already found, built, or bought a solution before you. About a year ago, I was having dinner with a friend who happens to work at another local software company. Throughout dinner we discussed a couple of different infrastructure technologies that our teams had recently built, some problems we were facing, and we shared ideas that might help solve some of those problems. We each walked away not only with ideas that might help solve our upcoming challenges, but also with a list of things that had already been tried and abandoned. While we each have intellectual property that we aren’t at liberty to share, that line leaves a ton of room for technology and ideas that our companies don’t wish to protect as IP. The big remaining question was “How do we do this again, and how do we get more of our team members learning from each other?” Based on that experience, a few months later Bazaarvoice hosted our first ‘tech talk’ for the outside community. We invited half a dozen other companies from around Austin to come and learn about our architecture and some technologies that we had built that might be useful to others. Specifically, our agenda for the night was: 1. An overview of Bazaarvoice for those in the audience that didn’t know us (5 min) 2. An overview of our application architecture to support scale and uptime (45 min) 3. An infrastructure technology to easily configure Spring for multiple environment types (20 min) 4. A technology to supporting backwards compatibility and versioning in a REST API (20 min) In return, the attending companies agreed to host their own talks at some point in the future. Fast forward a year, and the group has heard from four different companies and we’ve added a few new teams to the roster. For the investment of presenting 90 minutes worth of material, our teams have all continued to learn from the cumulative successes and mistakes of the whole group. The good news is that Austin is not unique. There are places all over the country with a rich software industry that can replicate this idea. Think of it as a way to open-source some of your good ideas without the overhead of starting a blog or supporting a true OSS project (though you might want to do those things too). If you do consider it (and I highly recommend it), here are some tips to help make your group successful: 1. You are who you work for One of the keys for the group was to have participation be officially sanctioned by the companies in attendance. While I can attend a technology group as an individual, I can’t share any of the great things that I’m working on unless Bazaarvoice agrees that those ideas are not IP that we want to protect. When it comes to sharing the inside scoop about your team’s technology, you certainly don’t want any misunderstandings or gray areas about what is appropriate and what isn’t. This also made it easy to continue the group, as we had established up front that every company in attendance for the original Bazaarvoice talk was also willing to share in return. 2. Like building software, don’t over engineer it Getting something up and running doesn’t take a lot of infrastructure. We organized the first few meetings via email with a representative from each company. Did we have a fancy logo? Nope. Did we have a website to manage attendees? Nope. Did we have a name for the group? Nope (although it has since been informally dubbed “Austin Scales”). Did we have a budget? Nope. We just invited everyone to our office and provided some frosty beverages (this last part is highly recommended). Then, I found a volunteer to present the next time and let them organize it. They did the same for the time after that. You can always add those other nice-to-haves once your group is successful. 3. The first rule of fight club is “No recruiting.” The second rule of fight club is “NO RECRUITING!” While the companies in attendance may not be competing with each other on the business front, we are often at war when it comes to finding and recruiting top talent. No company will want to approve participation in the group if they believe it will result in their best engineers being lured away. Make sure that everyone in attendance is on the same page. If your company is hiring, leave it at the door.", "date": "2011-10-20"},
{"website": "BazaarVoice", "title": "Listening for Search Improvements @ Lucene Revolution 2011", "author": ["Robby Morgan"], "link": "https://blog.developer.bazaarvoice.com/2011/12/02/listening-for-search-improvements-lucene-revolution-2011/", "abstract": "At Bazaarvoice, our business is powered by semi-structured data, and our mission is to unlock the potential of that data by delivering products and their reviews or answers to product-related questions, which are relevant to our clients and their consumers. Whether a consumer is trying to recommend a product that will answer another consumer’s question, or a merchandiser is trying to analyze reviews of the latest product launch, the vast majority of our platform functionality is powered by a search across a Solr/Lucene index. That’s why we were excited to attend the Lucene Revolution 2011 conference back in May 2011, and I wanted to share some key observations that can assist you in improving the search experience for your own users. When integrating search into any application, you should first recognize that search across free-text or semi-structured data is unlike other features in your application in that the “correct” output for text search is often not well-defined. Typically, your requirements will state that searches should return “relevant” results, but for a non-trivial text corpus and an unbounded user query space, it is effectively impossible to define “relevant” for an arbitrary user query. In the face of this uncertainty, we developers tend to implement search in the manner we are accustomed for other features — configuring indexing and querying in a way that makes sense for a handful of cases we can imagine, and then checking the overall outcome for a dozen or so sample queries that we expect our users might enter. Once we are seeing reasonable results for our sample queries, and seeing no other ways to improve results across-the-board, we stamp the functionality as complete, and we move on to the next task, right? The truth is that the lack of well-defined “correct” output for text search is actually the starting point for the implementation of another process — listening to what your end users expect from search. Unless you are the sole user, you as developer likely have only the vaguest understanding of how your users actually search. This is not a critique of you — it’s because every individual has developed their own process for formulating the terms they enter into that free-text search box you’ve provided. Fortunately, there are a number of common techniques for understanding user search behavior, which Otis Gospodnetić of Sematext outlined in his talk, Search Analytics: What? Why? How? Akin to web analytics, Otis described a number of key reports to use in measuring how well search is meeting the needs of end users. Among these are the top queries with zero results, top queries with the highest exit rate, words per query, and the top queries overall. Each of these reports can generally be created from query logs alone, and they are important barometers for evaluating and tuning the effectiveness of your search function. Using query logs, you can gauge the potential benefit of adding spell-checking (to address zero hits due to misspellings), query auto-completion (to assist with long queries), and hit highlighting (to see why results were considered relevant). After deeper query log analysis, you may even decide to preprocess user queries in a tailored way, as Floyd Morgan of Intuit described how they distill variations of the same query (e.g. “How do I input my 1099” and “Where do I enter a 1099”) to simpler customized search terms (“wheretoent 1099”) that provide better precision. As you can see, you can gain a significant understanding of your users’ expectations from query logs alone, but they do not provide the whole picture… For even better analysis and performance of your search functionality, you need to pair query log data with some other form of feedback on result quality, usually click-through rates (CTR) on search results. Again, Otis described a number of metrics to compute when query logs are paired with CTR data. In a separate session, Andrzej Białecki of Lucid Imagination described how to take this data a step further, treating CTR as direct feedback on result relevance, and incorporating it into the actual document score. At first blush, this seems to be an ideal and straightforward search improvement, but Andrzej also identified a number of undesired effects that require conscious effort to avoid. Also, he highlighted that Solr/Lucene does not currently provide a preferred storage model for this type of data. Solr 3.x provides an ExternalFileField that is currently the best mechanism for incorporating document popularity based on CTR, while Lucene 4.0 is slated to deliver an efficient mechanism for storing popularity and other per-document values, which Simon Willnauer, a core Lucene committer, described in his session on Column Stride Fields a.k.a. DocValues . Finally, Timothy Potter of NREL described complementary techniques for improving result relevance by Boosting Documents by Recency, Popularity, and User Preferences . Obviously, every technique is not directly applicable to every scenario, but the approaches are common to many search applications, so it is worth considering how they could apply to your application. As you can see, integrating search into an application is not a write-once, run-forever task. There is a wealth of opportunity for improving search so that it meets the actual needs of your users, and most of the information necessary for learning how to improve can be obtained by simply listening to your users using common techniques. So, I highly encourage you to review the sessions I have highlighted, and you can check out slides or videos for all the rest at the Lucene Revolution 2011 recap . Enjoy!", "date": "2011-12-02"},
{"website": "BazaarVoice", "title": "The Tools We Use to Innovate in Bazaarvoice Labs (Part 2)", "author": ["Jon Loyens"], "link": "https://blog.developer.bazaarvoice.com/2012/01/05/the-tools-we-use-to-innovate-in-bazaarvoice-labs-part-2/", "abstract": "In the previous post , I provided a rundown on what Bazaarvoice Labs is, our process and why it is important to have flexibility in our toolset choices. I now want to give you some tool examples in the following categories: Operational Tools Server-side Application Development Environments Data Storage and Management Client-side Tools Measurement Tools Of course, no project, prototype or pilot would get off the ground in Bazaarvoice Labs if we couldn’t get at our customer’s data. In order to maintain agility, all Bazaarvoice Labs projects are written as free-standing applications that are not part of our core application stack (a somewhat traditional J2EE application built on Spring MVC). Early on in Labs, even though we had direct access to our databases, we knew we needed to maintain separation between our core stack and Labs applications. Since we maintain a very complex set of business rules that are configurable on a per client basis around content submission and display, if we were to write directly to the databases, there’d be a high risk that we’d compromise data integrity. Generally, we’d use our existing XML API for submission (because it was obvious that trying to write data into the DBs from a separate application was a recipe for disaster) but we’d still use replicas of our core MySQL database clusters for display. That was okay but there were still some business logic mistakes made in the display of content (unacceptable when your pilot clients are some of the biggest online retailers around). In order to get around this, we created a new API that supported significantly higher degree of queryability, JSON and JSON-P data formats and had much lighter weight responses. This allows Bazaarvoice Labs to talk to our core data sets in a much more efficient manner and be assured that business rules are followed. This new API has now be productized as The Bazaarvoice Developer API . We will often create new, experimental method calls or create application local data indexes, but every single Bazaarvoice Labs project leverages this API heavily. I hope I’ve given you a good overview of how Bazaarvoice Labs operates and the tools that keep us humming. It’s great to be able to work in an environment where exploration of new ideas and technologies are supported and encouraged. By operating the Bazaarvoice Labs team off-stack, it gives the Labs Engineers a chance not only to give input into what new products get built but what technologies get used to build them in a very low risk way.", "date": "2012-01-05"},
{"website": "BazaarVoice", "title": "Grilling up an API", "author": ["Roshan Gupta"], "link": "https://blog.developer.bazaarvoice.com/2012/01/20/grilling-up-an-api/", "abstract": "BBQ is a religion in Austin. Everyone has their opinion on who serves up the best BBQ. Debates between people defending their choices have been known to last into the wee hours of the night. Friendships have been ruined, and neighbors turned into enemies (okay, I might have made that last bit up…but you get my point). APIs are also like a religion to many in the developer community. Developers spend their precious time using the tools and APIs that companies create. The easiest tools to use will be the ones they turn to consistently – and tell their friends about. At Bazaarvoice, we are hyper-focused on how to make our API and Platform the best set of tools around. But how do you “serve up” good API? To answer that, let’s borrow an analogy from the world of BBQ. Imagine you wanted to make a BBQ dinner, and you came to Bazaarvoice for help. We could help you in a few different ways: Method #1: We can provide you with the raw ingredients & materials you need – e.g. spices to make your sauce, sticks to build your fire, and of course – a cow. Method #2: We can provide you with some pre-packaged ingredients & items – e.g. a bottle of BBQ sauce, a grill, and some prime cuts of meat. Method #3: We can provide you with a menu from the Salt Lick , as well as the number to their delivery service. So what does this translate to (aside from a yummy BBQ dinner)? Method #1: High innovation, high support costs, and low adoption. Method #2: Medium innovation, medium support costs, and medium adoption. Method #3: Low innovation, low support costs, and high adoption. At Bazaarvoice, we aim to provide the developer community with tools that support all three of the methods above: Method #1: Our API gives developers fine-grained control over the information they can request, the filters they can specify, etc. However, this flexibility comes at a cost. Developers will have to understand our object model and syntax to take full advantage of the API, and we at Bazaarvoice need to provide training and documentation to help with this process. Method #2: Our API documentation always starts off with example API calls and popular use cases. These “pre-packaged” examples can help you skip straight to the API calls that will get the job done. Method #3: We have reference apps available to download, and we will continue to add more over time. These reference apps serve two purposes. First, you can download the apps, enter your API credentials, and be off and running (just like BBQ takeout!). Second, you can use these apps as a learning tool to help you get familiar with the Bazaarvoice API faster. Like grilling up BBQ, it is hard to satisfy everyone. But when you get your product just right, you can turn customers into dedicated fans. So in conclusion – when you see an employee of Bazaarvoice feasting away at one of the many popular local BBQ joints , feel confident knowing that we are hard at work. Extremenly superb way of comparing APIs with BBQ though firstly was quite confussed while reading this article.. So finally can say enjoyed both Bazaarvoice API and Austin BBQ 😉", "date": "2012-01-20"},
{"website": "BazaarVoice", "title": "Engineers Giving Back at Random Hacks of Kindness", "author": ["RC Johnson"], "link": "https://blog.developer.bazaarvoice.com/2012/02/08/engineers-giving-back-at-random-hacks-of-kindness/", "abstract": "On December 3rd and 4th Bazaarvoice was the lead sponsor on an event in Austin called Random Hacks of Kindness (RHoK), a coordinated worldwide hackathon for social good. The event started Friday night with a reception for all of the hackers at the Volstead Lounge where over over 60 people celebrated, heard a few quick thoughts on how technology could solve some big problems, and of course had a drink. Representatives of the Chicago Community Emergency Response Team, Williamson County Office of Emergency Management, NASA (a global sponsor for RHoK) and others gave a quick preview of the projects they hoped would be completed during the hackathon, and our own Scott Bonneau (VP of Engineering) spoke about the power that engineers have to change the world. Scott reminded us all how only a few short years ago, creating a new technology meant years of R&D efforts by large teams of highly educated scientists, and that now, anyone with a laptop and a credit card can launch an application over the course of days. Saturday morning kicked off with coffee and presentations by each subject matter expert on the problems they had been researching to the over 50 hackers who were eager to get started. Additionally, NASA had a special surprise for the Austin attendees, and had arranged for Astronaut Ron Garan to speak about how his time on the international space station had provided him with a unique view into the power RHoK hackers have and the need for greater collaboration on the biggest problems of the world. The hackers organized themselves into teams based on their skillsets and desires, and before lunch the product design had begun. Teams were well fed throughout the weekend with great local food from P. Terry’s, The Peached Tortilla, and Freebirds, ensuring that no one ever went hungry or was lacking caffeine. Several teams coded late into the evening at the Capital City Event’s space, lounging on couches, and some even stayed the night. By the end of the hackathon Sunday afternoon, the teams had built a number of amazing applications, and you can read more about the applications on the RHoK web site . All of the teams presented their work, and the top teams as selected by our judges received some amazing prizes (iPads, Kindles, and Buckyballs). Overall we’re very proud to have helped support this amazing opportunity, and we couldn’t have done it without the generous help of our sponsors and partners Homeaway.com , Freebirds, Capital City Events Center, and Github, as well as the numerous volunteers from tech companies throughout the community. Not that our event went perfectly, but we certainly learned a few lessons along the way: To find out more about the next RHoK Austin, just follow the Pixadillo @rhokaustin . Want to help run RHoK Austin in the future? Send us a message @rhokaustin and we’ll connect you with the steering team for the next RHok Austin.", "date": "2012-02-08"},
{"website": "BazaarVoice", "title": "Platform API Release Notes, Version 5.1", "author": ["Craig Gilchrist"], "link": "https://blog.developer.bazaarvoice.com/2012/02/14/platform-api-release-notes-version-5-1/", "abstract": "We are pleased to announce that the following functionality has been developed for version 5.1: More detailed information on each of these items is listed below. For complete documentation, refer to the Platform API Documentation, version 5.1 . ReviewStats Review statistics for Products now returns ReviewStats, which account for product family data in the statistics calculations. Prior to version 5.1, the ReviewStats were returning NativeStats, which do not take product families into account. Moderator Codes Moderator codes (content codes) can now be explicitly requested and filtered. A new filter (ModeratorCode) has been added and can accept any moderator code values. For a list of all moderator codes, see the API Basics page. In addition, a new value for the attribute parameter (ModeratorCodes) has been added and must be requested in order to filter by ModeratorCode. Wildcard character in ContentLocale filter The ContentLocale filter now accepts the asterisk (‘*’) as a wildcard character. This wildcard character represents zero or more characters. Adding the wildcard enables filtering for multiple locales without requiring each one to be listed. For example, &Filter=ContentLocale:eq:en* would return all the English locales (en_US, en_CA, en_GB, etc.). When the ContentLocale filter is set to the wildcard character, it is the equivalent of requesting all locales. It is important to note that the value of the filter cannot start with a wildcard filter. For example, &Filter=ContentLocale:eq:*_US is not a valid filter. IP address in Content Display When the API key is configured to allow access to IP addresses, you will either get the IP address back or null inside the IpAddress element. If you are interested in getting access to IP address within an application, contact technical support. API key creation and management in the client portal For clients that have signed the API Data Agreement, they can now create and manage their Developer API keys via the client portal. This functionality allows you to self-manage the API keys that you use to create applications using the Developer Platform APIs and tools. A detailed list of the prerequisites and procedures for creating, viewing and editing your API keys can be found in the Bazaarvoice Release Notes, version 5.1 . (You have to log in to the Spark portal to view.) Additon of Wildcard character in ContentLocale filter is going to be very useful as it is required quite often. All the new functionalites might now change and make it easier to access the platform for new version.", "date": "2012-02-14"},
{"website": "BazaarVoice", "title": "Using the Cloud to Troubleshoot End-User Performance (Part 1)", "author": ["Matthew Bogner"], "link": "https://blog.developer.bazaarvoice.com/2012/02/08/using-the-cloud-to-troubleshoot-end-user-performance-part-1/", "abstract": "Debugging performance issues is hard. Debugging end-user performance issues from a distributed production software stack is even harder, especially if you are the 3rd-party service provider for one of your clients that actually is in control of how your code is integrated into their site. There are lots of articles on the web regarding what the performance best practices are, but few, if any, that discuss the tactics of developing and improving them. Primarily, the challenges stem from the fact that troubleshooting performance issues is always iterative. If your production operations can handle deploying test code to production on a rapidly iterative schedule, then you can stop reading this post — you are already perfect, and worthy of a cold brewski for your impressive skills. As our team and software stack continued to grow in both size and complexity, we underwent a restructuring of our client (js, css and html) code a while back that allows us to only deliver the code that is actually needed by our client’s site and configuration. We did this by reorganizing our code into modules that can be loaded asynchronously by the browser using require.js . Effectively this took us from a monolithic js & css include that contained a bunch of unused code and styles, to something that averaged out to a much smaller deliverable that was consumed in smaller chunks by the browser. This technique is a double-edged sword, and like all things, is best when done in moderation. Loading multiple modules asynchronously in the browser results in multiple http requests. Every HTTP request made by the browser results in some overhead spent doing the following: There are many great resources from Yahoo! and Google which discuss the details of best-practices for improving page performance. I won’t re-hash that great information, nor dispute any of the recommendations that the respective authors make. I will , on the other hand, discuss some tactics and tools that I have found beneficial in analyzing and iterating on performance-related enhancements to a distributed software stack. Mission A few months ago, we challenged ourselves to improve our page load performance with IE7 in a bandwidth constrained (let’s call it “ DSL -ish”) environment. DSL connections vary in speed and latency with each ISP. I won’t bore you with the details of the changes we ended up making, but I want to give you a flavor of the situation we were trying to solve before I talk about the tactics and tools we used to iterate on the best practices that are available all over the web. The unique challenges here are that IE7 only allows 2 simultaneous connections to the same host at a time. Since our software distributes multiple modules of js, css and images that are very small, we were running into this 2-connection-per-hostname issue with a vengeance. Commonly accepted solutions to this involve image spriting , file concatenation and distributing or “sharding” requests for static resources across multiple domains. The sharding tactic made us realize the other constraining factor we were dealing with — the longer latency of HTTP requests on a DSL connection that gets exaggerated when making multiple DNS lookups to a larger set of distinct host names. Tools The tools that we used to measure and evaluate our changes affected the tactics we used – so I’ll discuss them first. Charles Proxy Charles Proxy is a tool that runs on all platforms and provides some key features that really aided us in our analysis. Primarily, it had a built-in bandwidth throttling capability which allowed us to simulate specific latency and upload/download conditions from our local machine. We used CharlesProxy for a rougher on-the-spot analysis of changes. CharlesProxy also allowed us to easily and quickly see some aggregate numbers of specific metrics we were interested in. In particular, we were looking for the total # of requests, total durations of all requests and the total response size of all requests. Since these numbers are affected by the rest of the code (not ours) on our client’s site – Charles allowed us to filter out the resources that were not ours, but still allowed us to see how our software behaved in the presence of our client’s code. However, since we had multiple developers working on the project — each making isolated changes — we wanted a way to run a sort of “integration” test of all the changes at once in a manner that more closely aligned with how our software is delivered from our production servers. This led us to our next tool of choice – one that we’d never used until now. WebPageTest.org In it’s own words: WebPagetest is a tool that was originally developed by AOL for use internally and was open-sourced in 2008 under a BSD license. The platform is under active development by several companies and community contributors on Google code. The software is also packaged up periodically and available for download if you would like to run your own instance. In our case, WebPageTest provided two key things: At a high level, WebPageTest.org controls a bunch of compute agents that live in various geographic locations of the US that are able to simulate bandwidth conditions according to your specifications (under the hood it uses DummyNet ). It allows you to request one of it’s agents to load your page and interact with your site by simulating link clicks (if necessary) and monitors and captures the results for detailed analysis by you later. This tool is a great way for you to use an external entity to verify your changes and have a consistent pre & post benchmark of your page’s performance. Of course, having some random machine on the web poke your site means that your changes must be publicly accessible over the web. Password protection is fine since you can use WPT to script the login, but IMHO is non-ideal as that is not part of the normal end-user experience. Tactics Now that we have a good handle on the tools we used – we should discuss how we put them to work. Stay tuned for part 2, where we will explore the tactics for using these tools together effectively.", "date": "2012-02-08"},
{"website": "BazaarVoice", "title": "Using the Cloud to Troubleshoot End-User Performance (Part 2)", "author": ["com", "HttpClient", "client", "HttpClient", "XPath", "xpath", "XPathFactory", "Set", "String", "hostnameFilter", "HashSet", "String", "String", "args", "Exception", "hostnameFilter", "hostnameFilter", "PostMethod", "post", "PostMethod", "post", "post", "post", "post", "post", "post", "args", "String", "responseBody", "executeHttpMethod", "post", "System", "responseBody", "Node", "statusCodeNode", "Node", "xpath", "getXmlSrc", "responseBody", "XPathConstants", "String", "statusCode", "statusCodeNode", "System", "statusCode", "statusCode", "Node", "testIdNode", "Node", "xpath", "getXmlSrc", "responseBody", "XPathConstants", "waitForTestCompletion", "testIdNode", "InputSource", "String", "content", "Exception", "ByteArrayInputStream", "content", "String", "HttpMethod", "method", "Exception", "responseCode", "String", "responseBody", "responseCode", "client", "method", "responseBody", "IOUtils", "method", "method", "responseCode", "responseCode", "responseBody", "responseBody", "String", "testId", "Exception", "PostMethod", "post", "PostMethod", "post", "post", "testId", "String", "responseBody", "executeHttpMethod", "post", "Node", "statusCodeNode", "Node", "xpath", "getXmlSrc", "responseBody", "XPathConstants", "String", "statusCode", "statusCodeNode", "statusCode", "System", "responseBody", "statusCode", "System", "Thread", "waitForTestCompletion", "testId", "statusCode", "obtainTestResults", "testId", "System", "responseBody", "String", "testId", "Exception", "GetMethod", "get", "GetMethod", "testId", "String", "responseBody", "executeHttpMethod", "get", "Node", "statusCodeNode", "Node", "xpath", "getXmlSrc", "responseBody", "XPathConstants", "String", "statusCode", "statusCodeNode", "statusCode", "System", "responseBody", "NodeList", "requestsDataUrlNodes", "NodeList", "xpath", "getXmlSrc", "responseBody", "XPathConstants", "nodeCtr", "nodeCtr", "requestsDataUrlNodes", "nodeCtr", "Node", "requestsDataNode", "requestsDataUrlNodes", "nodeCtr", "String", "requestsDataUrl", "requestsDataNode", "analyzeTestResult", "requestsDataUrl", "String", "requestsDataUrl", "Exception", "System", "requestsDataUrl", "HashMap", "String", "Integer", "numRequestsPerHost", "HashMap", "String", "Integer", "HashMap", "String", "HashMap", "String", "Integer", "numRequestsPerHostPerContentType", "HashMap", "String", "HashMap", "String", "Integer", "HashMap", "String", "Integer", "totalTTFBPerHost", "HashMap", "String", "Integer", "HashMap", "String", "Integer", "totalDNSLookupPerHost", "HashMap", "String", "Integer", "HashMap", "String", "Integer", "totalInitialCnxnTimePerHost", "HashMap", "String", "Integer", "HashMap", "String", "HashMap", "String", "Integer", "totalBytesPerHostPerContentType", "HashMap", "String", "HashMap", "String", "Integer", "HashMap", "String", "Integer", "totalBytesPerHost", "HashMap", "String", "Integer", "String", "responseBody", "executeHttpMethod", "GetMethod", "requestsDataUrl", "String", "lines", "StringUtils", "responseBody", "lineCtr", "lineCtr", "lines", "lineCtr", "String", "line", "lines", "lineCtr", "String", "columns", "StringUtils", "line", "String", "hostname", "columns", "String", "contentType", "columns", "String", "ttfb", "StringUtils", "columns", "columns", "String", "dns", "StringUtils", "columns", "columns", "String", "cnxn", "StringUtils", "columns", "columns", "String", "bytes", "StringUtils", "columns", "columns", "bytes", "hostnameFilter", "hostnameFilter", "hostname", "numRequestsPerHost", "hostname", "numRequestsPerHost", "hostname", "Integer", "numRequestsPerHost", "hostname", "numRequestsPerHost", "hostname", "numRequestsPerHostPerContentType", "hostname", "HashMap", "String", "Integer", "tmp", "HashMap", "String", "Integer", "tmp", "contentType", "Integer", "numRequestsPerHostPerContentType", "hostname", "tmp", "numRequestsPerHostPerContentType", "hostname", "contentType", "numRequestsPerHostPerContentType", "hostname", "contentType", "Integer", "numRequestsPerHostPerContentType", "hostname", "contentType", "numRequestsPerHostPerContentType", "hostname", "contentType", "totalBytesPerHostPerContentType", "hostname", "HashMap", "String", "Integer", "tmp", "HashMap", "String", "Integer", "tmp", "contentType", "Integer", "bytes", "totalBytesPerHostPerContentType", "hostname", "tmp", "totalBytesPerHostPerContentType", "hostname", "contentType", "totalBytesPerHostPerContentType", "hostname", "contentType", "Integer", "bytes", "totalBytesPerHostPerContentType", "hostname", "contentType", "totalBytesPerHostPerContentType", "hostname", "contentType", "Integer", "bytes", "totalTTFBPerHost", "hostname", "totalTTFBPerHost", "hostname", "Integer", "ttfb", "totalTTFBPerHost", "hostname", "totalTTFBPerHost", "hostname", "Integer", "ttfb", "totalDNSLookupPerHost", "hostname", "totalDNSLookupPerHost", "hostname", "Integer", "dns", "totalDNSLookupPerHost", "hostname", "totalDNSLookupPerHost", "hostname", "Integer", "dns", "totalInitialCnxnTimePerHost", "hostname", "totalInitialCnxnTimePerHost", "hostname", "Integer", "cnxn", "totalInitialCnxnTimePerHost", "hostname", "totalInitialCnxnTimePerHost", "hostname", "Integer", "cnxn", "totalBytesPerHost", "hostname", "totalBytesPerHost", "hostname", "Integer", "bytes", "totalBytesPerHost", "hostname", "totalBytesPerHost", "hostname", "Integer", "bytes", "printMap", "numRequestsPerHost", "printMap", "numRequestsPerHostPerContentType", "printMap", "totalBytesPerHostPerContentType", "printMap", "totalTTFBPerHost", "printMap", "totalDNSLookupPerHost", "printMap", "totalInitialCnxnTimePerHost", "printMap", "totalBytesPerHost", "String", "title", "HashMap", "stats", "System", "title", "Iterator", "keyItr", "stats", "keyItr", "Object", "key", "keyItr", "Object", "value", "stats", "key", "System", "key", "value", "Matthew Bogner"], "link": "https://blog.developer.bazaarvoice.com/2012/03/14/using-the-cloud-to-troubleshoot-end-user-performance-part-2/", "abstract": "This is is the second of a two-part article that outlines how we used a various set of tools to improve our page load performance. If you haven’t read part 1 , go ahead and give it a read before continuing. We opted to not use our normal staging environment for this project, since our staging environment doesn’t run experimental code. In order to iterate rapidly on our changes and to provide a location that is publicly accessible over the web (so that WebPageTest can see it), we set up an Amazon EC2 instance running a complete copy of all of our software so that it effectively behaved exactly like a local developer instance with the exception that it could be hit from any external resource on the web. (Heh… I make this sound really easy) So now that we have a server on the web running a customized version of our software, the problem is now making requests that normally go to our production datacenter get redirected to our EC2 instance without redirecting real end-users. In my opinion, this is where the capabilities of WebPageTest really shined and began to flex it’s muscle. Let’s say that under normal conditions, your production application runs at foo.com/123.456.789.1 and that the EC2 instance you created and that is running a customized version of your app is running at ec2-123-456-789-2.aws.com/123.456.789.2. WebPageTest will allow you to override DNS resolution for foo.com to 123.456.789.2. This works in a similar manner to a host override except that WPT will still have the browser perform a DNS lookup of your production host so that you still get accurate timings for the DNS resolution. To take advantage of this, you need to provide the following “script” to your test execution: The other cool thing about WebPageTest is that the test execution and results parsing can be scripted via their REST-like APIs . In fact, check out this java class gist I wrote (embedded below) that makes use of this API to run some aggregated stats of the usage of Twitter on AOL.com. This class allows you to more easily view aggregated statistics for a narrow set of the resources that are actually used by a page — assuming that you only care about the resources being delivered by your servers into another page. Let’s recap what we have available to us… With these tools and tactics we had an externally facing environment that we could use to iterate on new ideas quickly.", "date": "2012-03-14"},
{"website": "BazaarVoice", "title": "Home Depot using the Platform API to show off their UGC", "author": ["Craig Gilchrist"], "link": "https://blog.developer.bazaarvoice.com/2012/03/26/home-depot-using-the-platform-api-to-show-off-their-ugc/", "abstract": "Clients like the Home Depot are using ideas from our Inspiration Gallery to find innovative ways to show off their user-generated content (UGC) and demonstrate the importance of listening to their customers. The following image is taken from a Home Depot Store Managers meeting which had all store managers as well as suppliers in attendance. It shows real-time reviews on a globe using a Google Earth placemark based on the reviewer’s IP address. Want to try it for yourself? Click here to apply for an API key, then click here to download the reference app. Don’t forget to send us a picture or video of your app in motion.", "date": "2012-03-26"},
{"website": "BazaarVoice", "title": "Daniel Marcotte on SproutCore 2.0 (now Ember.js)", "author": ["Michael Norman"], "link": "https://blog.developer.bazaarvoice.com/2012/03/21/daniel-marcotte-on-sproutcore-2-0-now-ember-js/", "abstract": "Daniel Marcotte is one of the top developers on our latest product offering, Customer Intelligence (CI). He spent quite a bit of time evaluating SproutCore 2.0 for the complex user interface requirements of CI, helping to work out some of the kinks in the product as it moved toward beta . Daniel has recently moved to San Francisco to work on a new project — still for Bazaarvoice, of course! Before he left, I had a chance to sit down with him to chat about his experience working with the framework. MDN: Welcome, Daniel! Thanks for letting me take some time out of your hectic schedule to get you on record, so to speak. DM: Ah, my pleasure, Michael. Thank you for doing all the hard work. MDN: No, no, not at all. You did all the hard work already — I’m just gonna put my name on it! DM: I’m a team player; you go right ahead! MDN: Awesome! So let’s get right down to it. You switch seamlessly between front-end and back-end development, and you’re very passionate about the user experience. A lot of developers in the industry want to work on back-end systems and don’t seem to get as excited about building great UIs. So, what do you think contributes to your passion? I’m really interested to hear what drives you because it also tends to drive the Web UI frameworks people choose. DM: Yeah, that’s fair. I’ve actually given this a little bit of thought because my new manager asked me something about this recently. Something like, ‘Wow, you know, you’re really a UI guy. That’s great; how did you get into that?’ And I had kind of a funny moment, thinking it might be fruitful to start thinking of myself as a UI guy because then it’ll focus me on taking time to even go the extra level. Maybe read some stuff about user experience and even more wacky technologies. Because, I hadn’t traditionally done that. So how did I even end up in this position where someone would think, ‘of course you’re a UI guy’? I think it’s because I don’t wanna do a shitty job. So if you’ve been tasked with building a UI feature, it needs to work. And it needs to be attractive. It needs to be usable. You need to put yourself in the user’s shoes. MDN: It’s amazing, but most people don’t think that way. It seems like you actually know a little bit more. Have you actually done any reading and learning about any of this? DM: I have, yeah…some. But nothing extensive. It’s a bit intuitive which I think gets people relatively far. And I think it has a lot to do with just being honest with yourself, of trying to put yourself in someone’s shoes maybe. I have a bit of a teaching background. It’s one of my great pleasures. (Let’s ramble a little bit.) And one of my favorite things was being able to find a way to explain something to someone who it was completely new and novel to. To never forget what that was like to learn it for the first time. ‘Cause you can get lost and be an expert and think, ‘oh, it’s obvious, it’s fine.’ And I think that bleeds into UI sometimes. ‘Oh, I know how to use this.’ And that’s not the bar we’re trying to meet here. MDN: You were instrumental in choosing SproutCore 2.0 for CI. What other Web UI frameworks did you and the team look at before deciding on Sproutcore? DM: I looked at what Dojo was offering because it has the same full app/infrastructure stuff. It was a touch inscrutable, but really appealing. But it kind of seemed to go down the Ext-JS packaged-widget route. And I knew that Alex Zelenak, our UX designer, would come in and do a lot of custom stuff. And the other big driver, though I’m perceived as choosing this, I was really just the person who made it possible. It was really Alex Sexton that said, ‘this is the one; this is it for these reasons.’ So that kind of focused it. And it sort of became the one, and it needed to lose for some reason as opposed to beat something else. MDN: Did Alex talk about Handlebars at all, because I know that he’s using it in other places. DM: Oh, that’s interconnected. You use Handlebars templating inside SproutCore. MDN: I’m just showing my ignorance! DM: It’s like a specialized version of Handlebars that’s bindings-aware. So as your values change, your templates re-render. MDN: You talked about this a little bit, but does SproutCore have widgets too? DM: No. They’d like to get there, but it’d be like a UI layer. This is more like the structure/spine of the app. MDN: So other than Alex’s stamp of approval, what are the other reasons you might use it in the future? DM: I like the way it makes me think. I like the flavor of thinking it lends itself to. This disconnect with the bindings…. (Let me see if I can package this a little bit.) Trying to wrangle web UI using HTML, CSS, JavaScript without making the classic disaster, the classic mess, has been a mission of mine for a while. What’s the answer? What’s the secret? How do we make this maintainable and obvious to the next developer? Extensible, that sort of stuff. And one of the big realizations I had a while ago was, with Don’t Repeat Yourself, the DOM, it’s the boss in so many things. I have a list of things I’m showing the user, and I don’t wanna duplicate in JavaScript having another list of things and try to synchronize those two. It’s much more a matter of saying something like… DOM: you’re the boss. You’re displaying it. You have these nodes that represent my things. So if I need to reevaluate, I’m just gonna ask you, ‘Hey dude, what’s my list look like?’ OK, I’m gonna modify it inside you ’cause I wanna display it. ‘Cause you’re the boss. And so in practice this works out really well. MDN: ‘Cause that’s kind of how JQuery works because you have this separate set of code over here and it’s overlayed on top of the DOM. DM: Yeah exactly. But for some richer apps and much more complex apps, asking the DOM those questions gets more complicated ’cause stuff is kinda all over the place. And so you end up with these really bonkers selectors. And it’s slow to be constantly asking the DOM these questions to see what’s going on with it. So then you start to pull back and say, ‘Oh, crap, I wish I had an object representation of this in my hand!’ Is it worth writing this connective tissue? And so now, enter SproutCore, and the app is sort of pure JS and then you bind it to the UI and you don’t maintain that connection; SproutCore does. Which is cool. So the classic collections example that ties into the list example earlier, where before the DOM was the boss, now javaScript’s the boss. I have this list and I’m gonna monkey with it. Using observers and bindings, SproutCore’s gonna hold onto the fact that the list corresponds to something in the DOM, that’s rendered by a template. When the data changes, the template is called to re-render that piece. MDN: So SproutCore owns the DOM then. DM: Effectively, and it can do smart stuff like buffer your updates. So it’ll just be like manually updating the DOM, but will be a lot faster. Then you kinda get into their flavor of MVC. SproutCore MVC gets quite appealing where you can really build an app that does its thing and then you can write unit tests against that. And when you bind the UI on it, that’s just another view. But independently, you have this app that you can validate does its thing and is independent of how it’s displayed. You do have to make sure you structure it carefully enough that you’re not getting into circular references in your bindings, but it’s super-appealing that your templates are HTML and you just bind them in with the app and there’s a lot of really great ideas. It’s technically very satisfying. MDN: And do you just bind it on top of a div element, because you’re going to have to put it in somewhere, right? So there’s some ID where you say, hey put this over here in this spot? DM: You have a script block that has a type “text/html” or “text/x-handlebars”. And one of the first things SproutCore does is it looks at the page and any script block of that type, if it has an ID on it, then it assumes you’re gonna reference this later and so it will just pull it out of the DOM and store it compiled. Otherwise, it’ll replace it inline. You get to write very naturally, right inline, if you’re not reusing that component. It’s got a nice flow to it where it will just replace that piece with the markup it corresponds to. MDN: Did you run into any problems with SproutCore’s architecture as you were building out the system at all? DM: So, to even decide if we wanted to do this, I threw together a prototype that recreated what the pilot’s UI — the interested parts — did. And I was really able to see the value because we’re managing different views on the same data. And the user gets to toggle displayed data and interact with what they’re seeing. And so this idea of an app that will hold that state and where you can just swap out these views, it’s great because you can just say “app, here’s some data, do your thing” and now you’re not fetching from the server again and again if you just wanna re-slice something or have a different look at it, you have all that power easily in the client side. So that was all going really well, but then I’d run into mundane stuff. Like: you’ve sent in a collection and you’re running over it and the designer wants to number each row. Perfectly reasonable request, but the context that you have in the collection rendering is just the collection item that you’re on. So do you artificially start adding indexes to your objects externally? Or do you say we should really have access to the index? And so that was kinda fun; I submitted a patch which exposed the current index when rendering a collection. Another example is that Tapestry [the server side web framework in this project] hosed us because you have almost no access to the head of the documents that it generates, which in general isn’t a problem, but in one of the early versions of SproutCore, it was only looking for stored templates in the head, which was an artificial restriction. MDN: But not really any big pains or roadblocks. Nothing you couldn’t work around or fix. DM: There was naturally a bit of instability in a few of the alpha versions I was using. And then there’s a tougher learning curve that goes with that: you can imagine being unable to distinguish between a flaw in understanding and a flaw in the library. But that’s come a long way. There are also things that it really super-facilitates incredibly, but as with a lot of these things, it can mean limiting you in other ways, like there being hoops to jump through to integrate with say a select control. And then we got into trouble where other developers started building out what I had there, and it wasn’t obvious that this code worked differently than they were used to. I hadn’t quite gotten it polished to the point of “this is how it’s done and you grow it from here and follow these patterns.” I hadn’t quite gotten there. The schedule comes in and you end up with this problem. So there was a bit of: “I wanna do this thing, and I can picture that vision in the ‘JQuery way.’ Well now, what’s the ‘SproutCore way’ of doing exactly that?” And you end up kind of cheating and creating artificial properties that you can bind to and kind of tricking it into doing things the ‘JQuery way’ instead of doing what is really the ‘SproutCore way’. And that’s really challenging. MDN: Well, it’s the speed of sort of doing it versus doing it in a way that it’s gonna be good later. DM: Yeah. Do you want the speed now, or more of it later? And that’s a really hard choice to make. And I’ve been ruminating on whether that’s a flaw on our part, or on the library’s part. How do we keep pushing that stuff forward and make it even better? I’m not sure. MDN: You made a pretty concerted effort to get SproutCore out of beta and into mainstream for release 2.0. So, you actually fixed a couple of issues. How did that process go? How did you work with the team and get them to take your patches? DM: Contributing to open source had been mysterious to me for a long time. And I’ve met a lot of people who are also like, ‘oh I want to contribute to open source, but I kinda like don’t know how. And IRC makes me cross-eyed.’ What do you do? In a lot of ways, this was just sort of really kinda lucky — a nice sort of confluence of events. And in my mind some of the things that made it possible was actually meeting Tom Dale and Yehuda Katz (who are great, by the way) and shaking their hands [at TXJS]. And they don’t have to remember it, but that’s kinda your way in later to politely say, ‘Hey, remember when I expressed interest in your framework? I’m using it and I don’t need anything from you. I’m just sharing good news. I’m not after you for anything.’ And then, GitHub makes a lot of this brilliantly easier. You make your fork, you monkey around, you learn something. Figuring out how Github works is pretty straightforward. You still see some trolling and joke pulls and this sort of stuff, but it’s kind of neat and friendly once you get your head around it. So then to contribute to a project: Keep it small. Keep it focused. Explain why. Write a unit test for whatever you’re putting up. And then you just post it, and then you wait. I also started a bit of not-constant (it was a real priority for me not to bother these guys) but still regular e-mail correspondence with Tom Dale, so I think for my initial patches, the SproutCore guys were able to connect them with me. So I think it sort of gets you in the door that you’re not just some random unknown pull request. And then it was pretty easy. It was super satisfying to get even simple messages like, ‘Hey, thanks!’ And then they merge in your code. and that’s pretty cool. Being able to take some time to work on that at BV is brilliant. People knew that I was investigating some of this stuff and there was a lot of support. It was cool. And John [Daniel’s manager] got a lot of satisfaction seeing my patches going in and that sort of stuff. So yeah, it was really cool. I still think navigating all this is kind of hard. It’s social. You get self-conscious someone’s probably going to be a dick to you. Someone’s probably going to ignore you ’cause you’re nobody. But this experience was really good so it’s worth trying more. And I would encourage other people too: find something you’re passionate about, some little tool you use you wish worked a bit different. You’re probably bright enough at this point; you can actually figure out how to make a useful change, even just for yourself initially. MDN: What kind of support did you get from the rest of the R&D organization here? You mentioned you could do some of it during work hours, which is great. DM: I just tried to not be stupid about it. As long as I was being productive and watching my diminishing returns level, I was free to put in the time I needed. The VP of Engineering said we should at least try and do something interesting. He wanted to know “Is that feasible?” And so that’s the kind of support. I’m like yeah, sure, I believe that it’s feasible and I have a vision for walling it off so we’re not betting the whole product on it. I can try, but you’ve just introduced some risk that we may have to pay for, we may have to mitigate. And he was fully supportive, and so when that directive to try something fresh comes from that high up, that’s kind of the definition of support. You know what I mean? And I felt like I had the freedom to at any time make the call that we were going to bail on that course of action (the diminishing returns thing). I don’t think anyone would have turned on me and been like, ‘what the hell, Daniel, you didn’t even use that technology. You spent all that time on it.’ I think they would have been like, ‘Thank you for actually evaluating this and making a technical decision.’ MDN: So, would you recommend SproutCore 2.0 for everybody? DM: I have no idea. It was still in its infancy during this project, so “everybody” seems a bit strong. But I really enjoyed using it, and I’m looking forward to using it more and watching it grow. MDN: Thanks Daniel, for taking the time to chat with me! It was very educational. Daniel brought in his laptop and showed me https://github.com/dmarcotte/ember-chess . Specifically, we walked through his very informative presentation in just a few minutes: https://github.com/dmarcotte/ember-chess/blob/master/presentation.js . Thank you for sharing this interview. I’m evaluating ember now, and it really excites me that we are finally seeing an appeal to structure and proper separation of concerns on the client-side in JavaScript. I too am a “hybrid” developer of sorts, with the user experience being at the forefront of my passion and enjoyment. It’s a good place to be nowadays; I remember not too long ago when the art of UI was not getting the respect it deserves. I think Steve Jobs did a good job of helping correct that. Anyway, having a fellow UX afficianado come off a successful project with ember is pretty encouraging. I’m now pretty confident I’m heading down the right path. P.S. ctrl + h, sproutcore, tab, ember, enter 😉", "date": "2012-03-21"}
]