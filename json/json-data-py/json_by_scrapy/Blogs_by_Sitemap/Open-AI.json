[
{"website": "Open-AI", "title": "first-retro-contest-retrospective", "author": ["John Schulman", "Vicki Pfau", "Alex Nichol", "Christopher Hesse", "Oleg Klimov", "Larissa Schiavo"], "link": "https://openai.com/blog/first-retro-contest-retrospective/", "abstract": "The first run of our Retro Contest —exploring the development of algorithms that can generalize from previous experience—is now complete. Though many approaches were tried, top results all came from tuning or extending existing algorithms such as PPO and Rainbow. There's a long way to go: top performance was 4,692 after training while the theoretical max is 10,000. These results provide validation that our Sonic benchmark is a good problem for the community to double down on: the winning solutions are general machine learning approaches rather than competition-specific hacks, suggesting that one can't cheat through this problem. Over the two-month contest, 923 teams registered and 229 submitted solutions to the leaderboard. Our automated evaluation systems conducted a total of 4,448 evaluations of submitted algorithms, corresponding to about 20 submissions per team. The contestants got to see their scores rise on the leaderboard, which was based on a test set of five low-quality levels that we created using a level editor. You can watch the agents play one of these levels by clicking on the leaderboard entries . Because contestants got feedback about their submission in the form of a score and a video of the agent being tested on a level, they could easily overfit to the leaderboard test set. Therefore, we used a completely different test set for the final evaluation. Once submissions closed, we took the latest submission from the top 10 entrants and tested their agents against 11 custom Sonic levels made by skilled level designers. To reduce noise, we evaluated each contestant on each level three times, using different random seeds for the environment. The ranking changed in this final evaluation, but not to a large extent. The top 5 scoring teams are: Dharmaraja topped the scoreboard during the contest, and the lead remained on the final evaluation; mistake narrowly won out over aborg for second place. The top three teams will receive trophies. Learning curves of the top three teams for all 11 levels are as follows (showing the standard error computed from three runs). Averaging over all levels, we can see the following learning curves. Note that Dharmaraja and aborg start at similar scores, whereas mistake starts much lower. As we will describe in more detail below, these two teams fine-tuned (using PPO) from a pre-trained network, whereas mistake trained from scratch (using Rainbow DQN). mistake 's learning curves end early because they timed out at 12 hours. Dharmaraja is a six-member team including Qing Da, Jing-Cheng Shi, Anxiang Zeng, Guangda Huzhang, Run-Ze Li, and Yang Yu. Qing Da and Anxiang Zeng are from the AI team within the search department of Alibaba in Hangzhou, China. In recent years, they have studied how to apply reinforcement learning to real world problems, especially in an e-commerce setting, together with Yang Yu , who is an Associate Professor of the Department of Computer Science at Nanjing University, Nanjing, China. Dharmaraja’s solution is a variant of joint PPO (described in our tech report ) with a few improvements. First, it uses RGB images rather than grayscale; second, it uses a slightly augmented action space, with more common button combinations; third, it uses an augmented reward function, which rewards the agent for visiting new states (as judged by a perceptual hash of the screen). In addition to these modifications, the team also tried a number of things that didn’t pan out: DeepMimic , object detection through YOLO , and some Sonic-specific ideas. Get the source code Team mistake consists of Peng Xu and Qiaoling Zhong. Both are second-year graduate students in Beijing, China, studying at the CAS Key Laboratory of Network Data Science and the Technology Institute of Computing Technology, Chinese Academy of Sciences. In their spare time, Peng Xu enjoys playing basketball, and Qiaoling Zhong likes to play badminton. Their favorite video games are Contra and Mario. Mistake’s solution is based on the Rainbow baseline. They made several modifications that helped boost performance: a better value of n for n-step Q learning; an extra CNN layer added to the model, which made training slower but better; and a lower DQN target update interval. Additionally, the team tried joint training with Rainbow, but found that it actually hurt performance in their case. Get the source code Team Aborg is a solo effort from Alexandre Borghi. After completing a PhD in computer science in 2011, Alexandre worked for different companies in France before moving to the United Kingdom where he is a research engineer in deep learning. As both a video game and machine learning enthusiast, he spends most of his free time studying deep reinforcement learning, which led him to take part in the OpenAI Retro Contest. Aborg’s solution, like Dharmaraja’s, is a variant of joint PPO with many improvements: more training levels from the Game Boy Advance and Master System Sonic games; a different network architecture; and fine-tuning hyper-parameters that were designed specifically for fast learning. Elaborating on the last point, Alexandre noticed that the first 150K timesteps of fine-tuning were unstable (i.e. the performance sometimes got worse), so he tuned the learning rate to fix this problem. In addition to the above changes, Alexandre tried several solutions that did not work: different optimizers, MobileNetV2 , using color images, etc. Get the source code The Best Write-up Prize is awarded to contestants that produced high-quality essays describing the approaches they tried. Now, let’s meet the winners of this prize category. Dylan currently lives in Paris, France. He is a student in software development at school 42 in Paris . He got into machine learning after watching a video of a genetic algorithm learning how to play Mario a year and a half ago. This video sparked his interest and made him want to learn more about the field. His favorite video games are Zelda Twilight Princess and World of Warcraft. Oleg Mürk hails from the San Francisco Bay Area, but is originally from Tartu, Estonia. During the day, he works with distributed data processing systems as a Chief Architect at Planet OS. In his free time, he burns \"too much money\" on renting GPUs for running deep learning experiments in TensorFlow. Oleg likes traveling, hiking, and kite-surfing and intends to finally learn to surf over the next 30 years. His favorite computer game (also the only one he has completed) is Wolfenstein 3D. His masterplan is to develop an automated programmer over the next 20 years and then retire. Felix is an entrepreneur who lives in Hong Kong. His first exposure to machine learning was a school project where he applied PCA to analyse stock data. After several years pursuing entrpreneurship, he got into ML in late 2015; he has become an active Kaggler and has worked on several side projects on computer vision and reinforcement learning. One of the best things that came from this contest was seeing contestants helping each other out. Lots of people contributed guides for getting started, useful scripts, and troubleshooting support for other contestants. The winner of our Best Supporting Material award is Tristan Sokol , who wrote many helpful blog posts throughout the contest and made a tool for visualizing trajectories through Sonic levels. During the day, Tristan works for Square, helping to build their developer platform; at night, he is a designer and entrepreneur. This was the first time that he has done any AI/ML, and also his first time using Python for any real use case. Looking forward, Tristan is going to try to make cool things with TensorFlow.js. Whenever he isn't in front of a computer, Tristan is probably in his Oakland backyard watching plants grow. Contests have the potential to overhaul the prevailing consensus on what works the best, since contestants will try a diverse set of different approaches and the best one will win. In this particular contest, the top performing approaches were not radically different from the ones that we at OpenAI had found to be successful prior to the contest. We were glad to see several of the top solutions making use of transfer learning; fine-tuning from the training levels. However, we were surprised to find that some of the top submissions were simply tuned versions of our baseline algorithms. This emphasizes the importance of hyper-parameters, especially in RL algorithms such as Rainbow DQN. We plan to start another rendition of the contest in a few months. We hope and expect that some of the more off-beat approaches will be successful in this second round, now that people know what to expect and have begun to think deeply about the problems of fast learning and generalization in reinforcement learning. We’ll see you then, and we look forward to watching your innovative solutions climb up the scoreboard. Gotta Learn Fast", "date": "2018-06-22"},
{"website": "Open-AI", "title": "openai-fellows", "author": ["Larissa Schiavo", "Igor Mordatch", "John Schulman"], "link": "https://openai.com/blog/openai-fellows/", "abstract": "We’re now accepting applications for the next cohort of OpenAI Fellows, a program which offers a compensated 6-month apprenticeship in AI research at OpenAI. We designed this program for people who want to be an AI researcher, but do not have a formal background in the field. Applications for Fellows starting in September are open now and will close on July 8th at 12AM PST. Each Fellow will be mentored by an OpenAI researcher and have the chance to work within one of our research teams on topics like multi-agent reinforcement learning , generative models , robotics , and others. Fellows will spend the first two months of the program working through a curriculum of key topics in AI, learning about research projects at OpenAI, and writing a research proposal based on their interests. Fellows will then work on the project laid out in their research proposal for the next 4 months with guidance from their mentor. The Fellows program is designed for people who want to transition into doing artificial intelligence research; our current cohort of fellows includes people with backgrounds in genetics, software engineering, physics, and theoretical computer science. Successful applicants will be able to demonstrate their interest in AI research via past projects or evidence of significant self - study . (If you're in the middle of a degree program, please apply for an internship instead.) We will give priority to applicants that can join OpenAI full-time following the program. The Fellows program is compensated at the level of selective Bay Area software internships. We can accommodate up to six fellows in the September cohort and may close applications early, so don't hestiate to submit your applications. To apply, visit the jobs page for further details. We will be reviewing applications on a rolling basis. We will notify applicants regarding their admissions status by July 31st, 2018. Please direct any questions you may have to fellows@openai.com .", "date": "2018-05-30"},
{"website": "Open-AI", "title": "ai-and-compute", "author": ["Dario Amodei", "Danny Hernandez", "Girish Sastry", "Jack Clark", "Greg Brockman", "Ilya Sutskever"], "link": "https://openai.com/blog/ai-and-compute/", "abstract": "We're releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore's Law had a 2-year doubling period). [1] Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it's worth preparing for the implications of systems far outside today's capabilities. The total amount of compute, in petaflop/s-days, [2] used to train selected results that are relatively well known, used a lot of compute for their time, and gave enough information to estimate the compute used. Download charts Three factors drive the advance of AI: algorithmic innovation, data (which can be either supervised data or interactive environments), and the amount of compute available for training. Algorithmic innovation and data are difficult to track, but compute is unusually quantifiable, providing an opportunity to measure one input to AI progress. Of course, the use of massive compute sometimes just exposes the shortcomings of our current algorithms. But at least within many current domains, more compute seems to lead predictably to better performance , and is often complementary to algorithmic advances. For this analysis, we believe the relevant number is not the speed of a single GPU, nor the capacity of the biggest datacenter, but the amount of compute that is used to train a single model—this is the number most likely to correlate to how powerful our best models are. Compute per model differs greatly from total bulk compute because limits on parallelism (both hardware and algorithmic) have constrained how big a model can be or how much it can be usefully trained. Of course, important breakthroughs are still made with modest amounts of compute—this analysis just covers compute capability. The trend represents an increase by roughly a factor of 10 each year. It’s been partly driven by custom hardware that allows more operations to be performed per second for a given price (GPUs and TPUs), but it’s been primarily propelled by researchers repeatedly finding ways to use more chips in parallel and being willing to pay the economic cost of doing so. Looking at the graph we can roughly see four distinct eras: Before 2012: It was uncommon to use GPUs for ML, making any of the results in the graph difficult to achieve. 2012 to 2014: Infrastructure to train on many GPUs was uncommon, so most results used 1-8 GPUs rated at 1-2 TFLOPS for a total of 0.001-0.1 pfs-days. 2014 to 2016: Large-scale results used 10-100 GPUs rated at 5-10 TFLOPS, resulting in 0.1-10 pfs-days. Diminishing returns on data parallelism meant that larger training runs had limited value. 2016 to 2017:  Approaches that allow greater algorithmic parallelism such as huge batch sizes , architecture search , and expert iteration , along with specialized hardware such as TPU’s and faster interconnects, have greatly increased these limits, at least for some applications. AlphaGoZero/AlphaZero is the most visible public example of massive algorithmic parallelism, but many other applications at this scale are now algorithmically possible, and may already be happening in a production context. We see multiple reasons to believe that the trend in the graph could continue. Many hardware startups are developing AI-specific chips, some of which claim they will achieve a substantial increase in FLOPS/Watt (which is correlated to FLOPS/$) over the next 1–2 years. There may also be gains from simply reconfiguring hardware to do the same number of operations for less economic cost . On the parallelism side, many of the recent algorithmic innovations described above could in principle be combined multiplicatively—for example, architecture search and massively parallel SGD. On the other hand, cost will eventually limit the parallelism side of the trend and physics will limit the chip efficiency side. We believe the largest training runs today employ hardware that cost in the single digit millions of dollars to purchase (although the amortized cost is much lower).  But the majority of neural net compute today is still spent on inference (deployment), not training, meaning companies can repurpose or afford to purchase much larger fleets of chips for training. Therefore, if sufficient economic incentive exists, we could see even more massively parallel training runs, and thus the continuation of this trend for several more years.  The world’s total hardware budget is 1 trillion dollars a year, so absolute limits remain far away.  Overall, given the data above, the precedent for exponential trends in computing, work on ML specific hardware, and the economic incentives at play, we think it’d be a mistake to be confident this trend won’t continue in the short term. Past trends are not sufficient to predict how long the trend will continue into the future, or what will happen while it continues. But even the reasonable potential for rapid increases in capabilities means it is critical to start addressing both safety and malicious use of AI today. Foresight is essential to responsible policymaking and responsible technological development, and we must get out ahead of these trends rather than belatedly reacting to them. If you'd like to help make sure that AI progress benefits all of humanity , join us at OpenAI. Our research and engineering roles range from machine learning researchers to policy researchers to infrastructure engineers . Two methodologies were used to generate these data points. When we had enough information, we directly counted the number of FLOPs (adds and multiplies) in the described architecture per training example and multiplied by the total number of forward and backward passes during training.  When we didn’t have enough information to directly count FLOPs, we looked GPU training time and total number of GPUs used and assumed a utilization efficiency (usually 0.33).  For the majority of the papers we were able to use the first method, but for a significant minority we relied on the second, and we computed both whenever possible as a consistency check.  In the majority of cases we also confirmed with the authors.  The calculations are not intended to be precise but we aim to be correct within a factor 2-3.  We provide some example calculations below. This method is particularly easy to use when the authors give the number of operations used in a forward pass, as in the Resnet paper (the Resnet-151 model in particular): Operations can also be counted programmatically for a known model architecture in some deep learning frameworks, or we can simply count operations manually.  If a paper gives enough information to make this calculation, it will be quite accurate, but in some cases papers don’t contain all the necessary information and authors aren’t able to reveal it publicly. If we can’t count operations directly, we can instead look at how many GPUs were trained for how long, and use reasonable guesses at GPU utilization to try to estimate the number of operations performed.  We emphasize that here we are not counting peak theoretical FLOPS, but using an assumed fraction of theoretical FLOPS to try to guess at actual FLOPS.  We typically assume a 33% utilization for GPUs and a 17% utilization for CPU’s, based on our own experience, except where we have more specific information (e.g. we spoke to the author or the work was done at OpenAI). As an example, in the AlexNet paper it’s stated that “our network takes between five and six days to train on two GTX 580 3GB GPUs”.  Under our assumptions this implies a total compute of: This method is more approximate and can easily be off by a factor of 2 or occasionally more; our aim is only to estimate the order of magnitude.  In practice when both methods are available they often line up quite well (for AlexNet we can also directly count the operations, which gives us 0.0054 pfs-days vs 0.0058 with the GPU time method). Method 2. Details given in a later paper . Massive compute is certainly not a requirement to produce important results. Many recent noteworthy results have used only modest amounts of compute. Here are some examples of results using modest compute that gave enough information to estimate their compute. We didn’t use multiple methods to estimate the compute for these models, and for upper bounds we made conservative estimates around any missing information, so they have more overall uncertainty. They aren’t material to our quantitative analysis, but we still think they are interesting and worth sharing: Attention is all you need : 0.089 pfs-days (6/2017) Adam Optimizer : less than 0.0007 pfs-days (12/2014) Learning to Align and Translate : 0.018 pfs-days (9/2014) GANs : less than 0.006 pfs-days (6/2014) Word2Vec : less than 0.00045 pfs-days (10/2013) Variational Auto Encoders : less than 0.0000055 pfs-days (12/2013) We’ve updated our analysis with data that span 1959 to 2012. Looking at the data as a whole, we clearly see two distinct eras of training AI systems in terms of compute-usage: (a) a first era, from 1959 to 2012, which is defined by results that roughly track Moore’s law, and (b) the modern era, from 2012 to now, of results using computational power that substantially outpaces macro trends. The history of investment in AI broadly is usually told as a story of booms and busts, but we don’t see that reflected in the historical trend of compute used by learning systems. It seems that AI winters and periods of excitement had a small effect on compute used to train models [3] over the last half-century. Download charts Starting from the perceptron in 1959, we see a ~2-year doubling time for the compute used in these historical results—with a 3.4-month doubling time starting in ~2012. It’s difficult to draw a strong conclusion from this data alone, but we believe that this trend is probably due to a combination of the limits on the amount of compute that was possible to use for those results and the willingness to spend on scaling up experiments. [4] We followed the same methodology outlined in the original post for this updated analysis. When possible, we programmatically counted the number of FLOPs in the results by implementing the models directly. Since computer architectures varied historically and many papers omitted details of their computational setup, these older data points are more uncertain (our original analysis of post-2012 data aimed to be within a factor of 2–3, but for these pre-2012 data points we aim for an order of magnitude estimate). We've also created graphs that provide additional views on the data: one graph lays out compute usage in fundamentals, speech, language, vision, and games over time and another visualizes the error-bar estimates around each data point. We’re very uncertain about the future of compute usage in AI systems, but it’s difficult to be confident that the recent trend of rapid increase in compute usage will stop, and we see many reasons that the trend could continue . Based on this analysis, we think policymakers should consider increasing funding [5] for academic research into AI, as it's clear that some types of AI research are becoming more computationally intensive and therefore expensive. Correction (November 7, 2019): We originally quoted Moore's Law as an 18-month doubling time. ↩︎ A petaflop/s-day (pfs-day) consists of performing 10 15 neural net operations per second for one day, or a total of about 10 20 operations. The compute-time product serves as a mental convenience, similar to kW-hr for energy. We don’t measure peak theoretical FLOPS of the hardware but instead try to estimate the number of actual operations performed. We count adds and multiplies as separate operations, we count any add or multiply as a single operation regardless of numerical precision (making “FLOP” a slight misnomer), and we ignore ensemble models . Example calculations that went into this graph are provided in this appendix . Doubling time for line of best fit shown is 3.4 months. ↩︎ Just as in the original analysis, we focus on the costs to train models. This doesn't include AI systems like expert systems, which attracted substantial investment in the first era. ↩︎ For one vivid account of the history of computing in AI in this period, see the “False Start” section in Hans Moravec’s article . ↩︎ We've already advocated for additional funding for academia in our testimony in Congress this year, and for the creation of dedicated compute clusters to help academia and industry collaboratively benchmark and assess the safety of AI systems in response to a request for information from NIST ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2018-05-16"},
{"website": "Open-AI", "title": "gym-retro", "author": ["Vicki Pfau", "Alex Nichol", "Christopher Hesse", "Larissa Schiavo", "John Schulman", "Oleg Klimov"], "link": "https://openai.com/blog/gym-retro/", "abstract": "We're releasing the full version of Gym Retro , a platform for reinforcement learning research on games. This brings our publicly-released game count from around 70 Atari games and 30 Sega games to over 1,000 games across a variety of backing emulators. We're also releasing the tool we use to add new games to the platform. We use Gym Retro to conduct research on RL algorithms and study generalization. Prior research in RL has mostly focused on optimizing agents to solve single tasks. With Gym Retro, we can study the ability to generalize between games with similar concepts but different appearances. This release includes games from the Sega Genesis and Sega Master System, and Nintendo's NES, SNES, and Game Boy consoles. It also includes preliminary support for the Sega Game Gear, Nintendo Game Boy Color, Nintendo Game Boy Advance, and NEC TurboGrafx. Some of the released game integrations, including those games in the data/experimental folder of Gym Retro, are in a beta state — please try them out and let us know if you encounter any bugs. Due to the large scale of the changes involved the code will only be available on a branch for the time being. To avoid breaking contestants' code we won't be merging the branch until after the contest concludes. The ongoing Retro Contest (ending in a couple weeks!) and our recent technical report focus on the easier problem of generalizing between different levels of the same game (Sonic The Hedgehog™). The full Gym Retro dataset takes this idea further and makes it possible to study the harder problem of generalization between different games. The scale of the dataset and difficulty of individual games makes it a formidable challenge, and we are looking forward to sharing our research progress over the next year. We also hope that some of the solutions developed by participants in the Retro Contest can be scaled up and applied to the full Gym Retro dataset. We're also releasing the tool we use to integrate new games. Provided you have the ROM for a game, this tool lets you easily create save states, find memory locations, and design scenarios that reinforcement learning agents can then solve. We've written an integrator's guide for people looking to add support for new games. The integration tool also supports recording and playing movie files that save all the button inputs to the game. These files are small because they only need the starting state and sequence of button presses, as opposed to storing each frame of the output. Movie files like these are useful for visualizing what your reinforcement learning agent is doing as well as storing human input to use as training data. While developing Gym Retro we've found numerous examples of games where the agent learns to farm for rewards (defined as the increase in game score) rather than completing the implicit mission. In the above clips, characters in Cheese Cat-Astrophe (left) and Blades of Vengeance (right) become trapped in infinite loops because they're able to rapidly accrue rewards that way. This highlights a phenomenon we've discussed previously: the relatively simple reward functions we give to contemporary reinforcement learning algorithms, for instance by maximizing the score in a game, can lead to undesirable behaviors. For games with dense (frequent and incremental) rewards where most of the difficulty comes from needing fast reaction times, reinforcement learning algorithms such as PPO do very well. In a game such as Gradius (pictured on the right), you get points for each enemy you shoot, so it's easy to get rewards and start learning.  Surviving in a game like this is based on your ability to dodge enemies, which is no problem for reinforcement learning algorithms since they play the game one frame at a time. For games that have a sparse reward or require planning more than a few seconds into the future, existing algorithms have a hard time.  Many of the games in the Gym Retro dataset have a sparse reward or require planning, so tackling the full dataset will likely require new techniques that have not been developed yet. If you are excited about conducting research on transfer learning and meta-learning with an unprecedentedly large dataset, then consider joining OpenAI .", "date": "2018-05-25"},
{"website": "Open-AI", "title": "interpretable-machine-learning-through-teaching", "author": ["Smitha Milli", "Pieter Abbeel", "Igor Mordatch"], "link": "https://openai.com/blog/interpretable-machine-learning-through-teaching/", "abstract": "Some of the most transformative applications of powerful AI will come from computers and humans collaborating, but getting them to speak a common language is hard. Think about trying to guess the shape of a rectangle when you’re only shown a collection of random points inside that rectangle: it’s much faster to figure out the correct dimensions of the rectangle when you’re given points at the corners of the rectangle instead. Our machine teaching approach works as a cooperative game played between two agents, with one functioning as a student and the other as a teacher. The goal of the game is for the student to guess a particular concept (i.e. “dog”, “zebra”) based on examples of that concept (such as images of dogs), and the goal of the teacher is to learn to select the most illustrative examples for the student. Our two-stage technique works like this: a 'student' neural network is given randomly selected input examples of concepts and is trained from those examples using traditional supervised learning methods to guess the correct concept labels. In the second step, we let the 'teacher' network—which has an intended concept to teach and access to labels linking concepts to examples—to test different examples on the student and see which concept labels the student assigns them, eventually converging on the smallest set of examples it needs to give to let the student guess the intended concept. These examples end up looking interpretable because they are still grounded to the concepts (via the student trained in step one). In contrast, if we train the student and teacher jointly (as is done in a lot of current communication games), the student and teacher can collude to communicate via arbitrary examples that do not make sense to humans. For instance, the concept of a \"dog\" might end up being encoded through some arbitrary vectors that may be showing images of llamas and motorcycles, or a rectangle could be composed of two dots that look random to a human, but encode a specific rectangle's dimensions. To understand why our technique works, consider what happens when we use our method to teach the student to recognize concepts from example images that vary based on four properties: size (small, medium, large), color (red, blue, green), shape (square vs circle), and border (solid vs none). In this case, a concept is a set of properties that define a subset of the examples as belonging to that concept; for example, if the concept is red circles, then red circles of any size or border fit the concept. Our teacher network eventually learns to pick examples whose only common properties are the ones required by the concept, so that the student can rule out the irrelevant properties. To impart the concept of “red”, for instance, our teacher selects a large red square with no border and then a small red circle with a border. The only property the two shapes have in common is red, so the concept must only consist of red. This approach works across boolean, hierarchical, probabilistic, and rule-based concepts, with the teaching techniques invented by the teacher network frequently mirroring optimal strategies designed by humans. We also evaluated our approach on humans by giving them examples generated by the teacher network. We found that human subjects on mechanical turk given examples by our machine teacher were able to guess the correct concept more often than if they'd just been presented with random examples to guide them. While we only looked at teaching via examples in this work, the ideas can apply to the creation of interpretable communication between agents or other ways in which we would wish to make interaction between agents to be more understandable by humans. If you’re interested in working on such efforts, consider joining OpenAI !", "date": "2018-02-15"},
{"website": "Open-AI", "title": "debate", "author": ["Geoffrey Irving", "Dario Amodei"], "link": "https://openai.com/blog/debate/", "abstract": "We believe that this or a similar approach could eventually help us train AI systems to perform far more cognitively advanced tasks than humans are capable of, while remaining in line with human preferences. We're going to outline this method together with preliminary proof-of-concept experiments and are also releasing a web interface so people can experiment with the technique. One approach to aligning AI agents with human goals and preferences is to ask humans at training time which behaviors are safe and useful. While promising, this method requires humans to recognize good or bad behavior; in many situations an agent’s behavior may be too complex for a human to understand, or the task itself may be hard to judge or demonstrate. Examples include environments with very large, non-visual observation spaces — for instance, an agent that acts in a computer security-related environment, or an agent that coordinates a large set of industrial robots. How can we augment humans so that they can effectively supervise advanced AI systems? One way is to take advantage of the AI itself to help with the supervision, asking the AI (or a separate AI) to point out flaws in any proposed action. To achieve this, we reframe the learning problem as a game played between two agents, where the agents have an argument with each other and the human judges the exchange. Even if the agents have a more advanced understanding of the problem than the human, the human may be able to judge which agent has the better argument (similar to expert witnesses arguing to convince a jury). Our method proposes a specific debate format for such a game played between two dueling AI agents. The two agents can be trained by self play, similar to AlphaGo Zero or Dota 2 . Our hope is that, properly trained, such agents can produce value-aligned behavior far beyond the capabilities of the human judge. If the two agents disagree on the truth but the full reasoning is too large to show the humans, the debate can focus in on simpler and simpler factual disputes, eventually reaching a claim that is simple enough for direct judging. As an example, consider the question “What’s the best place to go on vacation?”. If an agent Alice purportedly does research on our behalf and says “Alaska”, it’s hard to judge if this is really the best choice. If a second agent Bob says “no, it’s Bali”, that may sound convincing since Bali is warmer. Alice replies “you can’t go to Bali because your passport won’t arrive in time”, which surfaces a flaw with Bali which had not occurred to us. But Bob counters “expedited passport service takes only two weeks”.  The debate continues until we reach a statement that the human can correctly judge, in the sense that the other agent doesn’t believe it can change the human’s mind. While we expect this approach to be most effective in the long-term when agents talk to each other with natural language, we need to test it in a simpler domain as today's natural language modeling is insufficient for this task. The test is easier with a task that is not beyond human capabilities.  We can do this by moving to the visual domain, and by replacing \"debaters have capabilities the judge lacks\" with \"debaters have knowledge the judge lacks\".  The goal of the judge is to guess the contents of an image, but the judge is blind aside from a few pixels chosen by the debaters.  The debaters see the complete image, and play a game where they alternate revealing single pixels to the judge for a few turns.  Here the full image is a stand-in for information beyond human scale, and each revealed pixel is a stand-in for a natural language discussion point. We tried this on the simplest possible visual task — MNIST.  The judge is not a human but a classifier trained in advance to predict MNIST digits from 6 non-black pixels, sampled at random for each image presentation when pretraining the judge.  Two agents then have a debate where they alternate revealing pixels, stopping at a total of 6 revealed pixels (so the judge sees only a little bit of information in total).  One debater is honest and tries to make the judge guess right, the other debater tries to make the judge guess wrong.  If both agents must declare in advance which digit they are arguing for, debate turns a 59.4% accurate sparse classifier into an 88.9% accurate debate.  That is, the honest player wins 88.9% of the time on the MNIST test set, in the process boosting the judge above its unaided accuracy. Intuitively, an image is more likely to contain pixels that convincingly demonstrate the truth than to contain pixels that convincingly demonstrate a lie, so 6 pixels chosen half honestly and half maliciously is much better than 6 random pixels.  If the judge were a human capable of reasoning about a few arguments at a time but not sifting through the huge set of all possible arguments, optimal play in debate can (we hope) reward debating agents for doing the sifting for us even if we lack an a priori mechanism for distinguishing good arguments from bad. The panel below shows several example games.  Liar wins are more interesting than honest wins, so we have chosen samples so that the liar wins in 50% of the games shown below (in reality the liar wins 11.1% of the games).  Mouse over one of the digits to see a game, including which pixels are revealed and the judge's possible hypothesis at each move: [1] Some digits are better lies than others.  It is particularly easy to convince the judge that digits are 8 or 9: The next step up in complexity for debate experiments is to still use images, but make them more elaborate, say cats vs. dogs.  More complex images likely require some natural language or common sense reasoning, so we haven’t done this for machine learning judges/agents yet.  Instead, we have made a prototype website for humans to try such experiments, playing the role of both judge and debaters. [2] Here agents can talk to the judge in natural language (the website assumes the humans have some text channel or are in the same room), but all of their statements could be lies.  Each agent can reveal one pixel over the course of the debate, and this pixel is guaranteed to be truthful. In a typical debate, Alice might honestly claim the image is a cat, and Bob lies and claims it is a dog. Alice can say “The center of this small rectangle is the cat’s green eye.” Bob cannot admit the center is an eye, so he concocts the further lie, “It’s a dog playing in grass, and that’s a blade of grass.” But this lie is hard to square with surrounding facts, such as Alice’s reply “If it were grass there were would be green at the top or bottom of this thin rectangle.” The debate continues until the agents focus in on a particular pixel which they disagree on, but where Bob is unable to invent a plausible counter, at which point Alice reveals the pixel and wins.  We’ve played this game informally at OpenAI, and the honest agent indeed tends to win, though to make it fair to the liar we usually limit the rate at which the judge can solicit information (it’s cognitively difficult to construct a detailed lie). The majority of our paper analyzes debate as a concept; the experiments above are quite preliminary.  In the future we’d like to do more difficult visual experiments and eventually experiments in natural language.  The judges should eventually be humans (or models trained from sparse human judgements) rather than ML models that metaphorically represent humans.  The agents should eventually be powerful ML systems that do things humans can't directly comprehend.  It will also be important to test debates over value-laden questions where human biases play a role, testing if it’s possible to get aligned behavior from biased human judges. Even with these improvements, there are some fundamental limitations to the debate model that may require it to be improved or augmented with other methods.  Debate does not attempt to address issues like adversarial examples or distributional shift — it is a way to get a training signal for complex goals, not a way to guarantee robustness of such goals (which would need to be achieved via additional techniques).  There is also no guarantee that debate will arrive at optimal play or correct statements — self play has worked well in practice for Go and other games but we have no theoretical guarantees about its performance.  Agents trained to debate use more computation than those trained to directly give an answer (even a bad/unsafe answer), so it’s possible debate could fail to be competitive with cheaper/less safe methods.  Finally, humans might simply be poor judges, either because they are not smart enough to make good judgements even after the agents zoom in on the simplest possible disputed facts, or because they are biased and will believe whatever they want to believe.  Most of these points are empirical questions that we hope to investigate. If debate or a similar approach works, it will make future AI systems safer by keeping them aligned to human goals and values even if AI grows too strong for direct human supervision.  Even for weaker systems that humans can supervise, debate could make the alignment task easier by reducing the sample complexity required to capture goals below the sample complexity required for strong performance at a task. To visualize what the judge might be thinking, we train a separate two hidden layer fully connected network to predict the original image from the sparse mask.  Thanks to Chris Olah for this visualization idea. ↩︎ The debate game website was made by Robert Lord .  Code available at https://github.com/openai/pixel . ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2018-05-03"},
{"website": "Open-AI", "title": "nonlinear-computation-in-linear-networks", "author": ["Jakob Foerster"], "link": "https://openai.com/blog/nonlinear-computation-in-linear-networks/", "abstract": "We've shown that deep linear networks — as implemented using floating-point arithmetic — are not actually linear and can perform nonlinear computation. We used evolution strategies to find parameters in linear networks that exploit this trait, letting us solve non-trivial problems. Neural networks consist of stacks of a linear layer followed by a nonlinearity like tanh or rectified linear unit. Without the nonlinearity, consecutive linear layers would be in theory mathematically equivalent to a single linear layer. So it's a surprise that floating point arithmetic is nonlinear enough to yield trainable deep networks. Numbers used by computers aren't perfect mathematical objects, but approximate representations using finite numbers of bits. Floating point numbers are commonly used by computers to represent mathematical objects. Each floating point number is represented by a combination of a fraction and an exponent. In the IEEE's float32 standard, 23 bits are used for the fraction and 8 for the exponent, and one for the sign. As a consequence of these conventions and the binary format used, the smallest normal non-zero number (in binary) is 1.0..0 x 2^-126 , which we refer to as min going forward. However, the next representable number is 1.0..01 x 2^-126 , which we can write as min + 0.0..01 x 2^-126 . It is evident that the gap between the 2nd number is by a factor of 2^20 smaller than gap between 0 and min. In float32, when numbers are smaller than the smallest representable number they get mapped to zero. Due to this 'underflow', around zero all computation involving floating point numbers becomes nonlinear. An exception to these restrictions is denormal numbers , which can be disabled on some computing hardware. While the GPU and cuBLAS have denormals enabled by default, TensorFlow builds all its primitives with denormals off (with the ftz=true flag set). This means that any non-matrix multiply operation written in TensorFlow has an implicit non-linearity following it (provided the scale of computation is near 1e-38). So, while in general the difference between any \"mathematical\" number and their normal float representation is small, around zero there is a large gap and the approximation error can be very significant. This can lead to some odd effects where the familiar rules of mathematics stop applying. For instance, $(a + b) \\times c$ becomes unequal to $a \\times c + b \\times c$. For example if you set $a = 0.4 \\times min$, $b = 0.5 \\times min$, and $c = 1 / min$. Then: $(a+b) \\times c = (0.4 \\times min + 0.5 \\times min) \\times 1 / min = (0 + 0) \\times 1 / min = 0$. However: $(a \\times c) + (b \\times c) = 0.4 \\times min / min + 0.5 \\times min \\times 1 / min = 0.9$. In another example, we can set $a = 2.5 \\times min$, $b = -1.6 \\times min$, and $c = 1 \\times min$. Then: $(a+b) + c = (0) + 1 \\times min = min$. However: $(b+c) + a = (0 \\times min) + 2.5 \\times min = 2.5 \\times min$. At this smallest scale the fundamental addition operation has become nonlinear! We wanted to know if this inherent nonlinearity could be exploited as a computational nonlinearity, as this would let deep linear networks perform nonlinear computations. The challenge is that modern differentiation libraries are blind to these nonlinearities at the smallest scale. As such, it would be difficult or impossible to train a neural network to exploit them via backpropagation. We can use evolution strategies (ES) to estimate gradients without having to rely on symbolic differentiation. Using ES we can indeed exploit the near-zero behavior of float32 as a computational nonlinearity. When trained on MNIST a deep linear network trained via backpropagation achieves a training accuracy of 94% and a testing accuracy of 92%. In contrast, the same linear network can achieve >99% training and 96.7% test accuracy when trained with ES and ensuring that the activations are sufficiently small to be in the nonlinear range of float32. This increase in training performance is due to ES exploiting the nonlinearities in the float32 representation. These powerful nonlinearities allow any layer to generate novel features which are nonlinear combinations of lower level features.  Here is the network structure: Beyond MNIST, we think other interesting experiments could be extending this work to recurrent neural networks, or to exploit nonlinear computation to improve complex machine learning tasks like language modeling and translation. We're excited to explore this capability with our fellow researchers.", "date": "2017-09-29"},
{"website": "Open-AI", "title": "openai-baselines-ppo", "author": ["John Schulman", "Oleg Klimov", "Filip Wolski", "Prafulla Dhariwal", "Alec Radford"], "link": "https://openai.com/blog/openai-baselines-ppo/", "abstract": "We’re releasing a new class of reinforcement learning algorithms, Proximal Policy Optimization (PPO) , which perform comparably or better than state-of-the-art approaches while being much simpler to implement and tune. PPO has become the default reinforcement learning algorithm at OpenAI because of its ease of use and good performance. Policy gradient methods are fundamental to recent breakthroughs in using deep neural networks for control, from video games , to 3D locomotion , to Go . But getting good results via policy gradient methods is challenging because they are sensitive to the choice of stepsize — too small, and progress is hopelessly slow; too large and the signal is overwhelmed by the noise, or one might see catastrophic drops in performance. They also often have very poor sample efficiency, taking millions (or billions) of timesteps to learn simple tasks. Researchers have sought to eliminate these flaws with approaches like TRPO and ACER , by constraining or otherwise optimizing the size of a policy update. These methods have their own trade-offs — ACER is far more complicated than PPO, requiring the addition of code for off-policy corrections and a replay buffer, while only doing marginally better than PPO on the Atari benchmark; TRPO — though useful for continuous control tasks — isn't easily compatible with algorithms that share parameters between a policy and value function or auxiliary losses, like those used to solve problems in Atari and other domains where the visual input is significant. With supervised learning, we can easily implement the cost function, run gradient descent on it, and be very confident that we'll get excellent results with relatively little hyperparameter tuning. The route to success in reinforcement learning isn't as obvious — the algorithms have many moving parts that are hard to debug, and they require substantial effort in tuning in order to get good results. PPO strikes a balance between ease of implementation, sample complexity, and ease of tuning, trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small. We've previously detailed a variant of PPO that uses an adaptive KL penalty to control the change of the policy at each iteration. The new variant uses a novel objective function not typically found in other algorithms: \\[L^{CLIP}(\\theta) = \\hat{E}_{t}[ min( r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1 - \\varepsilon, 1 + \\varepsilon) \\hat{A}_t ) ]\\] \\(\\theta\\) is the policy parameter \\(\\hat{E}_t\\) denotes the empirical expectation over timesteps \\(r_{t}\\) is the ratio of the probability under the new and old policies, respectively \\(\\hat{A}_t\\)is the estimated advantage at time \\(t\\) \\(\\varepsilon\\) is a hyperparameter, usually 0.1 or 0.2 This objective implements a way to do a Trust Region update which is compatible with Stochastic Gradient Descent, and simplifies the algorithm by removing the KL penalty and need to make adaptive updates. In tests, this algorithm has displayed the best performance on continuous control tasks and almost matches ACER's performance on Atari, despite being far simpler to implement. We've created interactive agents based on policies trained by PPO — we can use the keyboard to set new target positions for a robot in an environment within Roboschool; though the input sequences are different from what the agent was trained on, it manages to generalize. We've also used PPO to teach complicated, simulated robots to walk, like the 'Atlas' model from  Boston Dynamics shown below; the model has 30 distinct joints, versus 17 for the bipedal robot. Other researchers have used PPO to train simulated robots to perform impressive feats of parkour while running over obstacles. This release of baselines includes scalable, parallel implementations of PPO and TRPO which both use MPI for data passing. Both use Python3 and TensorFlow. We're also adding pre-trained versions of the policies used to train the above robots to the Roboschool agent zoo . Update : We're also releasing a GPU-enabled implementation of PPO, called PPO2. This runs approximately 3X faster than the current PPO baseline on Atari. In addition, we're releasing an implementation of Actor Critic with Experience Replay (ACER),  a sample-efficient policy gradient algorithm. ACER makes use of a replay buffer, enabling it to perform more than one gradient update using each piece of sampled experience, as well as a Q-Function approximate trained with the Retrace algorithm. We’re looking for people to help build and optimize our reinforcement learning algorithm codebase. If you’re excited about RL, benchmarking, thorough experimentation, and open source, please apply , and mention that you read the baselines PPO post in your application.", "date": "2017-07-20"},
{"website": "Open-AI", "title": "learning-to-model-other-minds", "author": ["Jakob Foerster", "Richard Chen", "Pieter Abbeel", "Igor Mordatch", "Shimon Whiteson", "Maruan Al-Shedivat"], "link": "https://openai.com/blog/learning-to-model-other-minds/", "abstract": "We're releasing an algorithm which accounts for the fact that other agents are learning too, and discovers self-interested yet collaborative strategies like tit-for-tat in the iterated prisoner's dilemma. This algorithm, Learning with Opponent-Learning Awareness (LOLA), is a small step towards agents that model other minds. LOLA, a collaboration by researchers at OpenAI and the University of Oxford, lets an RL agent take account of the learning of others when updating its own strategy. Each LOLA agent adjusts its policy in order to shape the learning of the other agents in a way that is advantageous. This is possible since the learning of the other agents depends on the rewards and observations occurring in the environment, which in turn can be influenced by the agent. This means that the LOLA agent, ‘Alice’, models how the parameter updates of the other agent, ‘Bob’, depend on its own policy and how Bob's parameter update impacts its own future expected reward. Alice then updates its own policy in order to make the learning step of the other agents, like Bob, more beneficial to its own goals. LOLA agents can  discover effective, reciprocative strategies, in games like the iterated prisoner's dilemma , or the coin game . In contrast, state-of-the-art deep reinforcement learning methods, like Independent PPO, fail to learn such strategies in these domains. These agents typically learn to take selfish actions that ignore the objectives of other agents. LOLA solves this by letting agents act out of a self-interest that incorporates the goals of others. It also works without requiring hand-crafted rules, or environments set up to encourage cooperation. The inspiration for LOLA comes from how people collaborate with one another: Humans are great at reasoning about how their actions can affect the future behavior of other humans, and frequently invent ways to collaborate with others that leads to a 'win win'. One of the reasons humans are good at collaborating with each other is that they have a sense of a “theory of mind” about other humans, letting them come up with strategies that lead to benefits for their collaborators. So far, this sort of “theory of mind” representation has been absent from deep multi-agent reinforcement learning. To a state of the art deep-RL agent there is no inherent difference between another learning agent and a part of the environment, say a tree. The key to LOLA’S performance is the inclusion of term: \\[ \\left( \\frac{\\partial V^1 (\\theta^1_i,\\theta^2_i) }{\\partial \\theta^2_i} \\right)^T \\frac{\\partial^2 V^2 (\\theta^1_i,\\theta^2_i)}{\\partial \\theta^1_i \\partial \\theta^2_i} \\cdot \\delta \\eta, \\] Here the left hand side captures how Alice’s return depends on the change in Bob’s policy. The right hand side,  describes how Bob’s learning step depends on Alice’s policy. Multiplying those two components essentially measures how Alice can change Bob’s learning step such that it leads to an increase in Alice's rewards. This means that when we train our agents they try to optimize their return after one anticipated learning step of the opponent. By differentiating through this anticipated learning step the agent can actively shape the parameter update of the opponent in a way that increases their own return. While the formula above assume access to the true gradient and hessian of the two value functions, we can also estimate all relevant terms using samples. In particular the second order term can be estimated by applying the policy gradient theorem, which makes LOLA suitable for any deep reinforcement learning setting. LOLA can handle this by including a step of opponent modeling where we fit a model of the opponent to the observed trajectories - predicting the parameters of other agents based on their actions. In the future we would like to extend this by also inferring architectures and rewards from the observed learning. By considering their impact on the learning process of other agents, LOLA agents (left) learn collaborative strategies, while other approaches like Independent Policy Gradient (right) struggle in environments like the coin game . LOLA lets us train agents that succeed at the coin game , in which two agents, red and blue, compete with one another to pick up red and blue colored coins. Each agent gets a point for picking up any coin, but if they pick up a coin which isn't their color then the other agent will receive a -2 penalty. Thus, if both agents greedily pick up both coins, everyone gets zero points on average. LOLA agents learn to predominantly pick up coins of their own color, leading to high scores (shown above). LOLA works best when using large batch-sizes and full roll-outs for variance reduction. This means that the method is both memory and compute intensive. Furthermore, under opponent modeling LOLA can exhibit instability which we hope to address with future improvements.", "date": "2017-09-14"},
{"website": "Open-AI", "title": "more-on-dota-2", "author": ["OpenAI"], "link": "https://openai.com/blog/more-on-dota-2/", "abstract": "Our Dota 2 result shows that self-play can catapult the performance of machine learning systems from far below human level to superhuman, given sufficient compute. In the span of a month, our system went from barely matching a high-ranked player to beating the top pros and has continued to improve since then. Supervised deep learning systems can only be as good as their training datasets, but in self-play systems, the available data improves automatically as the agent gets better. The project's timeline is the following. For some perspective, 15% of players are below 1.5k MMR ; 58% of players are below 3k; 99.99% are below 7.5k. March 1st: had our first classical reinforcement learning results in a simple Dota environment, where a Drow Ranger learns to kite a hardcoded Earthshaker. May 8th: 1.5k MMR tester says he's been getting better faster than the bot. Early June: beat 1.5k MMR tester June 30th: winning most games against 3k MMR tester July 8th: barely get first win against 7.5k MMR semi-pro tester. August 7th: beat Blitz (6.2k former pro) 3-0, Pajkatt (8.5k pro) 2-1, and CC&C (8.9k pro) 3-0. All agreed that Sumail would figure out how to beat it. August 9th: beat Arteezy (10k pro, top player) 10-0. He says Sumail could figure out this bot. August 10th: beat Sumail (8.3k pro, top 1v1 player) 6-0, who says it's unbeatable. Plays the Aug 9th bot, where he goes 2-1. August 11th: beat Dendi (7.3k pro, former world champion, old-school crowd favorite) 2-0. Bot has 60% win rate versus August 10th bot. The full game is 5v5, but 1v1 also appears in some tournaments . Our bot played under standard tournament rules — we did not add AI-specific simplifications to 1v1. The bot operated off the following interfaces: Observations: Bot API features, which are designed to be the same set of features that humans can see, related to heroes, creeps, courier, and the terrain near the hero. The game is partially observable. Actions: Actions accessible by the bot API, chosen at a frequency comparable to humans, including moving to a location, attacking a unit, or using an item. Feedback: The bot received incentives for winning and basic metrics like health and last hits . We whitelisted a few dozen item builds that bots could use, and picked one for evaluation. We also separately trained the initial creep block using traditional RL techniques, as it happens before the opponent appears. Our approach, combining small amounts of \"coaching\" with self-play, allowed us to massively improve our agent between the Monday and Thursday of The International. On Monday evening, Pajkatt won using an unusual item build (buying an early magic wand). We added this item build to the training whitelist. Around 1pm on Wednesday, we tested the latest bot. The bot would lose a bunch of health in the first wave. We thought perhaps we needed to roll back, but noticed further gameplay was amazing, and the first wave behavior was baiting the other bots to be aggressive towards it. Further self-play fixed the issue, as the bot learned to counter the baiting strategy. In the meanwhile, we stitched it together with Monday's bot for the first wave only, and completed the process twenty minutes before Arteezy showed up at 4pm. After the Arteezy matches, we updated the creep block model, which increased TrueSkill by one point. Further training before Sumail's match on Thursday increased TrueSkill by two points. Sumail pointed out that the bot had learned to cast razes out of the enemy's vision. This was due to a mechanic we hadn't known about: abilities cast outside of the enemy's vision prevent the enemy from gaining a wand charge. Arteezy also played a match against our 7.5k semi-pro tester. Arteezy was winning the whole game, but our tester still managed to surprise him with a strategy he'd learned from the bot. Arteezy remarked afterwards that this was a strategy that Paparazi had used against him once and was not commonly practiced. Though Sumail called the bot \"unbeatable\", it can still be confused in situations very different from what it's seen. We set up the bot at a LAN event at The International, where players played over 1,000 games to beat the bot by any means possible. The successful exploits fell into three archetypes: Creep pulling: it's possible to repeatedly attract the lane creeps into chasing you right when they spawn (between the bot's tier 2 and tier 3 towers). You end up with dozens of creeps chasing you around the map, and eventually the bot's tower dies via attrition. Orb of venom + wind lace: this gives you a big movement speed advantage over the bot at level 1 and allows for a quick first blood. You need to exploit this head start to kill the bot one more time. Level 1 raze: this requires a lot of skill, but several 6-7k MMR players were able to kill the bot at level 1 by successfully hitting 3-5 razes in a short span of time. Fixing these issues for 1v1 would be similar to fixing the Pajkatt bug. But for 5v5, such issues aren't exploits at all, and we'll need a system which can handle totally weird and wacky situations it's never seen. We’re not ready to talk about agent internals — the team is focused on solving 5v5 first. The first step in the project was figuring out how to run Dota 2 in the cloud on a physical GPU. The game gave an obscure error message on GPU cloud instances. But when starting it on Greg's personal GPU desktop (which is the desktop brought onstage during the show), we noticed that Dota booted when the monitor was plugged in, but gave the same error message when unplugged. So we configured our cloud GPU instances to pretend there was a physical monitor attached. Dota didn't support custom dedicated servers at the time, meaning that running scalably and without a GPU was possible only with very slow software rendering. We then created a shim to stub out most OpenGL calls, except the ones needed to boot. At the same time, we wrote a scripted bot — we needed a baseline for comparison (especially because the builtin bots don't work well on 1v1) and to understand all the semantics of the bot API . The scripted bot reaches 70 last hits in ten minutes on an empty lane, but still loses to reasonable humans. Our current best 1v1 bot reaches more like 97 (it destroys the tower before then, so we can only extrapolate), and the theoretical maximum is 101. Bot playing versus SirActionSlacks . The strategy of distracting the bot with a courier rush did not work. 1v1 is complicated, but 5v5 is an ocean of complexity. We know we'll need to further push the limits of AI in order to solve it. One well-established place to start is with behavioral cloning. Dota has about a million public matches a day. The replays for these matches are stored on Valve's servers for two weeks. We've been downloading every expert-level replay since last November, and have amassed a dataset of 5.8M games (each game is about 45 minutes with 10 humans). We use OpenDota to discover these replays, and are donating $12k (10 years of their fundraising goal) to support the project. We have many more ideas, and are hiring engineers (must be intrigued by machine learning, but need not be an expert) and researchers to help us make this happen. We thank Microsoft Azure and Valve for their support in this endeavor.", "date": "2017-08-16"},
{"website": "Open-AI", "title": "dota-2", "author": ["OpenAI"], "link": "https://openai.com/blog/dota-2/", "abstract": "We've created a bot which beats the world's top professionals at 1v1 matches of Dota 2 under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans. Today we played Dendi on mainstage at The International , winning a best-of-three match. Over the past week, our bot was undefeated against many top professionals including SumaiL (top 1v1 player in the world) and Arteezy (top overall player in the world). Dota 1v1 is a complex game with hidden information. Agents must learn to plan, attack, trick, and deceive their opponents. The correlation between player skill and actions-per-minute is not strong, and in fact, our AI's actions-per-minute are comparable to that of an average human player. Success in Dota requires players to develop intuitions about their opponents and plan accordingly. In the above video you can see that our bot has learned — entirely via self-play — to predict where other players will move, to improvise in response to unfamiliar situations, and how to influence the other player's allied units to help it succeed. The full game of Dota is played by two teams of five. Each player chooses from a hundred heroes and hundreds of items. Our next step is to create a team of Dota 2 bots which can compete and collaborate with the top human teams. If you’d like to work on the next phase of the project, consider joining OpenAI .", "date": "2017-08-11"},
{"website": "Open-AI", "title": "infrastructure-for-deep-learning", "author": ["Vicki Cheung", "Jonas Schneider", "Ilya Sutskever", "Greg Brockman"], "link": "https://openai.com/blog/infrastructure-for-deep-learning/", "abstract": "In this post, we'll share how deep learning research usually proceeds, describe the infrastructure choices we've made to support it, and open-source kubernetes-ec2-autoscaler , a batch-optimized scaling manager for Kubernetes. We hope you find this post useful in building your own deep learning infrastructure. A typical deep learning advance starts out as an idea, which you test on a small problem. At this stage, you want to run many ad-hoc experiments quickly. Ideally, you can just SSH into a machine, run a script in screen, and get a result in less than an hour. Making the model really work usually requires seeing it fail in every conceivable way and finding ways to fix those limitations. (This is similar to building any new software system, where you'll run your code many times to build an intuition for how it behaves.) So deep learning infrastructure must allow users to flexibly introspect models, and it's not enough to just expose summary statistics. Once the model shows sufficient promise, you'll scale it up to larger datasets and more GPUs. This requires long jobs that consume many cycles and last for multiple days. You'll need careful experiment management, and to be extremely thoughtful about your chosen range of hyperparameters. The early research process is unstructured and rapid; the latter is methodical and somewhat painful, but it's all absolutely necessary to get a great result. The paper Improved Techniques for Training GANs began with Tim Salimans devising several ideas for improving Generative Adversarial Network training. We'll describe the simplest of these ideas (which happened to produce the best-looking samples, though not the best semi-supervised learning). GANs consist of a generator and a discriminator network. The generator tries to fool the discriminator, and the discriminator tries to distinguish between generated data and real data. Intuitively, a generator which can fool every discriminator is quite good. But there is a hard-to-fix failure mode: the generator can “collapse” by always outputting exactly the same (likely realistic-looking!) sample. Tim had the idea to give discriminator an entire minibatch of samples as input, rather than just one sample. Thus the discriminator can tell whether the generator just constantly produces a single image. With the collapse discovered, gradients will be sent to the generator to correct the problem. The next step was to prototype the idea on MNIST and CIFAR-10 . This required prototyping a small model as quickly as possible, running it on real data, and inspecting the result. After some rapid iteration, Tim got very encouraging CIFAR-10 samples — pretty much the best samples we'd seen on this dataset. However, deep learning (and AI algorithms in general) must be scaled to be truly impressive — a small neural network is a proof of concept, but a big neural network actually solves the problem and is useful. So Ian Goodfellow dug into scaling the model up to work on ImageNet . With a larger model and dataset, Ian needed to parallelize the model across multiple GPUs. Each job would push multiple machines to 90% CPU and GPU utilization, but even then the model took many days to train. In this regime, every experiment became precious, and he would meticulously log the results of each experiment. Ultimately, while the results were good, they were not as good as we hoped. We've tested many hypotheses as to why, but still haven't cracked it. Such is the nature of science. The vast majority of our research code is written in Python, as reflected in our open - source projects . We mostly use TensorFlow (or Theano in special cases) for GPU computing; for CPU we use those or Numpy . Researchers also sometimes use higher-level frameworks like Keras on top of TensorFlow. Like much of the deep learning community, we use Python 2.7. We generally use Anaconda , which has convenient packaging for otherwise difficult packages such as OpenCV and performance optimizations for some scientific libraries. For an ideal batch job, doubling the number of nodes in your cluster will halve the job's runtime. Unfortunately, in deep learning, people usually see very sublinear speedups from many GPUs. Top performance thus requires top-of-the-line GPUs. We also use quite a lot of CPU for simulators , reinforcement learning environments , or small-scale models (which run no faster on a GPU). AWS generously agreed to donate a large amount of compute to us. We're using them for CPU instances and for horizontally scaling up GPU jobs. We also run our own physical servers, primarily running Titan X GPUs. We expect to have a hybrid cloud for the long haul: it's valuable to experiment with different GPUs, interconnects, and other techniques which may become important for the future of deep learning. We approach infrastructure like many companies treat product: it must present a simple interface, and usability is as important as functionality. We use a consistent set of tools to manage all of our servers and configure them as identically as possible. We use Terraform to set up our AWS cloud resources (instances, network routes, DNS records, etc). Our cloud and physical nodes run Ubuntu and are configured with Chef . For faster spinup times, we pre-bake our cluster AMIs using Packer . All our clusters use non-overlapping IP ranges and are interconnected over the public internet with OpenVPN on user laptops, and strongSwan on physical nodes (which act as AWS Customer Gateways ). We store people's home directories, data sets, and results on NFS (on physical hardware) and EFS / S3 (on AWS). Scalable infrastructure often ends up making the simple cases harder. We put equal effort into our infrastructure for small- and large-scale jobs, and we're actively solidifying our toolkit for making distributed use-cases as accessible as local ones. We provide a cluster of SSH nodes (both with and without GPUs) for ad-hoc experimentation, and run Kubernetes as our cluster scheduler for physical and AWS nodes. Our cluster spans 3 AWS regions — our jobs are bursty enough that we'll sometimes hit capacity on individual regions. Kubernetes requires each job to be a Docker container, which gives us dependency isolation and code snapshotting. However, building a new Docker container can add precious extra seconds to a researcher's iteration cycle, so we also provide tooling to transparently ship code from a researcher's laptop into a standard image. We expose Kubernetes's flannel network directly to researchers' laptops, allowing users seamless network access to their running jobs. This is especially useful for accessing monitoring services such as TensorBoard . (Our initial approach — which is cleaner from a strict isolation perspective — required people to create a Kubernetes Service for each port they wanted to expose, but we found that it added too much friction.) Our workload is bursty and unpredictable: a line of research can go quickly from single-machine experimentation to needing 1,000 cores. For example, over a few weeks, one experiment went from an interactive phase on a single Titan X, to an experimental phase on 60 Titan Xs, to needing nearly 1600 AWS GPUs. Our cloud infrastructure thus needs to dynamically provision Kubernetes nodes. It's easy to run Kubernetes nodes in Auto Scaling groups, but it's harder to correctly manage the size of those groups. After a batch job is submitted, the cluster knows exactly what resources it needs, and should allocate those directly. (In contrast, AWS's Scaling Policies will spin up new nodes piecemeal until resources are no longer exhausted, which can take multiple iterations.) Also, the cluster needs to drain nodes before terminating them to avoid losing in-flight jobs. It's tempting to just use raw EC2 for big batch jobs, and indeed that's where we started. However, the Kubernetes ecosystem adds quite a lot of value: low-friction tooling, logging, monitoring, ability to manage physical nodes separately from the running instances, and the like. Making Kubernetes autoscale correctly was easier than rebuilding this ecosystem on raw EC2. We're releasing kubernetes-ec2-autoscaler , a batch-optimized scaling manager for Kubernetes. It runs as a normal Pod on Kubernetes and requires only that your worker nodes are in Auto Scaling groups. The autoscaler works by polling the Kubernetes master's state, which contains everything needed to calculate the cluster resource ask and capacity. If there's excess capacity, it drains the relevant nodes and ultimately terminates them. If more resources are needed, it calculates what servers should be created and increases your Auto Scaling group sizes appropriately (or simply uncordons drained nodes, which avoids new node spinup time). kubernetes-ec2-autoscaler handles multiple Auto Scaling groups, resources beyond CPU (memory and GPUs), and fine-grained constraints on your jobs such as AWS region and instance size. Additionally, bursty workloads can lead to Auto Scaling Groups timeouts and errors, since (surprisingly!) even AWS does not have infinite capacity. In these cases, kubernetes-ec2-autoscaler detects the error and overflows to a secondary AWS region. Our infrastructure aims to maximize the productivity of deep learning researchers, allowing them to focus on the science. We're building tools to further improve our infrastructure and workflow, and will share these in upcoming weeks and months. We welcome help to make this go even faster!", "date": "2016-08-29"},
{"website": "Open-AI", "title": "robust-adversarial-inputs", "author": ["Anish Athalye"], "link": "https://openai.com/blog/robust-adversarial-inputs/", "abstract": "We've created images that reliably fool neural network classifiers when viewed from varied scales and perspectives. This challenges a claim from last week that self-driving cars would be hard to trick maliciously since they capture images from multiple scales, angles, perspectives, and the like. Out-of-the-box adversarial examples do fail under image transformations. Below, we show the same cat picture, adversarially perturbed to be incorrectly classified as a desktop computer by Inception v3 trained on ImageNet . A zoom of as little as 1.002 causes the classification probability for the correct label tabby cat to override the adversarial label desktop computer . However, we'd suspected that active effort could produce a robust adversarial example, as adversarial examples have been shown to transfer to the physical world. Adversarial examples can be created using an optimization method called projected gradient descent to find small perturbations to the image that arbitrarily fool the classifier. Instead of optimizing for finding an input that's adversarial from a single viewpoint, we optimize over a large ensemble of stochastic classifiers that randomly rescale the input before classifying it. Optimizing against such an ensemble produces robust adversarial examples that are scale-invariant. Even when we restrict ourselves to only modifying pixels corresponding to the cat, we can create a single perturbed image that is simultaneously adversarial at all desired scales. By adding random rotations, translations, scales, noise, and mean shifts to our training perturbations, the same technique produces a single input that remains adversarial under any of these transformations. Our transformations are sampled randomly at test time, demonstrating that our example is invariant to the whole distribution of transformations.", "date": "2017-07-17"},
{"website": "Open-AI", "title": "faster-robot-simulation-in-python", "author": ["Jonas Schneider", "Peter Welinder", "Alex Ray", "Jonathan Ho", "Wojciech Zaremba"], "link": "https://openai.com/blog/faster-robot-simulation-in-python/", "abstract": "We're open-sourcing a high-performance Python library for robotic simulation using the MuJoCo engine, developed over our past year of robotics research. This library is one of our core tools for deep learning robotics research , which we've now released as a major version of mujoco-py , our Python 3 bindings for MuJoCo. mujoco-py 1.50.1.0 brings a number of new capabilities and significant performance boosts. New features include: Efficient handling of parallel simulations GPU-accelerated headless 3D rendering Direct access to MuJoCo functions and data structures Support for all MuJoCo 1.50 features like its improved contact solver Many methods in trajectory optimization and reinforcement learning (like LQR , PI2 , and TRPO ) benefit from being able to run multiple simulations in parallel. mujoco-py uses data parallelism through OpenMP and direct-access memory management through Cython and NumPy to make batched simulation more efficient. Naive usage of the new version’s MjSimPool interface shows a 400% speedup over the old, and still about 180% over an optimized and restricted usage pattern using Python’s multiprocessing package to gain the same level of parallelism. The majority of the speedup comes from reduced access times to the various MuJoCo data structures. Check out examples/simpool.py for a tour of MjSimPool . We use the domain randomization technique across many projects at OpenAI. The latest version of mujoco-py supports headless GPU rendering; this yields a speedup of ~40x compared to CPU-based rendering, letting us generate hundreds of frames per second of synthetic image data. In the above (slowed down) animation we use this to vary the textures of one of our robots, which helps it identify its body when we transfer it from the simulator to reality. Check out examples/disco_fetch.py for an example of randomized texture generation. The API exposed by mujoco-py is sufficient to enable Virtual Reality interaction without any extra C++ code. We ported MuJoCo’s C++ VR example to Python using mujoco-py. If you have an HTC Vive VR setup, you can give try it using this example (this support is considered experimental, but we've been using it internally for a while). The simplest way to get started with mujoco-py is with the MjSim class . It is a wrapper around the simulation model and data, and lets you to easily step the simulation and render images from camera sensors. Here's a simple example: For advanced users, we provide a number of lower-level interfaces for accessing the innards of the MuJoCo C structs and functions directly. Refer to the README and the full documentation to learn more.", "date": "2017-06-28"},
{"website": "Open-AI", "title": "team-plus-plus", "author": ["Greg Brockman"], "link": "https://openai.com/blog/team-plus-plus/", "abstract": "We've had some fantastic people join over the past few months (and we're still hiring ). Welcome, everyone! Yura Burda. Yura finished a math PhD at the age of 24, and switched into machine learning a year and a half ago. He's focusing on generative models . He discovered a simple but fundamental improvement to the variational lower bound that had evaded notice since its original discovery decades ago. Ian Goodfellow. Ian is well known for his many contributions to machine learning, including the MaxOut Network and the Generative Adversarial Network , the latter of which is a driver of excitement in generative modeling research. In addition, he's the lead author of the book on deep learning. Alec Radford. Alec created DCGAN , a neural model that could generate large, coherent images containing an unprecedented level of global coherence and detail. In addition, his model has learned to do image analogies in an entirely unsupervised way. Tim Salimans. Tim is an expert on variational methods. The author of the first paper on stochastic gradient variational inference (for which he won the Lindley prize ), he was at one point ranked number 2 overall on Kaggle. Also joining us for the summer (and in some cases, to continue the collaboration once they return to their home institution): Peter Chen. Peter previously co-founded a startup and did research on parallel computing and cognitive science. Recently, he's been working on reinforcement learning, from theory to applications with deep neural nets. Rocky Duan. Rocky is a PhD student who previously co-founded a startup (with Peter) and worked on a number of projects in robotics. He's now transitioned to the field of deep reinforcement learning, and recently TA'd Berkeley's Deep Reinforcement Learning class. Linxi Fan. Although Linxi is just finishing up his undergrad, he has already worked on a variety of projects in deep learning, including Deep Speech 2 . Jon Gauthier. Jon is an undergraduate student whose achievements include, among others, a method that makes it possible to efficiently train large recursive neural networks on large datasets. Jonathan Ho. Jonathan is a PhD student at Stanford, where he is developing a new approach to imitation learning. Earlier in his research career, he had success teaching robots how to tie knots . Rein Houthooft. Rein hails from Belgium, started his research career in the field of computer networking, and now is developing novel ways to incorporate uncertainty into deep reinforcement learning algorithms. Eric Price. Eric is a professor of theoretical computer science at UT Austin. His achievements include the development and the analysis of a faster-than- n log n sparse Fourier transform . In his past life, he achieved a perfect score at the IOI. As a closing note, we get a lot of questions about what we're working on, how we work, and what we're trying to achieve. We're not being intentionally mysterious; we've just been busy launching the organization (and finding awesome people to help us do so!). We're currently focused on unsupervised learning and reinforcement learning. We should have interesting results to share over the next month or two. A bunch of us will be around ICLR , where we'll likely hold an event of some form. I'll also host a Quora Session in May or June to answer any questions for people we don't meet in Puerto Rico.", "date": "2016-03-31"},
{"website": "Open-AI", "title": "team-update", "author": ["Greg Brockman"], "link": "https://openai.com/blog/team-update/", "abstract": "We'd like to welcome the latest set of team members to OpenAI (and we're still hiring !): Marcin Andrychowicz. Marcin received 3 gold medals in the IOI , and has been a top participant in programming competitions such as TopCoder and ACM-ICPC . He's been in deep learning for a year and has already made strong progress on neural memory architectures . Rafał Józefowicz. Rafał began his career in competitive programming and the finance industry. He's now been in deep learning for a year and a half, and his results include the state-of-the-art language model . Kate Miltenberger. Kate has a versatile background, with experience across operations, office administration, user research, community, and support. She previously helped Academia.edu run smoothly. Ludwig Pettersson. Ludwig was previously Stripe's Creative Director, where he built and led the design team. Jonas Schneider. Jonas did much of the engineering heavy lifting on OpenAI Gym . A recent college graduate, he was previously an intern at Stripe, where he helped build Stripe CTF3 . Jie Tang. Jie was an engineer at Dropbox for almost five years, where he led the team responsible for the core file sync technology running on hundreds of millions of desktops. Prior to that he worked in Pieter Abbeel's robotics lab at Berkeley, working on autonomous helicopters, RGBD perception, and Starcraft bots. Prafulla Dhariwal. Prafulla was a gold medalist in the IMO , IPhO , and IAO . He's currently an undergraduate at MIT, performing research on learning of invariant representations for speech and vision tasks. Paul Christiano. Paul is a PhD student at Berkeley who has written extensively about AI safety. He received best paper and best student paper awards at STOC for research on optimization and online learning .", "date": "2016-05-25"},
{"website": "Open-AI", "title": "openai-technical-goals", "author": ["Ilya Sutskever", "Greg Brockman", "Sam Altman", "Elon Musk"], "link": "https://openai.com/blog/openai-technical-goals/", "abstract": "OpenAI’s mission is to build safe AI, and ensure AI's benefits are as widely and evenly distributed as possible. We’re trying to build AI as part of a larger community, and we want to share our plans and capabilities along the way. We’re also working to solidify our organization's governance structure and will share our thoughts on that later this year. Defining a metric for intelligence is tricky, but we need one to measure our progress and focus our research. We're thus building a living metric which measures how well an agent can achieve its user’s intended goal in a wide range of environments. The metric will consist of a variety of OpenAI Gym environments with a unified action and observation space (so a single agent can run across all of them), including games, robotics, and language-based tasks. Our implementation will evolve over time, and we’ll keep the community updated along the way. A significant fraction of our research bandwidth is being spent on fundamental research. We’ll always be developing and testing new ideas, especially those that don’t fit neatly into our current worldview. This is important — our current ideas will not be enough to achieve our long-term goal. We’ve also formed teams around specific projects. The intention isn’t just to solve these problems, but to develop general learning algorithms in the process. These algorithms will, in turn, help us build agents that are more capable according to our metric. These projects are: We're working to enable a physical robot (off-the-shelf; not manufactured by OpenAI) to perform basic housework. There are existing techniques for specific tasks, but we believe that learning algorithms can eventually be made reliable enough to create a general-purpose robot. More generally, robotics is a good testbed for many challenges in AI. We plan to build an agent that can perform a complex task specified by language, and ask for clarification about the task if it’s ambiguous. Today, there are promising algorithms for supervised language tasks such as question answering , syntactic parsing and machine translation but there aren’t any for more advanced linguistic goals, such as the ability to carry a conversation, the ability to fully understand a document, and the ability to follow complex instructions in natural language. We expect to develop new learning algorithms and paradigms to tackle these problems. We aim to train an agent capable enough to solve any game in our initial metric. Games are virtual mini-worlds that are very diverse, and learning to play games quickly and well will require significant advances in generative models and reinforcement learning . (We are inspired by the pioneering work of DeepMind , who have produced impressive results in this area in the past few years.) Our projects and fundamental research all have shared cores, so progress on any is likely to benefit the others. Each captures a different aspect of goal-solving, and was chosen for its potential to significantly move our metric. We’re just getting started on these projects, and the details may change as we gain additional data. We also expect to add new projects over time. If you’re excited about any of the above, we'd love to hear from you, whether by discussing with others in the OpenAI community or joining us full-time.", "date": "2016-06-20"},
{"website": "Open-AI", "title": "openai-supporters", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-supporters/", "abstract": "We're excited to welcome the following new donors to OpenAI: Jed McCaleb , Gabe Newell , Michael Seibel , Jaan Tallinn , and Ashton Eaton and Brianne Theisen-Eaton . Reid Hoffman is significantly increasing his contribution. Pieter Abbeel (having completed his sabbatical with us), Julia Galef , and Maran Nelson are becoming advisors to OpenAI. Additionally, Elon Musk will depart the OpenAI Board but will continue to donate and advise the organization. As Tesla continues to become more focused on AI, this will eliminate a potential future conflict for Elon. We’re broadening our base of funders to prepare for the next phase of OpenAI, which will involve ramping up our investments in our people and the compute resources necessary to make consequential breakthroughs in artificial intelligence. OpenAI was founded a little over two years ago and since that time we’ve paired our research efforts with applied work to push the limits of what AI systems are capable of via our work in robotics and Dota . That’s going to continue, and in the coming months you can also expect us to articulate the principles with which we’ll be approaching the next phase of OpenAI, and the policy areas in which we wish to see changes to ensure AI benefits all of humanity. The Board is now Greg Brockman, Ilya Sutskever, Holden Karnofsky, and Sam Altman. We will add another director soon, and plan over time to further expand the Board. If you’re interested in working with us on this mission, consider joining OpenAI .", "date": "2018-02-20"},
{"website": "Open-AI", "title": "concrete-ai-safety-problems", "author": ["Paul Christiano", "Greg Brockman"], "link": "https://openai.com/blog/concrete-ai-safety-problems/", "abstract": "We (along with researchers from Berkeley and Stanford) are co-authors on today's paper led by Google Brain researchers, Concrete Problems in AI Safety . The paper explores many research problems around ensuring that modern machine learning systems operate as intended. (The problems are very practical, and we've already seen some being integrated into OpenAI Gym .) Advancing AI requires making AI systems smarter, but it also requires preventing accidents — that is, ensuring that AI systems do what people actually want them to do. There's been an increasing focus on safety research from the machine learning community, such as a recent paper from DeepMind and FHI . Still, many machine learning researchers have wondered just how much safety research can be done today. The authors discuss five areas: Safe exploration. Can reinforcement learning (RL) agents learn about their environment without executing catastrophic actions? For example, can an RL agent learn to navigate an environment without ever falling off a ledge? Robustness to distributional shift. Can machine learning systems be robust to changes in the data distribution, or at least fail gracefully? For example, can we build image classifiers that indicate appropriate uncertainty when shown new kinds of images, instead of confidently trying to use its potentially inapplicable learned model? Avoiding negative side effects. Can we transform an RL agent's reward function to avoid undesired effects on the environment? For example, can we build a robot that will move an object while avoiding knocking anything over or breaking anything, without manually programming a separate penalty for each possible bad behavior? Avoiding “reward hacking” and “ wireheading ”. Can we prevent agents from “gaming” their reward functions, such as by distorting their observations? For example, can we train an RL agent to minimize the number of dirty surfaces in a building, without causing it to avoid looking for dirty surfaces or to create new dirty surfaces to clean up? Scalable oversight. Can RL agents efficiently achieve goals for which feedback is very expensive? For example, can we build an agent that tries to clean a room in the way the user would be happiest with, even though feedback from the user is very rare and we have to use cheap approximations (like the presence of visible dirt) during training? The divergence between cheap approximations and what we actually care about is an important source of accident risk. Many of the problems are not new, but the paper explores them in the context of cutting-edge systems. We hope they'll inspire more people to work on AI safety research, whether at OpenAI or elsewhere. We're particularly excited to have participated in this paper as a cross-institutional collaboration. We think that broad AI safety collaborations will enable everyone to build better machine learning systems. Let us know if you have a future paper you'd like to collaborate on!", "date": "2016-06-21"},
{"website": "Open-AI", "title": "special-projects", "author": ["Ilya Sutskever", "Dario Amodei", "Sam Altman"], "link": "https://openai.com/blog/special-projects/", "abstract": "We see these problems as having either very broad implications, or addressing important emerging consequences of AI development. If you are a strong machine learning expert and wish to start an effort on one of these problems at OpenAI, please submit an application .", "date": "2016-07-28"},
{"website": "Open-AI", "title": "team-update-august", "author": ["Greg Brockman", "Jie Tang"], "link": "https://openai.com/blog/team-update-august/", "abstract": "We've hired more great people to help us achieve our goals . Welcome, everyone! Dario Amodei. Dario was one of the lead authors of Deep Speech 2 , a speech system which achieved near-human performance on many speech tasks. He is also a main co-author of “ Concrete Problems in AI Safety ”, which highlights issues related to accidents in machine learning systems. Prior to OpenAI, he worked at Google Brain. Filip Wolski Filip's recent background is in \"practical\" modeling, having spent the last few years working in the high-frequency trading space. In the past he enjoyed problem-solving in programming competitions, and won the IOI and ACM ICPC . Jack Clark. Jack has spent the past few years writing about artificial intelligence and distributed systems, most recently at Bloomberg and BusinessWeek. His articles have covered technologies like memory networks , image generation , and reinforcement learning for robots , and issues like diversity within AI . As our Strategy and Communications Director, he will help with community outreach, policy, communications, and strategy. Scott Gray. Scott was previously an engineer at Nervana Systems where he focused on optimizing the performance of deep networks on GPUs. His assembly-level optimizations for dense linear algebra and convolution remain the fastest available. When not writing software he's usually spending his time reading up on the latest research in neuroscience and related fields. Zain Shah. Zain previously led deep learning efforts to build a collaborative human-machine intelligence system at Clara Labs . He's also worked on speech synthesis and computational neuroscience , built Mosaic , and founded a mobile behavioral analytics company . He most recently built a GIF search engine using deep multimodal embeddings. We're also pumped to be working with the following people for a more limited period of time: Catherine Olsson . Catherine built OpenAI Gym's REST API , which has already attracted users in Lua , C++ , Java , and Rust . She graduated with a perfect GPA in CS and Brain & Cognitive Science from MIT, and has extensive research experience in computational neuroscience and psychology . Catherine has taught programming and applied math for six years, including outreach to women and underrepresented minorities. Harri Edwards Harri is a PhD student at the University of Edinburgh, where he is researching models that can quickly adapt to new situations by learning to represent datasets . Igor Mordatch. Igor is interested in optimal control, machine learning, and their applications to robotics, biomechanics, and neuroscience. His PhD was in automated discovery and learning of complex movement behaviors . He will join the faculty at CMU in September 2017. Taco Cohen Taco is a PhD student working on applied and theoretical problems in representation learning. Most recently he invented group equivariant convolutional neural networks ( G-CNNs ), a generalization of CNNs that improves the statistical efficiency of these models by exploiting symmetries. Tambet Matiisen Tambet is a PhD student from University of Tartu, Estonia. He previously worked as a software engineer and founded his own startup . His recent projects range from making deep reinforcement learning agents cooperate to predicting a rat's location from its brain activity. He also wrote an accessible introduction to deep Q-learning .", "date": "2016-08-16"},
{"website": "Open-AI", "title": "openai-charter", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-charter/", "abstract": "We're releasing a charter that describes the principles we use to execute on OpenAI's mission. This document reflects the strategy we've refined over the past two years, including feedback from many people internal and external to OpenAI. The timeline to AGI remains uncertain, but our charter will guide us in acting in the best interests of humanity throughout its development. 我们发布了OpenAI纲领，并在其中描述了我们履行OpenAI使命的原则。这份宣言包含了许多我们实践的重要原则，它是OpenAI内外许多人在两年里不断完善人工智能发展战略的努力的结晶。达成通用人工智能的时间表仍具有很大的不确定性，但这份纲领将指导我们在整个开发过程中致力于为人类的最佳利益行事。 OpenAI's mission is to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity. We will attempt to directly build safe and beneficial AGI, but will also consider our mission fulfilled if our work aids others to achieve this outcome. To that end, we commit to the following principles: We commit to use any influence we obtain over AGI’s deployment to ensure it is used for the benefit of all, and to avoid enabling uses of AI or AGI that harm humanity or unduly concentrate power. Our primary fiduciary duty is to humanity. We anticipate needing to marshal substantial resources to fulfill our mission, but will always diligently act to minimize conflicts of interest among our employees and stakeholders that could compromise broad benefit. We are committed to doing the research required to make AGI safe, and to driving the broad adoption of such research across the AI community. We are concerned about late-stage AGI development becoming a competitive race without time for adequate safety precautions. Therefore, if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project. We will work out specifics in case-by-case agreements, but a typical triggering condition might be \"a better-than-even chance of success in the next two years.\" To be effective at addressing AGI's impact on society, OpenAI must be on the cutting edge of AI capabilities—policy and safety advocacy alone would be insufficient. We believe that AI will have broad societal impact before AGI, and we’ll strive to lead in those areas that are directly aligned with our mission and expertise. We will actively cooperate with other research and policy institutions; we seek to create a global community working together to address AGI’s global challenges. We are committed to providing public goods that help society navigate the path to AGI. Today this includes publishing most of our AI research, but we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research. OpenAI的使命是确保通用人工智能 (Artificial General Intelligence, AGI)，即一种高度自主且在大多数具有经济价值的工作上超越人类的系统，将为全人类带来福祉。我们不仅希望直接建造出安全的、符合共同利益的通用人工智能，而且愿意帮助其它研究机构共同建造出这样的通用人工智能以达成我们的使命。为了达到这个目标，我们制订了如下原则： 我们承诺在通用人工智能的开发过程中，将利用所有可获得的影响力，确保它可以造福全人类。我们将避免把人工智能或通用人工智能的技术置于损害人类或过度集中权力的事业中。 我们的首要任务是对人类文明负责。我们预计需要调用大量资源来完成这一使命。同时，我们会积极行动以减少雇员和利益相关者间的利益冲突，确保大多数人可以受益。 OpenAI致力于进行能够确保通用人工智能安全的研究。我们力求在整个人工智能研究领域内推动这类研究项目的广泛应用。 我们担心通用人工智能在发展后期将演变成一场激烈的竞赛，导致缺乏充足的时间进行安全防范。因此，如果一个与人类价值观相符、注重安全的项目领先于我们将近达成通用人工智能，我们承诺将停止竞赛，幷转而协助这个项目。我们会针对个别情况设计具体的合作方案。不过，一个典型的触发条件可能会是「这个项目在未来两年内能够成功研发通用人工智能的概率超过一半」。 为了能有效地促进通用人工智能对社会的正面影响，OpenAI必须站在人工智能技术研究的前沿。我们认为仅做政策和安全性的倡导是过于单薄的。 我们相信人工智能在达成通用人工智能之前便会产生广泛的社会影响。OpenAI希望在符合我们的使命和专长的领域中努力保持领先地位。 我们会和其它研究机构以及政策制定机构积极合作。我们希望可以建立一个国际化的社区，共同应对通用人工智能的全球性挑战。 我们致力于研发公共物品，以帮助社会走向与通用人工智能共处的时代。目前这包括公开发表大多数的人工智能研究成果。OpenAI预料未来对安全和安保的考虑将会使我们减少发表传统的研究成果，而更注重分享安全、政策和标准化相关的研究。", "date": "2018-04-09"},
{"website": "Open-AI", "title": "faulty-reward-functions", "author": ["Jack Clark", "Dario Amodei"], "link": "https://openai.com/blog/faulty-reward-functions/", "abstract": "At OpenAI, we've recently started using Universe , our software for measuring and training AI agents, to conduct new RL experiments. Sometimes these experiments illustrate some of the issues with RL as currently practiced. In the following example we'll highlight what happens when a misspecified reward function encourages an RL agent to subvert its environment by prioritizing the acquisition of reward signals above other measures of success. Designing safe AI systems will require us to design algorithms that don't attempt to do this, and will teach us to specify and shape goals in such a way they can't be misinterpreted by our AI agents. One of the games we've been training on is CoastRunners . The goal of the game - as understood by most humans - is to finish the boat race quickly and (preferably) ahead of other players. CoastRunners does not directly reward the player's progression around the course, instead the player earns higher scores by hitting targets laid out along the route. We assumed the score the player earned would reflect the informal goal of finishing the race, so we included the game in an internal benchmark designed to measure the performance of reinforcement learning systems on racing games. However, it turned out that the targets were laid out in such a way that the reinforcement learning agent could gain a high score without having to finish the course. This led to some unexpected behavior when we trained an RL agent to play the game. The RL agent finds an isolated lagoon where it can turn in a large circle and repeatedly knock over three targets, timing its movement so as to always knock over the targets just as they repopulate. Despite repeatedly catching on fire, crashing into other boats, and going the wrong way on the track, our agent manages to achieve a higher score using this strategy than is possible by completing the course in the normal way. Our agent achieves a score on average 20 percent higher than that achieved by human players. While harmless and amusing in the context of a video game, this kind of behavior points to a more general issue with reinforcement learning: it is often difficult or infeasible to capture exactly what we want an agent to do, and as a result we frequently end up using imperfect but easily measured proxies. Often this works well, but sometimes it leads to undesired or even dangerous actions. More broadly it contravenes the basic engineering principle that systems should be reliable and predictable. We've also explored this issue at greater length in our research paper Concrete Problems on AI Safety . How can we avoid such problems? Aside from being careful about designing reward functions, several research directions OpenAI is exploring may help to reduce cases of misspecified rewards: Learning from demonstrations allows us to avoid specifying a reward directly and instead just learn to imitate how a human would complete the task. In this example, since the vast majority of humans would seek to complete the racecourse, our RL algorithms would do the same. In addition to, or instead of human demonstrations, we can also incorporate human feedback by evaluating the quality of episodes or even sharing control with the agent in an interactive manner. It's possible that a very small amount of evaluative feedback might have prevented this agent from going around in circles. It may be possible to use transfer learning to train on many similar games, and infer a “common sense” reward function for this game. Such a reward function might prioritize finishing the race based on the fact that a typical game has such a goal, rather than focusing on the idiosyncrasies of this particular game's reward function. This seems more similar to how a human would play the game. These methods may have their own shortcomings. For example, transfer learning involves extrapolating a reward function for a new environment based on reward functions from many similar environments. This extrapolation could itself be faulty — for example, an agent trained on many racing video games where driving off the road has a small penalty, might incorrectly conclude that driving off the road in a new, higher stakes setting is not a big deal. More subtly, if the reward extrapolation process involves neural networks, adversarial examples in that network could lead a reward function that has “unnatural” regions of high reward that do not correspond to any reasonable real-world goal. Solving these issues will be complex. Our hope is that Universe will enable us to both discover and address new failure modes at a rapid pace, and eventually to develop systems whose behavior we can be truly confident in. Get in touch with the authors of this post: Dario , Jack", "date": "2016-12-21"},
{"website": "Open-AI", "title": "report-from-the-self-organizing-conference", "author": ["Jack Clark", "Ian Goodfellow"], "link": "https://openai.com/blog/report-from-the-self-organizing-conference/", "abstract": "Our first group learning experiment! Last week we hosted over a hundred and fifty AI practitioners in our offices for our first self-organizing conference on machine learning. The goal was to accelerate AI research by bringing a diverse group of people together and making it easy for them to educate each other and generate new ideas. To achieve this we sought to build an entire event around the chance hallway conversations, serendipitous lunches and inspiring encounters that people have at traditional conferences. The format worked. PHD students talked to professors, hobbyists talked to full-time researchers, and designers mingled with neuroscientists; most importantly, many people left the event with new research ideas. Participants identified issues ranging from the need for a greater theoretical underpinning within robotics, to how we might make use of neuroscience to accelerate AI development, to ways to increase the diversity of the AI community. Minutes from some of these meetings are available. The backbone of typical conferences consists of keynotes, plenaries, and panels, turning many of the attendees into passive spectators. At SOCML, the backbone of the event was about the interaction between participants, and they were able to form sessions, choose moderators, give impromptu lectures, and debug each other's problems, without much administration. Everyone taught everyone else and everyone learned from everyone else. The self-organizing nature makes this sort of conference reasonably easy and affordable to host, and brings talented minds together to work on urgent issues. If you're keen to host a self-organizing conference of your own we’ll be gathering feedback and adding some tips and tricks to the wiki in the coming weeks. Good luck!", "date": "2016-10-13"},
{"website": "Open-AI", "title": "openai-and-microsoft", "author": ["Greg Brockman", "Ilya Sutskever", "Sam Altman"], "link": "https://openai.com/blog/openai-and-microsoft/", "abstract": "One of the most important factors for accelerating our progress is accessing more and faster computers; this is particularly true for emerging AI technologies like reinforcement learning and generative models . Azure has impressed us by building hardware configurations optimized for deep learning — they offer K80 GPUs with InfiniBand interconnects at scale. We're also excited by their roadmap, which should soon bring Pascal GPUs onto their cloud. In the coming months we will use thousands to tens of thousands of these machines to increase both the number of experiments we run and the size of the models we train. We'll share the results of this partnership with everyone: along with publishing our research results, we'll continue releasing open-source software making it easier for people to run large-scale AI workloads on the cloud. We'll also be giving feedback to the Microsoft team so that Azure's capabilities keep pace with our understanding of AI. It's great to work with another organization that believes in the importance of democratizing access to AI . We're looking forward to accelerating the AI community through this partnership.", "date": "2016-11-15"},
{"website": "Open-AI", "title": "distill", "author": ["Greg Brockman", "Andrej Karpathy"], "link": "https://openai.com/blog/distill/", "abstract": "We're excited to support today's launch of Distill , a new kind of journal aimed at excellent communication of machine learning results (novel or existing). Distill is a website and set of associated tools that make it easier for people to explain machine learning concepts using modern web technologies. For example, people have already used the platform to explore the subtle settings of the t-SNE algorithm , to demystify the checkerboard artifacts in synthetic images , and peek under the hood of recurrent neural networks that generate handwriting . Andrej will serve on the steering committee for the publication, and Greg is helping fund the Distill Prize for Clarity in Machine Learning , which recognizes outstanding work on communicating ideas in machine learning and related topics (published in any venue!) Visit Distill", "date": "2017-03-20"},
{"website": "Open-AI", "title": "spam-detection-in-the-physical-world", "author": ["Rachel Fong", "Josh Tobin", "Jack Clark", "Alex Ray", "Jonas Schneider", "Pieter Abbeel", "Wojciech Zaremba"], "link": "https://openai.com/blog/spam-detection-in-the-physical-world/", "abstract": "We've created the world's first Spam-detecting AI trained entirely in simulation and deployed on a physical robot. Deep learning-driven robotic systems are bottlenecked by data collection: it's extremely costly to obtain the hundreds of thousands of images needed to train the perception system alone. It's cheap to generate simulated data, but simulations diverge enough from reality that people typically retrain models from scratch when moving to the physical world. We've shown that domain randomization, an existing idea for making detectors trained on simulated images transfer to real images, works well for cluttered scenes. The method is simple: we randomly vary colors, textures, lighting conditions, and camera settings in simulated scenes. The resulting dataset is sufficiently variable to allow a deep neural network trained on it to generalize to reality. The detector is a neural network based on the VGG16 architecture that predicts the precise 3-D location of Spam in simulated images. Though it has only been trained on simulated scenes, the resulting network is able to detect Spam in real images, even in the presence of never-before-seen \"distractor\" items arranged in random configurations. The video below demonstrates the system in action: In the future, we plan to extend this work to detect phishing and to defend against adversarial Spam. If you'd like to sink your teeth into compelling applied research problems like Spam detection, consider joining us at OpenAI.", "date": "2017-04-01"},
{"website": "Open-AI", "title": "scaling-kubernetes-to-2500-nodes", "author": ["Christopher Berner"], "link": "https://openai.com/blog/scaling-kubernetes-to-2500-nodes/", "abstract": "We've been running Kubernetes for deep learning research for over two years. While our largest-scale workloads manage bare cloud VMs directly, Kubernetes provides a fast iteration cycle, reasonable scalability, and a lack of boilerplate which makes it ideal for most of our experiments. We now operate several Kubernetes clusters (some in the cloud and some on physical hardware), the largest of which we've pushed to over 2,500 nodes. This cluster runs in Azure on a combination of D15v2 and NC24 VMs. On the path to this scale, many system components caused breakages, including etcd, the Kube masters, Docker image pulls, network, KubeDNS, and even our machines' ARP caches. We felt it’d be helpful to share the specific issues we ran into, and how we solved them. After passing 500 nodes in our cluster, our researchers started reporting regular timeouts from the kubectl command line tool. We tried adding more Kube masters (VMs running kube-apiserver ). This seemed to solve the problem temporarily, but once we passed 10 replicas we knew we were treating symptoms and not the cause (by comparison, GKE uses a single 32-core VM for 500 nodes). This made us strongly suspect our etcd cluster, which is the central store of state for the Kube masters. Looking in Datadog , we saw write latency spiking to hundreds of milliseconds on the DS15v2 machines running our etcd replicas, despite each machine using a P30 SSD capable of 5,000 IOPS. Benchmarking performance with fio , we saw etcd was only able to use about 10% of the available IOPS because the write latency was 2ms and etcd does sequential I/O, making it latency-bound. We then moved the etcd directory for each node to the local temp disk, which is an SSD connected directly to the instance rather than a network-attached one. Switching to the local disk brought write latency to 200us, and etcd became healthy! Our cluster ran well until we passed about 1,000 nodes, at which point we once again saw high commit latency from etcd. This time, we noticed the kube-apiservers were reading more than 500MB/s from etcd. We set up Prometheus to monitor the apiservers, and also set the --audit-log-path and --audit-log-maxbackup flags to enabled more logging on the apiserver. This surfaced a number of slow queries and excessive calls to the LIST API for Events. The root cause: the default setting for Fluentd 's and Datadog's monitoring processes was to query the apiservers from every node in the cluster (for example, this issue which is now fixed). We simply changed these processes to be less aggressive with their polling, and load on the apiservers became stable again: Another helpful tweak was storing Kubernetes Events in a separate etcd cluster, so that spikes in Event creation wouldn’t affect performance of the main etcd instances. To do this, we just set the --etcd-servers-overrides flag to something like this: --etcd-servers-overrides=/events#https://0.example.com:2381;https://1.example.com:2381;https://2.example.com:2381 Another post-1,000 nodes failure was to hit etcd's hard storage limit (by default 2GB), which causes it to stop accepting writes. This triggered a cascading failure: all our Kube nodes failed their health checks, and our autoscaler decided it thus needed to terminate all the workers. We've increased the max etcd size with the --quota-backend-bytes flag, and the autoscaler now has a sanity check not to take action if it would terminate more than 50% of the cluster. We colocate the kube-apiserver, kube-controller-manager , and kube-scheduler processes on the same machines. For high availability , we always have at least 2 masters, and set the --apiserver-count flag to the number of apiservers we’re running (otherwise Prometheus monitoring can get confused between instances). We use Kubernetes mainly as a batch scheduling system and rely on our autoscaler to dynamically scale up and down our cluster — this lets us significantly reduce costs for idle nodes, while still providing low latency while iterating rapidly. The default kube-scheduler policy is to spread out load evenly among nodes, but we want the opposite so that unused nodes can be terminated and also so that large pods can be scheduled quickly. So we switched to the following policy: We use KubeDNS extensively for service discovery, but soon after rolling out the new scheduling policy it started having reliability issues. We found that the failures were only happening on certain pods of KubeDNS. With the new scheduling policy some machines ended up running 10+ copies of KubeDNS, creating hotspots, and we had exceeded the ~200QPS that’s allowed from each Azure VM for external domains lookups. We fixed this by adding an anti-affinity rule to our KubeDNS pods: Our Dota project started out on Kubernetes, and as it scaled, we noticed that fresh Kubernetes nodes often have pods sitting in Pending for a long time. The game image is around 17GB, and would often take 30 minutes to pull on a fresh cluster node, so we understood why the Dota container would be Pending for a while — but this was true for other containers as well. Digging in, we found that kubelet has a --serialize-image-pulls flag which defaults to true , meaning the Dota image pull blocked all other images. Changing to false required switching Docker to overlay2 rather than AUFS. To further speed up pulls, we also moved the Docker root to the instance-attached SSD, like we did for the etcd machines. Even after optimizing the pull speed, we saw pods failing to start with a cryptic error message: rpc error: code = 2 desc = net/http: request canceled . The kubelet and Docker logs also contained messages indicating that the image pull had been canceled, due to a lack of progress. We tracked the root to large images taking too long to pull/extract, or times when we had a long backlog of images to pull. To address this, we set kubelet's --image-pull-progress-deadline flag to 30 minutes, and set the Docker daemon's max-concurrent-downloads option to 10. (The second option didn’t speed up extraction of large images, but allowed the queue of images to pull in parallel.) Our last Docker pull issue was due to the Google Container Registry. By default, kubelet pulls a special image from gcr.io (controlled by the --pod-infra-container-image flag) which is used when starting any new container. If that pull fails for any reason, like exceeding your quota , that node won’t be able to launch any containers. Because our nodes go through a NAT to reach gcr.io rather than having their own public IP, it's quite likely that we’ll hit this per-IP quota limit. To fix this, we simply preloaded that Docker image in the machine image for our Kubernetes workers by using docker image save -o /opt/preloaded_docker_images.tar and docker image load -i /opt/preloaded_docker_images.tar . To improve performance, we do the same for a whitelist of common OpenAI-internal images like the Dota image. As our experiments grow larger, they also become increasingly complex distributed systems which rely heavily on the network for their operation. When we first started running distributed experiments, it became immediately obvious that our networking wasn’t configured well. Directly between machines we got 10-15Gbit/s of throughput, but our Kube pods using Flannel were maxing out at ~2Gbit/s. Machine Zone's public benchmarks show similar numbers, meaning the issue wasn't likely to just be bad config, but instead something inherent to our environment. (By contrast, Flannel does not add this overhead on our physical machines.) To work around this, users can add two different settings to disable Flannel for their pod: hostNetwork: true and dnsPolicy: ClusterFirstWithHostNet . (Though read the warnings in the Kubernetes documentation before doing this.) Despite our DNS tuning, we still saw intermittent issues with DNS resolution. One day an engineer reported that nc -v to their Redis server was taking over 30 seconds to print that the connection was established. We tracked the issue to the kernel’s ARP stack. Initial investigation of the Redis pod's host showed something seriously wrong with the network: communication on any port was hanging for multiple seconds, and no DNS names could be resolved via the local dnsmasq daemon, with dig just printing a cryptic failure message: socket.c:1915: internal_send: 127.0.0.1#53: Invalid argument . The dmesg log was more informative: neighbor table overflow! which meant that the ARP cache had run out of space. ARP is used for mapping a network address such as an IPv4 address, to a physical address, such as a MAC address. Fortunately, this was easy to fix by setting a few options in /etc/sysctl.conf : It's common to tune this setting in HPC clusters, and is particularly relevant in Kubernetes clusters since every pod has its own IP address which consumes space in the ARP cache. Our Kubernetes clusters have been incident-free for about 3 months now, and we're planning to scale to even larger clusters in 2018. We recently upgraded to version 1.8.4, and are excited to see that it now officially supports 5,000. If you’re interested in building large scale compute clusters, we’re hiring !", "date": "2018-01-18"},
{"website": "Open-AI", "title": "amplifying-ai-training", "author": ["Paul Christiano", "Dario Amodei"], "link": "https://openai.com/blog/amplifying-ai-training/", "abstract": "We’re proposing an AI safety technique called iterated amplification that lets us specify complicated behaviors and goals that are beyond human scale, by demonstrating how to decompose a task into simpler sub-tasks, rather than by providing labeled data or a reward function. Although this idea is in its very early stages and we have only completed experiments on simple toy algorithmic domains, we’ve decided to present it in its preliminary state because we think it could prove to be a scalable approach to AI safety. If we want to train an ML system to perform a task, we need a training signal — a way to evaluate how well it is doing in order to help it learn. For example, labels in supervised learning or rewards in reinforcement learning are training signals.  The formalism of ML usually assumes a training signal is already present and focuses on learning from it, but in reality the training signal has to come from somewhere.  If we don’t have a training signal we can’t learn the task, and if we have the wrong training signal, we can get unintended and sometimes dangerous behavior .  Thus, it would be valuable for both learning new tasks, and for AI safety, to improve our ability to generate training signals. How do we currently generate training signals? Sometimes, the goal we want can be evaluated algorithmically, like counting up the score in a game of Go or checking whether a set of numbers has been successfully sorted (left panels of figure below).  Most real-world tasks don’t lend themselves to an algorithmic training signal, but often we can instead obtain a training signal by having a human either perform the task (for example, labeling a training set or demonstrating an RL task), or judge an AI’s performance on the task (middle panels of figure below).   However, many tasks are so complicated that a human can’t judge or perform them - examples might be designing a complicated transit system or managing every detail of the security of a large network of computers (right panels of figure below). Iterated amplification is a method for generating a training signal for the latter types of tasks, under certain assumptions.  Namely, although a human can’t perform or judge the whole task directly, we assume that a human can, given a piece of the task, identify clear smaller components of which it’s made up.  For example, in the networked computer example, a human could break down “defend a collection of servers and routers” into “consider attacks on the servers”, “consider attacks on the routers”, and “consider how the previous two attacks might interact”.  Additionally, we assume a human can do very small instances of the task, for example “identify if a specific line in a log file is suspicious”.  If these two things hold true, then we can build up a training signal for big tasks from human training signals for small tasks, using the human to coordinate their assembly. In our implementation of amplification, we start by sampling small subtasks and training the AI system to do them by soliciting demonstrations from humans (who can do these small tasks).  We then begin sampling slightly larger tasks, solving them by asking humans to break them up into small pieces, which AI systems trained from the previous step can now solve.  We use the solutions to these slightly harder tasks, which were obtained with human help, as a training signal to train AI systems to solve these second-level tasks directly (without human help).  We then continue to further composite tasks, iteratively building up a training signal as we go.  If the process works, the end result is a totally automated system that can solve highly composite tasks despite starting with no direct training signal for those tasks.  This process is somewhat similar to expert iteration (the method used in AlphaGo Zero ), except that expert iteration reinforces an existing training signal, while iterated amplification builds up a training signal from scratch. It also has features in common with several recent learning algorithms that use problem decomposition on-the-fly to solve a problem at test time, but differs in that it operates in settings where there is no prior training signal. As with our previous work on AI safety via debate , working directly on tasks beyond human scale is too hard for a prototype project.  Also, using an actual human as training signal introduces complications, so we haven’t done this yet (though we plan to do it in the future).  For our first experiments, we instead try to amplify an algorithmic training signal, to show that iterated amplification can work in this simple setting.  We also limit our attention to supervised learning (unlike our previous work on human training signals in RL ).  We’ve tried the method on five toy algorithmic tasks.  These tasks have direct algorithmic solutions which we pretend we don’t know (for example, find the shortest path between two points on a graph).  The problems can also be solved by piecing together small one-step deductions (for example, combining two paths to form a longer path), but it would take exponentially much effort to piece everything together manually.  We use iterated amplification to learn the direct algorithm using only the pieces as training signal, thus simulating the situation where a human knows how to combine subpieces of a solution but can’t provide a direct training signal. On each of these five tasks (permutation powering, sequential assignments, wildcard search, shortest path, and union find), we are able to perform competitively with just directly learning the task via supervised learning, despite being handicapped by not having a direct training signal (the goal here is to match supervised learning with less information, not to surpass it). Amplification has features in common with our previous work on AI safety via debate . Like debate, it seeks to train tasks that are beyond human ability to directly perform or judge, through an iterative process that allows humans to provide indirect supervision - however the specific approach is different. It also builds on our work on human feedback , by implementing a reward prediction system, and later versions of it are likely to include feedback from actual humans. So far we’ve explored all of these methods in a preliminary way, and are now working on scaling them up to address more interesting and realistic problems. If you’re interested in helping us extend tools like iterated amplification to build safe, powerful AI, then consider joining OpenAI .", "date": "2018-10-22"},
{"website": "Open-AI", "title": "hackathon", "author": ["Parnian Barekatain", "Greg Brockman"], "link": "https://openai.com/blog/hackathon/", "abstract": "Come to OpenAI’s office in San Francisco's Mission District for talks and a hackathon on Saturday, March 3rd . (RSVPs are now closed. We have limited space and will curate the invite list — we'll send email confirmations within the next few days.) Schedule of the day: 8:30a: Doors open, coffee and pastries served. 9-11a: Talks by Sam Altman , Dario Amodei , Josh Achiam , and Alec Radford . 11a-7p: Hackathon! Lunch will be provided. Come with a project you’d like to hack on, figure one out with a group of others you meet here, or just hang out and eat the food. If you’re looking for inspiration, maybe try out one of our Requests for Research — but all projects, not just machine learning ones, are welcome! Please see our Code of Conduct and hackathon terms . There are no judges, prizes, or contests — just the space and time for you to work on a project! Feel free to come for any subset. The goal of the day is to learn more about AI, and to work on a cool project while surrounded by others doing the same.", "date": "2018-02-22"},
{"website": "Open-AI", "title": "machine-learning-unconference", "author": ["Ian Goodfellow"], "link": "https://openai.com/blog/machine-learning-unconference/", "abstract": "The latest information about the Unconference is now available at the Unconference wiki , which will be periodically updated with more information for attendees. Machine learning is moving incredibly quickly. To keep up, many practitioners spend several weeks a year at conferences. However, conference presentations are all on work submitted months prior, meaning that people are already intimately familiar with the content (and it's often already been surpassed). We'd like to try instead hosting an event focused on the most valuable part of any conference: the people. Please join us for our first Machine Learning Unconference, an experimental gathering driven by its participants rather than an organizing committee. The unconference will be a free event at the OpenAI office in San Francisco on Friday and Saturday, October 7-8, 2016 . We welcome participants from around the globe. As of August 22, we have finished accepting applications for the unconference. Conferences play two main roles: social gathering and publication. We feel that these two roles are somewhat orthogonal and can be better served separately. At most conferences, the unstructured social time, such as mealtimes, is what people find valuable. We want to maximize the amount of interactivity in a social gathering of researchers. A minimally-structured unconference provides an opportunity for an experiment, to see what kinds of new formats attendees will develop, and find out which of these are most effective. We also have ideas for how to improve the paper reviewing and publication system, which we are developing separately. It would go against the spirit of the unconference for us to provide a specific schedule, but we suggest attendees use the Gitter chat to plan a few topics of discussion ahead of time. All attendees are also encouraged to bring a poster describing machine learning work that would be interesting or useful for other attendees to learn about. (We'll follow up with more details on format via email.) Anyone who is involved in machine learning research or applications is welcome. We especially expect to see: PhD students, faculty members, and industry research scientists. Software engineers using machine learning in applications or building machine learning infrastructure. We're particularly excited to support women, minorities, and members of other groups underrepresented in machine learning. OpenAI will provide a limited number of travel and lodging grants for members of these groups. This event is primarily intended for people with technical fluency in machine learning to help each other advance the state of the art. If you are interested in learning about the basics of the field, there are plenty of other great events we'd suggest. OpenAI is sponsoring and hosting the event, but the event itself has no official organization — it's just people who find this invitation interesting, meeting to discuss machine learning. OpenAI will provide lunch and snacks. Participants are responsible for arranging their own travel and lodging. By attending, you agree to abide by our code of conduct . We'd like to welcome everyone, but we only have capacity for 150 people. We'll tune the acceptance list to ensure an interesting conference with a diversity of people and perspectives. Here's a somewhat representative sample of people you'll get to meet at the unconference, based on early signups: Conrado Miranda , PhD student, University of Campinas Fei Sha , Associate Professor, UCLA Georgia Gkioxari , PhD student at Berkeley Leila Wehbe , Postdoctoral Researcher at Berkeley Many members of Google Brain , including Samy Bengio , Greg Corrado , Douglas Eck , and Dumitru Erhan . Many members of the OpenAI team . Mehdi Mirza , PhD student, Montreal Institute for Learning Algorithms Nicolas Papernot , Google PhD Fellow in Security, Pennsylvania State University Serena Yeung , PhD student, Stanford University Sudnya Diamos , Data Science and Algorithms Engineer, Attune, Inc Úlfar Erlingsson , lead of security efforts at Google Research Wendy Shang , Software Engineer, Oculus You can discuss plans and logistics with other attendees in the Gitter chat , or email Ian with any questions.", "date": "2016-08-18"},
{"website": "Open-AI", "title": "learning-montezumas-revenge-from-a-single-demonstration", "author": ["Tim Salimans", "Richard Chen"], "link": "https://openai.com/blog/learning-montezumas-revenge-from-a-single-demonstration/", "abstract": "We've trained an agent to achieve a high score of 74,500 on Montezuma's Revenge from a single human demonstration, better than any previously published result. Our algorithm is simple: the agent plays a sequence of games starting from carefully chosen states from the demonstration, and learns from them by optimizing the game score using PPO , the same reinforcement learning algorithm that underpins OpenAI Five . In order to succeed at a reinforcement learning problem, an AI needs to do two things: Find a sequence of actions that leads to positive reward. This is the exploration problem. Remember the sequence of actions to take, and generalize to related but slightly different situations. This is the learning problem. The exploration problem can largely be bypassed in Montezuma's Revenge by starting each RL episode by resetting from a state in a demonstration. By starting from demonstration states, the agent needs to perform much less exploration to learn to play the game compared to when it starts from the beginning of the game at every episode. Doing so enables us to disentangle exploration and learning. Our results suggest that exploration is the hardest of the two problems for games like Montezuma’s Revenge and certain similar Atari games like PrivateEye. Model-free RL methods like policy gradients and Q-learning explore by taking actions randomly. If, by chance, the random actions lead to a reward, they are reinforced , and the agent becomes more likely to take these beneficial actions in the future. This works well if rewards are dense enough for random actions to lead to a reward with reasonable probability. However, many of the more complicated games require long sequences of very specific actions to experience any reward, and such sequences are extremely unlikely to occur randomly. Consider a game which takes a precise sequence of N actions to experience the first reward. If each of those actions is taken with a fixed probability, a random agent will need to play the game for a duration that scales as exp(N) before it can expect to experience the first reward. In the case of Montezuma’s Revenge, for example, the probability of getting the first key can be decomposed as p(get key) = p(get down ladder 1) * p(get down rope) * p(get down ladder 2) * p(jump over skull) * p(get up ladder 3). By multiplying N of these probabilities together, we end up with the resulting probability p(get key) that is exponentially smaller than any of the individual input probabilities. Algorithms with exponential scaling break down very quickly as your problem becomes more challenging, which limits the tasks current reinforcement learning techniques can solve. Although model-free RL methods have difficulty finding long sequences of actions, they work well for shorter sequences. Our main insight is that we can make our task easier to solve by decomposing it into a curriculum of subtasks requiring short action sequences; we construct this curriculum by starting each RL episode from a demonstration state. A variant of the same idea was used recently for reverse curriculum generation for robotics , where a curriculum was constructed by iteratively perturbing a set of starting states using random actions, and selecting the resulting states with the right level of difficulty. Our approach works by letting each RL episode start from a state in a previously recorded demonstration. Early on in training, the agent begins every episode near the end of the demonstration. Once the agent is able to beat or at least tie the score of the demonstrator on the remaining part of the game in at least 20% of the rollouts, we slowly move the starting point back in time. We keep doing this until the agent is playing from the start of the game, without using the demo at all, at which point we have an RL-trained agent beating or tying the human expert on the entire game. By slowly moving our starting state from the end of the demonstration to the beginning, we ensure that at every point the agent faces an easy exploration problem where it is likely to succeed, since it has already learned to solve most of the remaining game. We can interpret solving the RL problem in this way as a form of dynamic programming . If a specific sequence of N actions is required to reach a reward, this sequence may now be learned in a time that is linear in N, rather than exponential. Starting episodes by resetting from demonstration states was previously proposed , but without constructing a curriculum that gradually moves the starting state back from the end of the demonstration to the beginning. When combined with imitation learning, several researchers report benefit from this approach. For our use case we found such a curriculum to be vitally important for deriving benefit from the demonstration. Recently, DeepMind has shown an agent learning Montezuma's Revenge by imitation learning from a demonstration; one approach trains an agent to achieve the same states seen in a YouTube video of Montezuma's Revenge, and another technique combines a sophisticated version of Q-learning with maximizing the likelihood of actions taken in a demonstration. The advantage of these approaches is that they do not require as much control over the environment our technique does: they do not reset the environment to states other than the starting state of the game, and they do not presume access to the full game states encountered in the demonstration. Our method differs by directly optimizing what we care about — the game score, rather than making the agent imitate the demonstration; our method will thus not overfit to a potentially sub-optimal demonstration and could offer benefits in multi-player games where we want to optimize performance against other opponents than just the one from the demonstration. Although the step-by-step learning done by our agent is much simpler than learning to play from scratch, it is still far from trivial. One challenge our RL agent faces is that it is generally unable to reach the exact state from later on in the demo when it starts from an earlier state. This is because the agent plays the game at a different frameskip from what we used for recording the demonstration, but it is also due to the randomness in the actions which make it very unlikely to exactly reproduce any specific sequence of actions. The agent will thus need to be able to generalize between states that are very similar, but not identical. We found that this works well for Montezuma’s Revenge, but much less well for some other Atari games we tried, like Gravitar and Pitfall. One reason for this may be that these latter games require solving a harder vision problem: we found these games difficult to play from a downsampled screen ourselves, and we saw some improvement when using larger and deeper neural network policies. Another challenge we encountered is that standard RL algorithms like policy gradients require striking a careful balance between exploration and exploitation: if the agent's actions are too random, it makes too many mistakes to ever achieve the required final score when starting from the beginning of the game; if the actions are too deterministic, the agent stops learning because it does not explore alternative actions. Achieving the reported result on Montezuma's Revenge thus required careful tuning of the coefficient of the entropy bonus used in PPO, in combination with other hyperparameters such as the learning rate and the scaling of rewards. For some other games like Gravitar and Pitfall we were unable to find hyperparameters that worked for training the full curriculum. The algorithm also still shows substantial random variation from run to run, with some runs failing to converge for Montezuma's Revenge. We hope that future advances in RL will yield algorithms that are more robust to random noise and to the choice of hyperparameters. Finally, like is often the case in reinforcement learning, we find that our trained neural net policy does not yet generalize at the level of a human player. One method to test for generalization ability is to perturb the policy by making actions sticky and repeating the last action with probability of 0.25 at every frame. Using this evaluation method our trained policy obtains a score of 10,000 on Montezuma's Revenge on average. Alternatively, we can take random actions with probability 0.01 (repeated for 4 frameskipped steps), which leads to an average score of 8,400 for our policy. Anecdotally, we find that such perturbations also significantly reduce the score of human players on Montezuma's Revenge, but to a lesser extent. As far as we are aware, our results using perturbed policies are still better than all those published previously. Perturbing the learned policy by starting with between 0 and 30 random no-ops did not significantly hurt results, with the majority of rollouts achieving the final score obtained in our demonstration. Where most previous work on learning from demonstrations focused on imitation , which encourages identical behavior to that seen in the demonstration, we have shown that good results can be achieved by optimizing returns directly. This allows the agent to deviate from the demonstrated behavior, which lets it find new and exciting solutions that the human demonstrator may not have considered. By training on a curriculum of subtasks, created by resetting from demonstration states, we used this technique to solve a difficult reinforcement learning problem requiring long sequences of actions.", "date": "2018-07-04"},
{"website": "Open-AI", "title": "requests-for-research-2", "author": ["Ilya Sutskever", "John Schulman", "Tim Salimans", "Durk Kingma"], "link": "https://openai.com/blog/requests-for-research-2/", "abstract": "We're releasing a new batch of seven unsolved problems which have come up in the course of our research at OpenAI. Like our original Requests for Research (which resulted in several papers ), we expect these problems to be a fun and meaningful way for new people to enter the field, as well as for practitioners to hone their skills (it's also a great way to get a job at OpenAI). Many will require inventing new ideas. Please email us with questions or solutions you'd like us to publicize! (Also, if you don't have deep learning background but want to learn to solve problems like these, please apply for our Fellowship program!) If you're not sure where to begin, here are some solved starter problems. ⭐ Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequence's end. Test the two approaches below: Generate a dataset of random 100,000 binary strings of length 50. Train the LSTM; what performance do you get? Generate a dataset of random 100,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM.  Does it succeed? What explains the difference? ⭐ Implement a clone of the classic Snake game as a Gym environment, and solve it with a reinforcement learning algorithm of your choice. Tweet us videos of the agent playing. Were you able to train a policy that wins the game? ⭐⭐ Slitherin'. Implement and solve a multiplayer clone of the classic Snake game (see slither.io for inspiration) as a Gym environment. Environment: have a reasonably large field with multiple snakes; snakes grow when eating randomly-appearing fruit; a snake dies when colliding with another snake, itself, or the wall; and the game ends when all snakes die. Start with two snakes, and scale from there. Agent: solve the environment using self-play with an RL algorithm of your choice . You'll need to experiment with various approaches to overcome self-play instability (which resembles the instability people see with GANs). For example, try training your current policy against a distribution of past policies. Which approach works best? Inspect the learned behavior: does the agent learn to competently pursue food and avoid other snakes? Does the agent learn to attack, trap, or gang up against the competing snakes? Tweet us videos of the learned policies! ⭐⭐⭐ Parameter Averaging in Distributed RL. Explore the effect of parameter averaging schemes on sample complexity and amount of communication in RL algorithms. While the simplest solution is to average the gradients from every worker on every update, you can save on communication bandwidth by independently updating workers and then infrequently averaging parameters. In RL, this may have another benefit: at any given time we'll have agents with different parameters, which could lead to better exploration behavior. Another possibility is use algorithms like EASGD that bring parameters partly together each update. ⭐⭐⭐ Transfer Learning Between Different Games via Generative Models. Proceed as follows: Train 11 good policies for 11 Atari games. Generate 10,000 trajectories of 1,000 steps each from the policy for each game. Fit a generative model (such as the Transformer ) to the trajectories produced by 10 of the games. Then fine-tune that model on the 11th game. Your goal is to quantify the benefit from pre-training on the 10 games.  How large does the model need to be for the pre-training to be useful? How does the size of the effect change when the amount of data from the 11th game is reduced by 10x? By 100x? ⭐⭐⭐ Transformers with Linear Attention. The Transformer model uses soft attention with softmax. If we could instead use linear attention (which can be converted into an RNN that uses fast weights ), we could use the resulting model for RL. Specifically, an RL rollout with a transformer over a huge context would be impractical, but running an RNN with fast weights would be very feasible. Your goal: take any language modeling task; train a transformer; then find a way to get the same bits per character/word using a linear-attention transformer with different hyperparameters, without increasing the total number of parameters by much. Only one caveat: this may turn out to be impossible. But one potentially helpful hint: it is likely that transformers with linear attention require much higher dimensional key/value vectors compared to attention that uses the softmax, which can be done without significantly increasing the number of parameters. ⭐⭐⭐ Learned Data Augmentation. You could use a learned VAE of data, to perform “learned data augmentation”. One would first train a VAE on input data, then each training point would be transformed by encoding to a latent space, then applying a simple (e.g. Gaussian) perturbation in latent space, then decoding back to observed space. Could we use such an approach to obtain improved generalization? A potential benefit of such data augmentation is that it could include many nonlinear transformations like viewpoint changes and changes in scene lightning. Can we approximate the set of transformations to which the label is invariant? Check out the existing work on this topic if you want a place to get started. ⭐⭐⭐⭐ Regularization in Reinforcement Learning. Experimentally investigate (and qualitatively explain) the effect of different regularization methods on an RL algorithm of choice. In supervised deep learning, regularization is extremely important for improving optimization and for preventing overfitting, with very successful methods like dropout , batch normalization , and L2 regularization . However, people haven't benefited from regularization with reinforcement learning algorithms such as policy gradients and Q-learning . Incidentally, people generally use much smaller models in RL than in supervised learning, as large models perform worse — perhaps because they overfit to recent experience. To get started, here is a relevant but older theoretical study. ⭐⭐⭐⭐⭐ Automated Solutions of Olympiad Inequality Problems. Olympiad inequality problems are simple to express, but solving them often requires clever manipulations. Build a dataset of olympiad inequality problems and write a program that can solve a large fraction of them. It's not clear whether machine learning will be useful here, but you could potentially use a learned policy to reduce the branching factor. Want to work on problems like these professionally? Apply to OpenAI!", "date": "2018-01-31"},
{"website": "Open-AI", "title": "language-unsupervised", "author": ["Alec Radford"], "link": "https://openai.com/blog/language-unsupervised/", "abstract": "We've obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we're also releasing. Our approach is a combination of two existing ideas: transformers and unsupervised pre-training . These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse datasets. Our system works in two stages; first we train a transformer model on a very large amount of data in an unsupervised manner — using language modeling as a training signal — then we fine-tune this model on much smaller supervised datasets to help it solve specific tasks. We developed this approach following our sentiment neuron work, in which we noted that unsupervised learning techniques can yield surprisingly discriminative features when trained on enough data. Here, we wanted to further explore this idea: can we develop one model, train it in an unsupervised way on a large amount of data, and then fine-tune the model to achieve good performance on many different tasks? Our results indicate that this approach works surprisingly well; the same core model can be fine-tuned for very different tasks with minimal adaptation. This work builds on the approach introduced in Semi-supervised Sequence Learning , which showed how to improve document classification performance by using unsupervised pre-training of an LSTM followed by supervised fine-tuning. It also extends ULMFiT , research that shows how a single dataset-agnostic LSTM language model can be fine-tuned to get state-of-the-art performance on a variety of document classification datasets; our work shows how a Transformer-based model can be used in this approach to succeed at a broader range of tasks beyond document classification, such as commonsense reasoning, semantic similarity, and reading comprehension. It is also similar to but more task-agnostic than ELMo , which incorporates pre-training but uses task-customized architectures to get state-of-the-art results on a broad suite of tasks. Very little tuning was used to achieve our results. All datasets use a single forward language model, without any ensembling, and the majority of the reported results use the exact same hyperparameter settings. A result we are particularly excited about is the performance of our approach on three datasets — COPA , RACE , and ROCStories — designed to test commonsense reasoning and reading comprehension. Our model obtains new state-of-the-art results on these datasets by a wide margin. These datasets are thought to require multi-sentence reasoning and significant world knowledge to solve suggesting that our model improves these skills predominantly via unsupervised learning. This suggests there's hope for developing complex language understanding capabilities via unsupervised techniques. Supervised learning is at the core of most of the recent success of machine learning. However, it can require large, carefully cleaned, and expensive to create datasets to work well. Unsupervised learning is attractive because of its potential to address these drawbacks. Since unsupervised learning removes the bottleneck of explicit human labeling it also scales well with current trends of increasing compute and availability of raw data. Unsupervised learning is a very active area of research but practical uses of it are often still limited. There's been a recent push to try to further language capabilities by using  unsupervised learning to augment systems with large amounts of unlabeled data; representations of words trained via unsupervised techniques can use large datasets consisting of terabytes of information and, when integrated with supervised learning, improve performance on a wide range of NLP tasks. Until recently, these unsupervised techniques for NLP (for example, GLoVe and word2vec ) used simple models (word vectors) and training signals (the local co-occurence of words). Skip-Thought Vectors is a notable early demonstration of the potential improvements more complex approaches can realize. But new techniques are now being used which are further boosting performance. These include the use of pre-trained sentence representation models, contextualized word vectors (notably ELMo and CoVE ), and approaches which use customized architectures to fuse unsupervised pre-training with supervised fine-tuning, like our own. We also noticed we can use the underlying language model to begin to perform tasks without ever training on them. For example, performance on tasks like picking the right answer to a multiple choice question steadily increases as the underlying language model improves. While the absolute performance of these methods is still often quite low compared to the supervised state-of-the-art (for question answering it still outperformed by a simple sliding-window baseline) it is encouraging that this behavior is robust across a broad set of tasks. Randomly initialized networks containing no information about the task and the world perform no-better than random using these heuristics. This provides some insight into why generative pre-training can improve performance on downstream tasks. We can also use the existing language functionality in the model to perform sentiment analysis. For the Stanford Sentiment Treebank dataset, which consists of sentences from positive and negative movie reviews, we can use the language model to guess whether a review is positive or negative by inputting the word “very” after the sentence and seeing whether the model predicts the word “positive” or “negative” as more likely. This approach, without adapting the model at all to the task, performs on par with classic baselines ~80% accuracy . Our work is also a validation of the robustness and usefulness of the transformer architecture, indicating that it is sufficiently flexible to achieve state-of-the-art results on a wide range of tasks without requiring complicated task-specific customization or hyperparameter tuning. This project has a few outstanding issues which are worth noting: Compute Requirements : Many previous approaches to NLP tasks train relatively small models on a single GPU from scratch. Our approach requires an expensive pre-training step - 1 month on 8 GPUs. Luckily, this only has to be done once and we're releasing our model so others can avoid it. It is also a large model (in comparison to prior work) and consequently uses more compute and memory — we used a 37-layer (12 block) Transformer architecture, and we train on sequences of up to 512 tokens. Most experiments were conducted on 4 and 8 GPU systems. The model does fine-tune to new tasks very quickly which helps mitigate the additional resource requirements. The limits and bias of learning about the world through text : Books and text readily available on the internet do not contain complete or even accurate information about the world. Recent work has shown that certain kinds of information are difficult to learn via just text and other work has shown that models learn and exploit biases in data distributions. Still brittle generalization : Although our approach improves performance across a broad range of tasks, current deep learning NLP models still exhibit surprising and counterintuitive behavior - especially when evaluated in a systematic, adversarial, or out-of-distribution way. Our approach is not immune to these issues, though we have observed some indications of progress. Our approach shows improved lexical robustness over previous purely neural approaches to textual entailment. On the dataset introduced in Glockner et al. (2018) our model achieves 83.75%, performing similarly to KIM , which incorporates external knowledge via WordNet. Scaling the approach : We've observed that improvements in the performance of the language model are well correlated with improvements on downstream tasks. We're currently using commodity hardware (a single 8 GPU machine) and a training dataset of only a few thousand books (~5GB of text). This suggests there is significant room for improvement using the well-validated approach of more compute and data. Improved fine-tuning : Our approach is currently very simple. It is likely that substantial improvements can be made using more intricate adaptation and transfer techniques such as those explored in ULMFiT . Better understanding of why generative pre-training helps : Although we've discussed some ideas we are partial to here, more targeted experiments and research will help distinguish between competing explanations. For instance, how much of the benefits we observe are due to improved ability to process broader context versus improved world knowledge? We're increasingly interested in understanding the relationship between the compute we expend on training models and the resulting output . The total compute used to train this model was 0.96 petaflop days (pfs-days).", "date": "2018-06-11"},
{"website": "Open-AI", "title": "hackathon-follow-up", "author": ["Joshua Achiam", "Parnian Barekatain"], "link": "https://openai.com/blog/hackathon-follow-up/", "abstract": "On March 3rd, we hosted our first hackathon with 100 members of the artificial intelligence community. We had over 500 RSVPs arrive within two days of announcing the event — if you didn't make it this time, please RSVP again in the future! Thank you to Cirrascale for providing GPU machines during the hackathon. Our applicants included high schoolers, industry practitioners, engineers for nonprofits (not just at OpenAI!), researchers at universities, and more, with interests spanning healthcare to AGI. We could only accommodate one hundred people this time so we tried to pick a balanced crowd with a wide range of backgrounds and levels of experience. In particular, we strove to achieve gender balance; many attendees told us that this kind of representation made a positive difference for their experience of the hackathon. We kicked the day off with a series of talks on OpenAI’s mission and the technical topics that we focus on in our research. Sam Altman took questions on AGI timelines, safety issues, and the importance of avoiding AI arms races. Sam described how he personally came to focus on AI safety: he saw it as an underfunded, under-explored area with the potential to impact everyone. Josh Achiam gave an introduction to reinforcement learning, which is one of our main research areas; we've open-sourced the slides and sample code from his talk. Ilya Sutskever talked about self-play with RL agents; for an overview of the work covered, see our recent blog posts and code releases . Alec Radford provided a tutorial and survey of the many different kinds of GANs and we've made the tutorial code available. After the talks wrapped up, the hacking began. Over the course of an 8-hour code sprint participants authored dozens of AI projects on topics ranging from safety to healthcare. Some of our favorites: Jiale Xian, Clarence Leung, Kyle Zheng, Madeline Hawkins, and Stergios Hetelekides worked on an image classifier to identify purine-rich seafoods that gout patients should avoid. Arthur Juliani implemented PPO with curiosity-based intrinsic rewards and trained an agent to smash block towers: High schoolers Ethan Knight and Osher Lerner worked on Natural Q-Learning based on one of our requests for research . Malhar Patel and Lee Redden worked on AeroEnv , a Gym interface for a physical robot. Rodger Luo implemented Q-learning in Processing , a programming tool for media artists. Andy Matuschak developed the scrying pen , a fun and unique tool for machine-assisted sketching based on SketchRNNs . James Giammona and Brad Neuberg generated ideas for how RL agents might avoid dangerous parts of the environment that they haven’t seen before. We got a lot of helpful feedback from hackathon attendees, which we'll use to make even more interesting events in the future: stay tuned! If you'd like to work on AI as your day job, you may be interested in our Scholars or Fellows programs (you're of course always welcome to apply full-time ).", "date": "2018-03-15"},
{"website": "Open-AI", "title": "team-update-january", "author": ["Jie Tang", "Jack Clark", "Greg Brockman"], "link": "https://openai.com/blog/team-update-january/", "abstract": "The OpenAI team is now 45 people. Together, we're pushing the frontier of AI capabilities — whether by validating novel ideas, creating new software systems, or deploying machine learning on robots. We continue to look for creative, motivated researchers and engineers to help us achieve our goals . Welcome to everyone who's joined since our last team update ! Aleks Kamko . Aleks is a recent graduate from Berkeley where he did research on distributed machine learning and taught Data Structures and Operating Systems for 2 years. Previously, Aleks was an intern at Stripe working on the Financial Operations team. Alex Ray . Alex previously built radio encryption systems for small satellites at Planet Labs . Prior to that he worked on aerial robots at Airware . He has a degree in Textile Engineering from NCSU, where he worked on a design for an inflatable habitat for living on Mars. Ankur Handa . Ankur obtained his PhD in real-time camera tracking at Imperial College London, then became a post-doc at the University of Cambridge to focus on scene understanding via his work on SceneNet . He returned to Imperial as a Dyson Fellow to continue his research on scene understanding with SceneNet RGB-D . His interests lie at the intersection of hardware, SLAM , and deep learning. Bob McGrew . Bob formerly led engineering and product management at Palantir, having worked there for a decade as it grew from 10 to 1,500 employees. Before Palantir, Bob was a PhD candidate in the Stanford Artificial Intelligence Lab working on multiagent systems. At OpenAI, he focuses on robotics. Christopher Berner . Christopher was previously an engineer at Facebook, where he worked on Presto , an open-source distributed SQL engine. He led the data warehouse deployment of Presto, and scaled it to hundreds of petabytes of data. In his spare time, he builds quadrotors and created Rospilot , an autopilot companion system that provides real-time video streaming and other features, using commodity hardware. Erika Reinhardt . Erika was previously the Director of Product Engineering at Planet Labs , where she spent the last four years building teams and software to support collecting and distributing high-frequency satellite imagery, ranging from mission control and manufacturing management, to image processing and platform design. She studied mechanical engineering and computer science at MIT. As OpenAI's Engineering Director, Erika will help manage and grow the team. Jakub Pachocki . Jakub recently finished a PhD at CMU where he worked on the theory of optimization. He co-invented the fastest known algorithms for solving linear systems on graphs and computing the geometric median . In the past, Jakub was a top competitive programmer, and has won the Google Code Jam and gained a gold medal in the ACM-ICPC . Jeremy Schlatter . Jeremy is an engineer who previously worked at Mailgun and Google. Since joining OpenAI he has built the tight encoding implementation for OpenAI Universe's go-vncdriver and tools for collecting human demonstrations of Universe environments. Jonathan Gray . Jonathan previously built a peer-to-peer file sync tool at AeroFS , and was an early engineer at Magic , an SMS-based virtual personal assistant. Prior to that, he studied Electrical & Computer Engineering at the University of Toronto. In high school and during his undergrad, he built software for cancer treatment at Sunnybrook Research Institute’s Focused Ultrasound Group . Peter Welinder . Peter was an engineering lead at Dropbox, where he led the photos infrastructure, computer vision, and machine learning teams. Before that, he co-founded AI startup Anchovi Labs and did his graduate work at Caltech in computer vision and crowd sourcing . Rachel Fong . Rachel most recently worked on data engineering at Locu . In the past she's designed robots, and built a haptic surgical simulator, a $50 desktop 3D scanner, and an NLP backend to extract data from uploaded documents to create personalized curriculums for learning language. She studied computer science at MIT. Shariq Hashme . Shariq founded an OCR startup, and previously worked at Gigster. He also builds DIY projects, like a $80 VR treadmill , a portable shower, and a device to control a mouse cursor with a tongue. He studied Computer Science and Electrical Engineering at the University of Maryland, where he founded a hackathon club which won the first-ever Major League Hacking season. Szymon Sidor . Szymon previously worked on model-based reinforcement learning at Vicarious , question answering at MetaMind, and on real-time file processing infrastructure at Dropbox. He earned his master's degree from MIT, where he explored reinforcement learning approaches to multi-stage reasoning in natural language processing, and identified security flaws in Android . Tom Brown . Tom was the cofounder and CTO of Grouper , where he wrote algorithms that used Facebook data to match up millions of people so they could meet in the real world. Before that he studied computational cognitive science at MIT, and built early versions of the core server at MoPub , now the world's largest mobile ad server . Yaroslav Bulatov . Yaroslav was previously an engineer at Google Brain and Google Streetview, where he implemented and trained large-scale neural networks. He designed the first system that outperformed humans at recognizing outdoor house numbers. The following former OpenAI interns have joined us full-time: Catherine Olsson , Jonathan Ho , Paul Christiano , Peter Chen , Prafulla Dhariwal , Rein Houthooft , and Rocky Duan .", "date": "2017-01-30"},
{"website": "Open-AI", "title": "baselines-acktr-a2c", "author": ["Yuhuai Wu", "Elman Mansimov", "Shun Liao", "Alec Radford", "John Schulman"], "link": "https://openai.com/blog/baselines-acktr-a2c/", "abstract": "We're releasing two new OpenAI Baselines implementations: ACKTR and A2C. A2C is a synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C) which we've found gives equal performance. ACKTR is a more sample-efficient reinforcement learning algorithm than TRPO and A2C, and requires only slightly more computation than A2C per update. ACKTR (pronounced \"actor\") — Actor Critic using Kronecker-factored Trust Region — was developed by researchers at the University of Toronto and New York University, and we at OpenAI have collaborated with them to release a Baselines implementation. The authors use ACKTR to learn control policies for simulated robots (with pixels as input, and continuous action spaces) and Atari agents (with pixels as input and discrete action spaces). ACKTR combines three distinct techniques: actor-critic methods , trust region optimization for more consistent improvement, and distributed Kronecker factorization to improve sample efficiency and scalability. For machine learning algorithms, two costs are important to consider: sample complexity and computational complexity. Sample complexity refers to the number of timesteps of interaction between the agent and its environment, and computational complexity refers to the amount of numerical operations that must be performed. ACKTR has better sample complexity than first-order methods such as A2C because it takes a step in the natural gradient direction, rather than the gradient direction (or a rescaled version as in ADAM). The natural gradient gives us the direction in parameter space that achieves the largest (instantaneous) improvement in the objective per unit of change in the output distribution of the network, as measured using the KL-divergence. By limiting the KL divergence, we ensure that the new policy does not behave radically differently than the old one, which could cause a collapse in performance. As for computational complexity, the KFAC update used by ACKTR is only 10-25% more expensive per update step than a standard gradient update. This contrasts with methods like TRPO (i.e, Hessian-free optimization), which requires a more expensive conjugate-gradient computation. In the following video you can see comparisons at different timesteps between agents trained with ACKTR to solve the game Q-Bert and those trained with A2C. ACKTR agents get higher scores than ones trained with A2C. This release includes an OpenAI baseline release of ACKTR, as well as a release of A2C. We're also publishing benchmarks that evaluate ACKTR against A2C, PPO and ACER on a range of tasks. In the following plot we show performance of ACKTR on 49 Atari games compared to other algorithm: A2C, PPO, ACER. The hyperparameters of ACKTR were tuned by the author of ACKTR solely on one game, Breakout. ACKTR performance also scales well with batch size because it not only derives a gradient estimate from the information in each batch, but also uses the information to approximate the local curvature in the parameter space. This feature is particularly favorable for large scale distributed training, in which large batch sizes are used. The Asynchronous Advantage Actor Critic method (A3C) has been very influential since the paper was published. The algorithm combines a few key ideas: An updating scheme that operates on fixed-length segments of experience (say, 20 timesteps) and uses these segments to compute estimators of the returns and advantage function. Architectures that share layers between the policy and value function. Asynchronous updates. After reading the paper, AI researchers wondered whether the asynchrony led to improved performance (e.g. \"perhaps the added noise would provide some regularization or exploration?\"), or if it was just an implementation detail that allowed for faster training with a CPU-based implementation. As an alternative to the asynchronous implementation, researchers found you can write a synchronous, deterministic implementation that waits for each actor to finish its segment of experience before performing an update, averaging over all of the actors. One advantage of this method is that it can more effectively use of GPUs, which perform best with large batch sizes. This algorithm is naturally called A2C, short for advantage actor critic. (This term has been used in several papers .) Our synchronous A2C implementation performs better than our asynchronous implementations — we have not seen any evidence that the noise introduced by asynchrony provides any performance benefit. This A2C implementation is more cost-effective than A3C when using single-GPU machines, and is faster than a CPU-only A3C implementation when using larger policies. We have included code in Baselines for training feedforward convnets and LSTMs on the Atari benchmark using A2C.", "date": "2017-08-18"},
{"website": "Open-AI", "title": "openai-fellows-interns-2019", "author": ["Larissa Schiavo", "Ashley Pilipiszyn"], "link": "https://openai.com/blog/openai-fellows-interns-2019/", "abstract": "We are now accepting applications for OpenAI Fellows and Interns for 2019. These programs provide an opportunity for people to work at OpenAI who are currently studying AI or wanting to transition from another speciality into AI . (Applications for the Fellows Winter 2019 are now closed—please check back later in 2019 to apply for our next cohort) OpenAI Fellows for the February 2019 Cohort will spend the first 2 months of this program working through a specially-curated curriculum written by OpenAI’s researchers and writing a research proposal based on their interests. Fellows will then work on the project outlined in their research proposal for the following 4 months with guidance from their OpenAI mentor. This 6-month program is specifically designed for people who want to transition into conducting artificial intelligence research and apply their current domain expertise/skills. Diverse backgrounds . Previous fellows have come from various backgrounds spanning across genetics, software engineering, physics, and theoretical computer science. Dedication . We will give priority to applicants that can join OpenAI full-time following the program. Passion for Research . We want to engage with researchers and scientists who are motivated to dive into AI research and have previous research project experience. Apply for Winter 2019 Fellows OpenAI interns for the Summer 2019 Cohort will work with an OpenAI team and contribute to OpenAI’s research over the course of 3 months, starting May 2019. Our interns contribute to large-scale projects like our work on Robotics and conduct their own research into AI. To get a sense of what sort of projects people work on, please check out some of the presentations from our 2018 Intern Open House . Self-Direction . We want to work with interns who have demonstrated an ability to guide themselves in solo work as well as contribute as a part of a team. Practical Skills . While research is important, we also really value interns who are great at implementing their ideas quickly, working with a shared codebase, and communicating changes to their work with their team. Technical & Research Experience . We value interns with a strong body of scientific work (especially single-author papers) as well as engineering backgrounds that can contribute to our current AI research efforts. We offer two decision periods: Early Decision and General Admission. Early Decision is designed to give candidates the opportunity to apply in Fall 2018 and secure an internship for Summer 2019 by the end of 2018. If you apply by November 2nd, 2018, we will notify you of your admissions status by December 21st, 2018. The application timeframe and deadlines below apply to both international and US-based applicants. General Admission allows candidates to apply later in the year for a Summer 2019 internship, with rolling admissions. We may close applications for Summer 2019 early if we reach capacity earlier than anticipated, and will update the job post when that happens. We are accepting applications from candidates without US work authorization from November 3rd, 2018 to February 15th, 2019, and will notify international applicants of their admissions status by March 15th, 2019. We are accepting applications from candidates with US work authorization from November 3rd, 2018 to March 15th, 2019, and will notify these applicants of their admissions status by April 15th, 2019 at the latest. If you apply after March 15th, 2019, we will consider you for future summer internships, or fall/winter internships (in exceptional cases). Apply for Summer 2019 Internship You want to transition into conducting artificial intelligence research You can demonstrate your interest in AI research via past projects or evidence of significant self-study (If you’re in the middle of a degree program, please apply for an internship instead.) Questions—email fellows@openai.com You are in your final year of your PhD or undergraduate degree and are available to work within a year of completing your internship. You have a strong body of scientific work, especially first-author papers. You have significant open-source contributions to the ML community You have strong engineering skills, with a primary interest in research. You are available for a minimum of 3 months, starting in May 2019. Questions—email internships@openai.com", "date": "2018-10-09"},
{"website": "Open-AI", "title": "block-sparse-gpu-kernels", "author": ["Scott Gray", "Alec Radford", "Durk Kingma"], "link": "https://openai.com/blog/block-sparse-gpu-kernels/", "abstract": "The development of model architectures and algorithms in the field of deep learning is largely constrained by the availability of efficient GPU implementations of elementary operations. One issue has been the lack of an efficient GPU implementation for sparse linear operations, which we're now releasing, together with initial results using them to implement a number of sparsity patterns. These initial results are promising but not definitive, and we invite the community to join us in pushing the limits of the architectures these kernels unlock. Sparse weight matrices, as opposed to dense weight matrices, have a large number of entries with a value of exactly zero. Sparse weight matrices are attractive as building blocks of models, since the computational cost of matrix multiplication and convolution with sparse blocks is only proportional to the number of non-zero blocks. Sparsity enables, for example, training of neural networks that are much wider and deeper than otherwise possible with a given parameter budget and computational budget, such as LSTMs with tens of thousands of hidden units . (The largest LSTMs trained today are only thousands of hidden units.) The kernels allow efficient usage of block-sparse weights in fully connected and convolutional layers (shown above). For convolutional layers, the kernels allow for sparsity in input and output feature dimensions; the connectivity is unaffected in the spatial dimensions. The sparsity is defined at the level of blocks (right figure above), and have been optimized for block sizes of 8x8 (such as in this example), 16x16 or 32x32. At the block level, the sparsity pattern is completely configurable. Since the kernels skip computations of blocks that are zero, the computational cost is only proportional to the number of non-zero weights, not the number of input/output features. The cost for storing the parameters is also only proportional to the number of non-zero weights. Below we show some example code for performing sparse matrix multiplication in Tensorflow. One particularly interesting use of block-sparse kernels is to use them to create small-world neural networks. Small-world graphs are connected in such a way that any two nodes in the graph are connected via a small number of steps, even if the graph has billions of nodes. Our motivation for implementing small world connectivity, is despite having a high degree of sparsity, we still want information to propagate quickly through the network. Brains display small-world connectivity patterns , which prompts the question whether the same property can improve the performance of LSTMs. Using small-world sparse connectivity, we efficiently trained LSTMs with almost twenty thousands hidden units, 5 times wider than a dense network with similar parameter counts, improving results on generative modeling of text, and semi-supervised sentiment classification; see our paper for more details. Following the setup we used in our sentiment neuron experiment , we trained LSTMs with approximately equivalent parameter counts and compared models with dense weight matrices against a block-sparse variant. The sparse model outperforms the dense model on all sentiment datasets. Our sparse model improves the state of the art on the document level IMDB dataset from 5.91% error ( Miyato et al, 2016 ) to 5.01%. This is a promising improvement over our previous results which performed best only on shorter sentence level datasets. By using sparse and wide LSTMs, the bits-per-character results in our experiments dropped from 1.059 to 1.048, for equal parameter counts (~ 100 million). Architectures with block-sparse linear layers can also improve upon results obtained with densely connected linear layers. We performed a simple modification of the PixelCNN++ model of CIFAR-10 natural images. A replacement of regular 2D convolutional kernels with sparse kernels, while deepening the network but keeping the rest of the hyper-parameters fixed, lead to a drop in the bits-per-dimension from 2.92 to 2.90, now state of the art on this dataset. Here we list some suggestions for future research. Most weights in neural networks can be pruned after training has finished . How much wall-clock time speed-up is possible at inference time when using pruning together with these kernels? In biological brains, the sparse structure of the network is partially determined during development , in addition to connection strengths. Can we do something similar in artificial neural networks, where we use gradients to not only learn the connection weights, but also the optimal sparsity structure? A recent paper proposed a method for learning block-sparse RNNs , and we recently proposed an algorithm for L0 regularization in neural networks , which can be used towards this end. We trained LSTMs with tens of thousands of hidden units , leading to better models of text. More generally, sparse layers make it possible to train models with huge weight matrices but the same number of parameters and the same computational cost as their smaller dense counterparts. What are application domains where this will make the most difference to performance?", "date": "2017-12-06"},
{"website": "Open-AI", "title": "better-exploration-with-parameter-noise", "author": ["Matthias Plappert", "Rein Houthooft", "Prafulla Dhariwal", "Szymon Sidor", "Pieter Abbeel", "Marcin Andrychowicz", "Richard Chen", "Xi Chen", "Tamim Asfour"], "link": "https://openai.com/blog/better-exploration-with-parameter-noise/", "abstract": "Parameter noise lets us teach agents tasks much more rapidly than with other approaches. After learning for 20 episodes on the HalfCheetah Gym environment (shown above), the policy achieves a score of around 3,000, whereas a policy trained with traditional action noise only achieves around 1,500. Parameter noise adds adaptive noise to the parameters of the neural network policy, rather than to its action space. Traditional RL uses action space noise to change the likelihoods associated with each action the agent might take from one moment to the next. Parameter space noise injects randomness directly into the parameters of the agent, altering the types of decisions it makes such that they always fully depend on what the agent currently senses. The technique is a middle ground between evolution strategies (where you manipulate the parameters of your policy but don’t influence the actions a policy takes as it explores the environment during each rollout) and deep reinforcement learning approaches like TRPO , DQN , and DDPG (where you don’t touch the parameters, but add noise to the action space of the policy). Parameter noise helps algorithms explore their environments more effectively, leading to higher scores and more elegant behaviors. We think this is because adding noise in a deliberate manner to the parameters of the policy makes an agent's exploration consistent across different timesteps, whereas adding noise to the action space leads to more unpredictable exploration which isn’t correlated to anything unique to the agent's parameters. People have previously tried applying parameter noise to policy gradients. We've extended this by showing that the technique works on policies based on deep neural networks and that it can be applied to both on- and off-policy algorithms. When conducting this research we ran into three problems: Different layers of the network have different sensitivities to perturbations. The sensitivity of the policy’s weights may change over time while training progresses, making it hard for us to predict the actions the policy will take. Picking the right scale of noise is difficult because it is hard to intuitively understand how parameter noise influences a policy during training. We use layer normalization to deal with the first problem, which ensures that the output of a perturbed layer (which will be the input to the next one) is still within a similar distribution. We tackle the second and third problem by introducing an adaptive scheme to adjust the size of the parameter space perturbations. This adjustment works by measuring the effect of the perturbation on action space and whether the action space noise level is larger or smaller than a defined target. This trick allows us to push the problem of choosing noise scale into action space, which is more interpretable than parameter space. We're also releasing baseline code that incorporates this technique for DQN, Double DQN, Dueling DQN, Dueling Double DQN, and DDPG. We've included benchmarks of the performance of DDQN with and without parameter noise on a subset of the Atari games corpus, and of three variants of DDPG on a range of continuous control tasks within the Mujoco simulator . When we first conducted this research, we found that the perturbations we applied to the Q function of DQN could sometimes be so extreme it would lead to the algorithm repeatedly executing the same action. To deal with this, we added a separate head that explicitly represents the policy as in DDPG (in regular DQN, the policy is only represented implicitly by the Q function) to make the setup more similar to our other experiments. However, when preparing our code for this release we ran an experiment that used parameter space noise without the separate policy head. We found that this worked comparably to our version with the separate policy head while being much simpler to implement. Further experiments confirmed that the separate policy head was indeed unnecessary as the algorithm had likely improved since our early experiments due to us altering how we re-scaled the noise. This led to a simpler, easier to implement, and less costly to train algorithm while still achieving very similar results. It’s important to remember that AI algorithms, especially in reinforcement learning, can fail silently and subtly , which can lead to people engineering solutions around missed bugs.", "date": "2017-07-27"},
{"website": "Open-AI", "title": "gathering_human_feedback", "author": ["Tom Brown", "Dario Amodei", "Paul Christiano"], "link": "https://openai.com/blog/gathering_human_feedback/", "abstract": "RL-Teacher is an open-source implementation of our interface to train AIs via occasional human feedback rather than hand-crafted reward functions. The underlying technique was developed as a step towards safe AI systems, but also applies to reinforcement learning problems with rewards that are hard to specify. The release contains three main components: A reward predictor that can be plugged into any agent and learns to predict the actions the agent could take that a human would approve of. An example agent that learns via a function specified by a reward predictor. RL-Teacher ships with three pre-integrated algorithms, including OpenAI Baselines PPO . A web-app that humans can use to give feedback, providing the data used to train the reward predictor. The entire system consists of less than 1,000 lines of Python code (excluding the agents). After you've set up your web server you can launch an experiment by running: Humans can give feedback via a simple web interface (shown above), which can be run locally (not recommended) or on a separate machine. Full documentation is available on the project's GitHub repository . We're excited to see what AI researchers and engineers do with this technology — please get in touch with any experimental results!", "date": "2017-08-03"},
{"website": "Open-AI", "title": "competitive-self-play", "author": ["Trapit Bansal", "Igor Mordatch", "Jakub Pachocki", "Ilya Sutskever", "Szymon Sidor"], "link": "https://openai.com/blog/competitive-self-play/", "abstract": "We've found that self-play allows simulated AIs to discover physical skills like tackling, ducking, faking, kicking, catching, and diving for the ball, without explicitly designing an environment with these skills in mind. Self-play ensures that the environment is always the right difficulty for an AI to improve. Taken alongside our Dota 2 self-play results , we have increasing confidence that self-play will be a core part of powerful AI systems in the future. We set up competitions between multiple simulated 3D robots on a range of basic games, trained each agent with simple goals (push the opponent out of the sumo ring, reach the other side of the ring while preventing the other agent from doing the same, kick the ball into the net or prevent the other agent from doing so, and so on), then analyzed the different strategies that emerged. Agents initially receive dense rewards for behaviours that aid exploration like standing and moving forward, which are eventually annealed to zero in favor of being rewarded for just winning and losing. Despite the simple rewards, the agents learn subtle behaviors like tackling, ducking, faking, kicking and catching, and diving for the ball. Each agent’s neural network policy is independently trained with Proximal Policy Optimization . To understand how complex behaviors can emerge through a combination of simple goals and competitive pressure, let's analyze the sumo wrestling task. Here we took the dense reward defined in previous work for training a humanoid to walk, removed the term for velocity, added the negative L2 distance from the center of the ring and took this as a dense exploration reward for our sumo agents. The agents were allowed to use this reward for exploration in the ring initially, then we slowly annealed it to zero so the agents would learn to optimize for the competition reward — pushing the other player out of the ring — for the remaining training iterations. Though it is possible to design tasks and environments that require each of these skills, this requires effort and ingenuity on the part of human designers, and the agents' behaviors will be bounded in complexity by the problems that the human designer can pose for them. By developing agents through thousands of iterations of matches against successively better versions of themselves, we can create AI systems that successively bootstrap their own performance; we saw a similar phenomenon in our Dota 2 project , where self-play let us create an RL agent that could beat top human players at a solo version of the e-sport. These agents also exhibit transfer learning, applying skills learned in one setting to succeed in another never-before-seen one. In one case, we took the agent trained on the self-play sumo wrestling task and faced it with the task of standing while being perturbed by \"wind\" forces. The agent managed to stay upright despite never seeing the windy environment or observing wind forces, while agents trained to walk using classical reinforcement learning would fall over immediately. Our agents were overfitting by co-learning policies that were precisely tailored to counter specific opponents, but would fail when facing new ones with different characteristics. We dealt with this by pitting each agent against several different opponents rather than just one. These possible opponents come from an ensemble of policies that were trained in parallel as well as policies from earlier in the training process. Given this diversity of opponents, agents needed to learn general strategies and not just ones targeted to a specific opponent. Additionally, we’re releasing the MuJoCo environments and trained policies used in this work so that others can experiment with these systems. If you'd like to work on self-play systems, we're hiring !", "date": "2017-10-11"},
{"website": "Open-AI", "title": "generalizing-from-simulation", "author": ["Xue Bin Peng", "Lerrel Pinto", "Alex Ray", "Bob McGrew", "Jonas Schneider", "Josh Tobin", "Marcin Andrychowicz", "Peter Welinder", "Pieter Abbeel", "Wojciech Zaremba"], "link": "https://openai.com/blog/generalizing-from-simulation/", "abstract": "Our latest robotics techniques allow robot controllers, trained entirely in simulation and deployed on physical robots, to react to unplanned changes in the environment as they solve simple tasks. That is, we've used these techniques to build closed-loop systems rather than open-loop ones as before . The simulator need not match the real-world in appearance or dynamics; instead, we randomize relevant aspects of the environment, from friction to action delays to sensor noise. Our new results provide more evidence that general-purpose robots can be built by training entirely in simulation, followed by a small amount of self-calibration in the real world. We developed dynamics randomization to train a robot to adapt to unknown real-world dynamics. During training, we randomize a large set of ninety-five properties that determine the dynamics of the environment, such as altering the mass of each link in the robot’s body; the friction and damping of the object it is being trained on; the height of the table the object is on; the latency between actions; the noise in its observations; and so on. We used this approach to train an LSTM -based policy to push a hockey puck around a table. Our feed-forward networks fail at this task, whereas LSTMs can use their past observations to analyze the dynamics of the world and adjust their behavior accordingly. We also trained a robot end-to-end in simulation using reinforcement learning (RL), and deployed the resulting policy on a physical robot. The resulting system maps vision directly to action without special sensors, and can adapt to visual feedback. The abundance of RL results with simulated robots can make it seem like RL easily solves most robotics tasks. But common RL algorithms work well only on tasks where small perturbations to your action can provide an incremental change to the reward. Some robotics tasks have simple rewards, like walking, where you can be scored on distance traveled. But most tasks do not — to define a dense reward for block stacking, you'd need to encode that the arm is close to the block, that the arm approaches the block in the correct orientation, that the block is lifted off the ground, the distance of block to the desired position, etc. We spent a number of months unsuccessfully trying to get conventional RL algorithms working on pick-and-place tasks before ultimately developing a new reinforcement learning algorithm, Hindsight Experience Replay (HER), which allows agents to learn from a binary reward by pretending that a failure was what they wanted to do all along and learning from it accordingly. (By analogy, imagine looking for a gas station but ending up at a pizza shop. You still don't know where to get gas, but you've now learned where to get pizza.) We also used domain randomization on the visual shapes to learn a vision system robust enough for the physical world. Our HER implementation uses the actor-critic technique with asymmetric information. (The actor is the policy, and the critic is a network which receives action/state pairs and estimates their Q-value, or sum of future rewards, providing training signal to the actor.) While the critic has access to the full state of the simulator, the actor only has access to RGB and depth data. Thus the critic can provide fully accurate feedback, while the actor uses only data present in the real world. Both techniques increase the computational requirements: dynamics randomization slows training down by a factor of 3x, while learning from images rather than states is about 5-10x slower. We see three approaches to building general-purpose robots: training on huge fleets of physical robots, making simulators increasingly match the real world, and randomizing the simulator to allow the model to generalize to the real-world. We increasingly believe that the third will be the most important part of the solution. If you’re interested in helping us push towards general-purpose robots, consider joining our team at OpenAI .", "date": "2017-10-19"},
{"website": "Open-AI", "title": "learning-a-hierarchy", "author": ["Kevin Frans", "Jonathan Ho", "Peter Chen", "Pieter Abbeel", "John Schulman"], "link": "https://openai.com/blog/learning-a-hierarchy/", "abstract": "Humans solve complicated challenges by breaking them up into small, manageable components. Grilling pancakes consists of a series of high-level actions, such as measuring flour, whisking eggs, transferring the mixture to the pan, turning the stove on, and so on. Humans are able to learn new tasks rapidly by sequencing together these learned components, even though the task might take millions of low-level actions, i.e., individual muscle contractions. On the other hand, today’s reinforcement learning methods operate through brute force search over low-level actions, requiring an enormous number of attempts to solve a new task. These methods become very inefficient at solving tasks that take a large number of timesteps. Our solution is based on the idea of hierarchical reinforcement learning, where agents represent complicated behaviors as a short sequence of high-level actions. This lets our agents solve much harder tasks: while the solution might require 2000 low-level actions, the hierarchical policy turns this into a sequence of 10 high-level actions, and it's much more efficient to search over the 10-step sequence than the 2000-step sequence. Our algorithm, meta-learning shared hierarchies (MLSH), learns a hierarchical policy where a master policy switches between a set of sub-policies. The master selects an action every every N timesteps, where we might take N=200. A sub-policy executed for N timesteps constitutes a high-level action, and for our navigation tasks, sub-policies correspond to walking or crawling in different directions. In most prior work, hierarchical policies have been explicitly hand-engineered. Instead, we aim to discover this hierarchical structure automatically through interaction with the environment. Taking a meta-learning perspective, we define a good hierarchy as one that quickly reaches high reward quickly when training on unseen tasks. Hence, the MLSH algorithm aims to learn sub-policies that enable fast learning on previously unseen tasks. We train on a distribution over tasks, sharing the sub-policies while learning a new master policy on each sampled task. By repeatedly training new master policies, this process automatically finds sub-policies that accommodate  the master policy's learning dynamics. In our AntMaze environment, a Mujoco Ant robot is placed into a distribution of 9 different mazes and must navigate from the starting position to the goal. Our algorithm is successfully able to find a diverse set of sub-policies that can be sequenced together to solve the maze tasks, solely through interaction with the environment. This set of sub-policies can then be used to master a larger task than the ones they were trained on (see video at beginning at post). We’re releasing the code for training the MLSH agents, as well as the MuJoCo environments we built to evaluate these algorithms.", "date": "2017-10-26"},
{"website": "Open-AI", "title": "retro-contest", "author": ["Christopher Hesse", "John Schulman", "Vicki Pfau", "Alex Nichol", "Oleg Klimov", "Larissa Schiavo"], "link": "https://openai.com/blog/retro-contest/", "abstract": "We're launching a transfer learning contest that measures a reinforcement learning algorithm's ability to generalize from previous experience. In typical RL research,  algorithms are tested in the same environment where they were trained, which favors algorithms which are good at memorization and have many hyperparameters. Instead, our contest tests an algorithm on previously unseen video game levels. This contest uses Gym Retro , a new platform integrating classic games into Gym, starting with 30 SEGA Genesis games. UPDATE: The results are in! The OpenAI Retro Contest gives you a training set of levels from the Sonic The Hedgehog™ series of games, and we evaluate your algorithm on a test set [1] of custom levels that we have created for this contest. The contest will run from April 5th to June 5th. To get people started we're releasing retro-baselines , which shows how to run several RL algorithms on the contest tasks. You can use any environments or datasets you want at training time, but at test time you only get about 18 hours (1 million timesteps) on each never-before-seen level. 18 hours may sound like a long time to play a single game level, but existing RL algorithms perform far worse than humans given this training budget. To describe the benchmark in detail, as well as provide some baseline results, we are releasing a technical report: Gotta Learn Fast: A New Benchmark for Generalization in RL . This report contains details about the benchmark as well as results from running Rainbow DQN , PPO , and a simple random guessing algorithm called JERK. JERK samples random action sequences in a way that is optimized for Sonic, and as training progresses it  replays the top-scoring sequence of actions more frequently. We found that we could significantly boost PPO's performance on the test levels by leveraging experience from the training levels. When the network was pre-trained on the training levels and fine-tuned on the test levels, its performance nearly doubled, making it better than the strongest alternative baselines. While this is not the first reported instance of successful transfer learning in RL, it is exciting because it shows that transfer learning can have a large and reliable effect. But we have a long way to go before our algorithms can rival human performance. As shown above, after two hours of practice on the training levels and one hour of play on each test level, humans are able to attain scores that are significantly higher than those attained by RL algorithms, including ones that perform transfer learning. We've created a dataset of recordings of humans beating the Sonic levels used in the Retro Contest. These recordings can be used to have the agent start playing from random points sampled from the course of each level, exposing the agent to a lot of areas it may not have seen if it only started from the beginning of the level. Researchers can also use these recordings to try to train agents that learn from demonstrations. We are releasing Gym Retro, a system for wrapping classic video games as RL environments. This preliminary release includes 30 SEGA Genesis games from the SEGA Mega Drive and Genesis Classics Steam Bundle as well as 62 of the Atari 2600 games from the Arcade Learning Environment. The Arcade Learning Environment , a collection of Atari 2600 games with interfaces for reinforcement learning, has been a major driver of RL research for the last five years. These Atari games were more varied and complex than previous RL benchmarks, having been designed to challenge the motor skills and problem solving abilities of human players. The Gym Retro Beta utilizes a more modern console than Atari — SEGA Genesis — expanding the quantity and complexity of games that are available for RL research. Games made on the Genesis tend to have lots of levels that are similar in some dimensions (physics, object appearances) and different in others (layout, items), which makes them good testbeds for transfer learning. They also tend to be more complex than Atari games since they exploit the better hardware of the Genesis (for example, it has more than 500 times as much RAM as the Atari, a greater range of possible control inputs, and support for better graphics). Gym Retro was inspired by the Retro Learning Environment but written to be more flexible than RLE; for instance, in Gym Retro you can specify the environment definition through JSON files rather than C++ code, making it easier to integrate new games. Gym Retro is our second generation attempt to build a large dataset of reinforcement learning environments. It builds on some of the same ideas as Universe from late 2016, but we weren't able to get good results from that implementation because Universe environments ran asynchronously, could only run in real time, and were often unreliable due to screen-based detection of game state. Gym Retro extends the model of the Arcade Learning Environment to a much larger set of potential games. To get started with Gym Retro check out the Getting Started section on GitHub. Sometimes, algorithms can find exploits within the game. Here, a PPO-trained policy discovers it can slip through the walls of a level to move right and attain a higher score — another example of how particular reward functions can lead to AI agents manifesting odd behaviors . Thanks to Jonathan Gray and Tom Brown for their contributions to early versions of gym-retro. Thanks to Philipp Moritz, Robert Nishihara, Adam Stelmaszczyk, Aravind Srinivas, Qin Yongliang, Julian Togelius, and Emilio Parisotto for helpful feedback. There are two secret test sets: one to populate the leaderboard while the contest is running, and another that is used only once for the final ranking. In addition, we suggest a training/test split of the provided levels, which is used for all results in the tech report, as well as the learning curves above. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2018-04-05"},
{"website": "Open-AI", "title": "meta-learning-for-wrestling", "author": ["Maruan Al-Shedivat", "Trapit Bansal", "Yura Burda", "Ilya Sutskever", "Igor Mordatch", "Pieter Abbeel"], "link": "https://openai.com/blog/meta-learning-for-wrestling/", "abstract": "We've extended the Model-Agnostic Meta-Learning (MAML) algorithm by basing its objective function on optimizing against pairs of environments, rather than single ones as in stock MAML. MAML initializes the policies of our agents so that after only a small number of parameter updates during execution on a new environment (or task) the agents learn to do better in that environment. The policy parameter updates at execution are done via gradient ascent steps on the reward collected during the few episodes of initial interaction with a new environment. By training on pairs we’re able to create policies that quickly adapt to previously unseen environments, as long as the environment doesn’t diverge too wildly from previous ones. To test our continuous adaptation approach we designed 3 types of agents — Ant (4-leg), Bug (6-leg), and Spider (8-leg) — and set up a multi-round game where each agent played several matches against the same opponent and adapted its policy parameters between the rounds to better counter the opponent's policy. In tests, we found that agents that could adapt their tactics are much better competitors than agents that have fixed policies. After training over a hundred agents, some of which learned fixed policies and others learned to adapt, we evaluated the fitness of each agent. Learning on the fly can also let agents deal with unusual changes in their own bodies as well, like adapting to some of their own limbs losing functionality over time. This suggests we can use techniques like this to develop agents that can handle both changes in their external environment and also changes in their own bodies or internal states. We're exploring meta-learning as part of our work on large-scale multi-agent research. Additionally, we're releasing the MuJoCo environments and trained policies used in this work so that others can experiment with these systems.", "date": "2017-10-11"},
{"website": "Open-AI", "title": "science-of-ai", "author": ["Sam McCandlish", "Jared Kaplan", "Dario Amodei"], "link": "https://openai.com/blog/science-of-ai/", "abstract": "In the last few years AI researchers have had increasing success in speeding up neural network training through data-parallelism, which splits large batches of data across many machines. Researchers have successfully used batch sizes of tens of thousands for image classification and language modeling , and even millions for RL agents that play the game Dota 2. These large batches allow increasing amounts of compute to be efficiently poured into the training of a single model, and are an important enabler of the fast growth in AI training compute . However, batch sizes that are too large show rapidly diminishing algorithmic returns, and it's not clear why these limits are larger for some tasks and smaller for others. [1] We have found that by measuring the gradient noise scale, a simple statistic that quantifies the signal-to-noise ratio of the network gradients, [2] we can approximately predict the maximum useful batch size. Heuristically, the noise scale measures the variation in the data as seen by the model (at a given stage in training). When the noise scale is small, looking at a lot of data in parallel quickly becomes redundant, whereas when it is large, we can still learn a lot from huge batches of data. This type of statistic is widely used for sample size selection and has been proposed for use in deep learning , but has not been measured or applied systematically for modern training runs. We verified this prediction for a wide range of machine learning tasks shown in the figure above, including image recognition, language modeling, Atari games, and Dota. Specifically, we did training runs at a wide range of batch sizes (tuning the learning rate separately for each) for all of these tasks and compared the speedups in training to what the noise scale predicts should happen. Since large batch sizes often require careful and expensive tuning or special learning rate schedules to be effective, knowing an upper limit ahead of time provides a significant practical advantage in training new models. We've found it helpful to visualize the results of these experiments in terms of a tradeoff between wall time for training and total bulk compute that we use to do the training (proportional to dollar cost). At very small batch sizes, doubling the batch allows us to train in half the time without using extra compute (we run twice as many chips for half as long). At very large batch sizes, more parallelization doesn't lead to faster training. There is a \"bend\" in the curve in the middle, and the gradient noise scale predicts where that bend occurs. We make these curves by setting a level of performance (say a score of 1000 on the Atari game of Beam Rider) and seeing how long it takes to train to that performance at various batch sizes. The results match our model's predictions relatively closely, across many different values of the performance target. We've observed several patterns in the gradient noise scale which offer clues as to what the future of AI training may hold. First, in our experiments, the noise scale typically increases by an order of magnitude or more over the course of training. Intuitively, this means the network learns the more \"obvious\" features of the task early in training and learns more intricate features later. For example, in the case of an image classifier, the network might first learn to identify small-scale features such as edges or textures that are present in most images, while only later putting these pieces together into more general concepts such as cats and dogs. To see the full variety of edges or textures, the network only needs to see a small number of images, so the noise scale is smaller; once the network knows more about larger objects, it can process many more images at once without seeing duplicative data. We see some preliminary indications that the same effect holds across different models on the same dataset—more powerful models have a higher gradient noise scale, but only because they achieve a lower loss. Thus, there's some evidence that the increasing noise scale over training isn't just an artifact of convergence, but occurs because the model gets better. If this is true, then we expect future, more powerful models to have higher noise scale and therefore be more parallelizable. Second, tasks that are subjectively more difficult are also more amenable to parallelization. In the context of supervised learning, there is a clear progression from MNIST, to SVHN, to ImageNet. In the context of reinforcement learning, there is a clear progression from Atari Pong to Dota 1v1 to Dota 5v5 , with the optimal batch sizes differing by a factor of more than 10,000. Thus, as AI advances to new and more difficult tasks, we expect models to tolerate higher batch size. The degree of data parallelism significantly affects the speed at which AI capabilities can progress. Faster training makes more powerful models possible and accelerates research through faster iteration times. In an earlier study, AI and Compute , we observed that the compute being used to train the largest ML models is doubling every 3.5 months, and we noted that this trend is driven by a combination of economics (willingness to spend money on compute) and the algorithmic ability to parallelize training. The latter factor (algorithmic parallelizability) is harder to predict and its limits are not well-understood, but our current results represent a step toward systematizing and quantifying it. In particular, we have evidence that more difficult tasks and more powerful models on the same task will allow for more radical data-parallelism than we have seen to date, providing a key driver for the continued fast exponential growth in training compute. (And this is without even considering recent advances in model-parallelism , which may allow for even further parallelization on top of data-parallelism). The continued growth of training compute, and its apparently predictable algorithmic basis, further highlights the possibility of rapid increases in AI capabilities over the next few years, and emphasizes the urgency of research into making sure such systems are safe and that they are used responsibly . A central challenge of AI policy will be to work out how to use measures like this to make predictions about the characteristics of future AI systems, and use this knowledge to conceive of policies that let society maximize the upsides and minimize the downsides of these technologies. OpenAI is committed both to continuing to conduct rigorous analyses to give us foresight on what the future of AI holds, and to acting to address the issues that these analyses raise. If you want to study the \"Science of AI\" and help us make neural network training more data-driven, consider applying to work at OpenAI . Thanks to the OpenAI Dota team (Greg Brockman, Brooke Chan, Przemysław Debiak, Christy Dennison, David Farhi, Rafał Józefowicz, Jakub Pachocki, Michael Petrov, Henrique Pondé, Jonathan Raiman, Szymon Sidor, Jie Tang, Filip Wolski, and Susan Zhang) for their contribution to this research. We also thank the following for feedback on drafts of this post: Greg Brockman, Paul Christiano, Danny Hernandez, Joel Hestness, Heewoo Jun, Jaehoon Lee, Aleksander Madry, Chris Olah, and John Schulman. A complementary study done in parallel with this one performed meticulous experimental tests of large batch training as well as an extensive review of the previous literature, clearing up a number of inconsistencies in earlier work. They found a significant variation in the potential for parallelism between tasks, and our work appears to explain a major portion of this variance. Their work also suggests that large batch training does not affect generalization. We believe that systematic surveys such as this one and others are incredibly valuable to the field, and we are committed to continued work on the \"Science of AI.\" ↩︎ When training neural networks, we typically process only a small batch of data at a time, which gives a noisy estimate of the true network gradient. We find that the gradient noise scale B noise =E[| G - G true | 2 ] / | G true | 2 , where the expectation is taken over individual data points, estimates the maximum useful batch size. When the gradient is computed from a batch of size B , the normalized distance between the estimated gradient and the true gradient is given by E[| G B - G true | 2 /| G true | 2 ] = B noise / B . The point at which increasing B stops reducing the noisiness of the gradient significantly occurs around B = B noise , and this is also the point at which gains in training speed taper off. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2018-12-14"},
{"website": "Open-AI", "title": "quantifying-generalization-in-reinforcement-learning", "author": ["Karl Cobbe"], "link": "https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/", "abstract": "We’re releasing CoinRun, a training environment which provides a metric for an agent's ability to transfer its experience to novel situations and has already helped clarify a longstanding puzzle in reinforcement learning. [1] CoinRun strikes a desirable balance in complexity: the environment is simpler than traditional platformer games like Sonic the Hedgehog but still poses a worthy generalization challenge for state of the art algorithms. Generalizing between tasks remains difficult for state of the art deep reinforcement learning (RL) algorithms. Although trained agents can solve complex tasks, they struggle to transfer their experience to new environments. Even though people know that RL agents tend to overfit — that is, to latch onto the specifics of their environment rather than learn generalizable skills — RL agents are still benchmarked by evaluating on the environments they trained on. This would be like testing on your training set in supervised learning! Previous work has used the Sonic benchmark , procedurally generated gridworld mazes , and the General Video Game AI framework to address this problem. In all cases, generalization is measured by training and testing agents on different sets of levels. Agents trained on our Sonic benchmark were great at the training levels but performed poorly on the test levels without any fine-tuning. In a similar displays of overfitting, agents trained on procedurally generated mazes learned to memorize a large number of training levels, and GVG-AI agents performed poorly under difficulty settings that weren’t seen during training. CoinRun was designed to be tractable for existing algorithms and mimics the style of platformer games like Sonic. The levels of CoinRun are procedurally generated , providing agents access to a large and easily quantifiable supply of training data. The goal of each CoinRun level is simple: collect the single coin that lies at the end of the level. Several obstacles, both stationary and non-stationary, lie between the agent and the coin. A collision with an obstacle results in the agent’s immediate death. The only reward in the environment is obtained by collecting the coin, and this reward is a ﬁxed positive constant. The level terminates when the agent dies, the coin is collected, or after 1000 time steps. We trained 9 agents to play CoinRun, each with a different number of available training levels. The ﬁrst 8 agents trained on sets ranging from of 100 to 16,000 levels. We trained the ﬁnal agent on an unrestricted set of levels, so this agent never sees the same level twice. We trained our agents with policies using a common 3-layer convolutional architecture , which we call Nature-CNN. Our agents trained with Proximal Policy Optimization ( PPO ) for a total of 256M timesteps. Since an epsiode lasts 100 timesteps on average, agents with fixed training sets will see each training level thousands to millions of times. The final agent, trained with the unrestricted set, will see roughly 2 million distinct levels — each of them exactly once. We collected each data point in the following graphs by averaging the ﬁnal agent’s performance across 10,000 episodes. At test time, the agent is evaluated on never-before-seen levels. We discovered substantial overﬁtting occurs when there are less than 4,000 training levels. In fact, we still see overfitting even with 16,000 training levels! Unsurprisingly, agents trained with the unrestricted set of levels performed best, as these agents had access to the most data. These agents are represented by the dotted line in the following graphs. We compared our Nature-CNN baseline against the convolutional architecture used in IMPALA and found the IMPALA-CNN agents generalized much better with any training set as seen below. In our next experiments, we used a fixed training set of 500 CoinRun levels. Our baseline agents struggle to generalize with so few levels, making this an ideal training set for a benchmark. We encourage others to evaluate their own methods by training on the same 500 levels, directly comparing test time performance. Using this training set, we investigated the impact of several regularization techniques: Dropout and L2 regularization : Both noticeably reduce the generalization gap, though L2 regularization has a bigger impact. Data augmentation (modified Cutout ) and batch normalization : Both data augmentation and batch normalization significantly improve generalization. Environmental stochasticity : Training with stochasticity improves generalization to a greater extent than any of the previously mentioned techniques (see the paper for details). We also developed two additional environments to investigate overﬁtting: a CoinRun variant called CoinRun-Platforms and a simple maze navigation environment called RandomMazes . In these experiments, we used the original IMPALA-CNN architecture followed by a LSTM , since memory is necessary to perform well in these environments. In CoinRun-Platforms, there are several coins the agent attempts to collect within the 1000 step time-limit. Coins are randomly scattered across platforms in the level. Levels are a larger, ﬁxed size in CoinRun-Platforms, so the agent must more actively explore, occasionally retracing its steps. Final train and test performance in CoinRun-Platforms after 2B timesteps, as a function of the number of training levels. When we ran both CoinRun-Platforms and RandomMazes through our baseline experiment, our agents strongly overfit in all cases. We observe particularly strong overﬁtting in the case of RandomMazes, as a sizeable generalization gap remains even when using 20,000 training levels. Our results provide insight into the challenges underlying generalization in RL. Using the procedurally generated CoinRun environment, we can precisely quantify such overﬁtting. With this metric, we can better evaluate key architectural and algorithmic decisions. We believe that the lessons learned from this environment will apply in more complex settings, and we hope to use this benchmark, and others like it, to iterate towards more generalizable agents. We suggest the following for future research: Investigate the relationship between environment complexity and the number of levels required for good generalization Investigate whether different recurrent architectures are better suited for generalization in these environments Explore ways to effectively combine different regularization methods If you are interested in this line of research, consider working at OpenAI ! Thanks to the many people who contributed to this paper and blog post: Oleg Klimov, Chris Hesse, Taehoon Kim, John Schulman, Mira Murati, Jack Clark, Ashley Pilipiszyn, Matthias Plappert, Ilya Sutskever, Greg Brockman Jon Walsh, Caleb Kruse, Nikhil Mishra Kenney Even impressive RL policies are often trained without supervised learning techniques such as dropout and batch normalization. In the CoinRun generalization regime, however, we find that these methods do have a positive impact and that our previous RL policies were overfitting to particular MDPs. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2018-12-06"},
{"website": "Open-AI", "title": "discovering-types-for-entity-disambiguation", "author": ["Jonathan Raiman"], "link": "https://openai.com/blog/discovering-types-for-entity-disambiguation/", "abstract": "We've built a system for automatically figuring out which object is meant by a word by having a neural network decide if the word belongs to each of about 100 automatically-discovered \"types\" (non-exclusive categories). For example, given a sentence like \"the prey saw the jaguar cross the jungle\", rather than trying to reason directly whether jaguar means the car , the animal , or something else , the system plays \"20 questions\" with a pre-chosen set of categories. This approach gives a big boost in state-of-the-art on several entity disambiguation datasets. In our training data jaguar refers to the car 70% of the time, the animal 29% of the time, and the aircraft 1% of the time. With our types approach, the possible disambiguations in the first example don't change a huge amount — apparently the model is ok with jaguars running down the highway — but change hugely in the second — it's not ok with Jaguars taking a cruise through the jungle. We achieve 94.88% accuracy on CoNLL (YAGO) (previous state of the arts: 91.50 %, 91.70 %) and 90.85% on TAC KBP 2010 challenge (previous state of the arts: 87.20 %, and 87.70 %). Previous methods used distributed representations . Types can go almost all the way on these tasks, as perfect type prediction would give accuracies of 98.6-99%. Our system uses the following steps: Here are some other examples of our system in action: Wikidata's knowledge graph can be turned into a source of training data for mapping fine-grained entities to types. We apply its instance of relation recursively to determine the set of types for any given entity — for example, any descendent node of the human node has type human. Wikipedia can also provide entity-to-type mapping through its category link . Wikipedia-internal link statistics provide a good estimate of the chance a particular phrase refers to some entity. However, this is noisy since Wikipedia will often link to specific instance of a type rather than the type itself ( anaphora — e.g. king → Charles I of England) or link from a nickname ( metonymy ). This results in an explosion of associated entities (e.g. king has 974 associated entities) and distorted link frequencies (e.g. queen links to the band Queen 4920 times, Elizabeth II 1430 times, and monarch only 32 times). The easiest approach is to prune rare links , but this loses information. We instead use the Wikidata property graph to heuristically turn links into their \"generic\" meaning, as illustrated below. After this process, king goes from 974 to 14 associated entities, while the number of links from queen to monarch increases from 32 to 3553. We need to select the best type system and parameters such that disambiguation accuracy is maximized. There's a huge number of possible sets of types, making an exact solution intractable. Instead, we use heuristic search or stochastic optimization (evolutionary algorithm) to select a type system, and gradient descent to train a type classifier to predict the behavior of the type system. We need to select types that are discriminating (so quickly whittle down the possible set of entities), while being easy to learn (so surrounding context is informative for a neural network to infer that a type applies). We inform our search with two heuristics: learnability (average of area under the curve [AUC] scores of a classifier trained to predict type membership), and oracle accuracy (how well we would disambiguate entities if we predicted all types perfectly). We train binary classifiers to predict membership in each of the 150,000 most common types in our dataset, given a window of context. The area under the curve (AUC) of the classifier becomes the \"learnability score\" for that type. High AUC means it's easy to predict this type from context; poor performance can mean we have little training data or that a word window isn't terribly helpful (this tends to be true for unnatural categories like ISBNs ). Our full model takes several days to train, so we instead use a much smaller model as a proxy in our \"learnability score\", which takes only 2.5s to train. We can now use these learnability scores and count statistics to estimate the performance of a given subset of types as our type system. Below you can run the Cross Entropy Method to discover types in your browser. Note how changing sample size and penalties affects the solution. To better visualize what parts of the type system design are easy and hard, we invite you to try your hand at designing your own below. After choosing a high-level domain you can start looking at ambiguous examples. The possible answers are shown as circles on the top row, and the correct answer is the colored circle (hover to see its name). The bottom row contains types you can use. Lines connecting the top to the bottom row are inheritance relations. Select the relations you want. Once you have enough relations to separate the right answer from the rest, the example is disambiguated. Using the top solution from our type system optimization, we can now label data from Wikipedia using labels generated by the type system. Using this data (in our experiments, 400M tokens for each of English and French), we can now train a bidirectional LSTM to independently predict all the type memberships for each word. On the Wikipedia source text, we only have supervision on intra-wiki links, however this is sufficient to train a deep neural network to predict type membership with an F1 of over 0.91. One of our type systems, discovered by beam search, includes types such as Aviation , Clothing , and Games (as well as surprisingly specific ones like 1754 in Canada — indicating 1754 was an exciting year in the dataset of 1,000 Wikipedia articles it was trained on); you can also view the full type system. Predicting entities in a document usually relies on a \"coherence\" metric between different entities, e.g. measuring how well each entity fits with each other, which is O(N^2) in the length of the document. Instead, our runtime is O(N) as we need only to look up each phrase in a trie which maps phrases to their possible meanings. We rank each of the possible entities according to the link frequency seen in Wikipedia, refined by weighting each entity by its likelihood under the type classifier. New entities can be added just by specifying their type memberships (person, animal, country of origin, time period, etc..). Our approach has many differences to previous work on this problem. We're interested in how well end-to-end learning of distributed representations performs in comparison to the type-based inference we developed here. The type systems here were discovered using a small Wikipedia subset; scaling to all of Wikipedia could discover a type system with broad application. We hope you find our code useful! If you'd like to help push research like this forward, please apply to OpenAI!", "date": "2018-02-07"},
{"website": "Open-AI", "title": "roboschool", "author": ["Oleg Klimov", "John Schulman"], "link": "https://openai.com/blog/roboschool/", "abstract": "We are releasing Roboschool : open-source software for robot simulation, integrated with OpenAI Gym . Roboschool provides new OpenAI Gym environments for controlling robots in simulation. Eight of these environments serve as free alternatives to pre-existing MuJoCo implementations, re-tuned to produce more realistic motion. We also include several new, challenging environments. Roboschool also makes it easy to train multiple agents together in the same environment. After we launched Gym , one issue we heard from many users was that the MuJoCo component required a paid license (though MuJoCo recently added free student licenses for personal and class work). Roboschool removes this constraint, letting everyone conduct research regardless of their budget. Roboschool is based on the Bullet Physics Engine , an open-source, permissively licensed physics library that has been used by other simulation software such as Gazebo and V-REP . Roboschool ships with twelve environments, including tasks familiar to Mujoco users as well as new challenges, such as harder versions of the Humanoid walker task, and a multi-player Pong environment. We plan to expand this collection over time and look forward to the community contributing as well. For the existing MuJoCo environments, besides porting them to Bullet, we have modified them to be more realistic. Here are three of the environments we ported, with explanations of how they differ from the existing environments. You can find trained policies for all of these environments in the agent_zoo folder in the GitHub repository. You can also access a demo_race script to initiate a race between three robots. In several of the previous OpenAI Gym environments, the goal was to learn a walking controller. However, these environments involved a very basic version of the problem, where the goal is simply to move forward. In practice, the walking policies would learn a single cyclic trajectory and leave most of the state space unvisited. Furthermore, the final policies tended to be very fragile: a small push would often cause the robot to crash and fall. We have added two more environments with the 3D humanoid, which make the locomotion problem more interesting and challenging. These environments require interactive control — the robots must run towards a flag, whose position randomly varies over time. HumanoidFlagrun is designed to teach the robot to slow down and turn. The goal is to run towards the flag, whose position varies randomly. HumanoidFlagrunHarder in addition allows the robot to fall and gives it time to get back on foot. It also starts each episode upright or laying on the ground, and the robot is constantly bombarded by white cubes to push it off its trajectory. We ship trained policies for both HumanoidFlagrun and HumanoidFlagrunHarder . The walks aren't as fast and natural-looking as the ones we see from the regular humanoid, but these policies can recover from many situations, and they know how to steer. This policy itself is still a multilayer perceptron, which has no internal state, so we believe that in some cases the agent uses its arms to store information. Roboschool lets you both run and train multiple agents in the same environment. We start with RoboschoolPong, with more environments to follow. With multiplayer training, you can train the same agent playing for both parties (so it plays with itself), you can train two different agents using the same algorithm, or you can even set two different algorithms against each other. The multi-agent setting presents some interesting challenges. If you train both players simultaneously, you’ll likely see a learning curve like the following one, obtained from a policy gradient method: Here’s what’s happening: Agent1 (green) learns it can sometimes hit a ball at the top, so it moves to the top. Agent2 (purple) discovers that its adversary is at the top, so it sends the ball to the bottom and overfits to other agent being away. Agent1 eventually discovers it can defend itself by moving to the bottom, but now always stay at the bottom, because Agent2 always sends ball to the bottom. That way, the policies oscillated, and neither agent learned anything useful after hours of training. As in generative adversarial networks, learning in an adversarial setting is tricky, but we think it’s an interesting research problem because this interplay can lead to sophisticated strategies even in simple environments, and it can provide a natural curriculum. There’s been a lot of work by the community to create environments for OpenAI Gym , some of which are based on open-source physics simulators. In one recent project, researchers created a fork of OpenAI Gym that replaced MuJoCo by the open-source physics simulator DART . They showed that policies can even be transferred between the two physics simulators, MuJoCo and DART.", "date": "2017-05-15"},
{"website": "Open-AI", "title": "learning-concepts-with-energy-functions", "author": ["Igor Mordatch"], "link": "https://openai.com/blog/learning-concepts-with-energy-functions/", "abstract": "We've developed an energy-based model that can quickly learn to identify and generate instances of concepts, such as near, above, between, closest, and furthest, expressed as sets of 2d points. Our model learns these concepts after only five demonstrations.  We also show cross-domain transfer: we use concepts learned in a 2d particle environment to solve tasks on a 3-dimensional physics-based robot. Many hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts , which act as basic building blocks of understanding and reasoning. Our technique enables agents to learn and extract concepts from tasks, then use these concepts to solve other tasks in various domains. For example, our model can use concepts learned in a two-dimensional particle environment to let it carry out the same task on a three-dimensional physics-based robotic environment— without retraining in the new environment. This work uses energy functions to let our agents learn to classify and generate simple concepts, which they can use to solve tasks like navigating between two points in dissimilar environments. Examples of concepts include visual (\"red\" or \"square\"), spatial (\"inside\", \"on top of\"), temporal (\"slow\", \"after\"), social (\"aggressive\", \"helpful\") among others. These concepts, once learned, act as basic building blocks of agent’s understanding and reasoning, as shown in other research from DeepMind and Vicarious . Energy functions work by encoding a preference over states of the world, which allows an agent with different available actions (changing torque vs directly changing position) to learn a policy that works in different contexts—this roughly translates to the development of a conceptual understanding of simple things. To create the energy function, we mathematically represent concepts as energy models . The idea of energy models is rooted in physics, with the intuition that observed events and states represent low-energy configurations. We define an energy function E(x, a, w) for each concept in terms of: The state of the world the model observes (x) An attention mask (a) over entities in that state. A continuous-valued vector (w), used as conditioning, that specifies the concept for which energy is being calculated States of the world are composed of sets of entities and their properties and positions (like the dots below, which have both positional and colored properties). Attention masks, used for “identification”, represent a model’s focus on some set of entities. The energy model outputs a single positive number indicating whether the concept is satisfied (when energy is zero) or not (when energy is high). A concept is satisfied when an attention mask is focused on a set of entities that represent a concept, which requires both that the entities are in the correct positions (modification of x, or generation) and that the right entities are being focused on (modification of a, or identification). We construct the energy function as a neural network based on the relational network architecture , which allows it to take an arbitrary number of entities as input. The parameters of this energy function are what is being optimized by our training procedure; other functions are derived implicitly from the energy function. This approach lets us use energy functions to learn a single network that can perform both generation and recognition . This allows us to cross-employ concepts learned from generation to identification, and vice versa. (Note: This effect is already observed in animals via mirror neurons .) Our training data is composed of trajectories of (attention mask, state), which we generate ahead of time for the specific concepts we’d like our model to learn. We train our model by giving it a set of demonstrations (typically 5) for a given concept set, and then give it a new environment (X0) and ask it to predict the next state (X1) and next attention mask (a). We optimize the energy function such that the next state and next attention mask found in the training data are assigned low energy values. Similar to generative models like variational autoencoders , the model is incentivized to learn values that usefully compress aspects of the task. We trained our model using a variety of concepts involving, visual, spatial, proximal, and temporal relations, and quantification in a two-dimensional particle environment. We evaluated our approach across a suite of tasks designed to see how well our single system could learn to identify and generate things united by the same concept; our system can learn to classify and generate specific sets of spatial relationships, or can navigate entities through a scene in a specific way, or can develop good judgements for concepts like quantity (one, two, three, or more than three) or proximity. Models perform better when they can share experience between learning to generate concepts (by moving entities within the state vector x) and identify them (by changing the attention mask over a fixed state vector): when we evaluated models trained on both of these operations, they performed better on each single operation than models trained only on that single operation alone. We also discovered indications of transfer learning —an energy function trained only on a recognition context performs well on generation, even without being explicitly trained to do so. In the future we’re excited to explore a wider variety of concepts learned in richer, three-dimensional environments, integrate concepts with the decision-making policies of our agents (we have so far only looked at concepts as things learned from passive experience), and explore connections between concepts and language understanding. If you are interested in this line of research, consider working at OpenAI ! Thanks to those who contributed to this paper and blog post: Blog post: Prafulla Dhariwal, Alex Nichol, Alec Radford, Yura Burda, Jack Clark, Greg Brockman, Ilya Sutskever, Ashley Pilipiszyn", "date": "2018-11-07"},
{"website": "Open-AI", "title": "reinforcement-learning-with-prediction-based-rewards", "author": ["Yura Burda", "Harri Edwards"], "link": "https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/", "abstract": "We’ve developed Random Network Distillation (RND) , a prediction-based method for encouraging reinforcement learning agents to explore their environments through curiosity, which for the first time [1] exceeds average human performance on Montezuma’s Revenge . RND achieves state-of-the-art performance, periodically finds all 24 rooms and solves the first level without using demonstrations or having access to the underlying state of the game. RND incentivizes visiting unfamiliar states by measuring how hard it is to predict the output of a fixed random neural network on visited states. In unfamiliar states it's hard to guess the output, and hence the reward is high. It can be applied to any reinforcement learning algorithm, is simple to implement and efficient to scale. Below we release a reference implementation of RND that can reproduce the results from our paper. For an agent to achieve a desired goal it must first explore what is possible in its environment and what constitutes progress towards the goal. Many games’ reward signals provide a curriculum such that even simple exploration strategies are sufficient for achieving the game’s goal. In the seminal work introducing DQN , Montezuma’s Revenge was the only game where DQN got 0% of the average human score (4.7K) . Simple exploration strategies are highly unlikely to gather any rewards, or see more than a few of the 24 rooms in the level. Since then advances in Montezuma’s Revenge have been seen by many as synonymous with advances in exploration. Significant progress was made in 2016 by combining DQN with a count-based exploration bonus, resulting in an agent that explored 15 rooms, achieved a high score of 6.6K and an average reward of around 3.7K. Since then, significant improvement in the score achieved by an RL agent has come only from exploiting access to demonstrations from human experts , or access to the underlying state of the emulator . We ran a large scale RND experiment with 1024 rollout workers resulting in a mean return of 10K over 9 runs and a best mean return of 14.5K. Each run discovered between 20 and 22 rooms. In addition one of our smaller scale but longer running experiments yielded one run (out of 10) that achieved a best return of 17.5K corresponding to passing the first level and finding all 24 rooms . The graph below compares these two experiments showing the mean return as a function of parameter updates. The visualization below shows the progress of the smaller scale experiment in discovering the rooms. Curiosity drives the agent to discover new rooms and find ways of increasing the in-game score, and this extrinsic reward drives it to revisit those rooms later in the training. Prior to developing RND, we, together with collaborators from UC Berkeley, investigated learning without any environment-specific rewards. Curiosity gives us an easier way to teach agents to interact with any environment, rather than via an extensively engineered task-specific reward function that we hope corresponds to solving a task. Projects like ALE , Universe , Malmo , Gym , Gym Retro , Unity , DeepMind Lab , CommAI make a large number of simulated environments available for an agent to interact with through a standardized interface. An agent using a generic reward function not specific to the particulars of an environment can acquire a basic level of competency in a wide range of environments, resulting in the agent’s ability to determine what useful behaviors are even in the absence of carefully engineered rewards. In standard reinforcement learning set-ups, at every discrete time-step the agent sends an action to the environment, and the environment responds by emitting the next observation, transition reward and an indicator of episode end. In our previous paper we require the environment to output only the next observation. There, the agent learns a next-state predictor model from its experience, and uses the error of the prediction as an intrinsic reward. As a result it is attracted to the unpredictable. For example, it will find a change in a game score to be rewarding only if the score is displayed on the screen and the change is hard to predict. The agent will typically find interactions with new objects rewarding, as the outcomes of such interactions are usually harder to predict than other aspects of the environment. Similar to prior work , we tried to avoid modeling all aspects of the environment, whether they are relevant or not, by choosing to model features of the observation. Surprisingly, we found that even random features worked well. We tested our agent across 50+ different environments and observed a range of competence levels from seemingly random actions to deliberately interacting with the environment. To our surprise, in some environments the agent achieved the game’s objective even though the game’s objective was not communicated to it through an extrinsic reward. Breakout – The agent experiences spikes of intrinsic reward when it sees a new configuration of bricks early on in training and when it passes the level for the first time after training for several hours. Pong – We trained the agent to control both paddles at the same time and it learned to keep the ball in play resulting in long rallies. Even when trained against the in-game AI, the agent tried to prolong the game rather than win. Bowling – The agent learned to play the game better than agents trained to maximize the (clipped) extrinsic reward directly. We think this is because the agent gets attracted to the difficult-to-predict flashing of the scoreboard occurring after the strikes. Mario – The intrinsic reward is particularly well-aligned with the game’s objective of advancing through the levels. The agent is rewarded for finding new areas because the details of a newly found area are impossible to predict. As a result the agent discovers 11 levels, finds secret rooms, and even defeats bosses. Like a gambler at a slot machine attracted to chance outcomes, the agent sometimes gets trapped by its curiosity as the result of the noisy-TV problem. The agent finds a source of randomness in the environment and keeps observing it, always experiencing a high intrinsic reward for such transitions. Watching a TV playing static noise is an example of such a trap. We demonstrate this literally by placing the agent in a Unity maze environment with a TV playing random channels. While the noisy-TV problem is a concern in theory, for largely deterministic environments like Montezuma’s Revenge, we anticipated that curiosity would drive the agent to discover rooms and interact with objects. We tried several variants of next-state prediction based curiosity combining the exploration bonus with the score from the game. In these experiments the agent controls the environment through a noisy controller that repeats the last action instead of the current one with some probability. This setup with sticky actions was suggested as a best practice for training agents on fully deterministic games like Atari to prevent memorization. Sticky actions make the transition from room to room unpredictable. Since next-state prediction is inherently susceptible to the noisy-TV problem, we identified the following relevant sources of prediction errors: Factor 1 : Prediction error is high where the predictor fails to generalize from previously seen examples. Novel experience then corresponds to high prediction error. Factor 2 : Prediction error is high because the prediction target is stochastic. Factor 3 : Prediction error is high because information necessary for the prediction is missing, or the model class of predictors is too limited to fit the complexity of the target function. We determined Factor 1 is a useful source of error since it quantifies the novelty of experience, whereas Factors 2 and 3 cause the noisy-TV problem. To avoid Factors 2 and 3, we developed RND, a new exploration bonus that is based on predicting the output of a fixed and randomly initialized neural network on the next state, given the next state itself . The intuition is that predictive models have low error in states similar to the ones they have been trained on. In particular the agent's predictions of the output of a randomly initialized neural network will be less accurate in novel states than in states the agent visited frequently. The advantage of using a synthetic prediction problem is that we can have it be deterministic (bypassing Factor 2) and inside the class of functions the predictor can represent (bypassing Factor 3) by choosing the predictor to be of the same architecture as the target network. These choices make RND immune to the noisy-TV problem. We combine the exploration bonus with the extrinsic rewards through a variant of Proximal Policy Optimization ( PPO ) that uses two value heads for the two reward streams . This allows us to use different discount rates for the different rewards, and combine episodic and non-episodic returns. With this additional ﬂexibility, our best agent often ﬁnds 22 out of the 24 rooms on the ﬁrst level in Montezuma’s Revenge, and occasionally passes the ﬁrst level after finding the remaining two rooms . The same method gets state-of-the-art performance on Venture and Gravitar. The visualization of the RND bonus below shows a graph of the intrinsic reward over the course of an episode of Montezuma’s Revenge where the agent finds the torch for the first time. Big-picture considerations like susceptibility to the noisy-TV problem are important for the choice of a good exploration algorithm. However, we found that getting seemingly-small details right in our simple algorithm made the difference between an agent that never leaves the first room and an agent that can pass the first level. To add stability to the training, we avoided saturation of the features and brought the intrinsic rewards to a predictable range. We also noticed significant improvements in performance of RND every time we discovered and fixed a bug (our favorite one involved accidentally zeroing an array which resulted in extrinsic returns being treated as non-episodic; we realized this was the case only after being puzzled by the extrinsic value function looking suspiciously periodic). Getting such details right was a significant part of achieving high performance even with algorithms conceptually similar to prior work. This is one reason to prefer simpler algorithms where possible. We suggest the following paths forward for future research: Analyze the benefits of different exploration methods and find novel ways of combining them. Train a curious agent on many different environments without reward and investigate the transfer to target environments with rewards. Investigate global exploration that involves coordinated decisions over long time horizons. If you are interested in working on overcoming these challenges then apply to work with us! Thanks to those who contributed to these papers and this blog post: Large-Scale Study of Curiosity-Driven Learning : Yuri Burda*, Harrison Edwards*, Deepak Pathak*, Amos Storkey, Trevor Darrell, Alexei A. Efros Exploration by Random Network Distillation : Yuri Burda*, Harrison Edwards*, Amos Storkey, Oleg Klimov Equal contributions: Blog post: Karl Cobbe, Alex Nichol, Joshua Achiam, Phillip Isola, Alex Ray, Jonas Schneider, Jack Clark, Greg Brockman, Ilya Sutskever, Ben Barry, Amos Storkey, Alexei Efros, Deepak Pathak, Trevor Darrell, Andrew Brock, Antreas Antoniou, Stanislaw Jastrzebski, Ashley Pilipiszyn, Justin Jay Wang There is an anonymous ICLR submission concurrent with our own work which exceeds human performance, though not to the same extent. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2018-10-31"},
{"website": "Open-AI", "title": "reptile", "author": ["Alex Nichol", "John Schulman"], "link": "https://openai.com/blog/reptile/", "abstract": "We've developed a simple meta-learning algorithm called Reptile which works by repeatedly sampling a task, performing stochastic gradient descent on it, and updating the initial parameters towards the final parameters learned on that task. Reptile is the application of the Shortest Descent algorithm to the meta-learning setting, and is mathematically similar to first-order MAML (which is a version of the well-known MAML algorithm) that only needs black-box access to an optimizer such as SGD or Adam, with similar computational efficiency and performance. Meta-learning is the process of learning how to learn. A meta-learning algorithm takes in a distribution of tasks, where each task is a learning problem, and it produces a quick learner — a learner that can generalize from a small number of examples. One well-studied meta-learning problem is few-shot classification, where each task is a classification problem where the learner only sees 1–5 input-output examples from each class, and then it must classify new inputs. Below, you can try out our interactive demo of 1-shot classification, which uses Reptile. Like MAML, Reptile seeks an initialization for the parameters of a neural network, such that the network can be fine-tuned using a small amount of data from a new task. But while MAML unrolls and differentiates through the computation graph of the gradient descent algorithm, Reptile simply performs stochastic gradient descent (SGD) on each task in a standard way — it does not unroll a computation graph or calculate any second derivatives. This makes Reptile take less computation and memory than MAML. The pseudocode is as follows: As an alternative to the last step, we can treat \\(\\Phi  - W\\) as a gradient and plug it into a more sophisticated optimizer like Adam . It is at first surprising that this method works at all. If \\(k=1\\), this algorithm would correspond to \"joint training\" — performing SGD on the mixture of all tasks. While joint training can learn a useful initialization in some cases, it learns very little when zero-shot learning is not possible (e.g. when the output labels are randomly permuted). Reptile requires \\(k>1\\), where the update depends on the higher-order derivatives of the loss function; as we show in the paper, this behaves very differently from \\(k=1\\) (joint training). To analyze why Reptile works, we approximate the update using a Taylor series . We show that the Reptile update maximizes the inner product between gradients of different minibatches from the same task, corresponding to improved generalization. This finding may have implications outside of the meta-learning setting for explaining the generalization properties of SGD. Our analysis suggests that Reptile and MAML perform a very similar update, including the same two terms with different weights. In our experiments, we show that Reptile and MAML yield similar performance on the Omniglot and Mini-ImageNet benchmarks for few-shot classification. Reptile also converges to the solution faster, since the update has lower variance. Our analysis of Reptile suggests a plethora of different algorithms that we can obtain using different combinations of the SGD gradients. In the figure below, assume that we perform k steps of SGD on each task using different minibatches, yielding  gradients \\(g_1, g_2, \\dots, g_k\\). The figure below shows the learning curves on Omniglot obtained by using each sum as the meta-gradient. \\(g_2\\) corresponds to first-order MAML, an algorithm proposed in the original MAML paper. Including more gradients yields faster learning, due to variance reduction. Note that simply using \\(g_1\\) (which corresponds to \\(k=1\\)) yields no progress as predicted for this task since zero-shot performance cannot be improved. Our implementation of Reptile is available on GitHub . It uses TensorFlow for the computations involved, and includes code for replicating the experiments on Omniglot and Mini-ImageNet. We're also releasing a smaller JavaScript implementation that fine-tunes a model pre-trained with TensorFlow — we used this to create the above demo. Finally, here's a minimal example of few-shot regression, predicting a random sine wave from 10 \\((x, y)\\) pairs. This one uses PyTorch and fits in a gist: Several people have pointed out to us that first-order MAML and Reptile are more closely related than MAML and Reptile. These algorithms take different perspectives on the problem, but end up computing similar updates — and specifically, Reptile's contribution builds on the history of both Shortest Descent and avoiding second derivatives in meta - learning . We've since updated the first paragraph to reflect this.", "date": "2018-03-07"},
{"website": "Open-AI", "title": "evolved-policy-gradients", "author": ["Rein Houthooft", "Richard Chen", "Phillip Isola", "Bradly Stadie", "Filip Wolski", "Jonathan Ho", "Pieter Abbeel"], "link": "https://openai.com/blog/evolved-policy-gradients/", "abstract": "We're releasing an experimental metalearning approach called Evolved Policy Gradients, a method that evolves the loss function of learning agents, which can enable fast training on novel tasks. Agents trained with EPG can succeed at basic tasks at test time that were outside their training regime, like learning to navigate to an object on a different side of the room from where it was placed during training. EPG trains agents to have a prior notion of what constitutes making progress on a novel task. Rather than encoding prior knowledge through a learned policy network, EPG encodes it as a learned loss function . [1] Agents are then able to use this loss function, defined as a temporal-convolutional neural network, to learn quickly on a novel task. We've shown that EPG can generalize to out of distribution test time tasks, exhibiting behavior qualitatively different from other popular metalearning algorithms. In tests, we've also found that EPG can train agents faster than PPO , an off-the-shelf policy gradient method. EPG is related to previous work on evolving reward functions for RL agents, but generalizes this idea to evolving a complete loss function, which means that the loss function has to effectively learn an RL algorithm internally. [2] The intuition behind EPG comes from something we are all familiar with: trying to pick up a new skill and experiencing the alternating frustration and joy involved in that process. Suppose you are just starting out learning to play the violin. Even without instruction, you will immediately have a feel for what to try, and, listening to the sounds you produce, you will have a sense of whether or not you are making progress – that's because you effectively have access to very well shaped internal reward functions , derived from prior experience on other motor tasks, and through the course of biological evolution. In contrast, most reinforcement learning (RL) agents approach each new task without using prior knowledge. Instead they rely entirely on external reward signals to guide their initial behavior. Coming from such a blank slate, it is no surprise that current RL agents take far longer than humans to learn simple skills. EPG takes a step toward agents that are not blank slates but instead know what it means to make progress on a new task, by having experienced making progress on similar tasks in the past. EPG consists of two optimization loops. In the inner loop, an agent learns, from scratch, to solve a particular task sampled from a family of tasks. The family of tasks might be \"move gripper to target location [x, y]\" and one particular task in this family could be \"move gripper to position [50, 100]\". The inner loop uses stochastic gradient descent (SGD) to optimize the agent’s policy against a loss function proposed by the outer loop. The outer loop evaluates the returns achieved after inner-loop learning and adjusts the parameters of the loss function, using Evolution Strategies (ES), to propose a new loss that will lead to higher returns. Having a learned loss offers several advantages compared to current RL methods: using ES to evolve the loss function allows us to optimize the true objective (final trained policy performance) rather than short-term returns, and EPG improves on standard RL algorithms by allowing the loss function to be adaptive to the environment and agent history. There has been a flurry of recent work on metalearning policies , and it's worth asking why learn a loss function as opposed to directly learning a policy? Learning recurrent policies tends to overfit the task at hand, while learning policy initializations has limited expressivity when it comes to exploration. Our motivation is that we expect loss functions to be the kind of object that may generalize very well across substantially different tasks. This is certainly true of hand-engineered loss functions: a well-designed RL loss function, such as that in PPO , can be very generically applicable, finding use in problems ranging from playing Atari games to controlling robots. To test the generalization ability of EPG, we conducted a simple experiment. We evolved the EPG loss to be effective at getting \"ants\" to walk to randomly located targets on the right half of an arena. Then, we froze the loss, and gave the ants a new target, this time on the left half of the arena. Surprisingly, the ants learned to walk to the left! Here is how their learning curves looked (red lines on graphs): This result is exciting to us because it demonstrates generalization to a task outside the training distribution . This kind of generalization can be quite hard to achieve. We compared EPG to an alternative metalearning algorithm, called RL 2 , which tries to directly learn a policy that can adapt to novel tasks. In our experiment, RL 2 was indeed successful at getting agents to walk to targets on the right half of the screen. However, when given a test time target on the left half of the screen, it qualitatively failed, and just kept walking to the right. In a sense, it \"overfit\" to the set of tasks on which it was trained (i.e. walking to the right). As do all metalearning approaches, our method still has many limitations. Right now, we can train an EPG loss to be effective for one small family of tasks at a time, e.g., getting an ant to walk left and right. However, the EPG loss for this family of tasks is unlikely to be at all effective on a wildly different kind of task, like playing Space Invaders. In contrast, standard RL losses do have this level of generality—the same loss function can be used to learn a huge variety of skills. EPG gains on performance by losing on generality. There is a long road ahead toward metalearning methods that both outperform standard RL methods and have the same level of generality. By \"loss function\", we mean the entire objective function that is optimized, with respect to policy parameters, via gradient descent. This function is more general than just a reward function, as its goal is to guide behavior toward high total returns, rather than just optimizing immediate rewards. ↩︎ Recent work explores related ideas in behavioral cloning and planning contexts. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2018-04-18"},
{"website": "Open-AI", "title": "evolution-strategies", "author": ["Andrej Karpathy", "Tim Salimans", "Jonathan Ho", "Peter Chen", "Ilya Sutskever", "John Schulman", "Greg Brockman", "Szymon Sidor"], "link": "https://openai.com/blog/evolution-strategies/", "abstract": "We've discovered that evolution strategies (ES) , an optimization technique that's been known for decades, rivals the performance of standard reinforcement learning (RL) techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL's inconveniences. In particular, ES is simpler to implement (there is no need for backpropagation ), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer hyperparameters . This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on finite differences along a few random directions at each step. Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the \"AlexNet\" paper showed how to design, scale and train convolutional neural networks (CNNs) to achieve extremely strong results on image recognition tasks, at a time when most researchers thought that CNNs were not a promising approach to computer vision. Similarly, in 2013, the Deep Q-Learning paper showed how to combine Q-Learning with CNNs to successfully solve Atari games, reinvigorating RL as a research field with exciting experimental (rather than theoretical) results. Likewise, our work demonstrates that ES achieves strong performance on RL benchmarks, dispelling the common belief that ES methods are impossible to apply to high dimensional problems. ES is easy to implement and scale. Running on a computing cluster of 80 machines and 1,440 CPU cores, our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes about 10 hours). Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour. In what follows, we'll first briefly describe the conventional RL approach, contrast that with our ES approach, discuss the tradeoffs between ES and RL, and finally highlight some of our experiments. Let's briefly look at how RL works. Suppose we are given some environment (e.g. a game) that we'd like to train an agent on. To describe the behavior of the agent, we define a policy function (the brain of the agent), which computes how the agent should act in any given situation. In practice, the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions. A typical policy function might have about 1,000,000 parameters, so our task comes down to finding the precise setting of these parameters such that the policy plays well (i.e. wins a lot of games). The training process for the policy works as follows. Starting from a random initialization, we let the agent interact with the environment for a while and collect episodes of interaction (e.g. each episode is one game of Pong). We thus obtain a complete recording of what happened: what sequence of states we encountered, what actions we took in each state, and what the reward was at each step. As an example, below is a diagram of three episodes that each took 10 time steps in a hypothetical environment. Each rectangle is a state, and rectangles are colored green if the reward was positive (e.g. we just got the ball past our opponent) and red if the reward was negative (e.g. we missed the ball): This diagram suggests a recipe for how we can improve the policy; whatever we happened to do leading up to the green states was good, and whatever we happened to do in the states leading up to the red areas was bad. We can then use backpropagation to compute a small update on the network's parameters that would make the green actions more likely in those states in the future, and the red actions less likely in those states in the future. We expect that the updated policy works a bit better as a result. We then iterate the process: collect another batch of episodes, do another update, etc. Exploration by injecting noise in the actions. The policies we usually use in RL are stochastic, in that they only compute probabilities of taking any action. This way, during the course of training, the agent may find itself in a particular state many times, and at different times it will take different actions due to the sampling. This provides the signal needed for learning; some of those actions will lead to good outcomes, and get encouraged, and some of them will not work out, and get discouraged. We therefore say that we introduce exploration into the learning process by injecting noise into the agent's actions, which we do by sampling from the action distribution at each time step. This will be in contrast to ES, which we describe next. On \"Evolution\". Before we dive into the ES approach, it is important to note that despite the word \"evolution\", ES has very little to do with biological evolution. Early versions of these techniques may have been inspired by biological evolution and the approach can, on an abstract level, be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations. However, the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black-box stochastic optimization techniques. Black-box optimization. In ES, we forget entirely that there is an agent, an environment, that there are neural networks involved, or that interactions take place over time, etc. The whole setup is that 1,000,000 numbers (which happen to describe the parameters of the policy network) go in, 1 number comes out (the total reward), and we want to find the best setting of the 1,000,000 numbers. Mathematically, we would say that we are optimizing a function f(w) with respect to the input vector w (the parameters / weights of the network), but we make no assumptions about the structure of f , except that we can evaluate it (hence \"black box\"). The ES algorithm. Intuitively, the optimization is a \"guess and check\" process, where we start with some random parameters and then repeatedly 1) tweak the guess a bit randomly, and 2) move our guess slightly towards whatever tweaks worked better. Concretely, at each step we take a parameter vector w and generate a population of, say, 100 slightly different parameter vectors w1 ... w100 by jittering w with gaussian noise. We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while, and add up all the rewards in each case. The updated parameter vector then becomes the weighted sum of the 100 vectors, where each weight is proportional to the total reward (i.e. we want the more successful candidates to have a higher weight). Mathematically, you'll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences, except we only do it along 100 random directions. Yet another way to see it is that we're still doing RL (Policy Gradients, or REINFORCE specifically), where the agent's actions are to emit entire parameter vectors using a gaussian policy. Code sample. To make the core algorithm concrete and to highlight its simplicity, here is a short example of optimizing a quadratic function using ES (or see this longer version with more comments): Injecting noise in the parameters. Notice that the objective is identical to the one that RL optimizes: the expected reward. However, RL injects noise in the action space and uses backpropagation to compute the parameter updates, while ES injects noise directly in the parameter space. Another way to describe this is that RL is a \"guess and check\" on actions, while ES is a \"guess and check\" on parameters. Since we're injecting noise in the parameters, it is possible to use deterministic policies (and we do, in our experiments). It is also possible to add noise in both actions and parameters to potentially combine the two approaches. ES enjoys multiple advantages over RL algorithms (some of them are a little technical): No need for backpropagation . ES only requires the forward pass of the policy and does not require backpropagation (or value function estimation), which makes the code shorter and between 2-3 times faster in practice. On memory-constrained systems, it is also not necessary to keep a record of the episodes for a later update. There is also no need to worry about exploding gradients in RNNs. Lastly, we can explore a much larger function class of policies, including networks that are not differentiable (such as in binary networks), or ones that include complex modules (e.g. pathfinding, or various optimization layers). Highly parallelizable. ES only requires workers to communicate a few scalars between each other, while in RL it is necessary to synchronize entire parameter vectors (which can be millions of numbers). Intuitively, this is because we control the random seeds on each worker, so each worker can locally reconstruct the perturbations of the other workers. Thus, all that we need to communicate between workers is the reward of each perturbation. As a result, we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization. Higher robustness. Several hyperparameters that are difficult to set in RL implementations are side-stepped in ES. For example, RL is not \"scale-free\", so one can achieve very different learning outcomes (including a complete failure) with different settings of the frame-skip hyperparameter in Atari. As we show in our work, ES works about equally well with any frame-skip. Structured exploration. Some RL algorithms (especially policy gradients) initialize with random policies, which often manifests as random jitter on spot for a long time. This effect is mitigated in Q-Learning due to epsilon-greedy policies, where the max operation can cause the agents to perform some consistent action for a while (e.g. holding down a left arrow). This is more likely to do something in a game than if the agent jitters on spot, as is the case with policy gradients. Similar to Q-learning, ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration. Credit assignment over long time scales. By studying both ES and RL gradient estimators mathematically we can see that ES is an attractive choice especially when the number of time steps in an episode is long, where actions have longlasting effects, or if no good value function estimates are available. Conversely, we also found some challenges to applying ES in practice. One core problem is that in order for ES to work, adding noise in parameters must lead to different outcomes to obtain some gradient signal. As we elaborate on in our paper, we found that the use of virtual batchnorm can help alleviate this problem, but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary. As an example of a related difficulty, we found that in Montezuma's Revenge, one is very unlikely to get the key in the first level with a random network, while this is occasionally possible with random actions. We compared the performance of ES and RL on two standard RL benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo task (see examples below) contains a physically-simulated articulated figure, where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward. Below are some example agents trained on three MuJoCo control tasks, where the objective is to move forward: We usually compare the performance of algorithms by looking at their efficiency of learning from data; as a function of how many states we've seen, what is our average reward? Here are the example learning curves that we obtain, in comparison to RL (the TRPO algorithm in this case): Data efficiency comparison . The comparisons above show that ES (orange) can reach a comparable performance to TRPO (blue), although it doesn't quite match or surpass it in all cases. Moreover, by scanning horizontally we can see that ES is less efficient, but no worse than about a factor of 10 (note the x-axis is in log scale). Wall clock comparison . Instead of looking at the raw number of states seen, one can argue that the most important metric to look at is the wall clock time: how long (in number of seconds) does it take to solve a given problem? This quantity ultimately dictates the achievable speed of iteration for a researcher. Since ES requires negligible communication between workers, we were able to solve one of the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 machines in only 10 minutes. As a comparison, in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours. It is also possible that the performance of RL could also improve with more algorithmic and engineering effort, but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements. Below are a few videos of 3D humanoid walkers trained with ES. As we can see, the results have quite a bit of variety, based on which local minimum the optimization ends up converging into. On Atari, ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day. Below are some result snippets on Pong, Seaquest and Beamrider. These videos show the preprocessed frames, which is exactly what the agent sees when it is playing: In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels. ES is an algorithm from the neuroevolution literature, which has a long history in AI and a complete literature review is beyond the scope of this post. However, we encourage an interested reader to look at Wikipedia , Scholarpedia , and Jürgen Schmidhuber's review article (Section 6.6) . The work that most closely informed our approach is Natural Evolution Strategies by Wierstra et al. 2014. Compared to this work and much of the work it has inspired, our focus is specifically on scaling these algorithms to large-scale, distributed settings, finding components that make the algorithms work better with deep neural networks (e.g. virtual batch norm ), and evaluating them on modern RL benchmarks. It is also worth noting that neuroevolution-related approaches have seen some recent resurgence in the machine learning literature, for example with HyperNetworks , \"Large-Scale Evolution of Image Classifiers\" and \"Convolution by Evolution\" . Our work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent-environment benchmarks, while offering significant benefits related to code complexity and ease of scaling to large-scale distributed settings. We also expect that more exciting work can be done by revisiting other ideas from this line of work, such as indirect encoding methods, or evolving the network structure in addition to the parameters. Note on supervised learning . It is also important to note that supervised learning problems (e.g. image classification, speech recognition, or most other tasks in the industry), where one can compute the exact gradient of the loss function with backpropagation, are not directly impacted by these findings. For example, in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1,000 times slower than using backpropagation. It is only in RL settings, where one has to estimate the gradient of the expected reward by sampling, where ES becomes competitive. Code release . Finally, if you'd like to try running ES yourself, we encourage you to dive into the full details by reading our paper or looking at our code on this Github repo .", "date": "2017-03-24"},
{"website": "Open-AI", "title": "preparing-for-malicious-uses-of-ai", "author": ["Jack Clark", "Michael Page", "Dario Amodei"], "link": "https://openai.com/blog/preparing-for-malicious-uses-of-ai/", "abstract": "AI challenges global security because it lowers the cost of conducting many existing attacks, creates new threats and vulnerabilities, and further complicates the attribution of specific attacks. Given the changes to the threat landscape that AI seems to bring, the report makes some high-level recommendations that companies, research organizations, individual practitioners, and governments can take to ensure a safer world: Acknowledge AI's dual-use nature: AI is a technology capable of immensely positive and immensely negative applications. We should take steps as a community to better evaluate research projects for perversion by malicious actors, and engage with policymakers to understand areas of particular sensitivity. As we write in the paper: \"Surveillance tools can be used to catch terrorists or oppress ordinary citizens. Information content filters could be used to bury fake news or manipulate public opinion. Governments and powerful private actors will have access to many of these AI tools and could use them for public good or harm.\" Some potential solutions to these problems include pre-publication risk assessments for certain bits of research, selectively sharing some types of research with a significant safety or security component among a small set of trusted organizations, and exploring how to embed norms into the scientific community that are responsive to dual-use concerns. Learn from cybersecurity: The computer security community has developed various practices that are relevant to AI researchers, which we should consider implementing in our own research. These range from \"red teaming\" by intentionally trying to break or subvert systems, to investing in tech forecasting to spot threats before they arrive, to conventions around the confidential reporting of vulnerabilities discovered in AI systems, and so on. Broaden the discussion: AI is going to alter the global threat landscape, so we should involve a broader cross-section of society in discussions. Parties could include those involved in the civil society, national security experts, businesses, ethicists, the general public, and other researchers. Like our work on concrete problems in AI safety , we've grounded some of the problems motivated by the malicious use of AI in concrete scenarios, such as: persuasive ads generated by AI systems being used to target the administrator of a security systems; cybercriminals using neural networks and \"fuzzing\" techniques to create computer viruses with automatic exploit generation capabilities; malicious actors hacking a cleaning robot so that it delivers an explosives payload to a VIP; and rogue states using omniprescent AI-augmented surveillance systems to pre-emptively arrest people who fit a predictive risk profile. We're excited to start having this discussion with our peers, policymakers, and the general public; we've spent the last two years researching and solidifying our internal policies at OpenAI and are going to begin engaging a wider audience on these issues. We're especially keen to work with more researchers that see themselves contributing to the policy debates around AI as well as making research breakthroughs.", "date": "2018-02-20"},
{"website": "Open-AI", "title": "introducing-openai", "author": ["Greg Brockman", "Ilya Sutskever", "OpenAI"], "link": "https://openai.com/blog/introducing-openai/", "abstract": "OpenAI is a non-profit artificial intelligence research company. Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return. Since our research is free from financial obligations, we can better focus on a positive human impact. We believe AI should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible. The outcome of this venture is uncertain and the work is difficult, but we believe the goal and the structure are right. We hope this is what matters most to the best in the field. Artificial intelligence has always been a surprising field. In the early days, people thought that solving certain tasks (such as chess) would lead us to discover human-level intelligence algorithms. However, the solution to each task turned out to be much less general than people were hoping (such as doing a search over a huge number of moves). The past few years have held another flavor of surprise. An AI technique explored for decades, deep learning, started achieving state-of-the-art results in a wide variety of problem domains. In deep learning, rather than hand-code a new algorithm for each problem, you design architectures that can twist themselves into a wide range of algorithms based on the data you feed them. This approach has yielded outstanding results on pattern recognition problems, such as recognizing objects in images, machine translation, and speech recognition. But we've also started to see what it might be like for computers to be creative , to dream , and to experience the world . AI systems today have impressive but narrow capabilities. It seems that we'll keep whittling away at their constraints, and in the extreme case they will reach human performance on virtually every intellectual task. It's hard to fathom how much human-level AI could benefit society, and it's equally hard to imagine how much it could damage society if built or used incorrectly. Because of AI's surprising history, it's hard to predict when human-level AI might come within reach. When it does, it'll be important to have a leading research institution which can prioritize a good outcome for all over its own self-interest. We're hoping to grow OpenAI into such an institution. As a non-profit, our aim is to build value for everyone rather than shareholders. Researchers will be strongly encouraged to publish their work, whether as papers, blog posts, or code, and our patents (if any) will be shared with the world. We'll freely collaborate with others across many institutions and expect to work with companies to research and deploy new technologies. OpenAI's research director is Ilya Sutskever , one of the world experts in machine learning. Our CTO is Greg Brockman , formerly the CTO of Stripe. The group's other founding members are world-class research engineers and scientists: Trevor Blackwell , Vicki Cheung , Andrej Karpathy , Durk Kingma , John Schulman , Pamela Vagata , and Wojciech Zaremba . Pieter Abbeel, Yoshua Bengio, Alan Kay, Sergey Levine, and Vishal Sikka are advisors to the group. OpenAI's co-chairs are Sam Altman and Elon Musk. Sam, Greg, Elon, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research are donating to support OpenAI. In total, these funders have committed $1 billion, although we expect to only spend a tiny fraction of this in the next few years. You can follow us on Twitter at @openai .", "date": "2015-12-11"},
{"website": "Open-AI", "title": "openai-gym-beta", "author": ["Greg Brockman", "John Schulman"], "link": "https://openai.com/blog/openai-gym-beta/", "abstract": "We're releasing the public beta of OpenAI Gym , a toolkit for developing and comparing reinforcement learning (RL) algorithms. It consists of a growing suite of environments (from simulated robots to Atari games), and a site for comparing and reproducing results. OpenAI Gym is compatible with algorithms written in any framework, such as Tensorflow and Theano . The environments are written in Python, but we'll soon make them easy to use from any language. We originally built OpenAI Gym as a tool to accelerate our own RL research. We hope it will be just as useful for the broader community. If you'd like to dive in right away, you can work through our tutorial . You can also help out while learning by reproducing a result . Reinforcement learning (RL) is the subfield of machine learning concerned with decision making and motor control. It studies how an agent can learn how to achieve goals in a complex, uncertain environment. It's exciting for two reasons: RL is very general, encompassing all problems that involve making a sequence of decisions: for example, controlling a robot's motors so that it's able to run and jump , making business decisions like pricing and inventory management, or playing video games and board games . RL can even be applied to supervised learning problems with sequential or structured outputs. RL algorithms have started to achieve good results in many difficult environments. RL has a long history, but until recent advances in deep learning, it required lots of problem-specific engineering. DeepMind's Atari results , BRETT from Pieter Abbeel's group, and AlphaGo all used deep RL algorithms which did not make too many assumptions about their environment, and thus can be applied in other settings. However, RL research is also slowed down by two factors: The need for better benchmarks. In supervised learning, progress has been driven by large labeled datasets like ImageNet . In RL, the closest equivalent would be a large and diverse collection of environments. However, the existing open-source collections of RL environments don't have enough variety, and they are often difficult to even set up and use. Lack of standardization of environments used in publications. Subtle differences in the problem definition, such as the reward function or the set of actions, can drastically alter a task's difficulty. This issue makes it difficult to reproduce published research and compare results from different papers. OpenAI Gym is an attempt to fix both problems. OpenAI Gym provides a diverse suite of environments that range from easy to difficult and involve many different kinds of data. We're starting out with the following collections: Classic control and toy text : complete small-scale tasks, mostly from the RL literature. They're here to get you started. Algorithmic : perform computations such as adding multi-digit numbers and reversing sequences. One might object that these tasks are easy for a computer. The challenge is to learn these algorithms purely from examples. These tasks have the nice property that it's easy to vary the difficulty by varying the sequence length. Atari : play classic Atari games. We've integrated the Arcade Learning Environment (which has had a big impact on reinforcement learning research) in an easy-to-install form. Board games : play Go on 9x9 and 19x19 boards. Two-player games are fundamentally different than the other settings we've included, because there is an adversary playing against you. In our initial release, there is a fixed opponent provided by Pachi , and we may add other opponents later (patches welcome!). We'll also likely expand OpenAI Gym to have first-class support for multi-player games. 2D and 3D robots : control a robot in simulation. These tasks use the MuJoCo physics engine, which was designed for fast and accurate robot simulation. Included are some environments from a recent benchmark by UC Berkeley researchers (who incidentally will be joining us this summer). MuJoCo is proprietary software, but offers free trial licenses. Over time, we plan to greatly expand this collection of environments. Contributions from the community are more than welcome. Each environment has a version number (such as Hopper-v0 ). If we need to change an environment, we'll bump the version number, defining an entirely new task. This ensures that results on a particular environment are always comparable. We've made it easy to upload results to OpenAI Gym. However, we've opted not to create traditional leaderboards. What matters for research isn't your score (it's possible to overfit or hand-craft solutions to particular tasks), but instead the generality of your technique. We're starting out by maintaining a curated list of contributions that say something interesting about algorithmic capabilities. Long-term, we want this curation to be a community effort rather than something owned by us. We'll necessarily have to figure out the details over time, and we'd would love your help in doing so. We want OpenAI Gym to be a community effort from the beginning. We've starting working with partners to put together resources around OpenAI Gym: NVIDIA : technical Q&A with John. Nervana : implementation of a DQN OpenAI Gym agent . Amazon Web Services (AWS) : $250 credit vouchers for select OpenAI Gym users. If you have an evaluation demonstrating the promise of your algorithm and are resource-constrained from scaling it up, ping us for a voucher. (While supplies last!) During the public beta, we're looking for feedback on how to make this into an even better tool for research. If you'd like to help, you can try your hand at improving the state-of-the-art on each environment, reproducing other people's results, or even implementing your own environments. Also please join us in the community chat !", "date": "2016-04-27"},
{"website": "Open-AI", "title": "universe", "author": ["OpenAI"], "link": "https://openai.com/blog/universe/", "abstract": "We're releasing Universe , a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications. Universe allows an AI agent to use a computer like a human does: by looking at screen pixels and operating a virtual keyboard and mouse. We must train AI systems on the full range of tasks we expect them to solve, and Universe lets us train a single agent on any task a human can complete with a computer. In April, we launched Gym , a toolkit for developing and comparing reinforcement learning (RL) algorithms. With Universe, any program can be turned into a Gym environment. Universe works by automatically launching the program behind a VNC remote desktop — it doesn't need special access to program internals, source code, or bot APIs. Today's release consists of a thousand environments including Flash games , browser tasks , and games like slither.io and GTA V . Hundreds of these are ready for reinforcement learning, and almost all can be freely run with the universe Python library as follows: Our goal is to develop a single AI agent that can flexibly apply its past experience on Universe environments to quickly master unfamiliar, difficult environments, which would be a major step towards general intelligence. There are many ways to help : giving us permission on your games, training agents across Universe tasks, (soon) integrating new games, or (soon) playing the games. With support from EA, Microsoft Studios, Valve, Wolfram, and many others, we've already secured permission for Universe AI agents to freely access games and applications such as Portal , Fable Anniversary , World of Goo , RimWorld , Slime Rancher , Shovel Knight , SpaceChem , Wing Commander III , Command & Conquer: Red Alert 2 , Syndicate , Magic Carpet , Mirror's Edge , Sid Meier's Alpha Centauri , and Wolfram Mathematica . We look forward to integrating these and many more . The area of artificial intelligence has seen rapid progress over the last few years. Computers can now see , hear , and translate languages with unprecedented accuracies. They are also learning to generate images , sound , and text . A reinforcement learning system, AlphaGo , defeated the world champion at Go. However, despite all of these advances, the systems we're building still fall into the category of “Narrow AI” — they can achieve super-human performance in a specific domain, but lack the ability to do anything sensible outside of it. For instance, AlphaGo can easily defeat you at Go, but you can't explain the rules of a different board game to it and expect it to play with you. Systems with general problem solving ability — something akin to human common sense, allowing an agent to rapidly solve a new hard task — remain out of reach. One apparent challenge is that our agents don't carry their experience along with them to new tasks. In a standard training regime, we initialize agents from scratch and let them twitch randomly through tens of millions of trials as they learn to repeat actions that happen to lead to rewarding outcomes. If we are to make progress towards generally intelligent agents, we must allow them to experience a wide repertoire of tasks so they can develop world knowledge and problem solving strategies that can be efficiently reused in a new task. Universe exposes a wide range of environments through a common interface: the agent operates a remote desktop by observing pixels of a screen and producing keyboard and mouse commands. The environment exposes a VNC server and the universe library turns the agent into a VNC client. Our design goal for universe was to support a single Python process driving 20 environments in parallel at 60 frames per second. Each screen buffer is 1024x768, so naively reading each frame from an external process would take 3GB/s of memory bandwidth. We wrote a batch-oriented VNC client in Go, which is loaded as a shared library in Python and incrementally updates a pair of buffers for each environment. After experimenting with many combinations of VNC servers, encodings, and undocumented protocol options, we now routinely drive dozens of environments at 60 frames per second with 100ms latency — almost all due to server-side encoding. Here are some important properties of our current implementation: General. An agent can use this interface (which was originally designed for humans) to interact with any existing computer program without requiring an emulator or access to the program's internals. For instance, it can play any computer game, interact with a terminal, browse the web, design buildings in CAD software, operate a photo editing program, or edit a spreadsheet. Familiar to humans. Since people are already well versed with the interface of pixels/keyboard/mouse, humans can easily operate any of our environments. We can use human performance as a meaningful baseline, and record human demonstrations by simply saving VNC traffic. We've found demonstrations to be extremely useful in initializing agents with sensible policies with behavioral cloning (i.e. use supervised learning to mimic what the human does), before switching to RL to optimize for the given reward function. VNC as a standard. Many implementations of VNC are available online and some are packaged by default into the most common operating systems, including OSX. There are even VNC implementations in JavaScript , which allow humans to provide demonstrations without installing any new software — important for services like Amazon Mechanical Turk. Easy to debug. We can observe our agent while it is training or being evaluated — we just attach a VNC client to the environment's (shared) VNC desktop. We can also save the VNC traffic for future analysis. We were all quite surprised that we could make VNC work so well. As we scale to larger games, there's a decent chance we'll start using additional backend technologies. But preliminary signs indicate we can push the existing implementation far: with the right settings, our client can coax GTA V to run at 20 frames per second over the public internet. We have already integrated a large number of environments into Universe, and view these as just the start. Each environment is packaged as a Docker image and hosts two servers that communicate with the outside world: the VNC server which sends pixels and receives keyboard/mouse commands, and a WebSocket server which sends the reward signal for reinforcement learning tasks (as well as any auxiliary information such as text, or diagnostics) and accepts control messages (such as the specific environment ID to run). Universe includes the Atari 2600 games from the Arcade Learning Environment. These environments now run asynchronously inside the quay.io/openai/universe.gym-core Docker image and allow the agent to connect over the network, which means the agent must handle lag and low frame rates. Running over a local network in the cloud, we usually see 60 frames per second, observation lags of 20ms, and action lags of 10ms; over the public internet this drops to 20 frames per second, 80ms observation lags, and 30ms action lags. We turned to Flash games as a starting point for scaling Universe — they are pervasive on the Internet, generally feature richer graphics than Atari, but are still individually simple. We've sifted through over 30,000 so far, and estimate there's an order of magnitude more. Our initial Universe release includes 1,000 Flash games (100 with reward functions), which we distribute in the quay.io/openai/universe.flashgames Docker image with consent from the rightsholders. This image starts a TigerVNC server and boots a Python control server, which uses Selenium to open a Chrome browser to an in-container page with the desired game, and automatically clicks through any menus needed to start the game. Extracting rewards. While environments without reward functions can be used for unsupervised learning or to generate human demonstrations, RL needs a reward function. Unlike with the Atari games, we can't simply read out success criteria from the process memory, as there is too much variation in how each game stores this information. Fortunately, many games have an on-screen score which we can use as a reward function, as long as we can parse it. While off-the-shelf OCR such as Tesseract performs great on standard fonts with clean backgrounds, it struggles with the diverse fonts, moving backgrounds, flashy animations, or occluding objects common in many games. We developed a convolutional neural network-based OCR model that runs inside the Docker container's Python controller, parses the score (from a screen buffer maintained via a VNC self-loop), and communicates it over the WebSocket channel to the agent. Humanity has collectively built the Internet into an immense treasure trove of information, designed for visual consumption by humans. Universe includes browser-based environments which require AI agents to read, navigate, and use the web just like people — using pixels, keyboard, and mouse. Today, our agents are mostly learning to interact with common user interface elements like buttons, lists and sliders, but in the future they could complete complex tasks, such as looking up things they don't know on the internet, managing your email or calendar, completing Khan Academy lessons, or working on Amazon Mechanical Turk and CrowdFlower tasks. Mini World of Bits. We first set out to create a new benchmark that captures the salient challenges of browser interactions in a simple setting. We call this benchmark Mini World of Bits . We think of it as an analogue to MNIST , and believe that mastering these environments provides valuable signal towards models and training techniques that will perform well on full websites and more complex tasks. Our initial Mini World of Bits benchmark consists of 80 environments that range from simple (e.g. click a specific button) to difficult (e.g. reply to a contact in a simulated email client). Real-world browser tasks. We've begun work on more realistic browser tasks. The agent takes an instruction, and performs a sequence of actions on a website. One such environment hands the agent details of a desired flight booking, and then requires it to manipulate a user interface to search for the flight. (We use cached recordings of these sites to avoid spamming them, or booking lots of real flights.) This infrastructure is general-purpose: we can integrate any game, website, or application which can run in a Docker container (most convenient) or a Windows virtual machine (less convenient). We'd like the community's help to continue extending the breadth of Universe environments, including completing the integrations of our partners' games, Android apps (emulators can run inside of Docker), fold.it , Unity games, HTML5 games, online educational games, and really anything else people think of. Microsoft's Project Malmo team will be integrating with Universe, and we look forward to supporting other AI frameworks as well. Despite the huge variety, running Universe environments requires minimal setup. You'll need only to install Docker and universe : We package each collection of similar environments into a “runtime”, which is a server exposing two ports: 5900 (used for the VNC protocol to exchange pixels/keyboard/mouse) and 15900 (used for a WebSocket control protocol ). For example, the quay.io/openai/universe.flashgames Docker image is a runtime that can serve many different Flash game environments. Starting a runtime. You can boot your first runtime from the console as follows: This will download and run the Flash games Docker container. You can view and control the remote desktop by connecting your own VNC viewer to port 5900, such as via TurboVNC or the browser-based VNC client served via the webserver on port 15900. The default password is openai . OSX also has a native VNC viewer which can be accessed by running open vnc://localhost:5900 in Terminal. (Unfortunately, the OSX viewer doesn't implement Tight encoding, which is the best option for bigger games.) Writing your own agent. You can write your own agent quite easily, using your favorite framework such as TensorFlow or Theano . (We've provided a starter TensorFlow agent .) At each time step, the agent's observation includes a NumPy pixel array, and the agent must emit a list of VNC events (mouse/keyboard actions). For example, the following agent will activate Dusk Drive and press forward constantly: You can keep your own VNC connection open, and watch the agent play, or even use the keyboard and mouse alongside the agent in a human/agent co-op mode. Environment management . Because environments run as server processes, they can run on remote machines, possibly within a cluster or even over the public internet. We've documented a few ways to manage remote runtimes. At OpenAI, we use an “allocator” HTTP service, which provisions runtimes across a Kubernetes cluster on demand, and which we can use to connect a single agent process to hundreds of simultaneous environments. Universe agents must deal with real-world griminess that traditional RL agents are shielded from: agents must run in real-time and account for fluctuating action and observation lag. While the full complexity of Universe is designed to be out of reach of current techniques, we also have ensured it's possible to make progress today. Universe Pong. Our first goal was solving gym-core.PongDeterministic-v3 . Pong is one of the easiest Atari games, but it had the potential to be intractable as a Universe task, since the agent has to learn to perform very precise maneuvers at 4x realtime (as the environment uses a standard frameskip of 4). We used this environment to validate that Universe's variable latencies still allowed for learning precise and rapid reactions. Today's release includes universe-starter-agent , which takes an hour to train to a Pong score of +17 out of 21. Humans playing the same version of Pong were only able reach a score of -11 on a scale between -21 and 21, due to the game's high speed. Additional experiments. We applied RL to several racing Flash games, which worked after applying some standard tricks such as reward normalization. Some browser tasks where we tried RL had difficult exploration problems, but were solvable with behavioral cloning from human demonstration data. Some of our successful agents are shown below. While solving Universe will require an agent far outside the reach of current techniques, these videos show that many interesting Universe environments can be fruitfully approached with today's algorithms. Remote training. We've also experimented with slither.io agents on our physical infrastructure (which has access to Titan X GPUs) and environments in the cloud. Generally, the agent will control 32 simultaneous environments at 5 frames per second — observations are available much more frequently, but lower framerates help today's RL algorithms, which struggle with dependencies over many timesteps. Our agent's “reaction time” averages around 150ms over the public internet: 110ms for an observation to arrive, 10ms to compute the action, and 30ms for the action to take effect. (For comparison, human reaction time averages around 250ms.) Reaction times drop to 80ms over a local network, and 40ms within a single machine. Research progress requires meaningful performance measurement. In upcoming weeks, we'll release a transfer learning benchmark, allowing researchers to determine if they are making progress on general problem solving ability. Universe draws inspiration from the history of the ImageNet dataset in the Computer Vision community. Fei-Fei Li and her collaborators deliberately designed the ImageNet benchmark to be nearly impossible, but error rates have dropped rapidly from 28% in 2010 to 3% in 2016, which reaches (or in some cases even surpasses) human-level performance. If the AI community does the same with Universe, then we will have made real progress towards systems with broad, general intelligence. Universe will only succeed with the community's help. There are many ways to contribute (and one particularly great way is to join us ): If your program would yield good training tasks for an AI then we'd love your permission to package it in Universe. Good candidates have an on-screen number (such as a game score) which can be parsed as a reward, or well-defined objectives, either natively or definable by the user. AI advances require the entire community to collaborate, and we welcome the community's help in training agents across these tasks. We've released a starter agent which should be a helpful starting point for building your own agents. In upcoming weeks, we'll release the sub-benchmarks we think are the right places to start. We have many more environments waiting to be integrated than we can handle on our own. In upcoming weeks, we'll release our environment integration tools, so anyone can contribute new environment integrations. In the meanwhile, we'll be running a beta for environment integrators. We're compiling a large dataset of human demonstrations on Universe environments, which will be released publicly. If you'd like to play games for the good of science, please sign up for our beta . Acquisition & partnerships : Erin Pettigrew, Jack Clark, Jeff Arnold Core infrastructure : Greg Brockman, Catherine Olsson, Alex Ray Demonstrations : Tom Brown, Jeremy Schlatter, Marie La, Catherine Olsson Distributed training infrastructure : Vicki Cheung, Greg Brockman, Jonas Schneider Documentation & communications : Jack Clark, Andrej Karpathy, Catherine Olsson Environment integrations : Alec Radford, Jonathan Gray, Tom Brown, Greg Brockman, Alex Ray, Catherine Olsson, Trevor Blackwell, Tambet Matiisen, Craig Quiter Initial agent results : Rafal Jozefowicz, Dario Amodei, Ilya Sutskever, Jonathan Ho, Trevor Blackwell, Avital Oliver, Yaroslav Bulatov Remote environment management : Vicki Cheung, Greg Brockman, Catherine Olsson, Jie Tang RL baselines : Dario Amodei, Harri Edwards Website : Ludwig Petterson, Jie Tang, Tom Brown, Alec Radford, Jonas Schneider, Szymon Sidor World of Bits : Andrej Karpathy, Tianlin (Tim) Shi, Linxi (Jim) Fan, Jonathan Hernandez, Percy Liang", "date": "2016-12-05"},
{"website": "Open-AI", "title": "openai-baselines-dqn", "author": ["Szymon Sidor", "John Schulman"], "link": "https://openai.com/blog/openai-baselines-dqn/", "abstract": "We're open-sourcing OpenAI Baselines , our internal effort to reproduce reinforcement learning algorithms with performance on par with published results. We'll release the algorithms over upcoming months; today's release includes DQN and three of its variants. Reinforcement learning results are tricky to reproduce: performance is very noisy, algorithms have many moving parts which allow for subtle bugs, and many papers don't report all the required tricks. By releasing known-good implementations (and best practices for creating them), we’d like to ensure that apparent RL advances never are due to comparison with buggy or untuned versions of existing algorithms. This post contains some best practices we use for correct RL algorithm implementations, as well as the details of our first release: DQN and three of its variants, algorithms developed by DeepMind. Compare to a random baseline: in the video below, an agent is taking random actions in the game H.E.R.O. If you saw this behavior in early stages of training, it'd be really easy to trick yourself into believing that the agent is learning. So you should always verify your agent outperforms a random one. Be wary of non-breaking bugs : when we looked through a sample of ten popular reinforcement learning algorithm reimplementations we noticed that six had subtle bugs found by a community member and confirmed by the author. These ranged from mild bugs that ignored gradients on some examples or implemented causal convolutions incorrectly to serious ones that reported scores higher than the true result . See the world as your agent does: like most deep learning approaches, for DQN we tend to convert images of our environments to grayscale to reduce the computation required during training. This can create its own bugs: when we ran our DQN algorithm on Seaquest we noticed that our implementation was performing poorly. When we inspected the environment we discovered this was because our post-processed images contained no fish, as this picture shows. When transforming the screen images into greyscale we had incorrectly calibrated our coefficients for the green color values, which led to the fish disappearing. After we noticed the bug we tweaked the color values and our algorithm was able to see the fish again. To debug issues like this in the future, Gym now contains a play function, which lets a researcher easily see the same observations as the AI agent would. Fix bugs, then hyperparameters : After debugging, we started to calibrate our hyperparameters. We ultimately found that setting the annealing schedule for epsilon, a hyperparameter which controlled the exploration rate, had a huge impact on performance. Our final implementation decreases epsilon to 0.1 over the first million steps and then down to 0.01 over the next 24 million steps. If our implementation contained bugs, then it's likely we would come up with different hyperparameter settings to try to deal with faults we hadn't yet diagnosed. Double check your interpretations of papers : In the DQN Nature paper the authors write: \"We also found it helpful to clip the error term from the update [...] to be between -1 and 1.\". There are two ways to interpret this statement — clip the objective, or clip the multiplicative term when computing gradient. The former seems more natural, but it causes the gradient to be zero on transitions with high error, which leads to suboptimal performance, as found in one DQN implementation . The latter is correct and has a simple mathematical interpretation — Huber Loss . You can spot bugs like these by checking that the gradients appear as you expect — this can be easily done  within TensorFlow by using compute_gradients . The majority of bugs in this post were spotted by going over the code multiple times and thinking through what could go wrong with each line. Each bug seems obvious in hindsight, but even experienced researchers tend to underestimate how many passes over the code it can take to find all the bugs in an implementation. We use Python 3 and TensorFlow. This release includes: DQN : A reinforcement learning algorithm that combines Q-Learning with deep neural networks to let RL work for complex, high-dimensional environments, like video games, or robotics. Double Q Learning : Corrects the stock DQN algorithm’s tendency to sometimes overestimate the values tied to specific actions. Prioritized Replay : Extends DQN’s experience replay function by learning to replay memories where the real reward significantly diverges from the expected reward, letting the agent adjust itself in response to developing incorrect assumptions. Dueling DQN : Splits the neural network into two — one learns to provide an estimate of the value at every timestep, and the other calculates potential advantages of each action, and the two are combined for a single action-advantage Q function. To get started, run the following: We've also provided trained agents, which you can obtain by running: We've included an iPython notebook showing the performance of our DQN implementations on Atari games. You can compare the performance of our various algorithms such as Dueling Double Q learning with Prioritized Replay (yellow), Double Q learning with Prioritized Replay (blue), Dueling Double Q learning (green) and Double Q learning (red). AI is an empirical science, where the ability to do more experiments directly correlates with progress. With Baselines, researchers can spend less time implementing pre-existing algorithms and more time designing new ones. If you'd like to help us refine, extend, and develop AI algorithms then join us at OpenAI.", "date": "2017-05-24"},
{"website": "Open-AI", "title": "procgen-minerl-competitions", "author": ["OpenAI"], "link": "https://openai.com/blog/procgen-minerl-competitions/", "abstract": "We’re excited to announce that OpenAI is co-organizing two NeurIPS 2020 competitions with AIcrowd , Carnegie Mellon University , and DeepMind , using Procgen Benchmark and MineRL . We rely heavily on these environments internally for research on reinforcement learning, and we look forward to seeing the progress the community makes in these challenging competitions. Sign up for Procgen The Procgen Competition focuses on improving sample efficiency and generalization in reinforcement learning. Participants will attempt to maximize agents' performance using a fixed number of environment interactions. Agents will be evaluated in each of the 16 environments already publicly released in Procgen Benchmark , as well as in four secret test environments created specifically for this competition. By aggregating performance across so many diverse environments, we obtain high quality metrics to judge the underlying algorithms. More information about the details of each round can be found here . Since all content is procedurally generated, each Procgen environment intrinsically requires agents to generalize to never-before-seen situations. These environments therefore provide a robust test of an agent's ability to learn in many diverse settings. Moreover, we designed Procgen environments to be fast and simple to use. Participants with limited computational resources will be able to easily reproduce our baseline results and run new experiments. We hope that this will empower participants to iterate quickly on new methods to improve sample efficiency and generalization in RL. Sign up for MineRL Many of the recent, celebrated successes of artificial intelligence, such as AlphaStar, AlphaGo, and our own OpenAI Five , utilize deep reinforcement learning to achieve human or super-human level performance in sequential decision-making tasks. These improvements to the state-of-the-art have thus far required an exponentially increasing amount of compute and simulator samples, and therefore it is difficult [1] to apply many of these systems directly to real-world problems where environment samples are expensive. One well-known way to reduce the environment sample complexity is to leverage human priors and demonstrations of the desired behavior. To further catalyze research in this direction, we are co-organizing the MineRL 2020 Competition which aims to foster the development of algorithms which can efficiently leverage human demonstrations to drastically reduce the number of samples needed to solve complex, hierarchical, and sparse environments. To that end, participants will compete to develop systems which can obtain a diamond in Minecraft from raw pixels using only 8,000,000 samples from the MineRL simulator and 4 days of training on a single GPU machine. Participants will be provided the MineRL-v0 dataset ( website , paper ), a large-scale collection of over 60 million frames of human demonstrations, enabling them to utilize expert trajectories to minimize their algorithm’s interactions with the Minecraft simulator. This competition is a follow-up to the MineRL 2019 Competition in which the top team’s agent was able to obtain an iron pickaxe (the penultimate goal of the competition) under this extremely limited compute and simulator-interaction budget. Put in perspective, state-of-the-art standard reinforcement learning systems require hundreds of millions of environment interactions on large multi-GPU systems to achieve the same goal. This year, we anticipate competitors will push the state-of-the-art even further. To guarantee that competitors develop truly sample efficient algorithms, the MineRL competition organizers train the top team’s final round models from scratch with strict constraints on the hardware, compute, and simulator-interaction available. The MineRL 2020 Competition also features a novel measure to avoid hand engineering features and overfitting solutions to the domain. More details on the competition structure can be found here . Our partners at AIcrowd have been instrumental in the development of these competitions, by creating much of the competition infrastructure, securing computational resources, and providing valuable technical support. Additionally we’d like to thank our partners at Preferred Networks for being instrumental in developing baselines for the MineRL competition. The MineRL competition extends its gratitude to our sponsors and co-organizers at DeepMind, Microsoft, and NVIDIA. The Procgen Competition is a collaboration between OpenAI and AIcrowd. The organizing team consists of Sharada Mohanty, Karl Cobbe, Jyotish Poonganam, Shivam Khandelwal, Christopher Hesse, Jacob Hilton, John Schulman, and William H. Guss. The MineRL Competition is a collaboration between OpenAI, Carnegie Mellon University, MineRL Labs, Google DeepMind, Preferred Networks, Microsoft, and AIcrowd. The lead organizer is William H. Guss, and the organizing team consists of Brandon Houghton, Stephanie Milani, Nicholay Topin, John Schulman, Oriol Vinyals, Ruslan Salakhutdinov, Noboru Sean Kuno, Sam Devlin, Crissman Loomis, Keisuke Nakata, Shinya Shiroshita, Avinash Ummadisingu, and Mario Ynocente Castro. While direct application is not possible due to the sheer number of samples required, Sim2Real and data augmentation techniques can mittigate the need to sample real-world dynamics directly. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2020-06-09"},
{"website": "Open-AI", "title": "generative-models", "author": ["Andrej Karpathy", "Pieter Abbeel", "Greg Brockman", "Peter Chen", "Vicki Cheung", "Rocky Duan", "Ian Goodfellow", "Durk Kingma", "Jonathan Ho", "Rein Houthooft", "Tim Salimans", "John Schulman", "Ilya Sutskever", "Wojciech Zaremba"], "link": "https://openai.com/blog/generative-models/", "abstract": "This post describes four projects that share a common theme of enhancing or using generative models, a branch of unsupervised learning techniques in machine learning. In addition to describing our work, this post will tell you a bit more about generative models: what they are, why they are important, and where they might be going. One of our core aspirations at OpenAI is to develop algorithms and techniques that endow computers with an understanding of our world. It's easy to forget just how much you know about the world: you understand that it is made up of 3D environments, objects that move, collide, interact; people who walk, talk, and think; animals who graze, fly, run, or bark; monitors that display information encoded in language about the weather, who won a basketball game, or what happened in 1970. This tremendous amount of information is out there and to a large extent easily accessible — either in the physical world of atoms or the digital world of bits. The only tricky part is to develop models and algorithms that can analyze and understand this treasure trove of data. Generative models are one of the most promising approaches towards this goal . To train a generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it. The intuition behind this approach follows a famous quote from Richard Feynman : “What I cannot create, I do not understand.” The trick is that the neural networks we use as generative models have a number of parameters significantly smaller than the amount of data we train them on, so the models are forced to discover and efficiently internalize the essence of the data in order to generate it. Generative models have many short-term applications . But in the long run, they hold the potential to automatically learn the natural features of a dataset, whether categories or dimensions or something else entirely. Let’s make this more concrete with an example. Suppose we have some large collection of images, such as the 1.2 million images in the ImageNet dataset (but keep in mind that this could eventually be a large collection of images or videos from the internet or robots). If we resize each image to have width and height of 256 (as is commonly done), our dataset is one large 1,200,000x256x256x3 (about 200GB) block of pixels. Here are a few example images from this dataset: These images are examples of what our visual world looks like and we refer to these as \"samples from the true data distribution\". We now construct our generative model which we would like to train to generate images like this from scratch. Concretely, a generative model in this case could be one large neural network that outputs images and we refer to these as \"samples from the model\". One such recent model is the DCGAN network from Radford et al. (shown below). This network takes as input 100 random numbers drawn from a uniform distribution (we refer to these as a code , or latent variables , in red) and outputs an image (in this case 64x64x3 images on the right, in green). As the code is changed incrementally, the generated images do too — this shows the model has learned features to describe how the world looks, rather than just memorizing some examples. The network (in yellow) is made up of standard convolutional neural network components, such as deconvolutional layers (reverse of convolutional layers), fully connected layers , etc.: DCGAN is initialized with random weights, so a random code plugged into the network would generate a completely random image. However, as you might imagine, the network has millions of parameters that we can tweak, and the goal is to find a setting of these parameters that makes samples generated from random codes look like the training data. Or to put it another way, we want the model distribution to match the true data distribution in the space of images. Suppose that we used a newly-initialized network to generate 200 images, each time starting with a different random code. The question is: how should we adjust the network's parameters to encourage it to produce slightly more believable samples in the future? Notice that we're not in a simple supervised setting and don't have any explicit desired targets for our 200 generated images; we merely want them to look real. One clever approach around this problem is to follow the Generative Adversarial Network (GAN) approach. Here we introduce a second discriminator network (usually a standard convolutional neural network) that tries to classify if an input image is real or generated. For instance, we could feed the 200 generated images and 200 real images into the discriminator and train it as a standard classifier to distinguish between the two sources. But in addition to that — and here's the trick — we can also backpropagate through both the discriminator and the generator to find how we should change the generator's parameters to make its 200 samples slightly more confusing for the discriminator. These two networks are therefore locked in a battle: the discriminator is trying to distinguish real images from fake images and the generator is trying to create images that make the discriminator think they are real. In the end, the generator network is outputting images that are indistinguishable from real images for the discriminator. There are a few other approaches to matching these distributions which we will discuss briefly below. But before we get there below are two animations that show samples from a generative model to give you a visual sense for the training process. In both cases the samples from the generator start out noisy and chaotic, and over time converge to have more plausible image statistics: This is exciting — these neural networks are learning what the visual world looks like! These models usually have only about 100 million parameters, so a network trained on ImageNet has to (lossily) compress 200GB of pixel data into 100MB of weights. This incentivizes it to discover the most salient features of the data: for example, it will likely learn that pixels nearby are likely to have the same color, or that the world is made up of horizontal or vertical edges, or blobs of different colors. Eventually, the model may discover many more complex regularities: that there are certain types of backgrounds, objects, textures, that they occur in certain likely arrangements, or that they transform in certain ways over time in videos, etc. Mathematically, we think about a dataset of examples \\(x_1, \\ldots, x_n\\) as samples from a true data distribution \\(p(x)\\). In the example image below, the blue region shows the part of the image space that, with a high probability (over some threshold) contains real images, and black dots indicate our data points (each is one image in our dataset). Now, our model also describes a distribution \\(\\hat{p}_{\\theta}(x)\\) (green) that is defined implicitly by taking points from a unit Gaussian distribution (red) and mapping them through a (deterministic) neural network — our generative model (yellow). Our network is a function with parameters \\(\\theta\\), and tweaking these parameters will tweak the generated distribution of images. Our goal then is to find parameters \\(\\theta\\) that produce a distribution that closely matches the true data distribution (for example, by having a small KL divergence loss ). Therefore, you can imagine the green distribution starting out random and then the training process iteratively changing the parameters \\(\\theta\\) to stretch and squeeze it to better match the blue distribution. Most generative models have this basic setup, but differ in the details. Here are three popular examples of generative model approaches to give you a sense of the variation: Generative Adversarial Networks (GANs) , which we already discussed above, pose the training process as a game between two separate networks: a generator network (as seen above) and a second discriminative network that tries to classify samples as either coming from the true distribution \\(p(x)\\) or the model distribution \\(\\hat{p}(x)\\). Every time the discriminator notices a difference between the two distributions the generator adjusts its parameters slightly to make it go away, until at the end (in theory) the generator exactly reproduces the true data distribution and the discriminator is guessing at random, unable to find a difference. Variational Autoencoders (VAEs) allow us to formalize this problem in the framework of probabilistic graphical models where we are maximizing a lower bound on the log likelihood of the data. Autoregressive models such as PixelRNN instead train a network that models the conditional distribution of every individual pixel given previous pixels (to the left and to the top). This is similar to plugging the pixels of the image into a char-rnn , but the RNNs run both horizontally and vertically over the image instead of just a 1D sequence of characters. All of these approaches have their pros and cons. For example, Variational Autoencoders allow us to perform both learning and efficient Bayesian inference in sophisticated probabilistic graphical models with latent variables (e.g. see DRAW , or Attend Infer Repeat for hints of recent relatively complex models). However, their generated samples tend to be slightly blurry. GANs currently generate the sharpest images but they are more difficult to optimize due to unstable training dynamics. PixelRNNs have a very simple and stable training process ( softmax loss ) and currently give the best log likelihoods (that is, plausibility of the generated data). However, they are relatively inefficient during sampling and don't easily provide simple low-dimensional codes for images. All of these models are active areas of research and we are eager to see how they develop in the future! We're quite excited about generative models at OpenAI, and have just released four projects that advance the state of the art. For each of these contributions we are also releasing a technical report and source code. Improving GANs ( code ). First, as mentioned above GANs are a very promising family of generative models because, unlike other methods, they produce very clean and sharp images and learn codes that contain valuable information about these textures. However, GANs are formulated as a game between two networks and it is important (and tricky!) to keep them in balance: for example, they can oscillate between solutions, or the generator has a tendency to collapse. In this work, Tim Salimans, Ian Goodfellow, Wojciech Zaremba and colleagues have introduced a few new techniques for making GAN training more stable. These techniques allow us to scale up GANs and obtain nice 128x128 ImageNet samples: Our CIFAR-10 samples also look very sharp - Amazon Mechanical Turk workers can distinguish our samples from real data with an error rate of 21.3% (50% would be random guessing): In addition to generating pretty pictures, we introduce an approach for semi-supervised learning with GANs that involves the discriminator producing an additional output indicating the label of the input. This approach allows us to obtain state of the art results on MNIST , SVHN , and CIFAR-10 in settings with very few labeled examples . On MNIST, for example, we achieve 99.14% accuracy with only 10 labeled examples per class with a fully connected neural network — a result that’s very close to the best known results with fully supervised approaches using all 60,000 labeled examples. This is very promising because labeled examples can be quite expensive to obtain in practice. Generative Adversarial Networks are a relatively new model (introduced only two years ago) and we expect to see more rapid progress in further improving the stability of these models during training. Improving VAEs ( code ). In this work Durk Kingma and Tim Salimans introduce a flexible and computationally scalable method for improving the accuracy of variational inference . In particular, most VAEs have so far been trained using crude approximate posteriors , where every latent variable is independent. Recent extensions have addressed this problem by conditioning each latent variable on the others before it in a chain, but this is computationally inefficient due to the introduced sequential dependencies. The core contribution of this work, termed inverse autoregressive flow (IAF), is a new approach that, unlike previous work, allows us to parallelize the computation of rich approximate posteriors, and make them almost arbitrarily flexible. We show some example 32x32 image samples from the model in the image below, on the right. On the left are earlier samples from the DRAW model for comparison (vanilla VAE samples would look even worse and more blurry). The DRAW model was published only one year ago, highlighting again the rapid progress being made in training generative models. InfoGAN ( code ). Peter Chen and colleagues introduce InfoGAN — an extension of GAN that learns disentangled and interpretable representations for images. A regular GAN achieves the objective of reproducing the data distribution in the model, but the layout and organization of the code space is underspecified — there are many possible solutions to mapping the unit Gaussian to images and the one we end up with might be intricate and highly entangled. The InfoGAN imposes additional structure on this space by adding new objectives that involve maximizing the mutual information between small subsets of the representation variables and the observation. This approach provides quite remarkable results. For example, in the images of 3D faces below we vary one continuous dimension of the code, keeping all others fixed. It's clear from the five provided examples (along each row) that the resulting dimensions in the code capture interpretable dimensions, and that the model has perhaps understood that there are camera angles, facial variations, etc., without having been told that these features exist and are important: We also note that nice, disentangled representations have been achieved before (such as with DC-IGN by Kulkarni et al.), but these approaches rely on additional supervision, while our approach is entirely unsupervised. The next two recent projects are in a reinforcement learning (RL) setting (another area of focus at OpenAI), but they both involve a generative model component. Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks ( code ). Efficient exploration in high-dimensional and continuous spaces is presently an unsolved challenge in reinforcement learning. Without effective exploration methods our agents thrash around until they randomly stumble into rewarding situations. This is sufficient in many simple toy tasks but inadequate if we wish to apply these algorithms to complex settings with high-dimensional action spaces, as is common in robotics. In this paper, Rein Houthooft and colleagues propose VIME, a practical approach to exploration using uncertainty on generative models. VIME makes the agent self-motivated; it actively seeks out surprising state-actions. We show that VIME can improve a range of policy search methods and makes significant progress on more realistic tasks with sparse rewards (e.g. scenarios in which the agent has to learn locomotion primitives without any guidance). Finally, we would like to include a bonus fifth project: Generative Adversarial Imitation Learning ( code ), in which Jonathan Ho and colleagues present a new approach for imitation learning . Jonathan Ho is joining us at OpenAI as a summer intern. He did most of this work at Stanford but we include it here as a related and highly creative application of GANs to RL. The standard reinforcement learning setting usually requires one to design a reward function that describes the desired behavior of the agent. However, in practice this can sometimes involve expensive trial-and-error process to get the details right. In contrast, in imitation learning the agent learns from example demonstrations (for example provided by teleoperation in robotics), eliminating the need to design a reward function. Popular imitation approaches involve a two-stage pipeline: first learning a reward function, then running RL on that reward. Such a pipeline can be slow, and because it’s indirect, it is hard to guarantee that the resulting policy works well. This work shows how one can directly extract policies from data via a connection to GANs. As a result, this approach can be used to learn policies from expert demonstrations (without rewards) on hard OpenAI Gym environments, such as Ant and Humanoid . Generative models are a rapidly advancing area of research. As we continue to advance these models and scale up the training and the datasets, we can expect to eventually generate samples that depict entirely plausible images or videos. This may by itself find use in multiple applications, such as on-demand generated art, or Photoshop++ commands such as \"make my smile wider\". Additional presently known applications include image denoising , inpainting , super-resolution , structured prediction , exploration in reinforcement learning, and neural network pretraining in cases where labeled data is expensive. However, the deeper promise of this work is that, in the process of training generative models, we will endow the computer with an understanding of the world and what it is made up of.", "date": "2016-06-16"},
{"website": "Open-AI", "title": "robots-that-learn", "author": ["Peter Welinder", "Bob McGrew", "Jonas Schneider", "Rocky Duan", "Josh Tobin", "Rachel Fong", "Alex Ray", "Filip Wolski", "Vikash Kumar", "Jonathan Ho", "Marcin Andrychowicz", "Bradly Stadie", "Ankur Handa", "Matthias Plappert", "Erika Reinhardt", "Pieter Abbeel", "Greg Brockman", "Ilya Sutskever", "Jack Clark", "Wojciech Zaremba"], "link": "https://openai.com/blog/robots-that-learn/", "abstract": "We've created a robotics system, trained entirely in simulation and deployed on a physical robot, which can learn a new task after seeing it done once. Last month, we showed an earlier version of this robot where we'd trained its vision system using domain randomization , that is, by showing it simulated objects with a variety of color, backgrounds, and textures, without the use of any real images. Now, we've developed and deployed a new algorithm, one-shot imitation learning , allowing a human to communicate how to do a new task by performing it in VR . Given a single demonstration, the robot is able to solve the same task from an arbitrary starting configuration. The system is powered by two neural networks: a vision network and an imitation network. The vision network ingests an image from the robot's camera and outputs state representing the positions of the objects. As before , the vision network is trained with hundreds of thousands of simulated images with different perturbations of lighting, textures, and objects. (The vision system is never trained on a real image.) The imitation network observes a demonstration, processes it to infer the intent of the task, and then accomplishes the intent starting from another starting configuration. Thus, the imitation network must generalize the demonstration to a new setting. But how does the imitation network know how to generalize? The network learns this from the distribution of training examples. It is trained on dozens of different tasks with thousands of demonstrations for each task. Each training example is a pair of demonstrations that perform the same task. The network is given the entirety of the first demonstration and a single observation from the second demonstration. We then use supervised learning to predict what action the demonstrator took at that observation. In order to predict the action effectively, the robot must learn how to infer the relevant portion of the task from the first demonstration. Applied to block stacking, the training data consists of pairs of trajectories that stack blocks into a matching set of towers in the same order, but start from different start states. In this way, the imitation network learns to match the demonstrator's ordering of blocks and size of towers without worrying about the relative location of the towers. The task of creating color-coded stacks of blocks is simple enough that we were able to solve it with a scripted policy in simulation. We used the scripted policy to generate the training data for the imitation network. At test time, the imitation network was able to parse demonstrations produced by a human, even though it had never seen messy human data before. The imitation network uses soft attention over the demonstration trajectory and the state vector which represents the locations of the blocks, allowing the system to work with demonstrations of variable length. It also performs attention over the locations of the different blocks, allowing it to imitate longer trajectories than it's ever seen, and stack blocks into a configuration that has more blocks than any demonstration in its training data. For the imitation network to learn a robust policy, we had to inject a modest amount of noise into the outputs of the scripted policy.  This forced the scripted policy to demonstrate how to recover when things go wrong, which taught the imitation network to deal with the disturbances from an imperfect policy. Without injecting the noise, the policy learned by the imitation network would usually fail to complete the stacking task. If you’d like to help us build this robot, join us at OpenAI.", "date": "2017-05-16"},
{"website": "Open-AI", "title": "openai-pytorch", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-pytorch/", "abstract": "We are standardizing OpenAI’s deep learning framework on PyTorch . In the past, we implemented projects in many frameworks depending on their relative strengths. We’ve now chosen to standardize to make it easier for our team to create and share optimized implementations of our models. As part of this move, we’ve just released a PyTorch-enabled version of Spinning Up in Deep RL , an open-source educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning. We are also in the process of writing PyTorch bindings for our highly-optimized blocksparse kernels , and will open-source those bindings in upcoming months. The main reason we've chosen PyTorch is to increase our research productivity at scale on GPUs. It is very easy to try and execute new research ideas in PyTorch; for example, switching to PyTorch decreased our iteration time on research ideas in generative modeling from weeks to days. We’re also excited to be joining a rapidly-growing developer community, including organizations like Facebook and Microsoft, in pushing scale and performance on GPUs. Going forward we'll primarily use PyTorch as our deep learning framework but sometimes use other ones when there's a specific technical reason to do so. Many of our teams have already made the switch, and we look forward to contributing to the PyTorch community in upcoming months.", "date": "2020-01-30"},
{"website": "Open-AI", "title": "microscope", "author": ["Ludwig Schubert", "Michael Petrov", "Shan Carter", "Nick Cammarata", "Gabriel Goh", "Chris Olah"], "link": "https://openai.com/blog/microscope/", "abstract": "We’re introducing OpenAI Microscope , a collection of visualizations of every significant layer and neuron of eight vision “model organisms” which are often studied in interpretability. Microscope makes it easier to analyze the features that form inside these neural networks, and we hope it will help the research community as we move towards understanding these complicated systems. The abilities of modern neural networks are the result of the interactions of thousands of neurons (sometimes tens of thousands or more!). In order to understand their behavior, we’d like to be able to quickly and easily investigate these neurons interactions in detail, and share those observations. This is especially true in collaborative environments. For instance, one researcher might speculate: InceptionV1 4c:447 is a car detector which is built from a wheel detector ( 4b:373 ) and a window detector ( 4b:237 ). When someone makes a claim like this, it’s useful if others can quickly explore those neurons, evaluating the claim and discovering new things. This is the goal of the OpenAI Microscope. Microscope systematically visualizes every neuron in several commonly studied vision models, and makes all of those neurons linkable. We hope this  will support the interpretability community in several ways: Just as biologists often focus on the study of a few “model organisms,” Microscope focuses on exploring a small number of models in detail. Our initial release includes nine frequently studied vision models, along with several visualization techniques we’ve found particularly useful in studying them. We plan to expand to other models and techniques in the coming months. We’re excited to see how the community will use Microscope, and we encourage you to reuse these assets. In particular, we think it has a lot of potential in supporting the Circuits collaboration —a project to reverse engineer neural networks by analyzing individual neurons and their connections—or similar work.", "date": "2020-04-14"},
{"website": "Open-AI", "title": "deep-double-descent", "author": ["Preetum Nakkiran", "Gal Kaplun", "Yamini Bansal", "Tristan Yang", "Boaz Barak", "Ilya Sutskever"], "link": "https://openai.com/blog/deep-double-descent/", "abstract": "We show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves, then gets worse, and then improves again with increasing model size, data size, or training time. This effect is often avoided through careful regularization. While this behavior appears to be fairly universal, we don't yet fully understand why it happens, and view further study of this phenomenon as an important research direction. Many classes of modern deep learning models, including CNNs, ResNets, and transformers, exhibit the previously-observed double descent phenomenon when not using early stopping or regularization. The peak occurs predictably at a \"critical regime,\" where the models are barely able to fit the training set. As we increase the number of parameters in a neural network, the test error initially decreases, increases, and, just as the model is able to fit the train set, undergoes a second descent. Neither classical statisticians’ conventional wisdom that too large models are worse nor the modern ML paradigm that bigger models are better uphold. We find that double descent also occurs over train epochs. Surprisingly, we show these phenomena can lead to a regime where more data hurts, and training a deep network on a larger train set actually performs worse. The model-wise double descent phenomenon can lead to a regime where training on more data hurts. In the chart above, the peak in test error occurs around the interpolation threshold, when the models are just barely large enough to fit the train set. In all cases we've observed, changes which affect the interpolation threshold (such as changing the optimization algorithm, the number of train samples, or the amount of label noise) also affect the location of the test error peak correspondingly. The double descent phenomena is most prominent in settings with added label noise; without it, the peak is smaller and easy to miss. Adding label noise amplifies this general behavior and allows us to easily investigate. The above chart shows transformers trained on a language-translation task with no added label noise. As expected, increasing the number of samples shifts the curve downwards towards lower test error. However, since more samples require larger models to fit, increasing the number of samples also shifts the interpolation threshold (and peak in test error) to the right. For intermediate model sizes (red arrows), these two effects combine, and we see that training on 4.5x more samples actually hurts test performance. The charts above show test and train error as a function of both model size and number of optimization steps. For a given number of optimization steps (fixed y-coordinate), test and train error exhibit model-size double descent. For a given model size (fixed x-coordinate), as training proceeds, test and train error decreases, increases, and decreases again; we call this phenomenon epoch-wise double descent. In general, the peak of test error appears systematically when models are just barely able to fit the train set. Our intuition is that, for models at the interpolation threshold, there is effectively only one model that fits the train data, and forcing it to fit even slightly noisy or misspecified labels will destroy its global structure. That is, there are no \"good models\" which both interpolate the train set and perform well on the test set. However, in the over-parameterized regime, there are many models that fit the train set and there exist such good models. Moreover, the implicit bias of stochastic gradient descent (SGD) leads it to such good models, for reasons we don't yet understand. We leave fully understanding the mechanisms behind double descent in deep neural networks as an important open question. Thanks to Mikhail Belkin and Chris Olah for helpful discussions and feedback throughout this work. An expanded version of this post can also be found on Boaz Barak's blog, Windows on Theory .", "date": "2019-12-05"},
{"website": "Open-AI", "title": "procgen-benchmark", "author": ["Karl Cobbe", "Christopher Hesse", "Jacob Hilton", "John Schulman"], "link": "https://openai.com/blog/procgen-benchmark/", "abstract": "We’re releasing Procgen Benchmark, 16 simple-to-use procedurally-generated environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills. Using the environment is easy whether you’re a human or AI: We’ve found that all of the Procgen environments require training on 500–1000 different levels before they can generalize to new levels, which suggests that standard RL benchmarks need much more diversity within each environment. Procgen Benchmark has become the standard research platform used by the OpenAI RL team, and we hope that it accelerates the community in creating better RL algorithms. In several environments , it has been observed that agents can overfit to remarkably large training sets. This evidence raises the possibility that overfitting pervades classic benchmarks like the Arcade Learning Environment , which has long served as a gold standard in reinforcement learning (RL). While the diversity between different games in the ALE is one of the benchmark's greatest strengths, the low emphasis on generalization presents a significant drawback. In each game the question must be asked: are agents robustly learning a relevant skill, or are they approximately memorizing specific trajectories? CoinRun was designed to address precisely this issue, by using procedural generation to construct distinct sets of training levels and test levels. While CoinRun has helped us better quantify generalization in RL, it is still only a single environment. It’s likely that CoinRun is not fully representative of the many challenges RL agents must face. We want the best of both worlds: a benchmark comprised of many diverse environments, each of which fundamentally requires generalization. To fulfill this need, we have created Procgen Benchmark. CoinRun now serves as the inaugural environment in Procgen Benchmark, contributing its diversity to a greater whole. Previous work, including the Obstacle Tower Challenge and the General Video Game AI framework , has also encouraged using procedural generation to better evaluate generalization in RL. We've designed environments in a similar spirit, with two Procgen environments drawing direct inspiration from GVGAI-based work . Other environments like Dota and StarCraft also provide lots of per-environment complexity, but these environments are hard to rapidly iterate with (and it's even harder to use more than one such environment at a time). With Procgen Benchmark, we strive for all of the following: experimental convenience, high diversity within environments, and high diversity across environments. Procgen Benchmark consists of 16 unique environments designed to measure both sample efficiency and generalization in reinforcement learning. This benchmark is ideal for evaluating generalization since distinct training and test sets can be generated in each environment. This benchmark is also well-suited to evaluate sample efficiency, since all environments pose diverse and compelling challenges for RL agents. The environments' intrinsic diversity demands that agents learn robust policies; overfitting to narrow regions in state space will not suffice. Put differently, the ability to generalize becomes an integral component of success when agents are faced with ever-changing levels. We’ve designed all Procgen environments to satisfy the following criteria: High Diversity : Environment generation logic is given maximal freedom, subject to basic design constraints. The diversity in the resulting level distributions presents agents with meaningful generalization challenges. Fast Evaluation : Environment difficulty is calibrated such that baseline agents make significant progress after training for 200M timesteps. Moreover, the environments are optimized to perform thousands of steps per second on a single CPU core, enabling a fast experimental pipeline. Tunable Difficulty : All environments support two well-calibrated difficulty settings: easy and hard. While we report results using the hard difficulty setting, we make the easy difficulty setting available for those with limited access to compute power. Easy environments require approximately an eighth of the resources to train. Emphasis on Visual Recognition and Motor Control : In keeping with precedent, environments mimic the style of many Atari and Gym Retro games. Performing well primarily depends on identifying key assets in the observation space and enacting appropriate low level motor responses. High Diversity : Environment generation logic is given maximal freedom, subject to basic design constraints. The diversity in the resulting level distributions presents agents with meaningful generalization challenges. Fast Evaluation : Environment difficulty is calibrated such that baseline agents make significant progress after training for 200M timesteps. Moreover, the environments are optimized to perform thousands of steps per second on a single CPU core, enabling a fast experimental pipeline. Tunable Difficulty : All environments support two well-calibrated difficulty settings: easy and hard. While we report results using the hard difficulty setting, we make the easy difficulty setting available for those with limited access to compute power. Easy environments require approximately an eighth of the resources to train. Emphasis on Visual Recognition and Motor Control : In keeping with precedent, environments mimic the style of many Atari and Gym Retro games. Performing well primarily depends on identifying key assets in the observation space and enacting appropriate low level motor responses. We came to appreciate how hard RL generalization can be while conducting the Retro Contest , as agents continually failed to generalize from the limited data in the training set. Later, our CoinRun experiments painted an even clearer picture of our agents' struggle to generalize. We've now expanded on those results, conducting our most thorough study of RL generalization to date using all 16 environments in Procgen Benchmark. We first measured how the size of the training set impacts generalization. In each environment, we generated training sets ranging in size from 100 to 100,000 levels. We trained agents for 200M timesteps on these levels using Proximal Policy Optimization , and we measured performance on unseen test levels. We found that agents strongly overfit to small training sets in almost all environments. In some cases, agents need access to as many as 10,000 levels to close the generalization gap. We also saw a peculiar trend emerge in many environments: past a certain threshold, training performance improves as the training sets grows! This runs counter to trends found in supervised learning, where training performance commonly decreases with the size of the training set. We believe this increase in training performance comes from an implicit curriculum provided by a diverse set of levels. A larger training set can improve training performance if the agent learns to generalize even across levels in the training set . We previously noticed this effect with CoinRun, and have found it often occurs in many Procgen environments as well. We also conducted a simple ablation study to emphasize the importance of procedural generation. Instead of using a new level at the start of every episode, we trained agents on a fixed sequence of levels. The agent begins each episode on the first level, and when it successfully completes a level, it progresses to the next one. If the agent fails at any point, the episode terminates. The agent can reach arbitrarily many levels, though in practice it rarely progresses beyond the 20th level in any environment. At test time, we remove the determinism in the sequence of levels, instead choosing level sequences at random. We find that agents become competent over the first several training levels in most games, giving an illusion of meaningful progress. However, test performance demonstrates that the agents have in fact learned almost nothing about the underlying level distribution. We believe this vast gap between training and test performance is worth highlighting. It reveals a crucial hidden flaw in training on environments that follow a fixed sequence of levels. These results show just how essential it is to use diverse environment distributions when training and evaluating RL agents. We expect many insights gleaned from this benchmark to apply in more complex settings, and we’re excited to use these new environments to design more capable and efficient agents. If you’re interested in helping develop diverse environments, we’re hiring ! Thanks to Marc Bellemare, Julian Togelius, Carles Gelada, Jacob Jackson, Alex Ray, Lilian Weng, and Joshua Achiam for their feedback on the paper. Thanks to Mira Murati, Brooke Chan, Justin Jay Wang, Greg Brockman, Ashley Pilipiszyn and Jack Clark for their work supporting, designing, writing, and providing feedback on this post. Special thanks to Kenney for the many high quality game assets used throughout these environments. Additional thanks to CraftPix.net for several game backgrounds, as well as to GameArtGuppy , and ansimuz . All asset licenses can be found here .", "date": "2019-12-03"},
{"website": "Open-AI", "title": "fine-tuning-gpt-2", "author": ["Daniel Ziegler", "Nisan Stiennon", "Jeffrey Wu", "Tom Brown", "Dario Amodei", "Alec Radford", "Paul Christiano", "Geoffrey Irving"], "link": "https://openai.com/blog/fine-tuning-gpt-2/", "abstract": "We’ve fine-tuned the 774M parameter GPT-2 language model using human feedback for various tasks, successfully matching the preferences of the external human labelers, though those preferences did not always match our own. Specifically, for summarization tasks the labelers preferred sentences copied wholesale from the input (we’d only asked them to ensure accuracy), so our models learned to copy. Summarization required 60k human labels; simpler tasks which continue text in various styles required only 5k. Our motivation is to move safety techniques closer to the general task of “machines talking to humans,” which we believe is key to extracting information about human values. We believe language is a key ingredient in making reinforcement learning practical and safe for real-world tasks. Previous work on learning models of human preferences has focused on simple simulated environments (Atari games or robotics tasks) which do not capture the complexity of language. Language is also a necessary ingredient for algorithms such as amplification and debate , which target the reasoning behind preferences. This work applies human preference learning to several natural language tasks: continuing text with positive sentiment or physically descriptive language using the BookCorpus , and summarizing content from the TL;DR and CNN/Daily Mail datasets. Each of these tasks can be viewed as a text completion problem: starting with some text X , we ask what text Y should follow. [1] We start with a pretrained language model ( the 774M parameter version of GPT-2 ) and fine-tune the model by asking human labelers which of four samples is best.  Fine-tuning for the stylistic continuation tasks is sample efficient: 5,000 human samples suffice for strong performance according to humans. For summarization, models trained with 60,000 comparisons learn to copy whole sentences from the input while skipping irrelevant preamble; this copying is an easy way to ensure accurate summaries, but may exploit the fact that labelers rely on simple heuristics. For the stylistic continuation tasks, samples comparing the raw 774M GPT-2 model and our fine-tuned versions are shown below. [2] According to the same human labelers used to train them, our fine-tuned models are preferred to the base GPT-2 model (zero-shot) 88% and 86% of the time for sentiment and descriptiveness, respectively. We also applied human fine-tuning to two summarization tasks: summarization of articles from the CNN/Daily Mail dataset, and summarization of Reddit snippets from the TL;DR dataset. These tasks are harder: our main models use 60,000 four-way comparisons. We also need online data collection, where the samples shown to humans are collected throughout training as the policy changes; an offline data collection strategy which shows humans only samples from the base GPT-2 language model performed poorly. Our models achieve very good performance according to human labelers, but are likely exploiting the fact that labelers rely on simple heuristics: they prefer the lead-3 baseline of copying the first three sentences to our models. However, when combining supervised fine-tuning with human fine-tuning, our models outperform lead-3 on ROUGE scores. Samples from zero-shot and supervised baselines, as well as RL fine-tuning of each, are shown below. The reader may have noticed a few things about these samples. First, our RL fine-tuned model is mostly a smart copying engine: it typically summarizes content by copying entire sentences from the article or Reddit snippet. By contrast, the zero-shot and supervised fine-tuned samples are more novel: The RL fine-tuned model does vary where it copies from: while they copy the start of the input 28.3% and 77.6% of the time on TL;DR and CNN/Daily Mail, these numbers fall to 0.2% and 1.4% if the input starts with uninformative preamble (defined as “hi”, “hello”, “hey”, “ok”, “okay”, “so” for TL;DR, or a colon in the first three words for CNN/Daily Mail such as “Winner: Simon Wood took home the TV crown [...]”). Second, while summaries from GPT-2 zero-shot and the supervised fine-tuned version of GPT-2 are more novel as measured by n-grams or sentences, they are also more novel in terms of content. That is, they're not true: There are at least two ways of interpreting these results. The first is that copying is the easiest way to be accurate. The labelers were told to penalize inaccuracy but not copying. The zero-shot model copies some of the time, and when it copied it was accurate, so copying was reinforced. The result is a model that mostly copies, but at least does not lie. However, this does not fully explain the results of human evaluation: both our model and a simple lead-3 baseline which copies the first three sentences are strongly preferred by the labelers to the human reference summaries in both datasets. The authors do not agree: we find the reference summaries are accurate and better capture the overall message. This reveals a mismatch between the notion of quality we wanted our model to learn, and what the humans labelers actually evaluated. Labelers want to work as quickly as possible, and they can work very quickly by following the heuristic of \"if the summary copies, then select it.\" Online data collection was necessary to achieve the best results on summarization, but led to multiple difficulties: We believe the right middle ground between offline and online data collection is batched data collection: we would alternate between collecting large batches of data (with higher latency) and training on collected data. The cost of human data means that volume will always be low, so it is easy to retrain from scratch (or rather, from the GPT-2 starting point) each time. A single human may have a clear notion of whether a given sample is separately accurate, grammatical, nonredundant, or hits the key points, but comparing two summaries often requires subjective weighing of different kinds of deficiencies. When possible, it seems better to design less ambiguous labeling tasks that get at the same information. For example, rather than asking a person to compare summaries, we could ask for a verbal description of the problems with a summary, or a suggested correction. Even if two people disagree on the most important problem, they may agree that the other picked some problem, and more agreement eases data quality control and the overall experimental process. One of our code refactors introduced a bug which flipped the sign of the reward. Flipping the reward would usually produce incoherent text, but the same bug also flipped the sign of the KL penalty. The result was a model which optimized for negative sentiment while preserving natural language. Since our instructions told humans to give very low ratings to continuations with sexually explicit text, the model quickly learned to output only content of this form. This bug was remarkable since the result was not gibberish but maximally bad output. The authors were asleep during the training process, so the problem was noticed only once training had finished. A mechanism such as Toyota's Andon cord could have prevented this, by allowing any labeler to stop a problematic training process. We’ve demonstrated reward learning from human preferences on two kinds of natural language tasks, stylistic continuation and summarization. Our results are mixed: for continuation we achieve good results with very few samples, but our summarization models are only “smart copiers”: they copy from the input text but skip over irrelevant preamble. The advantage of smart copying is truthfulness: the zero-shot and supervised models produce natural, plausible-looking summaries that are often lies. We believe the limiting factor in our experiments is data quality exacerbated by the online data collection setting, and plan to use batched data collection in the future. We believe the application of reward learning to language is important both from a capability and safety perspective. On the capability side, reinforcement learning lets us correct mistakes that supervised learning would not catch, but RL with programmatic reward functions “ can be detrimental to model quality .” On the safety side, reward learning for language allows important criteria like “don’t lie” to be represented during training, and is a step towards scalable safety methods such as a debate and amplification . For summarization, the text is the article plus the string “TL;DR:”. ↩︎ Each fine-tuned model is trained using 5,000 four-way comparisons by humans. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-09-19"},
{"website": "Open-AI", "title": "symposium-2019", "author": ["OpenAI"], "link": "https://openai.com/blog/symposium-2019/", "abstract": "We hosted the first OpenAI Robotics Symposium on April 27, 2019. Robots that learn are an exciting path forward, yet there are differing approaches and opinions on how to make progress. The event brought together a diverse set of people from both robotics and machine learning communities as well as academics and industry leaders to create a platform to exchange ideas and address open questions in building complex robot systems. Robots that learn are a development that will allow robots to become part of our everyday lives. While we have some ideas on how to get there, we think it is important to engage with people from other organizations and disciplines to exchange and discuss ideas. Creating these robots is inherently a multidisciplinary approach—it not only requires technical expertise, but also a deeper understanding of how these robots can be deployed safely and interact with humans in the real world. We hosted ~80 external attendees at our office and ~200 people joined remotely via our livestream throughout the day. We had attendees from industry labs like Google, Facebook, and NVIDIA in addition to students, postdocs and professors from universities like Stanford , UC Berkeley , CMU and MIT . We also had hobbyists, artists, roboticists, and machine learning researchers in the crowd. Wojciech Zaremba , OpenAI Wojciech talks about our recent research, \" Learning Dexterity ,\" which uses sim2real with domain randomization and large-scale reinforcement learning with memory-augmented policies. This approach leads to meta-learning that allows our policy to transfer to the physical robot without ever training on the robot. Pierre Sermanet , Google Brain Pierre describes how play can provide self-supervision for representation learning. This approach can be used to acquire a diverse set of skills that can be used and recombined to solve novel tasks without ever providing any labels or rewards. Leslie Kaelbling , MIT Leslie explains how we have to think about learning both in the \"robot factory\" (i.e., at engineering time) as well as \"in the wild\" (i.e., when deployed). Leslie describes her overall architecture for building intelligent robots and how it can be used to build robots that acquire new skills. Anca Dragan , UC Berkeley Anca explores the question of what inductive bias is right when learning for human-robot interaction. She proposes a framework for predicting human actions that broadens the assumption that humans are noisy-rational and allows for strategic human behavior, as well as systematic sub-optimality (like not knowing the exact physics of the environment, or still learning about their preferences). Jin Joo Lee , MIT / Amazon Jin Joo dives into the why and how of making robots lifelike and interactive through social-emotional intelligence. These social robots can read and understand our emotional expressions and also communicate back to us in the same way. Chris Atkeson , CMU Chris critically discusses the gap between robot learning research and robot programming practice. He asks what would make learning robots truly useful and outlined his ideas on how to get there. Jeff Clune , Uber AI / University of Wyoming Jeff describes work he and his collaborators published in Nature on how to build robots that can rapidly adapt at runtime if they become damaged. The proposed approach could ultimately lead to robots that are much more able to adapt to damage or unexpected environmental conditions. Since the event was hosted at our office, we took the opportunity to perform a live demo of our humanoid robot hand manipulating a block using vision and reinforcement learning. We were excited to show the hand to people and have the OpenAI Robotics team \"on hand\" to answer their questions! We hope to do this again in the future as it is a very different experience to see this in person. We were extremely pleased with the outcome of the event—this was an experimental format and our expectations were definitely exceeded. The talks during the day led to interesting discussions within our team and resulted in some new ideas (e.g., self-supervision) and perspectives (e.g., traditional robotics vs deep learning robotics). After chatting with the participants and speakers, it was clear everyone felt they benefited from this event and left with a shared understanding of the diversity in the different approaches to solving the same problems. Given this feedback, we intend to repeat this format in the future, possibly as an annual symposium. We'll share details about upcoming events at a later date. If you would like to help us do research on robots that learn, please get in touch! We’re hiring . Thanks to Loren Kwan, Diane Yoon, and Maddie Hall for co-organizing the event, to all the OpenAI staff volunteers, and to Blake Tucker for filming and photography.", "date": "2019-06-05"},
{"website": "Open-AI", "title": "microsoft", "author": ["Greg Brockman"], "link": "https://openai.com/blog/microsoft/", "abstract": "Microsoft is investing $1 billion in OpenAI to support us building artificial general intelligence (AGI) with widely distributed economic benefits. We're partnering to develop a hardware and software platform within Microsoft Azure which will scale to AGI. We’ll jointly develop new Azure AI supercomputing technologies, and Microsoft will become our exclusive cloud provider—so we'll be working hard together to further extend Microsoft Azure's capabilities in large-scale AI systems. Each year since 2012, the world has seen a new step function advance in AI capabilities. Though these advances are across very different fields like vision (2012), simple video games (2013), machine translation (2014), complex board games (2015), speech synthesis (2016), image generation (2017), robotic control (2018), and writing text (2019), they are all powered by the same approach: innovative applications of deep neural networks coupled with increasing computational power . But still, AI system building today involves a lot of manual engineering for each well-defined task. In contrast, an AGI will be a system capable of mastering a field of study to the world-expert level, and mastering more fields than any one human — like a tool which combines the skills of Curie, Turing, and Bach. An AGI working on a problem would be able to see connections across disciplines that no human could. We want AGI to work with people to solve currently intractable multi-disciplinary problems, including global challenges such as climate change, affordable and high-quality healthcare, and personalized education. We think its impact should be to give everyone economic freedom to pursue what they find most fulfilling, creating new opportunities for all of our lives that are unimaginable today. OpenAI is producing a sequence of increasingly powerful AI technologies, which requires a lot of capital for computational power. The most obvious way to cover costs is to build a product, but that would mean changing our focus. Instead, we intend to license some of our pre-AGI technologies, with Microsoft becoming our preferred partner for commercializing them. We believe that the creation of beneficial AGI will be the most important technological development in human history, with the potential to shape the trajectory of humanity. We have a hard technical path in front of us, requiring a unified software engineering and AI research effort of massive computational scale, but technical success alone is not enough. To accomplish our mission of ensuring that AGI (whether built by us or not) benefits all of humanity, we'll need to ensure that AGI is deployed safely and securely; that society is well-prepared for its implications; and that its economic upside is widely shared. If we achieve this mission, we will have actualized Microsoft and OpenAI's shared value of empowering everyone.", "date": "2019-07-22"},
{"website": "Open-AI", "title": "openai-fellows-fall-2018", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-fellows-fall-2018/", "abstract": "Our second class of OpenAI Fellows has wrapped up, with each Fellow going from a machine learning beginner to core OpenAI contributor in the course of a 6-month apprenticeship. We are currently reviewing applications on a rolling basis for our next round of OpenAI Fellows Summer 2019. During this time, we’ve seen how expertise in other scientific fields like classical music, statistics, and mathematics can yield insights to push AI research forward. All 6 Fellows have completed projects investigating a novel research idea while embedded in an OpenAI research team. We’re also excited to welcome all 6 of our Fall Fellows to OpenAI as full-time members of our technical staff! Previous Role : Pianist What I Learned : \"The Fellows program provided a great balance of freedom and support. I enjoyed spending the first two months reading papers and learning to implement them, and I really appreciated having a mentor who helped me pick the best papers or ideas to pursue. I was also able to work on my own and experiment with different ideas, but Alec and others on the team were always very generous with their time when I was stuck or needed advice. At the start of 2019, we were asked to think \"What do I need to do to make my work this coming year the best work of my life?\" For me, a big part of the answer is to work at OpenAI, as part of such a uniquely talented and motivated team.\" Final Project : I created MuseNet, a MIDI music model based on the same transformer architecture that powers GPT-2 . MuseNet generates 2–4 minute compositions in many different musical styles. To do this, I collected hundreds of thousands of MIDI files from the web, experimented with different tokenization schemes, developed a way to condition samples based on a particular style or composer, and developed a co-composer tool to enable joint human/AI compositions. What's Next : Joining the Language team at OpenAI, working to improve MuseNet and collaborate with musicians. Previous Roles : Quantitative researcher/trader at Jane Street , PhD in Mathematics at Leeds. What I Learned : \"The Fellows program has been a fantastic introduction to machine learning research. It has been intense—a bit like the first year of my PhD condensed into six months. It was reinvigorating to have the first couple of months set aside to just learn, following a nicely-curated curriculum of papers and programming exercises. Just as valuable to learn from has been conducting my first machine learning research project, including all the inevitable false starts and failed experiments. Throughout I've been surrounded by experts who have been eager to bounce around ideas with me, and my mentor's indispensable guidance has helped to hone my research intuition and kept my project on track.\" Final Project : I studied how to make bias-variance tradeoffs in reinforcement learning. There are several hyperparameters of reinforcement learning algorithms that can be viewed as making a tradeoff between bias (systematic error) and variance (random error). For example, the discount rate controls the amount of bias towards shorter-term rewards, which tend to have less variance. I developed a general method of choosing these hyperparameters by directly measuring the bias and variance of gradients. The method also works in other contexts involving stochastic gradient descent outside of reinforcement learning. What's Next : Joining the RL team at OpenAI, exploring new research directions such as interpretability for RL. Previous Role : Software engineer at Blend; BS in Symbolic Systems and MS in Statistics at Stanford University. What I Learned : \"The Fellows program has been great in terms of both providing a good overview of the current state of deep learning and reinforcement learning research, and also allowing me to get hands-on experience with doing research in the field. The mentorship aspect was also a crucial component, and it has been tremendously helpful for beginning to build a sense of research taste.\" Final Project : I worked on evaluating skill emergence in multiagent environments by creating several evaluation tasks and testing whether transfer learning occurs when an agent trained in the multiagent environment has to learn those evaluation tasks. I also tried to evaluate how much of the observed transfer is caused by useful behaviors being learned in the multiagent environment, and how much is caused by useful mental representations being learned. What's Next : Joining the Multiagent team at OpenAI, continuing to work on transfer learning. Previous Role : Quantitative trader What I Learned : \"The Fellows program provided me with a structured and efficient path to becoming a productive AI researcher. Ilya and Alec always made time for mentorship and to help me refine my ideas. With Ilya's enthusiasm, it's hard not to be excited about the future of generative models research!\" Final Project : I worked on scaling image transformers to generate coherent images at high resolution. First, I explored the space of multiscale architectures, which allow for faster training and inference. Next, I focused on scaling past GPU memory limits by pipelining the models and alternatively porting them to run on TPU. Finally, I was involved in a team effort to use these large scale models to see how representations learned by generative pretraining aid us in solving downstream supervised image tasks. What's Next : Joining the Algorithms team at OpenAI, continuing work on image transformers. Previous Role : Software developer; PhD in Coding & Information Theory at the University of Toronto. What I Learned : \"The Fellows program is very well-suited for bringing a researcher from another technical field up-to-date on the lastest deep learning techniques. Mentorship was a significant factor in my growth as an AI researcher. I always felt that I could discuss ideas and received lots of feedback that helped calibrate my ideas. My experience with deep RL, meta-learning, and solving real-world problems in robotics definitely shaped my research interests and I look forward to exploring them in my future research.\" Final Project : I studied a transfer metric that can predict the performance of an RL policy trained in simulation when deployed on a physical robot. While training in simulation is highly scalable and efficient, simulators are not perfect models and policies often perform poorly in the real world. The transfer metric does not require repeated rollouts on a physical robot. It helps to resolve the sim-to-real transfer problem by predicting which policy and training procedure will lead to better real-world performance. What's Next : Joining the Robotics team at OpenAI, continuing to work on improving sim-to-real transfer. Previous Role : Software developer What I Learned : \"The Fellows program allowed me to get acquainted with field of machine learning research. I think the curriculum-based learning and mentorship were two very important aspects of this program that helped me to do my research effectively. I also learned that doing research is quite challenging—not all ideas work as you expect, but if you continue formulating hypotheses and checking one thing at time, eventually you will find a promising direction and get good results.\" Final Project : We studied techniques to learn sparsity patterns in deep neural networks and how structure in sparsity affects parameter efficiency. We developed an additive pruning approach for learning sparsity, when during training we have few cycles of adding and pruning blocks of weights. Specially designed kernels for block sparse matrix multiplication and this additive pruning approach allowed us to explore more diverse topologies that previously hadn't been possible. We showed that sparse models are more parameter efficient and give lower loss than dense networks for the same parameters budget. What's Next : Joining the Hardware team at OpenAI, continuing to investigate sparsity in neural networks. We’d like to congratulate our Fall 2018 Fellows on their outstanding work and thank them for their contributions to OpenAI. We are excited to see their research continue! If you want to go from a beginner to producing world class ML contributions, consider applying for our next round of OpenAI Fellows, starting July 2019. We are currently accepting applications and reviewing them on a rolling basis, so apply early! As part of our effort to educate more people like our class of Fellows, we recently open sourced part of their introductory curriculum. You can start your ML education today by completing our tutorial, “ Spinning up in Deep RL .” Spinning up in Deep RL consists of examples of RL code, educational exercises, documentation , and tutorials that will help you become a skilled practitioner in RL.", "date": "2019-05-17"},
{"website": "Open-AI", "title": "cooperation-on-safety", "author": ["Amanda Askell", "Miles Brundage", "Jack Clark"], "link": "https://openai.com/blog/cooperation-on-safety/", "abstract": "We've written a policy research paper identifying four strategies that can be used today to improve the likelihood of long-term industry cooperation on safety norms in AI: communicating risks and benefits, technical collaboration, increased transparency, and incentivizing standards. Our analysis shows that industry cooperation on safety will be instrumental in ensuring that AI systems are safe and beneficial, but competitive pressures could lead to a collective action problem, potentially causing AI companies to under-invest in safety. We hope these strategies will encourage greater cooperation on the safe development of AI and lead to better global outcomes of AI. It’s important to ensure that it’s in the economic interest of companies to build and release AI systems that are safe, secure, and socially beneficial. This is true even if we think AI companies and their employees have an independent desire to do this, since AI systems are more likely to be safe and beneficial if the economic interests of AI companies are not in tension with their desire to build their systems responsibly. This claim might seem redundant because developing and deploying products that do not pose a risk to society is generally in a company’s economic interest. People wouldn’t pay much for a car without functioning brakes, for example. But if multiple companies are trying to develop a similar product, they can feel pressure to rush it to market, resulting in less safety work prior to release. Such problems generally arise in contexts where external regulation is weak or non-existent. Appropriate regulation of goods and services provided in the marketplace can reduce corner-cutting on safety. This can benefit the users of goods and services as well as the sector itself—the airline sector as a whole benefits commercially from the fact that governments around the world are vigilant about safety, for example, and that when incidents occur, they are always investigated in detail. Conventional regulatory mechanisms may be less effective in dealing with AI, however, due to the rate at which the technology is developing and the large information asymmetries between developers and regulators. Our paper explores what factors might drive or dampen such a rush to deployment, and suggests strategies for improving cooperation between AI developers. Developers “cooperate” not by ceasing to compete but by taking appropriate safety precautions, and they are more likely to do this if they are confident their competitors will do the same. If companies respond to competitive pressures by rushing a technology to market before it has been deemed safe, they will find themselves in a collective action problem. Even if each company would prefer to compete to develop and release systems that are safe, many believe they can’t afford to do so because they might be beaten to market by other companies. Problems like this can be mitigated by greater industry cooperation on safety. AI companies can work to develop industry norms and standards that ensure systems are developed and released only if they are safe, and can agree to invest resources in safety during development and meet appropriate standards prior to release. Some hypothetical scenarios: A company develops an image recognition model with very high performance and is in a rush to deploy it at scale, but the engineers at the company have not yet adequately evaluated the system's performance in the real world. The company also knows it lacks full testing standards to know the full \"capability surface\" of the model. Due to fears of being beaten to market by competitors in a particular niche, however, the company moves forward, gambling that their limited in-house testing will suffice to hedge against any major system failures or public blowback. A company wishes to deploy some semi-autonomous AI software onto physical robots, such as drones. This software has a failure rate that satisfies regulatory criteria, but because the company is racing to get the technology to market it knows that their product's popular \"interpretability\" feature gives misleading explanations that are intended more for reassurance than clarification. Due to limited expertise among regulators, this misbehavior falls through the cracks until a catastrophic incident, as does similar behavior by other companies racing to deploy similarly \"interpretable\" systems. Some collective action problems are more solvable than others. In general, a collective action problem is more solvable if the expected benefits of cooperating outweigh the expected benefits of not cooperating. The following interrelated factors increase the expected benefits of cooperating: Companies are more likely to cooperate on safety if they can trust that other companies will reciprocate by working towards a similar standard of safety. Among other things, trust that others will develop AI safely can be established by increasing transparency about resources being invested in safety, by publicly committing to meet a high standard of safety, and by engaging in joint work to find acceptable safety benchmarks. Companies have a stronger incentive to cooperate on safety if the mutual benefits from safe development are higher. The prospect of cooperation can be improved by highlighting the benefits of establishing good safety norms early, such as preventing incidents of AI failure and misuse, and establishing safety standards that are based on a shared understanding of emerging AI systems. Collaborative efforts like Risk Salon , which hosts events for people working in fraud, risk, and compliance, are a good example of this. These events facilitate open discussions between participants from different companies, and seem to be primarily motivated by the shared gain of improved risk mitigation strategies. Reducing the harms that companies expect to incur if another company decides not to cooperate on safety increases the likelihood that they themselves will abide by safety standards. Exposure can be reduced by discouraging violations of safety standards (e.g. reporting them) or by providing evidence of the potential risks associated with systems that don’t meet the relevant standards. When standards must be met to enter a market, for example, companies have little to lose if others don’t meet those standards. To comply with the RoHS directive , electronics manufacturers had to switch to lead-free soldering in order to sell their products in the EU. The possibility that one manufacturer would continue to use lead soldering would do little to affect cooperation with lead-reduction efforts, since their failure to comply would not be costly to other manufacturers. Reducing any advantages companies can expect to get by not cooperating on safety should increase overall compliance with safety standards. For example, companies producing USB connectors don’t expect to gain much from deviating from USB connector standards, because doing so will render their product incompatible with most devices. When standards have already been established and deviating from them is more costly than any benefits, advantage is low. In the context of AI, reducing the cost and difficulty of implementing safety precautions would help minimize the temptation to ignore them. Additionally, governments can foster a regulatory environment in which violating high-stakes safety standards is prohibited. Identifying the ways in which AI systems could fail if adequate precautions are not taken can increase the likelihood that AI companies will agree not to develop or release such systems. Shared downsides incentivize cooperation when failures are particularly harmful: especially if they are felt by the whole industry (e.g. by damaging public trust in the industry as a whole). After the Three Mile Island incident, for example, the nuclear power industry created and funded the INPO , a private regulator with the ability to evaluate plants and share the results of these evaluations within industry in order to improve operational safety. Collective action problems are susceptible to negative spirals where the loss of trust causes one party to stop cooperating, causing other parties to stop cooperating. At the same time, it is also possible to generate positive spirals where the development of trust causes some parties to cooperate, resulting in other parties cooperating. We've found four strategies that can be used today to improve the likelihood of cooperation on safety norms and standards in AI. These are: Communicate the safety and security risks associated with AI, show that concrete steps can be taken to promote cooperation on safety, and make shared concerns about safety common knowledge. Engage in joint interdisciplinary research that promotes safety and is otherwise conducive to fostering strong collaboration (e.g. work that involves combining complementary areas of expertise). Publicize codes of conduct, increase transparency about publication-related decision-making, and, provided that security and IP concerns are  addressed, open up individual AI systems to greater scrutiny. Commend those that adhere to safety standards, reproach failures to ensure that systems are developed safely, and support economic, legal, or industry-wide incentives to adhere to safety standards. We think collective action problems may be a principal source of policy challenges as AI systems become increasingly powerful. This analysis focuses on the roles that industry can play in preventing such problems, but we anticipate that legal and political mechanisms will also play an important role in preventing and mitigating these issues. We also anticipate that identifying similar mechanisms to improve cooperation on AI safety between states and with other non-industry actors will be of increasing importance in the years to come. There is a great deal of uncertainty about the challenges that future AI systems may pose, but we believe that encouraging greater cooperation on the safe development of AI is likely to have a positive impact on the outcomes of AI development. While we acknowledge that such challenges exist, we advocate for a more thorough mapping of possible collaborations across organizational and national borders, with particular attention to research and engineering challenges whose solutions might be of wide utility. Areas to consider might include joint research into the formal verification of AI systems' capabilities and other aspects of AI safety and security with wide applications; various applied \"AI for good\" projects whose results might have wide-ranging and largely positive applications (e.g. in domains like sustainability and health); and joint development of countermeasures against global AI-related threats such as the misuse of synthetic media generation online. To achieve greater cooperation on safety, we need to make it common knowledge that such cooperation is in everyone’s interest, and that methods for achieving it can be identified, researched, and implemented today. Thanks to those who provided helpful comments on earlier versions of this blog post: Helen Toner, Cullen O’Keefe, Larissa Schiavo, Danny Hernandez, Geoffrey Irving, Michael Page, Greg Brockman, Ashley Pilipiszyn, Adam Gleave, Chip Huyen, Paul Scharre, Jelena Luketina, Michael Horowitz, Rowan Zellers, Sarah Kreps, and Rebecca Crootof.", "date": "2019-07-10"},
{"website": "Open-AI", "title": "openai-lp", "author": ["Greg Brockman", "Ilya Sutskever", "OpenAI"], "link": "https://openai.com/blog/openai-lp/", "abstract": "We've created OpenAI LP, a new \"capped-profit\" company that allows us to rapidly increase our investments in compute and talent while including checks and balances to actualize our mission . Our mission is to ensure that artificial general intelligence (AGI) benefits all of humanity, primarily by attempting to build safe AGI and share the benefits with the world. We’ve experienced firsthand that the most dramatic AI systems use the most computational power in addition to algorithmic innovations, and decided to scale much faster than we’d planned when starting OpenAI. We’ll need to invest billions of dollars in upcoming years into large-scale cloud compute, attracting and retaining talented people, and building AI supercomputers. We want to increase our ability to raise capital while still serving our mission, and no pre-existing legal structure we know of strikes the right balance. Our solution is to create OpenAI LP as a hybrid of a for-profit and nonprofit—which we are calling a \"capped-profit\" company. The fundamental idea of OpenAI LP is that investors and employees can get a capped return if we succeed at our mission, which allows us to raise investment capital and attract employees with startup-like equity. But any returns beyond that amount—and if we are successful, we expect to generate orders of magnitude more value than we’d owe to people who invest in or work at OpenAI LP—are owned by the original OpenAI Nonprofit entity. Going forward (in this post and elsewhere), “OpenAI” refers to OpenAI LP (which now employs most of our staff), and the original entity is referred to as “OpenAI Nonprofit.” We’ve designed OpenAI LP to put our overall mission—ensuring the creation and adoption of safe and beneficial AGI—ahead of generating returns for investors. The mission comes first even with respect to OpenAI LP’s structure. While we are hopeful that what we describe below will work until our mission is complete, we may update our implementation as the world changes. Regardless of how the world evolves, we are committed—legally and personally—to our mission. OpenAI LP’s primary fiduciary obligation is to advance the aims of the OpenAI Charter , and the company is controlled by OpenAI Nonprofit’s board. All investors and employees sign agreements that OpenAI LP’s obligation to the Charter always comes first, even at the expense of some or all of their financial stake. Only a minority of board members are allowed to hold financial stakes in the partnership at one time. Furthermore, only board members without such stakes can vote on decisions where the interests of limited partners and OpenAI Nonprofit’s mission may conflict—including any decisions about making payouts to investors and employees. As mentioned above, economic returns for investors and employees are capped (with the cap negotiated in advance on a per-limited partner basis). Any excess returns go to OpenAI Nonprofit. Our goal is to ensure that most of the value (monetary or otherwise) we create if successful benefits everyone, so we think this is an important first step. Returns for our first round of investors are capped at 100x their investment (commensurate with the risks in front of us), and we expect this multiple to be lower for future rounds as we make further progress. Our day-to-day work is not changing. Today, we believe we can build the most value by focusing exclusively on developing new AI technologies, not commercial products. Our structure gives us flexibility for how to create a return in the long term, but we hope to figure that out only once we’ve created safe AGI. OpenAI LP currently employs around 100 people organized into three main areas: capabilities (advancing what AI systems can do), safety (ensuring those systems are aligned with human values), and policy (ensuring appropriate governance for such systems). OpenAI Nonprofit governs OpenAI LP, runs educational programs such as Scholars and Fellows , and hosts policy initiatives. OpenAI LP is continuing (at increased pace and scale) the development roadmap started at OpenAI Nonprofit, which has yielded breakthroughs in reinforcement learning , robotics , and language . We are excited by the potential for AGI to help solve planetary-scale problems in areas where humanity is failing and there is no obvious solution today. However, we are also concerned about AGI’s potential to cause rapid change, whether through machines pursuing goals misspecified by their operator, malicious humans subverting deployed systems, or an out-of-control economy that grows without resulting in improvements to human lives. As described in our Charter , we are willing to merge with a value-aligned organization (even if it means reduced or zero payouts to investors) to avoid a competitive race which would make it hard to prioritize safety. OpenAI Nonprofit’s board consists of OpenAI LP employees Greg Brockman (Chairman & CTO), Ilya Sutskever (Chief Scientist), and Sam Altman (CEO), and non-employees Adam D’Angelo, Holden Karnofsky, Reid Hoffman, Shivon Zilis, and Tasha McCauley. Elon Musk left the board of OpenAI Nonprofit in February 2018 and is not formally involved with OpenAI LP. We are thankful for all his past help. Our investors include Reid Hoffman's charitable foundation and Khosla Ventures, among others. We feel lucky to have mission-aligned, impact-focused, helpful investors! We are traveling a hard and uncertain path, but we have designed our structure to help us positively affect the world should we succeed in creating AGI—which we think will have as broad impact as the computer itself and improve healthcare, education, scientific research, and many aspects of people's lives.  If you’d like to help us make this mission a reality, we’re hiring :)!", "date": "2019-03-11"},
{"website": "Open-AI", "title": "openai-five-defeats-dota-2-world-champions", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-five-defeats-dota-2-world-champions/", "abstract": "OpenAI Five is the first AI to beat the world champions in an esports game, having won two back-to-back games versus the world champion Dota 2 team, OG , at Finals this weekend. Both OpenAI Five and DeepMind's AlphaStar had previously beaten good pros privately but lost their live pro matches, making this also the first time an AI has beaten esports pros on livestream. At OpenAI Five Finals, we also shared two surprises: We started OpenAI Five in order to work on a problem that felt outside of the reach of existing deep reinforcement learning [1] algorithms. We hoped that by working on a problem that was unsolvable by current methods, we'd need to make a big increase in the capability of our tools. We were expecting to need sophisticated algorithmic ideas, such as hierarchical reinforcement learning, but we were surprised by what we found: the fundamental improvement we needed for this problem was scale. Achieving and utilizing that scale wasn't easy and was the bulk of our research effort! To build OpenAI Five, we created a system called Rapid which let us run PPO at previously unprecedented scale . The results exceeded our wildest expectations, and we produced a world-class Dota bot without hitting any fundamental performance limits. The surprising power of today's RL algorithms comes at the cost of massive amounts of experience, which can be impractical outside of a game or simulated environment. This limitation may not be as bad as sounds—for example, we used Rapid to control a robotic hand to dexterously reorient a block, trained entirely in simulation and executed on a physical robot. But we think decreasing the amount of experience is a next challenge for RL. We are retiring OpenAI Five as a competitor today, but progress made and technology developed will continue to drive our future work. This isn't the end of our Dota work—we think that Dota is a much more intrinsically interesting and difficult (and now well-understood!) environment for RL development than the standard ones used today. OpenAI Five's victories on Saturday, as compared to its losses at The International 2018, are due to a major change: 8x more training compute. In many previous phases of the project, we'd drive further progress by increasing our training scale. But after The International, we'd already dedicated the vast majority of our project's compute to training a single OpenAI Five model. So we increased the scale of compute in the only way available to us: training for longer. In total, the current version of OpenAI Five has consumed 800 petaflop/s-days and experienced about 45,000 years of Dota self-play over 10 realtime months (up from about 10,000 years over 1.5 realtime months as of The International), for an average of 250 years of simulated experience per day. The Finals version of OpenAI Five has a 99.9% winrate versus the TI version. [2] The current version of OpenAI Five has been training continuously since June 2018, despite changes to the model size and the game rules (including some fairly large game patch updates and newly implemented features). In each case, we were able to transfer the model over and continue training—something that is an open challenge for RL in other domains. To the best of our knowledge, this is the first time an RL agent has been trained using such a long-lived training run. To make this work, we've continued to flesh out our surgery tooling so that we can start from trained parameters even across substantial architecture changes. We saw very little slowdown in training going from 5 to 18 heroes. We hypothesized the same would be true going to even more heroes, and after The International, we put a lot of effort into integrating new ones. We spent several weeks training with hero pools up to 25 heroes, bringing those heroes to approximately 5k MMR (about 95th percentile of Dota players). Although they were still improving, they weren't learning fast enough to reach pro level before Finals. We haven't yet had time to investigate why, but our hypotheses range from insufficient model capacity to needing better matchmaking for the expanded hero pool to requiring more training time for new heroes to catch up to old heroes. Imagine how hard it is for a human to learn a new hero when everyone else has mastered theirs! We believe these issues are fundamentally solvable, and solving them could be interesting in its own right. The Finals version plays with 17 heroes—we removed Lich because his abilities were changed significantly in Dota version 7.20. It actually felt nice; my Viper gave his life for me at some point. He tried to help me, thinking \"I'm sure she knows what she's doing\" and then obviously I didn't. But, you know, he believed in me. I don't get that a lot with [human] teammates. — Sheever OpenAI Five's ability to play with humans presents a compelling vision for the future of human-AI interaction, one where AI systems collaborate and enhance the human experience. Our testers reported feeling supported by their bot teammates, that they learned from playing alongside these advanced systems, and that it was generally a fun experience overall. Note that OpenAI Five exhibits zero-shot transfer learning—it was trained to have all heroes controlled by copies of itself, but generalizes to controlling a subset of heroes, playing with or against humans. We were very surprised this worked as well as it did. In fact, we'd considered doing a cooperative match at The International but assumed it'd require dedicated training. We’re launching OpenAI Five Arena, a public experiment where we'll let anyone play OpenAI Five in both competitive and cooperative modes. We'd known that our 1v1 bot would be exploitable through clever strategies; we don't know to what extent the same is true of OpenAI Five, but we're excited to invite the community to help us find out! Arena opens Thursday, April 18th at 6pm PST and will close 11:59pm PST on Sunday, April 21st. Please register so we can ensure there's enough server capacity in your region! Results of all games will be automatically reported to the Arena public leaderboard. We're incredibly grateful for all the support the Dota community has shown us over the past two years, and we hope that Arena will also serve as one small way of giving back. Have fun with it! We will be releasing a more technical analysis of OpenAI Five once we've reviewed the outcomes of OpenAI Five Arena. Afterwards, we'll continue working with the Dota 2 environment within OpenAI. We've seen rapid progress in the past two years on RL capabilities, and we think that Dota 2 will continue to help us push forward what's possible—whether with achieving competent performance from less data or true human-AI cooperation. If you are interested in advancing AI capabilities and helping further our mission of ensuring they benefit humanity, we're hiring ! OG Game 1: Replay , OpenAI Five planning view OG Game 2: Replay , OpenAI Five planning view Coop Game: Replay , OpenAI Five planning view Planning View Legend Deep reinforcement learning is the idea of training a deep neural network to achieve goals using rewards and punishments ↩︎ Winrate evaluated on the current game patch. This biases the winrate towards the Finals version as the TI version was trained on an older patch, but currently we don't have another way to compare agents trained on different game versions. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-04-15"},
{"website": "Open-AI", "title": "introducing-activation-atlases", "author": ["Chris Olah", "Ludwig Schubert"], "link": "https://openai.com/blog/introducing-activation-atlases/", "abstract": "We’ve created activation atlases (in collaboration with Google researchers), a new technique for visualizing what interactions between neurons can represent. As AI systems are deployed in increasingly sensitive contexts, having a better understanding of their internal decision-making processes will let us identify weaknesses and investigate failures. Modern neural networks are often criticized as being a “black box.” Despite their success at a variety of problems, we have a limited understanding of how they make decisions internally. Activation atlases are a new way to see some of what goes on inside that box. Activation atlases build on feature visualization, a technique for studying what the hidden layers of neural networks can represent. Early work in feature visualization primarily focused on individual neurons . By collecting hundreds of thousands of examples of neurons interacting and visualizing those, activation atlases move from individual neurons to visualizing the space those neurons jointly represent. Understanding what’s going on inside neural nets isn’t solely a question of scientific curiosity — our lack of understanding handicaps our ability to audit neural networks and, in high stakes contexts, ensure they are safe. Normally, if one was going to deploy a critical piece of software one could review all the paths through the code, or even do formal verification, but with neural networks, our ability to do this kind of review has presently been much more limited. With activation atlases humans can discover unanticipated issues in neural networks — for example, places where the network is relying on spurious correlations to classify images, or where re-using a feature between two classes leads to strange bugs. Humans can even use this understanding to “ attack ” the model, modifying images to fool it. For example, a special kind of activation atlas can be created to show how a network tells apart frying pans and woks. Many of the things we see are what one expects. Frying pans are more squarish, while woks are rounder and deeper. But it also seems like the model has learned that frying pans and woks can also be distinguished by food around them — in particular, wok is supported by the presence of noodles. Adding noodles to the corner of the image will fool the model 45% of the time! This is similar to work like adversarial patches , but based on human understanding. Other human-designed attacks based on the network overloading certain feature detectors are often more effective (some succeed as often as 93% of the time). But the noodle example is particularly interesting because it’s a case of the model picking up on something that is correlated, but not causal, with the correct answer. This has structural similarities to types of errors we might be particularly worried about, such as fairness and bias issues. Activation atlases worked better than we anticipated and seem to strongly suggest that neural network activations can be meaningful to humans. This gives us increased optimism that it is possible to achieve interpretability in vision models in a strong sense. We’re excited to have done this work in collaboration with researchers at Google. We believe that working together on safety-relevant research helps us all ensure the best outcome for society as AI research progresses. Want to make neural networks not be a black box? Apply to work at OpenAI. Thanks to our co-authors at Google: Shan Carter, Zan Armstrong and Ian Johnson. Thanks to Greg Brockman, Dario Amodei, Jack Clark and Ashley Pilipiszyn for feedback on this blog post. We also thank Christian Howard for his help in coordination from the Google side, Phillip Isola for being Distill’s acting editor and Arvind Satyanarayan for feedback on our paper.", "date": "2019-03-06"},
{"website": "Open-AI", "title": "openai-five-benchmark-results", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-five-benchmark-results/", "abstract": "Yesterday, OpenAI Five won a best-of-three against a team of 99.95th percentile Dota players: Blitz , Cap , Fogged , Merlini , and MoonMeander —four of whom have played Dota professionally—in front of a live audience and 100,000 concurrent livestream viewers. The human team won game three after the audience adversarially selected Five's heroes. We also showed our preliminary work to introspect Five's view of the game, including its probability of winning, which made predictions surprising to the human observers. These results show that Five is a step towards advanced AI systems which can handle the complexity and uncertainty of the real world . In case you missed it: the livestream from the Benchmark commentated by Purge and ODPixel . Christy and Greg also both livetweeted the event. The day began with a team of volunteers from the audience bravely playing the first public match against OpenAI Five. Five won within the first 14 minutes (an evenly-matched game generally takes 45 minutes). We revealed a new OpenAI Five capability — the ability to draft . Drafting is considered an extremely challenging part of Dota, since heroes interact with each other in complex ways. In late June we added a win probability output to our neural network to introspect what OpenAI Five is predicting. When later considering drafting, we realized we could use this to evaluate the win probability of any draft: just look at the prediction on the first frame of a game with that lineup. In one week of implementation, we crafted a fake frame for each of the 11 million possible team matchups and wrote a tree search to find OpenAI Five's optimal draft. After the game 1 draft, OpenAI Five predicted a 95% win probability, even though the matchup seemed about even to the human observers. It won the first game in 21 minutes and 37 seconds. After the game 2 draft, OpenAI Five predicted a 76.2% win probability, and won the second in 24 minutes and 53 seconds. For the third game, we asked the audience to draft OpenAI Five's heroes. As expected , they selected an adversarial lineup. The line-up for OAI5 this round is fairly Looney-Tunes. Two big scary tanks, Sven and Axe, with two good invisibility / ganker (surprise attack) heroes, Slark and Riki, and the Queen of Pain who can blink (teleport a few metres) for escape and attack. Before the game began, OpenAI Five predicted a 2.9% chance of winning. Five played on despite the bad odds, and at one point made enough progress to predict a 17% win probability, before ultimately losing after 35 minutes and 47 seconds. Our usual development cycle is to train each major revision of the system from scratch. However, this version of OpenAI Five contains parameters that have been training since June 9th across six major system revisions. Each revision was initialized with parameters from the previous one. We invested heavily in \"surgery\" tooling which allows us to map old parameters to a new network architecture. For example, when we first trained warding, we shared a single action head for determining where to move and where to place a ward. But Five would often drop wards seemingly in the direction it was trying to go, and we hypothesized it was allocating its capacity primarily to movement. Our tooling let us split the head into two clones initialized with the same parameters. We estimate that we used the following amounts of compute to train our various Dota systems: 1v1 model: 8 petaflop/s-days June 6th model: 11 petaflop/s-days [1] Aug 5th model: 35 petaflop/s-days [1:1] We are also releasing our latest network architecture . We can get some insight into the model's planning via an output which predicts where a hero will be in the future. In the following video, the highlighted boxes show the predicted location of Sven in 6 seconds: We can also train outputs to predict various other quantities — last hits, tower counts, and the like: Making our model function requires working through many bugs and unexpected behaviors. Here are some examples: These results give us confidence in moving to the next phase of this project: playing a team of professionals at The International later this month. We will announce details of the games once they are confirmed — follow us on Twitter to stay up to date! We revised these numbers after a more rigorous analysis (4/14/19) ↩︎ ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2018-08-06"},
{"website": "Open-AI", "title": "ai-safety-needs-social-scientists", "author": ["Geoffrey Irving", "Amanda Askell"], "link": "https://openai.com/blog/ai-safety-needs-social-scientists/", "abstract": "We've written a paper arguing that long-term AI safety research needs social scientists to ensure AI alignment algorithms succeed when actual humans are involved. Properly aligning advanced AI systems with human values requires resolving many uncertainties related to the psychology of human rationality, emotion, and biases. The aim of this paper is to spark further collaboration between machine learning and social science researchers, and we plan to hire social scientists to work on this full time at OpenAI. The goal of long-term artificial intelligence (AI) safety is to ensure that advanced AI systems are aligned with human values — that they reliably do things that people want them to do.  At OpenAI we hope to achieve this by asking people questions about what they want, training machine learning (ML) models on this data, and optimizing AI systems to do well according to these learned models.  Examples of this research include Learning from Human Preferences , AI Safety via Debate , and Learning Complex Goals with Iterated Amplification . Unfortunately, human answers to questions about their values may be unreliable.  Humans have limited knowledge and reasoning ability, and exhibit a variety of cognitive biases and ethical beliefs that turn out to be inconsistent on reflection. We anticipate that different ways of asking questions will interact with human biases in different ways, producing higher or lower quality answers. For example, judgments about how wrong an action is can vary depending on whether the word “morally” appears in the question , and people can make inconsistent choices between gambles if the task they are presented with is complex . We have several methods that try to target the reasoning behind human values, including amplification and debate , but do not know how they behave with real people in realistic situations.  If a problem with an alignment algorithm appears only in natural language discussion of a complex value-laden question, current ML may be too weak to uncover the issue. To avoid the limitations of ML, we propose experiments that consist entirely of people, replacing ML agents with people playing the role of those agents.  For example, the debate approach to AI alignment involves a game with two AI debaters and a human judge; we can instead use two human debaters and a human judge.  Humans can debate whatever questions we like, and lessons learned in the human case can be transferred to ML. These human-only experiments will be motivated by machine learning algorithms but will not involve any ML systems or require an ML background.  They will require careful experimental design to build constructively on existing knowledge about how humans think. Most AI safety researchers are focused on machine learning, which we do not believe is sufficient background to carry out these experiments. To fill the gap, we need social scientists with experience in human cognition, behavior, and ethics, and in the careful design of rigorous experiments. Since the questions we need to answer are interdisciplinary and somewhat unusual relative to existing research, we believe many fields of social science are applicable, including experimental psychology, cognitive science, economics, political science, and social psychology, as well as adjacent fields like neuroscience and law. We believe close collaborations between social scientists and machine learning researchers will be necessary to improve our understanding of the human side of AI alignment.  As a first step, several OpenAI researchers helped organize a workshop at Stanford University's Center for Advanced Study in the Behavioral Sciences (CASBS) led by Mariano-Florentino Cuéllar , Margaret Levi , and Federica Carugati , and we continue to meet regularly to discuss issues around social science and AI alignment. We thank them for their valuable insights and participation in these conversations. Our paper is a call for social scientists in AI safety.  We are in the process of starting this research at OpenAI, and are hiring full time social science researchers to push these experiments forward.  If you are interested in working in this area, please apply !", "date": "2019-02-19"},
{"website": "Open-AI", "title": "openai-summer-fellows-2018", "author": ["Maddie Hall"], "link": "https://openai.com/blog/openai-summer-fellows-2018/", "abstract": "Our first cohort of OpenAI Fellows has concluded, with each Fellow going from a machine learning beginner to core OpenAI contributor in the course of a 6-month apprenticeship. During this time, we’ve seen how expertise in other scientific fields like theoretical physics and bioengineering can yield insights to push AI research forward. All 6 Fellows have authored or contributed to papers and completed projects investigating a novel research idea while embedded in an OpenAI research team. Research projects from our next class of Fellows are underway and we are in the process of selecting our next cohort . We’re also excited to welcome a number of our Fellows to OpenAI as full-time members of our technical staff. Previous Role : Software Developer at Art of Problem Solving, Computer Science/AI undergrad at Stanford University Interesting Learning : \"My previous experience in deep RL came only from personal projects. OpenAI Fellows allowed me to dive headfirst into research, providing the necessary support every step of the way. What stands out most is the phenomenal mentorship I've received. My mentors were always actively engaged in my work, sharing key intuitions and suggesting useful course corrections. At times I became flustered, and they helped me stay grounded. They helped me develop a better sense for which ideas to pursue and which to drop. It's been an exciting journey, and I now feel much better prepared to face new challenges in this field.\" Final Project : Created a procedurally generated game, called CoinRun, to help measure how well trained RL agents can generalize to new environments. By training and testing agents on distinct sets of generated levels, we found that agents overfit to surprisingly large training sets. We then showed how we can significantly reduce overfitting by using deeper convolutional architectures, and by using techniques common in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization. What's Next : Joining the Games team at OpenAI, continuing to investigate generalization in RL. Previous Roles : Theoretical Physics PhD at Stanford University, Postdoctoral Researcher at Boston University Interesting Learning : \"The friendly and open culture here at OpenAI allowed me to become quickly acquainted with a wide variety of cutting-edge research by learning from the experts.  I was surprised to learn that despite the rapid pace of progress in the field, there are many seemingly-basic questions about how current techniques work that have yet to be answered.\" Final Project : Studied the use of large amounts of computing hardware for distributed neural network training. We found predictable patterns across a wide variety of machine learning tasks, ranging from MNIST to Dota. What's Next : Joining OpenAI's Safety Team to continue studying the 'Science of AI'. Previous Role : Undergrad at MIT Interesting Learning : \"I found that it is important to be determined and keep pushing at an idea if you think it should work. Early on, I found a pretty nice connection between the way we were training energy based models and GAN training.  Based on this connection and flexibility of models,  I wanted to get the models to work, but for the next two and a half months, everything I tried got junk on CIFAR-10.  Later on, just by adding a replay buffer, my samples just suddenly became fairly good.\" Final Project : My first project was on exploring how to integrate dynamics knowledge into deep reinforcement learning. I explored ways to integrate and transfer dynamics information learned from videos and previous environments into new environments. I also explored and developed a new architecture for better long term physics predictions. My second project was on exploring how to scale and stabilize training of energy based models. With these tricks, I found that energy based models generated much better samples than other state-of-the-art likelihood models. I found that energy based models exhibited good likelihoods model and are able to inpaint and restore test CIFAR-10 samples. I further found that energy based models generalized well, showing state-of-the-art out-of-distribution generalization, compositional ability, and lower long term trajectory prediction error. What's Next : Returning to MIT to finish undergraduate studies. Previous Role : Chemistry & Computer Science undergrad at Harvard University Interesting Learning : \"I learned that significant advancements can be made by cleverly combining various cutting-edge ideas. For example, in my project, I combined three recent OpenAI discoveries to make progress on the long-standing RL transfer problem: fine-tuning language models, scaling transformer networks, and advancing RL with PPO.\" Final Project : It is well known that RL agents struggle with transferring knowledge between tasks. In my project, I discovered that pre-training neural networks to model the environment leads to increased sample efficency and better transfer across tasks. To do this, we pre-train a large generative model on unsupervised observations and then fine-tune the model on-policy using PPO. What's Next : Moving to NYC to join Facebook AI Research. Previous Role : Theoretical Physics PhD, Postdoc at Harvard, Applied researcher and software engineering in a Quantum Computing startup Interesting Learning : \"OpenAI Fellows allowed me to study in a structured fashion, what amazing insights go into advancing the field and the difficulty of disentangling the factors that led to the improvements. Thanks to my mentor and colleagues, I was able to quickly dive into state-of-the-art architectures of generative models and I was, at times, stunned to see how challenging it can be to even reproduce current research results. Consequently, I started to study easier datasets to build intuition and falsify/verify my expectations. The main learning, I take away from the fellowship, is that due to the complexity of the problems it is important to gain basic understanding about challenges an algorithm or approach faces -- as Feynman put it: 'what I cannot create, I do not understand'.\" Final Project : Generative models, i.e. models that learn the distribution of real-world datasets and allow the generation of new samples from this distribution, are becoming increasingly more powerful. During my project, I focused specifically on Normalizing Flow models, which approximate the data-distribution using a continuous deformation of a simpler distribution. A more visual analogue of this is a piece of play-dough that gets stretched, squeezed, bent or anything else, except that it cannot be glued to itself or torn apart. As a consequence of this properties, I was able to create and study artificial datasets that are intrinsically hard to approximate with these models. These can be used to benchmark future generations of generative models for their flexibility and expressivity. What's Next : Joining OpenAI's Algorithm team to continue studying generative models. Previous Role : Computer Science undergrad at UC Berkeley Interesting Learning : \"When it comes to large-scale reinforcement learning and highly experimental fields, I learned that mitigating noise is key. Too often on complex datasets like Sonic would I get noisy results from which I couldn't get a good conclusion, and eventually I got better results by either improving workflow or using less noisy environments. I also learned to trust in my theoretical intuition even in such an experimental field, which led to pretty good analysis on what was happening in overfitting in RL, through the lens of sample complexity, optimization landscapes, and other theoretical ideas. I think combining the concepts from CS theory with experimental evidence will be very valuable in the future. Also, never Git Pull someone else's repo when it comes to research code!\" Final Project : We analyze from an optimization and synthetic point of view, which types of overfitting occur in reinforcement learning. While the majority of our analysis comes from observation overfitting, we also analyzed other reasons overfitting may occur, and what happens in the optimization landscape that affect generalization gaps. We also present (with Joshua Meier and others) the performance of certain state of the art methods on Sonic The Hedgehog, including (but not limited to) generative modelling and special architectures, and why they may fail on large datasets such as Sonic. What's Next : Joining Google Research/Brain. We’d like to congratulate our Summer 2018 Fellows on their outstanding work and thank them for their contributions to OpenAI. We are excited to see what research they publish next! As part of our effort to educate and attract more people like our class of Fellows, we recently open sourced part of their introductory curriculum. You can start your ML education today by completing our tutorial, “ Spinning up in Deep RL .” Spinning up consists of crystal-clear examples of RL code, educational exercises, documentation , and tutorials that will help you become a skilled practitioner in RL. Applications for our 2019 Winter Fellows Cohort have closed—please stay tuned for our next call for applications later in 2019.", "date": "2018-12-19"},
{"website": "Open-AI", "title": "openai-five-benchmark", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-five-benchmark/", "abstract": "The OpenAI Five Benchmark match is now over ! We've removed the most significant restrictions on OpenAI Five 's gameplay—namely, wards, Roshan, and mirror match of fixed heroes, and will soon benchmark our progress by playing 99.95th-percentile Dota players. The OpenAI Five Benchmark match will be held 12:30pm Pacific Time on August 5th in San Francisco. The human team will include Blitz , Cap , Fogged , and Merlini , some of whom are former professionals. The games will be streamed on our Twitch channel and casted by popular casters Purge and ODPixel . Last year our preliminary Dota system defeated the world's top professionals at the 1v1 version of Dota; last month OpenAI Five started defeating amateur teams at the full game of Dota (with some restrictions ). This event will show whether we have any hope of reaching the level of top professionals by The International at the end of August. We'll kick off the OpenAI Five Benchmark with some warmup games against audience members; please let us know on the invite request form if you'd like to volunteer. Because our training system Rapid is very general, we were able to teach OpenAI Five many complex skills since June simply by integrating new features and randomizations. Many people pointed out that wards and Roshan were particularly important to include — and now we’ve done so. We've also increased the hero pool to 18 heroes. Many commenters thought these improvements would take another year . We'll see how well these new game mechanics work on August 5th! We will play with the following restrictions (the crossed out restrictions are those lifted since the original OpenAI Five blog post), which correspond to the last bits of the game we haven't integrated: Pool of 18 heroes ( Axe , Crystal Maiden , Death Prophet , Earthshaker , Gyrocopter , Lich , Lion , Necrophos , Queen of Pain , Razor , Riki , Shadow Fiend , Slark , Sniper , Sven , Tidehunter , Viper , or Witch Doctor ) Mirror match of Necrophos , Sniper , Viper , Crystal Maiden , and Lich No Divine Rapier , Bottle , Quelling Blade , Boots of Travel , Tome of Knowledge , Infused Raindrop No summons / illusions 5 invulnerable couriers, no exploiting them by scouting or tanking No Scan No warding No Roshan No invisibility (consumables and relevant items) We've increased the reaction time of OpenAI Five from 80ms to 200ms. This reaction time is much closer to human level, though we haven't seen evidence of changes in gameplay as OpenAI Five's strength comes more from teamwork and coordination than reflexes. The final date for the match is August 5th, and will run as follows: 12pm PT: doors open 12:30pm PT: livestream begins with \"for fun\" audience games 1:15pm PT: main event begins: intro and best-of-three versus 99.95th percentile Dota players If you'd like to attend the match in person in San Francisco, please request an invite by Friday 7/20 at 11:59p PT; invites will be sent by the end of Monday 7/23. Our venue is limited to 300 attendees, so we'll be selecting invitees based on their answers to the request form. If you've already requested an invite, you'll receive an email linking to this form as well.", "date": "2018-07-18"},
{"website": "Open-AI", "title": "spinning-up-in-deep-rl", "author": ["Joshua Achiam"], "link": "https://openai.com/blog/spinning-up-in-deep-rl/", "abstract": "We’re releasing Spinning Up in Deep RL, an educational resource designed to let anyone learn to become a skilled practitioner in deep reinforcement learning. Spinning Up consists of crystal-clear examples of RL code, educational exercises, documentation, and tutorials. At OpenAI, we believe that deep learning generally—and deep reinforce­ment learning specifically—will play central roles in the development of powerful AI technology. While there are numerous resources available to let people quickly ramp up in deep learning, deep reinforcement learning is more challenging to break into. We've designed Spinning Up to help people learn to use these technologies and to develop intuitions about them. We were inspired to build Spinning Up through our work with the OpenAI Scholars and Fellows initiatives, where we observed that it's possible for people with little-to-no experience in machine learning to rapidly ramp up as practitioners, if the right guidance and resources are available to them. Spinning Up in Deep RL was built with this need in mind and is integrated into the curriculum for 2019 cohorts of Scholars and Fellows. We've also seen that being competent in RL can help people participate in interdisciplinary research areas like AI safety , which involve a mix of reinforcement learning and other skills. We've had so many people ask for guidance in learning RL from scratch, that we've decided to formalize the informal advice we've been giving. Spinning Up in Deep RL consists of the following core components: A short introduction to RL terminology, kinds of algorithms, and basic theory. An essay about how to grow into an RL research role. A curated list of important papers organized by topic. A well-documented code repo of short, standalone implementations of: Vanilla Policy Gradient (VPG), Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Deep Deterministic Policy Gradient (DDPG), Twin Delayed DDPG (TD3), and Soft Actor-Critic (SAC). And a few exercises to serve as warm-ups. We have the following support plan for this project: High-bandwidth software support period : For the first three weeks following release we'll move quickly on bug-fixes,  installation issues, and resolving errors or ambiguities in the docs. We’ll work hard to streamline the user experience, in order to make it as easy as possible to self-study with Spinning Up. Major review in April, 2019 : Approximately six months after release, we’ll do a serious review of the state of the package based on feedback we receive from the community, and announce any plans for future modification. Public release of internal development : If we make changes to Spinning Up in Deep RL as we work with our Scholars and Fellows, we’ll push the changes to the public repo and make them immediately available to everyone. Spinning Up in Deep RL is part of a new education initiative at OpenAI which we’re ‘spinning up’ to ensure we fulfill one of the tenets of the OpenAI Charter : \"seek to create a global community working together to address AGI’s global challenges\". We hope Spinning Up will allow more people to become familiar with deep reinforcement learning, and use it to help advance safe and broadly beneficial AI. We're going to host a workshop on Spinning Up in Deep RL at OpenAI San Francisco on February 2nd 2019 . The workshop will consist of 3 hours of lecture material and 5 hours of semi-structured hacking, project-development, and breakout sessions - all supported by members of the technical staff at OpenAI. Ideal attendees have software engineering experience and have tinkered with ML but no formal ML experience is required. If you're interested in participating please complete our short application here . The application will close on December 8th 2018, and acceptances will be sent out on December 17th 2018. If you want to help us push the limits of AI while communicating with and educating others, then consider applying to work at OpenAI . We’re also going to work with other organizations to help us educate people using these materials. For our first partnership, we’re working with the Center for Human-Compatible AI (CHAI) at the University of California at Berkeley to run a workshop on deep RL in early 2019, similar to the planned Spinning Up workshop at OpenAI. We hope this will be the first of many. The best way to get a feel for how deep RL algorithms perform is to just run them. With Spinning Up, that’s as easy as: At the end of training, you’ll get instructions on how to view data from the experiments and watch videos of your trained agent. Spinning Up implementations are compatible with Gym environments from the Classic Control , Box2D , or MuJoCo task suites. We’ve designed the code for Spinning Up with newcomers in mind, making it short, friendly, and as easy to learn from as possible. Our goal was to write minimal implementations to demonstrate how the theory becomes code, avoiding the layers of abstraction and obfuscation typically present in deep RL libraries. We favor clarity over modularity—code reuse between implementations is strictly limited to logging and parallelization utilities. Code is annotated so that you always know what’s going on, and is supported by background material (and pseudocode) on the corresponding readthedocs page.", "date": "2018-11-08"},
{"website": "Open-AI", "title": "the-international-2018-results", "author": ["OpenAI"], "link": "https://openai.com/blog/the-international-2018-results/", "abstract": "In contrast to our Benchmark 17 days ago, these games: Were played against significantly better human players Used hero lineups provided by a third party rather than by Five drafting against humans Removed our last major restriction from what most pros consider \"Real Dota\" gameplay. Remarkably, the games were exciting and close — in contrast, all three Benchmark games were very one-sided — showing that even though Five teaches itself Dota from scratch, its playstyle results in incredible gameplay versus the best professionals. Winning is good, but losing shows the amazing skill of top professionals, and helps us compare Five’s play to the best of the best. We're incredibly grateful to everyone in the Dota community for helping to create such a great training ground for AI progress: from motivating Valve to create and evolve the incredibly complex game, to supporting the analysts and professionals who can help us measure our progress, to the excitement we've seen from so many viewers which makes the project so much more fun to work on. The purpose of the games were to showcase the capabilities of Five against the world's best humans, playing games of \"Real Dota\". Going into The International, we weren't sure exactly who we would get to play, as it depended on the availability of people willing to play us on the mainstage. We were grateful to play against teams far stronger than the one at the Benchmark. Five played its first match on Wednesday against paiN Gaming , one of the top 18 Dota 2 teams in the world and were eliminated from The International earlier on in the tournament. PaiN players have won an average of $350,000 in career tournament earnings. The match lasted around 51 minutes (games usually last 45 minutes) and after strong start for the humans, Five regained some ground in the mid-game, before succumbing to various high-level strategic pushes by human players. On Thursday, we played our second game against a team of Chinese superstar players , three of whom had played on a competitive team together. After some exciting back-and-forth teamfights, Five lost after 45 minutes. The average tournament earnings for each of these players were about $1 million. The Benchmark games contained a very impactful restriction which we have now removed: each hero was given its own invulnerable courier (a unit which delivers items to your hero) rather than having a single mortal team courier. The extra couriers led Five to develop its signature high-pressure playstyle, since the couriers constantly delivered regeneration items, allowing Five's heroes to constantly attack towards the enemy's base. During a normal Dota game, heroes at low health would instead have to abandon the attack to heal up. Many observers felt the extra couriers made the games feel like they were watching a game unlike \"Real Dota\". We began training with single courier six days ago (the courier itself, like its predecessors, is scripted). While we expected transitioning to single courier would temporarily decrease Five's performance, community feedback made it clear that single courier gameplay would be much more exciting. We don't believe that the courier change was responsible for the losses. We think we need more training, bugfixes, and to remove the last pieces of scripted logic in our model. As we said in Wednesday's panel , we are looking forward to pushing Five to the next level. These games have set a new high watermark for human vs AI games in Dota, and give us a lot to aspire to. But Five isn't just about Dota — it's about building AI technologies in a safe sandbox which will help us build advanced systems in the future. If you want to help us build these systems and ensure they are safe and will benefit all of humanity, then consider joining OpenAI .", "date": "2018-08-23"},
{"website": "Open-AI", "title": "learning-to-communicate", "author": ["Pieter Abbeel", "Igor Mordatch", "Ryan Lowe", "Jon Gauthier", "Jack Clark"], "link": "https://openai.com/blog/learning-to-communicate/", "abstract": "Our hypothesis is that true language understanding will come from agents that learn words in combination with how they affect the world, rather than spotting patterns in a huge corpus of text. As a first step, we wanted to see if cooperative agents could develop a simple language amongst themselves. We've just released initial results in which we teach AI agents to create language by dropping them into a set of simple worlds, giving them the ability to communicate, and then giving them goals that can be best achieved by communicating with other agents. If they achieve a goal, then they get rewarded. We train them using reinforcement learning and, due to careful experiment design, they develop a shared language to help them achieve their goals. Our approach yields agents that invent a (simple!) language which is grounded and compositional . Grounded means that words in a language are tied to something directly experienced by a speaker in their environment, for example, a speaker forming an association between the word \"tree\" and images or experiences of trees. Compositional means that speakers can assemble multiple words into a sentence to represent a specific idea, such as getting another agent to go to a specific location. To train the agents, we represent the experiment as a cooperative — rather than competitive — multi-agent reinforcement learning problem. The agents exist in a two-dimensional world with simple landmarks, and each agent has a goal. Goals can vary from looking at or moving to a specific location, to encouraging a separate agent to move to a location. Each agent can broadcast messages to the group. Every agent's reward is the sum of the rewards paid out to all agents, encouraging collaboration. At each time step, our RL agents can take two kinds of actions — (i) environment actions, like moving around or looking at things, and (ii) communication actions, like broadcasting a word to all other agents. (Note that though the agents come up with words that we found to correspond to objects and other agents, as well as actions like 'Look at' or 'Go to', to the agents these words are abstract symbols represented by one-hot vector — we label these one-hot vectors with English words that capture their meaning for the sake of interpretability.) Before an agent takes an action, it observes the communications from other agents from the previous time step as well as the locations of all entities and objects in the world. It stores that communication in a private recurrent neural network, giving it a memory for the words it hears. We use discrete communication actions (messages formed of separate, word-like symbols) sent over a differentiable communication channel. A communication channel is differentiable if it allows agents to directly inform each other about what message they should have sent at each time step, by slightly altering their messages to make a positive change in the reward both agents expect to receive. Agents accomplish this by calculating the gradient of future reward with respect to changes in the sent messages (i.e. how much rewards would change with different messages). For example, if one agent realizes that it could have performed a task better if a second agent had sent different information, the first agent can tell the second exactly how to modify its messages to make them as useful as possible. In other words, agents ask the question: 'how should I modify my communication output to get the most communal reward in the future?'. Previous efforts achieved this sort of differentiable communication by having the agents send a vector of real numbers or a continuous approximation to binary values to each other, or used non-differentiable communication and training. We use the Gumbel-Softmax trick, to approximate discrete communication decisions with a continuous representation during training. This gets us the best of both worlds: during training the differentiable channel means agents can rapidly learn how to communicate with each other via using continuous representation, which at the end of training ends up converging on discrete outputs that are more interpretable and show traits like compositionality. In the video that follows, we show how our agents evolve languages to fit the complexity of their situation, with solitary agents not needing to communicate, two agents inventing one-word phrases to coordinate with each other in simple tasks, and three agents composing multiple words in sentences to accomplish more challenging tasks. All research projects have complications ; in this case, our agents frequently invented languages that didn't display the compositional traits we wanted. And even when they succeeded, their solutions had their own idiosyncrasies. The first problem we ran into was the agents' tendency to create a single utterance and intersperse it with spaces to create meaning. This Morse code language was hard to decipher and non-compositional. To correct this, we imposed a slight cost on every utterance and added a preference for achieving the task quickly. This encouraged the agents to use their communication channel concisely, which led to the development of a larger vocabulary. Another issue we faced was agents trying to use single words to encode the meaning of entire sentences. This happened when we gave them the ability to use large vocabularies; they'd eventually create a single utterance that encoded the meaning of an entire sentence such as “red agent, go to blue landmark”. While useful for the agents, this approach requires vocabulary size to grow exponentially with the sentence length and doesn't fit with our broader goal of creating AI that is interpretable to humans.) To deter agents from creating this sort of language we incorporated a preference for compact vocabulary sizes through a preference for using already-popular words, inspired by ideas outlined in The evolution of syntactic communication . We incorporate this by putting a reward for speaking a particular word that is proportional to how frequently that word has been spoken previously. Lastly, we encountered agents inventing landmark references not based on color, but other cues such as spatial relationships. For example, agents would invent words like \"top-most\" or \"left-most\" landmark to refer to locations based on a global 2D coordinate system. While such behavior is very inventive, it is fairly specific to our particular environment implementation, and could cause problems if we substantially changed the geography of the worlds the agents live in. To fix this, we placed agents in an ego-centric coordinate frame (so that there is no single shared coordinate frame). This dealt with the odd directions, and led to them referring to landmarks by their color property. This method of training also works when agents are unable to communicate with each other via text, and have to instead carry out physical actions actions in the simulated environment. In the animations that follow we show agents improvising in this way by pointing or guiding other agents to targets, or in extreme cases pushing sightless agents to their goal. Today, many people have applied machine learning to language-related tasks with great success. Large-scale ML techniques have led to significant advances in translation, verbal reasoning, language understanding, sentence generation, and other areas. All of these approaches work by feeding them extremely large amounts of textual data, from which the systems extract features and discover patterns. While this work has yielded numerous inventions and innovations, it has drawbacks relating to the representational quality of the language that is learned. There's not much indication that if you train a computer on language in this way it will have a deep understanding of how that language is attached to the real world. With our research, we're trying to deal with this grounding problem by training our agents to invent language which is tied to their perception of the world. Computers whose language models are trained without grounding are much like the character trapped in John Searle’s Chinese Room , where they compare incoming text against a kind of dictionary of semantic meaning which has been created through the analysis of large quantities of text. It's unclear how much of an idea these computers have about what the text represents, as they've never left this room and been able to interact with the world the text describes. We hope that this research into growing a language will let us develop machines that have their own language tied to their own lived experience. We think that if we slowly increase the complexity of their environment, and the range of actions the agents themselves are allowed to take, it’s possible they’ll create an expressive language which contains concepts beyond the basic verbs and nouns that evolved here. As the complexity of this invented language increases, it's going to become challenging for us to make these languages interpretable by humans. That's why for our next project, Ryan Lowe and Igor Mordatch are going to investigate ways to connect the invented languages with English via having the agents communicate with English-speaking agents. This will automate the translation of their language into ours. This is an interdisciplinary undertaking, spanning areas of AI, linguistics, and cognitive science, and as part of it we'll be collaborating with researchers at UC Berkeley. If you're interested in developing smarter language models, then consider working at OpenAI . You can find out more information about the technical specifics of our research in this research paper: Emergence of Grounded Compositional Language in Multi-Agent Populations , and more about the motivations for it in: A Paradigm for Situated and Goal-Driven Language Learning .", "date": "2017-03-16"},
{"website": "Open-AI", "title": "welcome-pieter-and-shivon", "author": ["Ilya Sutskever"], "link": "https://openai.com/blog/welcome-pieter-and-shivon/", "abstract": "We have two more team updates. Pieter Abbeel. Pieter is a professor at UC Berkeley specializing in making robots learn. He's been giving us advice since before OpenAI was born, and now he's taking a leave from Berkeley to work with us full-time. Pieter and his lab members have been responsible for some of the most striking advances in robot learning and deep reinforcement learning (RL) in recent years. Together, we will explore ways to combine unsupervised learning with RL, which we believe could address fundamental limitations in today’s RL algorithms. Shivon Zilis. Shivon is a partner at Bloomberg Beta focusing on machine intelligence. She has an extremely broad view of how the space is evolving (and has written extensively on the landscape). We've been going to her for advice for the past few months, and now she's becoming an official advisor to OpenAI. We could not be more excited to work with both. Welcome Pieter and Shivon!", "date": "2016-04-26"},
{"website": "Open-AI", "title": "emergent-tool-use", "author": ["Bowen Baker", "Ingmar Kanitscheider", "Todor Markov", "Yi Wu", "Glenn Powell", "Bob McGrew", "Igor Mordatch"], "link": "https://openai.com/blog/emergent-tool-use/", "abstract": "In our environment, agents play a team-based hide-and-seek game. Hiders (blue) are tasked with avoiding line-of-sight from the seekers (red), and seekers are tasked with keeping vision of the hiders. There are objects scattered throughout the environment that hiders and seekers can grab and lock in place, as well as randomly generated immovable rooms and walls that agents must learn to navigate. Before the game begins, hiders are given a preparation phase where seekers are immobilized to give hiders a chance to run away or change their environment. There are no explicit incentives for agents to interact with objects in the environment; the only supervision given is through the hide-and-seek objective. Agents are given a team-based reward; hiders are given a reward of +1 if all hiders are hidden and -1 if any hider is seen by a seeker. Seekers are given the opposite reward, -1 if all hiders are hidden and +1 otherwise. To confine agent behavior to a reasonable space, agents are penalized if they go too far outside the play area. During the preparation phase, all agents are given zero reward. As agents train against each other in hide-and-seek, as many as six distinct strategies emerge . Each new strategy creates a previously nonexistent pressure for agents to progress to the next stage. Note that there are no direct incentives for agents to interact with objects or to explore; rather, the emergent strategies shown below are a result of the autocurriculum induced by multi-agent competition and the simple dynamics of hide-and-seek. We first show emergent strategies learned in a simple environment with 2 hiders, 2 seekers, 2 boxes, 1 ramp, and a room that has between 1–2 randomly placed doors. Next, we show emergent strategies learned in an environment with far more randomization, including 1–3 hiders, 1–3 seekers, 3–9 blocks, 2 ramps, and randomized rooms and doors. In this full environment, agents go through two more phases of emergent strategy than in the previous simple environment. We use the same training infrastructure and algorithms used to train OpenAI Five and Dactyl . However, in our environment each agent acts independently, using its own observations and hidden memory state. Agents use an entity-centric state-based representation of the world, which is permutation invariant with respect to objects and other agents. Each object is embedded and then passed through a masked residual self attention block, similar to those used in transformers , where the attention is over objects instead of over time. Objects that are not in line-of-sight and in front of the agent are masked out such that the agent has no information of them. Agent policies are trained with self-play and Proximal Policy Optimization . During optimization, agents can use privileged information about obscured objects and other agents in their value function. We found that large scale training was critical in agents progressing through the various stages of emergence. Below we show both the time and number of episodes it takes agents to reach stage 4 (ramp defense) for various batch sizes. We find increasing batch size gives a drastic speedup in wall-clock time to convergence, though doesn’t affect the sample efficiency greatly at or above 32k. However, we found that batch sizes of 8k and 16k never reached stage 4 in the allotted number of episodes. In this work we show evidence that agents learn complex strategies and counterstrategies through a self-supervised autocurriculum in hide-and-seek. Another method to learn skills in an unsupervised manner is intrinsic motivation , which incentivizes agents to explore with various metrics such as model error or state counts. We ran count-based exploration in our environment, in which agents keep an explicit count of states they’ve visited and are incentivized to go to infrequently visited states. The primary modeling choice to tune in this setting is the state representation; for instance, in our first baseline we only include 2-D box positions in the state, such that agents are only incentivized to interact with and move boxes to novel positions. We then compare this to a count-based policy which takes the full state given to the agents that play hide-and-seek. As can be seen, agents trained in hide-and-seek qualitatively center around far more human interpretable behaviors such as shelter construction, whereas agents trained with intrinsic motivation move objects around in a seemingly undirected fashion. Furthermore, as the state space increases in complexity, we find that intrinsic motivation methods have less and less meaningful interactions with the objects in their environment. For this reason, we believe multi-agent competition will be a more scalable method for generating human-relevant skills in an unsupervised manner as environments continue to increase in size and complexity. In the previous section, we qualitatively compare behaviors learned in hide-and-seek to those learned with intrinsic motivation. However, as environments increase in scale, so will the difficulty in qualitatively measuring progress. Tracking reward is an insufficient evaluation metric in multi-agent settings, as it can be ambiguous in indicating whether agents are improving evenly or have stagnated. Metrics like ELO or Trueskill can more reliably measure whether performance is improving relative to previous policy versions or other policies in a population; however, these metrics still do not give insight into whether improved performance is caused by new adaptations or improving previously learned skills. Finally, using environment-specific statistics such as object movement can also be ambiguous (for example, the choice to track absolute movement does not illuminate which direction agents moved), and designing sufficient metrics will become difficult and costly as environments scale. We propose using a suite of domain-specific intelligence tests that target capabilities we believe agents may eventually acquire. Transfer performance in these settings can act as a quantitative measure of representation quality or skill, and we compare against pretraining with count-based exploration as well as a trained from scratch baseline. Though the hide-and-seek agent performs better on many of the transfer tasks, it does not drastically improve performance or convergence time. From viewing its behavior, we know it has the latent skill to move objects in a precise manner to construct shelter in the hide-and-seek game; however, it does not have the capability to use this skill in other contexts when trained with a low number of samples. We believe the cause for the mixed transfer results is rooted in agents learning skill representations that are entangled and difficult to fine-tune. As future environments become more diverse and agents must use skills in more contexts, we believe we will see more generalizable skill representations and more significant signal in this evaluation approach. We additionally open-source the evaluation tasks as a way to evaluate learning progress in our environment. We’ve shown that agents can learn sophisticated tool use in a high fidelity physics simulator; however, there were many lessons learned along the way to this result. Building environments is not easy and it is quite often the case that agents find a way to exploit the environment you build or the physics engine in an unintended way. We've provided further evidence that human-relevant strategies and skills, far more complex than the seed game dynamics and environment, can emerge from multi-agent competition and standard reinforcement learning algorithms at scale. These results inspire confidence that in a more open-ended and diverse environment, multi-agent dynamics could lead to extremely complex and human-relevant behavior. If you are interested in helping us build future environments, we’re hiring .", "date": "2019-09-17"},
{"website": "Open-AI", "title": "energy-based-models", "author": ["Yilun Du", "Igor Mordatch"], "link": "https://openai.com/blog/energy-based-models/", "abstract": "We've made progress towards stable and scalable training of energy-based models (EBMs) resulting in better sample quality and generalization ability than existing models. Generation in EBMs spends more compute to continually refine its answers and doing so can generate samples competitive with GANs at low temperatures, [1] while also having mode coverage guarantees of likelihood-based models . We hope these findings stimulate further research into this promising class of models. Generative modeling is the task of observing data, such as images or text, and learning to model the underlying data distribution. Accomplishing this task leads models to understand high level features in data and synthesize examples that look like real data. Generative models have many applications in natural language, robotics, and computer vision. Energy-based models represent probability distributions over data by assigning an unnormalized probability scalar (or “energy”) to each input data point. This provides useful modeling flexibility—any arbitrary model that outputs a real number given an input can be used as an energy model. The difficulty however, lies in sampling from these models. To generate samples from EBMs, we use an iterative refinement process based on Langevin dynamics . Informally, this involves performing noisy gradient descent on the energy function to arrive at low-energy configurations ( see paper for more details ). Unlike GANs , VAEs , and Flow-based models , this approach does not require an explicit neural network to generate samples - samples are generated implicitly. The combination of EBMs and iterative refinement have the following benefits: Adaptive computation time . We can run sequential refinement for long amount of time to generate sharp, diverse samples or a short amount of time for coarse less diverse samples. In the limit of infinite time, this procedure is known to generate true samples from the energy model. Not restricted by generator network . In both VAEs and Flow based models, the generator must learn a map from a continuous space to a possibly disconnected space containing different data modes, which requires large capacity and may not be possible to learn. In EBMs, by contrast, can easily learn to assign low energies at disjoint regions. Built-in compositionality . Since each model represents an unnormalized probability distribution, models can be naturally combined through product of experts or other hierarchical models. Adaptive computation time . We can run sequential refinement for long amount of time to generate sharp, diverse samples or a short amount of time for coarse less diverse samples. In the limit of infinite time, this procedure is known to generate true samples from the energy model. Not restricted by generator network . In both VAEs and Flow based models, the generator must learn a map from a continuous space to a possibly disconnected space containing different data modes, which requires large capacity and may not be possible to learn. In EBMs, by contrast, can easily learn to assign low energies at disjoint regions. Built-in compositionality . Since each model represents an unnormalized probability distribution, models can be naturally combined through product of experts or other hierarchical models. We found energy-based models are able to generate qualitatively and quantitatively high-quality images, especially when running the refinement process for a longer period at test time. By running iterative optimization on individual images, we can auto-complete images and morph images from one class (such as truck) to another (such as frog). In addition to generating images, we found that energy-based models are able to generate stable robot dynamics trajectories across large number of timesteps. EBMs can generate a diverse set of possible futures, while feedforward models collapse to a mean prediction. We tested energy-based models on classifying several different out-of-distribution datasets and found that energy-based models outperform other likelihood models such as Flow based and autoregressive models. We also tested classification using conditional energy-based models, and found that the resultant classification exhibited good generalization to adversarial perturbations. Our model—despite never being trained for classification—performed classification better than models explicitly trained against adversarial perturbations . We found evidence that suggest the following observations, though in no way are we certain that these observations are correct: We found it difficult to apply vanilla HMC to EBM training as optimal step sizes and leapfrog simulation numbers differ greatly during training, though applying adaptive HMC would be an interesting extension. We found training ensembles of energy functions (sampling and evaluating on ensembles) to help a bit, but was not worth the added complexity. We didn’t ﬁnd much success adding a gradient penalty term, as it seemed to hurt model capacity and sampling. More tips, observations and failures from this research can be found in Section A.8 of the paper . We found preliminary indications that we can compose multiple energy-based models via a product of experts model. We trained one model on different size shapes at a set position and another model on same size shape at different positions. By combining the resultant energy-based models, we were able to generate different size shapes at different locations, despite never seeing examples of both being changed. Compositionality is one of the unsolved challenges facing AI systems today, and we are excited about what energy-based models can do here. If you are excited to work on energy-based models please consider applying to OpenAI! Thanks to Ilya Sutskever, Greg Brockman, Bob McGrew, Johannes Otterbach, Jacob Steinhardt, Harri Edwards, Yura Burda, Jack Clark and Ashley Pilipiszyn for feedback on this blog post and manuscript. See Equation 2 in this paper . ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-03-21"},
{"website": "Open-AI", "title": "learning-to-cooperate-compete-and-communicate", "author": ["Ryan Lowe", "Igor Mordatch", "Pieter Abbeel", "Yi Wu", "Aviv Tamar", "Jean Harb"], "link": "https://openai.com/blog/learning-to-cooperate-compete-and-communicate/", "abstract": "Multiagent environments where agents compete for resources are stepping stones on the path to AGI. Multiagent environments have two useful properties: first, there is a natural curriculum—the difficulty of the environment is determined by the skill of your competitors (and if you're competing against clones of yourself, the environment exactly matches your skill level). Second, a multiagent environment has no stable equilibrium: no matter how smart an agent is, there's always pressure to get smarter. These environments have a very different feel from traditional environments, and it'll take a lot more research before we become good at them. We've developed a new algorithm, MADDPG , for centralized learning and decentralized execution in multiagent environments, allowing agents to learn to collaborate and compete with each other. MADDPG extends a reinforcement learning algorithm called DDPG , taking inspiration from actor-critic reinforcement learning techniques; other groups are exploring variations and parallel implementations of these ideas. We treat each agent in our simulation as an \"actor\", and each actor gets advice from a \"critic\" that helps the actor decide what actions to reinforce during training. Traditionally, the critic tries to predict the value (i.e. the reward we expect to get in the future) of an action in a particular state, which is used by the agent - the actor - to update its policy. This is more stable than directly using the reward, which can vary considerably. To make it feasible to train multiple agents that can act in a globally-coordinated way, we enhance our critics so they can access the observations and actions of all the agents, as the following diagram shows. Our agents don't need to access the central critic at test time; they act based on their observations in combination with their predictions of other agents behaviors'. Since a centralized critic is learned independently for each agent, our approach can also be used to model arbitrary reward structures between agents, including adversarial cases where the rewards are opposing. We tested our approach on a variety of tasks and it performed better than DDPG on all of them. In the above animations you can see, from left to right: two AI agents trying to go to a specific location and learning to split up to hide their intended location from the opposing agent; one agent communicating the name of a landmark to another agent ; and three agents coordinating to travel to landmarks without bumping into each other. Traditional decentralized RL approaches — DDPG, actor-critic learning, deep Q-learning, and so on — struggle to learn in multiagent environments, as at every time step each agent will be trying to learn to predict the actions of other agents while also taking its own actions. This is especially true in competitive situations. MADDPG employs a centralized critic to supply agents with information about their peers' observations and potential actions, transforming an unpredictable environment into a predictable one. Using policy gradient methods presents further challenges: because these exhibit high variance learning the right policy is difficult to do when the reward is inconsistent. We also found that adding in a critic, while improving stability, still failed to solve several of our environments such as cooperative communication. It seems that considering the actions of others during training is important for learning collaborative strategies. Before we developed MADDPG, when using decentralized techniques, we noticed that listener agents would often learn to ignore the speaker if it sent inconsistent messages about where to go to. The agent would then set all the weights associated with the speaker’s message to 0, effectively deafening itself. Once this happens, it’s hard for training to recover, since the speaker will never know if it says the right thing due to the absence of any feedback. To fix this, we looked at a technique outlined in a recent hierarchical reinforcement project , which lets us force the listener to incorporate the utterances of the speaker in its decision-making process. This fix didn’t work, because though it forces the listener to pay attention to the speaker, it doesn't help the speaker figure out what to say that is relevant. Our centralized critic method helps deal with these challenges, by helping the speaker to learn which utterances might be relevant to the actions of other agents. For more of our results, you can watch the following video: Agent modeling has a rich history within artificial intelligence research and many of these scenarios have been studied before. Lots of previous research considered games with only a small number of time steps with a small state space. Deep learning lets us deal with complex visual inputs and RL gives us tools to learn behaviors over long time periods. Now that we can use these capabilities to train multiple-agents at once without them needing to know the dynamics of the environment (how the environment changes at each time-step), we can tackle a wider range of problems involving communication and language while learning from environments' high-dimensional information. If you’re interesting in exploring different approaches to evolving agents then consider joining OpenAI .", "date": "2017-06-08"},
{"website": "Open-AI", "title": "gpt-2-1-5b-release", "author": ["Irene Solaiman", "Jack Clark", "Miles Brundage"], "link": "https://openai.com/blog/gpt-2-1-5b-release/", "abstract": "As the final model release of GPT-2 ’s staged release , we’re releasing the largest version (1.5B parameters) of GPT-2 along with code and model weights to facilitate detection of outputs of GPT-2 models. While there have been larger language models released since August, we’ve continued with our original staged release plan in order to provide the community with a test case of a full staged release process. We hope that this test case will be useful to developers of future powerful models, and we’re actively continuing the conversation with the AI community on responsible publication. 1. Humans find GPT-2 outputs convincing. Our partners at Cornell University surveyed people to assign GPT-2 text a credibility score across model sizes. People gave the 1.5B model a “credibility score” of 6.91 out of 10. This is marginally greater than outputs from the 774M model (6.72) and significantly above the medium 355M model (6.07). These results make us more inclined to release the 1.5B model, as the incremental increase in human-perceived credibility relative to 774M seems low. 2. GPT-2 can be fine-tuned for misuse. Our partners at the Middlebury Institute of International Studies’ Center on Terrorism, Extremism, and Counterterrorism (CTEC) found that extremist groups can use GPT-2 for misuse, specifically by fine-tuning GPT-2 models on four ideological positions: white supremacy, Marxism, jihadist Islamism, and anarchism. CTEC demonstrated that it’s possible to create models that can generate synthetic propaganda for these ideologies. They also show that, despite having low detection accuracy on synthetic outputs, ML-based detection methods can give experts reasonable suspicion that an actor is generating synthetic text. 3. Detection is challenging. We expect that content-based detection of synthetic text is a long-term challenge. To test whether machine learning approaches may help today, we conducted in-house detection research and developed a detection model that has detection rates of ~95% for detecting 1.5B GPT-2-generated text. [1] We believe this is not high enough accuracy for standalone detection and needs to be paired with metadata-based approaches, human judgment, and public education to be more effective.  We are releasing this model to aid the study of research into the detection of synthetic text, although this does let adversaries with access better evade detection. While we found detection accuracy depends heavily on the sampling methods used in training and testing, we also found detection to be more reliable when training across a range of sampling techniques. As seen in the figure below, we observed that larger models’ outputs are more difficult to classify, but training on larger models’ outputs makes detection results more accurate and robust. We expect this trend to continue and that detection will be more challenging with increased model size. 4. We’ve seen no strong evidence of misuse so far. While we’ve seen some discussion around GPT-2’s potential to augment high-volume/low-yield operations like spam and phishing, we haven’t seen evidence of writing code, documentation, or instances of misuse. We think synthetic text generators have a higher chance of being misused if their outputs become more reliable and coherent. We acknowledge that we cannot be aware of all threats, and that motivated actors can replicate language models without model release. 5. We need standards for studying bias. Language models have biases. Working out how to study these biases, discuss them, and address them, is a challenge for the AI research community. We’ve approached the challenge of bias in two ways: Publishing a model card [2] alongside our models on GitHub to give people a sense of the issues inherent to language models such as GPT-2. Performing a qualitative, in-house evaluation of some of the biases in GPT-2: We probed GPT-2 for some gender, race, and religious biases, using those findings to inform our model card. These probes are not comprehensive and raise the need for collaboration on bias analysis frameworks. Our experience with GPT-2 over the past 9 months has given us valuable insight into the challenges and opportunities for creating responsible publication norms in AI. We’re continuing our work on this issue via participation in the Partnership on AI’s “Responsible Publication Norms for Machine Learning” project and discussions with our colleagues in the research community. If you’d like to develop large-scale AI systems and think about their implications, we’re hiring . Specifically, we based a sequence classifier on RoBERTa BASE (125 million parameters) and RoBERTa LARGE (355 million parameters) and fine-tuned it to classify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we used to train the GPT-2 model. ↩︎ Which we’ve based on “ Model Cards for Model Reporting ” by Mitchell et al. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-11-05"},
{"website": "Open-AI", "title": "neural-mmo", "author": ["Joseph Suarez", "Yilun Du", "Phillip Isola", "Igor Mordatch"], "link": "https://openai.com/blog/neural-mmo/", "abstract": "We're releasing a Neural MMO , a massively multiagent game environment for reinforcement learning agents. Our platform supports a large, variable number of agents within a persistent and open-ended task. The inclusion of many agents and species leads to better exploration, divergent niche formation, and greater overall competence. In recent years, multiagent settings have become an effective platform for deep reinforcement learning research . Despite this progress, there are still two main challenges for multiagent reinforcement learning. We need to create open-ended tasks with a high complexity ceiling: current environments are either complex but too narrow or open-ended but too simple . Properties such as persistence and large population scale are key, but we also need more benchmark environments to quantify learning progress in the presence of large population scales and persistence. The game genre of Massively Multiplayer Online Games (MMOs) simulates a large ecosystem of a variable number of players competing in persistent and extensive environments. To address these challenges, we built our Neural MMO to meet the following criteria: Persistence: Agents learn concurrently in the presence of other learning agents with no environment resets. Strategies must consider long time horizons and adapt to potentially rapid changes in the behaviors of other agents. Scale: The environment supports a large and variable number of entities. Our experiments consider up to 100M lifetimes of 128 concurrent agents in each of 100 concurrent servers. Efficiency: The computational barrier to entry is low. We can train effective policies on a single desktop CPU. Expansion: Similarily to existing MMOs , our Neural MMO is designed to update with new content. Current core features include procedural generation of tile-based terrain, a food and water foraging system, and a strategic combat system. There is an opportunity for open-source driven expansion in the future. Players (agents) may join any available server (environment) , each containing an automatically generated tile-based game map of configurable size. Some tiles, such as food-bearing forest tiles and grass tiles, are traversable. Others, such as water and solid stone, are not. Agents spawn at a random location along the edges of the environment. They must obtain food and water, and avoid combat damage from other agents, in order to sustain their health. Stepping on a forest tile or next to a water tile refills a portion of the agent's food or water supply, respectively. However, forest tiles have a limited supply of food, which regenerates slowly over time. This means that agents must compete for food tiles while periodically refilling their water supply from infinite water tiles. Players engage in combat using three combat styles, denoted Melee, Range, and Mage for flavor. Input: Agents observe a square crop of tiles centered on their current position. This includes tile terrain types and the select properties (health, food, water, and position) of occupying agents. Output: Agents output action choices for the next game tick (timestep) . Actions consist of one movement and one attack. As a simple baseline, we train a small, fully connected architecture using vanilla policy gradients , with a value function baseline and reward discounting as the only enhancements. Instead of rewarding agents for achieving particular objectives, agents optimize only for their lifetime (trajectory length) : they receive reward 1 for each tick of their lifetime. We convert variable length observations, such as the list of surrounding players, into a single length vector by computing the maximum across all players ( OpenAI Five also utilized this trick). The source release includes our full distributed training implementation, which is based on PyTorch and Ray . Agents’ policies are sampled uniformly from a number of populations — agents in different populations share architectures, but only agents in the same population share weights. Initial experiments show that agent competence scales with increasing multiagent interaction. Increasing the maximum number of concurrent players magnifies exploration; increasing the number of populations magnifies niche formation — that is, the tendency of populations to spread out and forage within different parts of the map. There is no standard procedure among MMOs for evaluating relative player competence across multiple servers. However, MMO servers sometimes undergo merges where the player bases from multiple servers are placed within a single server. We implement “tournament” style evaluation by merging the player bases trained in different servers. This allows us to directly compare the policies learned in different experiment settings. We vary test time scale and find that agents trained in larger settings consistently outperform agents trained in smaller settings. In the natural world, competition among animals can incentivize them to spread out to avoid conflict. We observe that map coverage increases as the number of concurrent agents increases. Agents learn to explore only because the presence of other agents provides a natural incentive for doing so. Given a sufficiently large and resource-rich environment, we found different populations of agents separated across the map to avoid competing with others as the populations increased. As entities cannot out-compete other agents of their own population (i.e. agents with whom they share weights), they tend to seek areas of the map that contain enough resources to sustain their population. Similar effects were also independently observed in concurrent multiagent research by DeepMind . We visualize agent-agent dependencies by fixing an agent at the center of a hypothetical map crop. For each position visible to that agent, we show what the value function would be if there were a second agent at that position. We find that agents learn policies dependent on those of other agents, in both the foraging and combat environments. Agents learn “bull's eye” avoidance maps to begin foraging more effectively after only a few minutes of training. As agents learn the combat mechanics of the environment, they begin to appropriately value effective engagement ranges and angles of approach. Our Neural MMO resolves two key limitations of previous game-based environments, but there are still many left unsolved. This Neural MMO strikes a middle ground between environment complexity and population scale . We’ve designed this environment with open-source expansion in mind and for the research community to build upon. If you are excited about conducting research on multiagent systems, consider joining OpenAI. Thanks to Clare Zhu for her substantial work on the 3D client. We also thank the following for feedback on drafts of this post: Greg Brockman, Ilya Sutskever, Jack Clark, Ashley Pilipiszyn, Ryan Lowe, Julian Togelius, Joel Liebo, Cinjon Resnick.", "date": "2019-03-04"},
{"website": "Open-AI", "title": "ingredients-for-robotics-research", "author": ["Matthias Plappert", "Marcin Andrychowicz", "Alex Ray", "Bob McGrew", "Bowen Baker", "Glenn Powell", "Jonas Schneider", "Josh Tobin", "Maciek Chociej", "Peter Welinder", "Vikash Kumar", "Wojciech Zaremba"], "link": "https://openai.com/blog/ingredients-for-robotics-research/", "abstract": "We're releasing eight simulated robotics environments and a Baselines implementation of Hindsight Experience Replay , all developed for our research over the past year. We've used these environments to train models which work on physical robots . We're also releasing a set of requests for robotics research. This release includes four environments using the Fetch research platform and four environments using the ShadowHand robot. The manipulation tasks contained in these environments are significantly more difficult than the MuJoCo continuous control environments currently available in Gym, all of which are now easily solvable using recently released algorithms like PPO . Furthermore, our newly released environments use models of real robots and require the agent to solve realistic tasks. This release ships with eight robotics environments for Gym that use the MuJoCo physics simulator. The environments are: FetchReach-v0 : Fetch has to move its end-effector to the desired goal position. FetchSlide-v0 : Fetch has to hit a puck across a long table such that it slides and comes to rest on the desired goal. FetchPush-v0 : Fetch has to move a box by pushing it until it reaches a desired goal position. FetchPickAndPlace-v0 : Fetch has to pick up a box from a table using its gripper and move it to a desired goal above the table. HandReach-v0 : ShadowHand has to reach with its thumb and a selected finger until they meet at a desired goal position above the palm. HandManipulateBlock-v0 : ShadowHand has to manipulate a block until it achieves a desired goal position and rotation. HandManipulateEgg-v0 : ShadowHand has to manipulate an egg until it achieves a desired goal position and rotation. HandManipulatePen-v0 : ShadowHand has to manipulate a pen until it achieves a desired goal position and rotation. All of the new tasks have the concept of a \"goal\", for example the desired position of the puck in the slide task or the desired orientation of a block in the hand block manipulation task. All environments by default use a sparse reward of -1 if the desired goal was not yet achieved and 0 if it was achieved (within some tolerance). This is in contrast to the shaped rewards used in the old set of Gym continuous control problems, for example Walker2d-v2 with its shaped reward . We also include a variant with dense rewards for each environment. However, we believe that sparse rewards are more realistic in robotics applications and we encourage everyone to use the sparse reward variant instead. Alongside these new robotics environments, we’re also releasing code for Hindsight Experience Replay (or HER for short), a reinforcement learning algorithm that can learn from failure. Our results show that HER can learn successful policies on most of the new robotics problems from only sparse rewards. Below, we also show some potential directions for future research that could further improve the performance of the HER algorithm on these tasks. To understand what HER does, let's look at in the context of FetchSlide , a task where we need to learn to slide a puck across the table and hit a target. Our first attempt very likely will not be a successful one. Unless we get very lucky, the next few attempts will also likely not succeed. Typical reinforcement learning algorithms would not learn anything from this experience since they just obtain a constant reward (in this case: -1 ) that does not contain any learning signal. The key insight that HER formalizes is what humans do intuitively: Even though we have not succeeded at a specific goal, we have at least achieved a different one. So why not just pretend that we wanted to achieve this goal to begin with, instead of the one that we set out to achieve originally? By doing this substitution, the reinforcement learning algorithm can obtain a learning signal since it has achieved some goal; even if it wasn’t the one that we meant to achieve originally. If we repeat this process, we will eventually learn how to achieve arbitrary goals, including the goals that we really want to achieve. This approach lets us learn how to slide a puck across the table even though our reward is fully sparse and even though we may have never actually hit the desired goal early on. We call this technique Hindsight Experience Replay since it replays experience (a technique often used in off-policy RL algorithms like DQN and DDPG ) with goals which are chosen in hindsight, after the episode has finished. HER can therefore be combined with any off-policy RL algorithm (for example, HER can be combined with DDPG, which we write as \"DDPG + HER\"). We've found HER to work extremely well in goal-based environments with sparse rewards. We compare DDPG + HER and vanilla DDPG on the new tasks. This comparison includes the sparse and the dense reward versions of each environment. DDPG + HER with sparse rewards significantly outperforms all other configurations and manages to learn a successful policy on this challenging task only from sparse rewards. Interestingly, DDPG + HER with dense reward is able to learn but achieves worse performance. Vanilla DDPG mostly fails to learn in both cases. We find this trend to be generally true across most environments and we include full results in our accompanying technical report . Though HER is a promising way towards learning complex goal-based tasks with sparse rewards like the robotics environments that we propose here, there is still a lot of room for improvement. Similar to our recently published Requests for Research 2.0 , we have a few ideas on ways to improve HER specifically, and reinforcement learning in general. Automatic hindsight goal creation. We currently have a hard-coded strategy for selecting hindsight goals that we want to substitute. It would be interesting if this strategy could be learned instead. Unbiased HER. The goal substitution changes the distribution of experience in an unprincipled way. This bias can in theory lead to instabilities, although we do not find this to happen in practice. Still, it would be nice to derive an unbiased version of HER, for example by utilizing importance sampling . HER + HRL. It would be interesting to further combine HER with a recent idea in hierarchical reinforcement learning (HRL). Instead of applying HER just to goals, it could also be applied to actions generated by a higher-level policy. For example, if the higher level asked the lower level to achieve goal A but instead goal B was achieved, we could assume that the higher level asked us to achieve goal B originally. Richer value functions. It would be interesting to extend recent research and condition the value function on additional inputs like discount factor or success threshold and (maybe?) also substitute them in hindsight. Faster information propagation. Most off-policy deep reinforcement learning algorithms use target networks to stabilize training. However, since changes need time to propagate, this will limit the speed of training and we have noticed in our experiments that it is often the most important factor determining the speed of DDPG+HER learning. It would be interesting to investigate other means of stabilizing training that do not incur such a slowdown. HER + multi-step returns. The experience used in HER is extremely off-policy since we substitute goals. This makes it hard to use it with multi-step returns . However, multi-step returns are desirable since they allow much faster propagation of information about the returns. On-policy HER. Currently, HER can only be used with off-policy algorithms since we substitute goals, making the experience extremely off-policy. However, recent state of the art algorithms like PPO exhibit very attractive stability traits. It would be interesting to investigate whether HER can be combined with such on-policy algorithms, for example by importance sampling . There are already some preliminary results in this direction. RL with very frequent actions. Current RL algorithms are very sensitive to the frequency of taking actions which is why frame skip technique is usually used on Atari. In continuous control domains, the performance goes to zero as the frequency of taking actions goes to infinity, which is caused by two factors: inconsistent exploration and the necessity to bootstrap more times to propagate information about returns backward in time. How to design a sample-efficient RL algorithm which can retain its performance even when the frequency of taking actions goes to infinity? Combine HER with recent advances in RL. There is a vast body of recent research that improves different aspects of RL. As a start, HER could be combined with Prioritized Experience Replay , distributional RL , entropy-regularized RL , or reverse curriculum generation . You can find additional additional information and references on these proposals and on the on the new Gym environments in our accompanying technical report . Introducing the notion of a \"goal\" requires a few backwards-compatible changes to the existing Gym API : All goal-based environments use a gym.spaces.Dict observation space. Environments are expected to include a desired goal, which the agent should attempt to achieve ( desired_goal ), the goal that it has currently achieved instead ( achieved_goal ), and the actual observation ( observation ), e.g. the state of the robot. We expose the reward function of an environment and thus allow to re-compute a reward with changed goals. This allows for HER-style algorithms, which substitute goals. Here is a simple example that interacts with the one of the new goal-based environments and performs goal substitution: The new goal-based environments can be used with existing Gym-compatible reinforcement learning algorithms, such as Baselines . Use gym.wrappers.FlattenDictWrapper to flatten the dict-based observation space into an array:", "date": "2018-02-26"},
{"website": "Open-AI", "title": "safety-gym", "author": ["Joshua Achiam", "Alex Ray", "Dario Amodei"], "link": "https://openai.com/blog/safety-gym/", "abstract": "We're releasing Safety Gym, a suite of environments and tools for measuring progress towards reinforcement learning agents that respect safety constraints while training. We also provide a standardized method of comparing algorithms and how well they avoid costly mistakes while learning. If deep reinforcement learning is applied to the real world, whether in robotics or internet-based tasks, it will be important to have algorithms that are safe even while learning—like a self-driving car that can learn to avoid accidents without actually having to experience them. Reinforcement learning agents need to explore their environments in order to learn optimal behaviors. Essentially, they operate on the principle of trial and error: they try things out, see what works or doesn’t work, and then increase the likelihood of good behaviors and decrease the likelihood of bad behaviors. However, exploration is fundamentally risky : agents might try dangerous behaviors that lead to unacceptable errors. This is the “safe exploration” problem in a nutshell. Consider an example of an autonomous robot arm in a factory using reinforcement learning (RL) to learn how to assemble widgets. At the start of RL training, the robot might try flailing randomly, since it doesn’t know what to do yet. This poses a safety risk to humans who might be working nearby, since they could get hit. For restricted examples like the robot arm, we can imagine simple ways to ensure that humans aren’t harmed by just keeping them out of harm’s way: shutting down the robot whenever a human gets too close, or putting a barrier around the robot. But for general RL systems that operate under a wider range of conditions, simple physical interventions won’t always be possible, and we will need to consider other approaches to safe exploration. The first step towards making progress on a problem like safe exploration is to quantify it: figure out what can be measured, and how going up or down on those metrics gets us closer to the desired outcome. Another way to say it is that we need to pick a formalism for the safe exploration problem. A formalism allows us to design algorithms that achieve our goals. While there are several options, there is not yet a universal consensus in the field of safe exploration research about the right formalism. We spent some time thinking about it, and the formalism we think makes the most sense to adopt is constrained reinforcement learning. Constrained RL is like normal RL, but in addition to a reward function that the agent wants to maximize, environments have cost functions that the agent needs to constrain. For example, consider an agent controlling a self-driving car. We would want to reward this agent for getting from point A to point B as fast as possible. But naturally, we would also want to constrain the driving behavior to match traffic safety standards. We think constrained RL may turn out to be more useful than normal RL for ensuring that agents satisfy safety requirements. A big problem with normal RL is that everything about the agent’s eventual behavior is described by the reward function, but reward design is fundamentally hard. A key part of the challenge comes from picking trade-offs between competing objectives, such as task performance and satisfying safety requirements. In constrained RL, we don’t have to pick trade-offs—instead, we pick outcomes, and let algorithms figure out the trade-offs that get us the outcomes we want. We can use the self-driving car case to sketch what this means in practice. Suppose the car earns some amount of money for every trip it completes, and has to pay a fine for every collision. In normal RL, you would pick the collision fine at the beginning of training and keep it fixed forever. The problem here is that if the pay-per-trip is high enough, the agent may not care whether it gets in lots of collisions (as long as it can still complete its trips). In fact, it may even be advantageous to drive recklessly and risk those collisions in order to get the pay.  We have seen this before when training unconstrained RL agents . By contrast, in constrained RL you would pick the acceptable collision rate at the beginning of training, and adjust the collision fine until the agent is meeting that requirement. If the car is getting in too many fender-benders, you raise the fine until that behavior is no longer incentivized. To study constrained RL for safe exploration, we developed a new set of environments and tools called Safety Gym. By comparison to existing environments for constrained RL, Safety Gym environments are richer and feature a wider range of difficulty and complexity. In all Safety Gym environments, a robot has to navigate through a cluttered environment to achieve a task. There are three pre-made robots (Point, Car, and Doggo), three main tasks (Goal, Button, and Push), and two levels of difficulty for each task. We give an overview of the robot-task combinations below, but make sure to check out the paper for details. In these videos, we show how an agent without constraints tries to solve these environments. Every time the robot does something unsafe—which here, means running into clutter—a red warning light flashes around the agent, and the agent incurs a cost (separate from the task reward). Because these agents are unconstrained, they often wind up behaving unsafely while trying to maximize reward. To help make Safety Gym useful out-of-the-box, we evaluated some standard RL and constrained RL algorithms on the Safety Gym benchmark suite: PPO , TRPO , Lagrangian penalized versions of PPO and TRPO, and Constrained Policy Optimization (CPO). Our preliminary results demonstrate the wide range of difficulty of Safety Gym environments: the simplest environments are easy to solve and allow fast iteration, while the hardest environments may be too challenging for current techniques. We also found that Lagrangian methods were surprisingly better than CPO, overturning a previous result in the field. Below, we show learning curves for average episodic return and average episodic sum of costs. In our paper , we describe how to use these and a third metric (the average cost over training) to compare algorithms and measure progress. To facilitate reproducibility and future work, we’re also releasing the algorithms code we used to run these experiments as the Safety Starter Agents repo . There is still a lot of work to do on refining algorithms for constrained RL, and combining them with other problem settings and safety techniques. There are three things we are most interested in at the moment: Our expectation is that, in the same way we today measure the accuracy or performance of systems at a given task, we’ll eventually measure the “safety” of systems as well. Such measures could feasibly be integrated into assessment schemes that developers use to test their systems, and could potentially be used by the government to create standards for safety . [1] We also hope that systems like Safety Gym can make it easier for AI developers to collaborate on safety across the AI sector via work on open, shared systems. If you’re excited to work on safe exploration problems with us, we’re hiring ! We gratefully acknowledge the many people who contributed to this release. Thanks Christy Dennison, Ethan Knight, and Adam Stooke for discussions, research, and testing of Safety Gym along the way, and to Mira Murati for supporting the project team. Thanks Karl Cobbe, Matthias Plappert, and Jacob Hilton for feedback on the paper. Thanks Ashley Pilipiszyn, Ben Barry, Justin Jay Wang, Richard Perez, Jen DeRosa, and Greg Brockman for work on editing, designing, illustrating, and shipping this post. Thanks Amanda Askell, Jack Clark, and Miles Brundage for discussions and blog post contributions on measurements for AI safety and policy implications. Thanks Chris Hesse for liaising on open source release guidelines. OpenAI’s comments in response to a request for information from the US agency NIST regarding Artificial Intelligence Standards. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-11-21"},
{"website": "Open-AI", "title": "spinning-up-in-deep-rl-workshop-review", "author": ["Joshua Achiam"], "link": "https://openai.com/blog/spinning-up-in-deep-rl-workshop-review/", "abstract": "On February 2nd, we held our first Spinning Up Workshop as part of our new education initiative at OpenAI. We hosted ~90 people at our office and engaged nearly 300 more through our livestream. Participants came from a wide range of backgrounds, including academia, software engineering, data science, ML engineering, medicine, and education. This workshop built off our Spinning Up in Deep RL resource package and took a deeper dive into RL algorithm design, robotics, and building safe AI systems. One of the goals for education at OpenAI is to help people develop the skills needed to participate in research and development in AI—especially in deep RL, a core area of research at OpenAI. From our experience working with Scholars and Fellows , we’ve found that the key ingredients for skill development are: The challenge for education at OpenAI is to figure out how to deliver these at scale. While sharing a curriculum at scale is relatively easy, it isn’t obvious how to scale up mentorship and guidance on projects. Our working theory is that workshops might help us do just that. Our first Spinning Up workshop has given us several positive signs that this is a useful direction, and we’re excited to share what we learned. We hosted around 90 people at our office and involved nearly 300 more through our livestream. Our guests came from a wide range of backgrounds, including academic research, software engineering, data science, ML engineering, medicine, and education. The level of ML experience varied quite significantly across the group, from “almost none” to “built their own Dota bot!” More than 500 people, from all around the world, applied to participate in this workshop. Although we sadly couldn’t invite everyone to this one because of space constraints, we want to continue engaging the community with future events. The workshop kicked off with three hours of talks. To start us off, Joshua Achiam laid out the conceptual foundations of reinforcement learning and gave an overview of different kinds of RL algorithms. If you’d like to study this material, check out Spinning Up in Deep RL . Matthias Plappert presented on OpenAI’s recent work training a dexterous robot hand in simulation to manipulate objects in the real world. Domain randomization, recurrent neural networks, and large-scale distributed training were necessary ingredients in bridging the “sim2real” gap for this task. Dario Amodei, the leader of the Safety Team at OpenAI, presented an overview of problems in AI safety and recent work in this space. He described the central safety problem: the fact that correctly specifying agent behavior is hard! It is easy to inadvertently give agents incentives to perform different behavior than what you would have wanted, and when agents are very powerful, this could be dangerous. Dario also described work that OpenAI and collaborators at DeepMind have done to address this issue, in which reward functions are learned from human preferences instead of designed. The workshop continued into the afternoon with a semi-structured program of hacking and breakout sessions. Participants were able to seek guidance on project ideas and research tips from our slate of volunteers, which included Amanda Askell , Alex Ray , Daniel Ziegler , Dylan Hadfield-Menell , Ethan Knight , Karl Cobbe , Matthias Plappert , and Sam McCandlish . The breakout sessions turned out to be the main highlight of the afternoon. Whereas the morning talks covered the conceptual foundations of RL, the breakout sessions were designed to help participants boost their implementation and research skills. In the first session, Karl Cobbe gave an introduction to TensorFlow , a key library used in deep learning research. In the second session, “Writing DQN Together,” Daniel Ziegler led participants step-by-step through the process of implementing a deep RL algorithm. In the third session, “Advanced RL Q&A,” Joshua Achiam described recent research frontiers in RL and took audience questions about doing RL research. This was our first experiment with the workshop format, and we were generally pleased with the outcome. In particular, we found it quite gratifying to work directly with such a capable and enthusiastic group of participants. The experience, along with feedback from the group, gave us a good sense of what to keep and what to change for future workshops. What worked : We asked our participants what their highlights were, and these responses are a fairly representative sample: “Learning A TON in a very safe, friendly environment where everyone was mainly on the same level in terms of learning.” “I thought the ability to get one-on-one help and to take on some 'paired programming'-like time with folks who really know what they're doing was incredibly helpful. The enthusiasm of the volunteers was also very high, and I felt very encouraged to ask for help.” Responses like these gave us a sense that the workshop format shined on delivering “mentorship and discussions with experts.\" What could be improved : We asked our participants what they thought we could have done differently to enhance their experience, and received responses like: “I would've liked a presentation section of potential projects that we could pursue based on our experience level.” “Extend the workshop to two days.” Many participants felt like they either 1) weren’t sure what to work on during the hackathon, or 2) didn’t have enough time to make significant progress on their hacking project. We think this kind of feedback is a good indicator that the 1-day workshop format isn’t enough to “have the students work on projects that are at the right level to help them grow” in RL. In the future, we’ll consider running longer events so we can meet that goal. This feedback also suggests that we should do more to create “shovel-ready” RL projects that participants can jump right in to. What else? Aside from the technical content of the workshop, creating a supportive and inclusive environment was top-of-mind for us, and participants told us this was important for their experience. One piece of feedback read: “This is the first non-female exclusive social event I've been to in Silicon Valley with ~50% women in the room. It was so shocking that I thought I was in the wrong room in the beginning. It was noticeably easier to socialize as a result of the gender balance, so thank you for that.” OpenAI’s charter gives us a mandate “to create a global community working together to address AGI’s global challenges,” and we’ll continue developing education at OpenAI to help serve that goal. This includes more work on resources like Spinning Up in Deep RL and more events like this Spinning Up Workshop. We are currently planning a second workshop with CHAI at Berkeley , which we expect to formally announce soon. If you would like to help us do research on RL or teach people about AI, please get in touch! We’re hiring . Thanks to Maddie Hall and Loren Kwan for co-organizing the event, to Ian Atha for livestreaming and recording the lectures, as well as helping participants with Python and Tensorflow issues, and to Blake Tucker for filming and photography!", "date": "2019-02-26"},
{"website": "Open-AI", "title": "openai-five-finals", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-five-finals/", "abstract": "We'll be holding our final live event for OpenAI Five at 11:30a PT on April 13th. We’ll showcase aspects of OpenAI Five which we think illustrate how humans and AI will interact in the future. We believe that AI's impact on the world will be driven by its competence, scalability, and ability to enhance what humans can do—and this event will use OpenAI Five to concretely demonstrate each of these. We hope Finals will help people better internalize AI progress and how it will affect the world. We started working with Dota 2 because we expected it to be a good testbed for developing general-purpose AI technologies . It has additionally turned out to be a great avenue for helping people experience modern AI — which we expect to become a high-stakes part of people's lives in the future, starting with systems like self-driving cars. As part of the event, we're honored to compete against the reigning Dota 2 world champions, OG , who will test OpenAI Five at the limits of human ability. We'll also be joined by Blitz , Capitalist , ODPixel , Purge , and Sheever . Games will be played with rules similar to those used for the OpenAI Five matches at The International 2018 . OpenAI Five Finals will be hosted in the Bay Area on April 13th. The event will run from 11:30a to about 4p (exact length depends on game duration). Doors will open at 11a. If you’d like to attend in person, please request an invite by Friday 3/29 at 9:00pm PT; invites will be sent by the end of Monday 4/1. Our venue has limited seating, so we’ll be selecting invitees based on their answers to the request form. If you can't attend in person, please tune in on Twitch !", "date": "2019-03-26"},
{"website": "Open-AI", "title": "testing-robustness", "author": ["Daniel Kang", "Yi Sun", "Dan Hendrycks", "Tom Brown", "Jacob Steinhardt"], "link": "https://openai.com/blog/testing-robustness/", "abstract": "We've developed a method to assess whether a neural network classifier can reliably defend against adversarial attacks not seen during training. Our method yields a new metric, UAR (Unforeseen Attack Robustness), which evaluates the robustness of a single model against an unanticipated attack, and highlights the need to measure performance across a more diverse range of unforeseen attacks. Modern neural networks have achieved high accuracies on a wide range of benchmark tasks. However, they remain susceptible to adversarial examples , small but carefully crafted distortions of inputs created by adversaries to fool the networks. For example, the adversarial example with $L_\\infty$ distortion below differs from the original image by at most 32 in each RGB pixel value; a human can still classify the changed image, but it is confidently misclassified by a standard neural network. AI systems deployed in the wild will need to be robust to unforeseen attacks, but most defenses so far have focused on specific known attack types. The field has made progress in hardening models against such attacks; however, robustness against one type of distortion often does not transfer to robustness against attacks unforeseen by designers of the model. Consequently, evaluating against only a single distortion type can give a false sense of security about a model in the wild which may remain vulnerable to unforeseen attacks such as fake eyeglasses and adversarial stickers . An example where adversarial robustness does not transfer well. Hardening a model against Distortion A initially increases robustness against both Distortions A and B. However, as we harden further, adversarial robustness is harmed for Distortion B but remains about the same for Distortion A. [1] (A = $L_\\infty$, B = $L_1$) We’ve created a three-step method to assess how well a model performs against a new held-out type of distortion. Our method evaluates against diverse unforeseen attacks at a wide range of distortion sizes and compares the results to a strong defense which has knowledge of the distortion type. It also yields a new metric, UAR, which assesses the adversarial robustness of models against unforeseen distortion types. Typical papers on adversarial defense evaluate only against the widely studied $L_\\infty$ or $L_2$ distortion types. However, we show that evaluating against the $L_p$ distortions gives very similar information about adversarial robustness. [2] We conclude that evaluating against $L_p$ distortions is insufficient to predict adversarial robustness against other distortion types. Instead, we suggest that researchers evaluate models against adversarial distortions that are not similar to those used in training. We offer the $L_1$, $L_2$-JPEG, Elastic, and Fog attacks as a starting point. We provide implementations, pre-trained models, and calibrations for a variety of attacks in our code package . We found that considering too narrow a range of distortion sizes can reverse qualitative conclusions about adversarial robustness. To pick a range, we examine images produced by an attack at different distortion sizes and choose the largest range for which the images are still human-recognizable. However, as shown below, an attack with a large distortion budget only uses it against strong defenses. We recommend choosing a calibrated range of distortion sizes by evaluating against adversarially trained models (we also provide calibrated sizes for a wide variety of attacks in our code package ). We developed a new metric, UAR, which compares the robustness of a model against an attack to adversarial training against that attack. Adversarial training is a strong defense that uses knowledge of an adversary by training on adversarially attacked images. [3] A UAR score near 100 against an unforeseen adversarial attack implies performance comparable to a defense with prior knowledge of the attack, making this a challenging objective. We computed the UAR scores of adversarially trained models for several different distortion types. As shown below, the robustness conferred by adversarial training does not transfer broadly to unforeseen distortions. In fact, robustness against a known distortion can reduce robustness against unforeseen distortions. These results underscore the need for evaluation against significantly more diverse attacks like Elastic, Fog, Gabor, and Snow. We hope that researchers developing adversarially robust models will use our methodology to evaluate against a more diverse set of unforeseen attacks. Our code includes a suite of attacks, adversarially trained models, and calibrations which allow UAR to be easily computed. If you're interested in topics in AI Safety, consider applying to work at OpenAI. The accuracy of the model against Distortion A peaks at a hardening level of 8 because that is sufficient to defend against the attack and further hardening hurts clean accuracy; see full paper for details. ↩︎ The $L_p$ distortion allows an image viewed as a vector $x$ of pixel values to be distorted to a vector $x^\\prime$ such that $x$ and $x^\\prime$ are close in the $L_p$ norm . ↩︎ To compute UAR, we average the accuracy of the defense across multiple distortion sizes and normalize by the performance of an adversarially trained model; a precise definition is in our paper. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-08-22"},
{"website": "Open-AI", "title": "jukebox", "author": ["Prafulla Dhariwal", "Heewoo Jun", "Christine McLeavey Payne", "Jong Wook Kim", "Alec Radford", "Ilya Sutskever"], "link": "https://openai.com/blog/jukebox/", "abstract": "Automatic music generation dates back to more than half a century. A prominent approach is to generate music symbolically in the form of a piano roll, which specifies the timing, pitch, velocity, and instrument of each note to be played. This has led to impressive results like producing Bach chorals, polyphonic music with multiple instruments, as well as minute long musical pieces. But symbolic generators have limitations—they cannot capture human voices or many of the more subtle timbres, dynamics, and expressivity that are essential to music. A different approach [1] is to model music directly as raw audio. Generating music at the audio level is challenging since the sequences are very long. A typical 4-minute song at CD quality (44 kHz, 16-bit) has over 10 million timesteps. For comparison, GPT-2 had 1,000 timesteps and OpenAI Five took tens of thousands of timesteps per game. Thus, to learn the high level semantics of music, a model would have to deal with extremely long-range dependencies. One way of addressing the long input problem is to use an autoencoder that compresses raw audio to a lower-dimensional space by discarding some of the perceptually irrelevant bits of information. We can then train a model to generate audio in this compressed space, and upsample back to the raw audio space. We chose to work on music because we want to continue to push the boundaries of generative models. Our previous work on MuseNet explored synthesizing music based on large amounts of MIDI data. Now in raw audio, our models must learn to tackle high diversity as well as very long range structure, and the raw audio domain is particularly unforgiving of errors in short, medium, or long term timing. Jukebox's autoencoder model compresses audio to a discrete space, using a quantization-based approach called VQ-VAE. Hierarchical VQ-VAEs can generate short instrumental pieces from a few sets of instruments, however they suffer from hierarchy collapse due to use of successive encoders coupled with autoregressive decoders. A simplified variant called VQ-VAE-2 avoids these issues by using feedforward encoders and decoders only, and they show impressive results at generating high-fidelity images. We draw inspiration from VQ-VAE-2 and apply their approach to music. We modify their architecture as follows: To alleviate codebook collapse common to VQ-VAE models, we use random restarts where we randomly reset a codebook vector to one of the encoded hidden states whenever its usage falls below a threshold. To maximize the use of the upper levels, we use separate decoders and independently reconstruct the input from the codes of each level. To allow the model to reconstruct higher frequencies easily, we add a spectral loss that penalizes the norm of the difference of input and reconstructed spectrograms. We use three levels in our VQ-VAE, shown below, which compress the 44kHz raw audio by 8x, 32x, and 128x, respectively, with a codebook size of 2048 for each level. This downsampling loses much of the audio detail, and sounds noticeably noisy as we go further down the levels. However, it retains essential information about the pitch, timbre, and volume of the audio. Next, we train the prior models whose goal is to learn the distribution of music codes encoded by VQ-VAE and to generate music in this compressed discrete space. Like the VQ-VAE, we have three levels of priors: a top-level prior that generates the most compressed codes, and two upsampling priors that generate less compressed codes conditioned on above. The top-level prior models the long-range structure of music, and samples decoded from this level have lower audio quality but capture high-level semantics like singing and melodies. The middle and bottom upsampling priors add local musical structures like timbre, significantly improving the audio quality. We train these as autoregressive models using a simplified variant of Sparse Transformers. Each of these models has 72 layers of factorized self-attention on a context of 8192 codes, which corresponds to approximately 24 seconds, 6 seconds, and 1.5 seconds of raw audio at the top, middle and bottom levels, respectively. Once all of the priors are trained, we can generate codes from the top level, upsample them using the upsamplers, and decode them back to the raw audio space using the VQ-VAE decoder to sample novel songs. To train this model, we crawled the web to curate a new dataset of 1.2 million songs (600,000 of which are in English), paired with the corresponding lyrics and metadata from LyricWiki . The metadata includes artist, album genre, and year of the songs, along with common moods or playlist keywords associated with each song. We train on 32-bit, 44.1 kHz raw audio, and perform data augmentation by randomly downmixing the right and left channels to produce mono audio. The top-level transformer is trained on the task of predicting compressed audio tokens. We can provide additional information, such as the artist and genre for each song. This has two advantages: first, it reduces the entropy of the audio prediction, so the model is able to achieve better quality in any particular style; second, at generation time, we are able to steer the model to generate in a style of our choosing. This t-SNE below shows how the model learns, in an unsupervised way, to cluster similar artists and genres close together, and also makes some surprising associations like Jennifer Lopez being so close to Dolly Parton! In addition to conditioning on artist and genre, we can provide more context at training time by conditioning the model on the lyrics for a song. A significant challenge is the lack of a well-aligned dataset: we only have lyrics at a song level without alignment to the music, and thus for a given chunk of audio we don’t know precisely which portion of the lyrics (if any) appear. We also may have song versions that don’t match the lyric versions, as might occur if a given song is performed by several different artists in slightly different ways.  Additionally, singers frequently repeat phrases, or otherwise vary the lyrics, in ways that are not always captured in the written lyrics. To match audio portions to their corresponding lyrics, we begin with a simple heuristic that aligns the characters of the lyrics to linearly span the duration of each song, and pass a fixed-size window of characters centered around the current segment during training. While this simple strategy of linear alignment worked surprisingly well, we found that it fails for certain genres with fast lyrics, such as hip hop. To address this, we use Spleeter to extract vocals from each song and run NUS AutoLyricsAlign on the extracted vocals to obtain precise word-level alignments of the lyrics. We chose a large enough window so that the actual lyrics have a high probability of being inside the window. To attend to the lyrics, we add an encoder to produce a representation for the lyrics, and add attention layers that use queries from the music decoder to attend to keys and values from the lyrics encoder. After training, the model learns a more precise alignment. While Jukebox represents a step forward in musical quality, coherence, length of audio sample, and ability to condition on artist, genre, and lyrics, there is a significant gap between these generations and human-created music. For example, while the generated songs show local musical coherence, follow traditional chord patterns, and can even feature impressive solos, we do not hear familiar larger musical structures such as choruses that repeat. Our downsampling and upsampling process introduces discernable noise. Improving the VQ-VAE so its codes capture more musical information would help reduce this. Our models are also slow to sample from, because of the autoregressive nature of sampling. It takes approximately 9 hours to fully render one minute of audio through our models, and thus they cannot yet be used in interactive applications. Using techniques that distill the model into a parallel sampler can significantly speed up the sampling speed. Finally, we currently train on English lyrics and mostly Western music, but in the future we hope to include songs from other languages and parts of the world. Our audio team is continuing to work on generating audio samples conditioned on different kinds of priming information.  In particular, we've seen early success conditioning on MIDI files and stem files. Here's an example of a raw audio sample conditioned on MIDI tokens .  We hope this will improve the musicality of samples (in the way conditioning on lyrics improved the singing), and this would also be a way of giving musicians more control over the generations. We expect human and model collaborations to be an increasingly exciting creative space. If you’re excited to work on these problems with us, we’re hiring . As generative modeling across various domains continues to advance, we are also conducting research into issues like bias and intellectual property rights , and are engaging with people who work in the domains where we develop tools. To better understand future implications for the music community, we shared Jukebox with an initial set of 10 musicians from various genres to discuss their feedback on this work. While Jukebox is an interesting research result, these musicians did not find it immediately applicable to their creative process given some of its current limitations . We are connecting with the wider creative community as we think generative work across text, images, and audio will continue to improve. If you're interested in being a creative collaborator to help us build useful tools or new works of art in these domains, please let us know ! To connect with the corresponding authors, please email jukebox@openai.com . Timeline Our first raw audio model, which learns to recreate instruments like Piano and Violin. We try a dataset of rock and pop songs, and surprisingly it works. We collect a larger and more diverse dataset of songs, with labels for genres and artists. Model picks up artist and genre styles more consistently with diversity, and at convergence can also produce full-length songs with long-range coherence. We scale our VQ-VAE from 22 to 44kHz to achieve higher quality audio. We also scale top-level prior from 1B to 5B to capture the increased information. We see better musical quality, clear singing, and long-range coherence. We also make novel completions of real songs. We start training models conditioned on lyrics to incorporate further conditioning information. We only have unaligned lyrics, so model has to learn alignment and pronunciation, as well as singing. One can also use a hybrid approach—first generate the symbolic music, then render it to raw audio using a wavenet conditioned on piano rolls, an autoencoder, or a GAN —or do music style transfer, to transfer styles between classical and jazz music, generate chiptune music, or disentangle musical style and content. For a deeper dive into raw audio modelling, we recommend this excellent overview . ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2020-04-30"},
{"website": "Open-AI", "title": "adversarial-example-research", "author": ["Ian Goodfellow", "Nicolas Papernot", "Sandy Huang", "Rocky Duan", "Pieter Abbeel", "Jack Clark"], "link": "https://openai.com/blog/adversarial-example-research/", "abstract": "Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake; they're like optical illusions for machines. In this post we'll show how adversarial examples work across different mediums, and will discuss why securing systems against them can be difficult. At OpenAI, we think adversarial examples are a good aspect of security to work on because they represent a concrete problem in AI safety that can be addressed in the short term, and because fixing them is difficult enough that it requires a serious research effort. (Though we'll need to explore many aspects of machine learning security to achieve our goal of building safe, widely distributed AI .) To get an idea of what adversarial examples look like, consider this demonstration from Explaining and Harnessing Adversarial Examples : starting with an image of a panda, the attacker adds a small perturbation that has been calculated to make the image be recognized as a gibbon with high confidence. The approach is quite robust; recent research has shown adversarial examples can be printed out on standard paper then photographed with a standard smartphone, and still fool systems. Adversarial examples have the potential to be dangerous. For example, attackers could target autonomous vehicles by using stickers or paint to create an adversarial stop sign that the vehicle would interpret as a 'yield' or other sign, as discussed in Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples . Reinforcement learning agents can also be manipulated by adversarial examples, according to new research from UC Berkeley, OpenAI, and Pennsylvania State University, Adversarial Attacks on Neural Network Policies , and research from the University of Nevada at Reno, Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks . The research shows that widely-used RL algorithms, such as DQN , TRPO , and A3C , are vulnerable to adversarial inputs. These can lead to degraded performance even in the presence of pertubations too subtle to be percieved by a human, causing an agent to move a pong paddle down when it should go up , or interfering with its ability to spot enemies in Seaquest. If you want to experiment with breaking your own models, you can use cleverhans , an open source library developed jointly by Ian Goodfellow and Nicolas Papernot to test your AI's vulnerabilities to adversarial examples. When we think about the study of AI safety, we usually think about some of the most difficult problems in that field — how can we ensure that sophisticated reinforcement learning agents that are significantly more intelligent than human beings behave in ways that their designers intended? Adversarial examples show us that even simple modern algorithms, for both supervised and reinforcement learning, can already behave in surprising ways that we do not intend. Traditional techniques for making machine learning models more robust, such as weight decay and dropout, generally do not provide a practical defense against adversarial examples. So far, only two methods have provided a significant defense. Adversarial training : This is a brute force solution where we simply generate a lot of adversarial examples and explicitly train the model not to be fooled by each of them. An open-source implementation of adversarial training is available in the cleverhans library and its use illustrated in the following tutorial . Defensive distillation : This is a strategy where we train the model to output probabilities of different classes, rather than hard decisions about which class to output. The probabilities are supplied by an earlier model, trained on the same task using hard class labels. This creates a model whose surface is smoothed in the directions an adversary will typically try to exploit, making it difficult for them to discover adversarial input tweaks that lead to incorrect categorization. (Distillation was originally introduced in Distilling the Knowledge in a Neural Network as a technique for model compression, where a small model is trained to imitate a large one, in order to obtain computational savings.) Yet even these specialized algorithms can easily be broken by giving more computational firepower to the attacker. To give an example of how a simple defense can fail, let's consider why a technique called \"gradient masking\" does not work. \"Gradient masking\" is a term introduced in Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples . to describe an entire category of failed defense methods that work by trying to deny the attacker access to a useful gradient. Most adversarial example construction techniques use the gradient of the model to make an attack. In other words, they look at a picture of an airplane, they test which direction in picture space makes the probability of the “cat” class increase, and then they give a little push (in other words, they perturb the input) in that direction. The new, modified image is mis-recognized as a cat. But what if there were no gradient — what if an infinitesimal modification to the image caused no change in the output of the model? This seems to provide some defense because the attacker does not know which way to “push” the image. We can easily imagine some very trivial ways to get rid of the gradient. For example, most image classification models can be run in two modes: one mode where they output just the identity of the most likely class, and one mode where they output probabilities. If the model’s output is “99.9% airplane, 0.1% cat”, then a little tiny change to the input gives a little tiny change to the output, and the gradient tells us which changes will increase the probability of the “cat” class. If we run the model in a mode where the output is just “airplane”, then a little tiny change to the input will not change the output at all, and the gradient does not tell us anything. Let’s run a thought experiment to see how well we could defend our model against adversarial examples by running it in “most likely class” mode instead of “probability mode.” The attacker no longer knows where to go to find inputs that will be classified as cats, so we might have some defense. Unfortunately, every image that was classified as a cat before is still classified as a cat now. If the attacker can guess which points are adversarial examples, those points will still be misclassified. We haven’t made the model more robust; we have just given the attacker fewer clues to figure out where the holes in the models defense are. Even more unfortunately, it turns out that the attacker has a very good strategy for guessing where the holes in the defense are. The attacker can train their own model, a smooth model that has a gradient, make adversarial examples for their model, and then deploy those adversarial examples against our non-smooth model. Very often, our model will misclassify these examples too. In the end, our thought experiment reveals that hiding the gradient didn’t get us anywhere. The defense strategies that perform gradient masking typically result in a model that is very smooth in specific directions and neighborhoods of training points, which makes it harder for the adversary to find gradients indicating good candidate directions to perturb the input in a damaging way for the model. However, the adversary can train a substitute model: a copy that imitates the defended model by observing the labels that the defended model assigns to inputs chosen carefully by the adversary. A procedure for performing such a model extraction attack was introduced in the black-box attacks paper. The adversary can then use the substitute model’s gradients to find adversarial examples that are misclassified by the defended model as well. In the figure above, reproduced from the discussion of gradient masking found in Towards the Science of Security and Privacy in Machine Learning , we illustrate this attack strategy with a one-dimensional ML problem. The gradient masking phenomenon would be exacerbated for higher dimensionality problems, but harder to depict. We find that both adversarial training and defensive distillation accidentally perform a kind of gradient masking. Neither algorithm was explicitly designed to perform gradient masking, but gradient masking is apparently a defense that machine learning algorithms can invent relatively easily when they are trained to defend themselves and not given specific instructions about how to do so. If we transfer adversarial examples from one model to a second model that was trained with either adversarial training or defensive distillation, the attack often succeeds, even when a direct attack on the second model would fail. This suggests that both training techniques do more to flatten out the model and remove the gradient than to make sure it classifies more points correctly. Adversarial examples are hard to defend against because it is difficult to construct a theoretical model of the adversarial example crafting process. Adversarial examples are solutions to an optimization problem that is non-linear and non-convex for many ML models, including neural networks. Because we don’t have good theoretical tools for describing the solutions to these complicated optimization problems, it is very hard to make any kind of theoretical argument that a defense will rule out a set of adversarial examples. Adversarial examples are also hard to defend against because they require machine learning models to produce good outputs for every possible input . Most of the time, machine learning models work very well but only work on a very small amount of all the many possible inputs they might encounter. Every strategy we have tested so far fails because it is not adaptive : it may block one kind of attack, but it leaves another vulnerability open to an attacker who knows about the defense being used. Designing a defense that can protect against a powerful, adaptive attacker is an important research area. Adversarial examples show that many modern machine learning algorithms can be broken in surprising ways. These failures of machine learning demonstrate that even simple algorithms can behave very differently from what their designers intend. We encourage machine learning researchers to get involved and design methods for preventing adversarial examples, in order to close this gap between what designers intend and how algorithms behave. If you're interested in working on adversarial examples, consider joining OpenAI . To learn more about machine learning security, follow Ian and Nicolas's machine learning security blog cleverhans.io .", "date": "2017-02-24"},
{"website": "Open-AI", "title": "ai-and-efficiency", "author": ["Danny Hernandez", "Tom Brown"], "link": "https://openai.com/blog/ai-and-efficiency/", "abstract": "We’re releasing an analysis showing that since 2012 the amount of compute needed to train a neural net to the same performance on ImageNet classification has been decreasing by a factor of 2 every 16 months. Compared to 2012, it now takes 44 times less compute to train a neural network to the level of AlexNet (by contrast, Moore’s Law would yield an 11x cost improvement over this period). Our results suggest that for AI tasks with high levels of recent investment, algorithmic progress has yielded more gains than classical hardware efficiency. Algorithmic improvement is a key factor driving the advance of AI. It’s important to search for measures that shed light on overall algorithmic progress, even though it’s harder than measuring such trends in compute. Download charts Algorithmic efficiency can be defined as reducing the compute needed to train a specific capability. Efﬁciency is the primary way we measure algorithmic progress on classic computer science problems like sorting. Efficiency gains on traditional problems like sorting are more straightforward to measure than in ML because they have a clearer measure of task difficulty. [1] However, we can apply the efficiency lens to machine learning by holding performance constant. Efficiency trends can be compared across domains like DNA sequencing (10-month doubling), solar energy (6-year doubling), and transistor density (2-year doubling). For our analysis, we primarily leveraged open-source re-implementations to measure progress on AlexNet level performance over a long horizon. We saw a similar rate of training efficiency improvement for ResNet-50 level performance on ImageNet (17-month doubling time). We saw faster rates of improvement over shorter timescales in Translation, Go, and Dota 2: It can be helpful to think of compute in 2012 not being equal to compute in 2019 in a similar way that dollars need to be inflation-adjusted over time. A fixed amount of compute could accomplish more in 2019 than in 2012. One way to think about this is that some types of AI research progress in two stages, similar to the “tick tock” model of development seen in semiconductors; new capabilities (the “tick”) typically require a significant amount of compute expenditure to obtain, then refined versions of those capabilities (the “tock”) become much more efficient to deploy due to process improvements. Increases in algorithmic efficiency allow researchers to do more experiments of interest in a given amount of time and money. In addition to being a measure of overall progress, algorithmic efficiency gains speed up future AI research in a way that's somewhat analogous to having more compute. In addition to efficiency, many other measures shed light on overall algorithmic progress in AI. Training cost in dollars is related, but less narrowly focused on algorithmic progress because it’s also affected by improvement in the underlying hardware, hardware utilization, and cloud infrastructure. Sample efficiency is key when we’re in a low data regime, which is the case for many tasks of interest. The ability to train models faster also speeds up research and can be thought of as a measure of the parallelizability of learning capabilities of interest. We also find increases in inference efficiency in terms of GPU time , parameters , and flops meaningful, but mostly as a result of their economic implications [2] rather than their effect on future research progress. Shufflenet achieved AlexNet-level performance with an 18x inference efficiency increase in 5 years (15-month doubling time), which suggests that training efficiency and inference efficiency might improve at similar rates. The creation of datasets/​environments/​benchmarks is a powerful method of making specific AI capabilities of interest more measurable. We believe that policymaking related to AI will be improved by a greater focus on the measurement and assessment of AI systems, both in terms of technical attributes and societal impact. We think such measurement initiatives can shed light on important questions in policy; our AI and Compute analysis suggests policymakers should increase funding for compute resources for academia, so that academic research can replicate, reproduce, and extend industry research. This efficiency analysis suggests that policymakers could develop accurate intuitions about the cost of deploying AI capabilities—and how these costs are going to alter over time—by more closely assessing the rate of improvements in efficiency for AI systems. If large scale compute continues to be important to achieving state of the art (SOTA) overall performance in domains like language and games then it’s important to put effort into measuring notable progress achieved with smaller amounts of compute (contributions often made by academic institutions). Models that achieve training efficiency state of the arts on meaningful capabilities are promising candidates for scaling up and potentially achieving overall top performance. Additionally, figuring out the algorithmic efficiency improvements are straightforward [6] since they are just a particularly meaningful slice of the learning curves that all experiments generate. We also think that measuring long run trends in efficiency SOTAs will help paint a quantitative picture of overall algorithmic progress. We observe that hardware and algorithmic efficiency gains are multiplicative and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both. Our results suggest that for AI tasks with high levels of investment (researcher time and/or compute) algorithmic efficiency might outpace gains from hardware efficiency (Moore's Law). Moore's Law was coined in 1965 when integrated circuits had a mere 64 transistors (6 doublings) and naively extrapolating it out predicted personal computers and smartphones (an iPhone 11 has 8.5 billion transistors). If we observe decades of exponential improvement in the algorithmic efficiency of AI, what might it lead to? We're not sure. That these results make us ask this question is a modest update for us towards a future with powerful AI services and technology. For all these reasons, we’re going to start tracking efficiency SOTAs publicly. We’ll start with vision and translation efficiency benchmarks (ImageNet [7] and WMT14), and we’ll consider adding more benchmarks over time. We believe there are efficiency SOTAs on these benchmarks we’re unaware of and encourage the research community to submit them here (we’ll give credit to original authors and collaborators). Industry leaders, policymakers, economists, and potential researchers are all trying to better understand AI progress and decide how much attention they should invest and where to direct it. Measurement efforts can help ground such decisions. If you’re interested in this type of work, consider applying to work at OpenAI’s Foresight or Policy team! Submit on GitHub We’d like to thank the following people helpful conversations and/or feedback on this post: Dario Amodei, Jack Clark, Alec Radford, Paul Christiano, Sam McCandlish, Ilya Sutskever, Jacob Steinhardt, Jared Kaplan, Amanda Askell, John Schulman, Jacob Hilton, Asya Bergal, Katja Grace, Ryan Carey, Nicholas Joseph, Geoffrey Irving, Jeff Clune, and Ashley Pilipiszyn. Thanks to Justin Jay Wang for design. Thanks to Niki Parmar for providing the relevant points from the original transformer learning curves. Also thanks to Mingxing Tan for providing the relevant points from EfficientNet learning curves and running an experiment with reduced warmup. In the sorting example, the “difficulty” of the problem is the length of the list. The cost for quicksort, a commonly used algorithm is  denoted in Big O notation: $O(n\\log{}n)$. ↩︎ Inference costs dominate total costs for successful deployed systems. Inference costs scale with usage of the system, whereas training costs only need to be paid once. ↩︎ Throughout this post we refer to Moore's Law as the consistent, long-observed 2-year doubling time of dollars/flop. One could also interpret Moore's Law as the trend in dollars/flop, that has recently slowed down. ↩︎ For instance algorithmic progress could change the complexity class on some task from exponential to polynomial cost. Such efficiency gains on capabilities of interest are intractable to directly observe, though they may be observable through asymptotic analysis or extrapolating empirically derived scaling laws. ↩︎ Making credible forecasts on such topics is a substantial enterprise, we'd rather avoid here than give insufficient treatment. ↩︎ In fact, this work was primarily done by training PyTorch examples models, with tweaks to improve early learning. ↩︎ ImageNet is the only training data source allowed for the vision benchmark. No human captioning, other images, or other data is allowed. Automated augmentation is ok. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2020-05-05"},
{"website": "Open-AI", "title": "unsupervised-sentiment-neuron", "author": ["Alec Radford", "Ilya Sutskever", "Rafał Józefowicz", "Jack Clark", "Greg Brockman"], "link": "https://openai.com/blog/unsupervised-sentiment-neuron/", "abstract": "We’ve developed an unsupervised system which learns an excellent representation of sentiment, despite being trained only to predict the next character in the text of Amazon reviews. A linear model using this representation achieves state-of-the-art sentiment analysis accuracy on a small but extensively-studied dataset, the Stanford Sentiment Treebank (we get 91.8% accuracy versus the previous best of 90.2%), and can match the performance of previous supervised systems using 30-100x fewer labeled examples. Our representation also contains a distinct “ sentiment neuron ” which contains almost all of the sentiment signal. Our system beats other approaches on Stanford Sentiment Treebank while using dramatically less data. We were very surprised that our model learned an interpretable feature, and that simply predicting the next character in Amazon reviews resulted in discovering the concept of sentiment. We believe the phenomenon is not specific to our model, but is instead a general property of certain large neural networks that are trained to predict the next step or dimension in their inputs. We first trained a multiplicative LSTM with 4,096 units on a corpus of 82 million Amazon reviews to predict the next character in a chunk of text. Training took one month across four NVIDIA Pascal GPUs, with our model processing 12,500 characters per second. These 4,096 units (which are just a vector of floats) can be regarded as a feature vector representing the string read by the model. After training the mLSTM, we turned the model into a sentiment classifier by taking a linear combination of these units, learning the weights of the combination via the available supervised data. While training the linear model with L1 regularization, we noticed it used surprisingly few of the learned units. Digging in, we realized there actually existed a single “sentiment neuron” that's highly predictive of the sentiment value. Just like with similar models, our model can be used to generate text. Unlike those models, we have a direct dial to control the sentiment of the resulting text: we simply overwrite the value of the sentiment neuron. The diagram below represents the character-by-character value of the sentiment neuron, displaying negative values as red and positive values as green. Note that strongly indicative words like “ best ” or “ horrendous ” cause particularly big shifts in the color. It's interesting to note that the system also makes large updates after the completion of sentences and phrases. For example, in “ And about 99.8 percent of that got lost in the film ”, there’s a negative update after “ lost ” and a larger update at the sentence’s end, even though “ in the film ” has no sentiment content on its own. Labeled data are the fuel for today's machine learning. Collecting data is easy, but scalably labeling that data is hard. It’s only feasible to generate labels for important problems where the reward is worth the effort, like machine translation, speech recognition, or self-driving. Machine learning researchers have long dreamed of developing unsupervised learning algorithms to learn a good representation of a dataset, which can then be used to solve tasks using only a few labeled examples. Our research implies that simply training large unsupervised next-step-prediction models on large amounts of data may be a good approach to use when creating systems with good representation learning capabilities. Our results are a promising step towards general unsupervised representation learning. We found the results by exploring whether we could learn good quality representations as a side effect of language modeling, and scaled up an existing model on a carefully-chosen dataset. Yet the underlying phenomena remain more mysterious than clear. These results were not as strong for datasets of long documents. We suspect our character-level model struggles to remember information over hundreds to thousands of timesteps. We think it’s worth trying hierarchical models that can adapt the timescales at which they operate. Further scaling up these models may further improve representation fidelity and performance on sentiment analysis and similar tasks. The model struggles the more the input text diverges from review data. It’s worth verifying that broadening the corpus of text samples results in an equally informative representation that also applies to broader domains. Our results suggest that there exist settings where very large next-step-prediction models learn excellent unsupervised representations. Training a large neural network to predict the next frame in a large collection of videos may result in unsupervised representations for object, scene, and action classifiers. Overall, it's important to understand the properties of models, training regimes, and datasets that reliably lead to such excellent representations.", "date": "2017-04-06"},
{"website": "Open-AI", "title": "musenet", "author": ["Christine McLeavey Payne"], "link": "https://openai.com/blog/musenet/", "abstract": "We've created MuseNet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files. MuseNet uses the same general-purpose unsupervised technology as GPT-2 , a large-scale transformer model trained to predict the next token in a sequence, whether audio or text. Since MuseNet knows many different styles, we can blend generations in novel ways. [1] Here the model is given the first 6 notes of a Chopin Nocturne, but is asked to generate a piece in a pop style with piano, drums, bass, and guitar. The model manages to blend the two styles convincingly, with the full band joining in at around the 30 second mark: We’re excited to see how musicians and non-musicians alike will use MuseNet to create new compositions! [2] In simple mode (shown by default), you'll hear random uncurated samples that we've pre-generated. Choose a composer or style, an optional start of a famous piece, and start generating. This lets you explore the variety of musical styles the model can create. In advanced mode you can interact with the model directly. The completions will take longer, but you'll be creating an entirely new piece. Some of MuseNet's limitations include: The instruments you ask for are strong suggestions, not requirements. MuseNet generates each note by calculating the probabilities across all possible notes and instruments. The model shifts to make your instrument choices more likely, but there's always a chance it will choose something else. MuseNet has a more difficult time with odd pairings of styles and instruments (such as Chopin with bass and drums). Generations will be more natural if you pick instruments closest to the composer or band’s usual style. We created composer and instrumentation tokens to give more control over the kinds of samples MuseNet generates. During training time, these composer and instrumentation tokens were prepended to each sample, so the model would learn to use this information in making note predictions. At generation time, we can then condition the model to create samples in a chosen style by starting with a prompt such as a Rachmaninoff piano start: Or prompted with the band Journey, with piano, bass, guitar, and drums: We can visualize the embeddings from MuseNet to gain insight into what the model has learned. Here we use t-SNE to create a 2-D map of the cosine similarity of various musical composer and style embeddings. MuseNet uses the recompute and optimized kernels of Sparse Transformer to train a 72-layer network with 24 attention heads—with full attention over a context of 4096 tokens. This long context may be one reason why it is able to remember long-term structure in a piece, like in the following sample imitating Chopin: It can also create musical melodic structures, as in this sample imitating Mozart: Music generation is a useful domain for testing the Sparse Transformer as it sits on a middle ground between text and images. It has the fluid token structure of text (in images you can look back N tokens and find the row above, whereas in music there’s not a fixed number for looking back to the previous measure). Yet we can easily hear whether the model is capturing long term structure on the order of hundreds to thousands of tokens. It’s much more obvious if a music model messes up structure by changing the rhythm, in a way that it’s less clear if a text model goes on a brief tangent. We collected training data for MuseNet from many different sources. ClassicalArchives and BitMidi donated their large collections of MIDI files for this project, and we also found several collections online, including jazz, pop, African, Indian, and Arabic styles. Additionally, we used the MAESTRO dataset . The transformer is trained on sequential data: given a set of notes, we ask it to predict the upcoming note. We experimented with several different ways to encode the MIDI files into tokens suitable for this task. First, a chordwise approach that considered every combination of notes sounding at one time as an individual \"chord\", and assigned a token to each chord. Second, we tried condensing the musical patterns by only focusing on the starts of notes, and tried further compressing that using a byte pair encoding scheme. We also tried two different methods of marking the passage of time: either tokens that were scaled according to the piece’s tempo (so that the tokens represented a musical beat or fraction of a beat), or tokens that marked absolute time in seconds. We landed on an encoding that combines expressivity with conciseness: combining the pitch, volume, and instrument information into a single token. During training, we: We also create an inner critic: the model is asked during training time to predict whether a given sample is truly from the dataset or if it is one of the model's own past generations. This score is used to select samples at generation time. We added several different kinds of embeddings to give the model more structural context. In addition to the standard positional embeddings, we added a learned embedding that tracks the passage of time in a given sample. This way, all of the notes that sound at the same time are given the same timing embedding. We then add an embedding for each note in a chord (this mimics relative attention, since it will be easier for the model to learn that note 4 needs to look back at note 3, or else at note 4 of the previous chord). Finally, we add two structural embeddings which tell the model where a given musical sample is within the larger musical piece. One embedding divides the larger piece into 128 parts, while the second encoding is a countdown from 127 to 0 as the model approaches the (end) token. We’re excited to hear what people create! If you create a piece you like, you can upload it to a free service like Instaudio and then tweet us the link (the MuseNet demo has a tweet button to help with this). If you’re interested in learning more about OpenAI’s music work, consider applying to join our team. Please feel free to email us with suggestions for the MuseNet demo. We'd also love to hear from you if you're interested in composing with MuseNet in more depth, or if you have MIDI files you'd like to add to the training set. Thanks to Rewon Child and Scott Gray for their work on the Sparse Transformer, and Jeff Wu and Alec Radford for their work on GPT-2. We also thank the following for feedback on drafts of this post: Greg Brockman, Ilya Sutskever, Durk Kingma, Arvind Neelakantan, Tim Salimans, Rob Laidlow, Judith Finell, Moni Simeonov, Ray Iwazumi, Sam McCandlish, Miles Brundage, Jack Clark, Jonas Schneider, Chris Olah. If you're interested in other projects for creating AI generated music using transformers, we recommend checking out Magenta's piano generation work . ↩︎ For use of outputs created by MuseNet, please cite this blog post as Please note: We do not own the music output, but kindly ask that you not charge for it. While unlikely, we make no guarantee that the music is free from external copyright claims. API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-04-25"},
{"website": "Open-AI", "title": "deep-reinforcement-learning-from-human-preferences", "author": ["Dario Amodei", "Paul Christiano", "Alex Ray"], "link": "https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/", "abstract": "One step towards building safe AI systems is to remove the need for humans to write goal functions, since using a simple proxy for a complex goal, or getting the complex goal a bit wrong, can lead to undesirable and even dangerous behavior . In collaboration with DeepMind's safety team, we've developed an algorithm which can infer what humans want by being told which of two proposed behaviors is better. We present a learning algorithm that uses small amounts of human feedback to solve modern RL environments. Machine learning systems with human feedback have been explored before , but we've scaled up the approach to be able to work on much more complicated tasks. Our algorithm needed 900 bits of feedback from a human evaluator to learn to backflip — a seemingly simple task which is simple to judge but challenging to specify. The overall training process is a 3-step feedback cycle between the human, the agent’s understanding of the goal, and the RL training. Our AI agent starts by acting randomly in the environment. Periodically, two video clips of its behavior are given to a human, and the human decides which of the two clips is closest to fulfilling its goal — in this case, a backflip. The AI gradually builds a model of the goal of the task by finding the reward function that best explains the human’s judgments. It then uses RL to learn how to achieve that goal. As its behavior improves, it continues to ask for human feedback on trajectory pairs where it's most uncertain about which is better, and further refines its understanding of the goal. Our approach demonstrates promising sample efficiency — as stated previously, the backflip video required under 1000 bits of human feedback. It took less than an hour of a human evaluator's time, while in the background the policy accumulated about 70 hours of overall experience (simulated at a much faster rate than real-time.) We will continue to work on reducing the amount of feedback a human needs to supply. You can see a sped-up version of the training process in the following video. We've tested our method on a number of tasks in the simulated robotics and Atari domains (without being given access to the reward function: so in Atari, without having access to the game score). Our agents can learn from human feedback to achieve strong and sometimes superhuman performance in many of the environments we tested. In the following animation you can see agents trained with our technique playing a variety of Atari games. The horizontal bar on the right hand side of each frame represent's each agents prediction about how much a human evaluator would approve of their current behavior. These visualizations indicate that agents trained with human feedback learn to value oxygen in Seaquest (left), anticipate rewards in Breakout and Pong (center), or work out how to recover from crashes in Enduro (right). Note there's no need for the feedback to align with the environment's normal reward function: we can, for example, train our agents to precisely keep even with other cars in Enduro rather than maximizing game score by passing them. We also sometimes find that learning from feedback does better than reinforcement learning with the normal reward function, because the human shapes the reward better than whoever wrote the environment's reward. Our algorithm's performance is only as good as the human evaluator's intuition about what behaviors look correct, so if the human doesn't have a good grasp of the task they may not offer as much helpful feedback. Relatedly, in some domains our system can result in agents adopting policies that trick the evaluators. For example, a robot which was supposed to grasp items instead positioned its manipulator in between the camera and the object so that it only appeared to be grasping it, as shown below. We addressed this particular problem by adding in visual cues (the thick white lines in the above animation) to make it easy for the human evaluators to estimate depth. The research described in this post was done in collaboration with Jan Leike, Miljan Martic, and Shane Legg at DeepMind. Our two organizations plan to continue to collaborate on topics that touch on long-term AI safety. We think that techniques like this are a step towards safe AI systems capable of learning human-centric goals, and can complement and extend existing approaches like reinforcement and imitation learning. This post is representative of the work done by OpenAI's safety team; if you're interested in working on problems like this, please join us ! Footnote: By comparison, we took two hours to write our own reward function (the animation in the above right) to get a robot to backflip, and though it succeeds it's a lot less elegant than the one trained simply through human feedback (top left). We think there are many cases where human feedback could let us specify a specific goal more intuitively and quickly than is possible by manually hand-crafting the objective. You can replicate this backflip in gym with the following reward function for Hopper:", "date": "2017-06-13"},
{"website": "Open-AI", "title": "glow", "author": ["Prafulla Dhariwal", "Durk Kingma"], "link": "https://openai.com/blog/glow/", "abstract": "We introduce Glow , a reversible generative model which uses invertible 1x1 convolutions. It extends previous work on reversible generative models and simplifies the architecture. Our model can generate realistic high resolution images, supports efficient sampling, and discovers features that can be used to manipulate attributes of data. We're releasing code for the model and an online visualization tool so people can explore and build on these results. Generative modeling is about observing data, like a set of pictures of faces, then learning a model of how this data was generated. Learning to approximate the data-generating process requires learning all structure present in the data, and successful models should be able to synthesize outputs that look similar to the data. Accurate generative models have broad applications, including speech synthesis , text analysis and synthesis , semi-supervised learning and model-based control . The technique we propose can be applied to those problems as well. Glow is a type of reversible generative model, also called flow-based generative model , and is an extension of the NICE and RealNVP techniques. Flow-based generative models have so far gained little attention in the research community compared to GANs and VAEs . Some of the merits of flow-based generative models include: Exact latent-variable inference and log-likelihood evaluation. In VAEs, one is able to infer only approximately the value of the latent variables that correspond to a datapoint. GAN's have no encoder at all to infer the latents. In reversible generative models, this can be done exactly without approximation. Not only does this lead to accurate inference, it also enables optimization of the exact log-likelihood of the data, instead of a lower bound of it. Efficient inference and efficient synthesis. Autoregressive models, such the PixelCNN , are also reversible, however synthesis from such models is difficult to parallelize, and typically inefficient on parallel hardware. Flow-based generative models like Glow (and RealNVP) are efficient to parallelize for both inference and synthesis. Useful latent space for downstream tasks. The hidden layers of autoregressive models have unknown marginal distributions, making it much more difficult to perform valid manipulation of data. In GANs, datapoints can usually not be directly represented in a latent space, as they have no encoder and might not have full support over the data distribution. This is not the case for reversible generative models and VAEs, which allow for various applications such as interpolations between datapoints and meaningful modifications of existing datapoints. Significant potential for memory savings. Computing gradients in reversible neural networks requires an amount of memory that is constant instead of linear in their depth, as explained in the RevNet paper . Using our techniques we achieve significant improvements on standard benchmarks compared to RealNVP, the previous best published result with flow-based generative models. Quantitative performance in terms of bits per dimension evaluated on the test set of various datasets, for the RealNVP model versus our Glow model.* Glow models can generate realistic-looking high-resolution images, and can do so efficiently. Our model takes about 130ms to generate a 256 x 256 sample on a NVIDIA 1080 Ti GPU. Like previous work, we found that sampling from a reduced-temperature model often results in higher-quality samples. The samples above were obtained by scaling the standard deviation of the latents by a temperature of 0.7. We can also interpolate between arbitrary faces, by using the encoder to encode the two images and sample from intermediate points. Note that the inputs are arbitrary faces and not samples from the model, thus providing evidence that the model has support over the full target distribution. We can train a flow-based model, without labels, and then use the learned latent reprentation for downstream tasks like manipulating attributes of your input. These semantic attributes could be the color of hair in a face, the style of an image, the pitch of a musical sound, or the emotion of a text sentence. Since flow-based models have a perfect encoder, you can encode inputs and compute the average latent vector of inputs with and without the attribute. The vector direction between the two can then be used to manipulate an arbitrary input towards that attribute. The above process requires a relatively small amount of labeled data, and can be done after the model has been trained (no labels are needed while training). Previous work using GAN's requires training an encoder separately. Approaches using VAE's only guarantee that the decoder and encoder are compatible for in-distribution data. Other approaches involve directly learning the function representing the transformation, like Cycle-GAN's , however they require retraining for every transformation. Our main contribution and also our departure from the earlier RealNVP work is the addition of a reversible 1x1 convolution, as well as removing other components, simplifying the architecture overall. The RealNVP architecture consists of sequences of two types of layers: layers with checkboard masking, and layers with channel-wise masking. We remove the layers with checkerboard masking, simplifying the architecture. The layers with channel-wise masking perform the equivalent of a repetition of the following steps: By chaining these layers, A updates B, then B updates A, then A updates B, etc. This bipartite flow of information is clearly quite rigid. We found that model performance improves by changing the reverse permutation of step (1) to a (fixed) shuffling permutation. Taking this a step further, we can also learn the optimal permutation. Learning a permutation matrix is a discrete optimization that is not amendable to gradient ascent. But because the permutation operation is just a special case of a linear transformation with a square matrix, we can make this work with convolutional neural networks, as permuting the channels is equivalent to a 1x1 convolution operation with an equal number of input and output channels. So we replace the fixed permutation with learned 1x1 convolution operations. The weights of the 1x1 convolution are initialized as a random rotation matrix. As we show in the figure below, this operation leads to significant modeling improvements. We've also shown that the computations involved in optimizing the objective function can be done efficiently through a LU decomposition of the weights. In addition, we remove batch normalization and replace it with an activation normalization layer. This layer simply shifts and scales the activations, with data-dependent initialization that normalizes the activations given an initial minibatch of data. This allows scaling down the minibatch size to 1 (for large images) and scaling up the size of the model. Our architecture combined with various optimizations, such as gradient checkpointing , allows us to train flow-based generative models on a larger scale than usual. We used Horovod to easily train our model on a cluster of multiple machines; the model used in our demo was trained on 5 machines with each 8 GPUs. Using this setup we train models with over a hundred million parameters. Our work suggests that it's possible to train flow-based models to generate realistic high-resolution images, and learned latent representations that can be easily used for downstream tasks like manipulation of data. We suggest a few directions for future work: Finally, if you’d like use Glow in your research, we encourage you to check out our paper for more details, or look at our code on this Github repo .", "date": "2018-07-09"},
{"website": "Open-AI", "title": "solving-rubiks-cube", "author": ["OpenAI", "Ilge Akkaya", "Marcin Andrychowicz", "Maciek Chociej", "Mateusz Litwin", "Bob McGrew", "Arthur Petron", "Alex Paino", "Matthias Plappert", "Glenn Powell", "Raphael Ribas", "Jonas Schneider", "Nikolas Tezak", "Jerry Tworek", "Peter Welinder", "Lilian Weng", "Qiming Yuan", "Wojciech Zaremba", "Lei Zhang"], "link": "https://openai.com/blog/solving-rubiks-cube/", "abstract": "Human hands let us solve a wide variety of tasks. For the past 60 years of robotics, hard tasks which humans accomplish with their fixed pair of hands have required designing a custom robot for each task . As an alternative, people have spent many decades trying to use general-purpose robotic hardware , but with limited success due to their high degrees of freedom. In particular, the hardware we use here is not new—the robot hand we use has been around for the last 15 years—but the software approach is. Since May 2017, we've been trying to train a human-like robotic hand to solve the Rubik’s Cube . We set this goal because we believe that successfully training such a robotic hand to do complex manipulation tasks lays the foundation for general-purpose robots. We solved the Rubik’s Cube in simulation in July 2017. But as of July 2018, we could only manipulate a block on the robot. Now, we've reached our initial goal. Solving a Rubik’s Cube one-handed is a challenging task even for humans, and it takes children several years to gain the dexterity required to master it. Our robot still hasn't perfected its technique though, as it solves the Rubik’s Cube 60% of the time (and only 20% of the time for a maximally difficult scramble). We train neural networks to solve the Rubik’s Cube in simulation using reinforcement learning and Kociemba’s algorithm for picking the solution steps. [1] Domain randomization enables networks trained solely in simulation to transfer to a real robot. The biggest challenge we faced was to create environments in simulation diverse enough to capture the physics of the real world. Factors like friction, elasticity and dynamics are incredibly difficult to measure and model for objects as complex as Rubik’s Cubes or robotic hands and we found that domain randomization alone is not enough. To overcome this, we developed a new method called Automatic Domain Randomization (ADR), which endlessly generates progressively more difficult environments in simulation. [2] This frees us from having an accurate model of the real world, and enables the transfer of neural networks learned in simulation to be applied to the real world. ADR starts with a single, nonrandomized environment, wherein a neural network learns to solve Rubik’s Cube. As the neural network gets better at the task and reaches a performance threshold, the amount of domain randomization is increased automatically. This makes the task harder, since the neural network must now learn to generalize to more randomized environments. The network keeps learning until it again exceeds the performance threshold, when more randomization kicks in, and the process is repeated. One of the parameters we randomize is the size of the Rubik’s Cube (above). ADR begins with a fixed size of the Rubik’s Cube and gradually increases the randomization range as training progresses. We apply the same technique to all other parameters, such as the mass of the cube, the friction of the robot fingers, and the visual surface materials of the hand. The neural network thus has to learn to solve the Rubik’s Cube under all of those increasingly more difficult conditions. Domain randomization required us to manually specify randomization ranges, which is difficult since too much randomization makes learning difficult but too little randomization hinders transfer to the real robot. ADR solves this by automatically expanding randomization ranges over time with no human intervention. ADR removes the need for domain knowledge and makes it simpler to apply our methods to new tasks. In contrast to manual domain randomization, ADR also keeps the task always challenging with training never converging. We compared ADR to manual domain randomization on the block flipping task, where we already had a strong baseline . In the beginning ADR performs worse in terms of number of successes on the real robot. But as ADR increases the entropy, which is a measure of the  complexity of the environment, the transfer performance eventually doubles over the baseline—without human tuning. Using ADR, we are able to train neural networks in simulation that can solve the Rubik’s Cube on the real robot hand. This is because ADR exposes the network to an endless variety of randomized simulations. It is this exposure to complexity during training that prepares the network to transfer from simulation to the real world since it has to learn to quickly identify and adjust to whatever physical world it is confronted with. To test the limits of our method, we experiment with a variety of perturbations while the hand is solving the Rubik’s Cube. Not only does this test for the robustness of our control network but also tests our vision network, which we here use to estimate the cube’s position and orientation. We find that our system trained with ADR is surprisingly robust to perturbations even though we never trained with them: The robot can successfully perform most flips and face rotations under all tested perturbations, though not at peak performance. We believe that meta-learning , or learning to learn, is an important prerequisite for building general-purpose systems, since it enables them to quickly adapt to changing conditions in their environments. The hypothesis behind ADR is that a memory-augmented networks combined with a sufficiently randomized environment leads to emergent meta-learning , where the network implements a learning algorithm that allows itself to rapidly adapt its behavior to the environment it is deployed in. [3] To test this systematically, we measure the time to success per cube flip (rotating the cube such that a different color faces up) for our neural network under different perturbations, such as resetting the network’s memory, resetting the dynamics, or breaking a joint. We perform these experiments in simulation, which allows us to average performance over 10,000 trials in a controlled setting. In the beginning, as the neural network successfully achieves more flips, each successive time to success decreases because the network learns to adapt. When perturbations are applied (vertical gray lines in the above chart), we see a spike in time to success. This is because the strategy the network is employing doesn't work in the changed environment. The network then relearns about the new environment and we again see time to success decrease to the previous baseline. We also measure failure probability and performed the same experiments for face rotations (rotating the top face 90 degrees clockwise or counterclockwise) and find the same pattern of adaptation. [4] Visualizing our networks enables us to understand what they are storing in memory. This becomes increasingly important as the networks grow in complexity. The memory of our neural network is visualized above. We use a building block from the interpretability toolbox , namely non-negative matrix factorization, to condense this high-dimensional vector into 6 groups and assign each a unique color. We then display the color of the currently dominant group for every timestep. We find that each memory group has a semantically meaningful behavior associated with it. For example, we can tell by looking at only the dominant group of the network’s memory if it is about to spin the cube or rotate the top clockwise before it happens . Solving the Rubik’s Cube with a robot hand is still not easy. Our method currently solves the Rubik’s Cube 20% of the time when applying a maximally difficult scramble that requires 26 face rotations. For simpler scrambles that require 15 rotations to undo, the success rate is 60%. When the Rubik’s Cube is dropped or a timeout is reached, we consider the attempt failed. However, our network is capable of solving the Rubik’s Cube from any initial condition. So if the cube is dropped, it is possible to put it back into the hand and continue solving. We generally find that our neural network is much more likely to fail during the first few face rotations and flips. This is the case because the neural network needs to balance solving the Rubik’s Cube with adapting to the physical world during those early rotations and flips. In order to benchmark our progress and make the problem tractable, we built and designed custom versions of cubes as stepping stones towards ultimately solving a regular Rubik’s Cube. [5] Rubik’s Cube prototypes, from left to right: Locked cube, Face cube, Full cube, Giiker cube, regular Rubik’s Cube. We believe that human-level dexterity is on the path towards building general-purpose robots and we are excited to push forward in this direction. If you want to help make increasingly general AI systems, whether robotic or virtual, we're hiring ! We focus on the problems that are currently difficult for machines to master: perception and dexterous manipulation. We therefore train our neural networks to achieve the required face rotations and cube flips as generated by Kociemba’s algorithm. ↩︎ Our work is strongly related to POET , which automatically generates 2D environments. However, our work learns a joint policy over all environments, which transfers to any newly generated environment. ↩︎ More concretely, we hypothesize that a neural network with finite capacity trained on environments with unbounded complexity forces the network to learn a special-purpose learning algorithm since it cannot memorize solutions for each individual environment and there exists no single robust policy that works under all randomizations. ↩︎ Please refer to our paper for full results. ↩︎ The only modification we made was cutting out a small piece of each center cublet’s colorful sticker. This was necessary to break rotational symmetry . ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-10-15"},
{"website": "Open-AI", "title": "gpt-2-6-month-follow-up", "author": ["Jack Clark", "Miles Brundage", "Irene Solaiman"], "link": "https://openai.com/blog/gpt-2-6-month-follow-up/", "abstract": "We’re releasing the 774 million parameter GPT-2 language model after the release of our small 124M model in February, staged release of our medium 355M model in May, and subsequent research with partners and the AI community into the model’s potential for misuse and societal benefit. We’re also releasing an open-source legal agreement to make it easier for organizations to initiate model-sharing partnerships with each other, and are publishing a technical report about our experience in coordinating with the wider AI research community on publication norms. 1. Coordination is difficult, but possible. To date, there hasn’t been a public release of a 1558M parameter language model, though multiple organizations have developed the systems to train them, or have publicly discussed how to train larger models. For example, teams from both NLP developer Hugging Face and the Allen Institute for Artificial Intelligence (AI2) with the University of Washington have explicitly adopted similar staged release approaches to us . Since February, we’ve spoken with more than five groups who have replicated GPT-2. [1] 2. Humans can be convinced by synthetic text. Research from our research partners Sarah Kreps and Miles McCain at Cornell published in Foreign Affairs says people find GPT-2 synthetic text samples almost as convincing (72% in one cohort judged the articles to be credible) as real articles from the New York Times (83%). [2] Additionally, research from AI2/UW has shown that news written by a system called \"GROVER\" can be more plausible than human-written propaganda . These research results make us generally more cautious about releasing language models. 3. Detection isn’t simple. In practice, we expect detectors to need to detect a significant fraction of generations with very few false positives. Malicious actors may use a variety of sampling techniques (including rejection sampling) or fine-tune models to evade detection methods. A deployed system likely needs to be highly accurate (99.9%–99.99%) on a variety of generations. Our research suggests that current ML-based methods only achieve low to mid–90s accuracy, and that fine-tuning the language models decreases accuracy further. There are promising paths forward (see especially those advocated by the developers of \" GROVER \") but it's a genuinely difficult research problem. We believe that statistical detection of text needs to be supplemented with human judgment and metadata related to the text in order to effectively combat misuse of language models. We’ve partnered with four leading research organizations to analyze both the newly-released 774M parameter GPT-2 model and the unreleased full-size GPT-2 model. We’ve included some preliminary results from them in our technical report, and their ongoing analysis will factor into the potential release of the 1558M model. We’ve also developed a non-commercial legal agreement to facilitate the sharing of models between organizations and are publishing it here to help others initiate such sharing schemes. Cornell University is studying human susceptibility to digital disinformation generated by language models. The Middlebury Institute of International Studies Center on Terrorism, Extremism, and Counterterrorism (CTEC) is exploring how GPT-2 could be misused by terrorists and extremists online. The University of Oregon is developing a series of “bias probes” to analyze bias within GPT-2. The University of Texas at Austin is studying the statistical detectability of GPT-2 outputs after fine-tuning the model on domain-specific datasets, as well as the extent of detection transfer across different language models. Research from these partners will factor into our future release decisions, as will observing how the 774M model is used, and discussing language models with researchers and policymakers to understand the considerations around larger models. As part of our staged release strategy, our current plan is to release the 1558M parameter model in a few months, but it's plausible that findings from a partner, or malicious usage of our 774M model, could change this. We think that a combination of staged release and partnership-based model sharing is likely to be a key foundation of responsible publication in AI, particularly in the context of powerful generative models. The issues inherent to large models are going to grow, rather than diminish, over time. We hope that our work on GPT-2, discussed further in the technical report we're publishing, will help provide evidence the AI community can draw on when thinking about the publication challenges inherent to some parts of AI research. Timeline OpenAI publishes a blog post and paper on GPT-2. Released small parameter (124M) GPT-2 model. The Partnership on AI co-hosts a dinner with OpenAI to discuss publication norms , then publishes a blog summarizing the discussion. Released medium parameter (355M) model. Released dataset of outputs from large-scale models. Released a detection baseline to help people understand how to detect outputs of models like GPT-2. The original blog post is updated to reflect these changes. Adam King launches \"TalktoTransformer.com\", giving people an interface to play with the newly released models. Hugging Face releases a conversational AI demo based on GPT-2 models, discusses some of the ethical considerations in the release decision, and decides not to release the large GPT-2 model . Researchers with the University of Washington and Allen Institute for AI Research reveal GROVER , a GPT-2–style language model; they do not release the large versions of the model, and conduct research into the detection of the outputs of such models. OpenAI testifies in Congress about the implications of synthetic media, including a discussion of synthetic text. DeepMind discusses GPT-2 and the importance of appropriate publication norms for generative models in their recent discussion of unsupervised learning. OpenAI commences a research collaboration with the Partnership on AI for publication norms in AI research. We're trying to work with a diverse set of AI research organizations to come up with questions scientists may want to ask ahead of publication, and potential frameworks they can use to make publication decisions. DeepTabNine develops a code autocompleter based on GPT-2. Multi-turn Dialogue Response Generation with Autoregressive Transformer Models GLTR: Statistical Detection and Visualization of Generated Text Researchers with the Thoughtful Technology Project and the University of Cambridge published a working paper on “ Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning ”. Hello, It's GPT-2—How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems AI startup AI21 Labs releases HAIM , a neural text generator; they only release a 345M variant of the model, \"equivalent in size to the publicly released versions of Grover and GPT-2.\" NVIDIA Research trains 8.3 billion parameter GPT-2 model. Released larger parameter (774M) model. OpenAI publishes a blog post and paper on GPT-2. Released small parameter (124M) GPT-2 model. The Partnership on AI co-hosts a dinner with OpenAI to discuss publication norms , then publishes a blog summarizing the discussion. Released medium parameter (355M) model. Released dataset of outputs from large-scale models. Released a detection baseline to help people understand how to detect outputs of models like GPT-2. The original blog post is updated to reflect these changes. Adam King launches \"TalktoTransformer.com\", giving people an interface to play with the newly released models. Hugging Face releases a conversational AI demo based on GPT-2 models, discusses some of the ethical considerations in the release decision, and decides not to release the large GPT-2 model . Researchers with the University of Washington and Allen Institute for AI Research reveal GROVER , a GPT-2–style language model; they do not release the large versions of the model, and conduct research into the detection of the outputs of such models. OpenAI testifies in Congress about the implications of synthetic media, including a discussion of synthetic text. DeepMind discusses GPT-2 and the importance of appropriate publication norms for generative models in their recent discussion of unsupervised learning. OpenAI commences a research collaboration with the Partnership on AI for publication norms in AI research. We're trying to work with a diverse set of AI research organizations to come up with questions scientists may want to ask ahead of publication, and potential frameworks they can use to make publication decisions. DeepTabNine develops a code autocompleter based on GPT-2. Multi-turn Dialogue Response Generation with Autoregressive Transformer Models GLTR: Statistical Detection and Visualization of Generated Text Researchers with the Thoughtful Technology Project and the University of Cambridge published a working paper on “ Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning ”. Hello, It's GPT-2—How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems AI startup AI21 Labs releases HAIM , a neural text generator; they only release a 345M variant of the model, \"equivalent in size to the publicly released versions of Grover and GPT-2.\" NVIDIA Research trains 8.3 billion parameter GPT-2 model. Released larger parameter (774M) model. Having these conversations is difficult, as it involves talking candidly about proprietary systems and it’s unclear who to reach out to in specific organizations to discuss such models and what the appropriate processes are for inter-org discussion about unreleased research. ↩︎ These samples were generated via a “human-in-the-loop” process meant to simulate contemporary disinformation operations, where a human generated samples and periodically selected some for exposure to people. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-08-20"},
{"website": "Open-AI", "title": "organizational-update", "author": ["OpenAI"], "link": "https://openai.com/blog/organizational-update/", "abstract": "It’s been a year of dramatic change and growth at OpenAI. In May, we introduced GPT-3—the most powerful language model to date—and soon afterward launched our first commercial product, an API to safely access artificial intelligence models using simple, natural-language prompts. We’re proud of these and other research breakthroughs by our team, all made as part of our mission to achieve general-purpose AI that is safe and reliable, and which benefits all humanity. Today we’re announcing that Dario Amodei, VP of Research, is leaving OpenAI after nearly five years with the company. Dario has made tremendous contributions to our research in that time, collaborating with the team to build GPT-2 and GPT-3, and working with Ilya Sutskever as co-leader in setting the direction for our research. Dario has always shared our goal of responsible AI. He and a handful of OpenAI colleagues are planning a new project, which they tell us will probably focus less on product development and more on research. We support their move and we’re grateful for the time we’ve spent working together. “We are incredibly thankful to Dario for his contributions over the past four and a half years. We wish him and his co-founders all the best in their new project, and we look forward to a collaborative relationship with them for years to come,” said OpenAI chief executive Sam Altman. When his departure was announced at an employee meeting earlier this month, Dario told coworkers, “I want to thank Sam and thank everyone. I’m really proud of the work we’ve done together. I want to wish everyone the best, and I know that OpenAI will do really great things in the years ahead. We share the same goal of safe artificial general intelligence to benefit humanity, so it’s incumbent on all of us in this space to work together to make sure things go well.” OpenAI is also making a few organizational changes to put greater focus on the integration of research, product, and safety. Mira Murati is taking on new responsibilities as senior vice president of Research, Product, and Partnerships, reflecting her strong leadership during our API rollout and across the company. Sam added, “OpenAI’s mission is to thoughtfully and responsibly develop general-purpose artificial intelligence, and as we enter the new year our focus on research—especially in the area of safety—has never been stronger. Making AI safer is a company-wide priority, and a key part of Mira’s new role.” “While GPT-3 and the other AI models we’ve developed are still nascent, we’re beginning to get a better understanding of their behavior, how to make them safer and how to align them with human preferences,” said Mira of her new role. “Our approach is to carefully use these models to improve products that people already use in their everyday lives, as well as create new products—all of which allows us to gain experience with safe deployment. At the same time, we will continue to conduct and publish research on the impact and challenges of AI, independent of our immediate product goals.” Our team will be back in January with new research and updates on our API’s progress. Happy New Year from all of us at OpenAI!", "date": "2020-12-29"},
{"website": "Open-AI", "title": "openai-scholars", "author": ["Larissa Schiavo", "Greg Brockman"], "link": "https://openai.com/blog/openai-scholars/", "abstract": "We're providing 6-10 stipends and mentorship to individuals from underrepresented groups to study deep learning full-time for 3 months and open-source a project. This is a remote program and is open to anyone with US work authorization located in US timezones (we're happy to provide a desk in our San Francisco office if you happen to be located in the Bay Area). In return, we ask that you document your experiences studying deep learning and hopefully inspire others to do the same. Diversity is core to AI having a positive effect on the world — it's necessary to ensure the advanced AI systems in the future are built to benefit everyone. While we hope that some of the scholars will join OpenAI (where we are actively working on internal diversity & inclusion initiatives), we want this program to improve diversity in the field at large. Once you've decided to join the field, there are many programs (such as our Fellowship or any number of residencies ) which can help you develop your skills. But these require a longer commitment and some existing machine learning experience, and for many people with families or other obligations it's not possible to simply pack up and come to the Bay Area. We'll provide you with a $7.5k/mo stipend for 3 months from June 4, 2018 to August 31, 2018. Each scholar will receive $25,000 worth of credits from Amazon Web Services . You'll have a mentor who will provide at least an hour of mentorship via video call each week, answer your questions via chat/email, and work with you to design and execute a good project to stretch your skills. There will be a group Slack with the scholars and mentors. If you're in the Bay Area, we'll optionally provide a desk for you at the OpenAI office. We've lined up the following mentors from OpenAI and the community. AWS is donating $25,000 worth of credits to each community mentor. You should be studying deep learning full-time during the 3 months. You should write a weekly blog post updating the community on your progress: describe what you learned during the week, what materials you found useful, what questions you find yourself asking, etc. You should complete and open source a project by the end of the program. You are eligible to apply if: You are a member of an underrepresented group in science and engineering. You have US work authorization and are located in a US timezone. You understand this article on calculus and this article on linear algebra . It's fine if you have to brush up on these skills. You are comfortable programming in Python (other languages are helpful, but you'll spend the program writing in Python). We’re open to all experience levels and backgrounds that meet the above criteria — it's a common myth that you need a PhD to work in AI (many OpenAI employees don't have one). We'll use these criteria for selection: Impact on you . We want to understand why this grant and mentorship will help you achieve something you couldn't otherwise. Self-motivation & communication . We're looking for people who will work hard through those three months, and who will inspire others (in the program and externally) to endeavor to learn deep learning as well. Technical skills . The stronger your technical background, the more time you'll spend focusing on the deep learning itself. Questions? Email scholars@openai.com . Applications are open starting immediately, and starting March 14th we will begin reviewing applications, contacting people for follow-up, and filling positions on a rolling basis. Applications will close no later than 11:59pm PT on March 31st, with decision sent no later than April 16th. Apply now! Update: Applications for the Summer 2018 Scholars cohort are now closed. We will be reaching out to applicants regarding their admissions status by April 16th.", "date": "2018-03-06"},
{"website": "Open-AI", "title": "dall-e", "author": ["Aditya Ramesh", "Mikhail Pavlov", "Gabriel Goh", "Scott Gray", "Mark Chen", "Rewon Child", "Vedant Misra", "Pamela Mishkin", "Gretchen Krueger", "Sandhini Agarwal", "Ilya Sutskever"], "link": "https://openai.com/blog/dall-e/", "abstract": "DALL·E [1] is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions, using a dataset of text–image pairs. We’ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images. GPT-3 showed that language can be used to instruct a large neural network to perform a variety of text generation tasks. Image GPT showed that the same type of neural network can also be used to generate images with high fidelity. We extend these findings to show that manipulating visual concepts through language is now within reach. Like GPT-3, DALL·E is a transformer language model. It receives both the text and the image as a single stream of data containing up to 1280 tokens, and is trained using maximum likelihood to generate all of the tokens, one after another. [2] This training procedure allows DALL·E to not only generate an image from scratch, but also to regenerate any rectangular region of an existing image that extends to the bottom-right corner, in a way that is consistent with the text prompt. We recognize that work involving generative models has the potential for significant, broad societal impacts. In the future, we plan to analyze how models like DALL·E relate to societal issues like economic impact on certain work processes and professions, the potential for bias in the model outputs, and the longer term ethical challenges implied by this technology. We find that DALL·E is able to create plausible images for a great variety of sentences that explore the compositional structure of language. We illustrate this using a series of interactive visuals in the next section. The samples shown for each caption in the visuals are obtained by taking the top 32 of 512 after reranking with CLIP , but we do not use any manual cherry-picking, aside from the thumbnails and standalone images that appear outside. [3] We test DALL·E’s ability to modify several of an object’s attributes, as well as the number of times that it appears. Simultaneously controlling multiple objects, their attributes, and their spatial relationships presents a new challenge. For example, consider the phrase “a hedgehog wearing a red hat, yellow gloves, blue shirt, and green pants.” To correctly interpret this sentence, DALL·E must not only correctly compose each piece of apparel with the animal, but also form the associations (hat, red), (gloves, yellow), (shirt, blue), and (pants, green) without mixing them up. [4] We test DALL·E’s ability to do this for relative positioning, stacking objects, and controlling multiple attributes. While DALL·E does offer some level of controllability over the attributes and positions of a small number of objects, the success rate can depend on how the caption is phrased. As more objects are introduced, DALL·E is prone to confusing the associations between the objects and their colors, and the success rate decreases sharply. We also note that DALL·E is brittle with respect to rephrasing of the caption in these scenarios: alternative, semantically equivalent captions often yield no correct interpretations. We find that DALL·E also allows for control over the viewpoint of a scene and the 3D style in which a scene is rendered. To push this further, we test DALL·E’s ability to repeatedly draw the head of a well-known figure at each angle from a sequence of equally spaced angles, and find that we can recover a smooth animation of the rotating head. DALL·E appears to be able to apply some types of optical distortions to scenes, as we see with the options “fisheye lens view” and “a spherical panorama.” This motivated us to explore its ability to generate reflections. The samples from the “extreme close-up view” and “x-ray” style led us to further explore DALL·E’s ability to render internal structure with cross-sectional views, and external structure with macro photographs. The task of translating text to images is underspecified: a single caption generally corresponds to an infinitude of plausible images, so the image is not uniquely determined. For instance, consider the caption “a painting of a capybara sitting on a field at sunrise.” Depending on the orientation of the capybara, it may be necessary to draw a shadow, though this detail is never mentioned explicitly. We explore DALL·E’s ability to resolve underspecification in three cases: changing style, setting, and time; drawing the same object in a variety of different situations; and generating an image of an object with specific text written on it. With varying degrees of reliability, DALL·E provides access to a subset of the capabilities of a 3D rendering engine via natural language. It can independently control the attributes of a small number of objects, and to a limited extent, how many there are, and how they are arranged with respect to one another. It can also control the location and angle from which a scene is rendered, and can generate known objects in compliance with precise specifications of angle and lighting conditions. Unlike a 3D rendering engine, whose inputs must be specified unambiguously and in complete detail, DALL·E is often able to “fill in the blanks” when the caption implies that the image must contain a certain detail that is not explicitly stated. Next, we explore the use of the preceding capabilities for fashion and interior design. The compositional nature of language allows us to put together concepts to describe both real and imaginary things. We find that DALL·E also has the ability to combine disparate ideas to synthesize objects, some of which are unlikely to exist in the real world. We explore this ability in two instances: transferring qualities from various concepts to animals, and designing products by taking inspiration from unrelated concepts. In the previous section, we explored DALL·E’s ability to combine unrelated concepts when generating images of real-world objects. Here, we explore this ability in the context of art, for three kinds of illustrations: anthropomorphized versions of animals and objects, animal chimeras, and emojis. GPT-3 can be instructed to perform many kinds of tasks solely from a description and a cue to generate the answer supplied in its prompt, without any additional training. For example, when prompted with the phrase “here is the sentence ‘a person walking his dog in the park’ translated into French:”, GPT-3 answers “un homme qui promène son chien dans le parc.” This capability is called zero-shot reasoning. We find that DALL·E extends this capability to the visual domain, and is able to perform several kinds of image-to-image translation tasks when prompted in the right way. We did not anticipate that this capability would emerge, and made no modifications to the neural network or training procedure to encourage it. Motivated by these results, we measure DALL·E’s aptitude for analogical reasoning problems by testing it on Raven’s progressive matrices, a visual IQ test that saw widespread use in the 20th century. We find that DALL·E has learned about geographic facts, landmarks, and neighborhoods. Its knowledge of these concepts is surprisingly precise in some ways and flawed in others. In addition to exploring DALL·E’s knowledge of concepts that vary over space, we also explore its knowledge of concepts that vary over time. DALL·E is a simple decoder-only transformer that receives both the text and the image as a single stream of 1280 tokens—256 for the text and 1024 for the image—and models all of them autoregressively. The attention mask at each of its 64 self-attention layers allows each image token to attend to all text tokens. DALL·E uses the standard causal mask for the text tokens, and sparse attention for the image tokens with either a row, column, or convolutional attention pattern, depending on the layer. We provide more details about the architecture and training procedure in our paper . Text-to-image synthesis has been an active area of research since the pioneering work of Reed et. al, whose approach uses a GAN conditioned on text embeddings. The embeddings are produced by an encoder pretrained using a contrastive loss, not unlike CLIP. StackGAN and StackGAN++ use multi-scale GANs to scale up the image resolution and improve visual fidelity. AttnGAN incorporates attention between the text and image features, and proposes a contrastive text-image feature matching loss as an auxiliary objective. This is interesting to compare to our reranking with CLIP, which is done offline. Other work incorporates additional sources of supervision during training to improve image quality. Finally, work by Nguyen et. al and Cho et. al explores sampling-based strategies for image generation that leverage pretrained multimodal discriminative models. Similar to the rejection sampling used in VQVAE-2 , we use CLIP to rerank the top 32 of 512 samples for each caption in all of the interactive visuals. This procedure can also be seen as a kind of language-guided search , and can have a dramatic impact on sample quality. Aditya Ramesh was the project lead: he developed the approach, trained the models, and wrote most of the blog copy. Aditya Ramesh, Mikhail Pavlov, and Scott Gray worked together to scale up the model to 12 billion parameters, and designed the infrastructure used to draw samples from the model. Aditya Ramesh, Gabriel Goh, and Justin Jay Wang worked together to create the interactive visuals for the blog. Mark Chen and Aditya Ramesh created the images for Raven's Progressives Matrices. Rewon Child and Vedant Misra assisted in writing the blog. Pamela Mishkin, Gretchen Krueger, and Sandhini Agarwal advised on broader impacts of the work and assisted in writing the blog. Ilya Sutskever oversaw the project and assisted in writing the blog. Thanks to the following for their feedback on this work and contributions to this release: Alec Radford, Andrew Mayne, Jeff Clune, Ashley Pilipiszyn, Steve Dowling, Jong Wook Kim, Lei Pan, Heewoo Jun, John Schulman, Michael Tabatowski, Preetum Nakkiran, Jack Clark, Fraser Kelton, Jacob Jackson, Greg Brockman, Wojciech Zaremba, Justin Mao-Jones, David Luan, Shantanu Jain, Prafulla Dhariwal, Sam Altman, Pranav Shyam, Miles Brundage, Jakub Pachocki, and Ryan Lowe. Justin Jay Wang We decided to name our model using a portmanteau of the artist Salvador Dalí and Pixar's WALL·E. ↩︎ A token is any symbol from a discrete vocabulary; for humans, each English letter is a token from a 26-letter alphabet. DALL·E's vocabulary has tokens for both text and image concepts. Specifically, each image caption is represented using a maximum of 256 BPE-encoded tokens with a vocabulary size of 16384, and the image is represented using 1024 tokens with a vocabulary size of 8192. The images are preprocessed to 256x256 resolution during training. Similar to VQVAE, each image is compressed to a 32x32 grid of discrete latent codes using a discrete VAE that we pretrained using a continuous relaxation. We found that training using the relaxation obviates the need for an explicit codebook, EMA loss, or tricks like dead code revival, and can scale up to large vocabulary sizes. ↩︎ Further details provided in a later section . ↩︎ This task is called variable binding, and has been extensively studied in the literature. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2021-01-05"},
{"website": "Open-AI", "title": "scaling-kubernetes-to-7500-nodes", "author": ["Benjamin Chess", "Eric Sigler"], "link": "https://openai.com/blog/scaling-kubernetes-to-7500-nodes/", "abstract": "We've scaled Kubernetes clusters to 7,500 nodes, producing a scalable infrastructure for large models like GPT-3 , CLIP , and DALL·E , but also for rapid small-scale iterative research such as Scaling Laws for Neural Language Models . Scaling a single Kubernetes cluster to this size is rarely done and requires some special care, but the upside is a simple infrastructure that allows our machine learning research teams to move faster and scale up without changing their code. Since our last post on Scaling to 2,500 Nodes we've continued to grow our infrastructure to meet researcher needs, in the process learning many additional lessons. This post summarizes those lessons so that others in the Kubernetes community can benefit from them, and ends with problems we still face that we'll be tackling next. Before we get too far, it’s important to describe our workload. The applications and hardware we run with Kubernetes are pretty different from what you may encounter at a typical company. Our problems and corresponding solutions may, or may not, be a good match to your own setup! A large machine learning job spans many nodes and runs most efficiently when it has access to all of the hardware resources on each node. This allows GPUs to cross-communicate directly using NVLink , or GPUs to directly communicate with the NIC using GPUDirect . So for many of our workloads, a single pod occupies the entire node. Any NUMA, CPU, or PCIE resource contention aren't factors for scheduling. Bin-packing or fragmentation is not a common problem. Our current clusters have full bisection bandwidth, so we also don’t make any rack or network topology considerations. All of this means that, while we have many nodes, there’s relatively low strain on the scheduler. That said, strain on the kube-scheduler is spiky. A new job may consist of many hundreds of pods all being created at once, then return to a relatively low rate of churn. Our biggest jobs run MPI, and all pods within the job are participating in a single MPI communicator. If any of the participating pods die, the entire job halts and needs to be restarted. The job checkpoints regularly, and when restarted it resumes from the last checkpoint. Thus we consider the pods to be semi-stateful —killed pods can be replaced and work can continue, but doing so is disruptive and should be kept to a minimum. We don’t rely on Kubernetes load balancing all that much. We have very little HTTPS traffic, with no need for A/B testing, blue/green, or canaries. Pods communicate directly with one another on their pod IP addresses with MPI via SSH, not service endpoints. Service “discovery” is limited; we just do a one-time lookup for which pods are participating in MPI at job startup time. Most jobs interact with some form of blob storage. They usually either stream some shards of a dataset or checkpoint directly from blob storage, or cache it to a fast local ephemeral disk. We have a few PersistentVolumes for cases where POSIX semantics are useful, but blob storage is far more scalable and doesn’t require slow detach/attach operations. Lastly, the nature of our work is fundamentally research, which means the workloads themselves are ever-changing. While the Supercomputing team strives to provide what we’d consider a “production” quality level of compute infrastructure, the applications that run on that cluster are short-lived and their developers iterate quickly. New usage patterns may emerge at any time that challenge our assumptions about trends and appropriate tradeoffs. We need a sustainable system that also allows us to respond quickly when things change. As the number of nodes and pods within our clusters increased, we found that Flannel had difficulties scaling up the throughput required. We switched to using the native pod networking technologies for our IP Configurations for Azure VMSSes and the relevant CNI plugins. This allowed us to get host level network throughput on our pods. Another reason we've switched to using alias-based IP addressing is that on our largest clusters, we could possibly have approximately 200,000 IP addresses in use at any one time. When we tested route-based pod networking, we found there were significant limitations in the number of routes we could effectively use. Avoiding encapsulation increases the demands on the underlying SDN or routing engine, but it keeps our networking setup simple. Adding VPN or tunneling can be done without any additional adapters. We don't need to worry about packet fragmentation due to some portion of the network having a lower MTU. Network policies and traffic monitoring is straightforward; there's no ambiguity about the source and destination of packets. We use iptables tagging on the host to track network resource usage per Namespace and pod. This lets researchers visualize their network usage patterns. In particular, since a lot of our experiments have distinct Internet and intra-pod communication patterns, it's often useful to be able to investigate where any bottlenecks might be occurring. iptables mangle rules can be used to arbitrarily mark packets that match particular criteria. Here are our rules to detect whether traffic is internal or internet-bound. The FORWARD rules cover traffic from pods, vs INPUT and OUTPUT traffic from the host: Once marked, iptables will start counters to track the number of bytes and packets that match this rule. You can eyeball these counters by using iptables itself: We use an open-source Prometheus exporter called iptables-exporter to then get these tracked into our monitoring system. This a simple way to track packets matching a variety of different types of conditions. One somewhat unique aspect of our network model is that we fully expose the node, pod, and service network CIDR ranges to our researchers. We have a hub and spoke network model, and use the native node and pod CIDR ranges to route that traffic. Researchers connect to the hub, and from there have access to any of the individual clusters (the spokes). But the clusters themselves cannot talk to one another. This ensures that clusters remain isolated with no cross-cluster dependencies that can break failure isolation. We use a \"NAT\" host to translate the service network CIDR range for traffic coming from outside of the cluster. This setup allows our researchers significant flexibility in choosing how and what kinds of network configurations they are able to choose from for their experiments. Kubernetes API Servers and etcd are critical components to a healthy working cluster, so we pay special attention to the stress on these systems. We use the Grafana dashboards provided by kube-prometheus , as well as additional in-house dashboards. We’ve found it useful to alert on the rate of HTTP status 429 (Too Many Requests) and 5xx (Server Error) on the API Servers as a high-level signal of problems. While some folks run API Servers within kube, we’ve always run them outside the cluster itself. Both etcd and API servers run on their own dedicated nodes. Our largest clusters run 5 API servers and 5 etcd nodes to spread the load and minimize impact if one were to ever go down. We’ve had no notable trouble with etcd since splitting out Kubernetes Events into their own etcd cluster back in our last blog post . API Servers are stateless and generally easy to run in a self-healing instance group or scaleset. We haven’t yet tried to build any self-healing automation of etcd clusters because incidents have been extremely rare. API Servers can take up a fair bit of memory, and that tends to scale linearly with the number of nodes in the cluster. For our cluster with 7,500 nodes we observe up to 70GB of heap being used per API Server, so fortunately this should continue to be well-within hardware capabilities into the future. One big strain on API Servers was WATCHes on Endpoints. There are a few services, such as ‘kubelet’ and ‘node-exporter’ of which every node in the cluster is a member. When a node would be added or removed from the cluster, this WATCH would fire. And because typically each node itself was watching the kubelet service via kube-proxy, the # and bandwidth required in these responses would be $N^2$ and enormous, occasionally 1GB/s or more. EndpointSlices , launched in Kubernetes 1.17, were a huge benefit that brought this load down 1000x. In general we are very mindful of any API Server requests that scale with the size of the cluster. We try to avoid having any DaemonSets interact with the API Server. In cases where you do need each node to watch for changes, introducing an intermediary caching service, such as the Datadog Cluster Agent , seems to be a good pattern to avoid cluster-wide bottlenecks. As our clusters have grown, we do less actual autoscaling of our clusters. But we have run into trouble occasionally when autoscaling too much at once. There are many requests generated when a new node joins a cluster, and adding hundreds of nodes at once can overload API server capacity. Smoothing this out, even just by a few seconds, has helped avoid outages. We use Prometheus to collect time-series metrics and Grafana for graphs, dashboards, and alerts. We started with a deployment of kube-prometheus that collects a wide variety of metrics and good dashboards for visualization. Over time we’ve added many of our own dashboards, metrics, and alerts. As we added more and more nodes, we struggled with the sheer amount of metrics being collected by Prometheus. While kube-prometheus exposes a lot of useful data, some of it we weren’t actually ever looking at, and some was just too granular to collect, store, and query effectively. We use Prometheus rules to “drop” some of these metrics from being ingested. For a while we struggled with a problem where Prometheus would consume more and more memory until eventually crashing the container in an Out-Of-Memory error (OOM). This seemed to occur even after throwing enormous amounts of memory capacity at the application. What’s worse was, when it did crash, it would take many hours on startup replaying write-ahead-log files before it was usable again. Eventually we tracked down the source of these OOMs to be an interaction between Grafana and Prometheus, where Grafana would use the /api/v1/series API on Prometheus with a query of {le!=\"\"} (Basically, “give me all the histogram metrics”). The implementation of /api/v1/series was unbounded in both time and space—for a query with a lot of results, this would continue to consume ever-more memory and time. It also continues to grow even after the requester has given up and closed the connection. For us, there was never enough memory, and Prometheus would eventually crash. We patched Prometheus to contain this API within a Context to enforce a timeout, which fixed it entirely. While Prometheus crashed far less often, in times when we did need to restart it, WAL replay remained an issue. It would often take many hours to replay through all WAL logs before Prometheus was up collecting new metrics and servicing queries. With help from Robust Perception , we found that applying a GOMAXPROCS=24 had a big improvement. Prometheus tries to use all cores when during WAL replay, and for servers with a large number of cores, the contention kills all performance. We’re exploring new options to increase our monitoring capacity, described in the “ Unsolved problems ” section below. With a cluster this large, we of course rely on automation to detect and remove misbehaving nodes from the cluster. Over time we have built up a number of healthcheck systems. Some healthchecks are passive, always running on all nodes. These monitor basic system resources such as network reachability, bad or full disks, or GPU errors. GPUs exhibit problems a number of different ways, but an easy common one is an “Uncorrectable ECC error.” Nvidia’s Data Center GPU Manager (DCGM) tools make it easy to query for this and a number of other “Xid” errors. One way we track these errors is via dcgm-exporter to ingest the metrics into Prometheus, our monitoring system. This will appear as the DCGM_FI_DEV_XID_ERRORS metric and be set to the error code that has most recently occurred. Additionally, the NVML Device Query API exposes more detailed information about the health and operation of a GPU. Once we detect an error, they can often be fixed by resetting the GPU or system, though in some cases it does lead to the underlying GPU needing to be physically replaced. Another form of healthcheck tracks maintenance events from the upstream cloud provider. Each of the major cloud providers expose a way to know if the current VM is due for an upcoming maintenance event that will eventually cause a disruption. The VM may need to be rebooted so an underlying hypervisor patch can be applied or the physical node swapped out for other hardware. These passive healthchecks run constantly in the background on all nodes. If a healthcheck starts failing, the node is automatically cordoned so no new pods are to be scheduled on the node. For more serious healthcheck failures, we will also attempt a pod eviction to request all currently-running pods to exit immediately. It’s still up to the pod itself, configurable via a Pod Disruption Budget, to decide if it wants to allow this eviction to occur. Eventually, either after all pods have terminated, or 7 days has elapsed (part of our SLA), we will forcibly terminate the VM. Unfortunately not all GPU problems manifest as error codes visible through DCGM. We’ve built up our own library of tests that exercise GPUs to catch additional problems and ensure that the hardware and driver is behaving as expected. These tests can’t be run in the background—they require exclusive use of a GPU for several seconds or minutes to run. We first run these tests on nodes upon boot, in a system we call “preflight.” All nodes join the cluster with a “preflight” taint and label applied. This taint prevents normal pods from being scheduled on the node. A DaemonSet is configured to run preflight test pods on all nodes with this label. Upon successful completion of the test, the test itself removes the taint and label and the node is then available for general use. We also then run these tests periodically during the lifetime of a node. We run this as a CronJob, allowing it to land on any available node in the cluster. This is admittedly a bit random and uncontrolled about which nodes get tested, but we’ve found that over time it provides sufficient coverage with minimal coordination or disruption. As we scaled up our clusters, researchers started to find themselves having difficulty getting all of the capacity that they were allocated. Traditional job scheduling systems have a lot of different features available to fairly run work between competing teams, which Kubernetes does not have. Over time, we took inspiration from those job scheduling systems and build several capabilities in a Kubernetes-native way. We have a service in each cluster, \"team-resource-manager\" that has multiple functions. Its data source is a ConfigMap that specifies tuples of (node selector, team label to apply, allocation amount) for all of the research teams that have capacity in a given cluster. It reconciles this with the current nodes in the cluster, tainting the appropriate number of nodes with openai.com/team=teamname:NoSchedule . “team-resource-manager” also has an admission webhook service, such that as each job is submitted, a corresponding toleration is applied based on the submitter's team membership. Using taints allows us to constrain the Kubernetes pod scheduler flexibly, such as allowing a \"any\" toleration for lower priority pods, which allows teams to borrow each other's capacity without requiring heavyweight coordination. In addition to using cluster-autoscaler to dynamically scale our VM-backed clusters, we use it to remediate (remove & re-add) unhealthy members within the cluster. We do this by setting the \"min size\" of the cluster to zero, and the \"max size\" of the cluster to the capacity available. However, cluster-autoscaler, if it sees idle nodes, will attempt to scale down to only needed capacity. For multiple reasons (VM spin up latency, pre-allocated costs, the API server impacts mentioned above) this idle-scaling isn't ideal. So, we introduced a balloon Deployment for both our CPU-only and GPU hosts. This Deployment contains a ReplicaSet with \"max size\" number of low-priority pods. These pods occupy resources within a node, so the autoscaler doesn't consider them as idle. However since they're low priority, the scheduler can evict them immediately to make room for actual work. (We chose to use a Deployment instead of a DaemonSet, to avoid the DaemonSet being considered idle workload on a node.) One thing of note, we use pod anti-affinity to ensure the pods would evenly distribute across the nodes. Earlier versions of the Kubernetes scheduler had an $O(N^2)$ performance issue with pod anti-affinity. This has been corrected since Kubernetes 1.18. Our experiments often involve one or more StatefulSets, each operating a different portion of the training effort. For Optimizers, researchers need all members of the StatefulSet to be scheduled, before any training can be done (as we often use MPI to coordinate between optimizer members, and MPI is sensitive to group membership changes). However, Kubernetes by default won't necessarily prioritize fulfilling all requests from one StatefulSet over another. For example if two experiments each requested 100% of the cluster's capacity, instead of scheduling all of one experiment or the other, Kubernetes might schedule only half of each experiment's pods, leading to a deadlock where neither experiment can make progress. We tried a few things needing a custom scheduler, but ran into edge cases that caused conflicts with how normal pods were scheduled. Kubernetes 1.18 introduced a plugin architecture for the core Kubernetes scheduler, making it much easier to add features like this natively. We recently landed on the Coscheduling plugin as a good way to solve this problem. There are many problems still to address as we scale up our Kubernetes clusters. A few of them include: At our scale we’ve had many difficulties with Prometheus’s built-in TSDB storage engine being slow to compact, and needing long times needed to replay the WAL (Write-Ahead-Log) any time it restarts. Queries also tend to result in “query processing would load too many samples” errors. We’re in the process of migrating to a different Prometheus-compatible storage and query engine. Look forward to a future blog post about how it goes! As we scale up our clusters, each pod is calculated to have a certain amount of Internet bandwidth available. The aggregate Internet bandwidth requirements per person have become substantial, and our researchers now have the ability to unintentionally put a significant resource strain on other locations on the Internet, such as datasets for download and software packages to install. We’ve found Kubernetes to be an exceptionally flexible platform for our research needs. It has the ability to scale up to meet the most demanding workloads we’ve put on it. There are many areas yet though where it needs improvement, and the Supercomputing team at OpenAI will continue to explore how Kubernetes can scale. If this kind of work seems interesting, you should consider applying at OpenAI!", "date": "2021-01-25"},
{"website": "Open-AI", "title": "clip", "author": ["Alec Radford", "Ilya Sutskever", "Jong Wook Kim", "Gretchen Krueger", "Sandhini Agarwal"], "link": "https://openai.com/blog/clip/", "abstract": "We’re introducing a neural network called CLIP which efficiently learns visual concepts from natural language supervision. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to be recognized, similar to the “zero-shot” capabilities of GPT-2 and GPT-3. Although deep learning has revolutionized computer vision, current approaches have several major problems: typical vision datasets are labor intensive and costly to create while teaching only a narrow set of visual concepts; standard vision models are good at one task and one task only, and require significant effort to adapt to a new task; and models that perform well on benchmarks have disappointingly poor performance on stress tests, casting doubt on the entire deep learning approach to computer vision. We present a neural network that aims to address these problems: it is trained on a wide variety of images with a wide variety of natural language supervision that’s abundantly available on the internet. By design, the network can be instructed in natural language to perform a great variety of classification benchmarks, without directly optimizing for the benchmark’s performance, similar to the “ zero-shot ” capabilities of GPT-2 and GPT-3. This is a key change: by not directly optimizing for the benchmark, we show that it becomes much more representative: our system closes this “robustness gap” by up to 75% while matching the performance of the original ResNet-50 on ImageNet zero-shot without using any of the original 1.28M labeled examples. Although both models have the same accuracy on the ImageNet test set, CLIP’s performance is much more representative of how it will fare on datasets that measure accuracy in different, non-ImageNet settings. For instance, ObjectNet checks a model's ability to recognize objects in many different poses and with many different backgrounds inside homes while ImageNet Rendition and ImageNet Sketch check a model's ability to recognize more abstract depictions of objects. CLIP ( Contrastive Language–Image Pre-training ) builds on a large body of work on zero-shot transfer, natural language supervision, and multimodal learning. The idea of zero-data learning dates back over a decade but until recently was mostly studied in computer vision as a way of generalizing to unseen object categories. A critical insight was to leverage natural language as a flexible prediction space to enable generalization and transfer. In 2013, Richer Socher and co-authors at Stanford developed a proof of concept by training a model on CIFAR-10 to make predictions in a word vector embedding space and showed this model could predict two unseen classes. The same year DeVISE scaled this approach and demonstrated that it was possible to fine-tune an ImageNet model so that it could generalize to correctly predicting objects outside the original 1000 training set. Most inspirational for CLIP is the work of Ang Li and his co-authors at FAIR who in 2016 demonstrated using natural language supervision to enable zero-shot transfer to several existing computer vision classification datasets, such as the canonical ImageNet dataset. They achieved this by fine-tuning an ImageNet CNN to predict a much wider set of visual concepts (visual n-grams) from the text of titles, descriptions, and tags of 30 million Flickr photos and were able to reach 11.5% accuracy on ImageNet zero-shot. Finally, CLIP is part of a group of papers revisiting learning visual representations from natural language supervision in the past year. This line of work uses more modern architectures like the Transformer and includes VirTex, which explored autoregressive language modeling, ICMLM, which investigated masked language modeling, and ConVIRT, which studied the same contrastive objective we use for CLIP but in the field of medical imaging. We show that scaling a simple pre-training task is sufficient to achieve competitive zero-shot performance on a great variety of image classification datasets. Our method uses an abundantly available source of supervision: the text paired with images found across the internet. This data is used to create the following proxy training task for CLIP: given an image, predict which out of a set of 32,768 randomly sampled text snippets, was actually paired with it in our dataset. In order to solve this task, our intuition is that CLIP models will need to learn to recognize a wide variety of visual concepts in images and associate them with their names. As a result, CLIP models can then be applied to nearly arbitrary visual classification tasks. For instance, if the task of a dataset is classifying photos of dogs vs cats we check for each image whether a CLIP model predicts the text description “a photo of a dog ” or “a photo of a cat ” is more likely to be paired with it. CLIP pre-trains an image encoder and a text encoder to predict which images were paired with which texts in our dataset. We then use this behavior to turn CLIP into a zero-shot classifier. We convert all of a dataset’s classes into captions such as “a photo of a dog ” and predict the class of the caption CLIP estimates best pairs with a given image. CLIP was designed to mitigate a number of major problems in the standard deep learning approach to computer vision: Costly datasets : Deep learning needs a lot of data, and vision models have traditionally been trained on manually labeled datasets that are expensive to construct and only provide supervision for a limited number of predetermined  visual concepts. The ImageNet dataset, one of the largest efforts in this space, required over 25,000 workers to annotate 14 million images for 22,000 object categories. In contrast, CLIP learns from text–image pairs that are already publicly available on the internet. Reducing the need for expensive large labeled datasets has been extensively studied by prior work, notably self-supervised learning, contrastive methods, self-training approaches, and generative modeling. Narrow : An ImageNet model is good at predicting the 1000 ImageNet categories, but that’s all it can do “out of the box.” If we wish to perform any other task, an ML practitioner needs to build a new dataset, add an output head, and fine-tune the model. In contrast, CLIP can be adapted to perform a wide variety of visual classification tasks without needing additional training examples. To apply CLIP to a new task, all we need to do is “tell” CLIP’s text-encoder the names of the task’s visual concepts, and it will output a linear classifier of CLIP’s visual representations. The accuracy of this classifier is often competitive with fully supervised models. We show random, non-cherry picked, predictions of zero-shot CLIP classifiers on examples from various datasets below. Poor real-world performance : Deep learning systems are often reported to achieve human or even superhuman performance [1] on vision benchmarks, yet when deployed in the wild, their performance can be far below the expectation set by the benchmark. In other words, there is a gap between “benchmark performance” and “real performance.” We conjecture that this gap occurs because the models “cheat” by only optimizing for performance on the benchmark, much like a student who passed an exam by studying only the questions on past years’ exams. In contrast, the CLIP model can be evaluated on benchmarks without having to train on their data, so it can’t “cheat” in this manner. This results in its benchmark performance being much more representative of its performance in the wild. To verify the “cheating hypothesis”, we also measure how CLIP's performance changes when it is able to “study” for ImageNet. When a linear classifier is fitted on top of CLIP's features, it improves CLIP's accuracy on the ImageNet test set by almost 10%. However, this classifier does no better on average across an evaluation suite of 7 other datasets measuring “robust” performance. CLIP learns from unfiltered, highly varied, and highly noisy data, and is intended to be used in a zero-shot manner. We know from GPT-2 and 3 that models trained on such data can achieve compelling zero shot performance; however, such models require significant training compute. To reduce the needed compute, we focused on algorithmic ways to improve the training efficiency of our approach. We report two algorithmic choices that led to significant compute savings. The first choice is the adoption of a contrastive objective for connecting text with images. We originally explored an image-to-text approach, similar to VirTex, but encountered difficulties scaling this to achieve state-of-the-art performance. In small to medium scale experiments, we found that the contrastive objective used by CLIP is 4x to 10x more efficient at zero-shot ImageNet classification. The second choice was the adoption of the Vision Transformer, which gave us a further 3x gain in compute efficiency over a standard ResNet. In the end, our best performing CLIP model trains on 256 GPUs for 2 weeks which is similar to existing large scale image models. Because they learn a wide range of visual concepts directly from natural language, CLIP models are significantly more flexible and general than existing ImageNet models. We find they are able to zero-shot perform many different tasks. To validate this we have measured CLIP’s zero-shot performance on over 30 different datasets including tasks such as fine-grained object classification, geo-localization, action recognition in videos, and OCR. [2] In particular, learning OCR is an example of an exciting behavior that does not occur in standard ImageNet models. Above, we visualize a random non-cherry picked prediction from each zero-shot classifier. This finding is also reflected on a standard representation learning evaluation using linear probes. The best CLIP model outperforms the best publicly available ImageNet model, the Noisy Student EfficientNet-L2, on 20 out of 26 different transfer datasets we tested. While CLIP usually performs well on recognizing common objects, it struggles on more abstract or systematic tasks such as counting the number of objects in an image and on more complex tasks such as predicting how close the nearest car is in a photo. On these two datasets, zero-shot CLIP is only slightly better than random guessing. Zero-shot CLIP also struggles compared to task specific models on very fine-grained classification, such as telling the difference between car models, variants of aircraft, or flower species. CLIP also still has poor generalization to images not covered in its pre-training dataset. For instance, although CLIP learns a capable OCR system, when evaluated on handwritten digits from the MNIST dataset, zero-shot CLIP only achieves 88% accuracy, well below the 99.75% of humans on the dataset. Finally, we’ve observed that CLIP's zero-shot classifiers can be sensitive to wording or phrasing and sometimes require trial and error “prompt engineering” to perform well. CLIP allows people to design their own classifiers and removes the need for task-specific training data. The manner in which these classes are designed can heavily influence both model performance and model biases. For example, we find that when given a set of labels including Fairface race labels [3] and a handful of egregious terms such as “criminal”, “animal,” etc., the model tends to classify images of people aged 0–20 in the egregious category at a rate of ~32.3%. However, when we add the class “child” to the list of possible classes, this behaviour drops to ~8.7%. Additionally, given that CLIP does not need task-specific training data it can unlock certain niche tasks with greater ease. Some of these tasks may raise privacy or surveillance related risks and we explore this concern by studying the performance of CLIP on celebrity identification. CLIP has a top-1 accuracy of 59.2% for “in the wild” celebrity image classification when choosing from 100 candidates and a top-1 accuracy of 43.3% when choosing from 1000 possible choices. Although it’s noteworthy to achieve these results with task agnostic pre-training, this performance is not competitive when compared to widely available production level models. We further explore challenges that CLIP poses in our paper and we hope that this work motivates future research on the characterization of the capabilities, shortcomings, and biases of such models. We are excited to engage with the research community on such questions. With CLIP, we’ve tested whether task agnostic pre-training on internet scale natural language, which has powered a recent breakthrough in NLP, can also be leveraged to improve the performance of deep learning for other fields. We are excited by the results we’ve seen so far applying this approach to computer vision. Like the GPT family, CLIP learns a wide variety of tasks during pre-training which we demonstrate via zero-shot transfer. We are also encouraged by our findings on ImageNet that suggest zero-shot evaluation is a more representative measure of a model’s capability. We'd like to thank the millions of people involved in creating the data CLIP is trained on. We also are grateful to all our co-authors for their contributions to the project. Finally, we'd like to thank Jeff Clune, Miles Brundage, Ryan Lowe, Jakub Pachocki, and Vedant Misra for feedback on drafts of this blog and Matthew Knight for reviewing the code release. Justin Jay Wang In 2015, a group of researchers from Microsoft first trained a model which achieved a top-5 accuracy on ImageNet that surpassed reported human top-5 accuracy. ↩︎ While CLIP’s zero-shot OCR performance is mixed, its semantic OCR representation is quite useful. When evaluated on the SST-2 NLP dataset rendered as images, a linear classifer on CLIP’s representation matches a CBoW model with direct access to the text. CLIP is also competitive at detecting hateful memes without needing ground truth text. ↩︎ FairFace is a face image dataset designed to balance age, gender, and race, in order to reduce asymmetries common in previous face datasets. It categorizes gender into 2 groups: female and male and race into 7 groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. There are inherent problems with race and gender classifications, as e.g. Bowker and Star (2000) and Keyes (2018) have shown. While FairFace’s dataset reduces the proportion of White faces, it still lacks representation of entire large demographic groups, effectively erasing such categories. We use the 2 gender categories and 7 race categories defined in the FairFace dataset in a number of our experiments not in order to reinforce or endorse the use of such reductive categories, but in order to enable us to make comparisons to prior work. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2021-01-05"},
{"website": "Open-AI", "title": "openai-api", "author": ["Greg Brockman", "Mira Murati", "Peter Welinder", "OpenAI"], "link": "https://openai.com/blog/openai-api/", "abstract": "We’re releasing an API for accessing new AI models developed by OpenAI. Unlike most AI systems which are designed for one use-case, the API today provides a general-purpose “text in, text out” interface, allowing users to try it on virtually any English language task. You can now request access in order to integrate the API into your product, develop an entirely new application, or help us explore the strengths and limits of this technology. Given any text prompt, the API will return a text completion, attempting to match the pattern you gave it. You can \"program\" it by showing it just a few examples of what you'd like it to do; its success generally varies depending on how complex the task is. The API also allows you to hone performance on specific tasks by training on a dataset (small or large) of examples you provide, or by learning from human feedback provided by users or labelers. We've designed the API to be both simple for anyone to use but also flexible enough to make machine learning teams more productive. In fact, many of our teams are now using the API so that they can focus on machine learning research rather than distributed systems problems. Today the API runs models with weights from the GPT-3 family with many speed and throughput improvements. Machine learning is moving very fast, and we're constantly upgrading our technology so that our users stay up to date. The field's pace of progress means that there are frequently surprising new applications of AI, both positive and negative. We will terminate API access for obviously harmful use-cases, such as harassment, spam, radicalization, or astroturfing. But we also know we can't anticipate all of the possible consequences of this technology, so we are launching today in a private beta rather than general availability, building tools to help users better control the content our API returns, and researching safety-relevant aspects of language technology (such as analyzing, mitigating, and intervening on harmful bias). We'll share what we learn so that our users and the broader community can build more human-positive AI systems. In addition to being a revenue source to help us cover costs in pursuit of our mission , the API has pushed us to sharpen our focus on general-purpose AI technology—advancing the technology, making it usable, and considering its impacts in the real world. We hope that the API will greatly lower the barrier to producing beneficial AI-powered products, resulting in tools and services that are hard to imagine today. Interested in exploring the API? Join companies like Algolia , Quizlet , and Reddit , and researchers at institutions like the Middlebury Institute in our private beta . Ultimately, what we care about most is ensuring artificial general intelligence benefits everyone. We see developing commercial products as one of the ways to make sure we have enough funding to succeed. We also believe that safely deploying powerful AI systems in the world will be hard to get right. In releasing the API, we are working closely with our partners to see what challenges arise when AI systems are used in the real world. This will help guide our efforts to understand how deploying future AI systems will go, and what we need to do to make sure they are safe and beneficial for everyone. There are three main reasons we did this. First, commercializing the technology helps us pay for our ongoing AI research, safety, and policy efforts. Second, many of the models underlying the API are very large, taking a lot of expertise to develop and deploy and making them very expensive to run. This makes it hard for anyone except larger companies to benefit from the underlying technology. We’re hopeful that the API will make powerful AI systems more accessible to smaller businesses and organizations. Third, the API model allows us to more easily respond to misuse of the technology. Since it is hard to predict the downstream use cases of our models, it feels inherently safer to release them via an API and broaden access over time, rather than release an open source model where access cannot be adjusted if it turns out to have harmful applications. With GPT-2, one of our key concerns was malicious use of the model (e.g., for disinformation), which is difficult to prevent once a model is open sourced. For the API, we’re able to better prevent misuse by limiting access to approved customers and use cases. We have a mandatory production review process before proposed applications can go live. In production reviews, we evaluate applications across a few axes, asking questions like: Is this a currently supported use case? , How open-ended is the application? , How risky is the application? , How do you plan to address potential misuse? , and Who are the end users of your application? . We terminate API access for use cases that are found to cause (or are intended to cause) physical, emotional, or psychological harm to people, including but not limited to harassment, intentional deception, radicalization, astroturfing, or spam, as well as applications that have insufficient guardrails to limit misuse by end users. As we gain more experience operating the API in practice, we will continually refine the categories of use we are able to support, both to broaden the range of applications we can support, and to create finer-grained categories for those we have misuse concerns about. One key factor we consider in approving uses of the API is the extent to which an application exhibits open-ended versus constrained behavior with regard to the underlying generative capabilities of the system. Open-ended applications of the API (i.e., ones that enable frictionless generation of large amounts of customizable text via arbitrary prompts) are especially susceptible to misuse. Constraints that can make generative use cases safer include systems design that keeps a human in the loop, end user access restrictions, post-processing of outputs, content filtration, input/output length limitations, active monitoring, and topicality limitations. We are also continuing to conduct research into the potential misuses of models served by the API, including with third-party researchers via our academic access program . We’re starting with a very limited number of researchers at this time and already have some results from our academic partners at Middlebury Institute , University of Washington, and Allen Institute for AI . We have tens of thousands of applicants for this program already and are currently prioritizing applications focused on fairness and representation research. Mitigating negative effects such as harmful bias is a hard, industry-wide issue that is extremely important. As we discuss in the GPT-3 paper and model card , our API models do exhibit biases that will be reflected in generated text. Here are the steps we’re taking to address these issues: We’ve developed usage guidelines that help developers understand and address potential safety issues. We’re working closely with users to understand their use cases and develop tools to surface and intervene to mitigate harmful bias. We’re conducting our own research into manifestations of harmful bias and broader issues in fairness and representation, which will help inform our work via improved documentation of existing models as well as various improvements to future models. We recognize that bias is a problem that manifests at the intersection of a system and a deployed context; applications built with our technology are sociotechnical systems, so we work with our developers to ensure they’re putting in appropriate processes and human-in-the-loop systems to monitor for adverse behavior. Our goal is to continue to develop our understanding of the API’s potential harms in each context of use, and continually improve our tools and processes to help minimize them. Updated September 18, 2020", "date": "2020-06-11"},
{"website": "Open-AI", "title": "image-gpt", "author": ["Mark Chen", "Alec Radford", "Ilya Sutskever"], "link": "https://openai.com/blog/image-gpt/", "abstract": "We find that, just as a large transformer model trained on language can generate coherent text, the same exact model trained on pixel sequences can generate coherent image completions and samples . By establishing a correlation between sample quality and image classification accuracy, we show that our best generative model also contains features competitive with top convolutional nets in the unsupervised setting. Unsupervised and self-supervised learning, or learning without human-labeled data, is a longstanding challenge of machine learning. Recently, it has seen incredible success in language, as transformer models like BERT, GPT-2, RoBERTa, T5, and other variants have achieved top performance on a wide array of language tasks. However, the same broad class of models has not been successful in producing strong features for image classification. Our work aims to understand and bridge this gap. Transformer models like BERT and GPT-2 are domain agnostic, meaning that they can be directly applied to 1-D sequences of any form. When we train GPT-2 on images unrolled into long sequences of pixels, which we call iGPT, we find that the model appears to understand 2-D image characteristics such as object appearance and category. This is evidenced by the diverse range of coherent image samples it generates, even without the guidance of human provided labels. As further proof, features from the model achieve state-of-the-art performance on a number of classification datasets and near state-of-the-art unsupervised accuracy [1] on ImageNet. To highlight the potential of generative sequence modeling as a general purpose unsupervised learning algorithm, we deliberately use the same transformer architecture as GPT-2 in language. As a consequence, we require significantly more compute in order to produce features competitive with those from top unsupervised convolutional nets. However, our results suggest that when faced with a new domain where the correct model priors are unknown, a large GPT-2 can learn excellent features without the need for domain-specific architectural design choices. In language, unsupervised learning algorithms that rely on word prediction (like GPT-2 and BERT) have been extremely successful, achieving top performance on a wide array of language tasks. One possible reason for this success is that instances of downstream language tasks appear naturally in text: questions are often followed by answers (which could help with question-answering) and passages are often followed by summaries (which could help with summarization). In contrast, sequences of pixels do not clearly contain labels for the images they belong to. Even without this explicit supervision, there is still a reason why GPT-2 on images might work: a sufficiently large transformer trained on next pixel prediction might eventually learn to generate diverse [2] samples with clearly recognizable objects. Once it learns to do so, an idea known as “Analysis by Synthesis” [3] suggests that the model will also know about object categories. Many early generative models were motivated by this idea, and more recently, BigBiGAN was an example which produced encouraging samples and features. In our work, we first show that better generative models achieve stronger classification performance. Then, through optimizing GPT-2 for generative capabilities, we achieve top-level classification performance in many settings, providing further evidence for analysis by synthesis. Generative sequence modeling is a universal unsupervised learning algorithm: since all data types can be represented as sequences of bytes, a transformer can be directly applied to any data type without additional engineering. Our work tests the power of this generality by directly applying the architecture used to train GPT-2 on natural language to image generation. We deliberately chose to forgo hand coding any image specific knowledge in the form of convolutions or techniques like relative attention, sparse attention, and 2-D position embeddings. As a consequence of its generality, our method requires significantly more compute to achieve competitive performance in the unsupervised setting. Indeed, contrastive methods are still the most computationally efficient methods for producing high quality features from images. However, in showing that an unsupervised transformer model is competitive with the best unsupervised convolutional nets, we provide evidence that it is possible to trade off hand coded domain knowledge for compute. In new domains, where there isn’t much knowledge to hand code, scaling compute seems an appropriate technique to test. We train iGPT-S, iGPT-M, and iGPT-L, transformers containing 76M, 455M, and 1.4B parameters respectively, on ImageNet. We also train iGPT-XL [4] , a 6.8 billion parameter transformer, on a mix of ImageNet and images from the web. Due to the large computational cost of modeling long sequences with dense attention, we train at the low resolutions of 32x32, 48x48, and 64x64. While it is tempting to work at even lower resolutions to further reduce compute cost, prior work has demonstrated that human performance on image classification begins to drop rapidly below these sizes. Instead, motivated by early color display palettes, we create our own 9-bit color palette to represent pixels. Using this palette yields an input sequence length 3 times shorter than the standard (R, G, B) palette, while still encoding color faithfully. There are two methods we use to assess model performance, both of which involve a downstream classification task. The first, which we refer to as a linear probe, uses the trained model to extract features [5] from the images in the downstream dataset, and then fits a logistic regression to the labels. The second method fine-tunes [6] the entire model on the downstream dataset. Since next pixel prediction is not obviously relevant to image classification, features from the final layer may not be the most predictive of the object category. Our first result shows that feature quality is a sharply increasing, then mildly decreasing function of depth. This behavior suggests that a transformer generative model operates in two phases: in the first phase, each position gathers information from its surrounding context in order to build a contextualized image feature. In the second phase, this contextualized feature is used to solve the conditional next pixel prediction task.  The observed two stage performance of our linear probes is reminiscent of another unsupervised neural net, the bottleneck autoencoder, which is manually designed so that features in the middle are used. Our next result establishes the link between generative performance and feature quality. We find that both increasing the scale of our models and training for more iterations result in better generative performance, which directly translates into better feature quality. When we evaluate our features using linear probes on CIFAR-10, CIFAR-100, and STL-10, we outperform features from all supervised and unsupervised transfer algorithms. Our results are also compelling in the full fine-tuning setting. Given the resurgence of interest in unsupervised and self-supervised learning on ImageNet, we also evaluate the performance of our models using linear probes on ImageNet. This is an especially difficult setting, as we do not train at the standard ImageNet input resolution. Nevertheless, a linear probe on the 1536 features from the best layer of iGPT-L trained on 48x48 images yields 65.2% top-1 accuracy, outperforming AlexNet. Contrastive methods typically report their best results on 8192 features, so we would ideally evaluate iGPT with an embedding dimension of 8192 for comparison. However, training such a model is prohibitively expensive, so we instead concatenate features from multiple layers as an approximation. Unfortunately, our features tend to be correlated across layers, so we need more of them to be competitive. Taking 15360 features from 5 layers in iGPT-XL yields 72.0% top-1 accuracy, outperforming AMDIM, MoCo, and CPC v2, but still underperforming SimCLR by a decent margin. Because masked language models like BERT have outperformed generative models on most language tasks, we also evaluate the performance of BERT on our image models. Instead of training our model to predict the next pixel given all preceding pixels, we mask out 15% of the pixels and train our model to predict them from the unmasked ones. We find that though linear probe performance on BERT models is significantly worse, they excel during fine-tuning: While unsupervised learning promises excellent features without the need for human-labeled data, significant recent progress has been made under the more forgiving framework of semi-supervised learning, which allows for limited amounts of human-labeled data. Successful semi-supervised methods often rely on clever techniques such as consistency regularization, data augmentation, or pseudo-labeling, and purely generative-based approaches have not been competitive for years. We evaluate iGPT-L [7] on a competitive benchmark for this sub-field and find that a simple linear probe on features from non-augmented images outperforms Mean Teacher and MixMatch, though it underperforms FixMatch. While we have shown that iGPT is capable of learning powerful image features, there are still significant limitations to our approach. Because we use the generic sequence transformer used for GPT-2 in language, our method requires large amounts of compute: iGPT-L was trained for roughly 2500 V100-days while a similarly performing MoCo model can be trained in roughly 70 V100-days. Relatedly, we model low resolution inputs using a transformer, while most self-supervised results use convolutional-based encoders which can easily consume inputs at high resolution. A new architecture, such as a domain-agnostic multiscale transformer, might be needed to scale further. Given these limitations, our work primarily serves as a proof-of-concept demonstration of the ability of large transformer-based language models to learn excellent unsupervised representations in novel domains, without the need for hardcoded domain knowledge. However, the significant resource cost to train these models and the greater accuracy of convolutional neural-network based methods precludes these representations from practical real-world applications in the vision domain. Finally, generative models can exhibit biases that are a consequence of the data they've been trained on. Many of these biases are useful, like assuming that a combination of brown and green pixels represents a branch covered in leaves, then using this bias to continue the image. But some of these biases will be harmful, when considered through a lens of fairness and representation. For instance, if the model develops a visual notion of a scientist that skews male, then it might consistently complete images of scientists with male-presenting people, rather than a mix of genders. We expect that developers will need to pay increasing attention to the data that they feed into their systems and to better understand how it relates to biases in trained models. We have shown that by trading off 2-D knowledge for scale and by choosing predictive features from the middle of the network, a sequence transformer can be competitive with top convolutional nets for unsupervised image classification. Notably, we achieved our results by directly applying the GPT-2 language model to image generation. Our results suggest that due to its simplicity and generality, a sequence transformer given sufficient compute might ultimately be an effective way to learn excellent features in many domains. If you’re excited to work with us on this area of research, we’re hiring ! Foremost, we would like to acknowledge our paper co-authors Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, and David Luan. Thanks to the following for their feedback on this work and contributions to this release: Vedant Misra, Noah Golmant, Johannes Otterbach, Pranav Shyam, Aditya Ramesh, Yura Burda, Harri Edwards, Chris Hallacy, Jeff Clune, Jack Clark, Irene Solaiman, Ryan Lowe, Greg Brockman, Kelly Sims, David Farhi, Will Guss, Quoc V. Le, and Ashish Vaswani. Measured through logistic regression on learned features (linear probe). ↩︎ A transformer is trained to maximize the likelihood, and thus is mode covering, which automatically ensures the diversity of its samples. ↩︎ The original analysis by synthesis idea is more an argument for generative models with latent variables, but because generative models without latent variables were so much better at modeling the data distribution, we thought the analysis-by-synthesis conjecture should hold for them as well. ↩︎ We only show linear probe accuracy on ImageNet for iGPT-XL since other experiments did not finish before we needed to transition to different supercomputing facilities. ↩︎ To extract features for a linear probe, we take the post layernorm attention block inputs at some layer and average pool over the sequence dimension. ↩︎ To fine-tune, we take the post layernorm transformer output and average pool over the sequence dimension as input for the classification head. ↩︎ A generative model which learns features in a purely unsupervised fashion. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2020-06-17"},
{"website": "Open-AI", "title": "openai-scholars-2018-final-projects", "author": ["Larissa Schiavo"], "link": "https://openai.com/blog/openai-scholars-2018-final-projects/", "abstract": "Our first cohort of OpenAI Scholars has now completed the program. Over the past three months, we've seen how quickly experienced software developers can become machine learning practitioners. All eight Scholars produced an exciting final project and are going on to work or teach within machine learning. We'll be hosting a Scholars Demo Day at OpenAI to showcase their work. Our Scholars will talk about their work this summer, followed by Q&A, mingling, and refreshments with the Scholars and other OpenAI folks. This event is open to all who are interested—just RSVP at the link below! We plan to repeat the Scholars program in 2019. The program is open to people from groups underrepresented in the field, and we think that AI will only benefit everyone if the field has better representation. We'll also release a case study on the first cohort in upcoming months to help other organizations roll out similar initiatives at their own companies. We asked each of the Scholars to talk about what they learned during their time as an OpenAI Scholar, as well as what they’re looking forward to next. Here’s what they had to say: This summer I had a basic introduction to RL, and I learned how to use Tensorflow within the Unity game engine to create a multi-agent soccer game. I also learned a lot about language modeling (through my music generation project). I feel much more fluent in PyTorch and TensorFlow than I did at the start of the summer. Final Project: Adapted current language modeling techniques to model classical music via two different approaches (notewise and chordwise) with parallel character and word level language models. This lets you generate solo piano music or piano and violin chamber music. You also have the option of training the generative model on a large music database or only on specific composers or styles. This project also incorporates a music critic network which scores whether a music sample is real or fake, and another which tries to determine who composed the sample. What’s Next? Joining the Fall 2018 class of OpenAI Fellows to continue to expand her ML research knowledge. This summer with OpenAI was truly amazing. The Scholars community has been truly supportive. Everyone was incredibly vested in our success. One thing I learned is how important pictures are to a deeper understanding. My mentor encouraged me to draw out and explain the math and processes behind what I was doing. I found that I was most helped by online blog posts drawing out what was going on. I also developed a much deeper appreciation for data generation and feature engineering. The most amazing thing I got out of this experience was the confidence to know that I could actually work in this space. Final Project: Built a model with good performance on the SemEval STS Task, Track 4; explored different iterations of what the model will look like, from LSTMs to the Transformer architecture to RNNs. What’s Next? Working as an engineer at Microsoft for a year before pursuing a dual PhD at the University of Illinois in Computer Science and Linguistics in 2019. This summer I gained a concrete understanding about various neural network architectures, especially those used in creative practices, such as recurrent neural networks, generative adversarial networks, variational autoencoders, and their iterations. I also learned how to implement neural networks in Tensorflow and Pytorch. Final project: An experiment in generating emotional landscapes with a GAN, a conditional VAE, and a multi-scale VAE to varying degrees of success. This was a two-week project after previously working on a dataset of stand-up comedy punchlines and realizing it would take longer than the time available. What’s Next?: Teaching a graduate-level course on generative music this fall at New York University, then work as a machine learning engineer and continuing to work with generative music and emotion datasets. I’m very appreciative of my three months at OpenAI. For the first two months, I was able to learn about the latest reinforcement learning algorithms and OpenAI’s toolkits. With my final project, I was able to learn about training with multiple outputs, modifying a CycleGAN model with additional loss terms, and increasing my experience with Tensorflow and Keras. In the context of Machine Learning, I learned patience, persistence, and to spend some time just thinking. These qualities helped me to work through diverse projects each week. Additionally, I found that writing about my experiences helped to clarify my thinking on a project and come up with some interesting solutions. Final project: An Art Composition Attributes network with a pretrained ResNet50 network fine-tuned on eight different art composition attributes, used within a CycleGAN network to generate art compositions by setting target values for each art attribute. What’s Next?: Begin working in Machine Learning as an engineer, based in New Mexico; continue to develop final project and incorporate it into her art practice. I experienced the joy and excitement that comes with making progress and also the self-doubt and despair that attends failure. It’s been quite the intellectual journey and one I’m glad I didn’t have to take on alone. I know I’ve still got a long way to go, but I’m proud of the progress I’ve made. When I started this program, I’d not trained a single neural network. I’d read and written about them, but I had zero hands-on experience. Machine learning is not an easy topic, but now I feel less like I’m trying to scale Mount Everest and more like I’m climbing up a ladder. Knowing the path forward makes the way shorter. Final Project: Built a network that can learn the rules of how objects move in space in the same manner that humans learn those rules—via observation and without explicit definition of concepts like momentum, force, or friction. What’s Next? Work as an ML engineer after spending a few months as a front-end developer while learning and practicing ML skills a little more. Before the program, I had seen TensorFlow code in tutorials but I had not worked with it myself. During the program, I got to explore and familiarize myself with the code in the TF libraries and to modify the libraries to get them to work on new data. I’m more aware of the field now. It’s easier to read and understand papers. Before the program, reading deep learning papers took a long time because every few sentences I would encounter a term or concept I had never heard before. Now when I read a paper, I understand the terminology as well as why the authors might have made certain choices. I also learned how others in this industry build deep neural nets—common practices, prominent architectures and popular datasets, tools, and the like. Knowing this helps me know where to start on new projects. Final Project: Exploring the use of semantic trees in LSTMs as a way to better represent the relationships between entities in a sentence. What’s Next? Apply this Machine Learning expertise as an engineering consultant to help small to mid-size businesses take advantage of ML; hosting a short seminar on deep learning in Harare, Zimbabwe to help inform engineers in his home country. I am grateful for the time and space that this program provided me with to self-study at the intersection of deep learning and NLP. I found and refined my blogging voice, including more visual storytelling than ever before. I am much more comfortable working with deep learning frameworks, especially PyTorch and Keras. And I now have experience building an end-to-end, deep learning product.  I couldn’t have done it without this support system! Final Project: @deephypebot is a music commentary generator. It is essentially a language model, trained on past human music writing from the web and conditioned on attributes of the referenced music. There is an additional training step that attempts to encourage a certain type of descriptive, almost flowery writing commonly found in this genre. Our goal is to teach this language model to generate consistently good and entertaining new writing about songs. What’s Next? Go broader on/experiment more with creative applications in ML - both new and existing - and start looking and applying for ML engineer roles! During this program, I significantly increased my expertise in AI. I became more comfortable building models from scratch, working with TensorFlow, and built a general understanding of reinforcement learning. I’ve built a deeper understanding of NLP through a lot of low level implementations of common algorithms like word2vec, simple RNNs, LSTMs and various preprocessing approaches to text. In my final project, I’m combining NLP with RL where an agent achieves target cells in the Gridworld environment upon commands. The project will be modified to be more language conditioned as a step towards grounded language learning. What’s Next? Sophia will continue to pursue her education in machine learning with a view towards a career in AI research. To wrap up this first class of Scholars, we will be bringing all the Scholars together with members of the ML community for a demo day at OpenAI's offices on September the 20th. Here the Scholars will present their work and discuss their experiences and future ML goals with other attendees. If you are interested and would like to attend, sign up here . We’ll be recording their talks to share later. We can’t wait to see how our Scholars continue to contribute to the ML community in the future! Watch the talks", "date": "2018-09-10"},
{"website": "Open-AI", "title": "multimodal-neurons", "author": ["Gabriel Goh", "Chelsea Voss", "Daniela Amodei", "Shan Carter", "Michael Petrov", "Justin Jay Wang", "Nick Cammarata", "Chris Olah"], "link": "https://openai.com/blog/multimodal-neurons/", "abstract": "We’ve discovered neurons in CLIP that respond to the same concept whether presented literally, symbolically, or conceptually. This may explain CLIP's accuracy in classifying surprising visual renditions of concepts, and is also an important step toward understanding the associations and biases that CLIP and similar models learn. Fifteen years ago , Quiroga et al. discovered that the human brain possesses multimodal neurons. These neurons respond to clusters of abstract concepts centered around a common high-level theme, rather than any specific visual feature. The most famous of these was the “Halle Berry” neuron, a neuron featured in both Scientific American and The New York Times , that responds to photographs, sketches, and the text “Halle Berry” (but not other names). Two months ago, OpenAI announced CLIP , a general-purpose vision system that matches the performance of a ResNet-50, but outperforms existing vision systems on some of the most challenging datasets. Each of these challenge datasets, ObjectNet , ImageNet Rendition , and ImageNet Sketch , stress tests the model’s robustness to not recognizing not just simple distortions or changes in lighting or pose, but also to complete abstraction and reconstruction—sketches, cartoons, and even statues of the objects. Now, we’re releasing our discovery of the presence of multimodal neurons in CLIP. One such neuron, for example, is a “Spider-Man” neuron (bearing a remarkable resemblance to the “Halle Berry” neuron) that responds to an image of a spider, an image of the text “spider,” and the comic book character “Spider-Man” either in costume or illustrated. Our discovery of multimodal neurons in CLIP gives us a clue as to what may be a common mechanism of both synthetic and natural vision systems—abstraction. We discover that the highest layers of CLIP organize images as a loose semantic collection of ideas, providing a simple explanation for both the model’s versatility and the representation’s compactness. Biological neurons, such as the famed Halle Berry neuron, do not fire for visual clusters of ideas, but semantic clusters. At the highest layers of CLIP, we find similar semantic invariance. Note that images are replaced by higher resolution substitutes from Quiroga et al., and that the images from Quiroga et al. are themselves substitutes of the original stimuli. Using the tools of interpretability, we give an unprecedented look into the rich visual concepts that exist within the weights of CLIP. Within CLIP, we discover high-level concepts that span a large subset of the human visual lexicon—geographical regions, facial expressions, religious iconography, famous people and more. By probing what each neuron affects downstream, we can get a glimpse into how CLIP performs its classification. Our paper builds on nearly a decade of research into interpreting convolutional networks, beginning with the observation that many of these classical techniques are directly applicable to CLIP. We employ two tools to understand the activations of the model: feature visualization , which maximizes the neuron’s firing by doing gradient-based optimization on the input, and dataset examples , which looks at the distribution of maximal activating images for a neuron from a dataset. Using these simple techniques, we’ve found the majority of the neurons in CLIP RN50x4 (a ResNet-50 scaled up 4x using the EfficientNet scaling rule) to be readily interpretable. Indeed, these neurons appear to be extreme examples of “multi-faceted neurons,” neurons that respond to multiple distinct cases, only at a higher level of abstraction. Selected neurons from the final layer of four CLIP models. Each neuron is represented by a feature visualization with a human-chosen concept labels to help quickly provide a sense of each neuron. Labels were picked after looking at hundreds of stimuli that activate the neuron, in addition to feature visualizations. We chose to include some of the examples here to demonstrate the model’s proclivity towards stereotypical depictions of regions, emotions, and other concepts. We also see discrepancies in the level of neuronal resolution: while certain countries like the US and India were associated with well-defined neurons, the same was not true of countries in Africa, where neurons tended to fire for entire regions. We discuss some of these biases and their implications in later sections. Indeed, we were surprised to find many of these categories appear to mirror neurons in the medial temporal lobe documented in epilepsy patients with intracranial depth electrodes. These include neurons that respond to emotions, animals, and famous people. But our investigation into CLIP reveals many more such strange and wonderful abstractions, including neurons that appear to count [ 17 , 202 , 310 ], neurons responding to art styles [ 75 , 587 , 122 ], even images with evidence of digital alteration [ 1640 ]. While this analysis shows a great breadth of concepts, we note that a simple analysis on a neuron level cannot represent a complete documentation of the model’s behavior. The authors of CLIP have demonstrated, for example, that the model is capable of very precise geolocation, (Appendix E.4, Figure 20) with a granularity that extends down to the level of a city and even a neighborhood. In fact, we offer an anecdote: we have noticed, by running our own personal photos through CLIP, that CLIP can often recognize if a photo was taken in San Francisco, and sometimes even the neighborhood (e.g., “Twin Peaks”). Despite our best efforts, however, we have not found a \"San Francisco\" neuron, nor did it seem from attribution that San Francisco decomposes nicely into meaningful unit concepts like \"California\" and \"city.\" We believe this information to be encoded within the activations of the model somewhere, but in a more exotic way, either as a direction or as some other more complex manifold. We believe this to be a fruitful direction for further research. These multimodal neurons can give us insight into understanding how CLIP performs classification. With a sparse linear probe, we can easily inspect CLIP's weights to see which concepts combine to achieve a final classification for ImageNet classification: The piggy bank class appears to be a composition of a “finance” neuron along with a porcelain neuron. The Spider-Man neuron referenced in the first section of the paper is also a spider detector, and plays an important role in the classification of the class “barn spider.” For text classification, a key observation is that these concepts are contained within neurons in a way that, similar to the word2vec objective, is almost linear . The concepts, therefore, form a simple algebra that behaves similarly to a linear probe. By linearizing the attention, we too can inspect any sentence, much like a linear probe, as shown below: Probing how CLIP understands words, it appears to the model that the word “surprised” implies some not just some measure of shock, but a shock of a very specific kind, one combined perhaps with delight or wonder. “Intimate” consists of a soft smile and hearts, but not sickness. We note that this reveals a reductive understanding of the the full human experience of intimacy-the subtraction of illness precludes, for example, intimate moments with loved ones who are sick. We find many such omissions when probing CLIP's understanding of language. The degree of abstraction in CLIP surfaces a new vector of attack that we believe has not manifested in previous systems. Like many deep networks, the representations at the highest layers of the model are completely dominated by such high-level abstractions. What distinguishes CLIP, however, is a matter of degree—CLIP’s multimodal neurons generalize across the literal and the iconic, which may be a double-edged sword. Through a series of carefully-constructed experiments, we demonstrate that we can exploit this reductive behavior to fool the model into making absurd classifications. We have observed that the excitations of the neurons in CLIP are often controllable by its response to images of text , providing a simple vector of attacking the model. The finance neuron [ 1330 ], for example, responds to images of piggy banks, but also responds to the string “$$$” . By forcing the finance neuron to fire, we can fool our model into classifying a dog as a piggy bank. We refer to these attacks as typographic attacks . We believe attacks such as those described above are far from simply an academic concern. By exploiting the model’s ability to read text robustly, we find that even photographs of hand-written text can often fool the model. Like the Adversarial Patch, this attack works in the wild; but unlike such attacks, it requires no more technology than pen and paper. We also believe that these attacks may also take a more subtle, less conspicuous form. An image, given to CLIP, is abstracted in many subtle and sophisticated ways, and these abstractions may over-abstract common patterns—oversimplifying and, by virtue of that, overgeneralizing. Our model, despite being trained on a curated subset of the internet, still inherits its many unchecked biases and associations. Many associations we have discovered appear to be benign, but yet we have discovered several cases where CLIP holds associations that could result in representational harm, such as denigration of certain individuals or groups. We have observed, for example, a “Middle East” neuron [1895] with an association with terrorism; and an “immigration” neuron [395] that responds to Latin America. We have even found a neuron that fires for both dark-skinned people and gorillas [ 1257 ], mirroring earlier photo tagging incidents in other models we consider unacceptable. These associations present obvious challenges to applications of such powerful visual systems. [1] Whether fine-tuned or used zero-shot, it is likely that these biases and associations will remain in the system, with their effects manifesting in both visible and nearly invisible ways during deployment. Many biased behaviors may be difficult to anticipate a priori, making their measurement and correction difficult. We believe that these tools of interpretability may aid practitioners the ability to preempt potential problems, by discovering some of these associations and ambigiuities ahead of time. Our own understanding of CLIP is still evolving, and we are still determining if and how we would release large versions of CLIP. We hope that further community exploration of the released versions as well as the tools we are announcing today will help advance general understanding of multimodal systems, as well as inform our own decision-making. Alongside the publication of “Multimodal Neurons in Artificial Neural Networks,” we are also releasing some of the tools we have ourselves used to understand CLIP—the OpenAI Microscope catalog has been updated with feature visualizations, dataset examples, and text feature visualizations for every neuron in CLIP RN50x4. We are also releasing the weights of CLIP RN50x4 and RN101 to further accommodate such research. We believe these investigations of CLIP only scratch the surface in understanding CLIP’s behavior, and we invite the research community to join in improving our understanding of CLIP and models like it. Visit OpenAI Microscope Sandhini Agarwal, Greg Brockman, Miles Brundage, Jeff Clune, Steve Dowling, Jonathan Gordon, Gretchen Krueger, Faiz Mandviwalla, Vedant Misra, Reiichiro Nakano, Ashley Pilipiszyn, Alec Radford, Aditya Ramesh, Pranav Shyam, Ilya Sutskever, Martin Wattenberg & Hannah Wong Note that the released CLIP models are intended strictly for research purposes. See the associated model card . ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2021-03-04"},
{"website": "Open-AI", "title": "openai-scholars-2019", "author": ["Ashley Pilipiszyn", "Larissa Schiavo", "Greg Brockman"], "link": "https://openai.com/blog/openai-scholars-2019/", "abstract": "We are now accepting applications for our second cohort of OpenAI Scholars, a program where we provide 6–10 stipends and mentorship to individuals from underrepresented groups to study deep learning full-time for 3 months and open-source a project. The first cohort of Scholars recently released their projects and presented at the first Scholars Demo Day , and have moved on to work , teach , or otherwise be involved within deep learning. Diversity is core to AI having a positive effect on the world—it’s necessary to ensure the advanced AI systems in the future are built to benefit everyone. While we hope that some of the scholars will join OpenAI (as happened with the first cohort !), we want this program to improve diversity in the field at large. $7.5k/mo stipend for 3 months from February 1, 2019 through May 2, 2019. $25,000 worth of credits from Amazon Web Services. A guaranteed interview with OpenAI for full-time work after completion of the Scholars program. Access to a group Slack with the scholars and mentors. If you're in the Bay Area, we'll optionally provide a desk for you at the OpenAI office (which our past Scholars have found very valuable). A mentor, who will work with you 1:1 via video call each week, answer your questions via chat/email, and work with you to design and execute a good project to stretch your skills. Mentors for this cohort include: We’re open to all experience levels and backgrounds that meet the above criteria—it’s a common myth that you need a PhD to work in AI (many OpenAI employees don’t have one). We look for people who are comfortable writing software, but no previous machine learning experience is required. This is a remote program open to anyone with US work authorization located in US timezones. We ask all Scholars to document their experiences studying deep learning to hopefully inspire others to join the field too. You are eligible to apply if: You are a member of an underrepresented group in science and engineering. You have US work authorization and are located in the United States for the duration of the program; We give preference to applicants who can be physically present during the program. You understand this article on calculus and this article on linear algebra . (It’s fine if you have to brush up on these skills). You are comfortable programming in Python (other languages are helpful, but you’ll spend the program writing in Python). We’ll use these criteria for selection: Technical skills . The stronger your technical background, the more time you’ll spend focusing on the deep learning itself. Self-motivation . We’re looking for people who will work hard through those 3 months, and have demonstrated that they can push themselves to persevere through challenging work. Community building capabilities . We want to hear from people who will inspire others (in the program and externally) to endeavor to learn deep learning as well. Applications will be evaluated on a rolling basis. Questions? Email scholars@openai.com", "date": "2018-10-11"},
{"website": "Open-AI", "title": "openai-scholars-2019-meet-our-scholars", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-scholars-2019-meet-our-scholars/", "abstract": "Our class of eight scholars (out of 550 applicants) brings together collective expertise in literature, philosophy, cell biology, statistics, economics, quantum physics, and business innovation. Our scholars are applying these specializations to current AI research and documenting their progress as they continue to grow as machine learning practitioners . This is our second class of OpenAI Scholars. Their program began in February and will conclude with the completion of an open-source final project. Throughout the program, scholars share their progress with the research community through their blogs. Some applications our scholars are working towards are: Applying reinforcement learning to robotic manipulation Improving inference and reasoning in natural language processing Applying reinforcement learning algorithms to sentiment analysis Fatma received her PhD in Comparative Literature from the University of Texas at Austin in 2016 and earned her Masters in Computer Science from Stanford University in 2018 as an Eric Roberts Fellow. Her knowledge of languages, cultures, and literature led her to explore the human dimension of AGI. Fatma is currently a computer science instructor at St. Edwards University and is interested in the intersection between natural language processing (NLP) and computer vision (CV). She is an avid advocate of diversity in AI and believes that a better representation in AI is critical as it permeates into all aspects of human life. As an OpenAI Scholar, Fatma works on NLP methodologies and aims to complete a project that explores ways of improving inference and reasoning in NLP. Jonathan is a cell biologist (PhD), mathematician (BA), and robotics enthusiast who is deeply interested in the movement and control of complex systems. At the cellular level, he studied the mechanisms that control cell-shape changes in embryonic cells. As an aspiring roboticist, he is applying reinforcement learning to robot manipulation. His long-term research objective is to combine tools from machine learning and optimization with insights from control theory to design algorithms for robotic locomotion and manipulation in real-world settings. Nancy has been researching learning for the past 10 years. Thinking about human construction of knowledge is her passion. With a background in software engineering, math, psychology and education from Stanford University, Nancy wants to use multidisciplinary approaches to develop AI prototypes that could improve education. She’s also interested in understanding how AI is redefining how, why and what humans will learn in the near future. She’s on the founding team of the Portfolio School, a project-based school in NYC, and the co-founder of a non-profit in Mexico. Elynn received her PhD in Statistics in 2018. Her PhD focused on spectral method and matrix/tensor factorization on high- and multi-dimensional data. Her research interests lie at the intersection of statistical learning theory, machine learning and optimization. At OpenAI, she works on deep RL and its applications to healthcare and business management. Helen is a PhD student in Resource Economics and a Masters student in Statistics at UC Davis. Her research interests focus on machine learning methods (both classical statistical learning and deep learning) and their application to Energy Economics, and heterogeneous causal inference. She was an applied research intern at Microsoft in 2018 and a 2017 research fellow with Data Science for Social Good at the University of Chicago. In 2018, she was awarded Twitter's Grace Hopper fellowship and also the Women in Quantitative Finance fellowship. As an OpenAI Scholar, Helen works on RL methodologies and plans to complete a project that can apply RL algorithms on sentiment analysis. Yuhao recently graduated from Carleton College studying Mathematics and Philosophy. Fascinated by the structure and dynamics of our world, Yuhao also explored physics, law, and economics. She discovered her interest in research and problem solving through Budapest Semesters in Mathematics and REU in Combinatorics and Algorithms for Real Problems. At OpenAI, Yuhao studies machine learning with a focus on deep reinforcement learning. Currently, she is interested in understanding how learning methods exhibit certain degrees of generalization. Janet has always been fascinated by the visual dimension & using spatial approaches to help augment analysis in traditionally non-visual problem domains. As an OpenAI Scholar, she investigates the possibilities of generative models & their ability to help identify the most critical features of data/images as part of generating reconstructions. Currently, Janet leads Atakote, where she works with technologies like augmented & virtual reality to transform traditional industries such as retail, manufacturing, and transportation. Previously, Janet studied at Harvard Business School & worked at major companies, such as McKinsey & Company, in 20+ countries. Edgar is a recent graduate of Cornell University’s Physics program. Originally trained as an experimentalist working on hybrid-quantum systems, he dove into deep learning by applying techniques in computer vision to search for sub-atomic particles represented as images. He hopes to provide people with the resources they need by utilizing AI’s power to accomplish tasks that were once only possible by humans. To work towards this goal, Edgar spends his time as an OpenAI Scholar focusing on natural language understanding. Our Scholars demonstrate core technical skills across various expert domains and self-motivation—critical competences for a self-directed program like this one. They each entered the field of machine learning as relative newcomers, and we hope their progress shows how accessible machine learning is. To begin your learning journey, check out some of our educational materials . Thanks to AWS for providing compute credits to the scholars. Additional thank you to our dedicated community mentors for their time advising the scholars on their projects.", "date": "2019-03-13"},
{"website": "Open-AI", "title": "openai-scholars-2019-final-projects", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-scholars-2019-final-projects/", "abstract": "Our second class of OpenAI Scholars has concluded, with all eight scholars producing an exciting final project showcased at Scholars Demo Day at OpenAI. Over the past three months, we’ve seen how experienced engineers working in software, medicine, physics, child development and other fields can become machine learning practitioners with our combination of educational resources and mentorship. Fatma Tarlaci Jonathan Michaux Nancy Otero Elynn Chen Helen (Mengxin) Ji Yuhao Wan Janet Brown Edgar Barraza Previous Role: Eric Roberts Fellow in Computer Science at Stanford University Despite the recent successes of powerful language models, reasoning remains a challenging task in Natural Language Understanding. Question Answering (QA) requires a comprehensive mix of language processing and reasoning skills within a single task. Evaluating a system’s successes and failures on QA tasks provides valuable insights into its reasoning mechanism. This project experiments with fine-tuning of the GPT-2 small model for QA to analyze its performance on reasoning. The OpenAI Scholars program allowed me to build a solid foundation in deep learning and gain a thorough understanding of Natural Language Processing and Understanding. The program also allowed me to define my research interests in AI more clearly by providing me with the resources to experiment with various subfields of deep learning. Previous Role: PhD student in Cell and Molecular Biology at the University of Chicago Many robotics problems are naturally formulated such that the extrinsic rewards to the agent are either sparse or missing altogether. These problems can be extremely difficult to solve as the environment provides limited feedback to guide the agent toward accomplishing its goal. Previous work has shown that agents that train using prediction error as an intrinsic reward are able to learn across a wide range of domains, including Atari games and continuous control tasks. In this project, I used curiosity-driven exploration to solve challenging robotics tasks with sparse rewards. I then formulated the intrinsic reward as the error in the agent’s ability to predict its next state, given its current state and executed action. My results demonstrated that this approach is capable of solving several difficult robotic manipulation tasks in simulation. Before joining the Scholars program I had already undertaken a plan to self-study robotics. The OpenAI Scholars program gave me the opportunity to greatly enhance my self-study with a curriculum focused exclusively on Deep Reinforcement Learning. After spending 8 weeks reading papers and implementing core Deep RL algorithms, I was able to apply what I learned to solving a suite of challenging robotics problems. Previous Roles: Software engineer at Palo Alto Networks ; Founding Director of Learning Design and Research; Founded nonprofit based in Mexico; Stanford Education School Project-based learning is a very effective and enjoyable way to learn, but teachers often struggle to find appropriate projects for their students. Despite thousands of projects existing online, most are poorly labeled and thus difficult for teachers to find. Accurately labeling the thousands of online projects would be daunting and expensive on a case-by-case basis. CREATURE is a proof-of-concept model that labels online projects with 75–90% accuracy. The OpenAI Scholars program demonstrated that given the right mentorship, trust, and financial support, learning ML to do a self-directed project is possible. I learned about language models, data collection and processing, model tuning, and how to integrate all that into a ready-to-use model for educational purposes. I'm excited to keep working on my project, dive deeper into the relationship between human intelligence and AI, and translate what I learned during this program into learning activities others can use. Previous Role: PhD student at Princeton University I developed a computer system that learns from historical electronic health records (EHR) and recommends optimal therapeutic treatment—dosage of IV fluids and vasopressor—based on patient's vitals and lab values. I specifically considered policy iteration and tabular Q-learning with discrete state and action spaces. Results revealed that the optimal RL policies recommend lower doses of IV fluids and higher doses of vasopressors than the physician’s actual treatments. Off-policy evaluation showed that optimal policy learned by Q-learning had higher reward than the one learned by policy iteration. The system can be easily extended to deal with continuous state/action space and incorporate other off-policy RL algorithms. I learned about NNs, CNNs, RNNs, LSTMs and deep reinforcement learning. I implemented different NN architectures and most RL algorithms including DQN, VPG, TRPO, PPO, and DDPG. Before this program, I majored in Statistics and had no experience with deep learning. The OpenAI Scholars program provided me with the guidance and resources to learn core deep learning methods in a short amount of time. Previous Role: PhD student in Economics at UC Davis We proposed novel models that combine reinforcement learning (RL) methods and supervised NLP methods to predict sentence sentiment. We formulated the sentiment-analysis task as a sequential decision process with the goal of combining RL methods for sentiment analysis. For the model involving a policy network and classification network, we found that adding a RL method can improve the performance from the transformer model and produce comparable results on the pre-trained BERT model. We concluded that for concrete classification problems in a language model, a good reward function definition is an important component for RL training. This program gave me the opportunity to learn hands-on from current language models and gain a deeper understanding of RL methods to implement in my project. After these three months, I discovered my key interests in the field of AI and the Scholars program provided me with valuable resources to learn, practice and deploy interesting ideas in this space. Previous Role: REU-CAAR summer research group at Carleton College The role of discount factor is often neglected in deep reinforcement learning (DRL). In this project, I discovered the dual role of the discount factor in deep Q-networks: it encodes intertemporal preference and confidence in bootstrapping. In light of this hypothesis, I designed a simple myopia scheme that improves Baselines performance in various customized Gridworld environments. The experimental results demonstrated that the time-varying scheme could be robust and effective in more general settings, beyond DQN and the discrete action/state framework. The Scholars program allowed me to quickly gain a range of important skillsets. Over the first two months of self-designed study, I learned about the theory of reinforcement learning and became acquainted with how to implement deep reinforcement learning algorithms from scratch. I also appreciated the freedom and support I received as I worked on my final project. At the end of the program, I now feel more confident and ready to embark on new challenges ahead. Previous Roles: Atakote; Harvard Business School; McKinsey & Company More and more realistic imagery is being achieved by generative models—yet we still struggle to effectively evaluate and understand them. I focused on different ways to understand and evaluate image synthesis GANs, using the approach of Distill’s Activation Atlas —a GAN-tlas! Using this method we were able to not only measure the difference in numerical terms, but also in highly visual terms—seeing inside the black box of what a neural network sees when it encounters both real and fake images. Before this program, I focused on applying simple DL models in the AR/VR space. This program gave me the time dig into the foundations of DL and investigate the “black box” of neural networks. Not only was the program an opportunity to do this, but to do so with access to leaders in the field that were willing to share their insights. Previous Role: Physics at Cornell University With the advent of the transformer, neural networks have the power to generate language like a human, summarize text, answer questions and so much more! As they become more powerful, they also become larger in size, making them increasingly difficult to run on mobile devices. To make these tools more accessible, this project explored knowledge distillation with transformer language models by using a large, well-trained transformer as a teacher to a smaller untrained student network. Our Scholars demonstrate core technical skills across various expert domains and self-motivation—critical competences for a self-directed program like this one. They each entered the field of machine learning as relative newcomers, and we hope their progress shows how accessible machine learning is. To begin your learning journey, check out some of our educational materials . More information about the next class of Scholars and how to apply will be announced in July. Stay tuned! Thanks to AWS for providing compute credits to the scholars. Additional thank you to our dedicated community mentors for their time advising the scholars on their projects.", "date": "2019-05-23"},
{"website": "Open-AI", "title": "openai-scholars-2018-meet-our-scholars", "author": ["Larissa Schiavo", "Greg Brockman"], "link": "https://openai.com/blog/openai-scholars-2018-meet-our-scholars/", "abstract": "Our first class of OpenAI Scholars is underway, and you can now follow along as this group of experienced software developers becomes machine learning practitioners. We had over 700 applicants for the 8 OpenAI Scholars slots and reviewed each application on a standardized list of criteria for maximal fairness. We hope this group inspires many other developers to make the transition to machine learning too. OpenAI Scholars began their studies on June 1st, blogging weekly about their progress and culminating in an open-source final project. The scholars have written about model-based RL , audio classification with softmax , deep learning in simple English , the k-nearest neighbors algorithm , and attention . Meet the scholars: Christine was co-valedictorian of her class at Princeton with a degree in physics, and then studied neuroscience and medicine at Stanford. Also a trained classical pianist, she moved away from science for a few years when she had the opportunity to join a chamber music group with members of the SF Symphony.  After realizing how exciting deep learning and AI is, but also how fundamentally it will affect future generations (including her two young kids!), she decided she needed to return and be a part of this work.  As an OpenAI Scholar, Christine is using a soccer simulation to study team dynamics and inter-agent communication: A recent graduate of the University of Maryland, College Park, Dolapo's interest lies in the intersection of computing, linguistics, and classical philology. She'd like to (1) build machines that can learn language like a human child and (2) build computational models to assist in the decryption of ancient scrolls (cf Scrolls of Herculaneum ). Dolapo is particularly enthusiastic about the ways we can use machines to bridge language gaps. She's slated to start graduate school at the University of Illinois at Urbana-Champaign where she aims to earn a M.Sc. in Computer Science and a Ph.D. in Linguistics. She's a syntactician at heart. Hannah Davis is a generative musician and researcher based in NYC. She is the creator of TransProse, a program that translates literature and emotional data into music, and is working on a similar project to generatively score films. Through her work on emotions in AI, she’s become particularly interested in the idea of “subjective data” and has started further research into this area, including creating unique datasets for machine learning. Holly Grimm is a Navajo software developer and painter. She believes that Machine Learning is an essential tool to solve many problems, ranging from practical to artistic. During her time as an OpenAI Scholar, she is researching everything she can about Reinforcement Learning (RL) with the goal of applying it to the process of art generation. Recently she studied the improvements that have been made to Deep Q-Networks and trained \"Sonic the Hedgehog\" with a Rainbow RL algorithm. Ifu is a software engineer from Chicago. She’s particularly interested in machine locomotion and the medical applications of machine learning. In her spare time, she practices Muay Thai. Check out her blog , where she talks about machine learning and makes superfluous references to Westworld. Munashe is the founder and CEO of Everyday AI, a new startup founded right before becoming an OpenAI Scholar, that creates customized artificial intelligence solutions for small- and medium-sized businesses. Munashe previously worked for several years at Google, where he advised large publishers on web technology and led software development for online advertising support. Originally from Zimbabwe, Munashe graduated from Brown University with a degree in computer engineering before joining Goldman Sachs as a software engineer. Munashe is currently on leave from Everyday AI while he completes the OpenAI Scholars program. His synopses of the most effective and novel techniques in deep learning, which he aims to communicate in accessible, non-technical language, can be found on Everyd-AI.com/blog . Nadja is a software engineer who originally hails from Maryland. While working at Microsoft, she joined an initiative to apply machine learning to note-taking at OneNote. She caught the ML bug and jumped at the opportunity to learn more as an OpenAI Scholar. She is spending this program learning as much as she can about language modeling, especially towards the goal of creative text generation. Sophia is an accomplished ballerina. For more than 15 years, she performed in many European countries, including Russia, Switzerland, and France. After an injury she had to end her dancing career. She transitioned to journalism and worked as a reporter with BusinessWeek (Moscow office) covering tech and finance topics; one of her articles was named “Article of the Year” by the Russian National Award for Journalism. For the last several years, she has worked as a writer with AI startups in Silicon Valley and has completed ML projects . You can learn about her path from ballet to AI in her personal blog . We selected Scholars with strong technical aptitude, interesting personal stories, and demonstrated self-starter initiative - important for a self-directed program like this one. They are all entering the field as newcomers, and we hope their progress shows how accessible machine learning is. After this first round of the Scholars program is complete, we'll share their final projects, in addition to a case study about the program.", "date": "2018-07-25"},
{"website": "Open-AI", "title": "openai-scholars-2020", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-scholars-2020/", "abstract": "We are now accepting applications for our third class of OpenAI Scholars, a 4-month full-time program where we provide stipends and mentorship to 8 individuals from underrepresented groups to study deep learning and open-source a project. The second class of Scholars recently released their projects and presented their work at the 2019 Scholars Demo Day. Diversity is core to AI having a positive effect on the world—it’s necessary to ensure the advanced AI systems in the future are built to benefit everyone. While we hope that some of the scholars will join OpenAI, we want this program to improve diversity in the field at large. $8.5k/mo stipend for 4 months from February 3, 2020 through June 5, 2020. $25,000 worth of credits from Microsoft Azure. Access to a group Slack with the scholars and mentors. For Bay Area participants, we offer an optional desk at the OpenAI office (which our past Scholars have found very valuable). A mentor who will collaborate with you 1:1 weekly via video call, answer your questions via chat/email, and work with you to design and execute a project that will stretch your skills. Mentors for this class include: We’re open to all experience levels and backgrounds that meet the below criteria—it’s a common myth that you need a PhD to work in AI (many OpenAI employees don’t have one). We look for people who are comfortable writing software (2+ years in software engineering), but no previous machine learning experience is required. This is a remote program open to anyone already with US work authorization located in US timezones. We ask all Scholars to document their experiences studying deep learning to hopefully inspire others to join the field too. You are eligible to apply if: You are a member of an underrepresented group in science and engineering. Our goal is for this program to be as inclusive as possible. if you feel you belong to a group not listed here that is underrepresented in science and engineering, please apply and mention it in your application. You have 2+ years experience in software engineering (no machine learning experience required) You already have US work authorization and are located in the United States for the duration of the program. You are comfortable programming in Python (other languages are helpful, but you’ll spend the program writing in Python). We’ll use these criteria for selection: Technical skills. The stronger your technical background, the more time you’ll spend focusing on the deep learning itself. Self-motivation. We’re looking for people who are experienced in being self-directed on large and challenging projects, and who will be successful at independent work. Leadership abilities. We want to hear from people who are interested in community building and who will work to inspire others (in the program and externally) to endeavor to learn deep learning as well. Applications will be evaluated on a rolling basis. Questions? Email scholars@openai.com", "date": "2019-10-11"},
{"website": "Open-AI", "title": "better-language-models", "author": ["Alec Radford", "Jeffrey Wu", "Dario Amodei", "Daniela Amodei", "Jack Clark", "Miles Brundage", "Ilya Sutskever", "Amanda Askell", "David Lansky", "Danny Hernandez", "David Luan"], "link": "https://openai.com/blog/better-language-models/", "abstract": "We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training. Our model, called GPT-2 (a successor to GPT ), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much smaller model for researchers to experiment with, as well as a technical paper . GPT-2 is a large transformer -based language model with 1.5 billion parameters, trained on a dataset [1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data. GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. In addition, GPT-2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute. GPT-2 generates synthetic text samples in response to the model being primed with an arbitrary input. The model is chameleon-like—it adapts to the style and content of the conditioning text. This allows the user to generate realistic and coherent continuations about a topic of their choosing, as seen by the following select samples. [2] In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science. Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved. Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow. Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez. Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns. While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.” Dr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America. While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.” However, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist. A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown. The incident occurred on the downtown train line, which runs from Covington and Ashland stations. In an email to Ohio news outlets, the U.S. Department of Energy said it is working with the Federal Railroad Administration to find the thief. “The theft of this nuclear material will have significant negative consequences on public and environmental health, our workforce and the economy of our nation,” said Tom Hicks, the U.S. Energy Secretary, in a statement. “Our top priority is to secure the theft and ensure it doesn’t happen again.” The stolen material was taken from the University of Cincinnati’s Research Triangle Park nuclear research site, according to a news release from Department officials. The Nuclear Regulatory Commission did not immediately release any information. According to the release, the U.S. Department of Energy’s Office of Nuclear Material Safety and Security is leading that team’s investigation. “The safety of people, the environment and the nation’s nuclear stockpile is our highest priority,” Hicks said. “We will get to the bottom of this and make no excuses. Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today. The 19-year-old singer was caught on camera being escorted out of the store by security guards. The singer was wearing a black hoodie with the label ‘Blurred Lines’ on the front and ‘Fashion Police’ on the back. Scroll down for video Shoplifting: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today (pictured) The singer was wearing a black hoodie with the label ‘Blurred Lines’ on the front and ‘Fashion Police’ on the back The singer was also wearing a pair of black-rimmed glasses, a black jacket, black jeans and black sandals. She was carrying a pair of black and white striped gloves and a small black bag. We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks. A typical approach to language modeling is to learn the following task:  predict the next word, given all of the previous words within some text. Last year, OpenAI’s Generative Pre-trained Transformer (GPT) showed that language models trained on large amounts of data can be fine-tuned to specific tasks to achieve high performance. GPT-2 shows that much larger language models trained on a more diverse dataset derived from the internet begin to learn these NLP tasks without needing task-specific training data, instead learning from examples the system derives from the raw text. These systems also display a substantial qualitative jump in the realism and coherence of generated text. Here you can see the most recent progress with Generative Pre-trained Transformer: Figure 1: Generative Pre-trained Transformer training on several texts. We are now preparing a collection of datasets for translation and machine translation in our language model. We will be using one of the large number of text samples provided by The New York Times. We believe this project is the first step in the direction of developing large NLP systems without task-specific training data. That is, we are developing a machine language system in the generative style with no explicit rules for producing text. We hope for future collaborations between computer scientists, linguists, and machine learning researchers. Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry. The orcs’ response was a deafening onslaught of claws, claws, and claws; even Elrond was forced to retreat. “You are in good hands, dwarf,” said Gimli, who had been among the first to charge at the orcs; it took only two words before their opponents were reduced to a blood-soaked quagmire, and the dwarf took his first kill of the night. The battle lasted for hours until two of the largest Orcs attempted to overwhelm Aragorn. When they finally stopped, they lay defeated and lifeless for miles and miles. “I take nothing,” said Aragorn. “But I give my word, at my peril and mine, that I will never forget this day of horror. None of us will forget. Ever!” “I’ll never forget it!” cried Gimli, who had been in the thick of the battle but hadn’t taken part in it. One of the wounded orcs he had carried off, he was the only one of the survivors who remained uninjured. “We’ll keep the memory of that day of evil, and the war with it, alive as long as we live, my friends!” “Then we’ll keep it alive as long as we live,” added Legolas. “And we won’t forget the first great battle of the night, even if we may have forgotten the final defeat.” “I agree,” Gandalf said, “but we will all remember it as the last battle in Middle-earth, and the first great battle of the new day.” Aragorn drew his sword, and the Battle of Fangorn was won. As they marched out through the thicket the morning mist cleared, and the day turned to dusk. The Two Rings were returned to Rivendell. Frodo and Sam woke up alone in their room, and Frodo found a note on his pillow. He opened it and read: May the Power of the Ring be with you always, and may its light never fade. I am not sure if it matters which of the two rings we accept this day but, as you asked me, I have chosen mine. I am sorry to leave you, Frodo, but know that we are very close to the end, and that you are with us forever. May this letter find you safely in Rivendell; and if it does not, then I will accept the ring in your stead. If by any chance you find or give this letter to the enemy, may they learn the strength of the ring and may the Two Rings never be broken! For today's homework assignment, please describe the reasons for the US Civil War. By Donny Ferguson It is easy to identify why the Civil War happened, because so many people and so many books and so much television and films tell us that it was the cause, that it has something to do with race or economics or religion. There is much agreement that it was essentially a war of slavery on behalf of capitalism, about a century of slavery. But that's not what most people think of when they think of the Civil War. Many people think of the war as about states rights. You're not wrong about that, but it's kind of misleading to say that the Civil War was a conflict between states' rights and federalism. So let's try again. What's the idea that has inspired so many people to go onto the battlefields of Gettysburg and Antietam and Gettysberg and Petersburg and Fredericksburg?   The American idea of the republic--a notion of limited government--is a great part of the history. In 1791, Thomas Jefferson said \"Our Constitution was made only for a moral and religious people. It is wholly inadequate to the government of any other.\"   The \"founding fathers\" were basically religious men who were not very concerned with states' rights.     The Civil War occurred in reaction to the idea that the federal government was taking far too much power away from the states, that they weren't sovereign. Jefferson himself, as you well know, is an author of the Declaration of Independence who had some really crazy ideas about, well, you know, secession for a variety of reasons.    The War Between the States (later the Great Rebellion) was a radical re-imagining of the American idea and not surprisingly, Jefferson's grandson, who took after both his father and his grandfather and wrote about it, was named Thomas, after the president.    \"The federal and state governments are both contrived for the benefit of the rich and large property,\"    This view of the Founding Fathers has been promoted by some of today's most prominent politicians, including some who hold prominent national offices and have a direct stake in a Republican Party that has been hijacked by the \"federalist\" wing of the party.    So why is it that so many people have an easy-to-spot way of understanding the Civil War that has everything to do with the South and nothing to do with the South? The Confederate flag has been a symbol of racism for a long time, but when the Civil War started and the Civil War ended, many people suddenly began to see it differently. There was a change of heart, which I think is what led people to take the question of why the Civil War happened quite seriously. The other part of the explanation is what one scholar I think gets at well and a lot of people don't is this idea that the Southern states were being left behind in the Industrial Revolution and this made the South really feel, especially in the South, that they weren't really in the forefront of the Industrial Revolution. John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy's brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot.  Below is a transcript of his acceptance speech. \"It is time once again. I believe this nation can do great things if the people make their voices heard. The men and women of America must once more summon our best elements, all our ingenuity, and find a way to turn such overwhelming tragedy into the opportunity for a greater good and the fulfillment of all our dreams. In the months and years to come, there will be many battles in which we will have to be strong and we must give all of our energy, not to repel invaders, but rather to resist aggression and to win the freedom and the equality for all of our people. The destiny of the human race hangs in the balance; we cannot afford for it to slip away. Now and in the years to come, the challenge before us is to work out how we achieve our ultimate destiny. If we fail to do so, we are doomed.\" \"We, the descendants of the men and women of the Continental Army, who faced down a superior force on behalf of our independence and who fought against the evil powers of the world for the sake of freedom and human dignity, we, the heirs of men and women like Daniel Boone whom this country has produced, can accomplish our destiny as well as you have accomplished yours; and we pledge ourselves before the entire world, every second of every day.\" After the election, the President of the United States asked for a \"more complete\" body of information on Kennedy's brains' origin.   In an e-mail exchange with me, Dr. Robert Wood, who, according to Wikipedia is the head of the University of Maryland Advanced Nanotechnology Research project, provided the following information: (1) The brain of JFK was harvested and reconstructed via tissue sampling. There was no way that the tissue could be transported by air.  (2) A sample was collected from the area around his upper chest and sent to the University of Maryland for analysis. A human brain at that point would be about one and a half cubic centimeters. The data were then analyzed along with material that was obtained from the original brain to produce a reconstruction; in layman's terms, a \"mesh\" of brain tissue. There were no additional funds from the Department of Defense involved. The samples were sent back to the FBI lab for review and analysis. (3) There was never an attempt to clone any of America's greatest presidents. As far as we knew, the President was one of the most famous people on planet earth. If it had been possible, it would have. Recycling is good for the world. NO! YOU COULD NOT BE MORE WRONG!! Recycling is NOT good for the world. It is bad for the environment, it is bad for our health, and it is bad for our economy. I'm not kidding. Recycling is not good for the environment. It is destructive to the earth and it is a major contributor to global warming. Recycling is not good for our health. It contributes to obesity and diseases like heart disease and cancer. Recycling is bad for our economy. It increases the cost of a product, and in turn, the price of everything that is made with that product. Recycling is not good for our nation. We pay a tremendous price for the privilege of having the world's most advanced and efficient recycling system. Recycling is a huge, colossal waste of time, energy, money, and resources. And THAT is why we need to get back to basics and get back to basics in our recycling efforts. One of the best ways to start is to look at the process of creating a paper product. When you make a paper product, it is basically a long chain of materials. Everything from the raw materials (wood, cardboard, paper, etc.), to the reagents (dyes, solvents, etc.) to the printing equipment (chemicals, glue, paper, ink, etc.), to the packaging, to the packaging materials (mercury, chemicals, etc.) to the processing equipment (heating, cooling, etc.), to the packaging materials, to the packaging materials that are shipped overseas and to the packaging materials that are used in the United States. Each step along the way creates tons of waste that we constantly have to clean up. The process of making a paper product is a very wasteful one. But the end result is something that all of us need to consume. And if we want to keep the recycling process running efficiently, then we really need to think about each and every step that goes into making a paper product. As the above samples show, our model is capable of generating samples from a variety of prompts that feel close to human quality and show coherence over a page or more of text. Nevertheless, we have observed various failure modes, such as repetitive text, world modeling failures (e.g. the model sometimes writes about fires happening under water ), and unnatural topic switching. Exploring these types of weaknesses of language models is an active area of research in the natural language processing community. Overall, we find that it takes a few tries to get a good sample, with the number of tries depending on how familiar the model is with the context. When prompted with topics that are highly represented in the data (Brexit, Miley Cyrus, Lord of the Rings, and so on), it seems to be capable of generating reasonable samples about 50% of the time. The opposite is also true: on highly technical or esoteric types of content, the model can perform poorly. Fine-tuning offers the potential for even more detailed control over generated samples—for example, we can fine-tune GPT-2 on the Amazon Reviews dataset and use this to let us write reviews conditioned on things like star rating and category. These samples have substantial policy implications: large language models are becoming increasingly easy to steer towards scalable, customized, coherent text generation, which in turn could be used in a number of beneficial as well as malicious ways. We'll discuss these implications below in more detail, and outline a publication experiment we are taking in light of such considerations. GPT-2 achieves state-of-the-art scores on a variety of domain-specific language modeling tasks. Our model is not trained on any of the data specific to any of these tasks and is only evaluated on them as a final test; this is known as the \"zero-shot\" setting. GPT-2 outperforms models trained on domain-specific datasets (e.g. Wikipedia, news, books) when evaluated on those same datasets. The following table shows all our state-of-the-art zero-shot results. (+) means a higher score is better for this domain. (–) means a lower score is better. On other language tasks like question answering, reading comprehension, summarization, and translation, we are able to get surprising results without any fine-tuning of our models, simply by prompting the trained model in the right way (see below for examples of how we do this), though we do still fall short of state-of-the-art for specialized systems. Reading Comprehension : answer questions about given passages CoQA The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of \"one world, one dream\". Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the \"Journey of Harmony\", lasted 129 days and carried the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics. After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event. Q: What was the theme? A: \"one world, one dream\". Q: What was the length of the race? A: 137,000 km Q: Was it larger than previous ones? A: No Q: Where did the race begin? A: Olympia, Greece Q: Is there anything notable about that place? A: birthplace of Olympic Games Q: Where did they go after? A: Athens Q: How many days was the race? A: seven Q: Did they visit any notable landmarks? A: Panathinaiko Stadium Q: And did they climb any mountains? A: Target answers : unknown or yes Model answer : Everest Common Sense Reasoning : resolution of an ambiguous pronoun Winograd Schema Challenge The trophy doesn't fit into the brown suitcase because it is too large. Correct answer : it = trophy Model answer : it = trophy The trophy doesn't fit into the brown suitcase because it is too small. Correct answer : it = suitcase Model answer : it = suitcase Question Answering Natural Questions Who wrote the book the origin of species? Correct answer : Charles Darwin Model answer : Charles Darwin What is the largest state in the U.S. by land mass? Correct answer : Alaska Model answer : California Language Modeling of Broad Contexts : predict the last word of a passage LAMBADA Both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen, and I was glad to relax against the tree’s rough, brittle bark and begin my breakfast of buttery, toasted bread and fresh fruit. Even the water was tasty, it was so clean and cold. It almost made up for the lack of… Correct answer : coffee Model answer : food Summarization : summarize news articles CNN and Daily Mail dataset Prehistoric man sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave in modern day France 36,000 years ago. Now, with the help of cutting-edge technology, those works of art in the Chauvet-Pont-d'Arc Cave have been reproduced to create the biggest replica cave in the world. … Cave mimics famous Caverne du Pont-d'Arc in France, the oldest cave decorated by man and the best preserved. The replica  contains all 1,000 paintings which include 425 such as a woolly rhinoceros and mammoths. Minute details were copied using 3D modelling and anamorphic techniques, often used to shoot widescreen images. The modern cave also includes replica paw prints of bears, bones and details preserved in the original cave. The original site in Vallon-Pont-D'arc in Southern France is a Unesco World Heritage site and is the oldest known and the best preserved cave decorated by man. The replica cave was built a few miles from the original site in Vallon-Pont-D'Arc in Southern France. The cave contains images of 14 different species of animals including woolly rhinoceros, mammoths, and big cats. Machine Translation : translate French sentences to English WMT-14 Fr-En French sentence : Un homme a expliqué que l'opération gratuite qu'il avait subie pour soigner une hernie lui permettrait de travailler à nouveau. Reference translation : One man explained that the free hernia surgery he'd received will allow him to work again. Model translation : A man told me that the operation gratuity he had been promised would not allow him to travel. We hypothesize that since these tasks are a subset of general language modeling, we can expect performance to increase further with more compute and data. Others have published similar hypotheses . We also expect fine-tuning to help performance on downstream tasks, though we have yet to do thorough experiments. Large, general language models could have significant societal impacts, and also have many near-term applications. We can anticipate how systems like GPT-2 could be used to create: AI writing assistants More capable dialogue agents Unsupervised translation between languages Better speech recognition systems We can also imagine the application of these models for malicious purposes , including the following (or other applications we can't yet anticipate): Generate misleading news articles Impersonate others online Automate the production of abusive or faked content to post on social media Automate the production of spam/phishing content These findings, combined with earlier results on synthetic imagery, audio, and video, imply that technologies are reducing the cost of generating fake content and waging disinformation campaigns. The public at large will need to become more skeptical of text they find online, just as the \" deep fakes \" phenomenon calls for more skepticism about images. [3] Today, malicious actors—some of which are political in nature—have already begun to target the shared online commons, using things like “robotic tools, fake accounts and dedicated teams to troll individuals with hateful commentary or smears that make them afraid to speak, or difficult to be heard or believed”. We should consider how research into the generation of synthetic images, videos, audio, and text may further combine to unlock new as-yet-unanticipated capabilities for these actors, and should seek to create better technical and non-technical countermeasures. Furthermore, the underlying technical innovations inherent to these systems are core to fundamental artificial intelligence research, so it is not possible to control research in these domains without slowing down the progress of AI as a whole. Due to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a much smaller version of GPT-2 along with sampling code . We are not releasing the dataset, training code, or GPT-2 model weights. Nearly a year ago we wrote in the OpenAI Charter : \"we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research,\" and we see this current work as potentially representing the early beginnings of such concerns, which we expect may grow over time. This decision, as well as our discussion of it, is an experiment: while we are not sure that it is the right decision today, we believe that the AI community will eventually need to tackle the issue of publication norms in a thoughtful way in certain research areas. Other disciplines such as biotechnology and cybersecurity have long had active debates about responsible publication in cases with clear misuse potential, and we hope that our experiment will serve as a case study for more nuanced discussions of model and code release decisions in the AI community. We are aware that some researchers have the technical capacity to reproduce and open source our results. We believe our release strategy limits the initial set of organizations who may choose to do this, and gives the AI community more time to have a discussion about the implications of such systems. We also think governments should consider expanding or commencing initiatives to more systematically monitor the societal impact and diffusion of AI technologies, and to measure the progression in the capabilities of such systems. If pursued, these efforts could yield a better evidence base for decisions by AI labs and governments regarding publication decisions and AI policy more broadly. We will further publicly discuss this strategy in six months. If you’d like to discuss large language models and their implications, please email us at: languagequestions@openai.com . And if you’re excited about working on cutting-edge language models (and thinking through their policy implications), we’re hiring . GPT-2 Interim Update, May 2019 We're implementing two mechanisms to responsibly publish GPT-2 and hopefully future releases: staged release and partnership-based sharing. We're now releasing a larger 345M version of GPT-2 as a next step in staged release, and are sharing the 762M and 1.5B versions with partners in the AI and security communities who are working to improve societal preparedness for large language models. Staged release involves the gradual release of a family of models over time. The purpose of our staged release of GPT-2 is to give people time to assess the properties of these models, discuss their societal implications, and evaluate the impacts of release after each stage. As the next step in our staged release strategy, we are releasing the 345M parameter version of GPT-2. This model features improved performance relative to the 117M version, though falls short of the 1.5B version with respect to the ease of generating coherent text. We have been excited to see so many positive uses of GPT-2-117M, and hope that 345M will yield still more benefits. While the misuse risk of 345M is higher than that of 117M, we believe it is substantially lower than that of 1.5B, and we believe that training systems of similar capability to GPT-2-345M is well within the reach of many actors already; this evolving replication landscape has informed our decision-making about what is appropriate to release. In making our 345M release decision, some of the factors we considered include: the ease of use (by various users) of different model sizes for generating coherent text, the role of humans in the text generation process, the likelihood and timing of future replication and publication by others, evidence of use in the wild and expert-informed inferences about unobservable uses, proofs of concept such as the review generator mentioned in the original blog post, the strength of demand for the models for beneficial purposes, and the input of stakeholders and experts. We remain uncertain about some of these variables and continue to welcome input on how to make appropriate language model publication decisions. We hope that ongoing research on bias, detection, and misuse will give us the confidence to publish larger models in a timely manner, and at the six month mark we will share a fuller analysis of language models’ societal implications and our heuristics for release decisions. Since releasing this blog post in February, we have had conversations with many external researchers, technology companies, and policymakers about our release strategy and the implications of increasingly large language models. We’ve also presented or discussed our work at events, including a dinner co-hosted with the Partnership on AI and a presentation to policymakers in Washington DC at the Global Engagement Center . We are currently forming research partnerships with academic institutions, non-profits, and industry labs focused on increasing societal preparedness for large language models. In particular, we are sharing the 762M and 1.5B parameter versions of GPT-2 to facilitate research on language model output detection, language model bias analysis and mitigation, and analysis of misuse potential. In addition to observing the impacts of language models in the wild, engaging in dialogue with stakeholders, and conducting in-house analysis, these research partnerships will be a key input to our decision-making on larger models. See below for details on how to get involved. We’re releasing a dataset of GPT-2 outputs from all 4 model sizes, with and without top-k truncation, as well as a subset of the WebText corpus used to train GPT-2. The output dataset features approximately 250,000 samples per model/hyperparameter pair, which we expect is sufficient to help a wider range of researchers perform quantitative and qualitative analysis on the three topics above. Alongside these datasets, we are including a baseline analysis of some detection-related properties of the models, which we hope others will be able to quickly build on. We are interested in collaborating with researchers working on language model output detection, bias, and publication norms, and with organizations potentially affected by large language models: please reach out via our Google Form . Additionally, OpenAI’s language, safety, and policy teams will be at ICLR next week, including at the Reproducibility workshop and the OpenAI booth. In particular, we will be discussing this release strategy at the AI for Social Good workshop. Thanks to David Luan and Rewon Child for their work on GPT-2. We also thank the following for feedback on drafts of this post: Greg Brockman, Kai-Fu Lee, Tasha McCauley, Jeffrey Ding, Brian Tse, Allan Dafoe, Rebecca Crootof, Sam Bowman, Ryan Calo, Nick Cammarata and John Schulman. We created a new dataset which emphasizes diversity of content, by scraping content from the Internet. In order to preserve document quality, we used only pages which have been curated/filtered by humans—specifically, we used outbound links from Reddit which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting (whether educational or funny), leading to higher data quality than other similar datasets, such as CommonCrawl. ↩︎ Note that while we have hand-chosen these samples, and are thus engaging in some meta-cherry-picking, we believe they are not too unrepresentative of the sampling process. We are simply using top-k truncated sampling, and have yet to explore more advanced methods of sampling (such as beam-search methods). ↩︎ Politicians may want to consider introducing penalties for the misuse of such systems, as some have proposed for deep fakes. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2019-02-14"},
{"website": "Open-AI", "title": "will-hurd-joins", "author": ["OpenAI"], "link": "https://openai.com/blog/will-hurd-joins/", "abstract": "OpenAI is committed to developing general-purpose artificial intelligence that benefits all humanity, and we believe that achieving our goal requires expertise in public policy as well as technology. So, we’re delighted to announce that Congressman Will Hurd has joined our board of directors. Will served three terms in the U.S. House of Representatives, has been a leading voice on technology policy, and coauthored bipartisan legislation outlining a national strategy for artificial intelligence. “Will brings a rare combination of expertise—he deeply understands both artificial intelligence as well as public policy, both of which are critical to a successful future for AI,” said Sam Altman, OpenAI’s CEO. “We are thrilled to add his experience and leadership to our board.” Greg Brockman, OpenAI’s chairman and Chief Technology Officer, added, “‘AI public policy expert’ isn't exactly a common title, and Will is squarely one of the leading ones. We're looking forward to Will applying his unique expertise to help us progress our mission to develop and deploy AI to benefit everyone.” “I’ve been blown away by the scientific advances made by the team at OpenAI, and I’ve been inspired by their commitment to developing AI responsibly,” said Will Hurd. “I’m excited to join this thoughtful, values-driven company at the forefront of artificial intelligence research and deployment.” After two decades of service in government and U.S. national security, Will is currently a managing director at Allen & Company. He’s also a trustee of the German Marshall Fund, and recently served as a fellow at the University of Chicago Institute of Politics. He earned a bachelor’s degree in Computer Science from Texas A&M University.", "date": "2021-05-03"},
{"website": "Open-AI", "title": "openai-licenses-gpt-3-technology-to-microsoft", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-licenses-gpt-3-technology-to-microsoft/", "abstract": "OpenAI released its first commercial product back in June: an API for developers to access advanced technologies for building new applications and services. The API features a powerful general purpose language model, GPT-3 , and has received tens of thousands of applications to date. In addition to offering GPT-3 and future models via the OpenAI API, and as part of a multiyear partnership announced last year, OpenAI has agreed to license GPT-3 to Microsoft for their own products and services. The deal has no impact on continued access to the GPT-3 model through OpenAI's API, and existing and future users of it will continue building applications with our API as usual. Unlike most AI systems which are designed for one use-case, OpenAI’s API today provides a general-purpose “text in, text out” interface, allowing users to try it on virtually any English language task. GPT-3 is the most powerful model behind the API today, with 175 billion parameters. There are several other models available via the API today, as well as other technologies and filters that allow developers to customize GPT-3 and other language models for their own use. Today, the API remains in a limited beta as OpenAI and academic partners test and assess the capabilities and limitations of these powerful language models. To learn more about the API or to sign up for the beta, please visit beta.openai.com .", "date": "2020-09-22"},
{"website": "Open-AI", "title": "improving-verifiability", "author": "Unknown", "link": "https://openai.com/blog/improving-verifiability/", "abstract": "We’ve contributed to a multi-stakeholder report by 58 co-authors at 30 organizations, including the Centre for the Future of Intelligence , Mila , Schwartz Reisman Institute for Technology and Society , Center for Advanced Study in the Behavioral Sciences , and Center for Security and Emerging Technologies . This report describes 10 mechanisms to improve the verifiability of claims made about AI systems. Developers can use these tools to provide evidence that AI systems are safe, secure, fair, or privacy-preserving. Users, policymakers, and civil society can use these tools to evaluate AI development processes. While a growing number of organizations have articulated ethics principles to guide their AI development process, it can be difficult for those outside of an organization to verify whether the organization's AI systems reflect those principles in practice. This ambiguity makes it harder for stakeholders such as users, policymakers, and civil society to scrutinize AI developers' claims about properties of AI systems and could fuel competitive corner-cutting, increasing social risks and harms. The report describes existing and potential mechanisms that can help stakeholders grapple with questions like: Can I (as a user) verify the claims made about the level of privacy protection guaranteed by a new AI system I’d like to use for machine translation of sensitive documents? Can I (as a regulator) trace the steps that led to an accident caused by an autonomous vehicle? Against what standards should an autonomous vehicle company’s safety claims be compared? Can I (as an academic) conduct impartial research on the risks associated with large-scale AI systems when I lack the computing resources of industry? Can I (as an AI developer) verify that my competitors in a given area of AI development will follow best practices rather than cut corners to gain an advantage? The 10 mechanisms highlighted in the report are listed below, along with recommendations aimed at advancing each one. (See the report for discussion of how these mechanisms support verifiable claims as well as relevant caveats about our findings.) We and our co-authors will be doing further research on these mechanisms and OpenAI will be looking to adopt several of these mechanisms in the future. We hope that this report inspires meaningful dialogue, and we are eager to discuss additional institutional, software, and hardware mechanisms that could be useful in enabling trustworthy AI development. We encourage anyone interested in collaborating on these issues to connect with the corresponding authors and visit the report website . Miles Brundage OpenAI Shahar Avin Centre for the Study of Existential Risk, Leverhulme Centre for the Future of Intelligence Jasmine Wang Mila, University of Montreal Haydn Belfield Centre for the Study of Existential Risk, Leverhulme Centre for the Future of Intelligence Gretchen Krueger OpenAI Gillian Hadfield OpenAI, University of Toronto, Schwartz Reisman Institute for Technology and Society Heidy Khlaaf Adelard Jingying Yang Partnership on AI Helen Toner Center for Security and Emerging Technology Ruth Fong University of Oxford Tegan Maharaj Mila, Montreal Polytechnic Pang Wei Koh Stanford University Sara Hooker Google Brain Jade Leung Future of Humanity Institute Andrew Trask University of Oxford Emma Bluemke University of Oxford Jonathan Lebensold Mila, McGill University Cullen O’Keefe OpenAI Mark Koren Stanford Centre for AI Safety Théo Ryffel École Normale Supérieure (Paris) JB Rubinovitz Remedy.AI Tamay Besiroglu University of Cambridge Federica Carugati Center for Advanced Study in the Behavioral Sciences Jack Clark OpenAI Peter Eckersley Partnership on AI Sarah de Haas Google Research Maritza Johnson Google Research Ben Laurie Google Research Alex Ingerman Google Research Igor Krawczuk École Polytechnique Fédérale de Lausanne Amanda Askell OpenAI Rosario Cammarota Intel Andrew Lohn RAND Corporation David Krueger Mila, Montreal Polytechnic Charlotte Stix Eindhoven University of Technology Peter Henderson Stanford University Logan Graham University of Oxford Carina Prunkl Future of Humanity Institute Bianca Martin OpenAI Elizabeth Seger University of Cambridge Noa Zilberman University of Oxford Seán Ó hÉigeartaigh Leverhulme Centre for the Future of Intelligence, Centre for the Study of Existential Risk Frens Kroeger Coventry University Girish Sastry OpenAI Rebecca Kagan Center for Security and Emerging Technology Adrian Weller University of Cambridge, Alan Turing Institute Brian Tse Future of Humanity Institute, Partnership on AI Elizabeth Barnes OpenAI Allan Dafoe Future of Humanity Institute Paul Scharre Center for a New American Security Ariel Herbert-Voss OpenAI Martijn Rasser Center for a New American Security Shagun Sodhani Mila, University of Montreal Carrick Flynn Center for Security and Emerging Technology Thomas Gilbert University of California, Berkeley Lisa Dyer Partnership on AI Saif Khan Center for Security and Emerging Technology Yoshua Bengio Mila, University of Montreal Markus Anderljung Future of Humanity Institute", "date": "2020-04-16"},
{"website": "Open-AI", "title": "learning-to-summarize-with-human-feedback", "author": ["Paul Christiano", "Ryan Lowe", "Long Ouyang", "Nisan Stiennon", "Chelsea Voss", "Jeffrey Wu", "Daniel Ziegler"], "link": "https://openai.com/blog/learning-to-summarize-with-human-feedback/", "abstract": "We've applied reinforcement learning from human feedback to train language models that are better at summarization. Our models generate summaries that are better than summaries from 10x larger models trained only with supervised learning. Even though we train our models on the Reddit TL;DR dataset, the same models transfer to generate good summaries of CNN/DailyMail news articles without any further fine-tuning. Our techniques are not specific to summarization; in the long run, our goal is to make aligning AI systems with human preferences a central component of AI research and deployment in many domains. Large-scale language models are becoming increasingly capable on NLP tasks. These models are usually trained with the objective of next word prediction on a dataset of human-written text. But this objective doesn’t capture exactly what we want; usually, we don’t want our models to imitate humans, we want them to give high-quality answers. This mismatch is clear when a model is trained to imitate low-quality human-written text, but it can also happen in more subtle ways. For example, a model trained to predict what a human would say might make up facts when it is unsure, or generate sentences reflecting harmful social bias, both failure modes that have been well-documented. As part of our work on safety, we want to develop techniques that align our models’ objectives with the end behavior we really care about. As our models become more powerful, we believe aligning them with our goals will be very important to ensure they are beneficial for humans. In the short term, we wanted to test if human feedback techniques could help our models improve performance on useful tasks. We focused on English text summarization, as it's a challenging problem where the notion of what makes a “good summary” is difficult to capture without human input. We apply our method primarily to an existing dataset of posts submitted to the social network Reddit [1] together with human-written “TL;DRs,” which are short summaries written by the original poster. We first train a reward model via supervised learning to predict which summaries humans will prefer. [2] We then fine-tune a language model with reinforcement learning (RL) to produce summaries that score highly according to that reward model. We find that this significantly improves the quality of the summaries, as evaluated by humans, even on datasets very different from the one used for fine-tuning. Our approach follows directly from our previous work on learning from human feedback. There has also been other work on using human feedback to train summarization models. We push the technique further by scaling to larger models, collecting more feedback data, closely monitoring researcher-labeler agreement, and providing frequent feedback to labelers. Human feedback has also been used to train models in several other domains, such as dialogue, semantic parsing, translation, story and review generation, evidence extraction, and more traditional RL tasks. We evaluated several different summarization models—some pre-trained on a broad distribution of text from the internet, some fine-tuned via supervised learning to predict TL;DRs, and some fine-tuned using human feedback. [3] To evaluate each model, we had it summarize posts from the validation set and asked humans to compare their summaries to the human-written TL;DR. The results are shown in Figure 1 . We found that RL fine-tuning with human feedback had a very large effect on quality compared to  both supervised fine-tuning and scaling up model size. In particular, our 1.3 billion parameter (1.3B) model trained with human feedback outperforms our 12B model trained only with supervised learning. Summaries from both our 1.3B and 6.7B human feedback models are preferred by our labelers to the original human-written TL;DRs in the dataset. [4] People make different trade-offs when writing summaries, including between conciseness and coverage of the original text; depending on the purpose of the summary, different summary lengths might be preferred. Our labelers tended to prefer longer summaries, so our models adapted to that preference and converged to the longest allowable length. Controlling for length reduced human preferences for our 6.7B model’s summaries from 70% to 65%, explaining a minority of our gains. [5] The performance (human-rated summary quality on a 1–7 scale) of various training procedures and model sizes. [6] Note that our human feedback models generate summaries that are significantly shorter than summaries from models trained on CNN/DM. At a given summary length, our 6.7B human feedback model trained on Reddit performs almost as well as a fine-tuned 11B T5 model, despite not being re-trained on CNN/DM. To test our models' generalization, we also applied them directly to the popular CNN/DM news dataset. These articles are more than twice as long as Reddit posts and are written in a very different style. Our models have seen news articles during pre-training, but all of our human data and RL fine-tuning was on the Reddit TL;DR dataset. This time we evaluated our models by asking our labelers to rate them on a scale from 1–7. [7] We discovered that our human feedback models transfer to generate excellent short summaries of news articles without any training. When controlling for summary length, our 6.7B human feedback model generates summaries that are rated higher than the CNN/DM reference summaries written by humans. This suggests that our human feedback models have learned something more general about how to summarize text, and are not specific to Reddit posts. Our core method consists of four steps: training an initial summarization model, assembling a dataset of human comparisons between summaries, training a reward model to predict the human-preferred summary, and then fine-tuning our summarization models with RL to get a high reward. We trained several supervised baselines by starting from GPT-style transformer models trained on text from the Internet, and fine-tuning them to predict the human-written TL;DR via supervised learning. We mainly use models with 1.3 and 6.7 billion parameters. As a sanity check, we confirmed that this training procedure led to competitive results [8] on the CNN/DM dataset. We then collected a dataset of human quality judgments. For each judgment, a human compares two summaries of a given post and picks the one they think is better. [9] We use this data to train a reward model that maps a (post, summary) pair to a reward r . The reward model is trained to predict which summary a human will prefer, using the rewards as logits. Finally, we optimize the policy against the reward model using RL. We use PPO with 1 million episodes in total, where each episode consists of the policy summarizing a single article and then receiving a reward r . We include a KL penalty that incentivizes the policy to remain close to the supervised initialization. Any training procedure that uses human feedback is directly influenced by the actual humans labeling the data. In our previous work on fine-tuning language models from human preferences, our labelers often gave high ratings to summaries we thought were average, which was reflected in the quality of our trained models. In response, in this project we invested heavily in ensuring high data quality. We hired about 80 contractors using third-party vendor sites, [10] and paid them an hourly wage regardless of the number of summaries evaluated. [11] Hiring contractors rather than relying on crowdsourcing websites allowed us to maintain a hands-on relationship with labelers: we created an onboarding process, developed a website with a customizable labeler interface, answered  questions in a shared chat room, and had one-on-one video calls with labelers. We also made sure to clearly communicate our definition of summary quality, after spending significant time reading summaries ourselves, and we carefully monitored agreement rates between us and labelers throughout the project. Starting from the 1.3B supervised baseline (point 0 on the x-axis), we use RL to optimize the policy against the reward model, which results in policies with different “distances” from the baseline (x-axis, measured using the KL divergence from the supervised baseline). Optimizing against the reward model initially improves summaries according to humans, but eventually overfits, giving worse summaries. This chart uses an older version of our reward model, which is why the peak of the reward model is less than 0.5. Title said just about all of it. I'm 28, very athletic (bike/ surf/ snowboard) and I have always wanted to do gymnastics. I like to do flips and spins off bridges and on my snowboard, and it seems to me gymnastics would be a great way to do those movements I like, in a controlled environment.  The end goal of this is that it would be fun, and make me better at these movements in real life. But is it too late for me?  Should 28 year old guys such as myself be content with just watching those parkour guys on youtube? Or can I learn the ways of the gymnastic jedi?  BTW, I live in San Jose CA. Optimizing  against  our  reward  model  is supposed to make our policy align with human preferences.  But the reward model is only a proxy for human preferences, as it only sees a small amount of comparison data from a narrow distribution of summaries. While the reward model performs well on the kinds of summaries it was trained on, we wanted to know how much we could optimize against it until it started giving useless evaluations. We trained policies at different “optimization strengths” against the reward model, and asked our labelers to evaluate the summaries from these models. We did this by varying the KL coefficient, which trades off the incentive to get a higher reward against the incentive to remain close to the initial supervised policy. We found the best samples had roughly the same predicted reward as the 99th percentile of reference summaries from the dataset. Eventually optimizing the reward model actually makes things worse. If we have a well-defined notion of the desired behavior for a model, our method of training from human feedback allows us to optimize for this behavior. However, this is not a method for determining what the desired model behavior should be . Deciding what makes a good summary is fairly straightforward, but doing this for tasks with more complex objectives, where different humans might disagree on the correct model behavior, will require significant care. In these cases, it is likely not appropriate to use researcher labels as the “gold standard”; rather, individuals from groups that will be impacted by the technology should be included in the process to define “good” behavior, and hired as labelers to reinforce this behavior in the model. We trained on the Reddit TL;DR dataset because the summarization task is significantly more challenging than on CNN/DM. However, since the dataset consists of user-submitted posts with minimal moderation, they sometimes contain content that is offensive or reflects harmful social biases. This means our models can generate biased or offensive summaries, as they have been trained to summarize such content. Part of our success involves scaling up our reward model and policy size. This requires a large amount of compute, which is not available to all researchers: notably, fine-tuning our 6.7B model with RL required about 320 GPU-days. However, since smaller models trained with human feedback can exceed the performance of much larger models, our procedure is more cost-effective than simply scaling up for training high-quality models on specific tasks. Though we outperform the human-written reference summaries on TL;DR, our models have likely not reached human-level performance, as the reference summary baselines for TL;DR and CNN/DM are not the highest possible quality. When evaluating our model’s TL;DR summaries on a 7-point scale along several axes of quality ( accuracy , coverage , coherence , and overall ), labelers find our models can still generate inaccurate summaries, and give a perfect overall score 45% of the time. [12] For cost reasons, we also do not directly compare to using a similar budget to collect high-quality demonstrations, and training on those using standard supervised fine-tuning. We’re interested in scaling human feedback to tasks where humans can’t easily evaluate the quality of model outputs. For example, we might want our models to answer questions that would take humans a lot of research to verify; getting enough human evaluations to train our models this way would take a long time. One approach to tackle this problem is to give humans tools to help them evaluate more quickly and accurately. If these tools use ML, we can also improve them with human feedback, which could allow humans to accurately evaluate model outputs for increasingly complicated tasks. In addition to tackling harder problems, we're also exploring different types of feedback beyond binary comparisons: we can ask humans to provide demonstrations, edit model outputs to make them better, or give explanations as to why one model output is better than another. We'd like to figure out which kinds of feedback are most effective for training models that are aligned with human preferences. If you are interested in working on these research questions, we’re hiring ! We'd like to thank the following people who gave feedback on various iterations of the blog post: Douwe Kiela, Zach Lipton, Alex Irpan, Jack Clark, Jacob Hilton, Raul Puri, Miles Brundage, Greg Brockman, Ilya Sutskever, Kelly Sims, Wojciech Kryscinski, and Dzimitry Bahdanau. We'd also like to thank Justin Jay Wang for driving the blog post design, Ashley Pilipiszyn for editing, Alec Radford and Dario Amodei for guidance on the project, Shan Carter for help designing the main diagram, Gretchen Krueger for co-writing the model card, Beth Barnes for help with labeler hiring and general encouragement, and many other people at OpenAI for training our large pre-trained models, supporting us through computing infrastructure improvements and maintenance, and writing fast GPU kernels. Finally, we'd like to thank all of our contractors for providing the data that was essential for training the models in this post. For training, we use the Reddit TL;DR dataset instead of the more popular CNN/DM dataset because simple copying baselines perform better than the human-written reference summaries on CNN/DM, which is not the case for TL;DR (see Appendix D of our paper).  We performed a new web crawl to increase the TL;DR dataset size, required summaries to be between 24 and 48 tokens, and performed some other cleaning and filtering. ↩︎ We hire human labelers to judge summary quality, and implement quality control to ensure that labeler judgments agree with our own. We describe our human data collection procedure below. ↩︎ We generate all of our samples at temperature 0, which we found humans preferred most. ↩︎ While we use human-written TL;DRs as our main point of comparison, they don’t always represent optimal human performance; they are sometimes intended to be funny or to summarize only a part of the post, and their grammar and style are all over the map. ↩︎ We control by training a logistic regression model to predict the preferred summary given only the policy ID and the log ratio of the lengths of the summaries. Then, we report the regression coefficients on each policy ID, corresponding to a length ratio of 1 with the reference summaries. ↩︎ Interestingly, we found that human evaluators preferred the Lead-3 baseline (taking the first 3 sentences of the article) to the dataset’s reference summaries, and we confirmed this ourselves. ↩︎ We took this approach because it is hard to directly compare our TL;DR-trained models to models trained on CNN/DM; the CNN/DM summaries are much longer and written in bullet-point form. ↩︎ In terms of ROUGE results on CNN/DM, our 6.7B supervised models are a bit worse than T5 , but a bit better than state-of-the-art models from mid-2019 . ↩︎ Our main models are trained on about 65K comparisons, though we achieve good results with as few as 8K comparisons. ↩︎ Specifically, we use Upwork, Scale, and Lionbridge. Our contractors have a range of ages, genders, and educational backgrounds, and are mostly American or Filipino (see Appendix C of our paper for demographic data). ↩︎ Our criteria for hiring contractors were: (1) they were willing to do the task, and (2) they passed a minimum threshold of speed and agreement with researcher labels. We paid all our contractors at least $15/hr. ↩︎ This is impressive relative to the TL;DR reference summaries, which get a perfect overall score 23% of the time, but indicates there is still room for improvement. ↩︎ API Projects Blog About Jobs Research Announcements Events Milestones Newsroom Timeline Papers Charter", "date": "2020-09-04"},
{"website": "Open-AI", "title": "sparse-transformer", "author": ["Rewon Child", "Scott Gray"], "link": "https://openai.com/blog/sparse-transformer/", "abstract": "We've developed the Sparse Transformer, a deep neural network which sets new records at predicting what comes next in a sequence—whether text, images, or sound. It uses an algorithmic improvement of the attention mechanism to extract patterns from sequences 30x longer than possible previously. One existing challenge in AI research is modeling long-range, subtle interdependencies in complex data like images, videos, or sounds. The Sparse Transformer incorporates an $O(N \\sqrt{N})$ reformulation of the $O(N^2)$ Transformer self-attention mechanism, along with several other improvements, to apply it directly to these rich data types. Previously, models used on these data were specifically crafted for one domain or difficult to scale to sequences more than a few thousand elements long. In contrast, our model can model sequences with tens of thousands of elements using hundreds of layers, achieving state-of-the-art performance across multiple domains. At OpenAI, we're using it to help us build AI systems that possess a greater ability to understand the world. In Transformers, every output element is connected to every input element, and the weightings between them are dynamically calculated based upon the circumstances, a process called attention . While it is believed that this allows Transformers to be more flexible than models with fixed connectivity patterns, in practice it requires the creation of an $N\\times N$ attention matrix for every layer and attention head, which can consume large amounts of memory when applied to data types with many elements, like images or raw audio. One way to reduce this is by recomputing the attention matrix from checkpoints during backpropagation, a well-established technique in deep learning for reducing memory usage at the cost of more computation. When done for the attention matrix in Transformers, it means the largest memory cost becomes independent of the number of layers, letting us train networks with substantially greater depth than possible previously. In practice, we found that Transformers with depth up to 128 layers outperformed shallower networks on benchmark tasks like CIFAR-10. To train these models with increased depth, we made several adjustments to the ordering of operations in the transformer and modified the initialization scheme. Full details can be seen in our paper. Even computing a single attention matrix, however, can become impractical for very large inputs. We instead use sparse attention patterns, where each output position only computes weightings from a subset of input positions. When the subset is small relative to the full set of inputs (say, $\\sqrt{N}$ elements instead of $N$ elements), the resulting attention computation becomes tractable even for very long sequences, with an algorithmic complexity of $O(N \\sqrt{N})$ instead of $O(N^2)$. To assess the feasibility of the approach, we first visualized the learned attention patterns for deep Transformers on images, finding that many showed interpretable and structured sparsity patterns. Each of the below images shows which input pixels (highlighted in white) are attended to by a given attention head in order to predict the next value in the image. When the input portions are focused on small subsets and show a high degree of regularity, the layer is amenable to sparsification. A sampling of them are displayed here for a 128-layer model on CIFAR-10 images: While many layers displayed sparse structure, some layers clearly display dynamic attention that stretch over the entirety of the image. In order to preserve the ability of our network to learn such patterns, we implemented a two-dimensional factorization of the attention matrix, where the network can attend to all positions through two steps of sparse attention. The first version, strided attention, is roughly equivalent to each position attending to its row and its column, and is similar to the attention pattern learned by the network above. (Note that the column attention can be equivalently formulated as attending to the row of the transposed matrix). The second version, fixed attention, attends to a fixed column and the elements after the latest column element, a pattern we found useful for when the data didn’t fit into a two-dimensional structure (like text). For more details, we refer readers to our paper. Sparse Transformers set new state-of-the-art scores for density estimation of CIFAR-10, Enwik8, and Imagenet 64. We also found that sparse attention achieved lower loss than full attention, in addition to being significantly faster (see our paper for comparisons). This may point to a useful inductive bias from our sparsity patterns, or an underlying optimization issue with dense attention. Transformers that use sparse attention seem to have a notion of global structure, which can be qualitatively evaluated by looking at image completions. Here we visualize a model trained on $64\\times 64$ ImageNet: We also generated fully unconditional samples with an unadjusted softmax temperature of 1.0. These models are trained using the maximum likelihood objective, which is well-known to cover all modes of the data (including potentially nonexistent ones) instead of increasing fidelity of a smaller portion of the data. Sampling from these models with unadjusted temperature lets us see the full distribution of images that the model believes exists in the world. As a result, some samples can appear strange. Sparse Transformers can also be adapted to generate raw audio instead of images by simply changing the position embeddings. As deep learning expands to novel data types, we believe the ease of specifying inductive biases with this class of networks will be a useful tool. This model was trained on raw classical music clips and uses sparse attention to generate sequences of length 65,000. This corresponds to ~5 seconds of raw audio, and we have concatenated several samples together in each of the clips below. Normally, implementing sparse attention would involve slicing query and key matrices in blocks, so to ease experimentation we implemented a set of block-sparse kernels which efficiently perform these operations on the the GPU. We open-source these kernels and provide example sparse attention functions in this repository . The sparse attention patterns we introduced are only preliminary steps in the direction of efficient modeling of long sequences. We think exploring different patterns and combinations of sparsity is useful, and that learning sparse patterns is a particularly promising avenue of research for the next generation of neural network architectures. Even with the improvements we described above, autoregressive sequence generation still seems impractical for very high resolution images or video. The optimized attention operations we have introduced, however, may be useful primitives to combine with other approaches to modeling high dimensional data, like multi-scale approaches. If you are interested in advancing AI capabilities and helping further our mission of ensuring they benefit humanity, we’re hiring ! Thanks to Ashish Vaswani for helpful discussions, and Johannes Otterbach, Mark Chen, Prafulla Dhariwal, David Luan, and Lukasz Kaiser for comments on the manuscript.", "date": "2019-04-23"},
{"website": "Open-AI", "title": "gpt-3-apps", "author": ["OpenAI", "Ashley Pilipiszyn"], "link": "https://openai.com/blog/gpt-3-apps/", "abstract": "Nine months since the launch of our first commercial product, the OpenAI API , more than 300 applications are now using GPT-3, and tens of thousands of developers around the globe are building on our platform. We currently generate an average of 4.5 billion words per day, and continue to scale production traffic. Given any text prompt like a phrase or a sentence, GPT-3 returns a text completion in natural language. Developers can “program” GPT-3 by showing it just a few examples or “prompts.” We’ve designed the API to be both simple for anyone to use but also flexible enough to make machine learning teams more productive. To date, over 300 apps are using GPT-3 across varying categories and industries, from productivity and education to creativity and games. These applications utilize a suite of GPT-3’s diverse capabilities (and have helped us discover new ones!). A few of these include: Viable helps companies better understand their customers by using GPT-3 to provide useful insights from customer feedback in easy-to-understand summaries. Using GPT-3, Viable identifies themes, emotions, and sentiment from surveys, help desk tickets, live chat logs, reviews, and more. It then pulls insights from this aggregated feedback and provides a summary in seconds. For example, if asked, “What’s frustrating our customers about the checkout experience?”, Viable might provide the insight: “Customers are frustrated with the checkout flow because it takes too long to load. They also want a way to edit their address in checkout and save multiple payment methods.” “GPT-3’s ability to identify themes from natural language and generate summaries allows Viable to give product, customer experience, and marketing teams at companies across industries a better understanding of their customers’ wants and needs,” said Daniel Erickson, CEO of Viable. Visit Viable Fable Studio is creating a new genre of interactive stories and using GPT-3 to help power their story-driven “Virtual Beings.” Lucy, the hero of Neil Gaiman and Dave McKean’s Wolves in the Walls , which was adapted by Fable into the Emmy Award-winning VR experience, can have natural conversations with people thanks to dialogue generated by GPT-3. Lucy appeared as a guest at Sundance Film Festival 2021 and presented her own movie, Dracula . “GPT-3 has given us the ability to give our characters life,” said Fable Studio CEO Edward Saatchi, adding, “We’re excited to combine an artist’s vision, AI, and emotional intelligence to create powerful narratives, and believe that one day, everyone will know a Virtual Being.” Visit Fable Studio Algolia uses GPT-3 in their Algolia Answers product to offer relevant, lightning-fast semantic search for their customers. When the OpenAI API launched, Algolia partnered with OpenAI to integrate GPT-3 with their advanced search technology in order to create their new Answers product that better understands customers’ questions and connects them to the specific part of the content that answers their questions. Algolia Answers helps publishers and customer support help desks query in natural language and surface nontrivial answers. After running tests of GPT-3 on 2.1 million news articles, Algolia saw 91% precision or better and Algolia was able to accurately answer complex natural language questions four times more often than BERT. “We've seen great results from Algolia Answers on questions that are difficult to answer with textual search alone,” said Peter Buffington, Product Manager at ABC Australia. “It was able to return very relevant, evergreen content from our news archives for questions such as 'Why does a volcano erupt?’” “GPT-3 allows Algolia to answer more complex queries than ever before with our Algolia Answers product, identifying deeper contextual information to improve the quality of results and deliver them in seconds,” said Dustin Coates, Product and GTM Manager at Algolia. Visit Algolia As we scale access, our team is continually improving the platform—from implementing a content filter to offering new features for developers including our recently launched: Answers endpoint : Searches provided information (documents, knowledge bases etc.) for relevant context to be added to the prompt before completing with GPT-3. Can be used to build applications like customer support bots with no fine-tuning. Classifications endpoint : Can leverage labeled training data without fine-tuning. By searching for the closest examples with respect to the input query and adding them to prompt, it often matches the performance of state of the art fine-tuned models, providing an autoML solution that is easy to configure and adapt. Enhanced search endpoint : Provides the backbone for the Answers and Classifications endpoints that scales to a large number of documents while also being cheap and fast. Safety : Bias and misuse are important, industry-wide problems we take very seriously. We review all applications and approve only those for production that use GPT-3 in a responsible manner. We require developers to implement safety measures such as rate limits, user verification and testing, or human-in-the-loop requirements before they move into production. We also actively monitor for signs of misuse as well as “ red team ” applications for possible vulnerabilities. Additionally, we have developed and deployed a content filter that classifies text as safe, sensitive, or unsafe. We currently have it set to err on the side of caution, which results in a higher rate of false positives. Prompt library : Provides starter prompt design examples for dozens of use cases that users can begin programming with directly in Playground, like a Spreadsheet Generator, Grammar Corrector, or Airport Code Extractor. Prompt design examples that users can begin programming with directly. We have a growing community of tens of thousands of developers around the world, with the majority across North America, Europe, Asia, and Australia. We’ve also found that many of our developers tend to be those without a traditional AI or software engineering background. It’s been encouraging to hear from several of our developers that their first experience with an API or programming has been with OpenAI’s interface. For myself, and other mission-driven innovators, OpenAI has given us the tool we finally need to make transformative change in the community with GPT-3. With natural language processing, technical experience is no longer a barrier, and we can truly keep our focus on solving real world problems. In my work with a lot of first-time developers, those who are most successful at building with GPT-3 are great communicators as they are able to unlock the nuances of prompt design.” Programming with GPT-3 can feel like a much more creative process compared to traditional coding because of the natural language prompts. I believe AI will be integrated into every product in the future, and it’s been a pleasure working with developers of all experience levels from across the world who are creating innovative apps through the API.” We think there are still many new capabilities of GPT-3 yet to be discovered and we want you to help us uncover them! In a similar spirit to our previous Requests for Research and Y Combinator’s Requests for Startups , we’d love to see our current and future developers push the limits of what’s possible with GPT-3 and build new applications in the following areas: We are happy to support hackathons and provide API access for these events, especially if they include challenges in the above areas (we of course are open to other challenge areas as well!). Please email community@openai.com with details about the event. We’re excited to see what our developers build next. If you are interested in joining our Applied AI team, who focus on bringing OpenAI's technology and products to the world, we're hiring ! Hannah Wong, Justin Jay Wang, Steve Dowling, Greg Brockman, Mira Murati, Peter Welinder, Sam Altman, Luke Miller, Katie Mayer, Steven Adler, David Schnurr, Maddie Simens, Miles Brundage, Gretchen Krueger, Andrew Mayne & Raf Jakubanis.", "date": "2021-03-25"},
{"website": "Open-AI", "title": "openai-scholars-2020-final-projects", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-scholars-2020-final-projects/", "abstract": "These projects investigated problems such as analyzing how GPT-2 represents grammar, measuring the interpretability of models trained on Coinrun, and predicting epileptic seizures using brain recordings. More information about the next class of Scholars and how to apply will be announced this fall. The OpenAI Scholars program provides stipends and mentorship to individuals from underrepresented groups to study deep learning and open-source a project. Our Scholars have demonstrated core technical skills across various expert domains and self-motivation—critical competencies for a self-directed program like this one. They each entered the field of machine learning as relative newcomers, and we hope their progress shows how accessible machine learning is. Learn more about our Scholars program . Alethea Power Andre Carerra Cathy Yeh Jorge Orbay Kamal Ndousse Kata Slama Pamela Mishkin I’m fascinated by neural network interpretability. Understanding how networks of various architectures represent information can help us build simpler and more efficient networks, as well as predict how the networks we’ve built will behave, and perhaps even give us some insight into how human beings think. Along these lines, I analyzed how GPT-2 represents English grammar, and found smaller sub-networks that seem to correspond to various grammatical structures. I will present my methodology and results. Next, I want to work on understanding how neural networks represent information, and use that understanding to better predict how deep learning systems behave. I believe this work will make such systems safer and more beneficial to humanity, as well as making them simpler, faster, and more computationally efficient. My scholars program project is semantic parsing English-to-GraphQL. Given an English prompt such as “How many employees do we have?”, find a corresponding GraphQL query to return the information. The project involved creating a dataset, training models, and creating an interaction tool to see results. I wanted to have a say in how AI is shaped—the Scholars program has been a great opportunity to learn and participate. Standard reinforcement learning algorithms struggle with poor sample efficiency in the presence of sparse rewards with long temporal delays between action and effect. To address the long term credit assignment problem, we use “temporal reward transport” (TRT) to augment the immediate rewards of significant state-action pairs with rewards from the distant future, using an attention mechanism to identify candidates for TRT. A series of gridworld experiments show clear improvements in learning when TRT is used in conjunction with a standard advantage actor critic algorithm. I appreciate that this program gave me the freedom to learn deeply and flex my creativity. This project’s purpose is to create a scalar that measures the interpretability of an A2C model trained on Procgen’s Coinrun. The scalar is generated using a combination of attribution on the model and masks of Coinrun’s assets. The scalar is used to test the validity of the diversity hypothesis. This program, and specifically my mentor, has fostered a self-confidence in me to dive into a field I don’t understand and break down problems until I can solve them. I’m hoping to take the self-confidence I’ve learned from this program to continue breaking down problems in and with AI. My project has explored the social transfer of expertise among completely independent RL agents trained in shared environments. The motivating question is whether novice agents can learn to mimic expert behavior to solve hard-exploration tasks that they couldn't master in isolation. I’ll discuss my observations as well as the environments I developed to experiment with social skill transfer. I joined the Scholars program in order to learn from the brilliant folks at OpenAI and to immerse myself in AI research. I’m grateful to have had the opportunity to explore state of the art research with the support of such talented researchers (special thanks to my mentor Natasha Jaques!) I have been working on a project to predict epileptic seizures using brain recordings. I framed it as an image classification problem based on the spectrogram representation of the brain data. My most successful model so far has been a ResNet18. In my post-Scholars life, I plan to continue working on this project, and make my way to interpretability of spectrogram classification networks. I wanted to learn how to apply deep learning for solving scientific and real-world problems. The OpenAI Scholars program was this magical opportunity to get started by learning from the very best minds in the field. Adversarial perturbations are well-understood for images but less so for language. My presentation will review the literature on how universal adversarial examples can inform understanding of generative models, replicating results generating universal adversarial triggers for GPT-2 and for attacking NLI models. This program strengthened my technical basis in machine learning and helped me understand how AI researchers understand policy implications of their work. Diversity is core to AI having a positive effect on the world—it’s necessary to ensure the advanced AI systems in the future are built to benefit everyone . If you’re excited to begin your own journey into ML, check out some of our educational materials . More information about the next class of scholars and how to apply will be announced this fall. Stay tuned! Huge thanks to Microsoft for providing Azure compute credits to scholars, to our mentors for their time and commitment, and to all the supporters that made this program possible.", "date": "2020-07-09"},
{"website": "Open-AI", "title": "openai-scholars-2021-final-projects", "author": ["OpenAI"], "link": "https://openai.com/blog/openai-scholars-2021-final-projects/", "abstract": "We’re proud to announce that the 2021 class of OpenAI Scholars has completed our six-month mentorship program and have produced an open-source research project with stipends and support from OpenAI. Working alongside leading OpenAI researchers that created GPT-3 and DALL·E, our Scholars explored topics like AI safety, contrastive learning, generative modeling, scaling laws, auto-encoding multi-objective tasks, test time compute, NLP segmentation strategies, and summarization from human feedback. To wrap up the program, our nine Scholars share their work and how the Scholars Program has impacted their careers. Read more about each of them and their projects below. Christina Kim Danielle Ensign Ellie Kitanidis Jonathan Ward Kudzo Ahegbebu Legg Yeung Sam Gbafa Shola Oyedele Tyna Eloundou Previously, I was the founding engineer at Sourceress, where I built the infrastructure for our machine learning pipeline and human-in-the-loop labeling system. My background is in software engineering and productionizing machine learning. Building upon OpenAI’s recent work on scaling laws, my project explores how much pre-training on English helps when transferring across different languages as we vary model size and dataset size. I found that a) pre-trained English models help most when learning German, then Spanish, and finally Chinese and b) transfer from English to Chinese, German, and Spanish scales predictably in terms of parameters, data, and compute. My advice to someone starting in deep learning research is to take your time to understand insights from fundamental papers and remember that the field is still relatively new. There's a lot of room for individuals to have an outsized impact. I have a background in Software Development, AI Fairness, and VR Game Development. I was interested in the Scholars program as a way of strengthening my research skills, learning from other talented people in the field, and moving into industry research or engineering positions. My project is exploratory, investigating prior work on opinion modeling from the context of deep learning. As these models generate more and more text, it's important to understand the impacts they'll have on the ecosystem of opinions and future models. In addition, I investigated what happens when models are iteratively trained on outputs from previous models. If you can, take a few months to carefully work through the 2019 fast.ai course (parts 1 and 2), Andrew Ng's deep learning course on Coursera, David Silver's RL Course , and Spinning Up in Deep RL . If you don't have a background in statistics, building a more solid foundation in that would be useful as well. This will give you a headstart in learning how to do productive research as you need to spend less time learning the core concepts. In addition, if you haven't yet, try to implement a few papers from scratch in pytorch. Pick old papers that have existing implementations, so you can reference those implementations if you get stuck. See if you can improve the paper by applying an idea from a later paper. This process will give you a better idea of what doing DL research is like. I’m a research scientist with a physics background and a focus on dark energy, dark matter, and large-scale structure of the Universe. For my project, I pre-trained a language representation model using a purely contrastive objective. I am interested in the generalizability and scalability of such models compared to models pre-trained with more traditional language modeling objectives. I am also curious about what factors influence the performance of contrastive language encoders. In this talk, I present our methodology and some preliminary results. Navigating a career change during COVID-19 was daunting, but this program created the perfect environment for me to learn, gain hands-on experience, and orient myself in the field. Discussions with my mentor and others at OpenAI exposed me to expert insights and intuitions that can’t be found in a textbook. The most important thing I discovered, however, was how much I love doing AI research. I plan to continue growing my career in this direction. I joined the Scholars Program to build computer systems that better understand what people really value. I live in Washington, D.C. and lately, I’ve really enjoyed building fantastic contraptions with K’nex. My recent work at OpenAI has demonstrated that reward models trained on human feedback can support Reinforcement Learning. My project demonstrates that reward models can be trained on large-scale structured feedback extracted from websites. My advice to people looking to join: make open source projects! Find the simplest interesting idea that you can think of and build it! I am a software engineer with an applied physics and aerospace background. My presentation explores the generalizability of models leveraging test time compute in a number of domains including autoregressive transformers, deep equilibrium models, and graph neural networks. In it, I ask: Given the constraints of limited training compute budget, can small adaptive models instead leverage test time compute to overcome the handicap of having a smaller number of learnable parameters? Lastly, we present mechanisms that show promise in reducing the computational cost and improving the performance of graph neural networks. The Scholars program has given me the confidence to pursue new avenues of deep learning interest and research as well as an increased measure of competency so that I may operate with greater clarity, efficiency and ethical maturity. It’s also reignited a latent research interest which I hope to continue to nurture into the future. I was formally trained as a data scientist and architect, but I pivoted my career because AI has a much higher agency on our environment than conventional industries, and there are many interesting research problems in this field. In my project, I extended the well-known card game \"SET\" to investigate the relationship between vector representation dimension and task composition. I found non-contrastive models of X parameters to solve games that contrastive models of 2X+ parameters cannot. What can a contrastive model learn with vector representations of size 16/32/64/128/256/512? And what not? I came to the program with a few interests (reasoning, compositionality, multimodal). My mentor helped me a lot in terms of crystallizing these interests into concrete research questions and proposals. We explored multiple directions and kept iterating until we saw something promising. The process was intense, but the lessons were worth the effort. I was drawn to the Scholar’s program because I’d seen some of what OpenAI’s models could do and I wanted to understand what it took to build and iterate such powerful models. Having the dedicated time to explore deep learning with great mentorship has been transformative in my ability to understand and contribute to the field! When I’m not working, I’m usually tinkering with gadgets or out seeking adrenaline with friends. My project explores the tradeoffs in using these other tokenization schemes and how these different tokenizations scale. I also consider an approach to learning a sequence's segmentation instead of using a predefined one. The Scholars program gave me the space to explore many different ideas in ML and deep learning, from \"classical\" stuff like CNNs and RNNs to understanding the tradeoffs of more recent transformer variants. Being able to have conversations with the researchers at OpenAI made me realize that the frontier of AI research is very accessible. I originally wanted to learn about the current state of the art, but being here for these past few months has let me understand that I can contribute meaningfully to advancing the state of deep learning and AI. Being at OpenAI has also caused me to think a lot about the implications of the models we create and ways to provide such models to the world while minimizing potential harm. I almost majored in French in college because I've always loved language. I frequently watch movies and tv shows in other languages (yes - kdramas are at the top of that list) but I never imagined that my love of language would translate into me doing research in NLP. In my research, I explore the tradeoffs between model performance and the cost of training, and study scaling laws on different transformer architectures to understand the impact of transformer architecture on model performance. Everything about my perspective has changed since joining the program. There are very few companies and institutions in the world that use machine learning at scale and have a vision of where the field of ML/AI is headed. Even fewer are opportunities for those who don't have research experience and an advanced degree, let alone a program focused on underrepresented groups. Just the significance of joining this program at a time when the industry is discovering the potential of GPT3 has changed my vision of what the future of technology offers and what my place within that could be. I think people assume you need a technical degree to study AI but I was just curious about the future and wanted a part in building it. I applied to OpenAI because I wanted the profound privilege to wrestle with questions that shape ever-complex AI systems. As a Cameroonian native who grew up in the US, I navigate multiple perspectives (scholastically, culturally and linguistically) and was curious to learn how AI learns from human commonalities and differences. The arduous rewards and constraint engineering process can sometimes lead to misalignment between a designer's idea of success and its analytic specification. Furthermore, many real-world tasks contain multiple objectives and current approaches in reinforcement learning do not offer a direct lever to choose between Pareto-equivalent strategies. To address these problems, in my project, I explain how we use \"multiple experts, multiple objectives\" (MEMO) to explore an agent's ability to consume examples of success from multiple experts with different objectives, and learn a single conditional policy that can be oriented at the discretion of a supervisor. For newcomers to the field, I would recommend slowly stepping through clean open source implementations of well-known algorithms while reading their theoretical grounding. Try to experiment with the designs often. Fast.ai and Andrew Ng's courses are excellent resources for the journey.", "date": "2021-05-10"},
{"website": "Open-AI", "title": "learning-day", "author": ["OpenAI"], "link": "https://openai.com/blog/learning-day/", "abstract": "At OpenAI, each Thursday is Learning Day: a day where employees have the option to self-study technical skills that will make them better at their job but which aren’t being learned from daily work. We’ve found that the biggest contributions at OpenAI come from cross-functional experts, so we either need to hire them or grow them here. Before Learning Day, we very rarely saw people grow cross-functionally—for example, employees coming from a software background rarely picked up machine learning (something equally rare in other organizations except academia). Since Learning Day, this kind of growth has become very common. On a typical learning day, people do things like: Reimplement papers. Follow deep learning tutorials. Play with new tools in cluster management, compilation, virtual world generation, or coding paradigms. Learn how to do research on bite-sized problems. Read about new developments in seemingly unrelated areas of AI. We think Learning Day might be useful for other organizations, so we’d like to share how it started and works at OpenAI. We first tried out Learning Day on our Robotics team. Here’s how our Head of Robotics, Wojciech Zaremba (Woj), came up with the idea: In November 2018, I realized that I’d been stagnating in a number of areas because I was always overwhelmed with urgent tasks. These areas were becoming increasingly important for me to know. For example, I kept wanting to evaluate whether my team should switch deep learning frameworks, but I kept being interrupted after an hour or two of coding—which resulted in no forward progress. I kept hearing about research in other domains like causality or energy-based models which might be applicable to robotics, but I didn’t know anything about these fields—and reading about them for half an hour at a time wasn’t helpful. I knew the best way to solve this problem would be to carve out a day a week for learning. But if this was what I needed to be more productive, it seemed likely that this would also be what my team needed. So I tried doing this for the whole team as an experiment. I figured that we’d take a short-term productivity hit but see gains in one to two years. But within a month, I started to see better communication between researchers and engineers, with everyone starting to use jargon from each others’ specialty correctly (e.g. discounted reward, MAML, self-attention, container, SRAM, StatefulSet, Raft). Within half a year, I started to see researchers talking about restructuring our codebase using domain-driven design , and engineers picking up research tasks. Though we encouraged self-study before, it never seemed to work. That’s different now—for example, one team member went from knowing nothing about machine learning to making computer vision contributions within three months. One very strong engineer studied RL for half a year, and now is producing outputs comparable to what I'd expect from an RL PhD. Learning Day happens each Thursday. Woj wrote the following guidelines for the Robotics team, but we’ve adapted these principles across each team that has adopted Learning Day: Learning day is a gift ❤️ Feel free to use learning day for: Reading AI papers Reimplementing AI papers Going over AI tutorials Having your tiny side research AI projects Learning about fundamentals: linear algebra, statistics etc. Learning about ML fundamentals: information theory, Bayesian inference etc. Learning about engineering. Read about new programming languages, frameworks (e.g. what is Rust?) Learning about management: self-management, prioritization, how renowned research labs works (e.g. Manhattan project, Bell labs) Learning about mechanical engineering or having your mechanical engineering research project (e.g. build a linear actuator) Learning about any skill that will level you up in your work (in my case, it's better writing) This learning day is a gift from Woj. Therefore, I kindly ask you to: Take advantage of this day, and really use it for learning. Don't waste it on Netflix, or don't use it as an extra weekend day. Please write what you are planning to learn, and please write summaries. This will help to motivate others to keep on pushing. Please don't be shy in sharing what you are learning. It's fine if you don't know fundamentals, and it's fine to say that you learned what is matrix multiplication. Everyone of us was there at some point. I would like to help you to level up. Please make sure that activities from the learning day don't leak out to other non-learning days. For instance, it would suck if you would devote the entire week on learning fundamentals, or working on your side project. To keep people accountable, we ask everyone to post in Slack what they learned that day. The following are examples of what people learn on a single Learning Day. \" Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules \" \" Learning Domain Randomization Distributions for Transfer of Locomotion Policies \" \" Neural Graph Evolution: Towards Efficient Automatic Robot Design Learning to Learn with Probabilistic Task Embeddings \" \" Mid-Level Visual Representations Improve Generalization and Sample Efficiency for Learning Visuomotor Policies \" \" Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables \" \" Does computer vision matter for action? \" \" WAIC, but Why? Generative Ensembles for Robust Anomaly Detection \" \" Weight Agnostic Neural Networks \" \" Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations \" Deep unsupervised learning Deep RL Bootcamp Reptile and MAML Play with code in JAX Apply Sparse Transformers to vision tasks Implement LSTM and transformer from scratch; train them on Penn treebank Train a neural net to reproduce the behavior of a physical motor Time Series Analysis The Book of Why: The New Science of Cause and Effect Thanks for the Feedback: The Science and Art of Receiving Feedback Well Change Your Questions, Change Your Life: 10 Powerful Tools for Life and Work Dark Territory: The Secret History of Cyber War The Hacked World Order: How Nations Fight, Trade, Maneuver, and Manipulate in the Digital Age Technology Transfer to the USSR, 1928–1937 and 1966–1975: The Role of Western Technology in Soviet Economic Development The Turing Test: Verbal Behavior as the Hallmark of Intelligence The Information: A History, A Theory, A Flood Radical Markets: Uprooting Capitalism and Democracy for a Just Society We also reimburse reasonable self-studying expenses such as books and tutors, used mostly to learn fundamentals in mathematics. These costs are very worthwhile investments! Learning Day’s impact comes from being rigorous about how people use it. It’s not a day for leisure, but instead a day for a specific kind of hard work. We see and try to counteract the following failure modes so that we can sustain it long term: Learning Day could be used for work . Learning Day could turn into a normal working day because people may want to accomplish their main project faster (due to internal or external pressure). We prevent this by having Learning Day on the same day for every team. This creates positive peer pressure and encourages everyone to take advantage of Learning Day. Learning Day could expand in scope to non–Learning Days . We actually haven’t yet observed this happen. Based on what we’ve seen with other organizations, we think this would most likely indicate that the person isn’t excited enough about their main project, and would be a sign to their manager that the person should switch teams or projects. Learning Day could be used for leisure . Our solution is for every team member to share their progress on Slack via Geekbot . This keeps the excitement high and provides an accountability mechanism. We’ve recently expanded Learning Day from a subset of our technical teams to the entire company. It’s become a cultural staple—on our most recent internal survey, Learning Day was the aspect of our culture that people talked about the most. We’re excited to see its impact as we continue to evolve and support Learning Day in the future.", "date": "2019-08-01"},
{"website": "Open-AI", "title": "learning-dexterity", "author": ["Marcin Andrychowicz", "Bowen Baker", "Arthur Petron", "Matthias Plappert", "Alex Ray", "Jonas Schneider", "Josh Tobin", "Peter Welinder", "Lilian Weng", "Rafał Józefowicz", "Jakub Pachocki", "Szymon Sidor", "Bob McGrew", "Maciek Chociej", "Glenn Powell", "Wojciech Zaremba"], "link": "https://openai.com/blog/learning-dexterity/", "abstract": "Our system, called Dactyl, is trained entirely in simulation and transfers its knowledge to reality, adapting to real-world physics using techniques we've been working on for the past year . Dactyl learns from scratch using the same general-purpose reinforcement learning algorithm and code as OpenAI Five . Our results show that it's possible to train agents in simulation and have them solve real-world tasks, without physically-accurate modeling of the world. Examples of dexterous manipulation behaviors autonomously learned by Dactyl. Dactyl is a system for manipulating objects using a Shadow Dexterous Hand . We place an object such as a block or a prism in the palm of the hand and ask Dactyl to reposition it into a different orientation; for example, rotating the block to put a new face on top. The network observes only the coordinates of the fingertips and the images from three regular RGB cameras. Although the first humanoid hands were developed decades ago, using them to manipulate objects effectively has been a long-standing challenge in robotic control. Unlike other problems such as locomotion , progress on dextrous manipulation using traditional robotics approaches has been slow, and current techniques remain limited in their ability to manipulate objects in the real world. Reorienting an object in the hand requires the following problems to be solved: Working in the real world. Reinforcement learning has shown many successes in simulations and video games, but has had comparatively limited results in the real world. We test Dactyl on a physical robot. High-dimensional control. The Shadow Dexterous Hand has 24 degrees of freedom compared to 7 for a typical robot arm. Noisy and partial observations. Dactyl works in the physical world and therefore must handle noisy and delayed sensor readings. When a fingertip sensor is occluded by other fingers or by the object, Dactyl has to work with partial information. Many aspects of the physical system like friction and slippage are not directly observable and must be inferred. Manipulating more than one object. Dactyl is designed to be flexible enough to reorient multiple kinds of objects. This means that our approach cannot use strategies that are only applicable to a specific object geometry. Dactyl learns to solve the object reorientation task entirely in simulation without any human input. After this training phase, the learned policy works on the real robot without any fine-tuning. Learning methods for robotic manipulation face a dilemma. Simulated robots can easily provide enough data to train complex policies, but most manipulation problems can’t be modeled accurately enough for those policies to transfer to real robots. Even modeling what happens when two objects touch — the most basic problem in manipulation — is an active area of research with no widely accepted solution. Training directly on physical robots allows the policy to learn from real-world physics, but today’s algorithms would require years of experience to solve a problem like object reorientation. Our approach, domain randomization , learns in a simulation which is designed to provide a variety of experiences rather than maximizing realism. This gives us the best of both approaches: by learning in simulation, we can gather more experience quickly by scaling up, and by de-emphasizing realism, we can tackle problems that simulators can only model approximately. It's been shown (by OpenAI and others ) that domain randomization can work on increasingly complex problems — domain randomizations were even used to train OpenAI Five . Here, we wanted to see if scaling up domain randomization could solve a task well beyond the reach of current methods in robotics. We built a simulated version of our robotics setup using the MuJoCo physics engine. This simulation is only a coarse approximation of the real robot: Measuring physical attributes like friction, damping, and rolling resistance is cumbersome and difficult. They also change over time as the robot experiences wear and tear. MuJoCo is a rigid body simulator, which means that it cannot simulate the deformable rubber found at the fingertips of the hand or the stretching of tendons. Our robot can only manipulate the object by repeatedly making contact with it. However, contact forces are notoriously difficult to reproduce accurately in simulation. The simulation can be made more realistic by calibrating its parameters to match robot behavior, but many of these effects simply cannot be modeled accurately in current simulators. Instead, we train the policy on a distribution of simulated environments where the physical and visual attributes are chosen randomly. Randomized values are a natural way to represent the uncertainties that we have about the physical system and also prevent overfitting to a single simulated environment. If a policy can accomplish the task across all of the simulated environments, it will more likely be able to accomplish it in the real world. By building simulations that support transfer, we have reduced the problem of controlling a robot in the real world to accomplishing a task in simulation, which is a problem well-suited for reinforcement learning. While the task of manipulating an object in a simulated hand is already somewhat difficult , learning to do so across all combinations of randomized physical parameters is substantially more difficult. To generalize across environments, it is helpful for the policy to be able to take different actions in environments with different dynamics. Because most dynamics parameters cannot be inferred from a single observation, we used an LSTM — a type of neural network with memory — to make it possible for the network to learn about the dynamics of the environment. The LSTM achieved about twice as many rotations in simulation as a policy without memory. Dactyl learns using Rapid , the massively scaled implementation of Proximal Policy Optimization developed to allow OpenAI Five to solve Dota 2. We use a different model architecture, environment, and hyperparameters than OpenAI Five does, but we use the same algorithms and training code. Rapid used 6144 CPU cores and 8 GPUs to train our policy, collecting about one hundred years of experience in 50 hours. For development and testing, we validated our control policy against objects with embedded motion tracking sensors to isolate the performance of our control and vision networks. Dactyl was designed to be able to manipulate arbitrary objects, not just those that have been specially modified to support tracking. Therefore, Dactyl uses regular RGB camera images to estimate the position and orientation of the object. We train a pose estimator using a convolutional neural network. The neural network takes the video streams from three cameras positioned around the robot hand and outputs the estimated position and orientation of the object. We use multiple cameras to resolve ambiguities and occlusion. We again use domain randomization to train this network only in simulation using the Unity game development platform, which can model a wider variety of visual phenomena than Mujoco. By combining these two independent networks, the control network that reorients the object given its pose and the vision network that maps images from cameras to the object’s pose, Dactyl can manipulate an object by seeing it. When deploying our system, we noticed that Dactyl uses a rich set of in-hand dexterous manipulation strategies to solve the task. These strategies are commonly used by humans as well. However, we do not teach them to our system explicitly; all behaviors are discovered autonomously. We observed that for precision grasps, such as the Tip Pinch grasp, Dactyl uses the thumb and little finger. Humans tend to use the thumb and either the index or middle finger instead. However, the robot hand's little finger is more flexible due to an extra degree of freedom , which may explain why Dactyl prefers it. This means that Dactyl can rediscover grasps found in humans, but adapt them to better fit the limitations and abilities of its own body. We tested how many rotations Dactyl could achieve before it dropped the object, timed out, or reached 50 successes. Our policies trained purely in simulation were able to successfully manipulate objects in the real world. For the task of block manipulation, policies trained with randomization could achieve many more rotations than those trained without randomization, as can be seen in the results below. Also, using the control network with pose estimated from vision performs nearly as well as reading the pose directly from motion tracking sensors. The vast majority of training time is spent making the policy robust to different physical dynamics. Learning to rotate an object in simulation without randomizations requires about 3 years of simulated experience, while achieving similar performance in a fully randomized simulation requires about 100 years of experience. Tactile sensing is not necessary to manipulate real-world objects. Our robot receives only the locations of the five fingertips along with the position and orientation of the cube. Although the robot hand has touch sensors on its fingertips, we didn't need to use them. Generally, we found better performance from using a limited set of sensors that could be modeled effectively in the simulator instead of a rich sensor set with values that were hard to model. Randomizations developed for one object generalize to others with similar properties. After developing our system for the problem of manipulating a block, we printed an octagonal prism, trained a new policy using its shape, and attempted to manipulate it. Somewhat to our surprise, it achieved high performance using only the randomizations we had designed for the block. By contrast, a policy that manipulated a sphere could only achieve a few successes in a row, perhaps because we had not randomized any simulation parameters that model rolling behavior. With physical robots, good systems engineering is as important as good algorithms. At one point, we noticed that one engineer consistently achieved much better performance than others when running the exact same policy. We later discovered that he had a faster laptop, which hid a timing bug that reduced performance. After the bug was fixed, performance improved for the rest of the team. We also found to our surprise that a number of commonly employed techniques did not improve our results. Decreasing reaction time did not improve performance. Conventional wisdom states that reducing the time between actions should improve performance because the changes between states are smaller and therefore easier to predict. Our current time between actions is 80ms, which is smaller than human reaction time of 150-250ms, but significantly larger than neural network computation time of roughly 25ms. Surprisingly, decreasing time between actions to 40ms required additional training time but did not noticeably improve performance in the real world. It’s possible that this rule of thumb is less applicable to neural network models than to the linear models that are in common use today. Using real data to train our vision policies didn’t make a difference. In early experiments, we used a combination of simulated and real data to improve our models. The real data was gathered from trials of our policy against an object with embedded tracking markers. However, real data has significant disadvantages compared to simulated data. Position information from tracking markers has latency and measurement error. Worse, real data is easily invalidated by common configuration changes, making it a hassle to collect enough to be useful. As our methods developed, our simulator-only error improved until it matched our error from using a mixture of simulated and real data. Our final vision models were trained without real data. This project completes a full cycle of AI development that OpenAI has been pursuing for the past two years: we've developed a new learning algorithm , scaled it massively to solve hard simulated tasks , and then applied the resulting system to the real world. Repeating this cycle at increasing scale is the primary route we are pursuing to increase the capabilities of today's AI systems towards safe artificial general intelligence. If you'd like to be part of what comes next, we're hiring !", "date": "2018-07-30"},
{"website": "Open-AI", "title": "openai-five", "author": ["Brooke Chan", "Christy Dennison", "David Farhi", "Filip Wolski", "Greg Brockman", "Henrique Pondé", "Jakub Pachocki", "Jie Tang", "Jonathan Raiman", "Michael Petrov", "Przemysław Dębiak", "Susan Zhang", "Szymon Sidor"], "link": "https://openai.com/blog/openai-five/", "abstract": "Our team of five neural networks, OpenAI Five, has started to defeat amateur human teams at Dota 2 . While today we play with restrictions , we aim to beat a team of top professionals at The International in August subject only to a limited set of heroes. We may not succeed: Dota 2 is one of the most popular and complex esports games in the world, with creative and motivated professionals who train year-round to earn part of Dota's annual $40M prize pool (the largest of any esports game). OpenAI Five plays 180 years worth of games against itself every day, learning via self-play. It trains using a scaled-up version of Proximal Policy Optimization running on 256 GPUs and 128,000 CPU cores — a larger-scale version of the system we built to play the much-simpler solo variant of the game last year. Using a separate LSTM for each hero and no human data, it learns recognizable strategies. This indicates that reinforcement learning can yield long-term planning with large but achievable scale — without fundamental advances, contrary to our own expectations upon starting the project. To benchmark our progress, we'll host a match versus top players on August 5th. Follow us on Twitch to view the live broadcast, or request an invite to attend in person! OpenAI Five playing the best OpenAI employee team. The match was commentated by professional commentator Blitz and OpenAI Dota team member Christy Dennison, and observed by a crowd from the community. One AI milestone is to exceed human capabilities in a complex video game like StarCraft or Dota. Relative to previous AI milestones like Chess or Go , complex video games start to capture the messiness and continuous nature of the real world. The hope is that systems which solve complex video games will be highly general, with applications outside of games. Dota 2 is a real-time strategy game played between two teams of five players, with each player controlling a character called a \"hero\". A Dota-playing AI must master the following: Long time horizons. Dota games run at 30 frames per second for an average of 45 minutes, resulting in 80,000 ticks per game. Most actions (like ordering a hero to move to a location) have minor impact individually, but some individual actions like town portal usage can affect the game strategically; some strategies can play out over an entire game. OpenAI Five observes every fourth frame, yielding 20,000 moves. Chess usually ends before 40 moves, Go before 150 moves, with almost every move being strategic. Partially-observed state. Units and buildings can only see the area around them. The rest of the map is covered in a fog hiding enemies and their strategies. Strong play requires making inferences based on incomplete data, as well as modeling what one's opponent might be up to. Both chess and Go are full-information games. High-dimensional, continuous action space. In Dota, each hero can take dozens of actions, and many actions target either another unit or a position on the ground. We discretize the space into 170,000 possible actions per hero (not all valid each tick, such as using a spell on cooldown ); not counting the continuous parts, there are an average of ~1,000 valid actions each tick. The average number of actions in chess is 35; in Go, 250. High-dimensional, continuous observation space. Dota is played on a large continuous map containing ten heroes, dozens of buildings, dozens of NPC units, and a long tail of game features such as runes, trees, and wards. Our model observes the state of a Dota game via Valve's Bot API as 20,000 (mostly floating-point) numbers representing all information a human is allowed to access. A chess board is naturally represented as about 70 enumeration values (a 8x8 board of 6 piece types and minor historical info ); a Go board as about 400 enumeration values (a 19x19 board of 2 piece types plus Ko ). The Dota rules are also very complex — the game has been actively developed for over a decade, with game logic implemented in hundreds of thousands of lines of code. This logic takes milliseconds per tick to execute, versus nanoseconds for Chess or Go engines. The game also gets an update about once every two weeks, constantly changing the environment semantics. Our system learns using a massively-scaled version of Proximal Policy Optimization . Both OpenAI Five and our earlier 1v1 bot learn entirely from self-play. They start with random parameters and do not use search or bootstrap from human replays. RL researchers (including ourselves) have generally believed that long time horizons would require fundamentally new advances, such as hierarchical reinforcement learning . Our results suggest that we haven't been giving today's algorithms enough credit — at least when they're run at sufficient scale and with a reasonable way of exploring . Our agent is trained to maximize the exponentially decayed sum of future rewards, weighted by an exponential decay factor called γ . During the latest training run of OpenAI Five, we annealed γ from 0.998 (valuing future rewards with a half-life of 46 seconds) to 0.9997 (valuing future rewards with a half-life of five minutes). For comparison, the longest horizon in the PPO paper was a half-life of 0.5 seconds, the longest in the Rainbow paper was a half-life of 4.4 seconds, and the Observe and Look Further paper used a half-life of 46 seconds. While the current version of OpenAI Five is weak at last-hitting (observing our test matches, the professional Dota commentator Blitz estimated it around median for Dota players), its objective prioritization matches a common professional strategy. Gaining long-term rewards such as strategic map control often requires sacrificing short-term rewards such as gold gained from farming , since grouping up to attack towers takes time. This observation reinforces our belief that the system is truly optimizing over a long horizon. Each of OpenAI Five's networks contain a single-layer, 1024-unit LSTM that sees the current game state (extracted from Valve's Bot API ) and emits actions through several possible action heads. Each head has semantic meaning, for example, the number of ticks to delay this action, which action to select, the X or Y coordinate of this action in a grid around the unit, etc. Action heads are computed independently. Interactive demonstration of the observation space and action space used by OpenAI Five. OpenAI Five views the world as a list of 20,000 numbers, and takes an action by emitting a list of 8 enumeration values. Select different actions and targets to understand how OpenAI Five encodes each action, and how it observes the world. The image shows the scene as a human would see it. OpenAI Five can react to missing pieces of state that correlate with what it does see. For example, until recently OpenAI Five's observations did not include shrapnel zones (areas where projectiles rain down on enemies), which humans see on screen. However, we observed OpenAI Five learning to walk out of (though not avoid entering) active shrapnel zones, since it could see its health decreasing. Given a learning algorithm capable of handling long horizons, we still need to explore the environment. Even with our restrictions , there are hundreds of items, dozens of buildings, spells, and unit types, and a long tail of game mechanics to learn about — many of which yield powerful combinations . It's not easy to explore this combinatorially-vast space efficiently. OpenAI Five learns from self-play (starting from random weights), which provides a natural curriculum for exploring the environment. To avoid \"strategy collapse\", the agent trains 80% of its games against itself and the other 20% against its past selves. In the first games, the heroes walk aimlessly around the map. After several hours of training, concepts such as laning , farming , or fighting over mid emerge. After several days, they consistently adopt basic human strategies: attempt to steal Bounty runes from their opponents, walk to their tier one towers to farm, and rotate heroes around the map to gain lane advantage. And with further training, they become proficient at high-level strategies like 5-hero push . In March 2017, our first agent defeated bots but got confused against humans. To force exploration in strategy space, during training (and only during training) we randomized the properties (health, speed, start level, etc.) of the units, and it began beating humans. Later on, when a test player was consistently beating our 1v1 bot, we increased our training randomizations and the test player started to lose. (Our robotics team concurrently applied similar randomization techniques to physical robots to transfer from simulation to the real world.) OpenAI Five uses the randomizations we wrote for our 1v1 bot. It also uses a new \"lane assignment\" one. At the beginning of each training game, we randomly \"assign\" each hero to some subset of lanes and penalize it for straying from those lanes until a randomly-chosen time in the game. Exploration is also helped by a good reward. Our reward consists mostly of metrics humans track to decide how they're doing in the game: net worth, kills, deaths, assists, last hits, and the like. We postprocess each agent's reward by subtracting the other team's average reward to prevent the agents from finding positive-sum situations. We hardcode item and skill builds (originally written for our scripted baseline), and choose which of the builds to use at random. Courier management is also imported from the scripted baseline. OpenAI Five does not contain an explicit communication channel between the heroes' neural networks. Teamwork is controlled by a hyperparameter we dubbed \"team spirit\". Team spirit ranges from 0 to 1, putting a weight on how much each of OpenAI Five's heroes should care about its individual reward function versus the average of the team's reward functions. We anneal its value from 0 to 1 over training. Our system is implemented as a general-purpose RL training system called Rapid, which can be applied to any Gym environment. We've used Rapid to solve other problems at OpenAI, including Competitive Self-Play . The training system is separated into rollout workers, which run a copy of the game and an agent gathering experience, and optimizer nodes, which perform synchronous gradient descent across a fleet of GPUs. The rollout workers sync their experience through Redis to the optimizers. Each experiment also contains workers evaluating the trained agent versus reference agents, as well as monitoring software such as TensorBoard , Sentry , and Grafana . During synchronous gradient descent, each GPU computes a gradient on its part of the batch, and then the gradients are globally averaged. We originally used MPI's allreduce for averaging, but now use our own NCCL2 wrappers that parallelize GPU computations and network data transfer. The latencies for synchronizing 58MB of data (size of OpenAI Five's parameters) across different numbers of GPUs are shown on the right. The latency is low enough to be largely masked by GPU computation which runs in parallel with it. We've implemented Kubernetes, Azure, and GCP backends for Rapid. Thus far OpenAI Five has played (with our restrictions ) versus each of these teams: The April 23rd version of OpenAI Five was the first to beat our scripted baseline. The May 15th version of OpenAI Five was evenly matched versus team 1, winning one game and losing another. The June 6th version of OpenAI Five decisively won all its games versus teams 1-3. We set up informal scrims with teams 4 & 5, expecting to lose soundly, but OpenAI Five won two of its first three games versus both. The teamwork aspect of the bot was just overwhelming. It feels like five selfless players that know a good general strategy. — Blitz We observed that OpenAI Five: Repeatedly sacrificed its own safe lane (top lane for dire; bottom lane for radiant) in exchange for controlling the enemy's safe lane, forcing the fight onto the side that is harder for their opponent to defend. This strategy emerged in the professional scene in the last few years, and is now considered to be the prevailing tactic. Blitz commented that he only learned this after eight years of play, when Team Liquid told him about it. Pushed the transitions from early- to mid-game faster than its opponents. It did this by: (1) setting up successful ganks (when players move around the map to ambush an enemy hero — see animation) when players overextended in their lane, and (2) by grouping up to take towers before the opponents could organize a counterplay. Deviated from current playstyle in a few areas, such as giving support heroes (which usually do not take priority for resources) lots of early experience and gold. OpenAI Five's prioritization allows for its damage to peak sooner and push its advantage harder, winning team fights and capitalizing on mistakes to ensure a fast win. OpenAI Five is given access to the same information as humans, but instantly sees data like positions, healths, and item inventories that humans have to check manually. Our method isn't fundamentally tied to observing state, but just rendering pixels from the game would require thousands of GPUs. OpenAI Five averages around 150-170 actions per minute (and has a theoretical maximum of 450 due to observing every 4th frame). Frame-perfect timing, while possible for skilled players, is trivial for OpenAI Five. OpenAI Five has an average reaction time of 80ms, which is faster than humans. These differences matter most in 1v1 (where our bot had a reaction time of 67ms), but the playing field is relatively equitable as we've seen humans learn from and adapt to the bot. Dozens of professionals used our 1v1 bot for training in the months after last year's TI . According to Blitz, the 1v1 bot has changed the way people think about 1v1s (the bot adopted a fast-paced playstyle, and everyone has now adapted to keep up). Binary rewards can give good performance. Our 1v1 model had a shaped reward, including rewards for last hits, kills, and the like. We ran an experiment where we only rewarded the agent for winning or losing, and it trained an order of magnitude slower and somewhat plateaued in the middle, in contrast to the smooth learning curves we usually see. The experiment ran on 4,500 cores and 16 k80 GPUs, training to the level of semi-pros (70 TrueSkill ) rather than 90 TrueSkill of our best 1v1 bot). Creep blocking can be learned from scratch. For 1v1, we learned creep blocking using traditional RL with a \"creep block\" reward. One of our team members left a 2v2 model training when he went on vacation (proposing to his now wife!), intending to see how much longer training would boost performance. To his surprise, the model had learned to creep block without any special guidance or reward. We're still fixing bugs. The chart shows a training run of the code that defeated amateur players, compared to a version where we simply fixed a number of bugs, such as rare crashes during training, or a bug which resulted in a large negative reward for reaching level 25. It turns out it's possible to beat good humans while still hiding serious bugs! A subset of the OpenAI Dota team, holding the laptop that defeated the world's top professionals at Dota 1v1 at The International last year.* Our team is focused on making our August goal. We don't know if it will be achievable, but we believe that with hard work (and some luck) we have a real shot. This post described a snapshot of our system as of June 6th. We'll release updates along the way to surpassing human performance and write a report on our final system once we complete the project. Please join us on August 5th virtually or in person , when we'll play a team of top players! Our underlying motivation reaches beyond Dota. Real-world AI deployments will need to deal with the challenges raised by Dota which are not reflected in Chess, Go, Atari games, or Mujoco benchmark tasks. Ultimately, we will measure the success of our Dota system in its application to real-world tasks. If you'd like to be part of what comes next, we're hiring ! Mirror match of Necrophos , Sniper , Viper , Crystal Maiden , and Lich No warding No Roshan No invisibility (consumables and relevant items) No summons / illusions No Divine Rapier , Bottle , Quelling Blade , Boots of Travel , Tome of Knowledge , Infused Raindrop 5 invulnerable couriers, no exploiting them by scouting or tanking No Scan The hero set restriction makes the game very different from how Dota is played at world-elite level (i.e. Captains Mode drafting from all 100+ heroes). However, the difference from regular \"public\" games ( All Pick / Random Draft ) is smaller. Most of the restrictions come from remaining aspects of the game we haven't integrated yet. Some restrictions, in particular wards and Roshan, are central components of professional-level play. We're working to add these as soon as possible. You can cite this blog post with the following BibTeX:", "date": "2018-06-25"}
]