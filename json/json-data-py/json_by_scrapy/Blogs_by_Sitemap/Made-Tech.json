[
{"website": "Made-Tech", "title": "Enhancing developer productivity in legacy codebases", "author": [" Scott Edwards"], "link": "https://www.madetech.com/blog/enhancing-developer-productivity-in-legacy-codebases/", "abstract": "Many public sector organisations find it difficult to update and add features to essential services because they are hamstrung by legacy codebases. There are various ways to identify legacy codebases, including the gov.uk guidelines on legacy technology . For the purpose of this blog though, we will define a legacy codebase as one that is not supported by automated tests. This lack of automated tests generally makes a codebase both unsafe to work with and difficult for technical teams to maintain and modify on an ongoing basis. To tackle this issue and enhance developer productivity you should consider modernising the application(s) at the heart of your service so that your technical teams can work safely and effectively on your codebases. Modernising applications to enhance productivity Before you begin the process of modernisation, you need to be aware that it requires investment, and that this investment will need to be justified within your organisation. An important element of this is demonstrating how legacy code affects everyone. If your legacy code results in a slow delivery cadence, this can affect your ability to reach organisational goals, and can also turn software development into an anxiety-inducing process for your team. Most importantly, it can affect your users as they suffer from buggy software that they cannot rely on. If you are part of a public sector organisation where it feels easier to put off modernisation, you are kicking the can down the road at just the wrong time. This is because the Government Digital Service has been mandated by the House of Commons Select Committee for Science and Technology to conduct an audit of all legacy applications across government by no later than December 2020. Before you start a modernisation project, you need to consider the risks you may encounter. Modernisation usually requires some capital expenditure up front, which may mean that you will need to hire consultants, as well as redirect your existing technical teams away from other projects. You should also consider how you will keep teams aligned, how to generate buy-in at all levels of your organisation, and how to identify any technology blockers. You should try to mitigate these risks as much as possible, but also make clear what the rewards are for getting modernisation right. These include reducing your long term operational expenditure by increasing your delivery cadence, and minimising the need for ongoing support. You will also be able to de-risk deployments, reduce defect levels, and increase the stability of your services over time. Finally, it’s worth highlighting how modernisation can improve both job satisfaction and self-esteem levels within your technical teams. Iterative modernisation of legacy applications Once you have decided that application modernisation is a worthwhile undertaking, you have three strategic options for how to accomplish it. These are: iterative modernisation, iterative replacement or a big bang transformation (all of which are examined in detail in our most recent book ‘ Modernising Legacy Applications in the Public Sector ’). If your main aim is to enhance developer productivity, iterative modernisation is probably the best approach to take as it is all about reviving an existing application so it is maintainable, sustainable and able to evolve in the future. It also tends to be the quickest and most cost effective of the three strategies because it builds on the technologies your technical team already knows and allows you to progress towards technical self-sufficiency. It should be noted that this approach is only really feasible if your application is bespoke and you have access to the source code. You should also be able to see a path for the application evolving beyond the initial modernisation stage. If all of these things are in place, there is an opportunity to enhance developer productivity by following the series of iterative modernisation steps below. Set your goals You need to give a lot of thought to which application you choose to modernise first. To do this, you should spend time mapping the applications in your organisation by weighing the technical effort of modernising them against the business value you would gain by doing so. The ideal application to modernise first is one that has high business value but low technical effort. Once you have chosen the application, you should focus on stabilising it by making it both maintainable and supportable. Then, you can look to enhance the application so that modifying it is relatively easy. Finally, it’s important to get your teams onboard with the strategy and empathise with them if they identify concerns about the work involved. Start with an audit This step is about auditing your application in its current state to establish a technical baseline of its components. This involves accessing all of the documentation that you can, as well as interviewing as many past and present technical and operational team members as possible in order to record their knowledge and insights. From a technical perspective, you are aiming to understand as much as possible about the architecture of the application, the code complexity, and any data dependencies or integration points. By covering all these areas you should be able to identify any large-scale technical blockers to modernisation, and to plan a path to your desired state. Implement a test harness In a perfect world, the application you have chosen to modernise would have been built using test-driven development. If it has been, the tests in place should describe the behaviour of the application and alert you to any modifications that change this behaviour. Sadly, this is rarely the case, and so we can focus on using a test harness to get the application to this stage. The test harness consists of a baseline testing framework along with a set of automated tests that set expectations for the existing application behaviour. This test harness creates a safety net that provides you with confidence that you aren’t breaking existing behaviour when you make changes to your application. It also provides the additional benefit of code-level documentation for developers that may work on the project in the future. Replatform the application This is actually an optional step which you may skip over if you only wish to modernise your codebase. If, however, you do want to migrate your application to the cloud, you should consider it as a step within your application modernisation. If executed properly, this step can provide large, ongoing cost-saving benefits, and reduce infrastructure maintenance workloads by outsourcing work to an established cloud provider. If you do replatform the application, focus on automated deployment and automated monitoring in order to enhance developer productivity. Automated deployment involves simplifying the complexity and cadence of releases by doing a lot of work upfront, while monitoring helps technical teams to detect and resolve issues as early as possible. Refactor the application The Agile Alliance describes refactoring as “improving the internal structure of an existing programme’s source code while preserving its external behaviour”. This step allows developers to enhance the structure of an application so that its codebase supports (rather than blocks) future development efforts. Having protected the application with a test harness, your team can start to disentangle any spaghetti code and upgrade old dependencies with confidence. When refactoring, the best approach is to focus on small, ongoing changes rather than on big bang transformations. And always remember, you should keep adding automated tests as you refactor! Add new value At this stage, your technical team will feel empowered to add new value to your applications. They will have a structured codebase to work with and a test harness to rely on, which will allow them to add new features, introduce enhancements, and fix bugs with confidence. It is extremely important that your teams continue using the new tools and processes learnt on this journey. Your developers should keep adding automated tests where necessary to ensure the codebase remains robust, ideally using a test-driven development approach. They can then keep refactoring the codebase with confidence on an ongoing basis. Enhancing developer productivity in your organisation Iterative modernisation is a great way to enhance developer productivity in legacy codebases, and hopefully this article has given you a good idea of the structured approach you should take if you choose to do so. If this approach does appeal, you can find out more about code-level techniques in the following three books: Working Effectively with Legacy Code, by Michael Feathers Refactoring, by Martin Fowler Test-Driven Development by Example, by Kent Beck Alternatively, our latest book, Modernising Legacy Applications in the Public Sector is available to download free or if you work in the Public Sector you can request a free printed copy here . If you would like to find out more about how we can support your organisation with this approach, please feel free to get in touch with us. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-12-02"},
{"website": "Made-Tech", "title": "Academy Week 12: Looking Back", "author": [" Emma Blott"], "link": "https://www.madetech.com/blog/academy-week-12-looking-back/", "abstract": "It’s been twelve weeks since we started the Made Tech Academy as nervous Software Engineer wannabes. It’s strange because it feels like a lifetime ago but also the time has flown by! When we started this journey, things like test-driven development, agile ceremonies, sprint backlogs and clean architecture were new and scary. But we’ve just finished a 5-week project where we designed and created our own web applications using all of these and more. At the same time, we’ve learnt about working together to find the right solution, giving and receiving feedback and picking up new tech stacks on the fly. If someone had told me at the beginning of the Academy that I could do all that, I don’t think I would have believed them! In the last three months, we’ve learnt so much about what it takes to become a software delivery expert. Now going forward into our new lives as Software Engineers at Made Tech, we look back at what drove us to apply in the first place and what we’ve gained from the experience. Q: Why did you apply to the Academy? CHLOE: I really wanted to work in software engineering but I hadn’t done much coding before so the Academy seemed like a great opportunity because no prior experience is needed. A big factor was also that Made Tech works with the public sector and there’s a sense that with the projects you do you are really making a difference in people’s lives and that was really important to me. BELLA: I applied to the Academy because it was really encouraging to see that they were specifically looking for applicants with no commercial experience but who were passionate about coding. I saw a posting in my local coding group so I thought ‘hey, why not give it a try?’ CLAIRE: I applied to the Academy because I wanted to change careers into software development. I was also interested in the projects Made Tech have done and the Academy seemed like a great opportunity to develop my knowledge. DEREK: The reason why I applied for the Academy was because I wanted something that was going to help foster my learning and development as a developer. I have gone down the self-taught path and I didn’t feel like I was completely ready to apply for a ‘proper’ developer role. I saw the Academy as a place where I could enhance those technical skills and technical knowledge to get me to that place. DUNCAN: I applied for the Academy because it was a chance to learn practices used in the industry before joining live projects. Also because Made Tech puts a strong emphasis on continuous learning not just during the Academy, which is important to me as it was my first job in tech. EMMA: I applied to the Academy because I’ve always been interested in software development and I’ve done a lot of programming whenever I’ve had the chance. I would try to teach myself how to do the next thing and I wanted to transfer into the tech industry but I was struggling to bridge the gap between self-taught and the software industry. The Academy sounded perfect because that’s exactly what it taught. RICHARD: The reason I applied to the Academy was I was looking for an opportunity to move into software development. I really liked the fact that the Academy meant there would be a big group of us starting from the same sort of point. I liked Made Tech’s mission statement and the projects it was involved with. ZACK: I applied to the Academy because I wanted to be a software professional. I liked the content, I liked the focus on good software design, engineering practices and meeting user needs and not so much about following the latest fads or fashions in technology. Although we do use the latest stuff as well. Q: How is the Academy benefiting you? CHLOE: The Academy has hugely boosted my confidence with coding, writing clean code, running retrospectives, pair programming, and so much more. I’m also feeling a lot happier in general because I’m doing a job that I love and it doesn’t even feel like a job, at least for now. BELLA: Before I joined the Academy, my primary experience with coding was doing front-end web development. I didn’t really know how to work in a team, what constituted good code, whether my applications were well organised or even what the point of testing was, among other things. The Academy has taught me the immense value of all these things and has encouraged good coding practices. CLAIRE: I’ve learnt a lot from the Academy already and it’s been great learning alongside the other Academy engineers. It means there is always someone to bounce ideas around with. DUNCAN: I’m learning new ways of programming and implementing them immediately in projects, which helps to consolidate my knowledge. I feel that I am really benefiting from both teaming up with other members of the Academy and receiving guidance from experienced engineers. EMMA: I’m finding the Academy really beneficial. There were a lot of gaps in my knowledge when it came to how to write scalable software, software that can be maintained. I always just wrote hacky scripts to do what I wanted and would often have issues later on. Learning practices like TDD and clean architecture have been really beneficial and will really help me going forward. RICHARD: I feel like the Academy is benefiting me because I’m learning new programming skills, I’m working collaboratively, which is something I hadn’t done a huge amount before with software, which is great. Also learning how to deliver projects, not just code. They’re all going to be really important things going forward. ZACK: I’m really happy now! I can’t believe I get paid to learn alongside fantastically intelligent and talented people. Every morning I wake up raring to go. Q: How are you finding the Academy? CHLOE: It’s great! The Academy is basically like uni, but normal working hours and you get paid. The atmosphere is so supportive and everyone is willing to help which makes it the perfect place to learn. Even though it is virtual, we’ve gotten to know each other really well because we’re chatting all the time and we also like to play fun games with each other. BELLA: The progress I’ve made in the six weeks I’ve been with the Academy has been immense. Not only in terms of technical know-how, but my own approach to working with others. My colleagues are some of the most positive and approachable people I’ve worked with. There is a huge culture of feedback here that really helps us grow. I couldn’t imagine a better start in software engineering than at Made Tech. CLAIRE: I’m really enjoying the Academy so far. Every day feels like we have something new and interesting to explore. Clare and Charlene , our Academy mentors, have been amazing. DUNCAN: It’s been a really great experience as although we are learning so much, there is a relaxed atmosphere where everyone is helpful and we’re focused on the learning process rather than worrying about the outcomes. EMMA: I’m finding the Academy really fun. It’s really great to work with other Engineers. I’ve only ever really done software stuff by myself. It’s been really great to hear from other people’s perspectives and do pair-programming. Solve problems together, mobbing, katas, that sort of thing. That has been really great. Also, just learning loads and having it all broken down for me because it is very easy to get bamboozled by all the words in the software development cycle. JOANNA: I’m loving the Academy so far. Everyone is so friendly and willing to help. I’ve learnt more in six weeks than in six months of learning to code by myself. Not just about how to write clean and efficient code, but also about working in a team, giving feedback, and how coding an unbeatable AI in tic tac toe is a lot harder than it sounds. RICHARD: I’m really enjoying the Academy. It’s great fun. It’s really supportive with the other people who are in the Academy and also the wider Made Tech community. All the Senior Engineers and Lead Engineers I have spoken to have been really helpful and more than willing to help us progress which has been absolutely brilliant. ZACK: It’s great! I like that there’s social interaction. We’re not just alone programming at home. We spend a lot of time pair programming and comparing our work. The remote party games that we’ve been playing fairly regularly are pretty good fun too. It’s all good. BEN: So far, the Academy has been a very enjoyable experience. What I find most exciting is that we get to work on our own small projects and integrate with the wider Made Tech team which really helps accelerate our growth. Made Tech puts a strong focus on personal development as well as feedback, which I think is a really important foundation to build. Looking ahead The Academy has been a great learning experience for all of us, the skills we’ve gained will be useful for years to come. It will be a big change to not log in every morning and see everyone’s smiling faces, ready to play a team game or tackle a new challenge together. The next chapter is going to be just as exciting, if not a little scary, but I reckon the Academy has given us the tools we need to tackle anything that’s thrown at us. I think we’re all raring to go! ? Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-11-25"},
{"website": "Made-Tech", "title": "Why public sector organisations must prioritise technology skills enablement", "author": [" Duncan Bell"], "link": "https://www.madetech.com/blog/why-public-sector-organisations-must-prioritise-technology-skills-enablement/", "abstract": "Developing and nurturing the technical skills of your employees is one of the biggest challenges facing public and private sector organisations throughout the world. There’s no denying that a technical skills shortage is driving this trend. However, it’s important to also recognise that learning and development isn’t just about allowing an organisation to reach its goals. Employees want to continue learning throughout their careers and will migrate towards those organisations that enable this. Public sector organisations face a unique set of challenges when it comes to technology skills enablement. They realise they must access the skills required to deliver user-centred digital services that citizens expect, while also battling with much wealthier private organisations over a limited pool of technical talent. In the first of a series of blogs covering how you can succeed at internal capacity building, I will look at the most important factors pushing technology skills enablement to the top of many organisations’ lists of priorities. In my next blog, I will explain how you can act on these dynamics to ensure skills development becomes an important outcome for your team. Finally, my colleague Rich will follow up with some practical tips for establishing learning as a strategic goal within your organisation. Developing skills will benefit organisations and individuals When training in the workplace comes up in conversation, I can’t help but refer back to the famous “What if we train them and they leave? What if we don’t and they stay?” cartoon. I have worked in organisations where training is seen as an investment and where they even had training course repayment clauses tied into contracts. With this in mind, let’s address the big issue straight away – every organisation knows that recruitment costs a lot! Recruiters, job advertisements and time spent doing interviews are all expensive. On the other hand, failing to recruit fresh talent can also be expensive if unproductive employees start to hold an organisation back. Employees often become less productive if they are unhappy, unsatisfied or are just struggling in their job. Also, employees that feel out of their depth or unsupported in their role can succumb to stress and burnout. Ultimately, organisations must realise that both retaining unskilled staff and having a high staff turnover will negatively affect their ability to achieve strategic goals. Furthermore, they shouldn’t only worry about skills development from the organisation’s perspective. According to the Microsoft Practice Development Playbook: Recruit, Hire, Onboard and Retain Talent , personal development is currently one of the top qualities prospective staff look for when job hunting. Hiring new recruits that love to learn is important not only because it will help to ensure they remain effective and productive throughout their employment. It also means they are more likely to support and mentor others, sharing their knowledge and experiences. If you fail to hire individuals who are interested in improving or learning a range of skills, this can negatively affect your organisation’s culture. An organisation that becomes too weighted towards senior staff who are labelled as ‘experts’ can suffer when these individuals are asked to participate in group sessions or pairing arrangements. Fear of showing weakness, of being seen as a failure or of not knowing something they should are all negative behaviours that you do not want your junior employees to inherit. This is why it is important for your teams to be open to learning from each other, whether that’s the most junior or the most experienced member of a team. Being able to admit that you don’t and can’t know everything but are willing to learn from others, is an important part of personal growth that new entrants to the industry should be made aware of. The technical skills shortage cannot be ignored Apart from reducing recruitment costs and encouraging an inclusive culture, establishing a learning and development programme can also enable you to fill the technical skills shortages that are affecting all organisations right now. This skills shortage is getting worse because of the incredible pace at which the technology and cloud computing industries are moving. Both the private and public sectors are moving applications and platforms from a traditional self-managed data centre model to the Cloud, resulting in a huge technical skills shortage that has been worsening for years. Although some of these positions can be backfilled from school and university leavers, there is simply not enough supply for the demand. Several reports have been produced that draw attention to this issue. The Open University’s 2019 Bridging the Digital Divide report highlights that 88% of organisations admit they have a shortage of digital skills, which is already having a significant negative impact on productivity, efficiency and competitiveness.  Microsoft’s Unlocking Potential report finds that 69% of UK leaders believe their organisation has a digital skills gap. These really are eye opening figures which can only be reduced through retraining and skills enablement in the workplace. For public sector organisations, the technology skills shortage is a particularly tricky problem. Firstly, the need to migrate and update legacy systems has increased with pressure from GDS (Government Digital Services), The Government Cyber Security Group and The National Security Centre. Secondly, public sector organisations simply cannot compete with high-profit industries in the private sector, such as gaming, gambling, social media, fintech and big Cloud providers, who offer much higher financial rewards and benefits packages. It’s also worth noting that, when a public sector organisation does recruit a Computer Science graduate, even the most capable ones will still need to be upskilled in some areas. That’s because Computer Science degrees do not necessarily demonstrate someone’s ability to work well within a team, give and receive feedback, share ideas, work collaboratively or mentor others. One potential solution is to hire from outside the tech industry and look at the different experiences people have gained in their past. Hiring enthusiastic problem solvers who want to learn from diverse backgrounds and under-represented groups is one way of tackling the issue, and there’s a huge pool of untapped talent in part-time workers. However, organisations also need to think about how this fits into a more wide-ranging technology skills enablement strategy. Nurturing talent is the best long-term approach Even the most affluent organisations are finding it difficult to buy their way out of the technical skills shortage, which is why companies like Microsoft are using apprenticeship programmes to ensure they have a quality pipeline of engineers to rely on. When you consider that public sector organisations have far less resources to lure in experienced engineers than the biggest tech companies, the introduction of apprenticeship programmes makes a lot of sense. In fact, nurturing talent internally should be a key organisational outcome, as long as you can measure its effectiveness. The Public Sector Apprenticeships Target states that 2.3% of new starters should come from apprenticeships as a metric for success. Having a percentage figure for apprenticeships or internally trained talent vs externally hired new starters is a valuable metric but it doesn’t paint a full picture of whether a programme is working or not. You must ensure that other measures are used, goals are established and assessments are implemented, so that your organisation and your apprentices are getting the most out of the experience. Our approach to nurturing talent has resulted in the development of the Made Tech Academy , a 12 week assessment programme based on SFIA guidance that ensures our graduates are ready to work within a delivery team once they complete the course. One of the key aspects of this process is teaching the Academy Engineers how to monitor their own progress and measure it against the SFIA Level 1 framework. They set their own goals and objectives, and are encouraged to gather feedback and evidence their progress. Teaching candidates how to manage and be responsible for their paths from the start of their careers is a priceless skill which enables them to progress in the right direction and achieve the promotions they deserve. Implementing change within your team In this blog post, I have outlined a variety of reasons why public sector organisations need to prioritise technology skills enablement in order to meet their strategic goals and ensure they protect themselves against a technical skills shortage. I have touched on the role apprenticeships can play in solving this problem and, in my next blog, will delve deeper into technology skills enablement processes by explaining how learning and development can become a team outcome. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-01-20"},
{"website": "Made-Tech", "title": "In digital projects, only the fastest survive", "author": [" Andreas England"], "link": "https://www.madetech.com/blog/in-digital-projects-only-the-fastest-survive/", "abstract": "Despite the evolution of digital engineering in the 50+* years since the first user interface for a computer system was commissioned, we tend to still believe that we must get everything right the first time. Anything less is regarded as a failure. This situation immediately puts the team working to this goal under pressure to deliver to undefined expectations, and their success (or level of failure) will be measured using the broadest of measurements that are incapable of providing any direction or feedback. The team is then caught in a downward spiral of spending time, resources and sanity developing a product that they are not certain will be fit for purpose. Inevitably they burn out, morale plummets and eventually the project dwindles. Traditional IT procurement and delivery strategy appears to still suffer from misdirection, if not outright waste of resources. Coupled with a high chance of delivering a product that’s not fit for purpose. IT news sites provide a shameful list of projects that have failed. Abandoned £10bn NHS IT system , £2.6bn Sainsbury’s warehouse automation failure , The UK’s worst public sector IT disasters , to mention a few. To an outsider, we appear to be members of an industry that has ways of working that are inefficient and run a high chance of failure so deeply ingrained that we regard modifying our behaviour with suspicion. In this post, I’m going to write about how my work as a proponent of User Centred Design, hypothesis-driven development and fast feedback cycles can be used to enable digital projects to be delivered quickly, and respond to failure without breaking stride. *In June 1964, NASA Commissioned the computer hardware and software that would enable the Apollo astronauts to control their spacecraft. User Centred Design (UCD) at speed My area of expertise is ‘User Centred Design’, and with a role of ‘Head of Product’, my remit is: Ensure that we identify as early as possible the organisation and its users needs with a reasonable level of confidence Translate those needs into a strategy that the development team can use as a plan for production Monitor the production to ensure that the organisation and its users’ needs are not lost in translation and represented by the project’s ‘definition of done’. The first issue I face is that I enable the team to have ‘a reasonable level of confidence’. This is not a certainty, nor is it an assumption, or hunch. I have Kev Murray’s & David Draper’s presentation at the Manchester NUXUK presentation in August 2016 to thank for bringing a way of working that’s formed the basis of how I can work with ‘a reasonable level of confidence’. Working with hypothesis Let’s suppose that I have the collated outcomes from a project’s user research, hopefully this is a mixture of qualitative and quantitative knowledge, that’s been formatted so that the project team can understand the user needs. I personally like to use an archetype with quadrants: Name and sketch, who they are, what they do (within the service), and why they’re motivated to do this. With the archetypes defined, it’s reasonable to ask a small group of domain experts (usually the project team) to define the goals, or discrete tasks that the users should (reasonably) be able to achieve within the service. With the user goals identified, we can prioritise these against predefined organisation’s goals (usually defined as an organisation’s vision or North star). Now we have a prioritised backlog that we can slice into our delivery method of choice (Scrum, sprint or kanban mashup for instance). However, we must remember that we’re working at two, if not three levels of abstraction from the users and their needs. A mistake could have been introduced at any point during this process, and for us to be true to the agile mantra ‘people over process’ (which I believe regards users and developers as people!), then we need a mechanism to ensure the team is working in the right direction. This is where hypothesis-based design comes in. There are many variants of formal hypothesis statements, I’ll describe the one that works for me. I was introduced to hypothesis statements by Jeff Gothelf at NUX6 in Manchester , and refined by Ben Holiday (He added the final statement that introduces measurement – We’ll know we’re right when we see this evidence ). We believe that: Meeting this user need With these features Will create this business outcome We’ll know we’re right when we see this evidence What a hypothesis statement gives the team is a neatly defined set of criteria that describe what we build should be able to do, how it will benefit the user, and with the closing statement, how we’ll know if we’ve built the right thing. Now imagine I’ve persuaded a delivery manager to allow me to use the team’s time and effort to produce hypothesis statements for the most pressing (highest on the backlog) user needs, now what? I need the fastest way of validating our hypothesis, which is by building a minimal viable product, and testing it. Iterative design through MVP (the real thing, not the corner cutting cynical version that you’ve seen before) Every time I present the agile delivery process, I show a slide with Henrik Kniberg’s skateboard to car diagram . Everyone nods and agrees, yes this is how we should deliver software. Then we continue with the project, and the ingrained desire to ‘just get our heads down and ship stuff’ takes over, and we end up shipping with a mindset that ‘ once it’s made, it’s done, and we can move on ’. This is the antithesis of what Henrik was describing. MVP’s teamed with hypothesis statements are a strategy whereby we build the quickest, cheapest, and dirtiest thing possible that enables us to validate our hypothesis. If it works, then we’re able to dedicate more resources to building a more refined version with a level of increased confidence. However, if it fails, we learn why, using the hypothesis statement as a grounding point, and build another MVP to repeat the experiment. To be able to work like this, the team must be capable of working together, pivoting quickly, communicating at all levels, and having an all-round level of agility that is only possible when they are a high performing team . More speed! Once a team embraces a hypothesis way of working, we find that we’ve started a repeatable process that’s capable of continuously delivering output with validation baked in. Therefore, is the only limit to the team its overall capacity? Unfortunately this is where waste, unevenness and overburdening come into play. These are the 3 major blockers as described in classic Lean/Kanban practice. In my experience, and thankfully backed up by Google’s state of dev-ops report 2019 , we can see that wherever wastage is reduced, we reduce the risk of a project’s failure. Rephrasing the Google dev-ops report to my needs: If we can increase software deployment frequency so that we are releasing multiple times per day, not only are we able to quickly validate our hypothesis, we are also demonstrating our direction of travel to the project stakeholders. If we can reduce the lead time from a user need being identified to an experiment being live on the service, we are able to respond to emergent needs, and (again) experiment efficiently. With a release methodology/pipeline in place like this, we’re able to help a delivery team move at pace, and enjoy the benefits of rapid delivery. A winning process I’ve described a project process with an inbuilt method for enabling us to have confidence in what we build and a strategy for iterating until we get there (and knowing both where ‘there’ is, as well as being able to measure whether we’ve arrived). If the team is coding using a TDD methodology (Test-Driven Development), or BDD methodology (Behavior-Driven Development), then we’re in a strong position to get the whole team designing from user research, through to writing the tests and the code that are the core of the project. This way of using hypothesis as the vehicle for describing needs and measurable success criteria is also valuable when working with services that have no direct users. In this scenario we can treat the business layer/logic, API, Interface as a service user with needs and measurements for success (response rate etc.). Delivering at speed However, we’ll raise our stakeholders’ level of stress if we don’t deliver at speed. If we’re seen to be ‘experimenting’ (which we are), without constantly moving towards the ultimate goal of meeting the organisation and its users goals, then we can erode the trust placed in us. Thankfully a fast pace is achievable if a few rules are in place: The team’s methodology is transparent and has buy-in from the project stakeholders. The organisation’s goals and a clear direction of travel towards them are agreed by the stakeholders. Stakeholders are obliged to give direction and feedback as quickly as possible. The team is happy. This is a single KPI (Key Performance Indicator) that embodies all the qualities that define a high-performance team. If your team has friction within it, it needs fixing now. Managers are servants to the creators. As a manager, your role is to make sure that anything that would block or delay the work of a researcher, developer, experience designer, etc. is managed, mitigated or at least deflected from your team. Teams are cross-functional. If your team is missing an interaction designer, or database engineer, then your pace will be lower than it could be. I’ve documented (admittedly at a very high level) how we approach uncertainty in digital projects, enable a team to rationalise the unknown, manage it and maintain their agility to respond when things don’t work. Coupling this with a methodology that integrates into the way code is written and tested, and with a clear measurement for determining when we’ve been successful, means that we’re ‘preparing for success’, and have a feedback loop that identifies failure as quickly as possible. Finally, with a rapid release/test cycle, we’re able to reduce wastage through delay and unevenness in our production cycle. Though I’ve written about a complete end-to-end process, please don’t be under the impression that this is a one-size-fits-all solution. All organisations are different, as are all projects. How Made Tech designs your project will be completely bespoke and respectful of your organisation’s culture. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-12-09"},
{"website": "Made-Tech", "title": "Made Tech Team Interview: Lewis Dale, Senior Software Engineer", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/made-tech-team-interview-lewis-dale-senior-software-engineer/", "abstract": "This month we continued our Made Tech team interview series with our Senior Engineer Lewis Dale to better understand his role and to feature the great work he has been doing. Our Senior Software Engineers deliver digital, data and technology outcomes that improve society. They do this by delivering and architecting software, and coaching others to do so in public sector organisations. If you would like to watch the full interview, it is available here . Q: How did you become interested in tech? A: It actually started when I was about 12 years old and I was a member of an online forum. It was eBay for kids basically and I realised ‘oh, I want to do something like this’. So I learnt how to set up one on my own, how to host, did the HTML, and learnt a bit of PHP. That kind of snowballed into learning how to code. Then I went off to university and got a Master’s degree in Software Engineering and now I’m here. Q: Outside of tech, what other hobbies do you have? A: I’m a big reader. I love reading sci-fi novels and horror novels. I’m reading Turn of the Screw by Henry James at the minute which is dense but I’m enjoying it. Q: Before you joined Made Tech, had you ever worked in the public sector before? A: I had a brief stint in the aerospace sector which involved a European Space Agency and the UK Space Agency, so I had some exposure to the public sector but not to the same extent Made Tech does. Q: How do you find working in the public sector? A: It’s good. You get some really, really interesting projects and the thing I like most about it is being able to do good. Having an impact on the way technology works in the public sector is nice. Q: How long have you been at Made Tech? A: A year next week. Q: How did you find out about the company? A: I actually found out because the Talent Lead at the time got in touch with me over a hiring website I had signed up for and about a day later I got a message from them telling me about the company. I got a look at the Handbook and it all looked really great so I sent over my CV and had an interview. Q: What attracted you to Made Tech? A: There were three major things I think: 1. There was the openness, so having the Handbook in the open was a big, big thing for me because I like working at places that value transparency. 2. Learn Tech . Sort of having that investment time in learning is something that I was really keen on doing. It was something that I’ve not had the chance to do at other companies and I like the opportunity to grow. 3. Having the Academy . I thought that was a great idea to be able to invest in new engineers. I was really impressed by that so it made me want to come on board. Q: What was the interview process like? A: It was quite a painless process. I think I sent off my CV to the Talent Lead at the time who took about 3 or 4 days to get back to me. Then they offered me a phone interview which I had with one of the Lead Engineers . It was a short 30 minute phone interview which checked that we were aligned and that it felt like a good fit. Then it was a 90 minute face-to-face interview, back when we had face-to-face interviews, where it was 30 minutes with two hiring managers having a chat about my work history and what got me into tech and stuff like that. 30 minute pairing exercise with a Lead Engineer and then a 30 minute more technical discussion where we did stuff like an architectural question. Q: What is a day in the life of your role like? A: That’s quite a hard one to answer really because it can vary quite a lot but typically as a Senior Engineer I’m involved in a single workstream on a project. So I’ll be working directly with one of Made Tech’s clients on a day to day basis. We’ll have stand up then the rest of the day we’ll pick up work. At the minute we’re doing a lot of mobbing so the whole development team is all on one call and we’re all trying to work through a problem together. Which is quite nice because it’s nice to pool ideas and everyone sort of gets the information propagated at the same time. That’s been my days recently anyway. Q: What do you like most about being a Senior Engineer? A: I like working on problems and writing code, that’s obviously why I became an Engineer. But I also like having people there who I can mentor and work with. Seeing people who have come in at a more junior level and then working with them and watching their knowledge and abilities really come out is really great. Having a role in that is probably my favourite part about being senior. Q: What are some of the most challenging things about this role? A: Obviously the work can be challenging. It’s mentally taxing to write. You’re given a specification from a customer, you’re given a set of requirements, and turning those into a finished feature. There’s things you have to do there and a big part of that is having the conversations with the client and drawing out ‘okay, so this is what you’ve said and how does that translate to a finished product? How do we verify that that’s the case?’  Having those conversations can be really challenging but really rewarding when it’s done. Q: What has been one of your favourite projects you have worked on? A: At Made Tech the one I’m on right now I think is one of my favourites because it’s such a different challenge. So we’re trying a digital transformation for a critical infrastructure within the UK government and we’re taking that and putting it into the cloud. Then we’re going to start modernising it. To be directly involved with improving the technology and the infrastructure of the government is a big deal. I really like it. Q: Do you have any advice for anyone who wants to be a Senior Software Engineer? A: It’s almost coming in as a junior I think. Just put as much time as you can to invest in learning. Do different projects, learn various different programming languages, making yourself more of a generalist helps you understand how to come at problems with a completely different approach. Invest in the learning I think. Q: What has been one of the biggest challenges you’ve had to face since lockdown started in this position and how did you overcome it? A: The biggest challenge I found was that we were lacking the personal interactions that we had before lockdown. Previously we used to commute to the client office and there were times there were four of us in a car and we’d have an hour of conversation. It was quite nice to kind of bond and overnight we sort of lost that and we weren’t in the office anymore. A couple of things we’ve done to sort of combat that were we had an open Google Meet room at all times. So we would go in there and you could just sit there and work away and people could join you. You could have off the cuff discussions and it was an attempt to recreate the sort of office environment. Then we’ve had more structured social sessions where we’ve had a few hours at the end of the day and we’d go grab a drink and food and sit on the call, play some video games, or do something together and just socialise in a way we haven’t been able to do in quite a while. Q: What do you like most about working at Made Tech? A: I think it’s definitely the people. The people that work for Made Tech seem to be consistently the nicest people I’ve ever worked with. It’s a really nice environment. It feels very safe. The concept of no stupid questions and things like that, that’s really baked into the way people are at Made Tech and that’s what I love about it. If you have any more questions for Lewis about his role here, you can get in touch by reaching out on Linkedin . Additionally, if you are interested in joining our team, you can view our current open positions here . Be sure to stay tuned for our next Made Tech Team Interview coming next month. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-11-18"},
{"website": "Made-Tech", "title": "Why learning is a strategic goal", "author": [" Richard Race"], "link": "https://www.madetech.com/blog/why-learning-is-a-strategic-goal/", "abstract": "This is the final blog post in a series about how to set up your organisation for success when it comes to capability building internally. Duncan has previously discussed prioritising skills enablement in the public sector and how to ensure skills development becomes a team outcome . In this post I’ll look into why learning is a strategic goal by exploring what we need to consider, why it’s a hard balancing act between individuals and the organisation’s needs and what questions we need to ask. We have previously spoken in depth about why skills enablement is important and how organisations can upskill their technical teams effectively but how do we go from “this is a good idea” to actively practicing it? What are some of the lessons we’ve learnt and what are the important questions to ask before we embark on this journey? And how do we keep checking in to ensure we’re still being effective? What is driving skills enablement in your organisation? Let’s start by asking “where is the need for skills enablement coming from?” If we cannot answer this question, it will be hard to ensure we’re able to give value to both the individual and the organisation. Addressing the elephant in the room, all staff are there to provide value to their organisation. Whether this is indirect or direct could only be measured by people on the ground. So if the drive of adding skills enablement is coming from the staff, we’d need to explore what their requirements or wants are. We need to get a clear idea of this to see how it aligns to the organisation’s strategy and current/future service(s). This is why it’s so important to measure which skills we are developing as an organisation. Are they serving a need? An example of an indirect value would be where staff happiness is a key metric and this is a way to achieve it. On the flip side, if there is a need for the organisation to move from one technology to another, is there buy-in from the staff? How does this fit in with the current technology landscape and where the organisation wants to be? Having open conversations with staff and allowing them to feedback on direction on technology changes will be vital. People will not buy into upskilling if they believe their current role will not exist in the future. Ultimately, there will be some people who are unable to change their skillset or have no desire to change. Do you have a plan for these situations? It’s often hard to balance the needs of individuals and the organisation. You’d need to have an open dialogue to have these frank conversations. Having a safe environment with a healthy feedback culture should be a prerequisite if you believe the skillset change will be disruptive. Setting your strategic goal I’ve talked about the motivation for why we want to add skills enablement to your organisation, so let’s explore this in more detail. For example, let’s imagine that our technology leadership has identified that our current programming language is not quick enough for our service. We need to move some of our core components to a more efficient programming language. They’ve evaluated the options available and have a Proof of Concept working. Making this change offers us a low risk of failure for a lot of value. Our main issue is that our engineers do not know this language. We have a need to upskill our staff, driven by business needs. We now can derive an immediate goal: to enable support and feature development of critical components of our service. We can measure success by tracking the number of engineers who would be able to work on this component. A problem arises when this specific goal is then applied to progression via line management. We should be giving everyone the same opportunities to progress. If not everyone is able to work towards this new solution, what other learning outcomes can we use? Resentment could also be introduced to the wider technology team. We can use this catalyst for introducing skills enablement throughout the team and wider company. We should be looking at what skills all people can be working towards, rather than just the specific immediate needs. This is where the strategy of the organisation should include continuous learning as a goal. We want to ensure that staff have the opportunity to grow and develop. Another key aspect is that we should not stop learning even if we achieve this particular goal. If our current technology leadership changes, do we have these skills that current leadership has demonstrated? What would happen if a similar problem presents itself in the future without those people? What if we need a new service or a component in the future? How can we be sure we’ll pick the right tool (programming language, framework etc), rather than carrying on using “what we know” and running into the same problems over and over again? Weighing organisational benefits vs individual benefits How do we ensure our organisation’s strategy is aligned with upskilling staff? A straightforward example would be where the strategy is focused around removing legacy services and replacing them with modern solutions. Hopefully, the strategy would clearly explain what steps are required to make this happen. It should also include how we’d like to ensure the staff, who have lots of indispensable domain knowledge, are able to carry out this journey and stay on to contribute to the future of the service. The harder example is where the staff want time to dedicate to learning without an obvious benefit to the organisation. How do we reflect this in our strategy? This ties back to what our goals are and to measure them. For example, a strategic goal might be to ensure we have in-house technology leadership at all levels. We’d then need to ensure that we have an environment where junior and mid-level engineers have the space to learn, and senior engineers have the opportunity to develop T-shaped skills so that they can excel in leadership roles. Sometimes, our day-to-day responsibilities do not offer these opportunities. So it’s important we have staff who want to grow, as Duncan mentioned in his post about hiring , so we don’t stand still in this ever evolving world of technology. What we’ve learnt at Made Tech Here at Made Tech we value continuous improvement at all levels. We spend every Friday afternoon learning, this is for all roles not just technology focused roles. We’ve gone through different iterations of how we organise our learning sessions. We’re lucky that we’re generalists and need to learn and know lots of different subjects, due to the nature of what we work on. This allows us to have more flexibility in what we can do and learn. Because of this, whenever we integrate with another team we look at opportunities for where we can upskill any members of the team. We can tap into our collective experience to help and ensure we’re delivering value to the user. Putting together a comprehensive plan for how to invest in your staff is challenging. Hopefully, by asking yourself some of the questions in this post you will get an idea of how to approach this problem, and some of the pitfalls to be aware of. As long as you are willing to have an open conversation, with a good feedback loop, most of the potential issues will be addressed before they become a problem. Iteration and continuous improvement are both critical parts of the process. Hopefully once you’ve set up your learning culture you should see the benefits by measuring learning opportunities and outcomes across all parts of the organisation. Our webinar on how to build a learning culture will help anyone that is starting to prioritise technology skills enablement in their organisation. It covers how you can keep the skills and knowledge of all your employees up to date, and includes many of the things we’ve learnt from helping public sector organisations like the Legal Aid Agency to upskill their teams effectively . Alternatively, if your need is more urgent, you can get in touch for a chat right now. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-02-03"},
{"website": "Made-Tech", "title": "Made Tech Team Interview: Matthew Leak, Principal Technologist", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/made-tech-team-interview-matthew-leak-principal-technologist/", "abstract": "This month we continued our Made Tech team interview series with our Principal Technologist Matthew Leak to better understand his role and to feature the great work he has been doing. Our Principal Technologists enable public sector organisations to better use technology in order to improve society. They do this by building and managing strategic relationships, leading accounts, finding new opportunities and advising as a senior technology leader. If you would like to watch the full interview, it is available here . Q: How did you become interested in tech? A: When I was about 10 or 11 years old I got my first computer and one of the first things I did was to tear it apart, see what was on the inside, and I got fascinated with the inner workings of the machine. Then a couple years on from that, I got into website development and was using the internet a lot more. Seeing these websites online, how were they made, looking at the source code and building my own websites from there. I built all sorts of fan-type pages for games and things that were current at that time. The journey just accelerated from there. I’m completely self-taught so I’ve never been to university or studied anything like computer science. It’s just born out of curiosity and that’s where the passion came from. Q: Outside of tech, what other hobbies do you have? A: I try to keep fit and healthy. Being sat down all day isn’t necessarily great for your body so I try to do a lot of swimming, although I can’t at the minute because of lockdown which is quite frustrating. Otherwise, I do a bit of yoga and of course going to the pub is also a hobby. Q: Before you joined Made Tech, had you ever worked in the public sector before? A: I worked on a public sector engagement for a consultancy I was working for prior to joining Made Tech. It was then that I was engrossed in the public sector ways of working. Q: How do you find working in the public sector? A: I love it. It’s great. It’s a completely different set of challenges to what you typically face in the private sector. There’s almost a stigma attached to public sector working in that it’s slow paced, the government is left behind, or you’re not going to be working with cool technology. There is an element of truth to that but the majority of the work that we’re doing now at Made Tech is focused on digital transformation. So taking stuff that isn’t cool, fun or exciting and bringing it into the modern world using new technologies which are better and more sustainable for those organisations that we’re working with. The challenges and projects that you get to work on as a result of being in the public sector are really exciting. Q: How long have you been at Made Tech? A: I’ve been at Made Tech for about 10 months now. It’s going so fast! I joined during lockdown and that was a very strange thing to do – to move from one company to another without actually meeting anybody in person. I got a Macbook delivered in the post and joined a video call on day one. I’m really enjoying it so far. Q: What attracted you to Made Tech? A: Having worked on the previous public sector engagements, I knew the type of work that was going to be involved. I had a few conversations with Luke Morton , our CTO, about the company and why Made Tech exists in the first place, its mission and vision and I really bought into that vision. As a result, here I am. Q: What does your role here involve? A: Typically, I work with senior leaders in the organisations that we go into to align technical vision with business goals and strategy. Occasionally I get my hands dirty developing proof of concepts to introduce new technologies into the business, introduce architecture principles, and things like that. Alongside client work, I do work to support the business which involves a lot of bid work. With it being the public sector, a lot of the work that we win is through RFPs and the like, so getting my hands dirty working with Ian Southward and facing the challenges and pressures that come with those tight deadlines. We’re helping support Made Tech by landing some of those contracts. I also get involved with recruitment and building out our engineering team. It’s quite a diverse role really. Q: What do you like most about being a Principal Technologist? A: I think for me taking companies on that journey of transformation is hugely rewarding. When you start the engagement you typically do an as is versus to be state analysis and then at the end of the engagement you do a retrospective. But when you’re actually working on the day-to-day within the project, it’s hard to see the journey that you’re going along. When you do the retrospective at the end of the project and you get to see how far you’ve brought this organisation on that journey of change and the benefits that they’re then going to reap as a result, that is probably the most rewarding thing for me. Q: What are some of the most challenging things about this role? A: Organisations don’t pay consultancies a lot of money to solve easy problems, they pay them to solve complex and challenging problems. So projects can be bad for a whole host of reasons. For example, tight deadlines, unreasonable demands, not enough resources, the list goes on. If you work in this industry long enough operating as a consultant you’re guaranteed to experience at least one of those at some point, so I think the most challenging thing is knowing how to navigate those types of situations and how to get out and see the other side. Q: What has been one of your favourite projects you have worked on? A: For me, the most recent project is always the favourite. Every new project you go on is like your baby that you then nurture and take on the journey, so it’s very hard to say what my favourite project is. I think why the engagement that I’m on is probably my current favourite is because it’s really helping align a whole team of engineers to a set of principles. It’s allowing them to adopt better ways of working, to see the current state of technology that’s available to them and bringing them on that journey of change. The stuff I alluded to previously on what I really like about this role, this particular project I’m working on now has a lot of that in it. Q: Do you have any advice for anyone who wants to be a Principal Technologist? A: This is probably down to more junior members, but it’s not something that can be rushed, it’s something that comes with experience. Getting some consultancy experience is going to be hugely advantageous to getting yourself into the Principal Technologist type role. Due to the nature of consultancy work given each project is quite short-lived, there’s always an end in sight and each project typically has a different set of challenges. The reason why working in consultancies is great is because you get so much experience in such a compact amount of time versus what you would typically get working in say a product organisation. So I would say, don’t rush it and get that experience if you can. Q: What has been one of the biggest challenges you’ve had to face since lockdown started in this position and how did you overcome it? A:  Cutting my own hair given the Peaky Blinders cut that I’ve accidentally given to myself. No, it’s difficult for everybody right now. The amount of mental stress it’s putting on people, not just because we’re all stuck inside our homes but because we actually find ourselves working a lot more. Finding that point of knowing when to switch off and how to be really strict with your time is incredibly difficult. Anything past 6pm is a hard stop for work and actually putting those things into place is important. When we started lockdown back in March of last year, it was all new and so we didn’t know what we were going to be faced with or how long this thing was going to go on for. We didn’t know how to manage it and I think a lot of people ended up burning themselves out because they were unable to switch off. It’s just so easy to keep on working way into the night because you’re at home already. Then you go to bed and the next morning you’re at your desk again at eight o’clock, so your brain is just constantly in overdrive. I think knowing when to switch off and looking after yourself are probably the biggest challenges that not just me but a lot of people have faced. Being really strict in making sure I leave the house every day because it’s so easy to let days go by without leaving the house. I try to get out for at the very least a walk every day whether that’s on my lunch break to just absorb some of the fresh air. Some people go for a walk in the morning to get a version of a commute in. That’s not what I do, I’ve tried that but it didn’t really work for me. But just trying to look after yourself is the biggest challenge and being strict with that. Q: What do you like most about working at Made Tech? A: I like the fact that it’s very people focused. Kudos to the people team and the amount of effort they put into actually ensuring that it is such a people-focused organisation. A lot of companies say they’re people-focused but they’re actually not. Not to throw shade at other companies and companies I’ve been a part of in the past, but the difference here is huge. Any pastoral care that you need you’re going to get, any progression that you want to take in your career you’re going to have a mechanism in place to support that, and your well-being is really looked after. The fact that you get unlimited holidays – any break that you need, that’s yours to take. I really love the fact that it is so people-focused. Q: Do you have any books or resources you would recommend for someone interested in this position? A: Crucial Conversations is definitely a good one. It helps you to know how to handle difficult conversations, which in this position there are a lot of those conversations that need to be had. Team Topologies is another really great one. It is helpful for the organisational transformation type work which I find myself doing a lot of. Accelerate is another good one for implementing a solid DevOps culture into organisations. Those are probably my top three at the minute. If you have any more questions for Matt about his here, you can get in touch by reaching out on Linkedin . Additionally, if you are interested in joining our team, you can view our current open positions here . Be sure to stay tuned for our next Made Tech Team Interview coming next month. Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-02-10"},
{"website": "Made-Tech", "title": "How to ensure skills development becomes a team outcome", "author": [" Duncan Bell"], "link": "https://www.madetech.com/blog/how-to-ensure-skills-development-becomes-a-team-outcome/", "abstract": "Once you have decided to prioritise technology skills enablement in your organisation , you need to start making the changes that will embed it within your working practices. One of the best ways to do this is by making learning and development a team outcome, not just an organisational one. By empowering your team and giving them the responsibility for driving the process forward, you’ll see your people find their own ways of developing skills and establishing the foundations of a sustainable model for the future. To kick this process off though, there are a number of best practice approaches developed within engineering communities that you can rely on. Involve the whole team from day one A team outcome is one that is defined by the members of the team, not one that is dictated by the management above. Skills development is often something that cannot be forced upon individuals who are not fully into it, so making learning a team outcome is important. You can and should hire people with experience of creating and participating in skills development programmes, just as you should hire individuals who buy into the concept and display a growth mindset. However, communities of practice, which play an important role in technology skills enablement, can only be homegrown and developed internally. To ensure your skills enablement strategy works, it also needs to be clear to see from day one of a new employee starting their job. Quality onboarding can make or break the experience of a new role and a structured programme of knowledge sharing plays a vital role in bringing new recruits up to speed with organisational values and delivery expectations. Setting those high standards and expectations from day one also lays the groundwork for continuous improvement. Therefore, it’s really important that the responsibility of onboarding new staff is given to existing team members. In this way, everyone takes ownership of including and upskilling new starters, resulting in it becoming an important team outcome that everyone recognises. This regular involvement by different members of the team also directs fresh eyes towards the onboarding process, so it can be reviewed, iterated and improved on an ongoing basis. Introduce Communities of Practice As mentioned, communities of practice are one of the best ways to start embedding learning within your organisation. This is not a new idea, as groups of like-minded people have been gathering and sharing ideas, knowledge, experiences and solving problems together throughout the ages. It is a well established concept, based on trust, openness and the ability to ask each other for help. Open source projects have been based on the principles of communities of practice for decades. The driving factors behind this have been shared goals, values, vision and interest to collaborate with like-minded people. A good example is the Linux Kernel, the backbone of the internet as we know, which started being discussed and worked on in Usenet newsgroups, email lists and IRC. The Linux Foundation and subset Cloud Native Computing Foundation are brilliant examples of how the open source community of practice has developed and thrived over the last three decades. Now, there are hundreds of thousands of supporters delivering and receiving training, webinars, conferences and communicating with each other. Over the last few years, the business world has started to embrace communities of practice too. This trend has occurred in line with a greater focus on employee wellbeing, happiness, mental health and support structures for employee development, which are all designed to aid productivity and improve staff retention. Structured support for employees to gather and form a community of practice within paid working time is now established as a valuable use of their time. The UK Government has been aligned with this approach since publishing the Civil Service Workforce Plan in 2016, a plan that focused on strategies for hiring, retaining and training staff. It is also aligned with the Government’s managing legacy technology programme, which depends on ensuring staff can constantly learn, evolve and keep up with their technology stack. Make knowledge sharing easy As your team starts to take on the responsibility for skills development, this needs to be encouraged and promoted as much as possible. We use a whole range of tools and opportunities to empower staff to share knowledge. On a very basic level, we promote the sharing of ideas using a variety of Slack channels.  Although some of them border on communities of interest rather than communities of practice, they are great for igniting ideas and group thought on different topics. We also invest in the learning and development of every employee by allocating half a day per week away from client work and towards learning. Learn Tech is an opportunity to learn new or develop existing skills, mixed with some more structured learning tracks to deepen employees’ knowledge and understanding of the company’s purpose and objectives. This is a company-wide activity that covers all departments and allows all our people to learn new things, whether they be technical or soft skills. Learn Tech is also a chance for people to prepare and deliver workshops to colleagues, providing a fantastic opportunity to brush up on those planning, presenting and facilitating skills in a safe environment. Learning by teaching others is the best way to fully understand a topic in depth and creating training content internally can also save you a lot of money on overpriced training courses. This is a constantly evolving programme and we are working with our team’s feedback to keep improving it going forward. We are also big on delivering showcases. We provide two fifteen-minute showcase slots on a Friday afternoon where colleagues can book and present anything interesting to the rest of the company. This is a great bonding opportunity, as well as being a way to bring some of your whole self to work or show some cool tech you used on delivery. We use showcasing a lot within client deliveries to demonstrate work and gather feedback, so these in-house sessions are a great way to practice presenting and build confidence in your abilities. Finally, we also encourage everyone to attend and speak at conferences, as well as writing blog posts about topics they can help others with. Conferences allow us to bring new ideas from outside back into the community, so we can test these ideas internally. Writing helps to document our knowledge for internal use and connect with like-minded people from outside. Measuring outcomes not outputs As you start to embed all of these skills enablement processes within your organisation, make sure you are measuring them to ensure the activities are worthwhile. When you do, it’s important to focus on outcomes over outputs. Just as in our software delivery life cycle, we want to be defining outcomes to work towards and not just measuring outputs like the number of bugs fixed or tickets closed. Capability building should be measured in terms of outcomes around behaviour, soft skills development and knowledge growth within the team. Counting metrics such as technical certifications gained or courses attended shouldn’t be dismissed entirely, as they demonstrate that employees are moving in the right direction. However you also need to consider wider team metrics alongside these indicators of individual success, in order to ensure you are improving the whole team or community’s knowledge. It is essential that everyone in the organisation is mindful of this and feels empowered to take time to define and achieve outcomes around capability building within a team. This mindset is essential in high performing teams, which are made up of individuals with a diverse range of skills rather than specific siloed ones. Imbalanced teams reliant on hero developers can produce good outcomes but it’s not a sustainable model we should depend on. Everyone should be equal and able to achieve all aspects of the team’s work and the only way to do this is by giving the team time to grow, share and develop their skills together. Supporting public sector skills enablement From 2017, the government began requiring any public sector employer in England with at least 250 staff to employ an average of 2.3% of their staff as new apprentices. As mentioned in my previous blog, this sort of initiative provides a great opportunity for organisations to build a consistent pipeline of engineers with the skillsets they need. Our own belief in this approach can be seen from the fact we have embraced it ourselves. The Made Tech Academy , which has run since 2017, is a twelve-week training programme that brings individuals from a variety of backgrounds into our company and pays them a full salary as they learn. After this initial training period, they go on to work on client deliveries. This allows them to continue growing their skills and knowledge, while also bringing the best practice techniques they learned in the Academy, as well as their own fresh ideas, into the public sector. I love working with Academy engineers because their ardour is contagious and they also ensure our communities of practice are continually refreshed with a new batch of talent. We have used our experience of developing apprentices to help a range of public sector organisations already. Most recently, this involved supporting the HMRC Multi Tax Digital Platform in Bristol with its Apprenticeship programme throughout 2020. Furthermore, as someone who works on the HMRC MDTP project, I have witnessed how developing apprentices has grown into an important outcome across the platform teams that will help maintain and progress the platform for years to come. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-01-27"},
{"website": "Made-Tech", "title": "Made Tech Team Interview: Anikh Subhan, Delivery Principal", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/made-tech-team-interview-anikh-subhan-delivery-principal/", "abstract": "This month we continued our Made Tech team interview series with our Delivery Principal Anikh Subhan to better understand his role and to feature the great work he has been doing. Our Delivery Principals help public sector organisations accelerate digital delivery, drive efficiency and ultimately improve society. They do this by leading large-scale teams (accounts), building and managing strategic relationships, finding new opportunities and advising as senior delivery leaders. If you would like to watch the full interview, it is available here . Q: Outside of Made Tech, what hobbies do you have? A: Things like fitness, football and food, specifically desserts. I also like to go for a few runs now and then and I’m really big into space and astronomy. Q: Before you joined Made Tech, had you ever worked in the public sector before? A: No, I hadn’t actually. That’s something that really appealed to me about Made Tech because it was a new experience and a new challenge to focus on the public sector. Q: How have you found working in the public sector? A: It’s really good actually. It’s very similar in a lot of ways to the private sector. You get to work on stuff that society and the nation are using day-to-day, which is quite rewarding. But being able to play a part in improving people’s lives has been hugely motivating. Q: How long have you been at Made Tech? A: I joined at the start of October, so it’s been 3 or 4 months now. Q: How did you find out about the company? A: I was looking for a new role and I came across an open one at Made Tech posted on Linkedin and applied and the rest is history. Q: What attracted you to Made Tech? A: The culture here is a big aspect and is really good. As well as the type of work that we are doing and the vision for the company for the future in terms of the growth and where we’re heading. Q: What was the interview process like? A: It was very smooth with a very quick turnaround as well. For my role, it was a couple of different interview stages. Everyone was very friendly and welcoming as well, so it was a really good opportunity to learn more about the company, the culture and about what is expected in the role and for working at Made Tech. Q: What does your role here involve? A: My role as Delivery Principal involves a few different bits. The key thing really is overseeing one of my key clients in the London office and the multiple work streams within that. Within that client, we’ve got a few different projects and deliveries going on and my role is about managing and providing the right kind of support and guidance to our teams. I also work with our key client stakeholders to help shape up successful deliveries of our products and services. Q: What do you like most about being a Delivery Principal? A: I think it has got to be the variety. There’s a lot of variety in the role, both in terms of the type of work we’re doing and the types of people and teams we’re working with. It’s also the reward of seeing the results of our hard work which is great and really exciting. Q: What are some of the most challenging things about this role? A: I think it is a good challenge actually and not necessarily one that is particularly painful or tricky because it’s a good opportunity to work on. It’s really evolving and improving how we do things in delivery as we continue to grow and scale as a company. We do some really good stuff and we work in some fantastic ways and methods with our clients and it’s about growing that as the company continues to scale going forward. Q: What has been one of your favourite projects you have worked on? A: One of the favourite ones, I can’t really favouritise my team, but one of them is the programme of work we are doing currently to help transform the technology and services that is used across prisons. Prisons, as you can imagine, are very old school in terms of the way they are built and the services people use. So the opportunity to really evolve that and bring it into the digital age that is very seamless in delivering value for the user is pretty exciting. Q: Do you have any advice for anyone who wants to be a Delivery Principal? A: I think it depends on where people are at the moment, but something to think about at a high level would be to continue to lead deliveries. Get a lot of experience leading deliveries, no matter how big or small. Another key part is continuing to learn, which is something that is important in any role. Have an appetite to learn and grab opportunities to do that as well. Also helping and supporting people with their own growth and development is a core part. Any opportunities to coach or mentor people and really support their growth is going to put you in a good position for this kind of role. We’re hiring as well, so get in touch if you are interested. There are plenty of opportunities to move into a Delivery Principal role, or similar kind of role. Q: What has been one of the biggest challenges you’ve had to face since lockdown started in this position and how did you overcome it? A: It’s probably a similar challenge to most parents with kids at home full time: juggling work with homeschooling. Everyone’s feeling the pain there. What this means is there’s no real breaks throughout the day, so it’s constantly switching between meetings, calls, learning to read, learning ABCs and 123s; it’s quite a juggling act. Made Tech and the culture here is so flexible though and allows the space around work to make sure that we can manage and juggle things. That’s so important especially with what everyone’s going through at the moment, so that’s definitely helped with that challenge. Q: What do you like most about working at Made Tech? A: It’s got to be the culture. It’s a fantastic culture that we’ve got here focused on learning and development. It’s a people-first culture that’s relaxed but determined with a focused mindset. Made Tech has ambitious growth and scaling plans, so it’s important to be centred and focused on people as well. So for me, it’s got to be the culture. Q: Do you have any books or resources you would recommend for someone interested in this position? A: There’s a variety of books and resources out there, but what I would say is it depends on where people are currently at. What type of goals or specific needs they have in terms of their development or growth that would determine which books and resources to go for. So on that note, if people are reading this and they are interested in things that could help them, feel free to reach out directly to me on Linkedin , I’m happy to speak and support people in their growth. Don’t hesitate to get in touch. If you have any more questions for Anikh about his role here, you can get in touch by reaching out on Linkedin or Twitter . Additionally, if you are interested in joining our team, you can view our current open positions here . Be sure to stay tuned for our next Made Tech Team Interview coming next month. Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-01-14"},
{"website": "Made-Tech", "title": "Made Tech Team Interview: Tess Barnes, Senior Software Engineer", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/made-tech-team-interview-tess-barnes-senior-software-engineer/", "abstract": "This month we continued our Made Tech team interview series with our Senior Engineer Tess Barnes to better understand her role and to feature the great work she has been doing. Our Senior Software Engineers deliver digital, data and technology outcomes that improve society. They do this by delivering and architecting software, and coaching others to do so in public sector organisations. If you would like to watch the full interview, it is available here . Q: How did you become interested in tech? A:  When I was a kid, my dad bought me a spectrum and we spent a lot of time programming games on that. Then I got involved in bulletin boards at University and started doing early websites and it’s just grown from there. Q: Outside of tech, what other hobbies do you have? A: I sing in a covers duo. We used to gig but that’s on hold at the moment. I’ve done some training in martial arts and I go out in nature with my camera now and again. I like to do lots of different things. Q: Before you joined Made Tech, had you ever worked in the public sector before? A: I hadn’t actually, so it was a new venture for me. I wanted to get involved in something a bit more meaningful and give something back. I’ve been in the tech industry for quite a while, but it was that jump from making money for stakeholders to actually supporting the rest of the community I live in. It was a good jump. Q: How do you find working in the public sector? A: I’ve found it really interesting. There’s a lot of constraints and challenges that I wasn’t expecting, but there’s a really good feeling of wanting to do things in an efficient and modern way. I had a terrible impression that it was all in the dark ages and it absolutely isn’t. It really has some good tech in there. Q: How long have you been at Made Tech? A: I joined Made Tech right at the end of February this year. It’s been an odd time to join due to COVID, but it’s quite interesting to think I’ve been here for nearly a year. It’s gone really quickly. Q: How did you find out about the company? A: I was looking for work after being made redundant in my previous role and a South West recruiter contacted me and said ‘there’s this really great company. Have you thought about working in the public sector?’ That sounded quite exciting. I’d not worked in the public sector before, as I’ve said, so it was a chance to see new companies and Made Tech really stood out. Q: What attracted you to Made Tech? A: The openness and the transparent culture was a really big sell. The idea that I could have a look at the Handbook before I even started talking to the recruiter about more serious contact. I liked the idea that there are books out there that Made Tech have written. There’s also quite a heavy involvement in the tech community from a speaking point of view. All of those things are really big pluses in my mind. Q: What was the interview process like? A: It was very slick and quite intense. There was a question part, a pairing part, and then more questions with HR. Being able to do most of it in one sitting was fabulous and I got some really rapid feedback. I heard back pretty much the same day that there was interest there which was a huge plus. It made me feel wanted. Q: What does your role here involve? A: I get involved in so much. I split quite a lot of my time between stakeholders on the project I am working with and getting some really good code written. I work with other engineers on the client side on good practices and getting some great tests written. I also work on upskilling and learning from everyone, which is a really good two way process. Outside the normal client stuff, I mentor a couple of people as well. I find it really rewarding to be able to share knowledge at any level and learn stuff every day. Those are all big wins. Q: What do you like most about being a Senior Engineer? A: It is the ability to get hands on, get into the code, and really enjoy the engineering. But also being able to step back and look at things from a slightly bigger picture. I like talking to stakeholders, understanding what we’re trying to solve, what outcomes are really important, and helping the culture and confidence building with my temporary colleagues on the client side. Also, sharing a growth in skill and growth mindset is a really big thing for me. Q: What are some of the most challenging things about this role? A: We discover so much working with the client. We tend to uncover a lot that they really want to change and we’re forever finding ways to improve, which is wonderful. The downside to that is that we can’t fix everything. We have to focus on the outcome that is most important for them right now. The other thing that I find I need to be very aware of is what pace the client is working at. At Made Tech, we can crack things quickly and we’re really good at doing things efficiently. But sometimes we have to go, ‘actually, the client’s working at a different pace and this is what they are comfortable with right now’. Measuring that pace is a really important thing. Q: What has been one of your favourite projects you have worked on? A: I think the thing I am working on today is the most important and most fun. I work at a project with HMRC on one of the internal tooling teams. I think the reason why I enjoy it so much is because we get to support those folks that are working on critical services. It means a lot to me to support these guys to get critical services out on time and to make their onboarding and offboarding experience much more painless. Just making their day better basically is what I enjoy most about it. Q: Do you have any advice for anyone who wants to be a Senior Software Engineer? A: Never stop learning and never believe that you know everything. There’s always going to be something new and interesting out there. I always have to temper that with, what is the best fit for the solution that is in front of us right now? What is the real problem? If you understand the real problem, then you can brainstorm solutions as much as you like. Starting with the problem is always the first thing I would suggest. Q: What has been one of the biggest challenges you’ve had to face since lockdown started in this position and how did you overcome it? A: I struggle with energy. I have to be really disciplined with myself about taking breaks. I like being a positive person and I like having conversations face to face as much as I can, even when we’re talking tech, so I’m having to pace myself during the day because video calls can be quite tiring. It’s just understanding where I am and being really clear and honest with myself about where my energy levels are. I just started doing yoga in the morning to see if that can build up my energy and so far it’s really helping. Q: What do you like most about working at Made Tech? A: The people are a really big part of working at Made Tech. We build our own culture and we have some great, remote extravaganzas and get-togethers. There’s a great social atmosphere. There’s also this wonderful drive forward to look at the best solutions and the best fit for the situation in front of us. Asking ‘what can we improve on?’ That continuous improvement and continuous learning is brillant. It makes it a great place to work. Q: Do you have any books or resources you would recommend for someone interested in this position? A: I find I depend more and more on a really good search engine and a critical thinking approach when it comes to a search engine. You can copy any old code out of stack overflow, but it doesn’t mean it’s going to work. Knowing which one to grab and which one to not is important. I also rely quite strongly on conversations with the team that I am working with. That’s really good for opening up thought processes. Many minds tend to look at a problem from lots of different angles, so that’s really important. From a book and resource point of view, I really value The Pheonix Project , which is a novelisation written by Gene Kim, Kevin Behr, and George Spafford. I found that to be a bit of a game changer looking at devops. From a cultural point of view, I really recommend Rebel Talent by Francesca Gino. She’s got some great insights from Harvard Business School as to when following the rules is a good idea and when not following the rules is a good idea. It can help with bouncing out of a rut. I also refer back to the Modernising Legacy Applications in the Public Sector book that Made Tech have produced quite a lot. I tend to trip over a lot of legacy applications and it’s really good to have that focus and ideas of how to approach it. If you have any more questions for Tess about her role here, you can get in touch by reaching out on Linkedin or Twitter . Additionally, if you are interested in joining our team, you can view our current open positions here . Be sure to stay tuned for our next Made Tech Team Interview coming next month. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-12-16"},
{"website": "Made-Tech", "title": "Made Tech Team Interview: Vikki Gill, Community Manager", "author": [" Camille Hogg"], "link": "https://www.madetech.com/blog/made-tech-team-interview-vikki-gill-community-manager/", "abstract": "This month we continued our Made Tech team interview series with our Community Manager, Vikki Gill, to better understand her role and to feature the great work she has been doing. Our Community Managers are responsible for a variety of areas at Made Tech including onboarding new team members into the company, supporting events, looking after our regional offices, and general team wellbeing and happiness. If you would like to watch the full interview, it is available here . Q: Outside of work, what hobbies do you have? A: I guess they’re kind of on hold a little bit at the moment because of the pandemic, but I like cooking, I’m quite big into film and tv, and then I’m just socialising and hanging out with my friends. I enjoy going out for dinner and drinks. Q: How long have you been at Made Tech – and how did you find out about the company? A: Since August 2019, so coming up to a year and a half now. I knew someone who worked at Made Tech — they’d been here over a year I think, and they had such great stuff to say about it. Q: What attracted you to Made Tech? A: It sounds cheesy, but the people and the culture. I was always hearing really good stories about how great the people are and the social side outside of work they had going on as well. It was also the work that Made Tech does as well. I think there’s a lot that needs to be done in the public sector, and especially during the pandemic it’s been pretty humbling to see the kind of work that we’re doing and how we’re helping people. Q: What does your role here involve? A: Lots and lots of stuff — one good thing is it’s never boring! It’s always changing, but the main focus of my role is onboarding. We’ve had a really successful year. We were around 50 when I started, and we’ve now onboarded about 150 people over the last year. It’s been really busy and the company’s been growing. It’s also people support, which I love. I also help support Learn Tech and how that’s evolved and just general wellbeing at Made Tech. I do a bit of office management here and there —less so these days — and basically try to be there for people with anything they need. Q: What do you like most about being a Community Manager? A: It’s going to be the people. It’s a really varied role which is great so it’s hard to get bored in a job where you’re always doing something different. My whole job is chatting with people and making sure they’ve got everything they need —what’s not to love about that? Seeing everyone as they come through the door is really nice. Q: What are some of the most challenging things about this role? A: I think not being able to see everyone and having to completely change your role due to being online. It’s harder to check in on people when we’ve got such a huge team and you don’t have those interactions in the office anymore. It’s just trying to check in on people to make sure they’re okay. You don’t want to bother them at the same time, but you want to make sure that they’re doing okay. Q: What has been one of your favourite projects you have worked on? A: Onboarding has been pretty great — it’s quite a big responsibility. Generally, my team is one of the first few faces that people see as they come into the business outside of the interview process. We’ve been able to make loads of changes over the year and just seeing everyone that comes into the company. We’re in a really lucky position to see a lot of people, especially as we’re growing and others don’t get to meet everyone. We genuinely do have a really great team and it is so nice to meet everyone as they come through the door. Q: Do you have any advice for anyone who wants to be a Community Manager? A: It sounds cliche, but just be yourself. We just want someone to be genuine and be really good at creating relationships with people. A big part of this role is support, and you just want to make sure that people that are wanting to be Community Managers are able to make those relationships with people and just be a friendly helpful person to go to. Q: What has been one of the biggest challenges you’ve had to face since lockdown started in this position and how did you overcome it? A: Probably the amount of stick I’ve had for working on an ironing board! No, but really I guess changing my whole role to being online. I rarely used to work remotely — we always had it as an option, but my role is very people-facing so I always enjoyed being in the office. Having to flip that over onto a whole virtual thing was a big challenge. I’ve proved I can work really effectively from home so that’s a win for me. There have obviously been a lot of challenges over the last year during the pandemic and I feel like we’ve been extremely lucky. It’s definitely caused some issues and now work is home and home is work, so setting boundaries as well. It’s about making sure you’re not just like, ‘oh, my laptop’s there. I could just do this a little bit’ — and I’m definitely setting that example for everyone that starts as well. Q: What do you like most about working at Made Tech? A: I say this in interviews as well — it’s the people. You meet a lot of people that you wouldn’t necessarily meet day to day. Everyone has so many different interests and backgrounds and is so nice and friendly. Hopefully, they’re all here because they share our culture and values and are interested in the work that Made Tech does. You always know that you’re going to get on with them because they have the same sort of views as you, but then everyone’s so different. I do think our interview team does well in making sure we have the people that are here for the right reasons. I think they’re smashing it. Q: Do you have any books or resources you recommend for someone interested in this position? A: One we often recommend is Radical Candor . It’s a really good book about being honest and saying what you mean to get the right results. I also try and attend a lot of webinars, so a few from 15five are really good. Happiness Index is good. Eventbrite have some good ones, Sapling and other resources. When I first joined, I started Codecademy for Learn Tech. I thought I was fairly technical until I got here and realised that I have no technical knowledge. Learning to do a bit of coding was really useful to see what most of our team do day-to-day. I think we’re over 110 strong now for engineers and what they do each day is very difficult — I get my hat off to them. If you have any more questions for Vikki about her role here, you can get in touch by reaching out on LinkedIn . Additionally, if you are interested in joining our team, you can view our current open positions here . Be sure to stay tuned for our next Made Tech Team Interview coming next month. Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-03-16"},
{"website": "Made-Tech", "title": "International Women’s Day: How we choose to challenge gender bias and inequality", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/international-womens-day-challenging-gender-bias-and-inequality/", "abstract": "We can all choose to seek out and celebrate women’s achievements and help create an inclusive world. As part of our team’s International Women’s Day​ celebration, our CEO Rory MacDonald, COO Chris Blackburn, Head of Marketing Laura Plaga, and Head of Operations Sam Paice, have shared some of the ways they “Choose to Challenge​” gender bias and inequality to create a better culture for people at Made Tech and the wider community. If you would like to watch the full interview, it is available here . How we choose to challenge gender bias and inequality at executive level Rory: I have an amazing daughter and I want to see growing up in a world that provides her with equal if not better opportunities than the ones I had. It strikes me as deeply unfair that because of her gender she would be at a disadvantage to others. I recognise that it’s on people like me, people who lead businesses and have significant influence over business strategy, to be challenging the status quo for this International Women’s Day. There are lots of things we’re doing at Made Tech but the one I want to highlight today is the thing I’m personally most focused on at the moment. Less than 10 of the UK’s public companies have equal gender split on their boards. We’re currently recruiting for a brand new board of directors at Made Tech and I’m working hard to achieve gender equality at that board level. It’s difficult because we’re fighting against the norms but we have to as we know change in gender equality needs to start at the top. How we choose to challenge gender bias and inequality in the decision-making process Sam: I think that organisations make better decisions when they encourage a more diverse set of viewpoints and experiences in decision making forums and when they involve more different types of people in how things get done. My experience when joined this company a couple of months ago at group leadership level is that we’ve got a really nice mix of men and women in those positions. I feel like we have good quality discussions and I hope that we continue to make that focus as we look to expand the leadership of the business as we get bigger. How we choose to challenge gender bias and inequality through mentorship Laura: Mentoring other women I work with who are getting started in their career is important. Learning and mentoring is one of our core values and it’s one that I feel particularly strong about. I think it can make such a huge difference to people. How we choose to challenge gender bias and inequality in our recruitment and hiring Chris: One approach I think has had a lot of impact was looking at the early stages of our recruitment pipeline and how we can attract a wider range of candidates. So including things such as, 1) looking at our benefits package and how we’re positioning that to cater for people with different wants and different needs 2) evaluating the language that’s used in our job adverts to try to remove any unconscious biases and then 3) supporting and engaging with underrepresented groups in meetups and the like to help build the Made Tech profile with a wider set of communities. What we’ve found through taking some of these steps is an increased volume of applicants from diverse backgrounds at the start of our recruitment pipeline and this is ultimately translated into an increased number of diverse hires coming out the other end. How we choose to challenge gender bias and inequality by recognising and addressing bias Sam: I think a place that I’m looking to kind of work myself on is any unconscious bias that I might still have. I feel like as a male in a leadership position who works with women, has managed women and been managed by women, it’s very easy to think that you don’t have any bias. But I think it’s always important to be aware or try and make yourself more aware of any unconscious bias you might have. Over the next few months that’s certainly something I’ll be thinking about in how I operate within the business. It’s something I’m looking forward to. I feel like it’s always good to challenge yourself on that kind of thing and I hope it will make me a better colleague to work with. How we choose to challenge gender bias and inequality by supporting and celebrating achievements Laura: It’s important we are supporting and celebrating other women’s achievements. I think by encouraging each other to challenge the gender confidence gap and have each other’s back that we can make such a huge difference. While Made Tech has made a number of really positive steps forward in recent years, unquestionably, we still have a long way to go. We would love to hear how other companies are working to be more inclusive and challenging gender bias and inequality. Happy International Women’s Day from the Made Tech team! Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-03-08"},
{"website": "Made-Tech", "title": "A proactive approach to integrating security with agile delivery", "author": [" Tom Foster"], "link": "https://www.madetech.com/blog/a-proactive-approach-to-integrating-security-with-agile-delivery-2/", "abstract": "37% of UK companies have reported a data breach incident to the Information Commissioner’s Office (ICO) in the past 12 months. There is a general consensus that security is paramount, however, what is less agreed upon is how to ensure that security is a priority to technical teams. Often in larger organisations, cyber security is seen to be handled by a separate team that will pen test the applications, and be purely responsible for ensuring that the end product is secure when released. This leads to a separation of responsibility between developers and the cyber security team, and is counterproductive to increased research pointing towards collaboration being key to embed good security practises in organisations. In this blog post, we will explore different ways to embed security into agile development and by doing so increase collaboration between different teams. Little and often security testing Often the first point of call when an organisation wants to focus on cyber security is to create a threat model for their product or service. This can be daunting as depending on the product or service there can be lots of complicated and realistic threats which once listed can be seen as almost impossible to protect against. Thus the process can be counterproductive if teams are looking for an exhaustive and complete threat model from the beginning. Instead of trying to devise a complete picture often development teams have found it useful to create an attack surface analysis in a thought shower session, first focusing on external vulnerabilities such as login pages or API endpoints then in later sessions expanding it to cover other areas. This creates an iterative approach that can pair well with agile delivery, especially if this is done from the beginning. Agile delivery is often described as iterative and incremental. To properly dive into delivering iteratively and incrementally I would recommend reading 3 strategies for transforming legacy applications . Identifying all the security vulnerabilities nearing the end of the project can be very expensive and lead to big delays in launch dates if they are severe enough. Mitigate this by looking to test little and often, focusing on certain parts of the code base when testing rather than trying to test everything at once. Collaborate and listen Siloed teams are a problem not just for security reasons but for many other aspects of software development. Because software teams can often be focused on sophisticated technical attacks that could occur, they miss how a hacker could manipulate the user flow to elevate their privileges allowing them to gain access to secret data. Stakeholders are often the best source of knowledge of how users will interact with their systems and thus can identify the potential ways a hacker could exploit the system. To bring stakeholders into the discussion, it helps to align security concerns with business objectives. Why would protecting user data be key in ensuring that customers sign up to a site? What added value does this firewall offer to mitigate risk of downtime of the service? Phrasing the question in a specific way is key to ensure everyone can understand why certain tasks need to be completed, and keep security at the forefront of concerns for a software project. Be proactive, not passive in security testing The top 10 proactive controls from OWASP offer security techniques that any organisation can implement to embed security into their software development process. One of the key controls is introducing security requirements along with performance and business requirements. When defining requirements it is about augmenting these with user stories and misuse cases to allow a clearer indication of what the requirements are attempting to do to protect the systems. If we focus on a misuse case we can create a story that focuses on an attacker’s actions: ‘As an attacker, I want to use a dictionary attack to guess passwords of users’ Then from this, a task can be added to block a user from their account after 3 failed attempts of guessing their password, meaning a dictionary attack would be less effective. For a thorough guide on how to write practical security stories, please read Safe Code’s Practical Security Stories . Another proactive way to embed security into the team’s daily processes is to have static and dynamic code analysis integrated into your CI/CD pipeline looking specifically for security vulnerabilities. However, many security experts do warn that finding security vulnerabilities is difficult and this step should form part of your security strategy rather than being the answer to it. You can also include security functional testing where teams have modified Selenium to perform unauthorised user flow. When getting ready to go into production it is vital to produce a response plan if there is a security breach. Often breaches catch organisations off guard, especially if it is a zero-day vulnerability. A response plan if a hack occurs can limit the damage radius of the hack. It should include steps such as how to notify affected users and an internal escalation plan to ensure the team can act quickly and effectively. Breaking down barriers Most central to a successful strategy in embedding security into agile delivery is communication. Breaking down barriers between different teams is going to allow a more holistic and thorough security strategy to be implemented. In this blog post, we have skimmed the surface of a manner of security techniques that can be incorporated into your agile delivery. Each software delivery team will have their own set of specific security criteria that they need to fulfil so it is not a one size fits all. However, having discussions and producing actionable items from these discussions can end the barriers between software developers and cyber security teams to bring about a more collaborative way of working, increasing security across the software development cycle. If you are looking for more ways to transform your team into one equipped to handle any software delivery challenge, you might be interested in reading our ebook, Building High Performance Agile Teams . From recruitment to pair programming, through to letting your teams shape the way they work, this book will give you the tools you need to overcome challenges and adopt agile ways of working. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-03-04"},
{"website": "Made-Tech", "title": "Why we need to make the tech sector more inclusive for women", "author": [" Camille Hogg"], "link": "https://www.madetech.com/blog/why-we-need-to-make-the-tech-sector-more-inclusive-for-women/", "abstract": "There were six pioneering women behind the programming of the world’s first computer. You may not have heard their names before, but back in 1946, Jean Jennings Bartik, Frances Elizabeth Holberton, Frances Bilas Spence, Marlyn Wescoff Meltzer, Kathleen McNulty Mauchly Antonelli, and Ruth Lichterman Teitelbaum became the first modern computer coders in history. Originally, the six women had been hired by the military as ‘computers’ as part of the war effort. They spent their time calculating ballistics trajectories by hand, using complex mathematical equations to work out angles of fire based on target distance, weather conditions, and other factors. But in 1945, everything changed with the invention of a new machine. The Electronic Numerical Integrator and Computer, or ENIAC as it was known, was designed to make this calculation process faster and more efficient — but the only trouble was that very few people knew how to program it. It fell, then, to Bartik, Holberton, Spence, Meltzer, Antonelli and Teitelbaum to master the machine, finding their way around the new tool using their knowledge and skill, rather than manuals and programming languages. Post-war, this computing boom continued. Despite the fact that programming was largely still seen as clerical work, women continued to dominate the sector. In the intervening years between then and now, however, something changed. Women went from being at the forefront of shaping tech history to often little more than a footnote. These days, it’s estimated that women occupy just 25% of roles across the tech sector — a place where they had once dominated. With March marking Women’s History Month and International Women’s Day globally, we looked to some of our own team here at Made Tech to reflect on their experiences of the sector, and think about how we can make tech a more inclusive space for women. We need more women in leadership roles For women, visibility at the top has always been a problem in every sector. In 2020, diversity report The Pipeline revealed that there were only five female CEOs in the FTSE 100 in the UK, which was a decrease from six the year before. In tech, only 23% of tech director roles are occupied by women. Chloe, a Software Engineer based in our London team who joined after completing our Academy initiative, believes that this visibility at the top level is a key problem that is a barrier to making tech and other sectors a more inclusive environment for women. “This industry is definitely a more male-dominated one — but coming into the industry right now, at a time when we’re striving for more gender parity, has meant that I’ve felt truly accepted and that I’ve been treated the same,” she notes. “My experience at Made Tech has shown me what the experiences of a woman in tech can, and should, be. But I feel that there needs to be a significant change in the way we design leadership in companies. The higher up you go, the more white males we see, and I think that seeing more females in positions of power, like in the boardroom or in executive roles, will help cascade this change down and bring about cultural change. “We often think that increasing the number of women in the sector comes from hiring more women at a graduate level, but I think this change needs to happen in both directions. I do think things are going in the right direction, but we’ve got a lot more work to do.” We need to make tech more accessible for young women In order to see more women in the boardroom, we need to help young women understand the possibilities of forging a career in tech much earlier on. A PwC report revealed that while around 60% of young men were actively considering a career in tech, only 27% of young women felt the same. Only 3% considered it their first choice. For Freya, a Delivery Manager based in our South Wales team, it all comes down to education. “When I was at school, we didn’t even have computer science or IT really,” she laughs. “Our lessons were more focused on helping us learn how to use computers, and we only had a couple of those in our school.” Freya began her career in tech 20 years ago, after taking on an extra credit course at university on Web Design. After completing a diploma in Computer Science, she eventually took on a software trainer role in a local company, over time taking on a director role. “When I joined, there were two females in the company,” she recalls. “By the time I left, we had 15 females in tech roles. I have been the only female director in the company to date. When I stepped into that role, I found it quite difficult initially. It had always been a male environment, and I had to change and grow to take the role — but they had to change, too. We had to change the way we communicated with one another, but this paved the way for other women in the company to feel safe doing the same. “I’ve always felt really accepted in tech, but I think the most important conversation we need to be having is how do we make it more accessible and inclusive towards women,” she adds. “I think there are a lot of misconceptions about the diversity of roles — the possibilities open to you whether you’re more creative, or whether you’re more of a problem solver. What I love about tech is that you’re giving people tools that make their lives much easier. But when we were talking about careers options at school, tech was never a topic of conversation.” Freya is a governor for a local school, and helping young women understand what they can become is something she firmly believes in. “We’ve all got a duty as organisations to make sure the diversity is there, but first we need to understand what that means to different groups. Schools can be a huge factor for changing this conversation for young women, dispelling the misconceptions, removing the barriers to entry, and making sure that we keep talking about it.” We need to rethink the language we use “I think that one of the easiest changes we can make to be more inclusive is around the language we use,” reflects Vix, a Client Principal based in our Bristol office. Vix began her career in tech as a software engineer before making the transition to client-facing roles. “I did a degree in Computer Science, which was heavily male-dominated,” she recounts. “Women made up around 10% of the people on my course — most of whom had dropped out by the time we made it to final year.” Vix has mostly found the tech sector welcoming and has worked alongside some really strong allies, but she has experienced sexism and gender bias in her career. “I have definitely had to work harder to be respected to the same level as my male peers for doing the same job,” she says. “There was one particular incident where we went out for a team meal and one of the guys made a comment that I was just “the token”. It was the first time that I remember something so overtly sexist being said to me in this environment — but I’d been aware of the bias for some time.” Vix’s experiences, which ranged from feeling she needed to prove her engineering competency more than her male counterparts, to clients addressing more junior male members of her team before her in meetings, to receiving emails addressed to “gents”, began to wear on her: “It’s the constant low-level things that can eat away at you over time. I had to fight for recognition, or for people to address me in meetings — sometimes even just to get eye contact. I’ve often wondered how much this has fed into my imposter syndrome and my perception of myself and my skills. “There are lots of times where we have casual conversations at work, where people will use terms like ‘man-days’,” she adds. “On a day-to-day basis those sorts of things can seem inconsequential, but can give the impression that women are afterthoughts. “We use this language all the time and it divides people as being either part of the club, or outside of it. Many of us might not think that deeply about how our words impact people, because there is no intention to be sexist, but language is powerful, and making an effort to re-evaluate how we use it is important. We need to reframe our language around man-days, stop using ‘hey guys’, and make tech a more inclusive space for everyone.” Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-03-26"},
{"website": "Made-Tech", "title": null, "author": [" Luke Morton", " Camille Hogg", " Luke Morton", " Camille Hogg", " Luke Morton", " Camille Hogg", " Camille Hogg", " Camille Hogg", " Karsyn Robb", " Camille Hogg"], "link": "https://www.madetech.com/blog/", "abstract": "In our final post in this series, we take a look at the 3 most important ways you can accelerate discovery in your organisation. This month we continued our Made Tech Team Interviews series with our Head of Design Harry to better understand his role and take a look at what he likes most about working here at Made Tech. In the second post in this series, Luke explains how accelerating discovery can be an advantage when it comes to digital service delivery. Is digital evolution the new transformation? Here are 3 key takeaways from our Head of Government Glen Ocskó’s recent talk at Digital Leaders. How can you deliver new digital services while realising value quickly? We explore 3 things you need to know about accelerated discovery. In episode 2, Clare talks to engineer Paula Paul about the things organisations need to consider when transforming their digital processes. How can we enhance our learning when working in teams? In the first episode of our podcast, Clare talks to software developer Jessica Kerr on this topic. How can you shape culture and process while handling a complex delivery? Lead Engineer Scott Edwards talks through our experience with the DVLA in a recent webinar. This month we continued our Made Tech team interview series with our Senior Delivery Manager Gráinne Blake to better understand her role and to feature the great work she has been doing. We’re excited to launch our brand new podcast! Exploring all things tech, find out more about our brilliant upcoming line-up.", "date": "2021-05-25"},
{"website": "Made-Tech", "title": "The pitfalls of neglecting legacy application transformation", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/the-pitfalls-of-neglecting-legacy-application-transformation/", "abstract": "Legacy technology is a huge threat to the public sector. While this won’t be the first time you’ve read this, it remains relevant because not enough action is being taken to prevent disaster. In July 2019, the House of Commons Select Committee for Science and Technology stated that “ legacy systems are a significant barrier to effective Government transformation and digitisation ”. It went on to say that the Government Digital Service (GDS) should conduct an audit of all legacy systems across government and that this should be completed no later than December 2020. This urgency should be welcomed. Legacy issues need to be identified as soon as possible, and public sector leaders across digital and technology must take responsibility for highlighting the specific risks that exist in their organisations. Then they need to develop appropriate strategies for transforming their legacy applications, which are funded properly and can be executed successfully. If they do not, recent examples from the private sector show us that the effects can be widespread and, in the worst cases, tragic. Reputational damage at British Airways The airline has been hit by a string of IT issues in recent years, including a major data breach in 2017. However, it was the glitch in its check-in and departure systems that exposed the company’s flawed legacy technology strategy. In August 2019, more than 100 flights were cancelled and 200 delayed, affecting tens of thousands of customers. While the company has been reticent about explaining why the issue occurred, some have speculated that it came down to building new service capabilities while ignoring core legacy systems. As layers and layers of new technology are built up, it becomes harder and harder to check transaction chains across these layers and to foresee how changes in one might affect the operation of another. Life savings lost at TSB In April 2018, customers of TSB experienced the first few days of what would become several weeks of significant disruptions to their banking experience. Many were unable to access their accounts. Some could see other customers’ accounts or were presented with large negative balances when they tried to view their own. In June 2018, the bank admitted that 1,300 customers had been hit by fraud, with some having their life savings stolen. These issues all came about because of the poorly executed migration of account data from legacy systems at the bank’s previous owner, Lloyds, to the systems of its new owner, Sabadell. TSB had taken the correct first step by identifying the bank’s change of ownership as a key driver of legacy transformation. However, in aiming for a big bang transformation rather than ongoing incremental change , it clearly failed to execute in a way that derisked the impact for its customers. Personal tragedy at Boeing Between October 2018 and March 2019, the US aerospace giant experienced two disastrous plane crashes, which had their roots in a policy of choosing financial restraint over legacy transformation. Investigations into both crashes have shown that the company’s decision to choose a software workaround, rather than make expensive changes to its legacy design, was at the heart of the problem. Boeing had decided to fit a bigger, more efficient engine which affected the angle of ascent during takeoff. Rather than fix how the new engine affected the plane by redesigning the build of the aeroplane though, it implemented a software workaround as a cheaper, quicker alternative. While it must be noted that a lack of training and communication about how the new software worked also played its part in the crashes, there’s no doubt that choosing to avoid large legacy transformation costs played a significant role too. Public sector beware The key lessons that public sector leaders should take from these private sector examples is that failing to address, finance and implement a legacy transformation programme is very risky. Furthermore, they should be fully aware of how failing to deal with legacy technology in highly-sensitive areas of the public sector could blow up into headline-grabbing events that rival the Boeing tragedies. But Digital and Technology leaders don’t need to be shocked into action. Simply paying attention to the issues that their users experience every day should make them realise that action is required. For example, it is already clear that issues caused by legacy technology are wasting vast amounts of time for people working in some of the most important areas of the public sector. Legacy issues abound At the end of 2019, it was revealed that doctors in a Midlands GPs surgery take 17 minutes to log in to their computer systems in the morning. Why? Because they are forced to use legacy technology in the shape of Windows 7, for which Microsoft no longer provides technical support. Ongoing login issues are just the tip of the iceberg when it comes to the problems of legacy software though. Looking back to May 2017, the NHS was one of many organisations across the world hit by the WannaCry ransomware attack. As a result, 34 trusts were infected and locked out of devices. Almost 600 GP practices were infected and NHS England estimated that as many as 19,000 appointments would have been cancelled as a result of the attack. All of the NHS organisations infected by WannaCry shared the same vulnerability. Like the slow login issue for the Midlands GP surgery, it relates to a reliance on legacy technology. However, in the case of WannaCry, the issue related to the use of unpatched or unsupported Windows operating systems. In other areas of the public sector, where modernisation has been attempted, poor execution has led to relatively new digital services quickly becoming legacy ones. In the case of the Legal Aid Agency, its Client and Cost Management System was rolled out for all civil legal aid work in April 2016 but was then described by the Legal Aid Practitioners Group as having problems ‘at every level’ less than a year later. To avoid new services becoming legacy services almost overnight, organisations should be aiming to build universally accessible and interoperable web applications in an incremental and derisked manner. Legacy AI? When you consider critical healthcare, nuclear energy or air traffic control systems, it doesn’t take a huge leap of the imagination to see how legacy issues in these areas could have tragic consequences. One less obvious but not less significant topic to consider though is how the increasing interest in Artificial Intelligence (AI) and Machine Learning (ML) for automating decisions might be affected by legacy technology. After all, just imagine the potentially disastrous effects in 20 years’ time if an AI algorithm is making benefits decisions or determining who should enter the country, based on a crumbling legacy technology and data infrastructure. We have already seen the biggest technology company in the world launch a new financial product to much fanfare only to find its AI algorithm is discriminating against women . This should serve as a stark reminder to all public sector organisations that they must invest in fixing the fundamentals before they are tempted to develop a headline-grabbing AI algorithm that is built on shaky foundations. Taking action in your organisation The time when legacy application transformation in the public sector could be kicked down the road has passed. GDS, NHSX and the Science and Technology Select Committee have all made it clear that this major risk needs to be addressed urgently. The responsibility for acting falls on Digital and Technology leaders but no one should expect a quick fix. Legacy transformation takes time and requires a series of significant steps to be taken, which all require support from board level. The most pragmatic first step is to look at the drivers of legacy transformation for your organisation. This will not only help your colleagues to understand the urgency of this work but also to start you on the path to developing a strategy that meets your specific needs. This blog post has been co-authored by our CEO Rory MacDonald and Content Consultant Joe Carstairs. It is the first in a series that covers modernising legacy applications . If you are interested in learning more about this topic, our team has released our ‘Modernising Legacy Applications In The Public Sector’ e-book which will empower you to define and implement the right approach to make legacy applications a thing of the past. Download a free copy here. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-03-31"},
{"website": "Made-Tech", "title": "3 important ways to accelerate public sector digital discovery", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/3-important-ways-to-accelerate-public-sector-digital-discovery/", "abstract": "As we’ve learned in my previous posts, accelerating the discovery phase of your digital delivery not only enables you to deliver value more quickly , but it also speeds up learning, reduces the potential for rework, and most importantly, gets your solution in the hands of real users at speed . So how can you accelerate your discovery phase without compromising the insights you gather? In the final post of this series, I’m going to recommend three key tips as to how you can achieve this successfully. 1. Decide what to do first The concept of solving a whole problem for users is a key part of the Government Service Standard . This means considering where your service fits in with the user’s journey, and making sure you deliver a service that meets their needs across all channels. This is important because fragmented services are difficult to use, and users should not have to understand how different organisations and services fit together in order to use a service. But having a complex problem to solve doesn’t mean you have do it all at once. Instead, you need to prioritise the smaller problems within your reach, so that you can deliver value to users frequently and incrementally. This approach can accelerate discovery because it means that teams are empowered to make key decisions on problem identification and recommendations for next steps more quickly without blockers. This means they are able to see the impact of their work more quickly. Having a senior sponsor to advocate for team decisions at this stage can help to avoid potential red tape, as well as frame expectations clearly around the solution. 2. Widen your team diversity Diversity is a critical part of our experience as humans, because it exposes us to different perspectives and challenges our biases. But it’s especially important in the public sector, because organisations must serve every member of society.  This is exactly why widening your team diversity is an important way of accelerating discovery — but it’s a step that’s often overlooked. Widening team diversity means that project teams need to think outside of the box when selecting the right people to help solve a problem more quickly. This may include recruiting team members from the service user group, or people from a non-digital background. Building a team that includes these non-digital perspectives means that teams are more likely to understand the context and constraints of a project far more quickly. They’re also more likely to avoid the pitfalls of groupthink, and are more comfortable with uncertainty, meaning they can progress more quickly through discovery. 3. Take a multidisciplinary approach Accelerating your discovery phase hinges on understanding and quantifying the problem you need to solve from a number of perspectives. During inception and kick off, this means taking a multidisciplinary approach to define the problem, map users and capture existing assets, as well as confirming ways of working, roles and responsibilities. We’ve identified three key elements where teams can speed up their processes and optimise their ways of working to accelerate discovery. Research Service mapping Technological review Research Research helps teams begin to build a picture of service user needs, and may also unearth unexpected or unknown details — but this process can take time to complete. Accelerating discovery in the research phase means that teams should research in sprints, and continually ask themselves whether or not they’ve learned enough to start designing. Everyone should get involved in this part of the delivery — including non-researchers such as developers — and teams should share findings little and often. Accelerate this by: Finding new ways to understand service user pain points more quickly, like listening in on phone calls. When we worked with Hackney Council, we accelerated the research phase of discovery by listening to calls between service users and caseworkers. This removed the need for large numbers of interviews, but helped us understand the user needs far more quickly. Service mapping Developing an as-is service blueprint enables teams to understand the user journey and their key pain points, as well as helping teams identify changes that are more cost-effective and better meet user needs. Accelerate this by: Engaging staff users in a short workshop, allowing them to map out the initial service and their experiences of using it. This enables teams to gain an understanding of the as-is service and pain points more quickly through the eyes of the users. Technology review When considering building a new service, teams must conduct a full review of any potential technological constraints, including existing technical skill sets and capabilities at the organisation, skills gaps, how the as-is technical infrastructure fits these skills, and cost of change. Accelerate this by: Using your service blueprint to help you make recommendations and possible temporary solutions. When we worked with UKRI to develop a deadline-driven migration and decommissioning strategy, we used the as-is service blueprint to help us propose temporary solutions such as APIs and microservices to help them migrate and decommission legacy technology while still delivering value. How we can help accelerate discovery in your organisation We’ve helped organisations including the NHS, Ministry of Justice, DVLA and many more accelerate their digital service delivery at all phases to improve the lives of millions of citizens across the UK. We recently published our newest whitepaper: Accelerating discovery in public sector digital service delivery. Download it here for free , or get in touch with us to find out how we can help you through discovery and beyond. Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-05-25"},
{"website": "Made-Tech", "title": "Why evolution is the new transformation: 3 key takeaways", "author": [" Camille Hogg"], "link": "https://www.madetech.com/blog/why-evolution-is-the-new-transformation-3-key-takeaways/", "abstract": "Until recently, the public sector wasn’t one typically associated with embracing new technologies and ways of working. Burdened with legacy systems , restricted budgets and chronically under-resourced, it has been constantly behind the curve when it comes to tech. But things are starting to change — and the key to continuing this momentum lies in digital evolution. In a recent talk with Digital Leaders: Insight Live Public Sector , our Head of Local Government Glen Ocskó offered his take on this topic. He explained why the only way to move forward — literally — in the public sector lies in digital evolution, not transformation. Hit play to watch, or read on for our three key takeaways. 1. Digital evolution is a direction, not an end goal Glen started out by explaining that one key problem with the idea of digital transformation is that it implies a clear end point, or a one-time, unidirectional change. Digital evolution, on the other hand, refers to a continuous, intentional process that accelerates an organisation’s rate of digital adoption and change. From a practical point of view, digital evolution means organisations must research and define an overall digital strategy as their first priority. But, as Glen cautioned, it’s about reframing evolution as a continuous process or direction, not a one-time event. “It involves defining an overall strategy for where you want to go,” he noted. “Even if you don’t know the overall destination — you’ve got a clear direction. We should be evolving to have more adaptable, interactive and interoperable platforms and systems that can evolve and change as the world changes itself.” 2. Covid-19 has turbo-charged public sector  digital progress Change doesn’t happen in a vacuum — and often, we need an outside influence to drive it. And over the course of the past year, we can attribute a lot of public sector digital change to the Covid-19 pandemic. “[Covid-19] has been awful, but it has turbo-charged digital evolution,” Glen explained. “We’ve seen several years’ worth of development taking place in several months. This would not have been possible without the challenges we’ve had to face. Moving people to remote working, making sure data was available to them in a secure and safe manner, and linking together systems that previously were locked away in physical locations behind firewalls and physical doors.” Glen added that these challenges have allowed the public sector to boost the speed and pace of its digital evolution to a depth never seen before. As a result, we’ve had to evolve quickly in response to new, unforeseen challenges. 3. Evolution is constant process — but we must keep challenging it Evolution, historically, is a process that follows cycles of constant change, growth, and repetition. And it’s precisely our understanding of this process which defines how we move forward with the changing tech landscape for the public sector. “It’s a similar process that needs to be trusted and repeated,” explained Glen. “We need to constantly go back and challenge ourselves to ask if we’re still best meeting the needs of our service users. Are there new tools out there to explore? Have expectations of our users changed?” “As the world changes, we need to look at what we’ve done afresh and reimagine it,” he added. “We need to retain the essence of what was good from the previous versions, but see if there’s a better fit for the modern world.” But, Glen cautioned, challenging our processes and systems doesn’t always mean we need to change all the time — nor should we focus on pet projects. Instead, we should focus on evolving the systems around these projects, creating interoperable ecosystems where our digital estates feed into each other and nurture each other. Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-05-18"},
{"website": "Made-Tech", "title": "Made Tech Team Interview: Harry Trimble, Head of Design", "author": [" Camille Hogg"], "link": "https://www.madetech.com/blog/made-tech-team-interview-harry-trimble-head-of-design/", "abstract": "This month we continued our Made Tech Team Interviews series with our Head of Design Harry to better understand his role and take a look at what he likes most about working here at Made Tech. Our User-Centred Design team helps public sector organisations design and deliver good public services that focus specifically on outcomes for people and society at large. The team does this by conducting user research to develop a detailed understanding of user needs, designing and prototyping user-centric solutions, and considering aspects such as content and interaction design to ensure accessibility. If you’d like to watch the full interview, you can see it here . Q: Outside of work, what hobbies do you have? A: I’m quite into trying to learn the trumpet at the minute. I think it’s a quite joyful instrument, and my partner got me some lessons a few years ago — so I’m just trying to keep up with that. I’ve also got this website of photographs of wheelie bins called govbins.uk , which has sort of mushroomed a bit in the last few years. People submit their own photos to it now! What was really just a silly side project turned into something else — so that takes up a bit of my time (but not too much, luckily!) Q: So tell us about your life before you joined Made Tech. Had you worked in the public sector before? A: Quite a lot of my professional life has been in the public sector. I worked in the NHS for a couple of years, particularly a clinic that treated people with gambling problems, and I was their first non-clinical member of the team. There was also Tim who ran reception, and he was the person I worked with most as we worked on the most common things people would come to the clinic with. Then I went to go and work at the Government Digital Service after that for a few years. I mostly worked on design patterns on the design system — they didn’t have a name then, but we called it Patterns and Guidance. Then, I worked at my friend’s design agency — we mostly worked with tech companies like Google DeepMind. Before I came here, I worked at the Red Cross for two years, mostly working on things like fires and floods, and a lot of stuff on the Civil Contingencies Act, which ties the Red Cross in with the government when big emergencies happen. And now I’m here! Q: How long have you been at Made Tech? How did you find out about us? A: This is week 5. I think it was this time last year when we were finding our feet being in lockdown. I saw a tweet about the virtual visits tool that we really quickly built for an NHS trust, and I was just really impressed with the speed and quality it got built. I thought there was something interesting there. Tom Taylor, who’s one of the Market Principals here at Made Tech, got in touch because I’d requested one of the books. We had a coffee in Bristol and I asked him about design roles — things followed on from there really! Q: What was your interview process like? A: At first it was a lot of informal chats, because I didn’t know anyone at Made Tech really. I chatted with Luke, the CTO and I talked him through some of the work I’d done in the public sector. Then we had a couple more formal interviews, I met more members of the team and I came here a few months later. Q: What does your role here involve? A: I guess it’s three different things. One — it’s a creative role. What should public services be like? How should they achieve outcomes for people? How are we going to really make technology work for people? Ultimately, design is the point at which humans meet technology. That’s a big part of my role — defining the creative vision for how we do public services and design them. Technology is really blurring our ideas of what institutions are, the boundaries between us, their responsibilities, and the speed at which stuff can happen. The second one is more like team management, so spending time with people who report into me, understanding how to help them grow in their roles and what things need to happen in their teams, and create a structure for people to grow into. The last one is operational — so pay bands, line management, the big and small stuff. I think they broadly fall into those three buckets. Q: What do you like most about your role? A: The variety of work Made Tech does. I love the public sector, and even just this morning in our weekly design meeting, we had people working in teams looking at support for witnesses in court cases, safety beacons at sea, a care register. As a step on from that, it’s making space for others to do really good work. I get a real buzz from making space for people to do design. It’s like the invisible bit of design — everyone sees the interesting prototypes and things we use. But actually the space that needs to be made to enable people to do really great work — that’s just as important. Q: What are some of the most challenging things about this role? A: I think trying to strike a balance between stuff that needs to be addressed really urgently with thinking and being strategic. I guess also design more broadly during the pandemic. There’s time away from screens when a team’s trying to get to grips with a shared understanding of a problem or idea. It’s so difficult to do that at the minute — I’ve had to make a point of turning Slack off. Q: Do you have any advice for someone who wants to join your team? A: First, have less things in your CV! We make a point of letting people share an example of their work — we want to understand how you work and how you think. Another one is try and have fewer words. There’s that old design idiom of ‘less is more’ and it genuinely is, especially if we’re looking at lots of applications. If someone has really spent time on describing what they’ve done and what they’re interested in really crisply, I always give a sigh of relief. Lastly, talk about what problems you’re interested in. The public sector has so many interesting problems to work in and on, so having a curiosity about that helps us understand you’re interested in problems. Q: What do you like most about working at Made Tech? A: I really like the emphasis on problem-solving. I’ve worked in a couple of places where there has been an attitude that we need to have enough decisions made so we can start building stuff. I’ve seen that there is a super-appetite here that engineers want to solve problems and be involved in research. I think that’s something I’ve really enjoyed. Q: Do you have any books or resources you’d recommend for someone interested in your team? A: Someone who is interested in design might already be aware of this, but there’s a great book called Good Services by Lou Downe. It’s a really great grounding in what makes a good service. Most design books focus on process rather than actually talking about the thing we’re designing, and Lou really nails what actually is a good service. For people who are more interested in the design management side of things, I’d recommend Org Design for Design Orgs by the O’Reilly house. I’d also recommend The Making of a Manager by Julie Zhuo, who was the first designers at Facebook — she’s got a really human way of writing, which is nice! If you have any more questions for Harry about his role here, you can get in touch by reaching out on LinkedIn . Additionally, if you are interested in joining our team, you can view our current open positions here . Be sure to stay tuned for our next Made Tech Team Interview coming next month. Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-05-21"},
{"website": "Made-Tech", "title": "How accelerating discovery can benefit your digital service delivery", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/how-accelerating-discovery-can-benefit-your-digital-service-delivery/", "abstract": "When considering the benefits of accelerating discovery in digital service delivery, we need look no further than Formula One for a powerful analogy. On the track, the drivers may rule supreme, but the car they speed to victory in comes a close second. From its lightweight frame to its stripped-back controls and aerodynamic design, it’s nothing like the vehicles most of us own and drive every day. But every single feature on it exists with one sole purpose: to speed over the finish line and take the title. It’s the same principle in the discovery phase. When we accelerate discovery, it forces us to focus on the outcomes that matter the most and deprioritise those that don’t — and this can lead to a whole host of additional advantages. In my previous post in this series, I explained what it means to accelerate discovery . In this post, I’m going to outline the four key advantages to accelerating discovery. Accelerating discovery can be useful to help you: Narrow focus on the most important problem See results sooner Accelerate team and individual learning Reduce rework and waste Let’s dive in and take a deeper look at these tangible and intangible benefits of accelerating discovery. Narrow focus on the most important problem When you run an effective discovery, you’re likely to find several problems that need solutions at once. But it’s impossible to deliver at pace if you focus on all of them at once. Accelerating discovery enables teams to focus on outcomes, not output. This means they are more quickly able to identify and deliver the highest value outcome first and avoid analysis paralysis. See results sooner When delivery teams spend less time in the discovery phase, they can test their hypothesis with real users sooner. This means they can see the results of their solution, and collect valuable feedback on what works, and what doesn’t. Seeing the fruits of your labour on an incremental basis helps you know if you’re solving the problem in the right way, giving you a direction of travel. This saves time, money, and crucially, allows you to pivot more quickly when your solution isn’t working. Accelerate team and individual learning Failing fast and learning quickly is a key part of the GDS agile principles — so it comes as no surprise that when you accelerate discovery, it paves the way for quicker learning, too. And we know that the faster our teams learn, the more confident and high-performing they are likely to become. Accelerating discovery has a direct advantage for team and individual learning because it enables teams to test their solutions and learn what works — or doesn’t work — more quickly. But this learning process also applies to non-technical skills too; it enables your team to be more innovative and find new ways of speeding things up. Reduce rework and waste The process of accelerating discovery is guided by lean principles. These are focused on reducing waste and rework, as well as delivering at pace and using evidence and data to make decisions. By accelerating discovery and getting your solution in the hands of real users, you are able to avoid significant waste and rework down the line, and focus on delivering a solution that truly meets the needs of your users. Want to know more about accelerating your digital service discovery? This blog post shares just a small snapshot of our recently published whitepaper: Accelerating discovery in public sector digital service delivery. To find out more about how you can accelerate discovery in your organisation, click here to download our latest whitepaper for free . Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-05-19"},
{"website": "Made-Tech", "title": "3 things you need to know about accelerating discovery in digital delivery", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/3-things-you-need-to-know-about-accelerating-discovery/", "abstract": "The public sector has never been in a more exciting period for digital transformation. While this transformation has meant we’re better able to respond to evolving user needs more quickly, we’ve barely scratched the surface of how technology will transform this sector in the future. But in the public sector, where resources, time, and budgets are often tight, delivering new services that deliver value quickly is critical. This is where understanding how to accelerate your discovery phase can be an enormous advantage. But what does accelerating discovery really mean? In this post, we’ll take a look at the agile delivery lifecycle and explain why accelerating discovery is more than just speeding things up. What is the discovery phase in digital service delivery? Whether we’re planning a big purchase or a new digital service, very few of us are likely to rush in and make a snap decision. Instead, our first port of call is to do research. If we think about buying a car for example, we might ask for recommendations from friends, or go for a test-drive. This helps us understand the problem we want to solve and make an effective decision. In the Government Digital Services framework, this research and understanding phase is known as discovery. In discovery, a team’s primary goal is to build a picture of the needs and requirements of service users. This involves understanding how they’re using the service, what they’re trying to achieve, and if there are any changes that can improve this experience. It also involves identifying any risks, constraints, issues, assumptions and dependencies, as well as understanding how a new digital service may fit alongside existing ones. This information helps teams decide if the benefits of solving the problem outweigh the costs of building or improving the service, and enables them to move to proposing solutions for the problem and testing on real users. What do we mean by ‘accelerated discovery’? Discovery is a critical part of any service delivery because it enables us to understand the problem in more depth, research the needs of different user groups, and validate what you need to do based on evidence. But when we talk accelerating this phase of the agile lifecycle, we don’t just mean speeding things up. Accelerated discovery is about seeing results sooner The key purpose of accelerating your discovery phase is that it means you can see results sooner. This means you can see whether or not your proposed solution solves the problem, allowing you to change direction if necessary. Accelerating discovery also means that teams are able to deliver value closer to the start, rather than finding out whether a solution will work at the end of an eight-week discovery process. Accelerating discovery focuses on lean processes… The average discovery phase usually takes around eight to 12 weeks to complete — sometimes longer. However, for organisations that need to solve a problem as soon as possible, this can often be too long. Instead, accelerating discovery takes a leaner approach. By streamlining processes such as interviews and insight collection, teams can move more quickly into the testing phase. This means they reduce the amount of potential rework or waste later on. … But it isn’t about prioritising speed over quality There can be a misconception that accelerating discovery means rushing this crucial phase of digital delivery — but that’s not the case. Accelerating the discovery phase means that teams spend less time exhausting every angle or option of a solution. Instead, they ask themselves often if they have enough information to start testing some of their hypotheses. This doesn’t mean that research ceases early either — research is carried out on a continuous basis throughout the delivery to continue testing solutions at every stage. Want to learn more? This post provides just a snapshot of our recent whitepaper: Accelerating discovery in public sector digital service delivery. To find out more about how to accelerate discovery in your organisation, download the whitepaper now for free . To find out more about how we can help your organisation accelerate your discovery phase and beyond, click here to get in touch. Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-05-14"},
{"website": "Made-Tech", "title": "Launching Making Tech Better — a podcast by Made Tech", "author": [" Camille Hogg"], "link": "https://www.madetech.com/blog/launching-making-tech-better-a-podcast-by-made-tech/", "abstract": "At Made Tech, our core mission is empowering public sector organisations to improve how they work through technology. It’s something we care about deeply. By helping companies modernise their working processes, upskill their people, and put their focus on the users, we’ve been able to see first-hand how our clients can improve not only their software delivery, but also the lives of millions of people. Along the way, we’ve had some great conversations on some of the most important challenges facing the public sector as it undergoes digital transformation — and it got us thinking…What does making tech better truly mean for us? For us, it means helping companies improve their processes and culture and helping organisations to make better decisions using their data. It also means empowering teams to collaborate compassionately on creating high quality software that helps the people that really matter — the users. This is why we’re really excited to announce the launch of our very first podcast dedicated to this topic. Launching on 13th April, Making Tech Better will bring you some of the best minds in tech and public sector organisations for a series of conversations all about improving software delivery. Led by our Lead Engineer Clare Sudbery alongside other members of the Made Tech team, Making Tech Better will explore topics including team collaboration, digital modernisation and transformation, and diversity to empower digital leaders to improve software delivery in the public sector. To start things with a band, we’re launching our first four episodes simultaneously — which you’ll be able to listen to from 13th April. Here’s what to expect… Do your teams embrace learning? That’s the question Clare aims to tackle in episode 1 when she chats to Jessica Kerr about the concept of symmathesy, and building better collaborations. In episode 2 , Clare explores what transformation means to software engineer Paula Paul , including its common pitfalls and a few basic principles to follow. What’s the difference between coding for fun and coding for your job? Clare asks Google engineer, author, and renowned Stack Overflow contributor Jon Skeet this very question in episode 3. Episode 4 sees Clare chat to Kit Collingwood , Assistant Director of Digital and Customer Services in the Royal Borough of Greenwich, about using tech to serve every citizen. With so much exciting content to delve into, we’ll be launching a new episode every fortnight to keep the conversation flowing. Some of our upcoming guests include former Ministry of Justice CTO and Head of Digital Dave Rogers, and Anne-Marie Imafidon, founder of Stemettes, a social enterprise dedicated to inspiring the next generation of young women in Science, Technology, Engineering, and Maths roles. Subscribe Make sure you don’t miss a single episode by subscribing to Making Tech Better on Spotify or Apple , and take a look at our landing page for upcoming episodes. What do you want to hear us talk about next? We’re passionate about having great conversations that inspire others to improve their software delivery. We want you to come away from our podcasts with a smile on your face, and a bunch of useful new ideas in your back pocket — not just for software delivery, but for your whole life. That’s why if you’ve got a good idea for an upcoming topic or a guest suggestion, we’d love to hear from you! Click here to fill in a short survey, or reach out to us on LinkedIn or Twitter . Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-04-08"},
{"website": "Made-Tech", "title": "Making Tech Better podcast: Transforming organisations with Paula Paul", "author": [" Camille Hogg"], "link": "https://www.madetech.com/blog/making-tech-better-paula-paul/", "abstract": "Modernisation is a key part of existence as humans. Over the course of our lifetimes, we’ve seen technology transform every aspect of our daily lives, from smaller things, such as how we consume entertainment, to bigger things like travel and ways of working. Often this step change in how we do things, like watch TV from the palm of our hand, can be revolutionary. But sometimes, this change can be uncomfortable — particularly when it changes how we work. But why do organisations need to modernise , and what kind of things do they need to consider before doing so? In our second episode, Clare chats with Distinguished Engineer Paula Paul . Paula’s career spans four decades of helping organisations modernise. Hit play below to listen, or keep reading for three key takeaways from their conversation. Modernisation methods evolve over time — but it looks a lot like moving things around As a modernisation expert, Paula has spent her career helping people simplify the way they work. But in the 80s, that looked very different to how it does now. “When I look back in time, everything that I’ve done in software engineering has been for modernisation of something,” she reflects. “My very first job at IBM was moving people from mechanical drafting boards, pen, and T-square to CAD systems — that’s modernisation. “We have a better way of doing things now [but] to me, it is still about changing the way of working and adopting better ways of working,” she adds. “I feel as though a lot of modernisation has always been about moving something somewhere.” Modernisation is a process that includes people, too Clare comments that companies may sometimes approach modernisation for the wrong reasons , such as in response to scaling problems. Paula agrees, adding that organisations undergoing modernisation need to focus on the adaptive changes that their people experience as well as their tech. “I always say that the technology is the easy part, and having the people think about their roles differently is quite complicated. That’s an adaptive process. Technical problems tend to have a very concrete solution; adaptive problems require change of people and thinking. “If the people involved don’t change their ways of working, it’s often treated the same way as existing systems, and fall back into the same problems of release cycles and slow change,” she adds. “So again, it’s technology plus people — [that’s] the holistic solution.” Digital transformation isn’t about a ‘big bang’ moment Clare notes that often, organisations can have a “big bang” approach to the process of modernisation. “Instead of trying to change it, you just throw it away and replace it,” she says. She adds that this is not only risky, but costly and time consuming. Instead, she usually recommends that clients replace their system piece by piece in an iterative way . This means that over time, organisations build a new system over the top of their old one. This is known as a strangle pattern. Paula agrees, adding that to find a starting point, organisations must consider their goals and what generates value for them. Then, they can solve both the technical and adaptive problems, and gradually introduce new ways of working. Stay tuned for more great conversations every two weeks, and subscribe on Spotify , Apple , or Google for the latest episodes. If you’ve got a suggestion for a guest or some feedback you want to share, fill in our short survey here . If you liked this content, check out our three key takeaways from episode one featuring guest Jessica Kerr . Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-05-07"},
{"website": "Made-Tech", "title": "Making Tech Better podcast: Enhancing team learning with Jessica Kerr", "author": [" Camille Hogg"], "link": "https://www.madetech.com/blog/making-tech-better-jessica-kerr/", "abstract": "From the moment we’re born, we’re on a quest for knowledge. First it starts with our senses, connecting us to the things in our environment that we can see, touch, hear, smell, or taste. Then we pick up things like social behaviours, how to do things, safety and danger — all by exploring and interacting with the world around us. This drive to explore and learn never leaves us. And when we enter the workplace and work alongside a team, the relationships we develop, the tasks we accomplish, and the tools we use all have an impact on how we learn, both as individuals, and as an organism. In the first episode, our Lead Engineer Clare chats to Jessica Kerr , a software developer with 20 years’ experience and independent consultant for Jessitron LLC , on the topic of how we learn in teams, and how we can enhance that mutual learning taking place. Hit play below to listen, and keep reading to find out our top three takeaways from this episode. Teams are learning organisms When we work as teams, we’re always learning from one another, whether intentionally or not. On this point, Jessica introduces the topic of ‘symmathesy’, a concept coined by anthropologist Nora Bateson. Symmathesy describes a learning system formed of entities or parts that are constantly learning. “Our teams are like this,” she notes. “We’re all learning from each other. Of course, we learn from each other as people but also, the software learns from us because we change it. We learn from the software when it throws exceptions or changes data. Also, our tools are part of this system, because we don’t change the software without tools.” We’re always learning for the next version of ourselves This constant loop of mutual learning is integral to how we work and live right now in the present. But Jessica says it also informs our future selves, too. Every action we take has a ripple effect on the world around us, such as code we write, or conversations we have with people — but it also has an impact on what we’ll do next. “What we learned that day is going to affect what we’re able to do in the future,” Jessica says. “So, the whole symmathesy has changed.” She adds that this consciousness of future learning and change could be particularly important for managers: “If you are pushing the developers that you are responsible for to only change the code, to only work on the software, and not giving them space to do it better and better and thoughtfully, in a way that also produces new versions of themselves with additional abilities, then you are really hampering your entire future.” Enhancing team learning is all about communication When asked how we can build learning teams that are better collaborators, Jessica responds that effective communication and collaboration are essential, as well as ensuring that teams have the right tools to enable them to accomplish their goals. She suggests that teams spend 80% of their time building and deploying software and fulfilling their wider purpose, but that the final 20% of their time should be spent looking internally at improving processes and ways of working for the future. Clare adds that we should recognise and celebrate how we learn, and be more conscious of how we can learn from our tools and systems. Stay tuned for more great conversations every two weeks, with upcoming guests including Kevlin Henney, Aino Vonge Corry, and Esther Derby. Make sure you subscribe on Spotify , Apple , or Google to make sure you never miss an episode — and if you’ve got a suggestion for a guest or some feedback you want to share, fill in our short survey here . Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-04-23"},
{"website": "Made-Tech", "title": "Made Tech Team Interview: Gráinne Blake, Senior Delivery Manager", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/made-tech-team-interview-grainne-blake-senior-delivery-manager/", "abstract": "This month we continued our Made Tech team interview series with our Senior Delivery Manager Gráinne to better understand her role and to feature the great work she has been doing. Our Senior Delivery Managers help public sector organisations accelerate digital delivery, drive efficiency and ultimately improve society. They do this by leading multiple large-scale teams, ensuring the effective delivery of complex, high-risk products and services and building and managing strategic relationships. If you would like to watch the full interview, it is available here . Q: What hobbies do you have? A: In this day and age I live a pretty small life given the time that we live in. I come from a musical background — I studied music — so normally it’s anything musical. You’d normally find me in a pub with a pint of Guinness, my bodhran (a musical instrument), or just listening to some good records on my record player from the 70s. Aside from that, walking my dog and I also picked up some chess during lockdown, so I play that remotely with my brother in Ireland. Q: Before you joined Made Tech had you worked in the public sector at all? A: I had worked with some public sector clients but not that many and primarily in the private sector. The public sector clients that I did work with weren’t really working within the GDS Framework as much, so not much exposure in that respect. Q: How do you find working in the public sector? A: I really like it. To be honest, I had a few reservations when I was joining Made Tech about the public sector but I find the work really rewarding and it’s that whole idea of impacting a citizen’s life that really hits home for a lot of people at Made Tech. Q: How long have you been at Made Tech? How did you find out about the company? A: I’ve been here for eight months — sometimes it feels like a lot longer! I actually used to use Made Tech as a supplier seven or eight years ago and I stayed in touch. I heard about the growth plans into Bristol and thought there would be an opportunity to join the team. So I stayed in touch with Chris Blackburn , our COO, and the rest is history. Q: What was the interview process like for you? A: It was really simple and quick, which I really liked. So there was a telephone interview in the first instance, which was pretty much a 30-minute call just to get to know a bit more about the role and to see if I was a fit. Then it was a face-to-face interview for an hour and then a job offer on the same day which was great. It was a great Friday! Q: What does your role here involve? A: It depends day to day. It’s a mix between hands-on delivery management and hands-off. To give a bit more context of what I mean by that, it’s probably best if I talk about a few of the deliveries I’ve been on just to give it more context. When I joined Made Tech I was involved in an eight-week discovery to essentially create a migration and decommissioning report for an end client. That was very hands-on and delivery management kind of acting as a consultant in that discovery like just one of the team to deliver that outcome. I’ve also been involved with a beta and delivering that service to end users. It’s been quite different from the discovery but again very hands-on working with the team to get them to be high-performing and to deliver outcomes for that client. Then, the third was a rainbow style engagement where Made Tech have landed really great engineers to contribute to client outcomes within a multi-supplier environment. So it really depends from delivery to delivery, which is fun! Q: Which has been your favourite project that you’ve worked on so far? A: It’s tricky, but it’s nice to be able to think about that after eight months. I suppose it’s the beta delivery that I spoke about and the reason for that is I come from a product management background. I love thinking about user needs and why we’re doing what we’re doing and so it really hits home with me in that respect. I’ve really enjoyed working on that delivery. Q: What do you like most about being a Delivery Manager? A: I like the variety of work. I’ve just talked about three very different deliveries that I’ve been involved with, and I love knowing that this week probably won’t be the same as next week and if it is there’s probably a problem that I need to work on. It’s really great having that variety of work from week to week or day to day. Q: What are some of the most challenging things you find about the role? A: Probably the prioritisation for me — it’s something I’m constantly trying to work on. Making sure that you look at deliveries right and you think about what’s going to deliver the most value for an end user. That you look at your diary and you ask what’s going to deliver the most value today. So it’s an ongoing prioritisation of my work efforts and where best to put my time considering I’m working on multiple things concurrently and asking that question on a daily basis. Q: Do you have any advice for someone who wants to be a Delivery Manager? A: I think it depends if someone’s trying to get into delivery management or tech just as a whole. There could be some people who are really technical looking to make the move across to delivery or they could be coming in completely green, so it would depend based on that and obviously it’s not binary. There’ll be a lot other situations too but I suppose in the first instance it’s all about people, right? So I’d really recommend getting in touch with people through friends of friends just to get those insights in terms of what delivery could mean on the ground and getting that context. I think past that, I’d read up. There’s lots of great books and tools out there so getting them around modern ways of working and modern software delivery would be really beneficial. Q: Do you have any books or resources you would recommend for someone interested in this position? A: I’m reading what I consider a really great book at the minute — it’s Marty Cagan’s Empowered . It’s a really great book about how to be credible and parenting a high-performing team. It’s a really brilliant book to understand what are the things that make a team high performing and how to get there. I’d really recommend any of Marty Cagan’s books, he’s great! Q: What is the biggest challenge you’ve had to face since lockdown in this position and how did you overcome it? A: This is going to sound really simple but switching off from work. The first four or five months I was working in my sitting room. I finally finished this little cave I have for me now but switching off is really tricky. But even with this little room now, I find it hard in the evenings and I’m having to be really strict with myself in terms of when is work-time and when is me-time, and trying to make that physical separation. Dog walks are great! Q: What do you like most about working at Made Tech? A: The culture. I haven’t worked in a place like this where there’s such a community. Just as an example, I really feel like if I had an issue on a delivery I could put my hand up, reach out to even the delivery community and I’d get a handful of people coming back to me to try and help. I haven’t necessarily had that before in a working environment. There are lots of different silos in other companies or even silos within silos, so it’s really refreshing to do this differently at Made Tech. If you have any more questions for Gráinne about her here, you can get in touch by reaching out on Linkedin . Additionally, if you are interested in joining our team, you can view our current open positions here . Be sure to stay tuned for our next Made Tech Team Interview coming next month. Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-04-15"},
{"website": "Made-Tech", "title": "Driving Modernisation with the DVLA — 3 key takeaways", "author": [" Camille Hogg"], "link": "https://www.madetech.com/blog/driving-modernisation-dvla-webinar/", "abstract": "Assembling and running a high-performance, collaborative team can be one of the biggest challenges in building a customer-facing service. And when you couple that with a complex technical delivery, it’s a process with a lot of moving parts. So how do you get these elements right and shape culture and process at the same time? In our latest Made Tech Talks Webinar , Scott Edwards, one of our Lead Engineers here at Made Tech, talked through a recent project we ran with the DVLA. We worked as part of a blended team to help the organisation modernise its Driver’s First Provisional service — all with the goal of helping new drivers get on the road. To watch the full webinar, hit play below — or keep on reading to find out our three key takeaways. Break down team barriers and encourage collaboration One of the key challenges the DVLA faced was centred around their collaboration between roles. And for Scott, driving collaboration meant breaking down these barriers. To encourage collaboration, Scott and the team implemented a SCRUM framework, which helped team members more easily understand the business needs of the project, split work into sensible iterations, and work as a single team. The team also focused on working together in the same room — albeit virtually — as well as championing transparent communication, which enabled them to raise any issues or challenges so that they could improve processes moving forward. Expand the scope of automated testing Another key challenge the DVLA faced was that as they continued to expand their service, their testing scope just kept getting larger. In traditional models of development, developers will typically build a feature, and then hand it over to testers to find any errors. But as the service scales, features are added, and the scope of testing increases, this can become a blocker to getting new services live on production. Scott explained that making this testing process more efficient starts with automation, and building up cumulative testing capability over time. By encouraging knowledge and skills sharing between roles , developers can feel empowered to learn how to test, while testers can likewise be empowered to develop code that automates the testing process. This means that teams can quickly identify errors and integration issues, but it also ensures lower operational costs and organisational risks. Introduce new ways of working slowly Ultimately, when you’re implementing new processes, it can be tempting to roll things out as quickly as possible — especially on the technical side of things. But, Scott cautioned, we need to remember that this process is also about cultural change, and will mean teams will need to learn to work together in a new way. Scott suggested being understanding as to how people’s identities intertwine with who they are at work. He also recommended that teams implement knowledge-sharing sessions, where team members can ask questions, or share domain-specific expertise, as well as hosting retrospectives to dive deeper into projects after completion. Going through this process slowly will help rally people around a shared goal, encourage deeper knowledge sharing, develop trust between team members, and allow teams to modify and optimise their processes over time. If you enjoyed our Driving Modernisation with the DVLA webinar, be sure to keep your eye on our page for more upcoming events. For more great conversations on all things tech in the public sector, take a look at our new podcast Making Tech Better . Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-04-21"},
{"website": "Made-Tech", "title": "How to make mob programming work for your whole team", "author": [" Tom Foster"], "link": "https://www.madetech.com/blog/how-to-make-mob-programming-work-for-your-whole-team/", "abstract": "When we think about the impact of good teamwork, business consultant and author Ken Blanchard said it best: “None of us is as smart as all of us.” Recently, I was working on a project with my team members where we needed a new way to collaborate on solving a problem. After building a service in a dev environment, we needed to promote it to a different testing environment for the external testing team. In the process, we came across a few error messages that we needed to solve, which was blocking other work on the sprint board. To get a more complete understanding of what we needed to do to fix them, we all got on a call at the same time and worked through the problems together. This meant we were able to fix the errors and deliver the service more quickly. This is a software development approach known as mob programming. But what is mob programming — and how can you make it work for your whole team? In this blog post, I’ll explore the benefits of this approach, its potential pitfalls and provide some tips on how to avoid them. What is mob programming? Mob programming is the software development approach whereby all the team are all looking at the same screen, and working together to deliver a feature or solve a software problem that impacts the whole team. Traditionally there is a driver, who controls the keyboard, whilst the other team members direct what should be typed. It requires all members of the team, including stakeholders, to get involved with the process to ensure that all communication is in one place. This avoids the need for having separate meetings about the feature away from the mob. What are the benefits of mob programming? Mob programming may be undertaken by a team for a number of reasons, including delivery of a large feature, solving a bug that is impacting development across the team, or to upskill all members of the team on the different aspects of the project. Mob programming has been very beneficial in government departments as well as the private sector (including us at Made Tech!). Here are three key reasons why we think it’s useful: It cuts down on communication issues During mobbing sessions, each member of the team is available to participate. In addition, all knowledge is shared amongst the team, so there are no additional meetings or email chains. A lot of teams have benefited from having stakeholders involved in the mobbing sessions. This is because then the team can directly query those who have knowledge of customer journeys and the application of this software by the business. It fosters shared ownership In teams, there can be a tendency for one person to become the authority on a certain section of the code base, but with mob programming the whole team is involved from the beginning increasing collaborative software development. The entire team is responsible for each line of code, meaning that there is a shared sense of ownership of what is being produced. It encourages more frequent knowledge transfer In a mobbing session, upskilling and knowledge transfer occurs more frequently as the team develops together, meaning information isn’t siloed but shared. Junior members of the team are encouraged to actively participate in the process to soften the learning curve. This increased collaborative style of working means that there isn’t a reliance of one engineer to complete a ticket, but a competency among all the team members to be able to pick up tasks. What are the pitfalls of mob programming? As with all software development practices, there are likely to be a few potential issues that arise in implementation. A regular retrospective and constant feedback loop is required to ensure that any concerns are caught quickly and addressed. Some common pitfalls include: Managing different opinions Too many voices and inputs can make it difficult to address the problem at hand. Each member of the team will have their own individual area of interest, and if these are not navigated successfully, it can mean the mobbing sessions can produce little output. Team comfort levels Certain individuals may feel uncomfortable working in large teams, and more inexperienced members of the team may feel that they have less to contribute so don’t participate in the discussions. This leads to more confident senior members of the team driving the sessions, negatively impacting the shared knowledge aspect of the mobbing sessions. Waning enthusiasm Often the enthusiasm for mobbing sessions is high at the beginning, but quickly fades as the team suffers from burnout. Mobbing sessions can be quite intensive, which can lead to the team suffering from feeling exhausted and later abandoning the process. 5 top tips for a successful mob programming session It’s important to enter mobbing sessions with an open mind and a collaborative spirit, otherwise you’re more likely to encounter a few of the pitfalls listed above. This requires all individual team members to buy into the process otherwise it cannot succeed. Here are our top five tips on how to get the most out of the mobbing process: 1. Focus on communication If the mobbing software development approach is new to the team, then focus efforts on ensuring communication flows well within the team. Make daily goals very manageable, and focus on building confidence in the process. Having a consistent feedback loop run throughout the sessions will highlight issues more quickly, and allow for the team to find a way to navigate these concerns early on. 2. Establish clear requirements Strive towards a minimal viable product rather than a perfect end product from the session. This ensures that the team has a clear set of requirements they have to meet, and a shared common goal to achieve. 3. Take regular breaks Mob programming can be intense and draining on team members, so taking a break every half hour, even if just for 5-10 minutes, can vastly decrease the burnout that engineers often experience. 4. Be mindful of the driver Being in the driving seat can be quite a lot of pressure, and swapping every 10-15 minutes is encouraged. When the driver is regularly rotated, this encourages other participants, including junior members, to become active participants. 5. Be ready to pause the timer for the driver Being a driver is a key feature of the learning process in a mob session, but often, a technical discussion can carry on longer than just a couple of minutes. When this happens and no active development is taking place, pause the timer so that your driver has the same opportunities for learning as the rest of the team. Mob programming has been an enormously beneficial tool I’ve used in many projects — but I’ve also encountered many of the pitfalls in my career. I hope that these tips will help you to implement mobbing sessions that work for every member of your team — increasing collaboration, cohesion, and communication. If you liked this post, check out our previous blogs on development strategies: Stop buying specialists and start delivering outcomes Documentation should meet outcomes too Don’t estimate without evidence Tweet 0 LinkedIn 0 Facebook 0", "date": "2021-04-06"},
{"website": "Made-Tech", "title": "The Business of Test Automation", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/the-business-of-test-automation/", "abstract": "We take pride in our extensive adoption of test automation. We see significant business value and commercial advantage both for us and our clients in providing automated test suites across our deliveries. We understand that computers are more efficient than people at running repetitive, and usually monotonous, tasks and look to harness this efficiency to reduce the amount of time it takes to make a change and be confident enough in this change to deploy it to a production environment where it can start delivering value ('cycle time'). What do we mean by Test Automation? When we talk about Test Automation, we mean that alongside programming the functionality behind a project, we also program tests that check that this functionality is working. Think of it as double-entry bookkeeping for programming. These tests that grow in number as the project is built out can then be run at the click of a button to assert that any new changes haven't adversely affected previously working functionality. Automated and manual testing In a more typical manual testing environment, time is often spent having testers (or on larger applications; a small army of testers) manually working through key user journeys ensuring they function as expected. Every time a change is made to the project, this functionality needs to be tested again, requiring a tester to repeat a test they've already run – perhaps tens of times previously. We still see and derive value from performing an element of manual testing on projects. However, we believe the human investment in testing is better used in focusing on testing that interaction works well and that the application visually appears and works as expected – things that computers struggle to do as well as humans. The cost of test automation Incorporating test automation increases the time spent on a project during the early phases when a test suite is being built up. However, on a project of any longevity, we believe dividends are quickly paid. The ability to make changes with confidence that you haven't affected part of the project that you may know nothing about is a significant advantage. Software applications (and certainly the sort of applications we typically produce) are rarely treated as immutable. Changing software to meet new business needs and challenges is a constant; and we believe it should be encouraged and embraced. Test automation is one of the primary enablers in allowing confident ongoing modification of software to meet changing commercial requirements. The cost of fixing software defects increase exponentially the later in the process they're caught. Research suggests that a defect identified by a secondary tester costs 10 times as much as a defect that is identified by the originating developer and that identifying a defect in production costs between 10 and 25 times as much when compared to a defect identified by the originating developer. Empowering everybody on the team who is making changes with the tools to get early-and-often feedback should significantly reduce the ongoing cost of change. Fast feedback Fast feedback (we'd typically be aiming for a test suite to take less than 10 minutes to complete) allows developers to make large changes to applications, which we often see as necessary to deal with changing requirements, without fear that they've introduced adverse side effects elsewhere in the application. We see that automated testing also increases the general level of quality and maintainability of underlying application code. Types of testingIn addition to automated acceptance testing, which we use to assert that functionality works as expected, we rely on automated tests to check for common security defects and to assert that we're producing code that adheres to accepted style and complexity (or rather, simplicity) standards. Test automation has been around in some form since the 60's. As web applications become more business critical and become viewed more widely as true software products, an extensive range of tools have emerged to provide many different flavours of automated testing against the many different aspects of web application development. We still see the adoption of test automation as surprisingly niche across the range of organisations that can be involved in the delivery of web applications. We'd anticipate that the significant commercial advantages afforded by test automation will show shifts over the coming years as the costs, both financial and in lost opportunity, of hard-to-maintain software cause challenges for more organisations. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-07-31"},
{"website": "Made-Tech", "title": "We’ve moved!", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/weve-moved/", "abstract": "Having spent our first four years based in Shoreditch, the past three in the heart of Hoxton Square; we sadly outgrew space at our previous digs and have made the giant leap across the river to Bankside. We're now calling home a former metal box factory on Great Guildford Street, just a stone's throw from Tate Modern. 2012 was a year of healthy growth for Made that saw us just about double in size, resulting in some \"creative furniture arranging\" to squeeze in more desks than we could really get away with. Since moving south a couple of weeks back we've been stretching our legs, arguing office decor, and building furniture in a space that is close to four times the size of our previous Shoreditch abode. You can now find us at Unit 309/310, 30 Great Guildford Street, SE1 0HS. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-01-22"},
{"website": "Made-Tech", "title": "R&D with Javascript Maps", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/research-into-javascript-maps/", "abstract": "We're currently working on a project for Global Stories which requires a fairly complex map interface. We've been looking into alternates to Google Maps, as we'd like to find something which gives us a bit more control over the look and feel and the interaction within the map. So far we've been exploring the following mapping services: PolyMaps A collaboration by SimpleGeo and Stamen . It's available as an open source Javascript mapping framework via Github . TileMill A downloadable mapping applicaiton by Tile Box. It makes use of Carto stylesheets for customising the look and feel of the map and can be output for usage on the web. Cloud Made CloudMate provides a point and click interface for customising the look and feel of their map. The styles are stored online and can be loaded into 3rd party applications via their API. Styled Google Maps Google provide a stylised version of their maps via their styled maps wizard, the colours and visibility of all elements can be altered dynamically at runtime through the Google Maps JS API v3. Our current favourite is PolyMaps , which looks to have been implemented nicely using SVG, has lots of ability to customise the look and feel and not to heavyweight. Check out some of the nice examples, like the tile visibility algorithm and the Flickr Where On Earth implementation. Tweet 0 LinkedIn 0 Facebook 0", "date": "2011-05-03"},
{"website": "Made-Tech", "title": "Telegraph Top 50 Websites", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/telegraph-top-50-websites/", "abstract": "The Telegraph have just announced their top 50 websites and we're delighted to see our Lucky Voice HOME site is one of them! Lucky Voice HOME is an online karaoke service, which turns your computer into the ultimate karaoke machine. There are thousands of tracks available with new content added on a weekly basis. We delivered the karaoke software and underlying technology platform for this project. Tweet 0 LinkedIn 0 Facebook 0", "date": "2010-05-25"},
{"website": "Made-Tech", "title": "NIVEA MEN website launched", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/nivea-mens-website-launched/", "abstract": "We're excited to have just delivered the template for the global rollout of the NIVEA MEN website . The new site has been designed, built and tested to work across mobile, tablet and desktop devices, with the responsive design re-prioritising the layout according to the screen size. The site also incorporates a number of modern best-practice SEO techniques, such as the inclusion of the schema.org vocabulary to help search engines better understand and rank the content. The first sites in the portfolio to launch are NIVEA MEN UK , NIVEA MEN Turkey and NIVEA MEN Spain . Over 2013 the same template will be rolled out across all 58 markets, including in a number of dialects that use the right-to-left text direction. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-03-19"},
{"website": "Made-Tech", "title": "Moving to Hoxton Square", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/moving-to-hoxton-square/", "abstract": "We're sad to say that we've expanded beyond the available space in our current Ryvington Street office, forcing us just round the corner in to Hoxton Square. We're excited to have more space for client collaboration sessions and generally a bit more room to stretch our wings. Tweet 0 LinkedIn 0 Facebook 0", "date": "2009-10-21"},
{"website": "Made-Tech", "title": "Made redesigns and relaunches Surface View homepage", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/made-redesigns-and-relaunches-surface-view-homepage/", "abstract": "As part of our ongoing relationship with Surface View , ahead of the Grand Designs Live event last week we designed and built a completely revamped homepage. In addition, we provided a number of performance-related updates including further SEO refinement, integration with KissMetrics, a content-managed customer competition component and proactive shopping cart monitoring tools. As part of a sprint cycle, we delivered everything from flat designs right through to the build of these incremental enhancements; from conception to launch in two weeks. We integrated the site with KissMetrics to help provide deep-level insights on key customer journeys such as the customisation and checkout process. The competition engine allows for the business to create their own competitions using the CMS and includes tooling for automated entry exports. We're looking forward to applying this high-performance sprint-based approach to a number of enhancements across the site during 2013. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-05-10"},
{"website": "Made-Tech", "title": "Made appointed to develop new Robert Moore Studio website", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/made-appointed-to-develop-new-robert-moore-studio-website/", "abstract": "We're delighted to announce that Made has been appointed by Robert Moore Studio to develop their new company website. Tweet 0 LinkedIn 0 Facebook 0", "date": "2010-12-01"},
{"website": "Made-Tech", "title": "Made appointed for Jordans and Ryvita Technical Delivery", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/made-appointed-for-jordans-and-ryvita-technical-delivery/", "abstract": "We're excited to announce that following a competitive pitch, we've been appointed as technology partner to deliver both the Jordans and Ryvita web builds in 2013. The technical roadmap that we presented sees a shift towards a Service Oriented Architecture (SOA) with loose-coupling between site components such as products, competitions and recipe databases. This will allow the sites to share common components, and provides a framework in which, without the need for rebuilding entire sites from scratch, the sites can evolve with the replacement of single components as they reach end of life. Part of the work we've delivered to date has involved a deep-dive in to the site analytics to truly understand how users are interacting with the site. Coupling this with an understanding Jordans commercial objectives, we've since been working on a user experience that more closely meets the needs of real user behaviour. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-01-24"},
{"website": "Made-Tech", "title": "Keeping It Clean", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/keeping-it-clean/", "abstract": "During my transition from fledgling independent developer to fully functioning member of the team at Made a few years ago, it quickly became apparent that the code I was writing needed a lot of improvement in terms of how legible it was to others. Left to my own devices, the temptation when writing code was to try and establish my own style as though I was the Terry Pratchett of web development, but joining a team made it easy to see how quickly all of that needs to go out of the window in favour of simply writing code that is easily understood by as many people as possible. CSS is an easy place to start, as the team at Made have adopted the style guide put forth by Google , a few key points being: Listing style declarations alphabetically Previously, I would try to group vaguely related declarations together: padding: 10px;\nborder: 1px solid #000;\nmargin: 10px;\nfont-size: 22px;\ncolor: #fff;\ntext-decoration: underline; But as projects go on it's not unusual for styles to be added, removed or altered, and often by someone other than the original author, who has their own ideas of how declarations should be listed. Listing declarations alphabetically removes all of that fuss immediately, and makes it very easy to find the line you're looking for whenever you need to dive back into a project's CSS. border: 1px solid #000;\ncolor: #fff;\nfont-size: 22px;\nmargin: 10px;\npadding: 10px;\ntext-decoration: underline; Whitespace and indentation This is a practice that extends to languages beyond just CSS, but the idea is to pay attention to the way your declarations are spaced out, so that you don't end up with headache inducing one-liners like this: #myElement{border:1px solid #000;padding:10px;color:#fff;font-size:14px;display:block;} And instead making sure each declaration is on it's own line, indented with two spaces, and the value is separated from the property by one space, like so: #myElement { \n  border: 1px solid #000; \n  color: #fff; \n  display: block; \n  float: left; \n  font-size: 14px; \n  margin: 10px; \n  padding: 10px;\n} Those two approaches are very easy to adopt, and make a noticeable impact on the readability of a stylesheet. At Made, the entire team works in Sublime Text 2, and we've found the following configuration options particularly useful in regards to the above: \"trim_trailing_white_space_on_save\": true\n\"tab_size\": 2\n\"translate_tabs_to_spaces\": true Of course, there is also SASS to help make your CSS look a lot prettier. It adds a lot of tools to your CSS arsenal, particularly variables and what are known as \"mixins\". For example, variables allow you to declare a colour as a variable, such as: $company_red: #880000; You can then reference that variable throughout your stylesheet, applying it to as many elements as the design dictates, the benefit being that if the design is later changed and the colours with it, you then only need to update the variable with the new colour value rather than chasing down every instance of it in your stylesheet. Mixins are similar to variables, but much more complex, in that you can define entire blocks of style declarations that you anticipate using repeatedly throughout your stylesheet: @standard_block { \n  border: 1px solid #ff0000; \n  color: #000; \n  display: block; \n  padding: 10px; \n  position: relative;\n} That then becomes available to use on any element you wish to apply those styles to, and your code ends up looking much DRYer: .foo { \n  @include standard_block;\n} Another personal favourite feature is the ability to import stylesheets into each other. Working in a text editor like Sublime Text 2, I permanently have the directory tree view sitting there in the left hand column, so I can jump between files easily. With the ability to import stylesheets into others, I can break stylesheets down into individual files that each have a specific responsibility and name them accordingly, a crude example being: fonts.scss\ncolours.scss\nheader.scss\nnews_articles.scss\nsidebar.scss\nfooter.scss I'm then able to navigate easily to the relevant set of styles as and when I need to. By then importing all of those stylesheets into a \"master\" stylesheet, I'm able to have my element require just that stylesheet, so the user's browser only makes one request and gets all the styles in one file. There are many more ways to make your CSS look more readable, and it's worth spending a little time on it now, as you'll reap the rewards later when you need to revisit old code. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-07-18"},
{"website": "Made-Tech", "title": "Made appointed by Global Stories to deliver digital strategy and website", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/made-appointed-by-global-stories-to-deliver-digital-strategy-and-website/", "abstract": "Made is pleased to announce that we have has been appointed by Global Stories to define their online offering, provide a digital strategy and to implement a new website and content management system. We'll be working closely over the coming 6 months to position Global Stories as the online destination for understanding global social challenges. Tweet 0 LinkedIn 0 Facebook 0", "date": "2011-03-26"},
{"website": "Made-Tech", "title": "Lacoste Championship", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/lacoste-championship/", "abstract": "What a month! We've been working flat out on a new project Lacoste Championship , which was launched earlier today by Jameela Jamil and Reggie Yates at the Lacoste store in the Westfield shopping centre. We were tasked with developing a modern version of the classic game Pong for use online , on Facebook , in-store and at the ATP Championships (which Lacoste were sponsoring). The in-store games were developed to run on the Wii console and will be available to play in the Lacoste Westfield store for the next 10 days, before being moved across to the APT tournament at the O2 centre for the rest of the month. If you get a chance to go to either, have a play for the chance to win a VIP tennis tournament weekend. Tweet 0 LinkedIn 0 Facebook 0", "date": "2010-11-08"},
{"website": "Made-Tech", "title": "Jordans Website Launch", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/jordans-website-launch/", "abstract": "We're excited to have launched the new Jordans Cereals website . We undertook the information architecture, interface design and development of the mobile-responsive website that provides a platform that can be shared across the Jordans and Ryvita Company portfolio of brands. The delivery heralds a move to a Service Oriented Architecture (SOA) with loose coupling between the different components of the site (Products, News, Competitions etc.), allowing a more evolutionary approach to changes in future and providing the platform for component sharing across multiple brand sites. The site was designed from the ground-up with mobile and touch-screen tablet devices on an equal footing to the desktop experience. The calls-to-action have been scaled with oversize hit areas, the majority of graphics are either vector or font-based to allow them to appear sharp on high pixel-density screens such as iPad, and content has been prioritised appropriately for the different use-cases of browsing on desktop and mobile devices. The build has incorporated some significant best-practice techniques in the delivery of modern web applications: A significant automated functional test suite that includes close to 1000 individual test steps. Automated security scanning of every build. Automated deployment and rollback using the Capistrano application. Automated code quality checking using the Cane tool. Deep-level application monitoring and error reporting using the New Relic platform. Almost exclusive use of vector-based graphics for super-sharp display on high pixel-density devices. Integration of the schema.org microformat to help search engines better understand the content on the website. Significant in-memory caching to completely remove database reads from the website frontend, coupled with cache sweeping in the CMS to ensure caches are invalidated as content becomes stale. We're looking forward to releasing further content areas to the site over the coming months. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-07-15"},
{"website": "Made-Tech", "title": "Appointed to develop Lucky Voice Pod Software", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/appointed-to-develop-lucky-voice-pod-software/", "abstract": "We're excited to announce that Made has been approached to develop the touchscreen karaoke software for Lucky Voice bars and pods. Over the coming months we'll be designing and building a platform that involves synchronising large amounts of audio data across venues up and down the country, along with a suite of management software for controlling the karaoke terminals. Tweet 0 LinkedIn 0 Facebook 0", "date": "2010-04-14"},
{"website": "Made-Tech", "title": "Global Stories Website Launch", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/global-stories-website-launch/", "abstract": "We're pleased to announce that we have launched a brand new website for startup Global Stories . The website provides aggregated content that showcases interesting stories from NGO’s and businesses on topics such as humanitarianism, climate and politics We've been working closely with the team at Global Stories over the past 9 months to understand their requirements and to create a website and content management system that allows their business to grow. Tweet 0 LinkedIn 0 Facebook 0", "date": "2012-02-03"},
{"website": "Made-Tech", "title": "The Made Code Dojo", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/the-made-code-dojo/", "abstract": "Following on from our adoption of the mob programming technique , we’ve been keeping our skills sharp by gathering the entire team in the Made Code Dojo (aka the meeting room) every fortnight and doing some kung fu. Mob Problems Mob programming sessions see us dive into our codebase and pull something out that needs refactoring, and it has been a useful tool for us in the sense that we can go back to old code with the benefit of hindsight and apply lessons we’ve learned in the time since it was originally written. However, refactoring code within a large Rails application (as most of our projects are) becomes problematic when it comes to testing our changes, as actually running the test suite is incredibly time consuming, taking huge chunks out of the hour that we’ve allocated for the session. With that in mind, we typically leave testing aside and the session becomes a discussion of other best practice techniques, which whoever is currently piloting the keyboard will demonstrate onscreen. The aim of the sessions is to take what is learned from those discussions and apply them in our day to day work. Coming out with production ready code is absolutely not a priority. Enter the Dojo Whilst looking into mob programming and how best to run those sessions, I discovered what are known as “code katas”: “A code kata is an exercise in programming which helps a programmer hone their skills through practice and repetition.” – http://en.wikipedia.org/wiki/Kata_(programming) This idea of honing our skills with a programming exercise outside of the context of existing work sounded very appealing, and I then discovered the notion of code dojos, where a group of people would band together to come up with the best solution to a given kata. I was especially fond of the way the London Python Code Dojo ran their dojo, particularly their Team Dojo approach and the dev bait (free beer and pizza), as it encouraged a more playful atmosphere. How We Do Kung Fu The format we use for the sessions is much the same as the one we use for mob programming: We have a limit of one hour regardless of the task We divide that hour evenly between each team member present Each team member spends his or her time at the keyboard writing code that is displayed for all to see on a large screen Where Code Dojos differ are that we focus specifically on good TDD first, and that we actually attempt to produce something that works by the end of that hour. I really wanted to incorporate the dev bait idea too, but since our Code Dojos happen early on a Wednesday and drinking in the morning is generally frowned upon, we have pastries in place of beer and pizza. I’ll usually select two or three katas from one of the many lists of exercises out there, and present them to the group to vote on which one we do that week. The resources below all have plenty of decent katas worth tackling: /r/dailyprogrammer Project Euler Coding Dojo And we can't forget about Uncle Bob, who has a couple of meaty katas to sink your teeth into. Previous Code Dojos at Made have seen us trying to do maths with Roman numerals , testing for pangrams and palindromes , and playing Rock, Paper, Scissors, Lizard, Spock against the computer . We’re not always successful – sometimes we’ve simply bitten off more than we can chew within the hour, or we’ve gone on a refactoring rampage after having actually produced something that works and not quite tidied everything up, and it’s not unusual for us to get caught up in a debate about the best way tackle a particular problem. What we’ve found, after a few months of running the Made Code Dojo, is that the sessions are an entertaining way to share skills with the other people on the team: it helps the more experienced members keep their skills sharp and consider alternative points of view, whilst the less experienced have the opportunity to get stuck in and learn very quickly as the solution to the kata unfolds before their eyes over the course of the hour. Also everyone likes pastries. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-05-28"},
{"website": "Made-Tech", "title": "Made appointed to develop the full e-commerce solution for Surface View", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/appointed-by-surface-view-to-develop-e-commerce-platform/", "abstract": "We're pleased to announce that we have been appointed to develop a new eCommerce solution for Surface View. We'll be working with the in-house product management team to help define their approach to online commerce, with a view to rolling out a new website and supporting applications over the coming months. Tweet 0 LinkedIn 0 Facebook 0", "date": "2011-03-06"},
{"website": "Made-Tech", "title": "ActiveRecord associations and the valid? flag when building", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/activerecord-associations-and-the-valid-flag-when-building/", "abstract": "I was debugging a mysterious bug using RSpec and it took a while to figure out what exactly was going on. I thought that I would document my findings here, as I could not find much information on the subject and ended up digging into the ActiveRecord source code. The problem itself may not be that interesting if you are a seasoned Rails developer, but the larger point that I am trying to make is that you should not be scared to look into the source code if you do not understand something.  I have even ended up deep in the MRI C code at times to get a better understanding of certain functionality. The line that seemed to be failing was a simple shovel << of a child object into its associated parent object.  The association simply was not being created. The << operator If you are not familiar with the ActiveRecord << method, below is a brief explanation. @category.products << @product Is the same as calling: @product.category_id = @category.id\n@product.save! You can look at its actual definition in the ActiveRecord source code. #.../gems/activerecord-3.2.18/lib/active_record/associations/collection_proxy.rb\n\ndef << (*records)\n  proxy_association.concat(records) && self\nend\n... The code in question Say we have a model ProductCategory, which has many Products (the idea of Products is actually composed of a set of Single Table Inheritance (also known as STI) models but I will leave it out for brevity.  However at the time, this did add some complexity to the debugging process and was considered as a potential point of failure). Each product also has many variants. It would look something like this in the models: class ProductCategory < ActiveRecord::Base\n  has_many :products\nend\n\nclass Product < ActiveRecord::Base\n  belongs_to :product_category\n  has_many :variants\nend\n\nclass Variant < ActiveRecord::Base\n  belongs_to :product\nend In the controller, I was creating a product, then trying to add variants to the newly created product, and finally add the product to the product category. The nested form was submitting all the required data for the related objects to be created. ProductController < ApplicationController\n  def create\n    @category = ProductCategory.find(params[:product_category_id])\n    @product = create_sti_product(params[:product])\n    @product.try_add_variants params[:products][:variants]\n    @category.products << @product\n  end\nend The method try_add_variants on the product model may seem unneccessary, and like something that could be handled in the controller but bear in mind that this was an STI setup and the different Product models each provided their own implementation of the method. class Product < ActiveRecord::Base\n...\n  def try_add_variants(variants_params)\n  ...\n    variant_params.each do |v|\n      variant = variants.build v\n      variant.save if variant.valid?\n    end\n  end\nend Note that 'variants' in this case is actually product.variants but is called directly on the instantiated product, so the reference to products can be omitted. I had a very simple controller spec that looked like this: ...\nit \"creates a product\" do\n  expect {\n    post :create, product_params\n  }.to change(category.products, :count).by(1)\nend This was my spec that I used to debug the problem, and was failing, so that was good.  Automated tests are the best way to debug a problem, even if you just use it to drive the application, which is much faster than trying to do it manually through a browser. I found that the try_add_variants method was causing the product to become invalid. This would set the valid? flag to false, and prevent it (or anything related to it) from being saved. p @product.valid? # true\n@product.try_add_variants params[:products][:variants]\np @product.valid? # false At first this seemed strange because the try_add_variants did not *really* touch the products, only the decendants of the products. This invalid state in the variant object is stored, and would cascade up to the product valid? flag, causing it to be set to false too. I decided that I would like to find the piece of code in ActiveRecord that was responsible for setting the valid? flag of the parent objects. I used pry-debugger, and followed the execution of the code deep into ActiveRecord.  I was happy to eventually stumble onto this code below. A look inside ActiveRecord The actual code that deals with setting the parent valid? flag when calling build can be found here: # .../gems/activerecord-3.2.18/lib/active_record/autosave_association.rb\n\n# Returns whether or not the association is valid and applies any errors to\n# the parent, self, if it wasn't. Skips any :autosave\n# enabled records if they're marked_for_destruction? or destroyed.\n\ndef association_valid?(reflection, record)\n  return true if record.destroyed? || record.marked_for_destruction?\n\n  unless valid = record.valid?\n    if reflection.options[:autosave]\n      record.errors.each do |attribute, message|\n        attribute = \"#{[reflection.name](http://reflection.name/)}.#{attribute}\"\n        errors[attribute] << message\n        errors[attribute].uniq!\n      end\n    else\n      errors.add([reflection.name](http://reflection.name/))\n    end\n  end\n  valid\nend I then commented out the entire definition of the method and just returned true.  Indeed, the error moved from the model to the database which also had a constraint on it.  The debugging p statements in the client application both evaluated to true so I was sure that this was the piece of code responsible for setting the flag. I often edit the actual gem source code to add debugging statements to it. You can find the path of the gem quite easily with bundler.  In this case you can do: bundle show activerecord After you are done debugging, you can restore the gem to its former state by calling: gem pristine activerecord Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-05-21"},
{"website": "Made-Tech", "title": "The UK government is better than industry in IT delivery", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/the-uk-government-is-better-than-industry-in-it-delivery/", "abstract": "Back in 2011, in the wake of the car crash that was the NHS records system, an interesting thing happened in the UK Government's IT program. Shifting from the \"bigger is better\" mantra that we still too often see desired from public and private sector IT projects, the rollout of the new gov.uk portal (the website that rolls up government functions from benefits to passport eligibility) adopted an approach that was previously the reserve of Silicon Valley startups, and often smaller, faster-moving technology companies. The project gathered a team of about 100 people together in a single Central London location and adopted delivery methods that focus on getting a minimal version of the product in to the hands of real customers as quickly as possible. The first release of gov.uk, realised 12 weeks after the team first assembled, and one day late, contained only a small subset of the desired features: but did provide a number of complete features that customers may have found useful. Since the first public launch, there have been hundreds of updates released to the site that have added new features and changed existing ones. In doing these fundamentally simple things, UK Government leapfrogged the vast majority of UK industry, most of whom are still delivering big, complex projects to timescales that should have long been deemed unacceptable for any modern organisation. IT projects, and particularly large-scale IT projects have significant complexity. They're also being delivered in to increasingly complex and competitive environments, where the need to demonstrate business agility and move quickly often delivers a significant commercial advantage. Until it's in the hands of customers, a product delivers no value. The number one focus of almost every project needs to be to get a minimum version of the product used by customers as quickly as possible. In the Lean Startup world, this is usually referred to as the Minimum Viable Product. There are two particularly powerful advantages that are realised from delivering early: Fast feedback Until you've got your product in to the hands of your customers, you're guessing about the features they want from it and you're guessing at how they will use it. The sooner you can get feedback from real customers the better. This feedback can prove (and indeed disprove) assumptions you've made and because you've got it early enough, you've not wasted too much time heading in the wrong direction. Always release ready In more traditional projects, you'll often see long running \"testing\" phases bolted on to the end of the project. If the product is being released often, testing needs to happen often. This gives much greater transparency as to the true state of the product – by keeping it in a release-ready state, when a feature is marked as done: it's done. For too long big IT projects have been approached in much the same way as bridge building and other civil engineering undertakings. Organisations around the globe bear the scars of SAP and CRM rollouts that have taken years to complete and have never delivered their promised value. Too often IT and its friend complexity are the barrier to businesses behaving with agility, to seizing commercial opportunities in acceptable timeframes, and to delivering truly outstanding customer experiences. The UK Government has demonstrated that things don't need to be this way. In the digital age big projects don't need to be delivered slowly. If your organisation isn't able to keep pace with government, your shareholders should be in revolt. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-05-13"},
{"website": "Made-Tech", "title": "Getting To Grips With Icon Fonts", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/getting-to-grips-with-icon-fonts/", "abstract": "Icon fonts are monochromatic images which are incredibly useful, scalable and easy to implement. In this post I’m going to talk about why we began using them, why you should use them, and how to do so. I was introduced to icon fonts last year, and initially I was reluctant to pay attention to them because I’d spent so much time learning how to create and use icon sprites, and then how to combine them into a single sprite sheet. Plus, with icon fonts only being able to support monochrome images I thought it best to keep all of my sprites in one place, rather than segregating them into multicoloured icon sheets and monochromatic font icons. However, they quickly became part of the solution to another problem, which was how to present icons and images nicely on high-resolution screens (such as retina displays) when traditional graphics were showing a noticeable drop in quality. We at Made, and indeed the entire industry, are seeing a noticeable shift towards mobile in terms of what devices visitors are viewing our projects on, such that we can’t afford not to go the extra mile on details like these. That notion, coupled with the fact that the “flat UI” design approach has become increasingly popular, has meant that icon fonts are now an essential part of anything we work on. Benefits Since icon fonts are vector based, like regular fonts, they’re much better equipped to handle being scaled up. Last year we implemented icon fonts in the Byron website [url], and the below side-by-side comparison is taken from there, as viewed on an iPhone 5. Additionally, the font files themselves are tiny, typically less than 10kb on an average project, and all icons are contained within a single file, allowing you to keep your HTTP requests to a bare minimum and your page load speed down. It’s worth looking at Chris Coyier’s demo to get a really good idea of what can be done with icon fonts as far as scaling them up and down, changing colours and adding drop shadows. Adding Them To Your Project To create the font files, you’re going to need to use one of the various tools out there that do just that. There are several online apps , but I was introduced to the whole concept of font icons via IcoMoon , and the user experience there is very intuitive, with tons of free icons to choose from (along with a few premium bundles), and the ability to save different sets of icons to your user account comes in very handy when working on projects where new icons are added at a later stage. IcoMoon also gives you the ability to upload your own vector-based images (aka SVGs) and have them converted to fonts, which is extremely useful on projects where the design demands custom icons that can't be found online. If you're not a convert already, try using icon fonts in your next project and I'm sure you soon will be. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-04-14"},
{"website": "Made-Tech", "title": "Mobile enable your email newsletters", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/mobile-enable-your-email-newsletters/", "abstract": "It’s no secret that over the past couple of years there’s been a very obvious upward trend in the use of mobile devices to visit websites. Only this week Google suggested that by December 2014, 50% of web searches will be conducted on mobile . The trend for accessing websites on mobile, which we currently see account for around 20% of traffic, is already outstripped by people reading emails on mobile. Across our client-base, we’re seeing in excess of 30% of email newsletters opened on a mobile device. If newsletters are sent out of usual office hours, we see this rise to typically over 60%. However, coupled with the increasing use of mobile to read emails, we generally see a reduction in click-throughs. For non-mobile optimised newsletters, we’re seeing around 50% fewer click-throughs on newsletter campaigns when the newsletter is opened on a mobile. For a channel that still typically drives 3x the conversion rate of social, we see this as a problem, and a problem that’s likely to increase as mobile adoption for email reading further increases. If you’re going to the effort of sending out an email newsletter, you need to ensure the newsletter works well for customers who open and interact with that email on a small-screen mobile device. – The techniques that are used to mobile optimise a website, particularly responsive web design , can also be applied to newsletter templates, meaning the same email will appear optimised for large screens when opened on a desktop client and optimised for small screens when opened on a mobile. While this technique will work well for most of the popular email clients (if your audience is US or Europe based, probably over 80%), there are unfortunately a number of popular email providers such as Gmail and Yahoo! Mail who don’t yet support responsive layouts for newsletter templates. While we expect this to change over the next twelve months or so as we see more newsletter senders adopt responsive email templates, there is still a current need to ensure desktop newsletter templates can be easily read on mobile devices. We’ve started to roll out responsive newsletter templates for a number of our clients. When coupled with concise copywriting and high-engagement subject lines, we’re seeing these optimised newsletters drive in the region of 30% more click-throughs when compared to non-mobile optimised newsletters. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-04-21"},
{"website": "Made-Tech", "title": "Best Practice Checkout Experience", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/best-practice-checkout-experience/", "abstract": "We have put together a series of articles on 'best practice approach', for some of the user experience challenges we run into regularly. In this first article, we focus on designing the perfect checkout experience. We assume the customer has added a product to their basket and their intention is to purchase. We walk through each of the touch points from cart page, right through to delivery to the customers door. Before we begin, there are a few fundamentals that should apply throughout. Mobile – Make sure you provide a mobile optimised experience. We're seeing traffic from mobile devices exceed 25% on most of our client websites. It's a must-have for any new checkout process. Speed – Whether it's page load times, delivery estimates or customer support response times, speed is vital to delivering a truly excellent customer experience. Secure – Security is the #1 concern for online consumers. You need to shout about the measures you have in place to reassure the customer that their details are kept safe. Data Driven – Ensure you track every interaction between the customer and your checkout process. This will help you to improve and optimise your processes moving forward. **Cart Page** Once a user is on your cart page, the primary objective should be to encourage them to move forward to the first step of the checkout process. As a base, you need to ensure that typical customer concerns are addressed: Does the site appear secure? Is it clear what payment options are available? Have you provided a delivery estimate? Is your returns policy visible or easily accessible? Once these have been addressed, we can start looking at functionality and experience design: Is there a clear call to action to move forward into the checkout? (We recommend double proceed buttons, above and below the cart) Are the checkout call to actions visually different to all other on-page graphics? Have we highlighted any discounted product prices or savings to the customer? Is it easy for the customer to update quantities or remove items from cart? Is stock availability highlighted? Can a coupon code be added or removed easily? Can you 'time limit' the cart? Once we've addressed these points we should have a design (or wireframe) which we can test the effectiveness of. We recommend running the following tests: A Blur Test to verify the CTA buttons stand out. A User Test which asks customers to answer the following questions: Will the site accept AMEX cards? Will I be able to checkout using my Paypal account? How long will the delivery take? Do I have to pay for shipping costs if I return the item? Providing all goes well, we would expect to see 20-30% of customers proceed from cart through to the next step in the process. On products with lower price points, we would expect this to be slightly higher, somewhere between 30-50%. **Login & Guest Checkout** At this point in the funnel, the customer is committed. You should be focusing on reducing page clutter and looking to minimise exit options. To do this, it's important that any questions that pop into your customers head can be answered without them leaving the purchase funnel. We tend to use an 'enclosed' checkout, which hides the main navigation, search and other standard layout features. We include links to FAQs, Return Policy & Customer Services; launching these as overlays or popup windows, so as not to take customers out of the funnel. We introduce a 'progress indicator', so the customer has a visual indication of the number of steps required to complete the purchase, how far along the process they are, and so they have a navigation device they can use to move back to a previous step. We also like to provide a basket summary on this page. During user testing we found customers tend to like the reassurance of the basket summary. Removing this caused a small percentage to users to click their 'back button' to review their cart. You should never force a user to sign up for an account. We’ve found the best approach is to capture their email via a guest checkout or social sign-in and let the customer know they can create an account on the final step of the checkout process. Customers who have an account should have the option to sign-in. Once signed-in you should retrieve their previously stored billing and shipping addresses. A word of warning here: If a customer has forgotten their password (this happens frequently) then you should allow them to go through the ‘forgotten password’ process on the same page and not direct them off elsewhere. We’ve found that the vast majority of customers who leave the checkout process for an external ‘forgotten password’ flow do not return. Customers tend to view the ‘guest checkout’ option as the simplest and most straightforward way of proceeding through to purchase. In our experience, it’s the most commonly used, so its vital that you provide this as an option to your customers. As per the cart page, we would recommend you run user testing against your designs or wireframes. Providing all goes well, we would expect to see at least 75% of customers proceed from login through to the next step in the process. Billing & Shipping Details At this point in the checkout process the customer is truly committed. Our job here is to ensure the customer can enter their address as quickly and effortlessly as possible, whilst ensuring we have captured all the correct details enabling us to fulfil the order. It’s important that we remove any duplicate effort, so if a customer's shipping and billing address are the same, they shouldn’t have to enter these details twice. Introducing a postcode lookup is an approach that can simplify things further, though it's important to have a fallback in place for cases where a postcode lookup doesn't return the correct address. Most modern web browsers support ‘autofill’ forms. If you ensure your input fields are setup to use the ‘autofill’ information, this will go a long way to ensuring the customer can proceed through the checkout quickly. It’s common to see customers change their delivery country at this stage. You need to ensure that if you have multiple stock locations, the product the customer has add to their basket is available to ship to their newly selected country. There is a huge amount of complexity that can be introduced by this country switch and this is why it's really important that they have the correct country selected early on in the purchase process – ideally before a customer has add a product to basket. We've often made use of tools that try to automatically figure out where the user is from. It's important to tell the customer that you've done this and given them the option to change, such as in the example below: **Payment** Once the customer has hit the payment page, we can have a high level of confidence they are committed to the purchase. We’d expect to see exit rates of less than 10%: anything above this would suggest technical or serious user experience issues. The focus on this page should be capturing the customer payment details as smoothly as possible. It’s sensible to offer multiple payment options: we tend to use a single credit card processor, Paypal and sometimes Google Checkout. It is important that customers are reminded exactly how much they will be charged before they enter their payment details. Many customers are wary of additional costs such as shipping or taxes being added unexpectedly, so it's important this total is shown.  We tend to favour a basket summary throughout the checkout process. Should this not be available, we would show a payment total at the top of the page. It's important that you only show the payment fields necessary for the type of card the customer is using. You shouldn’t confuse the customer by asking for fields that are only applicable to specific card types. One approach we like is to get the customer to enter their card number and to automatically highlight the card type after the first four characters have been entered , and then hide the unnecessary fields. There is a well known formula for determining the card types. Inline validation and form helpers are critical on this page. Customers will need the CVV number explained and the format for the expiry date etc. We tend to run an initial set of validation in the cart before the order is submitted to the payment gateway which helps to pickup any typos or issues before they are processed. It’s important that you bear in mind your security responsibilities when processing cards. There are a number of regulations (in particular PCI compliance) which enforce rules about how you process and handle card numbers. Providing everything goes to plan, we would expect to see at least 90% of customers complete the payment step. We would recommend that you track any errors closely and feed these through to your data analytics systems for regular review. **Summary & Confirmation** Now the user has placed their order, we should show them an order summary and confirmation page. We should provide reassurance around delivery times and a quick and easy to way to get in touch if they have any questions. It’s not uncommon to see customers get to this point in a checkout process and realise they have made a mistake, so providing a telephone number and support email address enables them to reach out easily. It’s at this point where you may want to introduce the option for a customer to sign up for a full account. By going through the checkout process the customer will have already provided all the details needed to create an account, so providing a quick and easy way to convert this into a full account is a good option. You could also provide the customer with the option to to ‘social share’ their recent purchase. This technique can be particularly successful if a customer is excited about the purchase (think tickets to a gig or an expensive piece of furniture). We’ve also used this step of the checkout process to ask for customer feedback, upsell alternate products and offer a discount on the customers next purchase. You may also want to make the page easy to print out or save as PDF. **Order Confirmation Email** The main purpose of the order confirmation email is to provide customers with a ‘hard copy’ of the order information. It's important that customers have a record of the transaction arrive quickly within their inbox as it provides them with additional reassurance that everything has gone through correctly. It’s also important that the confirmation email is sent from a ‘real’ email address. Do not use a ‘noreply@domain.com’, as it makes a bad impression on your customers: it's unfriendly and uninviting. Why shouldn't they be able to reply to the confirmation email? A better approach is to have the order confirmations (and dispatch notifications below) sent from a customer services email address, so when a customer does reply it can be picked up by your support staff. **Dispatch Notification Email** Once an order is ready to ship to the customer, you should send out a dispatch notification. This notification should include a tracking code which will enable the customer to track the delivery progress. It should also include a summary of the products being delivered, which is especially important if the order is being sent in multiple shipments. As the dispatch notification is the last communication before the products arrive, it's important that the customer can easily find the contact details of the logistics provider, as they may need to get in touch to re-arrange delivery. At this stage in the process it's generally too late for a customer to cancel any of the items being dispatched, but we recommend including details of the returns policy within the email. Explaining what steps the customer will need to take to send the product back and how long they have before the returns policy becomes void. Being upfront and clear at this stage helps the customer to have a good overall experience with your brand. **Arriving Today SMS** Finally, on the day the product is going to be delivered, it’s important you provide the customer with a delivery window. A two-to-three hour time period in which the product will arrive. This helps the customer to plan their day and enables them to re-arrange the delivery if they are not going to be in. We’ve seen businesses use delivery text messages as a way for customers to re-schedule their timeslot. e.g. ‘Text FRI’ to reschedule the delivery for Friday. We think this is a great approach and something that should be adopted by more online retailers. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-04-28"},
{"website": "Made-Tech", "title": "ActiveRecord Refactoring: Concerns", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/activerecord-refactoring-concerns/", "abstract": "ActiveRecord provides a lot of power – if not too much. To that power we add our own business logic to create rather large domain models. Here at Made I've started looking at various ways to tone down the responsibilities of my ActiveRecord models using various programming patterns. In this short series I will be looking at three ways of reducing the responsibilities of your models: Concerns Services (a.k.a. Interactors) Decorators Part One: Concerns The first design pattern for reducing the responsibility of your ActiveRecord models is the concern pattern. Concerns can help reduce repeated logic across models. They also help group certain concerns of logic together. By moving logic into a file and labelling it you are keeping the aforementioned logic away from other code it isn't concerned with. Imagine you have two models named Blog::Post and Blog::Comment. They both have duplicated logic for displaying a long form date: # app/models/blog/post.rb\nmodule Blog\n  class Post < ActiveRecord::Base\n    def long_date\n      date.strftime(\"%A, #{date.day.ordinalize} %B %Y\")\n    end\n  end\nend # app/models/blog/comment.rb\nmodule Blog\n  class Comment < ActiveRecord::Base\n    def long_date\n      date.strftime(\"%A, #{date.day.ordinalize} %B %Y\")\n    end\n  end\nend This clearly violates the DRY principle, that is, Don't Repeat Yourself. The DRY principle is a well known principle these days and I wouldn't be surprised if you're already familiar with using mixins in this instance: # app/models/concerns/blog/date_concern.rb\nmodule Blog\n  module DateFormattable\n    def long_date\n      date.strftime(\"%A, #{date.day.ordinalize} %B %Y\")\n    end\n  end\nend # app/models/blog/post.rb\nmodule Blog\n  class Post < ActiveRecord::Base\n    include Blog::DateFormattable\n  end\nend # app/models/blog/comment.rb\nmodule Blog\n  class Comment < ActiveRecord::Base\n    include Blog::DateFormattable\n  end\nend And that's it. You removed repetition of logic by moving the definition of #long_date to Blog::DateFormattable and then used include to mixin the method to both Blog::Post and Blog::Comment. Logical grouping and naming of concerns There are a few points to make about concerns, the first being logical grouping of methods. In the example above we created a Blog::DateFormattable module. This module clearly will contain date-specific methods. If for example you wanted to add a bunch of methods for formatting the authors name of both Blog::Post and Blog::Comment then you would use another concern module. # app/models/concerns/blog/authorable.rb\nmodule Blog\n  module Authorable\n    def author_full_name\n      \"#{author.first_name} #{author.last_name}\"\n    end\n  end\nend # app/models/blog/post.rb\nmodule Blog\n  class Post < ActiveRecord::Base\n    include Blog::DateFormattable\n    include Blog::Authorable\n  end\nend # app/models/blog/comment.rb\nmodule Blog\n  class Comment < ActiveRecord::Base\n    include Blog::DateFormattable\n    include Blog::Authorable\n  end\nend So now we've added #author_full_name to our models. We're still DRY and our concerns are descriptively named. Testing Concerns The second and final point I want to make is on testing concerns. You have two avenues here, testing the concern directly in isolation or testing the concern functionality in every class spec that includes it. Let's look at testing concerns directly in isolation. # spec/models/concerns/blog/authorable_spec.rb\ndescribe Blog::Authorable do\n  let(:authorable) {Class.new.extend(described_class)}\n\n  before(:each) do\n    authorable.stub(:author => double(:first_name => 'Luke', :last_name => 'Morton'))\n  end\n\n  context '#author_full_name' do\n    it \"should return the full name of the author\" do\n      authorable.author_full_name.should eq('Luke Morton')\n    end\n  end\nend In this example we use the ruby to it's full advantage and create an anonymous class with Class.new and mixin our concern. We then stub #author on our anonymous class which itself returns a double with the #first_name and #last_name methods. We then assert the return value of #author_full_name. That's all well and good but personally I only see this method of testing concerns useful when using the test to guide your design. This hardly ever happens. More often you will define the method in the model and then realise you want to reuse it elsewhere. At this point you've already written the tests for the first implementation so writing a concern specific test to reimplement the function seems somewhat unnecessary. The alternative is to use rspec's #shared_examples_for. Let's start with our spec for Blog::Post: # spec/models/blog/post_spec.rb\nrequire 'spec_helper'\n\ndescribe Blog::Authorable do\n  let(:post) {described_class.new}\n  context '#author_full_name' do\n    it \"should return the full name of the author\" do\n      post.stub(:author => double(:first_name => 'Luke', :last_name => 'Morton'))\n      post.author_full_name.should eq('Luke Morton')\n    end\n  end\nend Now we want to reuse this method in Blog::Comment so we should first move the test into a shared example. # spec/support/shared_examples/authorable_example.rb\nshared_example_for Blog::Authorable do\n  before(:each) do\n    authorable.stub(:author => double(:first_name => 'Luke', :last_name => 'Morton'))\n  end\n\n  context '#author_full_name' do\n    it \"should return a full name of the author\" do\n      authorable.author_full_name.should eq('Luke Morton')\n    end\n  end\nend And update the Blog::Post spec to use this shared example: # spec/models/blog/post_spec.rb\nrequire 'spec_helper'\n\ndescribe Blog::Post do\n  let(:post) {described_class.new}\n\n  it_should_behave_like Blog::Authorable do\n    let(:authorable) {post}\n  end\nend The tests should still be passing. You can then go ahead and reuse the shared example in your Blog::Comment spec. # spec/models/blog/post_spec.rb\ndescribe Blog::Comment do\n  let(:comment) {described_class.new}\n  it_should_behave_like Blog::Authorable do\n    let(:authorable) {comment}\n  end\nend Run the tests and they will be failing. Once we include Blog::Authorable in Blog::Comment our tests will be passing again. Shared examples do not add much more complexity to your tests and ensure that the methods work in the objects that include them rather than in isolation. Usually you want to test a unit of work in isolation – away from everything else – but since ruby mixins are essentially a way of copy and pasting methods into classes I believe they should be tested at the level they are being used. If you can't use a concern method directly why should it be tested directly? Your mileage may vary and you might prefer to test your concerns in isolation and that's cool. The major win is from testing them in the first place! OK, so there you have it. An explanation on using concerns in ActiveRecord models and testing concern logic. Next time I will be discussing an example of using the decorator pattern as an alternative to concerns. Disclaimer One In Rails < 4 you need to add the app/models/concerns directory to your autoload path in your application class like so: config.autoload_paths += %W(#{config.root}/app/models/concerns) Disclaimer Two I didn't even go into using ActiveSupport::Concern and its #include method. Further Reading DHH's introduction to concerns DHH's example gist Using concerns in controllers Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-05-08"},
{"website": "Made-Tech", "title": "Introducing our all new brand identity and positioning", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/introducing-our-new-brand-positioning-and-identity/", "abstract": "Today is an exciting day for us. After months of hard work, we are launching our new brand positioning, identity system and responsive website. It’s a change which marks the start of our next chapter, and formalises our stepping away from \"development agency\" towards \"software services company\". I’d like to take the time to explain a little bit about the thinking behind the changes and what they mean for our clients, partners, and the team at Made. First, a little background: over the past few years we've struggled with our positioning. We were seen as different things to different people. We would  meet designers who would see us as \"web developers\", end-clients who would describe us as their \"digital agency\" and advertising agencies who would describe us as their \"production partners\". None of this aligned very well with what we actually did. We've spent a lot of time re-thinking how we positioned our offering, working out what we we're best at, what we enjoy doing and what are the most valuable services we can provide to our clients. This thinking has helped us to define exactly what we want to achieve with the business and what we want Made to be known for in years to come. We’ve summed it up in this brand ideal: \"Made: creating reliable and intuitive software that helps businesses and people to be more successful.\" This may sound a little fluffy, but one of the things we've realised over the last couple of years is that a business needs a clear direction, or to cite a Bain & Company phrase: true north , to achieve success. With this new ideal, we've defined our true north and are now focused on executing against this and aligning our whole business around it. So, what do we mean by \"creating reliable and intuitive software that helps businesses and people to be more successful\"? We enjoy working with companies who are trying to drive forward change. When the software we create helps companies to successfully deliver this change, then we've done our job. We find that the best way to achieve this is to focus on creating software that is useful, intuitive and reliable. To give you a few examples of the types of software we mean: Working with a business to streamline internal workflows by creating productivity software. Helping an offline business expand online, by implementing an eCommerce platform. Creating an online service that will help a business to deliver a better service to its customers. I hope this helps to explain our new brand ideal and provides an insight into where our business is headed. We’ll be using this brand ideal to validate our internal decisions and to align our recruitment, marketing and communications. As a business, we're committed to only spending time on things which help us to deliver upon this long-term vision. We feel this puts us in good place to move into our next phase, which is launching our own Software as a Service (SaaS) products. Today we've got a new product that we would like to announce: YourDayTheirDay – An online service that will help you to run a more organised agency. This product is still in its infancy, though we're dedicating a significant proportion of our time to it. We're keen to hear from agencies who are interested in helping shape the direction of the product by getting involved in our beta program. If you're interested in a software platform that helps you run a more efficient agency, please do get in touch . Domain name You may have noticed that we've made a change to our domain name. We're dropping \"Made by Made\" and will be using \"Made Tech\". We’ll be rolling this name out across our social networks, collateral and other touch points over the coming weeks. Identity system We have redesigned our brand identity. We’ve worked closely with Simon Crofts from AttentionSeeking on this, and we think he’s done a fantastic job. Our brief to Simon was to create an identity system which would provide us with the building blocks and flexibility to grow over time. A system which would allow us to introduce sub-brands and spin off software products which all felt like one family, though at the same time, could stand strong in their own right. The new logo comes in two parts – the letterforms and the mark. The letterform is based on Gotham Bold — each of the letters have been redrawn to help provide a flow and symmetry across the word. The mark is a simple coloured shape, which mimics the M in Made. It’s this element which will vary across our sub-brands and will help us to develop software products which all feel like part of the Made family. We’ve also introduced a tagline that will be used on our sub-brands, and will help to reinforce the craft and care that goes on underneath the hood. If you see \"Powered by Made\", you’ll know it's a software product that we’ve created. We’ve spent time refining the elements to get them to a point where we feel they are right; but it only seems right that as a software company who strongly advocates lean principles that we have a brand identity that can evolve with us. You’re likely to see iterations and improvements to our brand as our products develop and feedback rolls in. Responsive website Finally, we've also launched our new website! We kicked this project off at the beginning of the year and have been working on the UX, design, build and content over the last 6 weeks. We’ll be doing a proper writeup of the new site in a couple of weeks, but in the meantime, take a look around and let us know your thoughts. If you have any questions or comments, please do get in touch . I'd love to hear them. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-02-27"},
{"website": "Made-Tech", "title": "Mob Programming", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/mob-programming-at-made/", "abstract": "At Made, we’re always looking for ways to improve the quality of our code, and anything that can help us do that while also encouraging us to solve problems as a team is something we want to spend time trying out. The Idea A recent Code Climate blog post introduced us to the concept of mob programming , the basic outline of which is: The whole team gathers around a single display A piece of code in need of refactoring is selected from an existing project As a team, refactor the code as much as possible in an hour There’s no pressure to create production ready code, instead the focus is for everyone to pick up best practice habits on code formatting from everyone else The Setup The meeting room at Made is home to a swanky HDTV, perfect for the whole team to gather around. We had one person share their laptop screen with the TV (using an Apple TV), and we then began discussing what the problems were within the code, and how we could begin tidying it up. To encourage collaboration, we experimented with ScreenHero , a piece of software designed for remote pair programming , so that anyone who wanted to could join the screen and help refactor the code. The Mobbing We chose to refactor a delayed job within one of our Rails projects, and the hour flew by, with debates springing up over whether a method was too long (they usually were), whether they could be named more aptly (they usually could), and how best to refactor the code out in ways that reduced complexity and increased readability. At the end of the hour, out of necessity, we’d extended the scope of the session to include a slight refactoring of a model associated with the delayed job, as we found there were one or two methods that ought to be contained within the model rather than the delayed job. The code was certainly not production ready, especially given that we’d forgone TDD for the sake of speed, but we managed to significantly refactor the code in a relatively short space of time, into something we all felt was at least on its way to being an improvement over the original code. The Future Overall, mob programming proved to be enjoyable and informative, so we’ll be running a session every other week, fine tuning the format where needs be as we go. In this week’s case, there was one area that needed tweaking in particular, and that was making sure everyone had a chance to engage with the code. Since ScreenHero limits the amount of users sharing one screen to two people, the freeform nature of the session, where anyone could pipe up and ask to join the screen, wasn’t the best approach, as the more experienced members of the team naturally had more ideas and could contribute a lot more. In hindsight, future sessions will have team members rotate and spend an equal amount of time at the keyboard, so that everyone has the opportunity to express on screen either their own ideas or those that are flying around the room. The main aim of these sessions is to learn collaboratively, so we want to encourage everybody to get involved. Also, in the weeks between mob programming sessions, we’re going to be running a Code Dojo . The aim of those sessions will be to challenge ourselves in a similar fashion to mob programming, the difference being that instead of scouring our existing projects for code in need of refactoring, we’ll be coming up with solutions to hypothetical problems that can incorporate TDD. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-02-17"},
{"website": "Made-Tech", "title": "Metaprogramming in Ruby", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/metaprogramming-in-ruby/", "abstract": "When you find yourself duplicating functionality that is similar to existing functionality in your system, it may be an indication that the process could be generalised to accommodate more scenarios. This has the benefit that this 'role' would be encapsulated and the source code would exist in only one place, becoming less of a maintenance burden. As web developers, we may not write a lot of meta code in our day to day activities, but it is widely used in most systems. Compilers, parsers, assemblers, DSLs, web frameworks and preprocessors make heavy use of it. When used correctly, it can help you write more declarative code (as opposed to imperative) and help keep your codebase DRY. The Factory Analogy I find the factory analogy very effective when thinking about metaprogramming: If you as a programmer had to build cars every day, you would soon be repeating the same process over and over. If you wanted to boost efficiency, and eliminate the risk of human error, you would build a factory to build the cars. Every execution of the car by the factory may be different as we pass in data for it to process, such as colour and make; but the role is broadly the same. Metaprogramming is the writing of computer programs that write or manipulate other programs (or themselves). You can also think of it as code that writes code. – Wikipedia As with most things, metaprogramming does come at a cost. The mental model increases significantly, and the ability to quickly scan and understand a codebase decreases. The dynamic nature of Ruby makes it really easy to do metaprogramming. Rails relies heavily on metaprogramming to make the interface as intuitive as it is, without reams of code to catch every possible scenario. A good example is the Dynamic finder methods in ActiveRecord like: find_product_by_name_and_colour(name: name, colour: colour) They are an example of how metaprogramming has made our lives much easier by implementing some complex internal logic elsewhere, out of sight. The public API exposed by metaprogramming is usually simplified, the implementation details are more complex. This example is wonderfully illustrative of metaprogramming. There is no method called find_product_by_name_and_colour and instead is handled by a method_missing method up the call chain. Hitting a method_missing is intended use of the framework in this case, and ends up being executed by some dynamic receiver. Associations are a set of macro-like class methods for tying objects together through foreign keys. Each macro adds a number of methods to the class which are specialised according to the collection or association symbol and the options hash. – Comments in ActiveRecord::Relation Open Classes In Ruby a = \"test\"\n\ndef a.my_method\n  puts \"something\"\nend   \n\nputs a.my_method  # => something As you can see above, the \"test\" string instance now contains a new method called \"my_method\". The String class (or the meta class of \"test\") is not affected, and the my_method method is only valid for the duration of the instantiated string object. Alternatively you could add the method to the String class by re-opening it and declaring the method on there.  This is only one of many ways to dynamically add, or modify code at runtime.  This is known as Monkey-patching , and is both powerful and dangerous at the same time. Many gems make use of this to add additional functionality to existing code. Metaprogramming Concepts in Ruby Dynamically defined methods def create_method(name, &block)\n  self.class.send(:define_method, name, &block)\nend With this method in a class, you can use it to define methods on an existing object. Self self is a special variable that contains the current object at any time a method is called.     If you don't specify a receiver object for the method it will default to main, which is the top level object in Ruby. If you are calling a method from within an object, self will be the object. A method always executes on self, there are no exceptions. Metaclass Also known as singleton classes, the metaclass is an instance of the class Class.  It is like an instance of the blueprint of the object.     Every object in Ruby has one of these meta objects associated with it. Contrary to Smalltalk 80, Ruby uses the Eigenclass.  This is considered by some to be a design improvement, even though at first glance it seems more complicated. You can read more about Eigenclass on Wikipedia . This class is said to be one \"meta-level\" higher than than the object that you created. Reflection You can also use metaprogramming to do Reflection. With reflection, a computer program is able to observe its own structure at runtime.     This makes it useful for introspection, and can be used to gather information about the program itself. An example of this is the .methods method in Ruby which will tell you which method an object responds to.     Reflection can be used to make a program intuitive and increase its 'knowledge' about itself.     Magic like __FILE__ and __LINE__, which contain internal knowledge about source code of the program can be helpful in building intuitive debugging messages. Binding The binding method on Kernel is really interesting. It is a container for the current local context in the method that it is in.     This snapshot of the local variables in that scope, at that time, can be passed around in a variable and executed against other functionality     elsewhere.  The eval method takes this binding as an optional second argument when executing code. Using Rails as an example again, it uses the binding object to execute view/layout related code in a certain context. Inherited When the inherited method is defined on an object, it will always be executed when it is subclassed.     Functional programming has influenced object oriented programmers to prefer composition over inheritance, so if you find yourself using this a     lot, it may be an indication that you are overcomplicating your design.  Always prefer composition over inheritance. Document Your Meta Code It is generally frowned upon to have any comments in your code (because they can quickly become outdated), but I think that there is an exception for metaprogramming. You should obviously be testing your code, but adding a parsed version of the meta code for reference along with the test is also helpful for other developers. I was very grateful to find a demonstration of a generated bit of example code in the Rails documentation, which instantly made it easier to understand. People have different opinions on this. Just make sure that you are not creating something that is too complex without some form of documentation and a clear use case for it. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-03-28"},
{"website": "Made-Tech", "title": "UNIX at Made", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/unix-at-made/", "abstract": "Everyone in the Made office seems to prefer Unix based systems.  Why is that? Sure everyone likes Apple products, they are considered stable, relatively speaking, but I don't think that everybody appreciates where most of that stability comes from. Under the hood, OSX runs on BSD UNIX . This family of operating systems was first designed in the 1980s, and was authored by Dennis Richie and Ken Thompson . Unix has its own philosophy, and when applied correctly, can lead to some beautiful software. What is the UNIX Philosophy ? Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. Read more about the UNIX philosophy . Extremely contrived example of doing one thing Consider the following functions: generate_report\nsend_email vs generate_report_and_send_email Which is better?  The first of course. Instead of generate_report_and_send_email as one function, we prefer smaller reusable components.  Big methods tend to violate the SRP principle , and are hard to maintain. When used correctly, we will also achieve another important design goal known as \"Composite simpler than the Sum of it's Parts\", as you abstract out concepts in the system. As a rule of thumb, if your function name contains an \"and\", it is probably doing too much. If your function name contains an \"or\", it is doing too much. UNIX at Made When we design complex systems, we have had good results with following the Unix philosophy, which is also in line with a lot of the Agile methodologies that we follow and practice: No more than 100 lines per class No more than 10 lines for executable function bodies. No more than 4 arguments for a function (Yes big hashes are cheating). No more than 80 characters per line We use software that makes sure this is true, or it will fail the build. We're programming a lot of Ruby at the moment, making Tailor and Cane our current tools of choice. Small functions and abstraction When you write small functions, you end up naming more of the system. This has many benefits and, as a nice free side effect, your code becomes dynamic documentation. Instead of complicated cryptic instructions (implementation details), you can read the name of the abstraction (function name) and get a better idea of what the author had in mind when they first wrote the code. We have found this style to be very effective, and I encourage everyone to learn more about the Unix philosophy and try to apply it to your code. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-02-17"},
{"website": "Made-Tech", "title": "SEO and design updates launched for VGL", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/seo-and-design-updates-launched-for-vgl/", "abstract": "As part of our ongoing relationship with VGL , the team behind Surface View , we recently rolled out an update that features refreshed designs for key areas of their site , responsive email templates, and a host of SEO-friendly enhancements. To help improve their visibility within search result listings, we’ve implemented a sitemap, canonical URLs and schema.org markup across the site, along with the ability to create SEO landing pages within the CMS that target particular keywords and help users quickly find what they’re searching for. On the design side, we’ve added a new, site-wide footer that gives visitors a number of options for navigating the site and getting in touch, and also highlights a number of key service features. Product category and detail pages have been given an overhaul, to allow a greater focus on the associated imagery, emphasise the services they provide and further encourage visitors to get in touch Recent statistics from newsletter campaigns across our entire client base have shown that more than 50% of emails opened were viewed on a mobile device, proving that it’s becoming increasingly more important to cater to mobile users. With that in mind, we’ve also created a pair of mobile responsive newsletter templates for VGL to keep their on-the-go customers up to speed. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-02-17"},
{"website": "Made-Tech", "title": "Viaduct mobile eCommerce store launches", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/viaduct-mobile-ecommerce-store-launches/", "abstract": "To kick off the year we've launched the mobile optimised Viaduct eCommerce store . Building on our initial delivery of the Viaduct eCommerce platform back in 2011, we've retrofitted a fully mobile optimised eCommerce experience to the site, right from browsing products through to checkout. We've also made a number of the secondary content areas of the site, such as the blog and case study sections mobile responsive. We'll be rolling out some further enhancements to the desktop experience over the coming weeks, including a design refresh for 2014. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-01-28"},
{"website": "Made-Tech", "title": "Tell, don’t ask. It’s a matter of principle.", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/tell-dont-ask-its-a-matter-of-principle/", "abstract": "When we design software with maintainability in mind, one obvious goal is to keep the unnecessary lines of code to a bare minimum. The less you need to see/read that is not vital to your understanding of the domain model , the better. You want your domain model to be free from too much implementation knowledge. By implementation knowledge I mean things that do not deal directly with the problem that you are trying to solve. A connection to a database is an example of this. It is an implementation detail that helps you achieve your goal, but is not your actual goal or the goal of your clients. If this type of knowledge is not abstracted away ( Information Hiding ), it only serves to detract from understanding of the system. You end up having to keep more of the program in your own memory to solve a particular problem. At a certain point bugs become inevitable because the human memory is actually pretty bad. What is the Tell Don't Ask principle? Very briefly, the Tell Don't Ask principle states that you should tell an object what to do and not query it for data, and then make decisions based on the answer you get back. In object oriented languages we like to focus on the messages passing between objects , and sending instructions to an object is better than querying it, and then altering it's state based on the outcome. The latter is how procedural , highly coupled disasters spring into existence. This style of Tell Don't Ask promotes both the encapsulation and cohesiveness of your objects and you end up with a less coupled, more pluggable system. A system that is not tightly coupled is less resistant to change, easier to maintain, and easier to test. If your code already follows the Law of Demeter (objects only communicate with objects that are close to them on the object graph), it will be easier to apply the Tell Don't Ask principle. Here is an example of code that does not follow the Tell Don't Ask Principle: class StockManager   \n  def self.get_stock_purchased     \n    stock_purchased_database_query  \n  end   \n\n  def self.get_stock_sold\n     stock_sold_database_query   \n  end\nend\n\nclass DisplayManager   \n  def display     \n    stock_purchased = StockManager.get_stock_purchased(product)\n    stock_sold = StockManager.get_stock_sold(product)\n    puts stock_purchased.to_i - stock_sold.to_i   \n  end\nend DisplayManager knows too much about how the stock on hand is calculated. Information from StockManager has leaked out and is not encapsulated. We also have less abstractions which name and clarify our intentions. Here is code that follows the Tell Don't Ask principle.  The knowledge of stock is kept inside the object. class StockManager  \n  def self.get_count_on_hand    \n    get_stock_purchased - get_stock_sold  \n  end  \n\n  private    \n  def self.get_stock_purchased      \n    stock_purchased_database_query    \n  end    \n\n  def self.get_stock_sold\n    stock_sold_database_query    \n  end\nend\n\nclass DisplayManager  \n  def display    \n    puts StockManager.get_count_on_hand  \n  end\nend This is a very basic example of where we put the logic with the data, and it has many benefits. It also promotes Composite Simpler Than the Sum of Its Parts and the Law of Demeter. The Tell Don't Ask principle has been known to conflict with the Single Responsibility Principle (which I think ranks higher in value), and objects can grow to become large if not used carefully. As with most principles/laws in software design, this is not set in stone and is also not a silver bullet to solve all your problems. However, by following the Tell Don't Ask principle and pushing complexity down to lower levels, you end up with a system that is much easier to reason about. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-10-09"},
{"website": "Made-Tech", "title": "Ryvita Shop Launches", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/ryvita-shop-launches/", "abstract": "We're excited to have launched the all new Ryvita Shop earlier this week. Building on top of the Spree open source eCommerce platform , we've delivered a bespoke online shop for a range of Ryvita-related products. The Spree platform provides full management of all aspects of the shop, including products, stock levels, delivery rates, tax rates and promotion codes. The delivery saw the integration of the SecureTrading payment gateway, which is seamlessly loaded in checkout process, Transworld Logistics to whom orders are automatically sent for fulfilment, and with PostcodeAnywhere for address lookups as part of the checkout process. The shop is the first part of the Ryvita website to get the mobile treatment, with the full end-to-end customer purchase journey being developed using a mobile responsive layout. Several of the services that were built as part of the Jordans website launch earlier this year have been reused, including the content management of promotional areas, commenting/review functionality and the ability to sign up for email newsletters anywhere an email address is captured. Following the launch we're working on further integration with the fulfilment partner to provide delivery confirmation emails. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-09-26"},
{"website": "Made-Tech", "title": "Time to Schema up", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/time-to-schema-up/", "abstract": "At Made we are always on the lookout for technologies and practices that enable us to deliver better code. Practices that can speed up page loads, help us write better tests, or in this case, offer up SEO improvements. With HTML5 now the norm in all the markup we deliver, we are able to add real semantic meaning to our pages with the new elements. Armed with this new markup we can create pages that enable search engines to better understand and categorise content. However, there are instances when markup alone simply won't fully publicise the content we display to search engines. This content could be contact information, event listings or even a recipe. For this we need to go one step further and add something to our page elements to expose the structured data within the page. There are a few different specifications to choose from that expose this information and enable better discovery: Microdata Vocabularies ) RDFa Microformat Open Graph At Made we use WHATWG's Schema.org microdata vocabulary. Depending on your attitude toward specifications created and maintained by big companies, you might not be keen on using Microdata, as the main supporters are Google, Yahoo and Microsoft. However the commercial reality of using something backed by the big players in search ensures that the markup will be properly understood by some of the most popular search engines – Yahoo!, Bing and Google. One of the main benefits of using Schema.org is the way it improves how your search result listing appears. Google, Yahoo! and Bing will show a rich snippet that is based on the structured data found in the structure of the page. You can test how the rich snippets will be displayed by using the Structured Data Testing Tool in Google Webmaster tools. Take the example rich snippet of a recipe from the recent update we deployed for Jordans Cereals: The rich snippet displays a thumbnail, rating and total cooking time which are all visible in the output from the test. This improved appearance is much more likely to engage a user's interest and the data exposed will enable Google to deliver your content in more relevant search results. Schema.org 's usage also extends beyond websites, it can be used in emails (albeit only Gmail for now and once registered ). Currently the only usage is to show order and shipping information , however when this proposal by Google is finalised its scope will increase. These new and existing features, like their equivalents within web pages, deliver a richer experience. SEO can be an afterthought when writing HTML, but adding in your chosen RDFa, Microformat or Microdata attributes can deliver real improvements in organic discovery, from both better indexing to richer looking search results. All front end developers should be including one of the many ways to expose structured data. Adding structured data can also benefit screen readers and therefore improve your site's accessibility. At Made we have implemented Schema.org on our recent site launches ( NIVEA MEN , Jordans Cereals and Forevermark Bridal ), as well as the refresh of Surface View . Because these sites have seen great benefits from a relatively small amount of effort, we will continue to include Schema.org in all of our future launches. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-08-28"},
{"website": "Made-Tech", "title": "OnAPI Launches", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/onapi-launches/", "abstract": "We're pleased to have launched the first phase of the OnAPI which exposes all of On's product information as a RESTful JSON API. Recognising the long-term ambitions of the business to provide up-to-date product information across many channels, and the need to provide a stable and versioned API for a number of existing services, we developed a fully machine discoverable API that exposes the current state of On's product range, right down to the number of each shoe in each warehouse. The OnAPI sources much of its data from Salesforce, exposing the data in a much more client-friendly and lightweight format. In addition, we see further stability gains in decoupling end-clients from directly interfacing with Salesforce with the business making regular changes to their data structures and with the Salesforce API on the receiving end of large changesets every quarter. The first client for the OnAPI, the eCommerce website that we relaunched just over a year ago has been re-architected to pull product data from the API, rather than interfacing directly with Salesforce, as was previously the case. We're excited to be exploring further opportunities for the OnAPI, both in applications within the business and with third parties interested in leveraging live product data from On. In addition, we're looking to expose more than just product information to the API over the coming year; with stockist data, order functionality and fulfilment all high on the list. From an implementation perspective, we'd heartily recommend the RSpec API Documentation Gem for other Rubyists looking to generate API documentation. Key for us was that documentation needed to be kept close to the code to ensure it doesn't go stale. This gem takes a great approach to repurposing your specs to drive your docs. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-12-18"},
{"website": "Made-Tech", "title": "On Running launches in Asia", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/on-running-launches-in-asia/", "abstract": "Following the launch of the On Running eCommerce store at the beginning of the year, we've just released support for three Asian markets: Japan, China and Korea. The latest launch includes full site translations for Japanese, Traditional Chinese and Korean. In addition, core site functionality has been brought under the control of feature toggles, enabling features such as the eCommerce functionality to be entirely disabled on a per-market basis. The update also includes a number of design changes to the product pages focused on further increasing conversion. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-08-22"},
{"website": "Made-Tech", "title": "New product listing pages and SEO improvements for Surface View", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/new-product-listing-pages-and-seo-improvements-for-surface-view/", "abstract": "Following the successful launch of a new Surface View homepage earlier in the year, we've started to rollout the updated look and feel to other areas of the site, including new product listings pages, SEO landing pages and MySV overlays. The updated site has had a search optimisation refresh, with the introduction of schema.org across the board and the development of a CMS tool, which enables SurfaceView to create optimised landing pages, which target particular keywords. As part of this deployment, we have re-developed a number of back-office tools, which will help to minimise the administrative tasks required whilst managing the 60,000+ products. This sprint marks the start of a move towards a 'Service Oriented Approach' (SOA) for the retailer, with many areas of the existing codebase planned for re-factor as we transition across to this approach later in the year. https://www.surfaceview.co.uk/shop/windows/window-film https://www.surfaceview.co.uk/l/beano-canvases Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-09-11"},
{"website": "Made-Tech", "title": "Byron Responsive M-Site Launched", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/byron-responsive-m-site-launch/", "abstract": "Last week we launched a new version of the Byron website that incorporates a responsive layout, and a completely overhauled locations page that lets visitors easily find their nearest restaurants. The new locations page is designed to support Byron's ever growing number of restaurants and their expansion outside of London. It takes full advantage of the Google Maps JavaScript API , giving users the ability to search for the three nearest restaurants based on either an address they've typed into the search field for or their GPS-determined physical location. Mobile users then have the option to then get directions to a chosen restaurant in their maps app. Mobile users also benefit from the new responsive layout, which presents page content in a manner appropriate to the device being used to view it and includes crisp, vector-based images for retina displays. Additionally, and in the wake of #BurgerGate , we've taken steps to handle sudden increases in the volume of traffic to the site by moving the site to a new cloud-based hosting environment and utilising a content distribution network. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-10-30"},
{"website": "Made-Tech", "title": "New On Running homepage launches", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/new-on-running-homepage-launches/", "abstract": "Last week we launched a homepage refresh for On Running , featuring an all-new design and new content management ability. The promotional areas across the homepage are now fully content managed, providing marketing teams at On significant agility in their ability to respond to events, such as their recent triumph at the Ironman World Championships. The content that is published to any of the 37 market homepages can be set by checking a box in the CMS. The CMS also provides the ability to translate content in to the 7 languages that the site is available in. Cache sweepers have been used to ensure the dynamic homepage content is served from fast memcached servers, making use of cache sweeping in the CMS to invalidate the caches as content becomes stale. We've had a great reception for the homepage so far. We're looking forward to measuring the commercial success of the changes over the coming weeks. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-10-28"},
{"website": "Made-Tech", "title": "Made appointed for Ryvita eCommerce build", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/made-appointed-for-ryvita-ecommerce-build/", "abstract": "We're pleased to announce that we're continuing our relationship with Jordans and Ryvita by getting cracking on the user experience, design and rebuild of the Ryvita eCommerce shop. It's planned that the shop will build on the Spree open source eCommerce platform and will reuse several of the SOA components we've already built for the Jordans Cereals website. The shop will spearhead the move to a mobile responsive site for Ryvita. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-08-08"},
{"website": "Made-Tech", "title": "Jordans Recipe section launches", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/jordans-recipe-section-launches/", "abstract": "Hot on the heels of our earlier launch of the Jordans Cereals website, we've delivered an all-new recipe section for the site, featuring 10 carefully curated recipes containing a range of Jordans products. We've made it possible to browse recipes both by category , for those who know what sort of delight they'd like to make, and by product , for those who know what their favourite Jordans product is. The section also features a number of user generated touchpoints, such as the ability to upload your own recipe to the site, a star rating mechanic and user reviews. We've incorporated rich snippets across the section to enable Google to show rich search results for the recipes. As with the rest of the Jordans Cereals website, the section is fully content managed, which allows the folks at Jordans to keep the recipe content fresh throughout the year. The recipe section also continues with the mobile responsive implementation; allowing recipes instructions to be followed along in the kitchen, and for the shopping list of ingredients to be easily carried along to the supermarket. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-08-01"},
{"website": "Made-Tech", "title": "Debeers Forevermark Bridal Launches", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/debeers-forevermark-bridal-launches/", "abstract": "We've launched the multi-market Forevermark Bridal website , currently available in Global English, India, China and Hong Kong. We delivered the Information Architecture, content management, website build and hosting for the project. The content management system provides for separately managed market sites with independent content running from a single codebase. The site makes use of the Amazon CloudFront content delivery network to ensure assets are served quickly to the global audience for the site. The site was designed from the ground up to be mobile compatible, using a responsive layout to prioritise and display the content appropriately for the device that's being used to access it. In addition, retina-compatible graphics and font-based iconography have been used to help maintain crisp images on high resolution and small screen devices. Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-08-05"},
{"website": "Made-Tech", "title": "Cache sweepers in Rails: Invalidating caches from outside the controller", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/cache-sweepers-invalidating-caches-from-outside-the-controller/", "abstract": "The problem Invoke cache sweeper through a rake task, which will loop over all the products in the database and expire the cache for each one. There is a fair amount of setup required for this to work. I will walk you through how to do this with a series of hacks. The rake task Create the rake task /lib/tasks/cache.rake which invokes ProductSweeper and calls the clear_all class method on it. namespace :cache do \n  desc 'Clears cache' \n  task :clear => :environment do \n    ProductSweeper.clear_all \n    puts 'All caches cleared' \n  end\nend The Product Sweeper I added the expire_all method to the product sweeper /app/sweepers/product_sweeper.rb, which would be invoked by the rake task. class ProductSweeper   \n  observe Product  \n\n  def after_save(object)    \n    clear_cache(object)  \n  end  \n\n  def self.expire_all    \n    new.clear_all  \n  end  \n\n  def clear_all    \n    Product.all.each do |product|      \n      clear_cache product    \n    end  \n  end  \n\n  def clear_cache(product)    \n    expire_action :controller => '/product',                   \n                  :action => :show,                   \n                  :product => product    \n  end  \nend Testing it out I ran the task and everything seemed to have worked fine. It printed 'All caches cleared', and there were no errors, but I expected to see the output below in the log file: Expire fragment views/localhost:5000/products/testproduct I double checked that it had indeed not cleared the caches by looking at the server log output when visiting the product on the front end. It kept on reading from the cache, and it was never invalidated. No errors to diagnose, it just failed silently. Diving deeper So I fired up the debugger and followed the code execution path into Rails. You can get the debugger by adding the following to your Gemfile. gem 'debugger' Then add add a method call to debug above the expire_cache method in the product sweeper. Unsurprisingly it turns out that when a call is made to the expire_cache method on a class that inherits from ActionController::Caching::Sweeper, the first thing it hits is a method_missing method. This source code looks like this looks like this: def method_missing(method, *arguments, \n  return unless @controller\n  @controller.__send__(method, *arguments, &block)\nend Note that it returns unless an instance variable named @controller exists. There's our problem. Initial diagnosis To get this to work, we need to find out what @controller should contain so that we can populate it ourselves. We can find this out by printing @controller while a sweeper is being invoked during normal operation. Meaning through an update, delete or create action on the ProductsController, printing out @controller yields a great wall of text which contains all the values stored in it. The important part is that the object contained in it is ProductsController, and as we know ProductsController inherits from ApplicationController. Faking it Now that we know what that method_missing is looking for to send the message to the ApplicationController, we can modify the sweeper to create a new instance of ApplicationController and assign it to the @controller variable. We modify our ProductSweeper to include a new method named instantiate_controller which has the responsibility of populating @controller if it does not exist yet. class ProductSweeper \n  observe Product  \n\n  def after_save(object)    \n    clear_cache(object)  \n  end  \n\n  def self.expire_all    \n    new.clear_all  \n  end  \n\n  def clear_all    \n    instantiate_controller    \n\n    Product.all.each do |product|      \n      clear_cache product    \n    end  \n  end  \n\n  def clear_cache(product)    \n    expire_action :controller => '/product',          \n                  :action => :show,  \n                  :product => product    \n  end  \n\n  protected    \n\n  def instantiate_controller      \n    @controller ||= ApplicationController.new    \n  end\nend After implementing this, we get a new error: NoMethodError (undefined method 'host' for nil:NilClass) After some research I found that manually populating the @controller.request.host value as demonstrated below will fix the error: protected\n\ndef instantiate_controller @controller ||= ApplicationController.new \n  if @controller.request.nil? \n    @controller.request = ActionDispatch::TestRequest.new \n    @controller.request.host = Rails.application.config.host \n  end\nend Note that we pass in a fake Request object, and then set the value of the host on that. Victory With this hack in place, and after restarting the server, we finally get to see that message in the server log that we were looking for: Expire fragment views/localhost:5000/products/testproduct (0.1ms) The feature was now fully functional, and I could clear the caches on the command line with: bundle exec rake cache:clear This was also built into an ActiveAdmin dashboard, where a button could be pressed which would call the rake task. All of it was also successfully wrapped with Cucumber tests, so mission accomplished. Resources Rails guides: Caching Rails API docs ActionController::Caching::Fragments Railscasts Caching This application was built with Rails 3.2.13 Tweet 0 LinkedIn 0 Facebook 0", "date": "2013-08-14"},
{"website": "Made-Tech", "title": "Design Patterns: Observer", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/design-patterns-observer/", "abstract": "To be meaningfully involved in the architecture of any web application your team is building, a basic grasp of what design patterns are, and knowledge of the patterns most commonly used, are good tools to have. There’s a shorthand that develops naturally when you’ve understood the concepts within that are otherwise impossible to blag. I’m a little more right than left brained, so it’s quite easy for me to get lost when the dev-speak starts flying around the office. To that end, to help myself as much as anyone else, this is the first in a series of posts I’ll be writing that focus on the subject of design patterns. Design patterns are solutions to software design problems that are presented in an almost conceptual way. That is to say, a given design pattern has the potential to be applied to a piece of software written in any number of languages but, at a project level, it's up to the developer to interpret that idea and make it work for them. The first pattern I’ll be digging into is the Observer pattern, and we at Made tend to favour working in Ruby at the moment, so the examples you'll see reflect that. **Problem:** You’ve been asked to build an application where a musician can sign up and create a page for their fans and potential sponsors. One of the requirements of the project is that those fans should then be able to subscribe to the musician’s page and be automatically notified via email whenever a new tour is announced. **Solution:** Let’s start with three classes, Musician, Fan and Sponsor. The Fan and Sponsor classes both have an #update! method the Musician class can call, but their #update! methods do different things with the data they’re given: class Musician\n  attr_reader :name, :tel_number, :tour_date, :tour_title\n\n  def initialize(name, tel_number, fan, sponsor)\n    @name = name\n    @tel_number = tel_number\n    @fan = fan\n    @sponsor = sponsor\n  end\n\n  def new_tour(title, date)\n    @tour_date = date\n    @tour_title = title\n    @fan.update!(self)\n    @sponsor.update!(self)\n  end\nend\n\nclass Fan\n  attr_reader :name, :email\n\n  def update!(musician)\n    email_body = \"#{musician.name} announces #{musician.tour_title} on #{musician.tour_date}!\"\n    ## send the email\n  end\nend\n\nclass Sponsor\n  attr_reader :name, :email\n\n  def update!(musician)\n    email_body = \"#{musician.name} will be touring on #{musician.tour_date}, call #{musician.tel_number} for enquiries.\"\n    ## send the email\n  end\nend\n\nfan = Fan.new\nsponsor = Sponsor.new\nmusician = Musician.new('Barry Manilow', '0207 444 4444', fan, sponsor)\nmusician.new_tour('Lock up your daughters', '01/01/2016') This is already pretty bad – the Musician knows explicitly about the Fan and the Sponsor, and if we wanted to notify other types of subscriber, a news outlet for example, we’d need to update the Musician class to make it aware of the new NewsOutlet class. Ideally the Musician shouldn’t care exactly which classes are affected when the #update! method is called, just that it is passing information on to its observers and that they have an #update! method. With that in mind, what we can do to tidy this up is add two new methods, #register_observer and #notify_observer, and update the Musician class like so: class Musician\n  attr_reader :name, :tel_number, :tour_date, :tour_title\n\n  def initialize(name, tel_number)\n    @name = name\n    @tel_number = tel_number\n    @observers = []\n  end\n\n  def new_tour(title, date)\n    @tour_date = date\n    @tour_title = title\n    update_observers\n  end\n\n  def register_observer(observer)\n    @observers << observer\n  end\n\n  def update_observers\n    @observers.each do |observer|\n      observer.update!(self)\n    end\n  end\nend With the #register_observer method, we now have a way for observers to make themselves known to the Musician class, and we never need to update said class if a new type of observer is introduced: fan = Fan.new\nsponsor = Sponsor.new\nmusician = Musician.new('Kenny Loggins', '0207 555 5555')\nmusician.register_observer(fan)\nmusician.register_observer(sponsor) Now, when we call the #new_tour method on the Musician class, #update_observers automatically ensures both the Fan and the Sponsor are notified: musician.new_tour('Into the Danger Zone', '27/04/2017') The benefits of this pattern are that we’re able to add new observers without needing to change the subject, and loose coupling; the subject doesn’t know anything about the observers, it simply passes the data to them and lets them get on with whatever it is they want to do with said data. The downside there is that an observer is potentially being given a lot of data it has no use for, in this instance the Fan class currently has no use for the telephone number, but it has access to it anyway. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-04-21"},
{"website": "Made-Tech", "title": "SHOWstudio Shop Launches", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/showstudio-shop-launches/", "abstract": "We are very pleased to announce the launch of SHOWstudio’s new e-commerce store . Founded by Nick Knight in 2000, SHOWstudio.com is a major online presence in the fashion industry which focuses particularly on film and multimedia as its medium, working on many high profile projects with people such as Alexander McQueen, Naomi Campbell, Christopher Kane and Lady Gaga. For their brand new storefront and e-commerce needs, we’ve once again taken advantage of the Spree platform , and built the site from the ground up with a mobile-first approach to design, ensuring a great user experience across desktops, mobiles and tablets. Using a combination of the Agile philosophy and Continuous Delivery we were able to launch a fully functioning e-commerce store in just seven weeks, meeting with the team at SHOWstudio after every two week sprint to showcase our progress and give them the opportunity to regularly provide us with useful feedback. In the weeks since, we’ve continued completing sprints fortnightly, and have been regularly rolling out additional features such as user accounts, product filtering and the ability to search for products by a particular designer. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-04-16"},
{"website": "Made-Tech", "title": "Continuous Delivery with Jenkins", "author": [" David Winter"], "link": "https://www.madetech.com/blog/continuous-delivery-with-jenkins/", "abstract": "At Made we're very keen to have our deployments run as safely, quickly and as easily as possible. When you've worked on a new feature for a project you want to get it in the hands of the client and end-user at the earliest possibility. To do this we use a couple of practices: Continuous Integration and Continuous Delivery . Continuous Integration Commit early and often is something you hear being preached at Made a lot. The smaller the chunks of work you develop on during the day, the more you can be keeping in sync with your git repository without the increased risk of having painful merge headaches. Each time code is pushed into the respository, automated tests are run so that if they fail, you're aware of any errors at the earliest opportunity. This lets you act on them fast. If errors happen—and they do—going through smaller changesets means you'll be able to resolve an issue quicker. Continuous Delivery The general idea is that all code that is pushed into the repository should be in a state that can be deployed at any time. Knowing a feature is ready, normally means having a test to ensure it works as expected. If a feature isn't quite ready then it should be hidden from the end user and/or have a feature switch. This is known as a dark launching. Once the repository has all of its tests passing for the codebase, get it automatically deployed to an environment in a single easy step. Having deployments in a repeatable condition makes them quick and easy. It also allows you to iron out any issues with your deployment strategy nearer the start of a project. Continuous Jenkins Jenkins is the glue that allows us to bring both of these practices together. The flow that we use at Made resembles a pipeline. The general flow is as follows: Developers work on a feature in their local development environments. Once it works with a passing test, the changes are pushed to the repository. We use git, so this is all done using the master branch. Be sure to treat Jenkins as the gatekeeper for this process. Only ever push into the master branch, and let Jenkins do the rest. Jenkins is continously polling our repository for changes on the master branch. When it detects new changes, it pulls down the code and runs a number of tools to ensure the codebase is stable and in a condition fit for deployment. Those tools can be tests, code standards, linting etc. Upon a successful build we automatically deploy to somewhere that all of the dev team can view the build. We call this the continuous environment. Once we're happy with the continuous environment we manually hit a button that deploys the code to our staging environment. This is where we let the client take a look at our work, and they can then show it to any of their internal stakeholders. When the client is happy, with one more manual button click, it's ready to go into the production environment for the end-user. Setup To build this flow in Jenkins we use the Build Pipeline Plugin . We create a number of Jenkins jobs that represent each stage. For an example project named falcon, we create the following jobs: Job name: falcon-build Git branch to poll: master Build steps: Uses a shell command to run our tests and code standards tools Post build actions: Automatically build falcon-continuous-deploy job on success Git push into branch: stable Job name: falcon-continuous-deploy Git branch to poll: stable Build steps: Runs a deploy script to our continuous server Post build actions: Manual build step for falcon-staging-deploy job Git push into branch: continuous Job name: falcon-staging-deploy Git branch to poll: continuous Build steps: Runs a deploy script to our staging server Post build actions: Manual build step for falcon-production-deploy job Git push into branch: staging Job name: falcon-production-deploy Git branch to poll: staging Build steps: Runs a deploy script to our production server Git push into branch: production Once all of these Jenkins jobs are created for the steps in the flow, we need to create the pipeline view. From the Jenkins dashboard—where you'll see all of the jobs you've just created—click on the + icon, and select 'Build Pipeline View' and give it the name falcon. For the initial job, select falcon-build. This is the first step in the pipeline. The plugin will then automatically be able to figure out the pipeline based on the post-build steps, both automatic ones (as used for continuous) and manual ones (for staging and production). It's a nice idea to set the number of display builds to 10 or more so that you get a short historical view of the flow. Once that's all done, you'll have a nice pipeline setup in Jenkins! When you next push some code up, you should see the falcon-build job run automatically. Upon passing, falcon-continuous-deploy will then run. At this point, it requires manual intervention to deploy to the next step using the build button in the falcon-staging-deploy box. Same deal with production. We really love using this workflow at Made, and have noticed big improvements in speed of delivering our projects. If you need to convince someone higher up the chain, share the big benefits with them: Reduce merge headaches Find errors and resolve them quicker Centralised location for deploys Faster, repeatable deploys Visibility and traceability of all deploys – you know who, when and what was deployed Faster feedback from clients and end-users Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-04-23"},
{"website": "Made-Tech", "title": "Spree vs Shopify", "author": [" Fareed Dudhia"], "link": "https://www.madetech.com/blog/spree-vs-shopify/", "abstract": "What is Spree? Spree is a full featured e-commerce platform written for the Ruby on Rails framework. It is designed to make programming commerce applications easier by making several assumptions about what most developers needs to get started. What is Shopify? Shopify is a complete e-commerce solution that allows you to set up an online store to sell your goods. It lets you organise your products, customise your storefront, accept credit card payments, track and respond to orders — all with a few clicks of the mouse. What does this actually mean? The key difference between the two is that Spree is an e-commerce platform, while Shopify is an e-commerce solution. Your Shopify store is created and hosted for you. Shopify handles security, and is relatively easy to extend or integrate with other services via the Shopify App Store. If your business simply wants to start selling products online, Shopify is a great choice. For a Shopify store, operating and implementing basic customisations (such as changing a theme) is possible without any code or deep technical knowledge. On the other hand, deeper customisations require both programming knowledge and intimate knowledge of the Shopify platform itself. For example, integrating with a shipping carrier that Shopify does not support can be a very difficult problem to fix. Developing features specific to your business can range from difficult to simply not possible. When using Shopify, your business is, for better or worse, locked into the Shopify way of doing things. There will always be large parts of your system that you cannot change. Spree is a free, open-source platform. The benefits for a business of using open-source software are beyond the scope of this article but are varied and well documented. Spree being a platform is an important idea because it means that technical knowledge is required to start selling products with Spree. The system must be installed and hosted manually. Most companies that sell products through Spree either have a dedicated IT team to handle the storefront, or alternatively employ a company that specialises in Spree to implement and maintain the storefront for them. Luckily there are proven companies out there with great track records of rapidly implementing, customising and maintaining beautiful and robust Spree-based storefronts for you ( hint hint ). The main upshot of using Spree over Shopify is in the ability to tailor the entire e-commerce system to meet any and every business requirement: Launching a new product and want to provide your favourite customers with the ability to order first? Want to test out two different layouts or features on your site at the same time to see which one is more effective at converting clicks into sales? Do you want to launch, say, an online magazine alongside your products which you can create, edit and release yourself as a writer, rather than a techie? Do you want to launch a mobile app that works with the same user accounts and data as your storefront? Want to get involved in serious data mining and custom analytics to completely smooth out your conversion funnel? Want to implement a loyalty program which is specifically tailored to the way your company operates? Is your business really a lot more than “just an e-commerce store”? These are all areas in which a flexible platform like Spree excels. Conclusion No one e-commerce system is ever right for all businesses. There are tradeoffs between cost, flexibility, simplicity and customisation to consider. This article barely scratches the surface, and both options have free trials available, so why not have a peek at both? Having said that, and with the preface that we may be a little biased, we believe that your e-commerce system should be a command H.Q. for your business. Your employees should be empowered to get creative and suggest new ideas without the fear of being told “I.T. says that isn’t possible”. Most of all, your e-commerce system should be built with you in mind, rather than built on what all businesses like yours have in common. Distinguishing your business from your competitors is very difficult when you’re both locked into the same technology. Indeed, this report for instance suggests that while there are around 100 times as many Shopify sites as there are Spree sites, there are only 5 times as many in the Alexa top 100k. Also, the Alexa top 10k apparently has more sites using Spree! To us, this seems to suggest that as businesses grow and adapt, the flexibility that Spree provides is what puts them ahead of the pack. In the end, it’s about choosing what’s right for your business. We haven’t met yet, so we have no way of advising you about whether Spree would be a good fit. We should probably change that . Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-04-10"},
{"website": "Made-Tech", "title": "ActiveRecord Refactoring: Presenters", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/activerecord-refactoring-presenters/", "abstract": "We've discussed using Concerns and Services to keep your ActiveRecords as healthy as they can be. These have dealt with repeated logic in models, and logic that could questionably be placed in either a controller or model. In the third and final part of this series we will be looking at presenters. In this short series I will be looking at three ways of reducing the responsibilities of your models: Concerns Services (a.k.a. Interactors) Presenters Presenters provide a way of patching additional functionality onto your model before passing it into a view. Usually these presenters will be page specific, but some may be more general and also modular or component based. In realms other than Rails, the Presenter pattern may be more commonly known as the View Model pattern. The term \"View Model\" helps to explain what the pattern is for, i.e. a view specific domain model, a place to put logic that adapts a domain model for a particular view. \"Presenter\" equally captures the idea, a proxy object that presents a model to a view. Typically presenters are a delegation object to the domain model. In other words, a presenter will encapsulate a model and for any method that does not exist on the presenter it will delegate back to the original object and call the method on it. Presenters are also a form of decorator, an object that adds functionality onto the object it decorates. In this case the presenter object is decorating view specific functionality onto a model. Let's dive into an example. A page specific presenter maybe UserProfilePresenter. UserProfilePresenter wraps the User model for the UsersController#view. class UsersController < ApplicationController\n  def view\n    @user = UserProfilePresenter.new(User.find(params[:id]))\n  end\nend As you can see, we pass the model into the presenter in the controller action. The view will then use '@user' just like you would use a model directly. We could of course pass 'User' in as '@user' and you should until you need to write some view specific logic. Say we need to use the username if a full name is not available for the page title of our profile page: require 'delegate'\n\nclass UserProfilePresenter < SimpleDelegator  \n  def page_title\n    I18n.t('user.profile.page_title', name: full_name || username)  \n  end\nend Here we've used a translation and passed 'full_name || username' as a variable. Of course we could have placed this line in our view, however '@user.page_title' is much tidier. If we want to display whether the user is online or the time they were last online, we can place this logic in an '#online_status' method: require 'delegate'\n\nclass UserProfilePresenter < SimpleDelegator\n  def online_status\n    if online?\n      I18n.t('user.profile.online_status.online')\n    else\n      I18n.t('user.profile.online_status.last_online', last_online: last_online)\n    end\n  end\nend Again, we call on our translations, but use some additional logic to decide whether to display an online message or last online. At this point you may be wondering where the methods '#full_name', '#username', '#online?' and '#last_online' come from. The trick is in 'SimpleDelegator'. This class included in Ruby's standard library provides an '#initialize' method that takes an object. As you can see in our 'UsersController' above, we passed in a 'User' object. Any method that does not exist in 'UserProfilePresenter' will be called on the 'User' object we passed in. This means our presenter will have all the methods our model has, any overrides we define and also new methods we define. This is useful since we will often want to use model methods directly in our views without modification. There is no need to reinvent the wheel with presenters. Sometimes you will have methods used in multiple 'User' related views. Take our '#online_status' from the previous example. If this was needed in multiple views then we have a few options. The first option would be to move it into the 'User' object. However if we want to keep our view related logic, especially translation specific calls to 'I18n.t', then we might want to put it somewhere else. Another option could be to use a helper, 'UserHelper' could well have a method like so: module UserHelper\n  def user_online_status\n    if @user.online?\n      I18n.t('user.profile.online_status.online')\n    else\n      I18n.t('user.profile.online_status.last_online', last_online: @user.last_online)\n    end\n  end\nend However, my preference would be to create a 'UserPresenter' mixin. class UserPresenter\n  module Mixin\n    def online_status\n      if online?\n        I18n.t('user.profile.online_status.online')\n      else\n        I18n.t('user.profile.online_status.last_online', last_online: last_online)\n      end\n    end\n  end\n\n  include Mixin\nend Here we have 'UserPresenter' which we can use standalone, and also 'UserPresenter::Mixin' that we can include in more specific presenters like 'UserProfilePresenter'. Great! Presenter objects make testing more complex edge cases of views easier. They don't replace acceptance testing using capybara but they do help reduce the need to test views at the unit level. Hopefully you will find use in presenters as I do for when your ActiveRecord models begin filling up with view related logic! Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-03-23"},
{"website": "Made-Tech", "title": "You don’t really want a CMS", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/you-dont-really-want-a-cms/", "abstract": "Customers often come to us with a requirement that they need a CMS (Content Management System) to drive their website. When we explore this a bit more, we're generally able to re-express this requirement such that the customer would like the ability for non-technical people to be able to manage content on their website. Depending on the specifics around this need, we've found more often than not that a CMS isn't the right solution. You probably want a CMF A CMF (Content Management Framework) is a slightly different way to deliver content management capability in a web application. In a traditional CMS setup (the likes of Sitecore , Drupal , and Adobe CQ ) all of the functionality of the website is generally built inside the CMS. And it turns out that while these CMS systems do the content management stuff pretty well (though in the case of Drupal we might dispute even that), what they don't do is provide a particularly productive platform on which to build rich customer-facing functionality. The Content Management Framework approach takes the opposite approach: start with a web framework that is built for delivering rich customer-facing functionality quickly, and then plug in the required content management capability on top of this. Built for purpose, not from the ground up Using a CMF shouldn't mean reinventing the wheel. By having the building blocks in place, a developer should be able to implement the required content management capability in just a few lines of configuration – but having a solid framework to fall back on when more bespoke needs arise. In our experience, we've found pre-packaged CMS platforms tend to fall down when more bespoke needs become apparent. If the needs aren't catered for out the box (and even if these needs are apparently catered for now, one should bear in mind how often requirements can change and be added during the life of a software product), developing more custom functionality often proves complex and costly. We've often seen procurement processes where every currently conceivable content management capability is listed as a requirement (even where there's no immediate need), with a view to ensuring the platform that is chosen provides this capability out of the box. This route fails for a couple of reasons: firstly, people are rubbish at predicting the future. Your requirements in two years time almost certainly do not fit neatly in to everything you've managed to list down today. And secondly, you're likely to be missing the most important requirement, and that is to choose a software platform that is designed to adapt to change. A better user experience We see feature-rich pre-packaged CMS platforms as victims of the 80/20 rule: whereby only a small subset of the functionality is actually required on any particular project. What this can often lead to is a hard-to-comprehend user experience for those using the CMS, where too many options and features that aren't used get in the way and provide confusion. The CMF approach, by nature of delivering a tailored set of functionality, also provides an equally tailored user experience. An architectural decision For a platform of any significant scale, there is usually a significant benefit in dividing the functions of the system in to discrete services (ala Service Oriented Architecture or Microservices ). Compared to the big-box-of-everything route that you almost always end up with by pursuing a monolithic CMS platform, this approach provides much stronger future proofing by allowing you to replace smaller components as they reach their end of life and move away from the traditional throw it all away cycle every 5 years. For just about every requirement we've seen, when we've interrogated them hard enough, we've discovered that the longer term needs are better served by first choosing a strong framework on which to build the application, and then plugging in content management capability that does just that: content management. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-04-14"},
{"website": "Made-Tech", "title": "Scalability with Spree Commerce", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/spree-scalability/", "abstract": "“Does Spree Commerce scale?”, “How many products can Spree handle?”, “How many concurrent users can Spree support?”, “What is the most popular Spree Storefront?” These are some of the questions we're asked when discussing Spree Commerce with new customers. You'll notice they are all about 'scalability' and whether Spree can cope with high levels of traffic or large product catalogs. These questions are understandable and the sort of things you should be asking about a relatively new platform. I hope this article will provide some reassurance that Spree is more than capable of scaling to very large numbers. We'll demonstrate the approach we take to scalability and what you should be doing to understand your systems constraints and the points at which you're going to need additional capacity. When people talk about scalability, they tend to be referring to one of two things: a) throughput or b) Product Catalog size. In this post, we're going to focus on throughput. What is throughput? Throughput is the number of requests your application can serve in a given time period. The higher this value, the more scalable your application is. Your application throughput is likely to vary between pages (as functionality and resource requirements will differ) and throughput will be constrained by the compute resources that you have available. e.g. your server type, server size or number of machines within a load balanced cluster. How does it affect scalability? The scalability of your application is directly linked to its throughput, as the more requests your application can serve, the more scalable your system is going to be and the fewer compute resources you are going to need. We've found the best way to get an accurate picture of your Spree scalability is to run volume tests. Volume tests are a technique we use to simulate large numbers of users accessing the store. They provide a realistic measure of how the store will perform under significant load. To run volume tests, you need to setup a server environment which mimics the resource constrains of your production environment. These constraints will vary between hosting environments, so it's very important that you benchmark on the exact same environment that you'll be using for production, or the results will be of no use. To begin the volume test, you will need to define a number of scenarios which mimic what your users would do on the site. They could be something like: 1. Visit Homepage 2. Visit Product Listings 3. Add to Basket 4. Add Coupon Code 5. Enter email address It's important to bear in mind that different pages on the site will have different throughput, so only testing pages with high throughputs will provide inconclusive results. You can use tools like NewRelic and Google Analytics to get an idea of the throughputs on your pages, and the user journeys customers take. Once you have these defined, you should write them in a format that a volume testing tool can consume. We've used BlazeMeter and LoadImpact to volume test Spree in the past, but other tools are available. Running the Volume Test Next you need to run the test. You should define the number of Virtual Users (VUs) you want to concurrently access the storefront and the period of time you want the test to run for. We tend to start with ~50 concurrent VUs for 5 minutes and increase from there. As you increase the number of concurrent users, you should be looking for your application performance to remain fairly consistent. If you see your response time decrease, this is a sign that optimisations need to be made. Benchmarks In the benchmarks that we ran, we deployed the standard Spree 2.4 storefront within a load balanced AWS environment, which had 2 Large Amazon EC2 instances running 14 Unicorn workers on each and was backed by a single large AWS RDS instance. This setup scaled out to approximately ~4800 requests per minute and to 30,000 orders per day. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-03-18"},
{"website": "Made-Tech", "title": "jQuery Navobile", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/jquery-navobile/", "abstract": "Last year we built and open sourced Navobile , a lightweight jQuery plugin to easily add an off-screen menu to your website. Since then it has been implemented countless times by ourselves and the wider community. Whilst the docs are are pretty self explanatory for someone confident with jQuery, we have never written a tutorial to guide a novice front end developer. **The Basics** Implementing Navobile is simple and requires a slight change to your html – an additional class/id and maybe a . However, it's worth doing it at the outset of a project rather than as an after thought. The barebones example in the Github repo demonstrates what is required in terms of HTML, along with the Navobile CSS and JS. Below is a very basic example: That is essentially it, and all you need to do now is bind Navobile on document ready: $(document).ready =>\n    $('#navigation').navobile() **Next steps** Navobile uses CSS transformations to achieve a smooth open/close where possible and if you are already using Modernizr in your project then you won’t need to do any more work to activate them. If you aren’t and you only want a minimal install, the only features Navobile requires are: CSS 2D Transform CSS 3D Transform Touch Events This helps us keep filesize down as we take advantage of Modernizr and the classes it creates on the html tag. **Customising via config** Navobile has a growing list of possible configuration options, a full list of which can be found here but the most commonly changed are: For example: $(document).ready =>\n    $('#navigation').navobile\n      cta: '#my-cta'\n      content: '#my-content'\n      changeDOM: true\n      copyBoundEvents: false Customising these parameters will suffice for most implementations, but in some advanced cases where you might want to trigger other plugins/events Navobile fires off some events of its own. **The Events** All of these are triggered on the document, e.g. $(document).on 'navobile:open’, =>\n     alert 'navobile is opening’ **Experimental Features** When used in conjunction with HammerJS (and the required config), Navobile has access to some experimental features. These features while they do work, they can be temperamental so can’t be deemed release ready. **The Future of Navobile** We have a few additional features in the pipeline for Navobile, like: Disabling the scrolling of body content when Navobile is open. Clicking or tapping the content area to close Navobile. But we’d also like to know what features you’d like added, so if you have a suggestion that you feel could improve Navobile either create a pull request (we always welcome new contributors) or raise an issue on the Github project page . Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-10-22"},
{"website": "Made-Tech", "title": "Finery Website Launches", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/finery-website-launches/", "abstract": "We're particularly proud (and a little tired!) to have publicly launched the Finery website today. Finery is a new online womenswear fashion label founded by the former Fashion Director of ASOS and the former Design Director of Topshop, with backing from Rocket Internet. We've been working with the team since July last year to develop the all new online storefront. Building on top of the Spree platform, we created a mobile first eCommerce experience, with integrations ranging from fulfilment platforms to data warehouses delivering a truly Internet-scale selling platform. With Finery having already picked up plenty of accolades in the press, from The Telegraph to Time Out , we're excited to see the public launch roll out over the coming days, and we will be continuing to work with the team over the rest of the year to roll out additional features and conversion optimisations. The website first opened its doors last November to a few thousand fashion industry and press folks. During this beta period we worked to streamline internal business processes, and incorporated our findings on how customers were using the site, all while delivering releases to production every 2 or 3 days. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-02-05"},
{"website": "Made-Tech", "title": "The Hierarchical Model-View-Controller Pattern", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/hierarchical-model-view-controller-pattern/", "abstract": "In this post I'm going to talk about a software architecture pattern we  use when we have a number of dynamic content types shared across multiple pages. It's called the Hierarchical Model-View-Controller pattern and it is an architectural pattern that would work well for heavily 'componentized' sites like BBC News or Guardian. In a traditional MVC web application, a routing component will handle the URL request. This request will be mapped through to a Controller action, where business logic is initiated and then passed through to the View for rendering. This works well for simple applications, but once your application grows in complexity, you tend to find you need to share functionality across controllers. public function indexAction()    \n{\n     $promos = $this->get('example.promotion_repository');        \n     $social = $this->get('example.social_repository');\n     $news = $this->get('example.news_repository');        \n     $locale = $this->get('request')->getLocale();        \n     $viewBuilder = $this->container->get('example.view.builder');        \n     $viewBuilder->assign('latest_news', $news->getByTypeAndPlace('latest', $locale));        \n     $viewBuilder->assign('primary_promotions', $promos->getByTypeAndPlace('primary', $locale));        \n     $viewBuilder->assign('latest_tweets', $social->getByTypeAndPlace('twitter', $locale));   \n     return $this->render('ExampleBundle:Home:Index.html.twig', $viewBuilder->getData());\n} This can be done in a few ways, but typically, it's by moving the shared functionality into super classes (like an ApplicationController class) or by abstracting this functionality into a service (or similar) which can be initiated in multiple actions. However neither of these are ideal solutions, as the responsibility for  rendering the content ends up duplicated across multiple controller actions. This is where HMVC comes into play. This pattern allows you to issue sub-requests, which are initiated by the view once it knows what content is required (think of it as a tree of MVC requests). So if your page is made up of, for example, latest news, tweets, product ranges and advertisement content, you'd have the base controller request and then four additional HMVC requests to provide content for the areas. {# app/Resources/views/base.html.twig #}{# ... #}    \n{{ render(controller('AcmeArticleBundle:Article:recentArticles', { 'max': 3 })) }} So why do we like this pattern? Mainly because it keeps our controller actions small and makes them far more reusable. Each controller action becomes responsible for precisely one thing and this helps us to adhere to SRP . Try it out, let us know what you think. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-10-01"},
{"website": "Made-Tech", "title": "Pulling with the –rebase flag", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/pulling-with-the-rebase-flag/", "abstract": "There is no denying that Git is a power tool.  Power tools command discipline and mastery to be used effectively. Git –rebase is an indispensable feature on this power tool, and when used correctly can provide many benefits. The Value of a Clean Commit History As programmers we value clean code.  We follow standards, use linters and do code reviews, to name just a few ways we facilitate this. So why put up with a less than perfect git history? One could argue that the commit history of a project is less likely to change than the source code itself. The commit history contains valuable metadata about the project, and should be treated as such. When you have a few developers working on the same project, there are guaranteed to be merge conflicts. This can lead to a lot of merging, and creating noisy merge bubbles and a bloated commit history. You could say that explicit merges have an adverse effect on the conciseness of our commit history. Atomic Commits When you do continuous integration, the goal is to keep each commit atomic and passing the tests. With this in place, you have great flexibility in managing your project. You could benchmark each commit to keep an eye on performance regressions, roll back to a previous commit and deploy it out to production with confidence, or run a Git bisect to find bugs. Thought-provoking metadata tends to just fall out of a clean linear Git history. Basics Since Git is distributed , we all get our own copy of the entire history of the repository. You can do a Git pull (composed of git fetch && git merge), or you can pull and rebase the changes. The difference is that the rebase will replay your local commits on top of the new commits from fellow team members, while merge preserves the exact commits. Rebase does not preserve metadata about the exact time that a commit was made, and only cares about when the commits were pushed. Both histories are preserved.  This creates a divergence and leaves an undesirable \"merge bubble\" as seen above. Pull & Rebase The resulting modification to your source code is identical to the non-rebase approach. The only difference is in the recorded metadata. Merging Under the Hood When you do a normal merge on diverged commits, Git takes 3 commits into consideration (Also known as a 3-way merge). Your current commit, the tip of the branch to be merged, and the base parent. When you rebase, Git will change the base parent to be the tip of the other branch, and replay the commits on top of that. Danger? Isn't using rebase changing the history of what's already been done (which is obviously bad)? Coming from a mercurial background, I feel almost compelled to agree with this, but it is actually not at all that uncommon in the Git world. There are strong, conflicting opinions on this topic, and I won't expand on it too much but I think it's a reasonable thing to do. There is a lot of valuable information recorded by a Git commit, but most merge commits do not contain anything of significant value. When Not to Rebase When you want to preserve explicit merges. At times you may want to keep a commit that represents the action of resolving the merge conflicts. I cannot think of a situation where this information would be valuable in terms of a build pipeline, but you may want it for debugging purposes. Dealing with Git Rebase Merge Conflicts Dealing with conflicts when the rebase flag is set is a little different from regular merge conflicts. The first big difference is that you are dropped onto a temporary branch where you should resolve your conflicts. You can use git mergetool to use the editor defined in your Git config, or you can correct the conflicts manually. Either way, once done, you need to add your file to the index when you are done. This is the way that you signal to Git that you have resolved the conflicts. Another important thing to note is: You should not commit your file after adding it. Also (from the rebase help page): \"Commits in HEAD which introduce the same textual changes as a commit in HEAD.. are omitted\". Instead run git rebase --continue as instructed in the prompt. Git will do what it needs to to rebase your work on top of the other commits. If all went well, you will be back on the branch you were working on with no outstanding commits, and a clean commit history. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-10-15"},
{"website": "Made-Tech", "title": "Ryvita Website Launches", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/ryvita-website-launches/", "abstract": "We're pleased to have launched the all new Ryvita website . Sharing the same foundations that we put in place for sister ABF brand Jordans Cereals , we delivered a fully content managed responsive website and eCommerce platform. Now that both sites share this common platform, any future enhancements can be easily shared across both brands, removing the waste associated with managing two isolated applications. The site provides the capability to easily run multivariate tests, allowing us to work with the team at Ryvita to continue to optimise the performance of key on-site functionality. The launch includes a number of A/B tests that are tracked through the analytics dashboard. Following this first phase release, we'll be working over the coming weeks to launch additional content areas of the site. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-09-01"},
{"website": "Made-Tech", "title": "ActiveRecord Refactoring: Services", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/activerecord-refactoring/", "abstract": "Continuing from my previous post , which introduced the idea of concerns to your models, in this post I will be discussing using services to keep your ActiveRecord's responsibilities toned down. In this short series I will be looking at three ways of reducing the responsibilities of your models: Concerns Services (a.k.a. Interactors) Decorators Services, also known as Interactors, are a Rails pattern with the aim of keeping the logic in a controller clear and concise without passing off controller-like responsibility to the model. Another major reason for using this pattern is to increase the amount of code you can test in isolation without booting Rails as abhorred by DHH and taken to crazy lengths in Jim Weirich's Decoupling from Rails talk. So what do I mean by controller-like responsibility? Controllers are the glue between the rest of your application. In the Rails world a controller is generally an interface for the CRUD operations of an ActiveRecord model. This involves handling the HTTP parameters of a request and using them to change data in an application, e.g. updating a user's profile. When making such a change the controller must ensure the model was happy with the update made by checking for validity. It must then pass errors back to the user if any occurred, say if a required field was missing, so that the user can correct the problem. On successful updates a controller will redirect and set a success flash message. Controllers are also used for sending mail using an ActionMailer class. Generally speaking this happens after another action of some kind, e.g. sending a welcome email after a user registers. These are just some of the controller-like responsibilities I see in applications I work with. I'm sure there are many other things they are used for. You may now be asking, what does this have to do with refactoring ActiveRecord models? Well in my experience you often see some of the responsibilities listed above leak their way into other parts of the application. Let's look at two examples that illustrate this. The first example looks at the need to create new Artists when creating a new Event for a gig listing site. class Event < ActiveRecord::Base\n  has_and_belongs_to_many :artists\n  accepts_nested_attributes_for :artists\nend\n\nclass Artist < ActiveRecord::Base\n  has_and_belongs_to_many :events\nend Okay, so ActiveRecord handles most of the logic for the relationship, you'd likely be calling Event#create in your controller #create action: class EventsController < ApplicationController\n  def create\n    @event = Event.create(params[:event])\n    if @event.valid?\n      flash[:success] = 'Saved'\n      redirect_to events_path    \n    else\n      render :new\n    end\n  end\nend Now things get hairy if we want to email each artist to say they've been added to an event. I often see this placed in the model: class Events < ActiveRecord::Base  \n  has_and_belongs_to_many :events\n  after_create :notify_artists  \n  def notify_artists\n    artists.each do |artist|\n      EventMailer.notify_artist(self, artist).deliver_later\n    end  \n  end\nend Your controller can stay the same. This isn't so bad right? But what if you then need to add a delayed job to poll for the artist's SoundCloud details? class Events < ActiveRecord::Base\n  # ...  \n  after_create :poll_soundcloud\n  def poll_soundcloud\n    artists.each do |artist|\n      ArtistSoundcloudJob.perform_later(artist)\n    end  \n  end\nend Now the Events model has methods for sending welcome emails to artists and requesting jobs to be performed later. At first glance this might not seem too problematic. When you test the Events model however you will now need to stub or mock test the calls out to EventMailer and ArtistSoundcloudJob . The responsibilities grow as we add more emails and jobs. You might send the event creator an email, or grab the tweets of the artists too. You might alternatively like to keep this logic out of your models data layer and instead call within your controller. class EventsController < ApplicationController\n  def create\n    @event = Event.create(params[:event])\n    if @event.valid?\n      notify_artists\n      poll_soundcloud\n      flash[:success] = 'Saved'\n      redirect_to events_path\n    else\n      render :new\n    end\n  end  \n\n  def notify_artists\n    @event.artists.each do |artist|\n      EventMailer.notify_artist(@event, artist).deliver    \n    end  \n  end  \n\n  def poll_soundcloud\n    @event.artists.each do |artist|\n      ArtistSoundcloudJob.perform_later(artist)\n    end  \n  end\nend The logic is now held by the controller. This can grow quite large too however. Think about when we add the email notification to the event creator, or grab the artist's tweets. Not only that but we might add emails and jobs for the other CRUD operations, e.g. sending artists emails if the event is cancelled. These are all realistic possibilities. Enter the Service class. Generally a service will handle one CRUD action. You might have classes such as EventCreate , EventUpdate and EventDelete for the Event model. Let's move the #notify_artists and #poll_soundcloud logic into an EventCreate service. class EventCreate\n  def exec(attrs)\n    event = Event.create(attrs)\n    if event.valid?\n      notify_artists\n      poll_soundcloud    \n    end    \n    event  \n  end  \n\n  def notify_artists\n    @event.artists.each do |artist|\n      EventMailer.notify_artist(@event, artist).deliver\n    end\n  end  \n\n  def poll_soundcloud\n    @event.artists.each do |artist|\n      ArtistSoundcloudJob.perform_later(artist)\n    end\n  end\nend The controller should be updated too. class EventsController < ApplicationController\n  def create\n    @event = EventCreate.new.exec(params[:event])\n    if @event.valid?\n      flash[:success] = 'Saved'\n      redirect_to events_path\n    else\n      render :new\n    end\n  end\nend The controller looks very much as it did at the start but it calls EventCreate#exec instead of Event.create . The difference lies in the fact that neither the controller nor the model now know about mail sending and job queuing, and we gain a number of advantages from this. We can create services for each CRUD operation and thus keep class length down which means classes are easier to read, understand and maintain. Since classes are smaller, the unit tests too can stay smaller. There is less mocking when testing controllers and models. In a controller you can stub or mock the EventCreate out completely. In the model you needn't stub or mock anything. In the service itself you can stub or mock the model, mailer and job queuer. So there you have it. An explanation on using services to keep logic out of ActiveRecord models and your controllers too. Next time I will be discussing the use of decorators and how you can keep your models and views clean with them! Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-10-08"},
{"website": "Made-Tech", "title": "Why a Single Source of Truth Matters", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/why-a-single-source-of-truth-matters/", "abstract": "With many of our clients, we see issues of data fragmentation: data that is duplicated in many places, often updated on a casual basis, and with little clarity as to which copy of the data is the most current. This data could be anything from product stock levels, through to customer contact information or retail sales figures. We would describe this data as not having a single source of truth, and we see organisations encumbered by a number of common challenges that stem from not investing in a single source of truth for their business intelligence: Workflows are slowed down All too often we see that people's day-to-day workflow is slowed down by not having confident access to business data. We discover that time is often spent checking with others in the organisation as to the validity of a particular dataset: often requiring the employee to wait for a response which may slow them down by hours or days in their original task. Decision making is impaired Organisations should be driving their decision making by using as much data as is available. Where there is uncertainty as to the currentness or validity of data, organisations are often unable to answer what should be relatively trivial, though important, questions such as \"how many units of Widget D did we sell in the UK in February 2014?\" or \"how many products were returned due to quality issues in the second quarter 2012?\". When these simple questions aren't easily answerable by an organisation, we typically find that they're unable at worst, or slow at best, to use data to make effective decisions. Particularly for organisations who rely heavily on forecasting, such as those who have an element of slower manufacturing as part of their process, there can be significant costs in terms of their ability to bring products to market fast enough, or to bring the right products to market at all. Ability to innovate is slowed An organisation that suffers from data fragmentation often finds that their ability to innovate and implement new systems is slowed. An organisation that is not in control of their customer data would find it harder, for example, to adopt a new CRM system that perhaps offers some exciting new functionality to capture customer data at the point of sale in third party retail stores, as there would be a lack of clarity as to where that customer data should ultimately reside, how to match offline customer sales to online customer sales, how to match retail purchases made by customers in different physical stores, and so forth. People spend too much time copying data While not always the case, the symptoms previously detailed usually hint that people are spending too much of their time performing manual and error prone copying of data from one system to another. Every time somebody manually inputs data is another opportunity for errors to be introduced. Not to mention, it's seldom a good use of your employees' time to be inputting data, both from a cost and motivational perspective. On their own and combined, these challenges too often put businesses at a disadvantage, slowly nibbling at the edges of operational efficiency, offering up real barriers to an organisation making timely decisions, or preventing the adoption of new innovations. With the increasing adoption of (or the re-labelling of existing initiatives as) \"big data\", the importance of data integrity and an organisation's ability to mine this data for insight becomes ever more apparent. By ignoring, or working around, source of truth challenges in your organisation, you're likely falling in to an ever-less competitive position. In a later post we'll be looking at techniques we've been able to employ to move organisations from having fragmented business data to benefit from a single source of truth. At the very least, defining which copy of data – if you absolutely must have data exist in more than one place – is the \"master\", and by automating as much of the work involved in duplicating data as possible, an organisation can start to make real steps forward in its business intelligence efforts. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-06-30"},
{"website": "Made-Tech", "title": "User Experience & Enterprise Software", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/user-experience-and-enterprise-software/", "abstract": "One of the big challenges all companies face is trying to find software that helps its workforce to do their jobs in an effective and efficient manner. Most large companies end up purchasing a broad 'enterprise' solution from a vendor like SAP, Salesforce or Microsoft and try to customise this to meet their internal needs. Whilst this has the virtue of providing a so called 'off-the-shelf' solution, it also suffers from compromises that often follow a ‘one-size-fits-all’ solution. One of those compromises is user experience. There’s the perception that solutions from big IT vendors are more secure, reliable & flexible (read enterprise ready ) and that once a business hits a certain size, this is the route it has to go down. However this is often not the case. It's common to hear of cases where enterprise software had become a bottleneck for a business and caused serious problems further down the line. The reality is enterprise software can be slow and difficult to use because it's designed for an imaginary industry-wide user, with little regard for the domain specific requirements of individual industries or sectors. The 'one size fits all' approach means the application footprint is large. Instead of being lean and well-tailored to the specific challenges that a company faces, these enterprise packages are full of unused code bloat that requires significantly more processing power, memory and storage. This often causes a slow end user experience for the consumer. The systems requires a lot of training and a significant effort to get up to speed with and make use of, as the terminology and processes are unfamiliar to the user. In many cases, this can be so challenging for members of the workforce that they will simply not use the systems. You even see companies pass on hiring good workers solely because they aren’t already trained on the package! These two quantifiable (yet generally unseen) costs of poorly designed user experiences in enterprise software end up reducing morale and can reduce productivity and efficiency. In a system which has a well designed user experience, a new starter should be able to quickly and easily perform tasks and access any information required. In turn, this should reduce the amount of training and support and can boost morale as workers spend less time griping about bad software and begin to feel empowered in doing their jobs. In many cases, it can be a better choice for a organisation to look at a more bespoke open source solution rather than diving in and purchasing a one-size-fits-all from one of the big IT vendors. We note that this month ThoughtWorks Technology Radar has put a ‘Hold’ status on ‘Big enterprise solutions’ . We see attraction in flipping the enterprise model on its head. Rather than starting with a huge platform that includes most of what you do want and a whole load of what you don't: pick and choose a smaller collection of things you do need. In keeping with the continued industry-wide adoption of microservices , we've seen success in pulling together a collection of smaller solutions that are best-fit for the organisation's domain, presenting them as a cohesive and tailored user experience. We expect that over the coming years you’ll see fewer ‘big enterprise solutions’ and a shift to organisations adopting industry-specific technology platforms that have a user experience tailored for the needs of that domain. We hope to see less enterprise bloat, fewer multi-year platform procurement and rollout plans and user experiences that actually work for users. Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-06-04"},
{"website": "Made-Tech", "title": "Ensuring the Code Quality of Your SCSS", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/ensuring-the-code-quality-of-your-scss/", "abstract": "At Made we create web software that is built to last, so every aspect of the codebase has to be easy to understand and maintainable by any member of the team. Because of this we run our server side code through tools like Tailor and Cane to keep complexity low and consistency high. To keep our front end styles at the same standard we rely heavily on the front end team to produce modular and stylistically consistent CSS. It’s all too easy to mistakenly wrap a background URL in double quotes when only single quotes are needed, or throw a global style in, whether it’s down to laziness or by mistake. With the aim of enforcing this consistency many front end teams choose to employ a naming convention, as well as other practices like sorting CSS declarations alphabetically. **SMACSS and BEM** There are many routes available to a front end developer wishing to write modular, scalable stylesheets. Broadly speaking they all do the same thing: they recommend a file structure, naming conventions and methodologies for inheritance. However it is most likely the established naming conventions that will dictate which one you use, primarily down to a personal preference. Some, like BEM, are very specific and lead to some very verbose classes, and are meant to be coupled with others like SMACSS. **Scalable and Modular Architecture for CSS** (**SMACSS)** I bought the SMACSS book a while ago, when I first started looking into best practices for CSS, and I really liked some of the concepts. In SMACCS you modify your modules by doubling up the base module with the modified module styles. This is great if you aren’t using SASS/LESS but you could easily end up with a “class soup” situation on your markup. However, the concept of applying state based CSS classes is great, and when this is coupled with JS you can achieve rich animations without writing CSS in JS. Unfortunately, SMACCS does do some things I don’t agree with, the main one being using IDs to style layout elements like headers and footers. Having IDs in your markup to be used as targets for your JS is fine, but in CSS they should be avoided in my opinion. Good CSS should be repeatable, yet IDs are not, unlike classes. In addition to that, IDs also add a high level of specificity to the styles applied to an element, which can lead to style inheritance hell in the long run. **Block Element Modifier (BEM)** Once I had started reading into ways to better my CSS I soon discovered BEM. BEM is a more OO way of naming your CSS and applying it to your HTML markup (and yes, I know OOCSS is a thing too). As the name suggests the BEM naming convention is quite simple: you namespace a module into a block (e.g. header) and then any elements and modifiers on those elements get delimited by double underscores (“__”) and dashes (“—“) respectively. The biggest pro of BEM is also its biggest con, which is the ability to get really specific with naming your classes. When used well this can lead to really succinct class names that describe their blocks and elements perfectly, but when used poorly you could end up with an eighty character-long class, which isn’t ideal. **Pre-processors Make It All So Easy…** Writing all these rules in CSS can be very laborious. Pre-processors like LESS and SASS eliminate many of the pain points that exist in CSS and they can make your modular CSS clearer too. Usage of a pre-processor can also help us with writing SMACSS and BEM. Through the usage of placeholders/extend declarations and mixins you can refactor common styles and apply them where needed. SMACCS subclassing: .thing {  \n  &-subclass {...}\n} becomes: .thing-subclass {...} BEM modifiers: .block {  \n  &__element { \n    &—modifier {}\n  }\n} become: .block__element--modifier {...} We Chose… BEM, and applied the stateful concepts from SMACSS. **Enforcing Quality** As with our back-end code, automating the quality control of our SCSS is important. Whilst searching for a solution to this I found SCSS Lint , a Ruby gem (there is also a gulp module) that can be used either as a standalone linting tool or as part of your CI workflow. SCSS Lint comes bundled with 37 configurable linting rules so you can tailor the behaviour of the linter to your own requirements. One of the more useful linting rules is the ability to specify a naming format , with one of the two out-of-the-box formats being BEM. If you are a Sublime Text 3 user you can also make use of the SCSS lint extension for SublimeLinter. **Getting SCSS Lint Ready for CI** One of the available output formats of SCSS Lint is an XML file that can be consumed by Jenkins CI . So if you piped the stdout into a report xml file this can then be integrated into your Jenkins build configuration. You could then fail a build if any of your front end team get sloppy. The below is an excerpt from our front end test suite: export FAIL_SCSS_WARNINGS=false\nmkdir -p reports\nbundle exec scss-lint --format=XML app/assets/stylesheets > reports/TEST-scss-lint.xml\n_scss_violations_errors = ` grep -o 'severity=\"error\"' ./reports/TEST-scss-lint.xml  | wc -l`\n_scss_violations_warnings = ` grep -o 'severity=\"warning\"' ./reports/TEST-scss-lint.xml  | wc -l`\n\nif [ $_scss_violations_errors -gt 0 ]; then\n  echo -e \"e[31mNumber of SCSS errors: $_scss_violations_errorse[39m\"\n  exit 1;\nfi\n\nif [ $_scss_violations_warnings -gt 0 ]; then\n  echo -e \"e[93mNumber of SCSS warnings: $_scss_violations_warningse[39m\"\nfi\nif [ $_scss_violations_warnings -a $FAIL_SCSS_WARNINGS ]; then\n  exit 1;\nfi tl;dr Whatever your tool chain, it’s important to enforce consistency within your team, and this applies just as much to your front end as it does to your back end. One way of introducing common practices is through using naming conventions like the ones used in SMACSS and BEM. Using a CSS pre-processor like SASS can make using these naming conventions faster. If, like us, you use SCSS then perhaps SCSS Lint will solve this for you too. SCSS Lint can also be combined with Jenkins CI (perhaps using the snippet above) to fail a build if you want to take it to such levels. Useful Links BEM SMACSS SCSS Lint for SublimeText Example .scss-lint.yml Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-07-16"},
{"website": "Made-Tech", "title": "Spree meets Travis CI", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/spree-meets-travis-ci/", "abstract": "At Made we love using Spree, it is the platform that powers all our recent eCommerce builds. Spree has a great community behind it who churn out some fantastic extensions that, when combined, can pull together a platform to fulfil almost any client requirement. For me one thing that makes Spree stand out from its contemporaries is the quality and quantity of its test coverage, and the visibility of those tests. If you are going to contribute back into the community with an extension you need to be ready to build something with an equally high level of code coverage. All the the tests within Spree use RSpec, and in my opinion any developer starting out writing an extension should follow this convention. I’m not saying you have to use the RSpec matchers, in fact many extensions use the shoulda matchers , but as the convention exists it should be adhered to. Another convention that exists within the Spree community is using Travis CI to run tests. Travis CI is a continuous integration platform that integrates with any open source GitHub repository on the free pricing tier. The results of the tests can then be displayed on your extension's README, and can be used to ensure any pull requests are good to merge and won’t break functionality. **Getting Started with** **Travis** **CI** Spree core has a generator included that will scaffold an extension for you, including a basic spec_helper ready to start your RSpec tests, so once you’ve populated your gemspec and committed into GitHub you are ready to use Travis CI. Activating any open source repository is quite simple: all you need to do is sign in with GitHub and toggle a switch to add the Travis CI web hook to your repository. Then, once you have a valid .travis.yml in your repository, every time you commit a build will be scheduled. The integration tests you’ll be running will require a headless browser. This will run out of the box over CLI, but within Travis you have to boot X Virtual Framebuffer (xvfb) before you can use Selenium/Poltergeist. To use xvfb you need to pass in some commands into the before_install commands. before_install:  \n  - \"export DISPLAY=:99.0\"  \n  - \"sh -e /etc/init.d/xvfb start\" Part of the beauty of Travis CI is the ability to test your code against multiple environment configurations. It's as simple as  listing the required ruby versions and different databases adapters within the config, and your test suite will be run against every permutation. While the convention for Spree is for each version to have its own branch (and therefore .travis.yml), this isn’t necessary; as Thoughtbot demonstrates with Paperclip you can also vary the gems within a single branch. **Moar pill images!** Whilst you have (of course) created a great extension, the inclusion of a Travis CI badge in your README will give your users confidence that your extension is a quality product and integrates well with Spree. However there are plenty of other status “pill” images you can add to your repository to increase the visibility of your coding genius, many of which are listed in this gist and aren’t necessarily for RoR projects. Out of those listed, I would recommend implementing Code Climate , Coveralls and Brakeman , which are all free to open source projects. These extra pill images will measure code quality, code coverage and test your extension for any security vulnerabilities. **A basic .travis.yml** The below YAML is taken from a recent extension I wrote, it's quite basic compared to some but it is a nice starting point. language: ruby\nrvm:  \n  - 1.9.3  \n  - 2.0.0\n\nbefore_install:\n  - \"export DISPLAY=:99.0\"    \n  - \"sh -e /etc/init.d/xvfb start\"\n\nbefore_script:  \n  - \"bundle exec rake test_app\"\n\nscript:  \n  - \"bundle exec rake spec\"\n\nnotifications:  \n  email:    \n    - [seb@madebymade.co.uk](mailto:seb@madebymade.co.uk)\n\nbranches:    \n  only:    \n    - master Useful Links Spree Extension Tutorial XVFB manual Environment Variables on Travis CI Building a Ruby Project on Travis CI Travis yml linter Tweet 0 LinkedIn 0 Facebook 0", "date": "2014-06-11"},
{"website": "Made-Tech", "title": "Feature testing with RSpec", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/feature-testing-with-rspec/", "abstract": "We love writing tests at Made HQ. They are part of the foundation on which we work to provide our clients with stable deliveries. We work fast and deploy daily so we test vital paths of our applications using feature tests. We also unit test, albeit less, when we need to cover a range of edge cases. In this post I will be discussing: Briefly, what we use for our feature and unit tests How our tests are structured in our rails apps What a feature test looks like Explaining why we do things this way RSpec for both Unit and Feature testing For both feature and unit tests we use RSpec. In the past we used Cucumber for feature tests and RSpec for unit tests but recently we have questioned the need for two separate testing tools. The reasons we originally used Cucumber were primarily because its gherkin syntax gave us a concise way of describing the features of our application. We moved away because we found a way to do this in RSpec. No, we didn't decide to go with Spinach. We read this blog post . We ended up dropping Cucumber because we could do the same thing in RSpec with half the configuration. Why duplicate configuration for DB cleaning? Why maintain helper files, VCR, code coverage, factories in two places? Another big benefit to moving to RSpec was the fact we could contain our features and the code that test the features in the same file. They are still readable by non-technical people and sure, they can't be written by them, but we've never asked them to anyway. Test structure in a Rails app We setup RSpec in our Rails applications the same way for every product. Here is an example of our directory structure for an ecommerce store: spec/\n  factories/\n    order_factory.rb\n    user_factory.rb\n  fixtures/\n    cassettes/\n  features/\n    shop/\n      checkout/\n        1_address_spec.rb\n        2_delivery_spec.rb\n        3_payment_spec.rb\n        4_confirmation_spec.rb\n      cart_spec.rb\n    users/\n      registrations_spec.rb\n      sessions_spec.rb\n  support/\n    capybara.rb\n    code_coverage.rb\n    factories.rb\n    rails.rb\n    vcr.rb\n  unit/\n    controllers/\n      locale_redirect_spec.rb\n    models/\n      order_spec.rb\n      user_spec.rb\n  spec_helper.rb I've purposely listed out all the factories and support files too. This should give you a good overview of how we separate tests and configuration out. We use subfolders inside features/ to categorise groups of features. It makes it easy to navigate our features as they grow, and also allows us to easily test a subject without using RSpec tags: rspec spec/features/shop/checkout/ This command would test out our entire checkout process, for example. Although out of the scope of this blog post, I'll briefly mention the fact we split out our unit tests into types, e.g. controllers, models, lib. We then, as required, use folders under these types to indicate namespacing. This is because of the nature of unit tests closely testing a particular class rather than a larger feature. What our feature specs look like So what do our feature specs look like, I hear you asking? feature 'Checkout Step 2: Delivery' do\n  scenario 'Standard delivery' do\n    given_a_customer_has_reached_the_delivery_step\n    when_they_choose_standard_delivery\n    then_they_should_be_shown_the_standard_delivery_cost\n    and_they_should_be_redirected_to_the_payment_step\n  end\n\n  scenario 'Nominate delivery date' do\n    given_a_customer_has_reached_the_delivery_step\n    when_they_choose_to_nominate_a_delivery_date\n    then_they_should_be_shown_the_nominated_day_delivery_cost\n    and_they_should_be_redirected_to_the_payment_step\n  end\n\n  def given_a_customer_has_reached_the_delivery_step\n    create_order_for_logged_in_user(:delivery)\n    visit spree.checkout_state_path(:delivery)\n  end\n\n  def when_they_choose_standard_delivery\n    choose :standard_delivery_method\n    click_button(I18n.t('shop.checkout.steps.delivery.submit'))\n  end\n\n  def then_they_should_be_shown_the_standard_delivery_cost\n    expect(page).to have_content(standard_delivery_cost)\n  end\n\n  def and_they_should_be_redirected_to_the_payment_step\n    expect(current_url).to eq(spree.checkout_state_path(:payment))\n  end\n\n  # second scenario steps excluded for brevity\n\n  private\n  def create_order_for_logged_in_user(step)\n    order = OrderWalkthrough.up_to(step)\n    login_as order.user\n  end\n\n  def standard_delivery_cost\n    Rails.application.config[:shop][:delivery_method][:standard][:cost]\n  end\nend That is a toned down version of a delivery step feature. As you can see it's pretty much the same style as that seen in FutureLearn's blog post . Why write feature tests this way? After we decided to move to using RSpec for \"all the tests\" we already knew we wanted to structure them similarly to FutureLearn, it was one of the main reasons we swapped to RSpec. The original blog post gave it's reason for not reusing code between features as being primarily for readability. We also discourage reuse of code between features and limit reuse within features to helper methods to reduce the fragility of our tests. By decoupling dependencies between tests we reduce the risk of changes to one scenario or feature affecting others. The readability is a great reason too! Instead of reusing steps, we tend to lean on helper methods still defined within the confines of the feature scope but we differentiate these from steps by declaring them private. There is no functionality gained from making them private, it simply helps us visually separate steps and helpers. As you can see there are 2 scenarios in this feature. We try to limit the number of scenarios to keep test times down. This means we have to focus on critical paths through the app. In this example there are actually 4 different delivery types but since standard delivery works in basically the same way as next day and same day delivery we only tested one of those options. The nominate delivery day works differently enough to require a scenario. Choosing which scenarios to cover can often be a hard judgement to make and will definitely be the subject of a future blog post. We do not often test form validation at feature level, although we occasionally test failure cases. For example we wouldn't test what happens when a visitor forgets to enter their password when logging in, but we would test a card declined scenario in checkout. So that's it, now you know what our feature testing looks like! That said, it is bound to have changed by the time you read this post. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-29"},
{"website": "Made-Tech", "title": "The Move to GitHub", "author": [" David Winter"], "link": "https://www.madetech.com/blog/the-move-to-github/", "abstract": "At Made we've used GitHub for years for all of our public and open source projects. That has always been free, and the experience has always been great. We have been hosting our private repositories with another service for a really long time. However, the user experience, speed and feature set of GitHub that we get with our public repositories has finally led us to make the jump, and to migrate all of our repositories across to GitHub with a paid plan. Pre-migration Our git repositories are used hundreds, if not thousands of times a day by our team, working on various projects. Minimising the access downtime during migration was essential as we were doing this during office hours, so we had a few hurdles to tackle during the migration. Importing to GitHub Build pipelines Deployments Local development Importing to GitHub This turned out to be the simplest of all the steps we had to take. We split our repositories up between the three of us that were working on the migration, and for each repository we would make a bare clone to our local machine. This means pulling down solely the data from a git repository (normally what is contained within the .git directory), and not having a working copy of the latest changes in a repo. We then push the bare repo into GitHub. This was essential as we wanted to maintain our existing branches and tags. git clone --bare git@path/to/repo/on/old/service/project.git\ncd project.git\ngit push --mirror git@github.com/madetech/project.git As simple as that! The git push –mirror ensures that all branches and tags get pushed into GitHub. Build Pipelines To simplify our Jenkins configuration, we decided on creating a dedicated GitHub user for our Jenkins, with an OAuth token allocated to it. This OAuth token had the repos permission assigned which is all our build pipelines require. Using the Jenkins credentials plugin, adding a new global credential was as easy as setting the username to user the OAuth token , and the password field to use x-oauth-basic. For each of our Jenkins jobs, we set the Git repository URL to use the https version, and then allocated the OAuth credentials. Finally, we created a 'Deploys' Team underneath our Organisation on GitHub, and assigned our dedicated Jenkins user to it. Then for each of the repositories that we wanted Jenkins to access, we assigned the team as a collaborator per repository. Deployments Our projects either use Capistrano or cf-deploy for deployments. cf-deploy always transfers files to the app instance from the directory it's being run from, whereas Capistrano will ssh into the instance, and then clone down the code. With our Capistrano deploys, we simply had to change the repo_url value in our config files: set :repo_url, 'git@github.com:madetech/project.git' We then just added our SSH keys to the dedicated Jenkins user so that Capistrano would be able to pull the changes in on the remote app instance. Local Development Finally, as our developers needed to push up new code, they needed to update the remote git references to our old service provider and replace them with a new GitHub URL. The default remote name used in git is origin, so it was a case of overriding the existing value with a new GitHub repo URL. git remote set-url origin git@github.com:madetech/project.git From then on, any remote action would use the GitHub repo. The migration went a lot easier than we first expected. We'll miss our previous service provider, but we look forward to using GitHub from here on out, and are excited by new features yet to come. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-25"},
{"website": "Made-Tech", "title": "How we communicate our ideas, or “English should still be our first language”", "author": [" Fareed Dudhia"], "link": "https://www.madetech.com/blog/how-we-communicate-our-ideas-or-english-should-still-be-our-first-language/", "abstract": "Our industry is all about the communication of ideas. It's not just about being able to communicate our ideas to a machine – a big part of our job involves the ability to communicate ideas to both technical and non-technical people, as well as translating and solidifying ideas into usable products. Why is it, then, that so many pieces of technical writing (emails, specifications, design notes, clarifications, books, source code documentation, bug reports, technical manuals and even simple feature requests) end up either way too long and turgid, or so brief as to miss the point entirely? I'll start by saying that I'm an expert at neither writing nor programming. I'm not preaching here. This is just about the kind of technical writing I like, the kind I don't like, and why. Bear in mind that this is also not at all about incoming writing. This is not about what the client is writing to us. It's about what we write to the client. It's about what we write to each other. Effective communication can be the difference between a client relationship that feels like a real alliance versus one that feels like the Cold War. I've worked at several companies, experiencing both ends of that spectrum, and I believe that we as engineers have to try and do better than throw our hands up and say \"[dealing with the client/communicating features/managing expectations] is simply not my job\". It's about so much more than that. I'll be referring here to technical books specifically, as writing about all the kinds of communication we handle on a daily basis is somewhat outside the scope of this article. Also, a good programming book is meant to take a topic you don't understand and make you understand it in the shortest possible timeframe. In this way, technical books resemble most of the communication we do on a daily basis. What I Like Brevity Shakespeare once wrote that \" brevity is the soul of wit \", which translates roughly to \" don't waste my goddamn time \". The amount of dry, humourless, completely rigid and horribly long prose that gets cranked out by this industry is astounding. Some people seem to write the way their university computer science textbooks were written, forgetting how much they hated reading those things in the first place. Textbooks have loads of hoops to jump through to qualify as textbooks in the first place, and not all of these hoops result in a better experience for the reader. They usually result in a 1000+ page book written in 9pt font that's pretty much dead on arrival. I've seen emails like this. I've written emails like this. It's bad. \"Javascript, The Good Parts\" is a great example of a book served well by its brevity. Fun-factor Continuing the previous point, the thing about textbooks is that they don't have to get you to read them. You have to read them, because they're the textbook for your course. You didn't \"hear good things\" about them. A book like Scott Meyers' \"Effective C++\", however, isn't a textbook. It's a fun book. The examples are interesting, he speaks in a reasonably informal style, and neither of those things detract from the fact that the book is drum-tight, and packed completely full with real programming gems. Pieces of concise information that you feel like a better programmer for having read it, but at the same time really light, and fun. Simplicity Einstein once said that you understand a concept if you can explain it to your grandmother. And he was talking about explaining the entire universe, rather than just some CRUD app. A good piece of writing can take a difficult topic and explain it in simple terms, without leaving anything out. The perfect example of a book that does this well is Steven Skiena's \"The Algorithm Design Manual\" which not only teaches you about algorithms, but also teaches you to think about algorithms. It's also personally my favourite book on programming. What I Don’t Like Hand-waviness When I started learning Rails, a lot of the Rails-related articles I read would hand-wave huge amounts of information that would have actually been really helpful as a beginner. This was usually in the service of making the Rails ecosystem look a lot simpler than it actually is. One really difficult thing about technical writing is figuring out what information is actually relevant. If you're just writing a tutorial, the only relevant pieces of information are the list of commands to get, say, a fresh Ubuntu box running nginx, with perhaps a brief explanation of why we're running each command. If you're writing an introduction to a new concept or framework, however, the question of what to leave out becomes a whole lot dicier. Most of what I initially read on Rails hand-waved enough for me to feel totally lost when my first bundle install failed. I didn't know what bundler was. I didn't even know what gems were! Einstein also said \" make things as simple as possible, but not simpler \". A lot of what I initially read about about Rails attempted to make it too simple. I actually found learning Objective-C to program for the iPhone easier than learning Rails, mainly because of the things people left out of the Rails tutorials I was reading. There's a good diagram of everything that's actually going on when we're introducing someone new to Rails here . Complexity Ah, complexity. In code, we hate it. We try to tame it. But, for some reason, when we talk about said code, we feel no such urge. I love the idea that \" a problem well put is half-solved \". There's something great about that phrase (other than its resonance); it conveys a really important, complex idea in eight syllables. We pay a lot of attention to eliminating redundancy in our code, but I've seen many cases where, in written and verbal communication, the important points that we're making get lost and forgotten in a sea of redundancy and irrelevant technical talk. Bikeshedding is another great example of this. We have a great ability to take a seemingly simple problem and break it down into its complex, fundamental components. It's part of our job. The problem is that not all of those parts are worth talking about immediately. Learning to decide which ones are is something that takes time and conscious effort. Arrogance Engineers are an arrogant bunch. There's no denying it. The best ones are usually humble, open to criticism, and always willing to learn (Good ol' Linus Torvalds being both a great counterexample of this and illustration of the point I'm about to make), but many of us feel the temptation to assume that others have the same knowledge as us. We assume that our opinion is correct. All of our stuff is logically correct, so we believe that our opinions about that stuff are also logically correct. This isn't the case, and it leads to endless hype-cycles, puritanical beliefs in certain ways of doing things, and a general closed-mindedness that is hard to shake. This all stems from the way we think about communication, or more specifically, the way we don't think about how we're communicating. Conclusion We spend a lot of time communicating, and a lot less time thinking about how we communicate. We're all about improving our practices and workflow in regards to our engineering, but if you can't effectively communicate the problems you want to and should be solving, you end up building a perfect, polished product that just isn't what was required. English should still be our main language, and we should act like it. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-23"},
{"website": "Made-Tech", "title": "Challenges in Contracting Agile Software Projects", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/challenges-in-contracting-agile-software-projects/", "abstract": "As a business that exclusively delivers software using agile methodologies, one of the most common challenges we face is contracting projects. With a more traditional methodology such as waterfall, big contracts and statements of work are drawn up before any work starts, that seek to specify in usually fine-grained detail exactly what features, and when, will be delivered. There's generally a fixed cost attached to contract and any change comes under a strict change control process where additional costs will be added to the project. By contrast, expending significant effort up front in specifying every detail of every feature, and implementing a strict change control procedure to deal with (almost certainly inevitable!) change would undermine at least two of the key policies of the agile manifesto: Customer collaboration over contract negotiation Responding to change over following a plan Some of the most common challenges we meet include: How much will it cost? The most regular, and not unfounded concern, is that of the open ended budget. When working with customers, it's seldom the case  that they'll have access to an unlimited budget to deliver the project, and accordingly will be looking for some certainty that the budget they're able to secure will deliver them the business value that they desire. What happens if things take longer than expected? In a project of any complexity, and humans being particularly poor at estimation, it's likely some things will take longer than expected (and in our experience, that some will take less time than expected) – and what happens then? What’s the incentive for delivering things quickly? If a project is being delivered on a time and materials basis, a question can be raised as to whether the vendor has enough incentive to deliver the project to a high cadence. There are a number of things you may consider to give your customers more confidence when contracting an agile project: How much certainty can you bake in to your contract without undermining your ability to deliver the project using the desired methodology? For us, the line is drawn somewhere around identifying the business objectives that we will collaborate with the customer to meet. What can you do to demonstrate confidence in your delivery? If you've got a proven track record with this customer, contracting later projects should be easier. Can you offer some level of satisfaction guarantee, at least in earlier sprints (or whatever currency you're using to slice the project)? Do you have credible professional references who can attest to your delivery of projects using similar methodologies in the past? Insist on some level of co-location for all or part of the project, such that the customer Product Owner is heavily involved in day-to-day delivery, prioritisation and shaping of the project. At the very least involve your customer PO in your stand-ups (or similar project check-in). What can you do to agree on roadblock resolution? Software delivery by nature is complex and unexpected things will crop up. Can you instil more confidence by contractually agreeing a collaborative resolution process? Customers will almost certainly not want to reach the end of an increment and be told that's something's been dropped. Do you need long running contracts? Is it sufficient for you to lock the customer in for any more than the next sprint? How punitive do you need to be about ownership of IP? Ultimately, we see that there's a leap of faith required from any new customer when delivering an agile project with a new vendor. In keeping with the agile manifesto, we try to keep our contracting to a minimum – preferring collaboration (or even just getting on with the job of delivering software!) – over contract negotiation. Aside from trying to build trust ahead of engaging on a project, in recognition of the additional challenges on the customer side, we steer our contracting such that we're always demonstrating confidence in our own ability to collaborate with the customer to deliver the project. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-18"},
{"website": "Made-Tech", "title": "Brevity vs. Comprehensibility", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/brevity-vs-comprehensibility/", "abstract": "We love using SCSS here at Made, and the way in which we use it is constantly evolving. However, one particular and admittedly quite clever feature of SCSS has bugged me from day one, which is referencing a parent selector inside the current one: .foo {\n  .bar & {\n    // your styles\n  }\n} SCSS is a beautiful thing and, I think, makes CSS a lot more readable and nicer to look at, especially when working to a style guide, or when a linter is part of your build process. The example above, however, I find horrible to read and I mentally trip over it every time I see it or need to use it, despite being completely aware of what it represents. All that feature does is say: \"Whenever .foo appears inside .bar, apply these styles.\" I like to read up on SCSS mixins and functions other developers have shared with the community, and I found a great alternative to the default parent selector method , a mixin that describes what that method does: @mixin when-inside($context) {\n  #{$context} & {\n    @content; \n  }\n} This can then be called within your SCSS like so: .foo {\n  @include when-inside('.bar') {\n    // your styles \n  }\n} The biggest argument against doing this, which was pointed out to me not long after introducing this mixin to a project a few of us were working on, is that it’s quite a few more characters to write (an additional 22), plus we’ve had to create a mixin to accommodate it. The trade off is that the method is much more descriptive, only slightly more verbose, and doesn't require so much esoteric knowledge of SCSS to understand (although if there was a way I could drop the '@include', I would). An unwritten rule of development seems to be that the fewer characters you use to write a piece of code, the better, and to an extent that makes sense. If you were to look at a controller in a Rails application and find methods over a hundred lines long, you could spend hours picking through it in an effort to comprehend it, and even longer refactoring it into something that doesn’t make people balk when they see it, so we as developers strive to write code that is both concise and efficient. Sticking to that unwritten rule encourages cleaner code ( the DRY principle , shorter methods with descriptive names, less extraneous white space, single quotes instead of double quotes), and can give you a real desire to find the most ingenious way to achieve the desired outcome a given language will allow. Having said that, we, the humans reading and writing the code, still need to be able to easily understand what we’re looking at, and I think sometimes there’s a temptation to sacrifice comprehensibility for brevity, to abbreviate or otherwise encrypt code as much as the language will allow without considering how well it'll read to other developers. For me, being able to read code written in plain English is far more preferable than spending time trying to decipher arbitrarily obfuscated code. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-16"},
{"website": "Made-Tech", "title": "Made Named as UK Market Leaders in Ruby on Rails Development", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/made-named-as-uk-market-leaders-in-ruby-on-rails-development/", "abstract": "We’re very excited and proud to announce that we have been named as a leading provider in Clutch.co ’s list of Top Ruby on Rails Development Firms , and that we’re the only company in the UK to have been given that accolade! In a recent press release , Clutch highlighted us as one of three companies they've added to that list, after an extensive evaluation that assessed our experience, output, customer satisfaction and standing within the developer community. Clutch.co is a research firm, founded in 2012 and based in Washington, D.C., created to identify leading services and software providers across the globe, in order to help potential clients more easily find a provider par excellence that is suited to their business needs, and also to help those providers stand apart from less distinguished competitors. Part of Clutch's evaluation process is talking to a given company's previous clients and asking for feedback in key areas, so we're extremely honoured to have been given glowing reviews by a number of our clients regarding their experiences working with us. As a team that strives daily to deliver software that is both beautiful and expertly engineered, whilst also providing a great client experience, it's incredibly rewarding for us to have been recognised in all of those areas. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-12"},
{"website": "Made-Tech", "title": "Spree Custom Gateway", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/spree-custom-gateway/", "abstract": "Spree makes it easy to take payments from any Payment Service Provider, and in this post I will briefly walk you through the process of creating and using your own custom gateway. Custom gateways in Spree encapsulate the operations required to take payments online.    This functionality is abstracted out of the order logic, which makes it easy to think about in isolation.    Ultimately what we want is to substitute a small subset of functionality in Spree for our own.    The way Spree knows whether a transaction was successful or not is that we tell it exactly what happened.    We return a Response object (ActiveMerchant::Billing::Response), which holds a state of successful or failed.    If we return a successful response object, everything went well and the user has now completed the checkout.    If the transaction was not successful, the user will be redirected back to the payment step with an error message where they can try again. Foopay gateway #app/models/spree/gateway/foopay.rb\n\nclass Spree::Gateway::Foopay < Spree::Gateway\n  def provider_class\n    Spree::Gateway::Foopay\n  end\n  def payment_source_class\n    Spree::CreditCard\n  end\n\n  def method_type\n    'foopay'\n  end\n\n  def purchase(amount, transaction_details, options = {})\n    ActiveMerchant::Billing::Response.new(true, 'success', {}, {})\n  end\nend In this case we have stubbed out the response to always be successful. This happens because the first argument is set to true.  If it were set to false, it would always indicate to Spree that the payment has failed. Typically you would look for something like 'success' in your xml, json or post data after you have completed communication with the external  gateway.  Note that the payment_source_class is Spree::CreditCard.  We could specify a custom model here if we wanted to add extra attributes to it.  The 'options' argument passed in contains only a set list of predefined options including: :email, :customer, :customer_id, :ip, :order_id, :shipping, :tax, :subtotal A complete list of these options can be seen here . Register the gateway We can register Foopay by adding the following code to the spree initializer: #config/initializers/spree.rb\n\nRails.application.config.spree.payment_methods << Spree::Gateway::Foopay Persisting the gateway We need to persist our gateway, and also add it to our Spree store: Spree::Gateway::Foopay.create(      \n  name: 'Foopay',      \n  description: 'My fancy new Foopay!',      \n  active: true,      \n  environment: 'development')    \n\nSpree::Store.first.payment_methods << payment_method Frontend In order to get the data we need from the user, we need to render a form for them to fill in.    Looking back at our custom gateway, we specified a 'method_type' method.    This will be used to look up the partial to be rendered in the payment step.    In Spree, we can see this happening in frontend/app/views/spree/checkout/_payment.html.erb . render partial: \"spree/checkout/payment/#{method.method_type}\" Our gateway specified 'foopay' as the method_type, so it will try and render the the _foopay partial.    Let's create it at app/views/spree/checkout/payment/_foopay.html.erb This file is used to customise what is sent through to the checkout controller. Note: Naming of the form inputs is important.    Custom payment values may also need to be added to the permitted attributes list. Behind the scenes I decided to take a few notes on the interesting things that happen when the request goes into Spree. We can see that the payment form points to: /checkout/update/payment The request hits the .update method on the Spree::Checkout controller.  The first code it hits is: @order.update_from_params(params, permitted_checkout_attributes, request.headers.env) This line will update the order with the new data submitted from the form, any associated models will also be updated or created. If this completes successfully, the order tries to advance to the 'complete' state. @order.next The state machine hooks are triggered. before_transition to: :complete do |order|\n  ...\n  order.process_payments!\nend process_payments! is defined in core/app/models/spree/order/payments.rb Both the payments and checkout modules are mixed into the order object, so all the methods they provide are added directly to it. Next, it finds each of the unprocessed_payments on the order, and runs process! on them. .process! checks whether the gateway has auto_capture? enabled. If it does, the purchase method is called directly, otherwise authorize is called. Following the call into the Payment model, we find the following: after_initialize :build_source It is within the .build_source method that Foopay is instantiated. Time to shine The actual line of code that hands over responsibility from Spree to Foopay looks like this: response = payment_method.send(action, money.money.cents, source, gateway_options) action in this case will be 'purchase'. In Foopay, our purchase method accepts the following arguments: amount payment source options The second payment_source argument will be an instance of Spree::CreditCard. The third argument, 'options', is a pre-defined list of gateway options, this argument was discussed above. We use the data in these arguments to finish the payment. Our method returns the appropriate response object, and the transaction is complete. ActiveMerchant::Billing::Response.new(true, 'success', {}, {}) Resources: https://guides.spreecommerce.com/developer/payments.html https://github.com/shopify/active_merchant https://github.com/spree/spree/tree/2-4-stable http://api.rubyonrails.org/classes/ActionController/Parameters.html Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-09"},
{"website": "Made-Tech", "title": "Microservices: Pros & Cons of Using Microservices On A Project.", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/microservices-pros-and-cons/", "abstract": "A microservice is a small networked service, that has specific responsibilities and an externally accessible API that other services or applications can interact with. When designing software, you would typically use classes, modules and patterns like SRP to separate concerns and isolate functionality within your codebase. Microservices effectively perform this same role, but the isolation is introduced at the network level (rather than at the application level), providing a stricter interface for interacting with your application's business logic. We see a number of business benefits that microservices can offer, such as improving the autonomy of your development teams, supporting an increased frequency of change, and reducing the complexity of large-scale systems. However, we also see significant overhead in their use, and a large cost to adopt microservices within a business. Below we've included 8 points that you might want to consider before using microservices: Positives: Microservices enable you to select the most appropriate technology stack for your service. This is incredibly powerful, as you do not need to rely on a framework that tries to do *everything* for you, and can select the framework which is most appropriate to the services requirements. Microservices have much smaller codebases when compared to 'monolith' applications, so deployment and build times should be  much quicker. Microservices allow you to release smaller change sets. A one-line change to a hundred-thousand-line monolith application requires the entire application to be deployed. A one-line change to a microservice only requires the service to be deployed. Microservices are easier and more cost-effective to scale, as they allow you to focus on scaling just those services that need scaling and not your whole application. Negatives: Microservices significantly increase the development time of new features, as Cloud Infrastructure, Continuous Integration, Build Pipelines, Databases etc need to be configured and deployed to support your new microservice. Microservices can make application refactoring difficult, as interfaces and application boundaries are spread across your microservices. Techniques like API versioning can be introduced to mitigate this, though this will add complexity into your microservice and to your clients. Microservices introduce additional operational complexity, as you have many moving parts that need to be monitored closely. This overhead will increase exponentially as you add more microservices to your application. To conclude, I would recommend you only consider microservices if your system is so complex that managing it as a monolith would be impossible. For the vast majority of applications, microservices are unnecessary and will end-up adding more complexities into the system than they will remove. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-11"},
{"website": "Made-Tech", "title": "Scary New Code", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/scary-new-code/", "abstract": "I've been writing software on my own for years, but one of my biggest challenges since starting at Made has been learning how to work in a team and get up to speed with huge, alien codebases in a short amount of time. I can't assume I know and understand everything that's going on in an app any more, and working in a support role means I'm having to get used to lots of apps quickly. The following are some tips I've received and learned since I started, old tricks which have become more useful than ever and techniques I want to work on going forward. You don’t have to know it all to work with it I came to Made with only a few weeks knowledge of Ruby and Rails, but I've learned more from diving into real app code and the source code of our most used gems as I've had from documentation and guides. Although software can get complex and codebases can get huge, when you get down to it, code is simple. Methods take data and change it or send it somewhere else. If you know one programming language, you can suss out others without too much trouble. The same goes for entering a codebase you're unfamiliar with. No matter how disconcerting staring at 4000 files may be, when you find the relevant place, you're in your comfort zone. Take the time to read through related code when working on a feature or bug fix, the extra knowledge will boost your acclimation to the project. Use what already exists The underlying MVC principles of Rails are expressed elegantly through its directory structure and class conventions. In general, if your frameworks and libraries are strong enough for you to use them in production, they're going to be smart enough to have a good separation of concerns and readable structure. The same goes for your app specific code, and this simplifies adding new code – you should follow the conventions of the framework or existing code wherever possible and examine similar functionality already in the app. When debugging, once you have found where functionality is being expressed it's often easy enough to reason about how and why it works the way it does. Foreign code is still just code, use what's there and you'll learn the overarching structure as you go. Check the tests When investigating a feature or module you're not familiar with, checking a respective test can illuminate exactly what it is and what it depends on. Comparing a bug report showing an error in an existing feature to a related test can sometimes clearly show where and why a fault is occurring, and looking at factories of a unit test can show you what the requirements of a piece of code are. As a general rule, feature tests show you how something is expected to be used, and unit tests expose how something works underneath. Hunt for some relevant tests, compare the steps to related controllers or modules and follow the trail they take through the app. What data do they receive? What is their expected function or output? Know your dependencies All of the code in your app is yours, and if it breaks your app it's your responsibility. Often, errors in your app can be traced back to the way you're using your third party libraries. Reading through documentation of a feature on Github or RubyDoc is a clear starting point for investigating a class or method, but if a quick read through relevant documentation isn't enough, an even better approach is going to the source code. Narrow the project down to the right version and find the file which defines the code you need to understand. Assess what it does and what it needs, and compare how you're using it. It may show you what you're doing wrong, or a more efficient way to achieve something you're overcomplicating. Better still, keeping a local copy of your major dependencies lets you treat it as you should – parts of your own software. At Made we use Spree as the base of most of our e-Commerce projects, and by cloning the version or commit a project uses you get write access to its source. It stops being external to your own code. Obviously you should rarely use a fork outside of your development environment, but quickly swapping your gem reference over to a local copy is great for introspection. It's your code anyway; now, investigating how something works in the context of your app as a whole can be as simple as dropping a 'puts' in the right place. Plus, having it on your system lets you search it as easily as you do your own code. In an app which relies on two frameworks as large as Spree and Rails, a huge amount of the code that makes it to your server will be code you or your team didn't write. Being informed about what it is doing underneath will improve your understanding on how to use it, and regularly searching through it is key. Ask for help This isn't the last step, it should often be the first and is essential. I'm a bit of a control freak about my own projects, and it shows when I talk about them. If you ask me about any function, I can tell you what it's for, why it works the way it does and challenges I had writing it. And of course I know the pain points. As developers we spend hours working on even small features in our programs, and ultimately come to know everything about them. This makes your teammates the most valuable sources of information you have. If you work in small teams like we do at Made, asking anyone on the project will usually yield valuable advice. They may have seen certain errors before, or have a good idea where it might be coming from. Even if they're as lost as you, they know the code better and can give you a place to start. When it comes to adding new features, asking for similar code already in the project can teach you common company practices and important decisions which have been made in a project. Plus they can prevent you from making mistakes which have already been made. Don't worry about your ego, your collegues are there to help. Get as much info as you can before starting a new task, you'll accomplish it faster and get used to the app quicker. At the end of the day, trying to understand the inner mechanics of a new project is always hard. Whether you're onboarding at a new company or contributing to open source, it takes time to become familiar with code patterns, structure, even issues that haven't been fixed yet. You'll be a tourist for a while, look around and ask questions. The more you do, the sooner you can act like a local. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-04"},
{"website": "Made-Tech", "title": "Enhancing Customer Experience With Email Schemas", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/enhancing-your-customers-experience-with-email-schemas/", "abstract": "Delivering a great end to end customer experience doesn’t just stop when your customers complete their checkout process. Every communication you have with them thereafter matters. Anything that can be done to make their continued interactions with your brand better is something worth implementing. **Enrich Emails To Your Customers With Schemas** Just like the extensive list of Schema types that can be applied to a website's HTML markup (something we’ve looked at here ), there is an equally complete list of schema types for emails which, in a similar fashion to the schemas applied to your website, allow you to stand out, but directly in your customer's inbox rather than in search results. Standing out in a crowd is a great thing, and there is no place more crowded than a person's inbox. – Seb Ashton What is Possible with Email Schemas? There are three main categories of schema for use in emails. These are: Orders, Promotions Reservations These then break down further into more specific types and, in the case of Orders , allow you to markup shipping updates. However, it's the possible actions that will make your emails stand out in a customer's inbox. With an action it is possible for a customer to perform some basic tasks without leaving their inbox. Tasks like track a parcel, review a product, or viewing their order details. Integrating Schemas into Your Emails Integration into a HTML email is very similar to integration within a HTML webpage. You can either write the required attributes into the markup inline, or wrap all the required information in a block of JSON. The JSON approach is probably more preferable in the context of email, as some templates can be very complex, and integrating the micro data with your visual content will get very complex, very quickly. To ensure your new email microdata is correct, Google provide a markup tester . It's the same one you may have used for your website, although you'll only be able to use the direct HTML input. Once you are satisfied that your email markup is correct, you are only one step away from it appearing in your customer's inbox, which is registering your markup and actions with Google . Pros and Cons Microdata in email is currently only supported by Gmail, and not in either Yahoo! Mail or Outlook so, with that in mind, you might want to hold fire on investing too much time in implementing the full suite of available markup. However, owing to the fact that Schema.org is a joint venture between Google, Yahoo, and Microsoft, it's very likely that integration on those platforms isn’t too far away, and if you have a high number of Gmail users in your customer base or on mailing lists, schema markup in your promotional emails will increase your conversion rates and help you deliver a greater experiences for your customers. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-02"},
{"website": "Made-Tech", "title": "Busting Agile Myths: Story Points and Time", "author": [" Nick Wood"], "link": "https://www.madetech.com/blog/busting-agile-myths-story-points-and-time/", "abstract": "In my last post I mentioned a common controversy linked with story points. Common wisdom asserts that they shouldn't be used to represent time, but this is on over-simplification which inhibits good estimation. A key question I like to ask Scrum teams from time to time, is: \"What is a story point?\". The first time I ask this of a team I'll normally get one of the following two answers: Story points are a measure of complexity X points = 1 day of work The first answer is much too vague. For a new developer on the team, John, it tells him nothing he needs to know. Even if he knows what story points are, how can he possibly estimate a story from this definition? Does he guess for his first few and hope to calibrate after getting a few times? As a team we risk losing out on potentially valuable insight from him for these first few as we'll naturally discount them as inaccurate. Finally, how do you even measure 'complexity'? The second answer also has problems though; let's call our hotshot developer on the team Carol, and our intern is Tim. They are estimating a story and they have the following argument: Carol: I can do that in an hour, so it's 1 point. Tim: It'll take me at least a day, so it's 8 points minimum. There's no way to break the deadlock here, as they are both right. Carol may easily be able to finish something in an hour that'd take Tim a day or more. What's worse, we'll have the same problem with anything we try and estimate if these two are on a team! We're missing the same thing from both answers: a good benchmark. We're trying to measure things without a ruler. Let's modify and combine our definitions so that we have the following: Story points measure time, where we define [feature X] to be 5 points. With a benchmark story laid out, we're in a much better place. Carol reasons that as [feature X] would take her 3 hours, then this new task which will take an hour is 2 points. Tim thinks [feature X] is a good couple of days work though, so this new feature is half that, or about 2/3 points. Tim and Carol then discuss and agree to settle on 2 points. The deadlock is broken and any future disagreement should encourage useful discussion, on what a feature represents, rather than getting caught up on the differing rates people work. John also has a much easier time getting up to speed. If [feature X] is something well understood like to create a login page for our website, he can build a fairly good picture of what 5 points looks like to help him estimate right away. He can immediately and confidently contribute to estimations and related discussions. I quietly sidelined the concept of 'complexity' earlier, but it's worth revisiting. Complexity is a very tough thing to measure – something that is complex isn't necessarily going to take a long time to do (doing a cartwheel) and conversely, something simple might take a long time (sleeping for 8 hours). It's for this reason I don't like to explicitly measure things in terms of complexity alone. I do recommend that teams factor complexity into their estimates though – if something is unusually complex I'd expect estimates on it to be higher than normal. Complexity isn't the only such factor that can influence an estimate though, and as a Scrum Master it's not my job to police whether an estimate is 'right'. Instead I like to keep an eye on the sorts of questions the team are asking as my guide for whether they are taking everything into account. Useful questions the team should be asking are the ones like the following: How many other systems are impacted by this change? Have we done this before? How many different members of the team need to be involved in this? Do we have all the information we need to get started? How long ago did we last change this bit of the code? What is the likelihood that the customer will have feedback on this that requires us to change our implementation once it's in progress? Can we write an automated test for this? It all comes back to the key point from my last piece. The main purpose of estimating stories is to open a dialogue around features early, which fosters an environment of collaboration and equality. It is this cultural shift, that is at the heart of Agile, which so many teams miss out on focussing instead on getting 'the right number'. This culture of conversation is far more valuable than the estimate we assign a story. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-05-28"},
{"website": "Made-Tech", "title": "Building a Product in a Day", "author": [" David Winter"], "link": "https://www.madetech.com/blog/building-a-product-in-a-day/", "abstract": "A couple of weeks ago we had our first ever Made Hackathon day, where we built a product—based on an existing idea—from scratch, and had it launched by 5pm. It was great fun, and very rewarding for the whole team. The result of our efforts was SSL Catch – a service to notify you of expiring SSL certificates for your domains. We've had a couple of clients who have had their SSL certificates expire because reminders were being sent to unmonitored or old email addresses. And there's nothing worse than getting a scary looking browser warning when visitors go to your site. Even mega-sites such as Instagram sometimes forget to renew their certificates. SSL Catch lets anyone setup automated reminders for any domain leading up to an SSL expiry. How were we to pull this off in around eight hours? MVP We had a 10 minute MVP (Minimum Viable Product) discussion at the start of the day to discuss what the bare minimum features that we'd need to implement for it to be useful to users would be. What we needed was the pure basics, with a few stretch goals if we had the time. Also, it was essential that we the product be launched by the end of the day – so any features included in the product had to be feasible within the time period we had. The MVP for SSL Catch? A form where a user can enter their email address and a domain name to monitor, submit the form, and get notified at various intervals in the run up to expiry. That's all we needed for the product in order for it to be useful. Breaking it down Based on the MVP, we used post-it notes to determine a breakdown of tasks that would allow us to achieve this, including the stretch goals. These were broken down small enough so that one person would be able to work on each. Once the tasks were in place, stuck on a wall, we spent a small amount of time trying to figure out a priority order. This included some thought on what things would need to be in place from the very start, and anything that might potentially block the team later on in the day – it'd be best to tackle those things early on. Based on the priority order, we went around and handed out one post-it note to each developer to take away to work on. The tasks we committed to: Build a CI pipeline with automatic deploys direct to production Work on a library to check for SSL expiry dates Setup background jobs to handle the expiry checks Background tasks to send reminder emails Styling of the website, including a logo Email template design Signup functionallity Communication is key Communication is key when working at such a fast pace and in a team. Everyone really needs to be aware of what everyone else is working on so that everything fits together nicely, and so that a steady pace can be kept throughout the day. We had a standup meeting every couple of hours to monitor progress. Everyone in the team would bring their current post-it note and update the group on its progress. If someone was finished with a task, they'd then claim a new post-it note off of the wall to tackle next. If anyone was having issues, or if a task was taking longer than expected, we'd try and break it down further so that another person could help on that feature, but without stepping on people's toes. If the task couldn't be split up, we'd pair program to try and pick up the pace. Only if existing tasks were completed, or outstanding tasks couldn't be broken down/split up, would we then work on stretch goals. End of play By around 5pm, SSL Catch was ready to showcase internally. All of the pieces come together and we did a demo, and then let it loose on the internet with a tweet . We had five people working on the project. For most of the day, that involved 2 working on code, 2 on styling and 1 on infrastructure, though we mixed it up a bit when needed. We had 108 builds on our CI server, and 59 production deploys. Our repository has a total of 186 commits. Our build time is 46 seconds, with a deploy time of just over 3 minutes. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-05-26"},
{"website": "Made-Tech", "title": "SirTrevorJS: King of the WYSIWYGs", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/sirtrevorjs-king-of-the-wysiwygs/", "abstract": "A lot of our projects include some sort of manageable content, whether it be landing pages, blog postings or news articles, so it's important for us to be able to provide our clients with an interface that allows them to easily create content that looks great. Over the years, we've tried various WYSIWYG editors, including TinyMCE , Redactor and CKEditor , which are all fine in their own way, they each allow content to be edited as if you were working within a word processor, and we understand the appeal that has, but we've found that they can quickly lead to badly and inconsistently formatted content, especially in larger organisations with multiple content editors. Good Evening, Sir Trevor We discovered SirTrevorJS some time ago, and we were mightily impressed. Rather than treating content like a word processor, SirTrevor instead allows you to create a series of blocks, each of which can be set as one of a large list of different types. Out of the box, these block types include Heading, Text, Quote, Image, List and Video. After selecting your block type, you're presented with a very simple UI within which to fill out your content, and by highlighting sections of text you're also able to format that text in bold, italics or with links. The modular nature of these blocks means that we can concentrate on styling them individually, secure in the knowledge that they will look great, and consistent, in whatever configuration you can think of. One of SirTrevor's most useful features is the ability to create and define custom block types, something we've taken advantage of to create our own little block library that is now in use on many of our projects. These include Banners, Carousels and Images with Captions, all of which are often either impossible or difficult to create in other WYSIWYGs. In the world of e-commerce, we've also created blocks that allow you to easily search your database and set products related to the article you're writing. While a relatively young plugin (version 0.5.0 at the time of writing), and bearing in mind that there are some cases where it might be preferable to use a more traditional WYSIWYG, we would encourage anyone building applications with content management requirements to spend some time trying SirTrevor out. Links: Community-created additional block types A columns block type we've found incredibly useful , but make sure to check out the README before trying to implement it, as the pull request associated with it was unfortuantely rejected . Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-05-21"},
{"website": "Made-Tech", "title": "Spree vs Magento", "author": [" Fareed Dudhia"], "link": "https://www.madetech.com/blog/spree-vs-magento/", "abstract": "There has been so much written about how Spree stacks up against Magento over the last few years that it’d be easy dismiss another article out of hand. They’re great to contrast as they are both open-source ecommerce platforms; I’ve worked professionally with both, and I can identify with both sides of the argument. The Spree folks say that Spree can do everything Magento can and does it in 1/200th the amount of code (their cores are 45k vs 8.2m lines of code respectively). The Magento folks say that Spree is new and relatively untested, and as such is only really good for smaller businesses that won’t hurt too much if their site is somewhat lacking in the reliability department. Well, Spree’s come of age now, and perhaps it’s time to contrast Spree against Magento again, in 2015. Maturity & Reliability The main argument levelled against Spree has mostly been that it’s a newcomer; immature technologies are generally less reliable than their better-established competitors. Magento beats Spree in maturity, that’s for sure. We’ve been hearing this for quite a few years now, and you’ll still see arguments levelled against Spree on the basis that it’s an immature technology. These arguments, of course, neglect the fact that larger codebases tend to be much less reliable than smaller ones (Magento is several hundred times larger, codebase-wise) and the fact that Spree is built upon the rock-solid Ruby on Rails framework. Rails powers some of the largest and most robust websites on the internet, including Airbnb, Square, Github, Hulu, and many others. So, for how many more years is Spree going to be considered a newcomer? At some point, ‘it came first’ stops being a reason that one product can be considered better than another. Spree already powers a whole bunch of successful, high-revenue online storefronts: Dulux , Bonobos , Fortnum & Mason , Finery and On Running all spring to mind (in fact, Bonobos switched from Magento to Spree). Spree has been around going on 8 years now, and in the tech world, that’s actually a pretty long time. Spree has come of age. Speed of Development No-one’s arguing that Magento is quicker to develop features for than Spree; Ruby on Rails is renowned for being the most productive web framework out there and adding new features to a gargantuan codebase like Magento takes considerable time. Where Magento does score points however is concerning plugins. The Magento store has integrations for pretty much everything under the sun, and you might find that you can buy all the integrations you need without having to pay anyone to write any code. There’s also a ton of themes that work well out of the box. Magento also scores points here in that it’s written in PHP. PHP is a very popular language, and finding PHP programmers is considerably easier than finding Ruby programmers. Having said that, Magento sites often suffer from the codebase becoming large and unwieldy. Hiring more developers does not fix this problem as those developers then have to be trained on the project. If the codebase becomes difficult enough to understand, hiring new developers does not guarantee that work will be done any quicker. These projects may grind to a halt as new work requires understanding of the old work. Project managers on these projects become very well acquainted with terms like “bus factor” and “development hell”. Features Back when Spree was relatively new, it was missing all kinds of features that online shoppers have come to expect. Proper product searching, sales, wishlists, BOGOF promotions, and multiple currencies were either not fully polished or missing entirely. Spree has come a long way since then, and now really can do everything that Magento can. I’d be happy to settle for a draw in this department were it not for one thing that Magento stores can really struggle with: loading a page quickly. It’s well known that page load speed can severely affect conversion rates , and this is an area that Magento will always struggle with, especially given the size of the codebase. Pricing If we disregard Magento Enterprise (the pricing of which would make Donald Trump’s wig fly off), the pricing of Magento and how that compares with Spree’s pricing is really more about the company that you get to do the work. It’s well known that quotes in this business can vary wildly, and finding the right company for you is a quest which you must embark upon yourself, as a business ( although we might know of one company that could help ). This is assuming the quote is, of course, for getting an initial site up. If your business then decides to add a bunch of features and remove a bunch of different features, the manpower required is significantly less for Spree, and as such should be reflected in the cost of getting the work done. Conclusion Comparisons in 2012 had a fairer pro/con list between these two platforms. In 2015, Magento is very similar to how it was, whereas Spree is a completely different beast. With the release of Spree 3, the main reason I’ve found for a business to build a project with Magento is that their developers are already familiar with PHP. I’ll fully admit to bias here, as I worked professionally with Magento for several years and consciously made the decision to move away from it for precisely the reasons I’ve described in this article. Things move pretty fast in the technology world, and often the slow-moving maturity that Magento exhibits is but a few short steps from obsolescence. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-05-14"},
{"website": "Made-Tech", "title": "Made appointed to redevelop CDP online platforms", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/made-appointed-to-redevelop-cdp-online-platforms/", "abstract": "We're pleased to have been appointed to redevelop the online platforms for CDP. Formerly the Carbon Disclosure Project, CDP provides the leading global platform for organisations, including more than 5,000 of the world’s largest corporations and major cities, to disclose emissions data. We're working with CDP on a handful of initiatives: 1) With their in-house development team, we're leading the redevelopment of their online platforms, migrating from a legacy Sharepoint setup with a high cost of change and slow release cycles, to a more modern, Ruby on Rails based platform built on strong software engineering practices, including a test-first approach. 2) Introduction of Continuous Delivery practices and tooling to facilitate fast and reliable changes to production applications. Using tooling such as Chef , Jenkins and Capistrano to provide an automated and repeatable pipeline. 3) Delivering a program of agile and process transformation to enable the organisation to deliver an ongoing software development program boxed by structured sprint cycles, with internal focus on constant communication, delivery team empowerment, and continuous inspection and improvement. We're already a couple of sprints into the project. With a number of legacy systems requiring integration, which have the added complication that they're managed by offshore third parties, we've pulled the risk forward and have completed much of the integration work during the early sprints – on top of bootstrapping environments from development through to production, with continuous integration right at the heart of affairs. We'll be releasing the first public version of the new platform about a month from now, with the plan that over the course of the summer further features will be released, replacing the existing platform piece-by-piece. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-05-12"},
{"website": "Made-Tech", "title": "An Argument for Immutable Class Design", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/an-argument-for-immutable-class-design/", "abstract": "If you write code you write bugs. It is a fact of life as a developer. The more code you write, the more complicated it gets and the more likely it will contain bugs. Consider this logic: If you want to introduce fewer bugs, write less code. What does this have to do with immutable class design I hear you ask? Well, there are many arguments for immutable designs. There are arguments that immutable code is easier and faster to test . There are arguments that immutable code is easier to use in distributed and threaded systems. There are arguments that immutable deployments can keep us from building snowflakes and enforce a scalable mindset from the start. I am not presenting any of these arguments today, instead I am presenting a much simpler argument. My argument today is: Writing immutable code leads to a reduction in lines of code. In a recent code dojo here at the Made offices, one colleague suggested a mutable object design to which I offered an alternative, immutable design. Rather than derailing the dojo with an explanation I will now outline my thoughts. The dojo itself was charged with building an anagram grouper. Given a file of words we had to produce an array of anagram arrays. The first step was to provide words to the class without loading from a file and producing an array of grouped anagrams. The mutable class design would require each word to be added to an AnagramGrouper object with #add_word and then a #group method being called to produce the output. class AnagramGrouper\n  def initialize\n    @words = []\n  end\n\n  def add_word(word)\n    @words << word\n  end\n\n  def group\n    # logic to group anagrams\n  end\nend This design means an AnagramGrouper represents one session of grouping. That is, it maintains an array of words to be grouped, so given a different set of words you would need to create a new instance of the grouper. grouper = AnagramGrouper.new\ngrouper.add_word('dog')\ngrouper.add_word('god')\ngrouper.add_word('bob')\ngrouper.group # => [['dog', 'god'], ['bob']]\n\ngrouper = AnagramGrouper.new\ngrouper.add_word('debit card')\ngrouper.add_word('bad credit')\ngrouper.group # => [['debit card', 'bad credit']] As you see here this design has lead to verbose usage. We could of course load all words at construct time: class AnagramGrouper\n  def initialize(words)\n    @words = words\n  end\n\n  def group\n    # logic to group anagrams\n  end\nend Which then leads to a shortening of usage code: grouper = AnagramGrouper.new(['dog', 'god', 'bob'])\ngrouper.group # => [['dog', 'god'], ['bob']]\n\ngrouper = AnagramGrouper.new(['debit card', 'bad credit'])\ngrouper.group # => [['debit card', 'bad credit']] However we could go one step further, let's turn this design into an immutable one. class AnagramGrouper\n  def group(words)\n    # logic to group anagrams\n  end\nend Now the usage is super simple and only requires one instance of AnagramGrouper. group = AnagramGrouper.new\ngrouper.group(['dog', 'god', 'bob']) # => [['dog', 'god'], ['bob']]\ngrouper.group(['debit card', 'bad credit']) # => [['debit card', 'bad credit']] So not only did an immutable class design lead to a reduction of code in the class but also a simpler interface and therefore simpler usage. This is a very superficial argument for immutability, but I think it's a powerful one. As developers we all too often lean on complicated object design when a simpler design would have been easier and quicker to implement. The mutable design stored state in an instance variable @words, but why? What benefit does it provide to be able to build up an array of words? What benefit does it provide to require a new AnagramGrouper instance every time you want to group a separate set of words? Why store state when you don't have to? My final question I leave you with is, why write code that didn't need to be written? Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-05-19"},
{"website": "Made-Tech", "title": "Getting things ‘Done Done’", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/getting-things-done-done/", "abstract": "At Made Tech, we’re big into Agile methodologies , particularly Scrum . As part of these practices, we’re constantly analysing and evolving the way we work and trying to find ways to improve our software delivery practices. One of the areas we focus on is how to ensure user stories are ‘done done’. To us, ‘done done’ is about ensuring a feature is complete and ready for deployment to production. There shouldn’t be any further work required on the feature, except possibly the click of a button to deploy the code to the production infrastructure. So, why is getting to ‘done done’ important to us? The Scrum methodology is fundamentally about breaking large complex projects into smaller chunks of work, which are delivered incrementally. These increments need to be shippable, so they can be used by end-consumers and provide value back to our customers. It’s therefore very important that when somebody says a user story is ‘done’, it’s truly complete and ready for launch. If it’s not, then you’ve got a story which is still a work in progress and it’s going to require additional time to complete. It’s not done. Internally we’ve got a number of criteria that help us to decide whether a user story is ‘done done’, or not: Does it meet the customers requirements? Does it have a good level of code coverage? Is the test suite still green? Has it been cross-browser tested? Has it been performance optimised? Does it need to be? Has it been checked for security vulnerabilities? Does it adhere to our code style guides? Is the code considered clean and maintainable? Does the code need a peer review? Has it been deployed out to a production-like environment? Are we happy with this feature? When an engineer considers a story ‘done done’ and the scrum team agree that the user story meets these criteria, we’ll consider it complete and the engineer can start working on another task, confident that the previous task is closed out. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-05-08"},
{"website": "Made-Tech", "title": "Busting Agile Myths: Story Points", "author": [" Nick Wood"], "link": "https://www.madetech.com/blog/busting-agile-myths-story-points/", "abstract": "Of all the tools in the Agile developer's toolbox, the most often misunderstood is the Story Point. Some people will tell you a story point is a measure of time, others will emphatically argue that a Story Point measures complexity, difficulty or effort (anything except time!). I'll tackle that particular can of worms in a future post, first I want to outline what Story Points are, and why they are useful. Let's imagine a scenario where you had to quickly estimate the weight of the following animals: a cat, a rabbit, a dog, and a horse. Unless you have in your mind what 1kg of animal looks like, this is actually quite tricky. Go ahead and give it a shot – there are answers at the end of this post. Instead we'll try and solve this problem another way. Let's define a new unit of weight called 'Points', we don't really care how many kilograms are in a Point, but we arbitrarily choose a starting point,  we'll say that a cat weighs 3 Points. Also, on our new scales we only have the following numbers: 1, 2, 3, 5, 8, 13, 20, 40 & 100. So let's now take our second animal, a rabbit. Picturing the two animals side by side, I see that the rabbit is smaller than a cat, so I already know that the rabbit can only be one of two numbers – it's 1 or 2 points. My gut feeling is that a rabbit would be about 2 points, as 1 point feels more like something smaller, like a squirrel. I'm already able to build up a mental picture for the smaller end of my scale, so next time an animal comes along that's comparable to one of these, I can bucket them together. Instead of trying to precisely pin down the weight of something, I'm just trying to make sure similar things are grouped together, which simplifies the problem considerably. Now let's estimate a dog. In my mental picture of these two side by side, the dog is maybe 3 or 4 times bigger than the cat. So if a cat is a 3, that would mean a dog is about 13 points. However when I ask my friend Jim the same question, he thinks a dog is a 5. One of us must be wrong! Sort of, dog breeds can vary considerably in size though. At this point we're forced to have a discussion, it turns out his mother breeds miniature Poodles, so that's what he thinks of as a dog. I grew up with a couple of labradors though, so my dog is a lot bigger. Neither of us is wrong, but by surfacing questions like this early on, we avoid a lot of pain later. Let's assume we decide that a 'more typical' dog is somewhere in between, and we call our dog 8 points. I can now continue mentally filling in the blanks on my scale: 1 – Squirrel 2 – Rabbit 3 – Cat 5 – Fox/Small Dog 8 – Dog 13 – Big Dog/Pig We've only estimated 2 animals (the cat was defined as 3 remember), but already we have a very good picture across a spectrum of weights which allows us to estimate future animals very rapidly. Our last animal to estimate is a horse. Even a small horse like a pony is considerably larger than anything we currently have on our scale. Even being conservative, an average horse is at least 10 times heavier than what we have for as a 13. It's bigger than 100 so it doesn't fit on our scale. We'd say this is too big to estimate and if it were a user story we'd look to break it down into smaller more estimatable chunks. I will spare our hypothetical horse from such butchery however. Here at Made we're often faced with a long list of features, which we need to estimate in order to figure out how long project will take. Instead of animals we have User Stories, and instead of weight our points are a measure of how long we think something will take to build. The purpose of estimating is then clear: Instead of focussing on exactly how long something is going to take to build, we instead try and group similarly sized features – we will gain on some and lose on others, but overall the number of points in a project should be about right. The conversation around a feature, in particular working with a customer to figure out what they mean by 'Feature X' is ultimately more important than the number of points we assign it. We aim to identify features that are too big too estimate, so that they can be refined into smaller deliverable chunks. So we can ensure we are delivering features to our users at a regular pace. Answers Rabbit ( Cotton Tail ): 0.8 – 1.5kg Cat ( Domestic ): 3.5 – 4.5kg Small Dog: ( Jack Russell ): 6 – 8kg Medium Dog: ( Bulldog ): 18 – 22kg Large Dog: ( Great Dane ): 50 – 80kg Horse: ( Arabian ): 400 – 500kg Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-05-05"},
{"website": "Made-Tech", "title": "Data Structures at the heart of your system", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/data-structures-at-the-heart-of-your-system/", "abstract": "You should always prefer to create complex data structures over complex program logic. In fact, making complex data structures that are intelligent will eliminate the need for complex program logic. This will lead to a more robust system that's easier to reason about and has less code to maintain. Data structures are language agnostic, programmers are free to discuss them without having to be tied down to a specific programming language or framework. Revealing intent Just by looking at a data structure, you should be able to tell what its role in the system is. Exhibit A: [100.00, 'Foo'] What is its purpose? There is no way of telling. The person reading this can only assume. If it were passed in as a method argument, the method would be coupled to the fact that the values occur in different positions. This is not the right way to package this data, especially if it is meant to represent an important concept in the system. Exhibit B: {\n  price: 100.00,\n  name: 'Foo'\n} The data is enriched with keys that we can reference. You could say that this data structure has more value in the system. Exhibit C: Product = Struct.new(:price, :name) do\n  def discount_price\n    price * 0.5\n  end\nend\n\n... This data structure conveys a lot more meaning. Note that the 'price' in the discount_price method does not reach through other objects to do what it needs to, it operates directly on the data. This lifts the burden from other application code from having to deal with it. Show me your flowchart and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won't usually need your flowchart; it'll be obvious. – Fred Brooks, The Mythical Man-Month. Being close to the data Being far from the data manifests itself in what are commonly referred to as train wrecks. Violation of the Law of Demeter. Chained methods are an indication that your code is not close to the data. You never want to reach vast distances across your program to execute functionality. You are at the wrong level of abstraction. Order.customer.profile.display_name vs. user.display_name The Rule of Representation Based on this UNIX rule, you should fold knowledge into data so program logic can be stupid and robust. Data is more tractable than procedural logic. Starting your program without data structures, adding application code as you go will lead to a bloated design. Start with your data structures first, and you will not be forced to write unnecessary code to compensate for missing functionality. Fixing problems at the source Never add more code to your program to address a problem that could be fixed at the source (the data). This may seem obvious, but it happens. Make it as easy for the next consumer of the data to do what needs to do. Example: Suppose that you have a controller responding to an HTTP request and you want to instantiate a new object with the incoming parameters. For this example I will use an Order object. Let's say that We need to set its currency. We can get this data from a different current_currency method. The wrong approach would be to fix the problem only at the point where you encounter it: @order_params = params[:order]\n@order_params[:currency] = current_currency\norder = Order.new(@order_params) While this seems innocent enough, we have unnecessarily convoluted the code dealing with the data structure. And it could be worse. Think about the origin of this data, could it be improved to make our lives easier when we get to this point? This is not always possible, but a lot of the time it is. In this case, We can modify the params hash to come pre-packaged with the currency. By changing the data, we have reduced the complexity of the program code. order = Order.new(params[:order]) Unnecessary intermediate representations of data is a form accidental complexity and should be avoided. Accidental complexity tends to snowball, and so systems grow. The data structures can accommodate a lot of data but need to remain cohesive . We put the valid? method on a value object so that its validity does not need to be computed outside of the object. class Order\n  ...\n\n  def valid?\n    paid? and not expired?\n  end\nend\n\n...\n\ndo_stuff if order.valid? As opposed to: do_stuff if order.paid? and !order.expired? Managing the visibility of the data that an object exposes is known as encapsulation . Cyclomatic complexity State and cyclomatic complexity are two big sources of confusion in programming. Mix the two together and the cognitive overhead becomes unwieldy, at some point too much to deal with. Considering your data structures first will help reduce the cyclomatic complexity of your program. Example: Let's say we want to send emails to different email accounts based on the type of request that has been submitted. First thing that comes to mind is conditional logic. Right? if params[:contact][:type] == :enquiry\n  email = 'enquiries@foobar.com'\nelse\n  email = 'admin@foobar.com'\nend\n\nsend_email email This does what we want, but it is a good example of unnecessary logic that could be handled by a data structure. Let's build up a hash, which conveniently comes pre-packaged with a .default method, and push this logic down into the data structure. If the key is not found in the hash, it will use the catch-all default value. admin_emails = Hash.new('admin@foobar.com')\nadmin_emails[:enquiry] = 'enquiries@foobar.com'\nsend_email admin_emails[params[:contact][:type]] We have removed our own conditional logic and replaced it with a pattern matching mechanism provided for free by the data structure. Whenever you see conditional logic in your code, consider whether it could be solved with a more intelligent data structure. Correctly chosen data structures promote elegant minimal solutions. Conclusion Designing good data structures upfront will have many positive side effects, including a less convoluted codebase. It is important that you choose a programming language that facilitates the creation of effective data structures. Clojure and Haskell (to name but a couple) provide a vast array of useful data structures out of the box. These data structures are also immutable, which is another example of complexity being hosted by the data structure itself. Once you have constructed your foundations out of data structures, the application code to glue this together will be simple. You start thinking about the relationships between the data structures, not the procedural steps to get from A to B. Developers should choose to make data more complicated rather than the procedural logic of the program when faced with the choice, because it is easier for humans to understand complex data compared with complex logic. This rule aims to make programs more readable for any developer working on the project, which allows the program to be maintained. – Eric S. Raymond on the Rule of Representation in UNIX Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-04-30"},
{"website": "Made-Tech", "title": "All looking rather Dashing", "author": [" David Winter"], "link": "https://www.madetech.com/blog/all-looking-rather-dashing/", "abstract": "At Made we collect a lot of metrics. From projects, to servers, to support, to admin. All of these are stored in various services and it's not that easy to get quick and simple visibility of these. Metrics such as error rates and response times are critical. If there is a sudden spike in one, then the experiences of visitors to our clients websites can be severely affected, which could result in loss of sales. We have automated alerts when metrics hit particular thresholds so that someone is always able to respond. However, it's not just about putting out fires, but seeing things that could be better and optimised. Besides wanting a dashboard with lots of squares, numbers and pretty colours, we wanted to make these metrics visble to the entire team in the office with just a quick glance. This sparks communication between the different teams and promotes knowledge sharing. Metrics, metrics, metrics With a couple of hours dedicated to the dashboard project, and hundreds of metrics to pick from, we prioritised and picked four to focus on so that at the end of the day we'd have something usable. We selected: Error rates Deploy deltas Apdex scores Support hours Being able to integrate with various services where this information was stored was essential so that we could pull this data out. We compared a number of different hosted services based on how easily they integrated with these services, along with pricing and the look-and-feel of the dashboard itself (we wanted it to look nice after all!). While the leaders in the market were impressive in the number of integrations they offered and the ease at which to set them up, when it came to demoing our initial setup to the rest of the team, it was clear that metrics that deserved some attention weren't getting noticed as they didn't stand out amongst the others. Visibility This meant changing tact, and switching to Dashing , an open-source solution that gives us a lot more control. It's written in Ruby and CoffeeScript so we were able to jump in and customise right away. The widgets that come with Dashing out of the box don't support changing the background colours of the tiles based on a conditional. For example, if an Apdex score went below 0.9 then we wanted the tile to turn yellow, and below 0.8 to turn red. However, there are some ' hidden ' styles under the hood that do exactly what we wanted, which was to be able to switch to a red or yellow background when needed. We created some new widgets based on the simple Text widget, and by hooking into the onData method we were able to alter the styling as needed: onData: (data) ->\n  score = support_hours / @daysInMonthRemaining()\n\n  if score > 4\n    $(@node).addClass('status-warning')\n  else if score > 2\n    $(@node).addClass('status-danger')\n  else\n     $(@node).removeClass('status-warning status-danger') As we have multiple projects, we decided on using a dashboard per metric, with a tile per project displayed. We then used Sinatra Cyclist (Dashing is built with Sinatra ) to cycle through the different dashboards every 25 seconds. There's certainly a lot more we can do with our status dashboard. However, Thursday allowed us to build a good solid foundation. I'm sure we'll now add more to it. We'll need to try and get a good balance between helpful and important metrics while not trying to overcrowd it by providing too much data. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-09-15"},
{"website": "Made-Tech", "title": "Why companies are choosing Spree Commerce to power their online storefronts", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/why-companies-are-choosing-spree-commerce-to-power-their-online-storefronts/", "abstract": "As Spree Commerce gains popularity, more and more companies are enlisting it to power their eCommerce offering. At Made we recommend Spree because of how well it scales , its flexibility, and the benefits it offers over paid solutions and other open source platforms. **Spree can be tailored to any scenario** When choosing Spree you have access to a wealth of official and community written Spree extensions. Furthermore, while they may require some integration work during development, your application will be able to take advantage of any gem available in the wider Ruby ecosystem. All of this ensures your storefront can be as unique in its feature set as it is visually. **Who is using Spree Commerce?** Spree is powering the online storefronts for companies from a diverse range of retail sectors. In luxury food, it backs the storefront for Fortnum and Mason . In DIY, it ensures potential Dulux customers cannot only get inspiration for their new projects but can also purchase the paint necessary to do so. In fashion, it’s helping to sell clothing for Finery London and SHOWStudio . In home decor, Spree enables SurfaceView to sell bespoke wall covering from an extensive catalogue of prints, maps, and murals. And in sportswear, it’s helping to sell the unique On Running cloud running shoes. And that is just a small snapshot, there are countless more. **Do all the stores look the same?** As you can see from the sites above, with a myriad of possibilities in terms of the features your software engineers can add to your application. The look and feel of any Spree Commerce storefront can be as flexible and unique as your business. Since Spree Commerce version 3 both the user facing site and admin area have utilised the Bootstrap framework . Using Bootstrap alone opens the door to a lot of customisation options without even editing any HTML, just pick your colours and you’ll have an individual looking shop in next to no time. However, if you do want a completely bespoke look, there's a little more work involved. Fear not though, because Spree has thought of this too. Spree maintains a Ruby gem called Deface . Deface will easily override and customise any template in the Spree codebase – it will even work on a given extension. This allows you to really tailor every page to your needs, or the needs of your client, and when coupled with custom styling you’ll get a one of a kind storefront. **tl;dr** Spree is a great platform, and by using official and community extensions you can create highly customised storefronts that are as unique as your business requirements. The flexibility of Spree doesn’t end with the possible application features either. Making use of Deface the front end of the your storefront is completely configurable too. So, a Spree storefront can be built to suit any client whatever their needs may be. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-04-28"},
{"website": "Made-Tech", "title": "IaaS vs. PaaS: what we’ve learned", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/iaas-vs-paas-what-weve-learned/", "abstract": "We've been long-time proponents of PaaS, or Platform as a Service . We've also been long-time consumers of IaaS, or Infrastructure as a Service . At its most simplistic, you could describe IaaS as renting something that closely resembles a server for a period of time. PaaS could be described as altogether something more full-featured: a cohesive suite of tools on which to run and manage your (typically web) application. Much of our leaning over the past three or four years has been towards AWS for IaaS, and Cloud Foundry for PaaS, though we've variously trialled Rackspace Cloud , Heroku and Appfog . In this post I'll compare our learnings on a number of key areas pertinent to both development, operations and business folk: performance, reliability, scalability, application management and cost. Performance Day-to-day, you're unlikely to see significant difference between IaaS and PaaS for a typical application at runtime. That said, as PaaS is further away from the 'tin', in much the same way as comparing physical and virtualised hardware, you're going to see some loss. The noisy-neighbour problem , where other workloads on the same public PaaS provider can adversely affect your application's performance, manifests itself in much the same way as they do on public IaaS providers. On IaaS you have the option to use larger instance types, which often means you end up with dedicated access to an underlying physical machine, leaving you no neighbours to contend with. Round 1: IaaS. But not by much. Reliability This should be an area in which PaaS excels, certainly if you believe the marketing hype. If you're looking at reliability from an application management perspective, and by that, we mean the ability to detect unstable or misbehaving application instances and take corrective action, then PaaS does indeed provide some strong tooling around this. However, if we're looking at the platform as a whole, we find PaaS to be more unreliable than IaaS. Because of the increase in moving parts involved in a PaaS system, there are more components to fail, and subsequently, increased instances where intervention is required. Over the past month, for example, we've seen 3 separate (albeit short) outages on the Pivotal Web Services Cloud Foundry platform , whereas our AWS EC2 fleet has experienced no such issues. This would broadly mirror a typical month that we've seen over the past year. Round 2: IaaS. Scalability In this area PaaS offers some benefits over IaaS, though does fall short on autoscaling options. In a typical PaaS setup, increasing or decreasing the number of application workers is no more complex than issuing a command. This can be particularly useful for predictable high traffic events, or for manually reacting moderately quickly to unexpected bursts in traffic. IaaS offers little out of the box in terms of instance scale. Assuming your application servers are behind a load balancer, you can add more instances and then deploy the latest version of your application to these instances to achieve the same outcome – though you're probably looking at 15 minutes over the 15 seconds for PaaS. IaaS, and particularly the AWS platform, offers some strong tooling around autoscaling, though there's a heavy amount of customisation required to generate machine images for every release and to manually configure rules that trigger up- and down-scales. Round 3: PaaS, though we'd like to see better autoscaling. Application Lifecycle Management (ALM) This is the area in which IaaS really doesn't stand a chance; the significant selling point of any PaaS platform. If you go the IaaS route, you'll need to roll your own solution for managing applications, from provisioning infrastructure, deploying application changes and scaling application instances. Many quality tools exist in this space, but you're going to miss out on the turnkey access to them that PaaS provides. Most (all?) PaaS platforms provide a uniform interface to perform common application management tasks. A particular selling point is the ease at which you can replicate entire environments. In simpler setups, you can expect to launch a clone of your entire production estate in a couple of minutes. In addition, most PaaS ecosystems include a marketplace that can allow you to easily bind third party services such as cache services, proxies, mail relays and database services. We've found for small requirements, such as bolting a Redis queue backend on to an app – the ease, cost and removed maintenance overhead is significantly attractive in the PaaS world. Other core services, such as an application's primary database, we've found to be lacking in the PaaS marketplace and have fallen back on services such as Amazon RDS. While many PaaS systems provide tooling for aggregating logs, we still often have issue gaining the transparency needed to look in to some operational issues. Untold hours can be wasted trying to debug an application that is not staging correctly, leaving you craving the ability to just SSH in to a box and tail some log files. Round 4: PaaS. But we crave better transparency. Cost For the purpose of this post I'll focus on cost in terms of the cash you need to part with to make use of these services. This of course depends very much on the specific choices you make on provider. As customers of Cloud Foundry, we've found the cost to be broadly in-line with what we'd expect to pay to run a similar number of applications direct on AWS. What we often find is that the ease of spinning up applications, you often inadvertently end up using more resource than you might otherwise; certainly without good discipline to clean up after yourself. For a comparison between Pivotal Web Services and AWS, on which it's built, an average application which needs 512MB RAM and 5 worker processes would cost you, at time of writing, in the region of $55 USD per month. A comparable EC2 medium instance with 3.75GB RAM would cost you around $40 USD per month. Another factor to bear in mind is how closely IaaS costs are able to mirror your workload. By its very nature, there are some incremental jumps between instance sizes which may not exactly match your needs, meaning that you may be carrying excess capacity that you don't need. PaaS on the other hand allows you to provision and pay for precisely the resources that you need to run your application workload. There are many discussions about the costs of other platforms, particularly Heroku, where the costs can very quickly become uncompetitive with IaaS. Round 5: swings and roundabouts. IaaS or PaaS, then? By some cruel twist, we've demonstrated that IaaS is stronger in some areas and PaaS in others, which is really the reality of the situation. From our experience, we'd have to recommend that for truly mission critical workloads, PaaS is still too immature with too many moving parts to be considered a viable option. We expect that, over time, as PaaS offerings become more battled hardened, stability and operational transparency will increase. Where you're able to stomach a higher level of risk, PaaS can offer significant agility in your ability to launch and manage new applications. Even if you're under good configuration management within your IaaS fleet, PaaS is likely to deliver you a runtime environment in a fraction of the time. For organisations with esoteric procurement cycles and manual infrastructure configuration, the leap forward is almost immeasurable. For organisations who are adopting a microservice or SOA -type approach, PaaS provides a particularly complimentary offering, with the ease in which you can spin up runtime environments for new services. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-09-10"},
{"website": "Made-Tech", "title": "Pros and Cons of Ruby on Rails", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/pros-and-cons-of-ruby-on-rails/", "abstract": "Here at Made Tech we're big fans of Ruby and use Ruby on Rails for most of our web applications. Over the years we've had countless conversations about the pros and cons of Ruby. In this article, I want to take you through some of these reasons and explain why we think Ruby is a fantastic choice for a modern web application. It is incredibly difficult to succinctly articulate the differences between programming languages. There are literally thousands of different languages, which have evolved over years and are influenced by one anothers ancestors. You've only got to take a look at the fantastic evolution diagram below to see the incredible ecosystem of programming languages available to engineers. All programming languages have their quirks and an army of advocates who will refute any negativity about their language of choice. This can make it incredible difficult for non-programmers to understand the differences between programming languages and the impact a poor language choice can have on a project. In the past, we've seen this result in non-technical customers insisting on using a language they were familiar with (i.e. they had have heard of), rather than selecting the language which was best for the project. In a sense, the language which had the most 'marketing', got selected. As an aside, we see this happen frequently when people are selecting CMS systems like Drupal too. Programming Languages as Cars If you're a non-programmer, then a useful analogy is to think of programming languages as being a little bit like cars; Some cars are slow and clunky, yet incredibly reliable. Some are ugly and difficult to drive, yet parts are cheap and plentiful. Others are fast and exciting, but dangerous to drive and expensive to service. It wouldn't make sense to take part in a Formula 1 race whilst driving a Ford Fiesta; and it wouldn't be cost-effective to commute into the office in a McLaren F1. So, if you want your project to run smoothly, then you need to select the most appropriate programming language for the road ahead. All of our engineers are Polyglots (proficient at programming in multiple languages), so we feel like we've got a good grasp on the pros and cons of many different languages and find Ruby really shines as a general purpose programming language. It's worth noting that the software engineering changes rapidly and we expect that within a few years there will be an alternate language / framework, which is more suitable for our needs. (There are a number of candidates on the horizon at the moment, but only time will tell if they prove to be viable long-term options.) Here's our take on popular programming languages and their car equivalents: Benefits of Ruby on Rails? So why Ruby on Rails? We find Ruby provides us with a combination of the best tooling, better quality code libraries and a more pragmatic approach to software. Plus, the Ruby community tends to have a higher calibre of engineer, who favour responsible development over the gung-ho approach that can be seen in other communities. Tooling – Rails provides fantastic tooling that helps you to deliver more features in less time. It provides a standard structure for web apps, where all the common patterns are taken care of for you. Libraries – There's a gem (3rd party module) for just about anything you can think of. They are all publicly available and searchable through https://rubygems.org/ . Code Quality – Generally, we find the quality of third party Ruby code to be significantly higher than their PHP or NodeJS equivalents. Test Automation – The Ruby community is big in to testing and test automation. We believe this is incredibly valuable in helping to deliver good quality software and is one of the reason the Ruby libraries are so great. Large Community – Pretty much every major city in the world has a Ruby community that runs regular meetups. It's one of the most popular languages on social coding site Github. Popular in The Valley – History has shown that technology that's been popular within Silicon Valley has gradually been adopted across the world. If you look at the big startup successes of recent years, such as Airbnb, Etsy, GitHub & Shopify – they are are all on Ruby on Rails. Responsible Developers – You tend to find Ruby developers are more closely aligned around the the rules of responsible development. If you start small, communicate well, tackle vertical slices, write simple code over smart code, share ownership etc, you tend to find your project ends up in better shape. Productivity – Ruby is an eloquent and succinct language, which when combined with the plethora of 3rd party libraries, enables you to development features incredibly fast. I would say it's the most productive programming language around. Next Generation – Ruby on Rails seems to be the language of choice for a number of the popular online code schools, such as Makers Academy , Steer and CodeCademy . This should mean an increase in talented programmers joining the Ruby community over the coming years. Disadvantages of Ruby on Rails? Of course Rails does have have its disadvantages and it's only fair that we share those in this post. Runtime Speed – The most cited argument against Ruby on Rails is that it's \"slow\". We would agree, certainly when compared to the runtime speed of NodeJS or GoLang. Though in reality, the performance of a Ruby application is incredibly unlikely to be a bottleneck for a business. In 99% of cases, the bottleneck is going to be elsewhere, such as within the engineering team, IO, database or server architecture etc. When you get to a significant enough scale to have to worry about Rails runtime speed, then you're likely to have a incredibly successful application (think Twitter volume) and will have many scaling issues to deal with. Boot Speed – The main frustration we hear from developers working in Rails is the boot speed of the Rails framework. Depending on the number of gem dependencies and files, it can take a significant amount of time to start, which can hinder developer performance. In recent versions of Rails this has been somewhat combatted by the introduction of Spring , but we feel this could still be faster. Documentation – It can be hard to find good documentation. Particularly for the less popular gems and for libraries which make heavy use of mixins (which is most of Rails). You'll often end up finding the test suite acts as documentation and you'll rely on this to understand behaviour. This isn't itself a bad thing, as the test suite should be the most up-to-date representation of the system, however, it can still be frustrating having to dive into code, when sometimes written documentation would have been much quicker. Multithreading – Rails supports multithreading, though some of the IO libraries do not, as they keep hold of the GIL (Global Interpreter Lock). This means if you're not careful, requests will get queued up behind the active request and can introduce performance issues. In practice, this isn't too much of a problem as, if you use a library that relies on GLI, you can switch to multiprocess setup. The knock-on effect of this is your application ends up consuming more compute resources than necessary, which can increase your infrastructure costs. ActiveRecord – AR is used heavily within the Ruby on Rails world and is a hard dependency for many of the RubyGems. Although we think it's a great design pattern, the biggest drawback we see is that your domain becomes tightly coupled to your persistence mechanism. This is far from ideal and can lead to bad architecture decisions. There are many ways to work around this, some of which are included in this 7 Patterns to Refactor Fat ActiveRecord Models article. We would like to see Rails become less reliant on ActiveRecord. Conclusion There are use-cases for pretty much every programming language referenced and you'll find advocates of them all. If you are delivering a new application for the web, then we would advise you to develop it in Ruby, Python, NodeJS or GoLang. The other languages will still get you you from A-B, but at the end of the day, who wants to be seen driving a Fiat Multipla? Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-09-08"},
{"website": "Made-Tech", "title": "Boundaries in Object Oriented Design", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/boundaries-in-object-oriented-design/", "abstract": "As responsible programmers we like to write programs to the best of our ability. As our profession has evolved so too have our languages and the tools we use. We identify around concepts, patterns and principles. Today I want to explore 4 common programming concepts and highlight how they can help to draw boundaries within your application to make it easier to maintain. I will illustrate how they relate to each other and then draw conclusions on how they can work together to build better programs. The four concepts I am talking about are: Data Behaviour Inheritance Functional composition Data and Behaviour Object oriented languages, such as Ruby, focus on structuring programs as objects. These languages bring together data and behaviour or rather fields and methods into one unit, the object. A traditional way to structure the idea of a bank account in an OOP language would be to use a single objectBankAccount. Before going straight into this example, let's first think about a bank account. It has a balance which is a number that represents how much money is held within the account. You can also perform actions or behaviours on the bank account, such as withdrawing and depositing money from and to it. In an object, the balance would be data, and the actions of withdrawing and depositing would be methods. In Ruby, a bank account might look something like this: In this example we have the @balance field and three methods #initialize, #withdraw and #deposit. The balance is a number that can be increased or decreased by withdrawing or depositing an amount into the bank account. Let's use the example above to do some withdrawing and depositing. In this usage example we assign an instance of the BankAccount class to a variable, lukes_account, and then perform actions upon the account. You can see why object oriented languages are so attractive. The ability to think of real world objects as objects in your programming language is useful for having a mental model of your program. Languages like Smalltalk took this a lot further, providing an IDE for the language which literally draws the objects and their relationships with each other on the programmer's screen. So data and behaviour: both useful concepts traditionally encapsulated together in one object. Inheritance Inheritance is another common object oriented concept. By using hierarchy behvaiour can be shared and adapted. This hierarchy is declared by one class extending another. Coming back to the bank account example, you might use inheritance to convey the relationship between a child's bank account and a standard one. The child's account may have a lower limit on the amount of money that may be taken out at one time. First of all let's apply a limit to the standard account: And now let's extend our bank account to represent a child account with a lower limit: Here our ChildBankAccount extends BankAccount and overrides the #initialize method to change the account limit after calling the original implementation of #initialize with #super. Immediately you see the continued attractiveness of object oriented languages. You can represent classes of things and their hierarchy. Descendants share properties with their ancestors but can also diverge from their data and behaviours. Functional composition Functional composition is often a foreign concept to object oriented programmers, and rightly so, as it's a functional language concept. Function composition breaks away from the idea of keeping behaviour in one real world object. In fact, in functional languages, behaviour is contained within functions only. The composition of these functions then build up the behaviour of the program. Going back to our bank account example, instead of having the behaviour within the bank account object you would instead have other objects representing the actions that can be taken on a bank account. In our Ruby example we might reduce our bank account object to have just its initialize and balance accessor: The BankAccount becomes a container for data only. No longer is the bank account in charge of withdrawing and depositing. Its role now is to hold the balance and withdraw limit of the account. We should also create our child bank account whilst we're at it: Nothing new here but it's worth noting this ChildBankAccount is behaviourless and just a container for data likeBankAccount. So we have our data described, what about our behaviour? To represent the withdraw and deposit behaviour we need two new composable objects: Both Withdrawer and Depositor do not store any data and instead only act on data given to them as arguments. Lonesome, standalone behaviour. Your eyebrows may well be raising but let me run the examples course a little longer. In order to enact on the BankAccount object we would have to use the objects in concert: Now we are functionally equivalent to our pure inheritance example. The example admittedly involves more objects than before. To fully explore functional composition, we need to actually compose something. One real life example would be the act of transferring money from one account to another. The act of transferring money involves withdrawing from one account and depositing in another. Representing the transferral of money in Ruby with functional composition would probably look something like this: We have now composed two functions into a third function. That third function is now more complex than the other two, but looks simple since it delegates out to well named pieces of functionality. Now we can use the Transferrer object to move money from my account to that of a friend: The Transferrer object sums up the power of functional composition. It isn't a crescendo of a summary, but you have to take a step back a think about it. We have composed two units of work into a third larger concept. This kind of composition gives us the ability to write small units of work and compose them together into bigger ideas. We can continue to stack concepts on top of one another to create an entire application. Unifying the two worlds Unless you're a functional flirter or fanatic you may not yet see the advantages of using composable functions over wrapping larger concepts in objects. If you're a Rails programmer you will have seen some rather large objects in your time. You'll know that classes become harder and harder to reason about the more lines they have. Ask yourself for a moment: Why are objects with many methods harder to reason about? – The answer to this question lies in the boundaries or lack thereof in large objects. Remember, objects are encapsulations of ideas. The greater that idea gets the harder it will eventually be to grasp. The more data an object is responsibile for, the more behaviour it has, the harder it is to fit into your brain. Functional composition is a remedy for when things get too tough. I wouldn't argue you should build your Rails apps with everything split out into tiny units of work, I would argue that when things get tough functional composition can make things easier for you. The more data an object has, the more moving parts, the harder it is to get under test. If it's hard to get under test, it's probably hard for your head to think about too. It's a sign that you've pushed your object too far. Let's go back to our example one more time. I want to implement the transfer functionality without composition: As our object gets larger and, let's be honest, this object isn't that large yet, it will have an increasing amount of responsibilities. The more responsibilities the more edge cases that object may have. The more complicated your tests will be to setup and tear down. Let's use our remedy to keep the same interface above, but move our logic out into composable units that can be individually tested away from the BankAccount: We have fused our two ideas together. Now we have one object, a BankAccount that still encapsulates all the behaviours associated with a bank account. However we've drawn some boundaries. We now have other objects responsible for carrying out the behaviour. These composable units only accept data via their method parameters, no other complexity can travel through into them. Lines are important for our mental well being as programmers. Knowing when and where to draw a line can help you reason about your program that much easier. When you draw a line with an interface you now have two separate concerns that are individually testable. Not only that but your mental model only has to concern itself up to the line. This allows you to mentally step through your program a lot better than if everything were in one file. Without these lines both you and your program are essentially evaluating in a global scope. We all know how bad globals are, don't we? Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-09-03"},
{"website": "Made-Tech", "title": "Four ways to break your code for the greater good", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/mutation-testing-fuzz-testing-fault-injection-bebugging/", "abstract": "Software development is all about managing complexity. We employ tools like automated testing to guard existing functionality from regression. This however raises the question of Quis custodiet ipsos custodes? or \"Who watches the watchmen?\". Below are some techniques that could point out edge cases in your tests that may have gone undetected. Mutation testing Mutation testing is a tool designed to evaluate the quality of your test suite. By modifying your program in small ways like changing an || to an &&, it would expect at least one test to fail. If a test fails, you have successfully killed a 'mutant'. Mutants that survive can be analysed and new tests can be written for them if necessary. There is one caveat to mutation testing which is the dreaded \"equivalent mutant\". It is possible for the mutant to modify your program in such a way that the behaviour is still exactly the same. Your tests would never fail and the mutant will always survive. This problem has been subject to a lot of research and experimentation. Another popular form of program mutation is Statement deletion. This too may point out interesting areas where you may want to increase test comprehensiveness. You can test your Rails application quite easily, right now with the awesome Mutant Gem . Here is a list of mutants that you can use to mutate your code. Fuzz testing The goal of Fuzz testing or Fuzzing is to get your program to fail by bombarding it with random data. This form of testing has been around since the 80s, and was even used by Apple to test MacPaint back in those days. Division by 0 errors, out of range values etc. may all be highlighted when this is run. This form of black box testing can reveal defects in your software in an automated way so that you do not have to try do this manually. This is quite popular for testing security systems to discover Trust Boundaries . Ideally you want your program to respond to exceptions in an appropriate way and not just proceed in an unknown state. Fault injection This form of testing aims to follow error code paths through your application that might otherwise rarely be triggered. It has been around since the 70s and is also popular in the hardware testing world. In hardware fault injection they would do things like short out connections on circuit boards, or even bombard specific parts of the circuit with heavy radiation to see how the overall system copes. Two primary techniques in the software world for testing this are compile-time injection and runtime injection. Faults are introduced into the program and these may propagate up the stack to cause further errors within the system boundary. Faults making it up to the observable system boundary are marked as failures. Bebugging This is another way to measure test comprehensiveness, and has also been around since the 70s. It is a tool to get an idea of the average amount of bugs that potentially make it through to production undetected. Known errors are seeded into the program, and it is up to the automated test suite or QA team to find them. Rate of detection and remaining undetected bugs are used to measure the amount of bugs making it through to production. You will need sufficient amounts of historical data to get an accurate indication of this. Conclusion By using these tools, you may gain insights into the edge cases that exist within your program which would otherwise be very hard to catch manually. I do not consider these tools vital for producing good software but they are definitely helpful and, let's face it, we need all the help we can get. The results may be thought provoking and lead you to think about your program in different ways. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-08-19"},
{"website": "Made-Tech", "title": "Stream Everything", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/stream-everything/", "abstract": "This is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. – Doug McIlroy, inventor of Unix pipes 'Streams' and 'pipes' have been an essential part of *nix systems since the 1970s, when Doug McIlroy introduced them into the first version of Unix. The story goes that he threatened to leave the project if they weren't implemented, he felt so strongly that they were a cornerstone of an effective modular operating system, and key to interoperability. They facilitate the Unix philosophies of 'do one thing and one thing well' and 'write programs that speak to other programs.' A small Unix program like 'cat', which concatenates the contents of one or more files and prints out the buffer, is able to communicate with other programs through piping. You could 'cat' a file containing a list of words, pipe that to a sorting program, and then finally to a text file. You could write this complex operation more simply than you could explain it. cat names.txt | sort  > output.txt Despite its long time popularity at the Operating System level, I'm sure you've written app code like the following. Everyone has. import { readFile } from 'fs';\n\nreadFile('./document.txt', function(err, buffer) {\n  let lines = buffer.toString().split(/n/);\n  lines.forEach(function(line) {\n    if(line.includes('test')) {\n      console.log(line);\n    }\n  });\n}); The issue with this is you have to load the entire document.txt file into memory, split it on every line, and print each out one by one. This introduces a huge overhead on a simple operation, and cannot scale. A 500MB GeoJSON file would ruin your app. Not only would it take up a huge amount of memory, but it would take 10 minutes to load. Writing inefficient code like this on the shell actually takes more work, because the technique of piping streams around is so deeply embedded. We would write it like this: cat document.txt | grep 'test' Simple and readable. Here, input will start flowing through each program, one by one, immediately. Each program receives a chunk of data, can choose to modify it and pass it along, or ignore it completely, then waits for the next chunk of data. The ideal way to write app code in Node.js can mirror this. If you substitute cat and grep with the fs module in Node's standard library and a custom stream you have essentially the same code. You could write a simple function which returns a 'grep' stream using a module like through . function grep(word) {\n  return through(function(line) {\n    // This function receives every chunk of data.\n    if(line.includes(word)) {\n      // Pass it to the next stream if it matches the word.\n      this.write(line);\n    }\n  });\n} And then put it to work. fs.createReadStream('./document.txt')\n  .pipe(grep('test'))\n  .pipe(process.stdout); Why would it need to change? Text streams were designed to be the \"universal interface\" by which all programs interacted with each other. A large amount of Node.js's standard library revolves around the use of streams. They have incredible composability. You can make an HTTP request and pipe its response directly to a text file, or to a remote party. For example, the most basic proxy server could look like this: app.get('/:url', function(req, response) {\n  // httprequest is returning a readable stream, which\n  // is piped directly to a writable stream ('response'.)\n  httprequest(req.params.url).pipe(response);\n}); This technique obviously isn't limited to the shell and Node.js – many languages support Streams to varying degrees. The reason Node's implementation is particularly successful is because it has been integrated with most of its standard library. This means the canonical best way to do most basic operations involves using a stream to do so. For example, reading and writing files, making HTTP requests or sending data to a client. DOMNode One of the more interesting applications of Streams is using them in the browser. DOMNode is a library which brings Node concepts and functionality to the browser. It uses browserify to wrap up Node standard modules where possible, and polyfills where not. For example, it allows you to use the events library straight from the Node source, or a browser specific fs library. Better still, it provides Node style replacements or extensions to browser specific operations. For example, it allows you to use Streams when making XHR requests or using websocket connections. import http from 'http-browserify';\n\nimport websocket from 'websocket-stream';\n\n// The following are both streams\nlet httpStream = http.request({ method: 'POST', host: '/' });\nlet wsStream = websocket('ws://localhost'); A really interesting interface included in DOMnode is its streaming geolocation API. import geolocation from 'geolocation-stream';\n\n// Create a movement stream\nlet movement = geolocation();\n\n// Every time data (movement) is made, log it.\nmovement.on('data', function(location) {\n  console.log(location);\n}); Again, this is more than just an event emitter. This movement stream could be piped to any other stream. For a slightly unnerving example, you could pipe all movements to a server, and write those to a JSON file. In the browser: // Every time movement is made, pipe its data to the websocket.\nmovement.pipe(wsStream); On the server: // Using a different websocket implementation, receive data through\n// a websocket as a stream. Pipe its data to a text file.\nwebsocket.createServer({ server: app }, function(stream) {\n  let file = fs.createWriteStream('geo-data.json');\n  stream.pipe(json()).pipe(file);\n}); Streams are essential technology. Not only do they have incredible composability, interoperability and reability, but they have real technical merit. They don't make you wait. Data is piped from stream to stream as it comes, not after it has buffered in each. This is critical for large amounts of data, but effective even for small amounts of data. We spend a lot of time optimising code for speed and efficiency, but ignore the channels by which our primary concern – data – is passed around our applications. In Ruby, for example, slow operations like file reads or Net requests are too frequently handled synchronously, blocking the rest of the app. Although Ruby does have streaming operations, they should be simpler to use and be included in more of the standard library. They are fundamental in Node because they are key to the OS running underneath. They should be that way in all languages. Further reading Stream handbook Harnessing The Awesome Power Of Streams (LXJS 2012) Basics of the UNIX Philosophy Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-08-13"},
{"website": "Made-Tech", "title": "Picking the right Product Owner", "author": [" Nick Wood"], "link": "https://www.madetech.com/blog/picking-the-right-product-owner/", "abstract": "For a lot of our customers working with Made is their first exposure to Agile. As part of any project, we'll ask them to nominate a Product Owner to work closely with us throughout. Picking the right person for the job can make or break the project – so how do you make sure you choose correctly? Why can’t someone at Made be a Product Owner for us? We believe the best software is built when the business side of things is working closely with the engineering team. Nominating a primary stakeholder is the best way of ensuring a good line of communication and therefore securing the best return on your investment. No-one at Made can understand your business as well as a member of your team. For us to try and gain such an understanding would add a lot of time and cost to a project, and would likely lead to inferior results. What makes a perfect Product Owner? Ideally you should appoint someone who has been with the company a little while and understands well how the business functions – but also knows your organisational dysfunctions. Once development starts we'll look to progress at a very rapid pace, so having someone on the inside able to get quick answers to questions is key to avoiding expensive project delays. Someone who sticks to their guns, especially to other stakeholders; at the start of a sprint we spend a lot of time breaking stories down into technical tasks in order to deliver them in the most efficient way, as a shippable increment at the end of the sprint. Changes to a sprint halfway through are often unnecessary and whilst we'll do our best to try and accommodate them, a Product Owner needs to understand that this is going to potentially push promised user stories out of the sprint. Being a Product Owner is pretty time consuming, and we generally advise that they should be available to us at least 3-4 hours a day, if not full time. We're probably not going to need all of that time, but having someone available to answer questions and help resolve issues as they arise will greatly accelerate the rate at which we can work. A willingness to compromise is important; there's no such thing as perfect software, but it's easy to get hung up on small details trying to get everything 'just right'. There's an issue of diminishing returns here though. We will happily spend many hours moving a pixel here or there to make sure your responsive site looks great at every screen resolution if you want us to, but this is going to push other user stories down the backlog and potentially out of reach of your budget. Understanding where we can add the most value, right now, is the hardest job for a Product Owner. We don't expect someone who lives and breathes the Agile manifesto, many of us have worked on Agile teams for many years, so we don't need you to be able to pick it up overnight. We strongly believe that Scrum gives the best results though, so for best results you should pick someone with a willingness to embrace something new, and work within the process rather than trying to subvert it. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-08-11"},
{"website": "Made-Tech", "title": "Our Favourite Retrospective Ideas", "author": [" Fareed Dudhia"], "link": "https://www.madetech.com/blog/our-favourite-retrospective-ideas/", "abstract": "Ah, the sprint retrospective. When I first joined Made and found out about retrospectives (I'd never had one before), I couldn't conceive of the idea that there would be any value in such a thing! So you're telling me, I said, that we get together and do group exercises? Not about the work that we're about to do, but work we've already done? I assumed that it must be a whine-fest about tasks we found tricky, or rationalising out loud about why something took so long to do. I was wrong. In one of my earliest retrospectives, we drew a quick graph of how happy we were over the course of the sprint. This turns out to have similarities and perhaps even roots in/with provably helpful mindfulness exercises that are outside the scope of this article, but are worth looking up all the same . The graph of how happy I was with the sprint would be similar to the graph of how productive I was during the sprint, if we were to plot that. We then went over why we were happy or unhappy at different times. Agile is all about continuously improving processes, and if there's no retrospective, there's no way to identify patterns and pain points that could be smoothed over for subsequent sprints. The happiness graph is a great way to get to the bottom of things that might need fixing or changing in your existing processes. In another retrospective, we participated in an exercise that was pretty cheesy at the time, but its benefits far outweighed its cheesiness. At this sprint retrospective there was the product owner from the client's company, and two developers including me. Our scrum master told us to come up with our favourite work-related thing to say about the person next to us going clockwise, and then again going counter-clockwise. I didn't understand why we were doing it at the time, only that it was nice to be complimented by my teammate and client. Later I realised that quick 'bonding sessions' like this had really started to help with communication, not only amongst the team but with the client as well. It's hard to be standoffish with someone you've shared such a cheesy experience with. Another retrospective idea I liked a lot was a very simple one. We were told to highlight something we thought we could improve upon based on performance in the last sprint and turn it into a poster. We deliberately were not allowed to use computers or Photoshop. Nowadays, adults seem to be so starved for the ability to express anything with pencil and paper that there's been a huge upswing in publishers publishing colouring books for adults. Drawing and colouring, aside from being therapeutic, really makes you focus on the idea that you're trying to express. I had been storing up commits a bit that sprint, so my poster said 'PUSH EARLY AND OFTEN', with a big arrow. Taking 15 minutes to write something really makes you think about it. This, again, is linked with the idea of mindfulness. So there are a few retrospectives that I liked. There were plenty of others, but those were three that really stood out in my mind. They all serve a serious purpose, as well as being, believe it or not, pretty good fun. Try them out! Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-08-06"},
{"website": "Made-Tech", "title": "Focus with well structured RSpec tests", "author": [" David Winter"], "link": "https://www.madetech.com/blog/focus-with-well-structured-rspec-tests/", "abstract": "Before joining Made, my experience with unit testing was always with PHPUnit. It's very flexible in allowing you to write tests quickly; create a class, add some methods that start with test , include some assertions, and away you go. What I don't think PHPUnit—and similar—allow you to do well is think about how to structure your tests, and what to focus them on. For that you have to rely on experience and good discipline. Take the following example test in RSpec, written in 'retro' style: describe Warehouse do\n  context 'when turning warehouse power off' do\n    it 'should turn all machines off' do\n      warehouse = Warehouse.new(power: true)\n      machine = Machine.new(name: 'Vending machine', power_switch: true)\n\n      warehouse.machines << machine\n\n      expect(machine.power).to be_true\n\n      warehouse.power_off\n\n      expect(warehouse.power).to be_false\n      expect(machine.power).to be_false\n    end\n  end\nend The above example is quite a common way of writing tests regardless of what language or test framework you're using. But there is a lot happening in that one test: Description of what to test Creation of objects that we'll be using in our tests Setup the state of the objects Assertions It's a bit mashed together. What are we actually testing? What if we want to reuse objects between tests? We actually have two subjects that we're testing here. The Warehouse and the Machine objects within it. At this level of testing, these should really be broken out into their own test specs: describe Warehouse::Building do\n  let(:warehouse) { create(:warehouse_building, power: true) }\n\n  subject { warehouse }\n\n  context 'when switching off power' do\n    before(:each) do\n      warehouse.power_off\n    end\n\n    its(:power) { is_expected.to be_false }\n  end\n\n  context 'when switching power on' do\n    before(:each) do\n      warehouse.power_on\n    end\n\n    its(:power) { is_expected.to be_true }\n  end\nend And the corresponding Machine test: describe Warehouse::Machine do\n  let(:machine) { create(:warehouse_machine) }\n\n  subject { machine }\n\n  context 'when switched on, and turning warehouse power off' do\n    before(:each) do\n      machine.warehouse.power_off\n    end\n\n    its(:power) { is_expected.to be_false }\n  end\n\n  context 'when switched off, and turning warehouse power on' do\n    before(:each) do\n      machine.power_switch = false\n      machine.warehouse.power_on\n    end\n\n    its(:power) { is_expected.to be_false }\n  end\nend Don't read too much into what's going on with the code example, the thing to take away is the structure of the test, and its readability. Breaking down the structure The test structure mirrors the Given-When-Then pattern, derived from Behaviour Driven Development. Given: Setup your tests using let and before blocks When: Define your test subject Then: Use it and/or its to make your assertions Setting the scene Using let along with Factory Girl to setup our test subjects allows us to have sensible defaults in a separate factory file so that our tests remain cleaner. Having these stored in a let block allows for reuse between tests, where the value is cached after its first use, but not between our test assertions. That means that we can use our let values when setting up our tests. And we do that using before blocks. Why use a before block rather than just doing setup in a subject block? Simply because it reads better. Rather than: subject do\n  setup_this\n  and_that\n  test_this\nend Do this: before(:each) do\n  setup_this\n  and_that\nend\n\nsubject { test_this } Great Expectations Once the tests are all setup and ready, it's time for those assertions. Depending on your test subject, you might choose to use it or its . When testing objects, it's always good to use its as you can easily test multiple properties and methods by simply passing the name as the first argument. its(:power) { is_expected.to be_false } The above is basically doing this behind the scenes: expect(subject.power).to be_false If you have multiple assertions to make, then you'd have a lot of lines repeated that look similar, so its is a much clearer way of defining them. For other data types, it might be a better fit. An array for example: it { is_expected.to include('Apple') } Try and keep the assertions as simple and as minimal as possible. If you find yourself writing a lot of assertions in a single context, perhaps try breaking them out into a separate context. With the various RSpec helpers on offer, you're able to be more disciplined in writing well structured tests, that read better, while remaining focused on small pieces of functionality. Consider the readability at all times, as your tests often turn out to be great documentation for others. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-07-30"},
{"website": "Made-Tech", "title": "Rules for Stylesheet Modularity", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/rules-for-stylesheet-modularity/", "abstract": "Keeping applications organised takes a lot of work. Furious bursts of development where deadlines are tight can lead to poorer design decisions. The frontend in particular for me is harder to get right when the pressure is on. I'm writing this post in order to clarify my hard and fast rules for writing modular stylesheets in a rush. My flavour of stylesheet modularity is in many ways a BEM implementation though I'd certainly say I am no expert in BEM. Also it's worth noting these rules are applicable to all stylesheets whether CSS, SCSS, LESS, or whatever. Without further ado, here are my rules: One BEM module per file Never reference one BEM Block from another Never reference one BEM Element from another Move repeating Element classes into their own Blocks Keep your modules less than 100 lines long Never name more than one class on an element Do not style elements without classes Compose rather than inherit One BEM module per file Example of doing it wrong .primary_navigation {}\n.secondary_navigation {} Why It is easier to find modules if you name your stylesheet files after their BEM Block class name. Not only that but longer files are a pain. Notable exceptions None. Keep your BEM modules less than 100 lines long Example of doing it wrong I think you can understand this one without an example. Why If your BEM module is getting lengthy then it's often a sign it's doing too much. Breaking down large modules makes your code easier to reason about. It also makes finding styles much easier! Notable exceptions If it's just over 100 lines don't go crazy on yourself. If you're hitting 150-200 then you ought to start breaking your module down. Never reference one BEM Block from another Example of doing it wrong .primary_navigation {\n  .secondary_navigation {}\n} Why It is easier to reason about where styles come from if all responsible styles are defined in one file. You can often tie yourself in knots when BEM modules become concerned with one another. This also breaks the first rule. Notable exceptions None. Never reference one BEM Element from another Block or Element Example of doing it wrong .primary_navigation {\n  .primary_navigation__link {}\n} .primary_navigation__link {\n  .primary_navigation__icon {}\n} Why Again it's easier to reason about styles when you do not nest style definitions inside one another. It simplifies specificity problems. Notable exceptions When the BEM Block is a modified Block. .primary_navigation__link {}\n.primary_navigation--mobile {\n  .primary_navigation__link {}\n} This is okay since the modified Block should affect the styles of the Element. Move repeating Element classes into their own Blocks Example of doing it wrong .primary_navigation__link {}\n.primary_navigation__link_icon {}\n.primary_navigation__link_title {}\n.primary_navigation__link_description {} Why When you see commonly repeated words in Element classes this often is a sign that you should break them out into their own module. In the example of doing it wrong above, .primary_navigation__link could become .primary_navigation_link. .primary_navigation_link {}\n.primary_navigation_link__icon {}\n.primary_navigation_link__title {}\n.primary_navigation_link__description {} Notable exceptions If it's one or two classes that are showing signs of being a new BEM Block I might leave it until there are three. Never name more than one class on an element Example of doing it wrong Why When you apply more than one class to an HTML element both those classes will now be fighting for control over that HTML element. If you instead use two different HTML elements you will save yourself a lot of headaches. A corrected version: Notable exceptions One exception to this rule would be Modifier classes applied by JS. Imagine you have an expanded version of your primary navigation, .primary_navigation–expanded that gets enabled and disabled by JS. Instead of removing the existing class you might instead just add the class to the element. This is okay since a Modifier extends from the original class. Do not style elements without classes Example of doing it wrong .header {\n  a {\n    color: green;\n    &:hover {\n      color: blue;\n    }\n  }\n}\n\n.primary_navigation {\n  a {\n    color: red;\n  }\n} Why Specificity is the problem yet again. If your BEM Block or Element finds itself inside another BEM module and both style an element you will sometimes get confusing results. In the example above, when hovering over a link in .primary_navigation the link would in fact go blue instead of staying red like you may expect. Instead of styling HTML elements you should add a class to the element and style that, like so: .header__link {\n  color: green;\n  &:hover {\n    color: blue;\n  }\n}\n\n.primary_navigation__link {\n  color: red;\n} Notable exceptions When styling WYSIWIG generated content that does not have any BEM classes it is acceptable to style elements. When I'm feeling lazy I do break this rule and it could perhaps be my most broken rule. Compose rather than inherit Example of doing it wrong .primary_navigation {\n  @extend %navigation;\n}\n\n.secondary_navigation {\n  @extend %navigation;\n} .primary_navigation {}\n\n.secondary_navigation {\n  @extend .primary_navigation;\n} Why Inheriting using @extend in SCSS couples your BEM modules too close together. When this rule is broken alone it's not so much a problem. However if you break my \"Do not style elements without classes\" or \"Never reference one BEM Element from another Block or Element\" rule as well then you can get some funky results. This is especially true if the class or placeholder you extended comes later in the alphabet than the class extending it. I'll leave the rest of this why to the reader at home. Notable exceptions None. Summary So there you have 'em. Learn these rules and practice them regularly and when the heat turns up at work you'll hopefully not have a big ball of styles to contend with a week or two later. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-07-28"},
{"website": "Made-Tech", "title": "Cross Browser Testing in IE7", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/cross-browser-testing-in-ie7/", "abstract": "We're getting ever closer to the point where we can finally stop supporting certain legacy browsers; IE6 is officially dead , and the support Microsoft provides for clients running IE7 is extremely limited , but it's still not particularly uncommon to meet a client that requires support for IE7. You may groan, you may even present a well thought out argument about how the less we support it the more we encourage people to move away from it and towards progress, but unfortunately a lot of corporate clients are locked into particular versions of Windows and, by extension, Internet Explorer. It costs money to upgrade them and, as the saying goes, if it ain't broke, don't do everyone (in our industry) a favour and upgrade it. With one foot forcibly stuck in the past on such a project, every front end developer knows the mounting sense of dread as you build something that looks beautiful in a modern browser but hasn't yet been cross browser checked, only to find something resembling Frankenstein's monster when you finally do. As a means to combat said monster, in this post I'll be detailing some of the more useful shims, polyfills, workarounds and ways to debug IE7 I've discovered. Setting Up Your Testing Environment Here at Made, we're all on OSX, so finding a way to test on Windows was a priority. BrowserStack offer a nice solution for cross browser testing, but a far more cost effective way to test IE specifically is to take advantage of the virtual machines modern.ie provides. There's no way to rollback to a previous version of IE once you have a later one installed, so if you need to test multiple versions, you'll need to download and install a separate VM for each one. It's unfortunate, as the VMs are huge, and can chew up your hard drive space pretty quickly. IE7 proper doesn't have any useful developer tools, so it's impossible to debug javascript errors, a lot of which will occur only in IE7 and can have you tearing your hair out. Fortunately, the developer tools that come with more recent versions of IE allow you to simulate older versions, making it much easier to squash those bugs (I've found the IE11 on Windows 8.1 VM to be the most user friendly). It's important to not lean too heavily on the simulated version of IE7 though, as it isn't a perfect simulation, and there will be small differences only present in true IE7. Do the majority of your cross browser testing/bug fixing in the simulation, and then boot up the IE7 VM to do a final sanity check. Setting Up Your Project First things first; set a strict doctype, like so: This allows you to use certain CSS properties like max-width and min-height, and prevents IE7 from defaulting to quirks mode, which is meant to simulate bugs present in even older browsers. Additionally, there will be a few instances where you need to apply certain styles for IE7 only, so make sure to include some conditional html tags that'll make it easy to target: < !-- You can also include multiple conditional tags if you want to specifically target other versions of IE. We use SCSS on all of our projects, so adding that class allows us to do the following: .foo {\n  // some styles\n\n  .ie7 & {\n    // some IE7 specific styles\n  }\n} (or you could also the much nicer looking when-inside mixin ) Another great tool to include whilst setting your project up is Modernizr . Not specific to IE7, Modernizr detects exactly what the current browser is and isn't capable of, and then adds classes to the html tag indicating what it finds. For example, if your browser is a legacy browser that doesn't support media queries, it'll add a no-mq class that you can target in much the same way as the ie7 class in the example above. [Polyfills](https://en.wikipedia.org/wiki/Polyfill) There's an exhaustive list of cross browser polyfills, but I'm going to talk about a couple I've gotten the most use out of: Box-sizing The box-sizing CSS property wasn't introduced until IE8, and using: * { box-sizing: border-box; } is quite a nice way to avoid having to do lots of mental gymnastics when calculating padding, width and margins. IE7 has no support for this whatsoever, so your layouts will tend to look a bit malformed in it. The box-sizing polyfill by Christian Schaefer is a great solution to the problem. Add the htc file to your project, and then update your CSS like so: * { box-sizing: border-box; *behavior: url(/polyfills/boxsizing.htc);} This will immediately fix the majority of layout issues you come across. That said, I have witnessed the polyfill cause strange issues on certain elements, such as ignoring all width styles completely on elements that are dynamically hidden and shown by javascript, and on input fields, which continuously shrink on focus and blur events, to the point that the text being input can't be read. In these cases, simply removing the behaviour fixes the issue: .foo {\n  // some styles\n\n  .ie7 & {\n    * { box-sizing: border-box; *behavior: none; }\n  }\n} Console.log You shouldn't have console.log commands lying around in code that's ready to go to production, but in development and staging environments they can be useful for debugging, unless you're in IE7, which doesn't know what to do with console and prevents all of your nice js-powered interactions from working. The fix is to add this polyfill by Paul Miller to the top of your javascript: (function(global) {\n  'use strict';\n  global.console = global.console || {};\n  var con = global.console;\n  var prop, method;\n  var empty = {};\n  var dummy = function() {};\n  var properties = 'memory'.split(',');\n  var methods = ('assert,clear,count,debug,dir,dirxml,error,exception,group,' +\n     'groupCollapsed,groupEnd,info,log,markTimeline,profile,profiles,profileEnd,' +\n     'show,table,time,timeEnd,timeline,timelineEnd,timeStamp,trace,warn').split(',');\n  while (prop = properties.pop()) if (!con[prop]) con[prop] = empty;\n  while (method = methods.pop()) if (!con[method]) con[method] = dummy;\n})(typeof window === 'undefined' ? this : window); Workarounds One of the dirtier workarounds I discovered recently, which I frankly didn't know was even possible, was how to deal with the :before and :after pseudo elements in IE7. We use Icomoon a lot for our icons; icon fonts are a nice solution for creating icons that look great on any device. In this particular project, we have an icomoon mixin that looks like this: @mixin icomoon($icon) {\n  font-family: 'icomoon';\n  speak: none;\n  font-style: normal;\n  font-weight: normal;\n  font-variant: normal;\n  text-transform: none;\n  line-height: 1;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n\n  &:before {\n    content: $icon;\n  }\n} However, icon fonts require using psuedo elements, which IE7 simply doesn't see. After some searching, I found the solution to this problem: javascript within css. By adding the following to the above mixin (before the &:before psuedo-selector): *zoom: expression(\n  this.runtimeStyle['zoom'] = '1', this.innerHTML = '#{$icon}'\n); We're able to target IE by using a CSS property only it will see (*zoom), and sneak in some javascript so that it will set the icon as the content within the element, bypassing the need for the psuedo element. This workaround has similarities to a much more innocent workaround, for display: inline-block. IE7 doesn't recognise inline-block at all, but you can achieve the same effect with the following well-known workaround: display: inline-block;\n*display: inline;\n*zoom: 1; All other browsers ignore CSS properties prefixed with *, but IE won't. What zoom does is trigger hasLayout on the element, which gives it block properties and, combined with *display: inline;, gives us the desired result. Of course, if you're using Compass with your SCSS, that's wrapped up for you in their mixin . There are many workarounds for the roadblocks IE7 throws up, and these are just a few that I hope others will find useful, but please, feel free to share any others that have saved you valuable time! Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-07-21"},
{"website": "Made-Tech", "title": "Your Server Is Not A Pet", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/your-server-is-not-a-pet/", "abstract": "You should treat your servers as cattle, not as pets. While we can't claim to have coined this catchphrase here at Made, it is certainly a philosophy that we subscribe to. Pets are things you form a bond with and give a fluffy name to, that exhibit a unique personality, and are nurtured when they become sick. Cattle on the other hand are branded with an identifier, are near identical to one another, and if they become sick, you put them down. We view manual configuration of both servers and network infrastructure as an anti-pattern that should be avoided. The role of a sysadmin logging in to a server and making changes is A Bad Thing, and an activity that should be resigned to history. Don’t hand-rear your servers There is a whole industry of configuration management tools that exist to allow you to provision servers in to a known good state quickly and repeatably. Ranging from the likes of Chef and Ansible which provide operating system-up configuration management, to BOSH and Terraform that enable you to manage the network, servers, and everything beyond. As soon as somebody logs in to a server and starts issuing commands to change the state of that server, its integrity has been compromised. You have no reliable way to get a server in to that state again and your ability to trust pre-production/production parity has now been removed. You've created a snowflake . Keep state where you can see it You should make moves to keep application state in very specific places within your herd. If you're talking about an average web application, this is probably going to mean data stored in your database and user uploaded content on some sort of filesystem. Unless a specific server role needs to permanently store data, do everything you can to avoid it. For our money, there are a number of services available that make it someone else's responsibility to store and replicate this data, such as Amazon RDS , Rackspace Cloud Databases , Amazon S3 and Rackspace Cloud Files . If you really can't avoid storing application state on your own infrastructure, you'll need to be mindful of the additional data restore step needed when replacing nodes. So what? So you've told your sysadmin to stop typing, invested in building a library of Chef recipes, and ensured your app is built to Twelve-Factor principles. What now? We see a number of significant benefits from adopting this approach to infrastructure management: Scalability: scaling is now as easy as either re-provisioning a node on better hardware (vertical scaling), or provisioning a new node on similar hardware (horizontal scaling) Disaster Recovery: replacing failed nodes requires nothing more than re-provisioning the failed instance (indeed, some tools such as BOSH can automatically orchestrate this for you) Single source of truth for server configuration: If you want to understand how a server is configured, you can read the code that configures it. As you've checked this configuration in to source control, you've got a fully audited history of every change. Reduced vendor lock-in: As you have the means to recreate your entire herd, usually in a matter of minutes, the barrier to moving to new infrastructure providers can be significantly reduced. Ability to reproduce production-like environments: For running things such as performance tests or destructive security testing, spinning up a complete replica of the production environment for a short time, particularly when paired with a pay-per-hour infrastructure provider gives significant flexibility. So, what next? We see significant interest in tooling such as BOSH and Terraform. The likes of Chef, Ansible, Puppet and friends provide excellent tooling for managing your server configuration, though we still often see manual steps involved in provisioning in the underlying infrastructure and configuration of things like network interfaces. Tools such as BOSH and Terraform either compliment or replace what is offered by these tools by taking management a step further – by interfacing direct with the underlying infrastructure. We're actively investigating Terraform as a means to document and manage our AWS fleet ongoing. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-07-16"},
{"website": "Made-Tech", "title": "Optimise SCSS Sprockets Performance in Rails", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/optimise-scss-sprockets-performance-in-rails/", "abstract": "We build rich websites at Made. In production, the SCSS we write is precompiled, minified, gzipped, and served from Amazon S3. However, in the development environment we're without any of this magic, and the rails app server can sometimes feel like it's ground to a halt. Why is it so slow? Sprockets will refresh every new or updated asset on every page load. This alone could be the cause of your slow down, depending on the number and complexity of your SCSS files. However, if that's not the case, and your SCSS is relatively small then you probably aren't using Sprockets to its full potential, and relying too much on LibSass to compile all your SCSS into one file. How to improve that first page load time Your main performance gain is going to come from better organisation of the number of files you @import in each of your SCSS files so: Now you have stripped down your SCSS imports, the next gain is to, if you aren't already, use the full power of Sprockets. As mentioned earlier, Sprockets will recompile every updated or new asset on every page load, so by using Sprockets properly only the file you've changed or added will get compiled. This is most evident in the  of your rendered HTML. The second example is more performant, because it only refreshes the individual files, which can then be cached out by the browser. Finally, you ought to be really stringent on the 3rd party vendor mixin library imports, for example Compass . If you simply \"@import 'compass'\" you get a whole lot of cruft you might not necessarily need, so you should only include the features you need. How have we achieved better SCSS compilation  perf. in our apps? We have improved our precompilation time by abstracting all our common @imports into one common file, which then gets included at the top of each new SCSS partial. Additionally we cut down the number of 3rd party imports – mainly Compass – to the bare essentials. Were there gains? Once all the SCSS files had been moved over to this way of working the gain in page load speed was instantly obvious. Pages that were taking 50 seconds to load were now rendering in a fraction of the time. When you think about the bigger picture, all those wasted seconds waiting for a page to load over the course of a project are really going to add up. Whilst in front end development, say you reload a page 100 times during its build, and through better stylesheet management you save an average 20, that's 34 mins. That 34 mins over the course of a project, with say 10 unique pages soon becomes 6 hours. 6 hours lost to page loads which could be easily saved. So, saving this time will ultimately benefit everyone. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-08-04"},
{"website": "Made-Tech", "title": "5 TDD Antipatterns", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/5-tdd-antipatterns/", "abstract": "The value of a test suite can easily go down when complexity goes up, and a lot of the time this complexity can be prevented. There is a great discussion on SO about this. There are many ways to complicate your tests, these are my 5 favourite and most encountered ones in the wild: The Liar Perhaps the most devious of them all. This test instills confidence that the system is correct even though the SUT is actually broken.  Exposing this test for what it really is can be challenging to say the least.  It's hard to accept the fact that tests have bugs too. This is also closely related to the Mockery anti-pattern where boundaries are stubbed out with expected values.  If you don't ensure that your boundaries are always correct in an ever changing codebase, you can start getting false positives. The Free Ride Instead of writing a new test case for the feature, you add another assertion to an existing test case.  This often results in additional setup to get both assertions to pass, a short term win as you have to write less code to make the assertion. Instead of preserving cohesion, you are creating the equivalent of god objects in your tests.  When they fail you cannot be sure which assertion to look at.  Integration tests with a large amount of required setup usually fall victim to this. Excessive Setup Every line of setup makes a test more difficult to understand, it also increases the possibility of introducing new bugs.  This is usually a sign that you have too many dependencies, and that a feature cannot exist without the known world being in a very particular state.  It becomes even worse when you start mixing in doubles and stubs etc. Sometimes you will still be required to do some setup,  and tools exist to help you with this such as FactoryGirl. You need to be able to make the distinction between required setup and setup due to  unreasonable application code.  For reasonable code, you need to master composing factories in a minimal way so that you preserve the  understandability of your tests. The Happy Path I see this happening most frequently in large integration tests.  You will write your happy path test, which requires a fair amount of setup, then discover an edge case.  You do not want to duplicate all the setup code (as this would be an anti-pattern itself \"Second Class Citizen\"), and you do not want to piggy-back  off the happy path test as we saw above. This discourages edge case testing, and only the happy path test gets written.  Important edge cases may be missed such as boundary testing where data may be too large to fit in a database field.  A solution to this would be to write the happy path test in a black box feature spec, and to test edge cases at a lower level. The Flickering Test This anti pattern is usually caused by race conditions. Ruby waiting for Javascript or vice versa is something that is very common in web apps these days.  \" Meh it's just a flicker \" the developer said, and ran the suite again, but what this test has done is declare the entire test suite as unreliable. These flickering tests need to be solved, and if that is not possible, need to be isolated from the healthy tests.  I prefer to not enable javascript in my Rspec tests, and if a piece of javascript is complicated enough, it should be tested with a Javascript  testing framework. Keep your tests clean and reasonable or you will very quickly lose faith in them. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-07-09"},
{"website": "Made-Tech", "title": "A Guide To Blue-Green Deployments & Going Live Every Day", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/a-guide-to-blue-green-deployments-and-going-live-every-day/", "abstract": "One of the riskiest parts of software delivery is the production deployment. It's the first time in the delivery cycle that code changes are exposed to end-users. If the application has a significant level of traffic, it could be the first opportunity to see how it performs under load. It's that scary moment when you open your code up to the scrutiny of a firehose of users, bots and services, hitting endpoints in ways you hadn't expected. It can be a very stressful point in the delivery process if you do not approach it in the right way. In this article, I am going to take you through the techniques we use to reduce deployment risk and explain how we've gone from an infrequent, stressful and error-prone deployment process, to one which enables us to execute production deploys at the click of a button. We'll show you how we rollback changes, architect our codebase and enforce delivery practices to reduce the risk, enabling us to deploy hundreds of applications into production every week. Before getting started, it might be worth reading the blog post my colleague David wrote on the Continuous Delivery practices that we run. This should help you to understand our approach to software delivery. We've been using the Blue-Green technique since 2010 and we've now got ~60 applications running Blue-Green deploys every week. This process is not perfect, but it has done wonders for our production deployment process, and enables us to get features delivered faster, without the fear of getting the changes into production. I'll provide a short introduction to Blue-Green deploys, for those who have not heard about it: A Blue-Green deployment is a technique in which you have two identical production environments that you can deploy an application to. One environment is named Blue, the other Green. You route live users to one environment, let's say Blue, and deploy the new application into Green. Once you've deployed and tested the new application on Green, you flip the traffic and direct the live traffic to the Green environment. The Green environment becomes the new live, and next time you'll deploy to Blue. There are many benefits to Blue-Green Deploys, such as an easy rollback process if the deployment doesn't go as expected and being able to deploy to production with zero-downtime. In the above example, if we had seen an increase in error rates, we could route the live traffic back onto blue without any downtime. Getting an application ready for Blue-Green There are many practices which we have adopted to enable us to get to a Blue-Green deployment process. These practices have also given us the ability to scale horizontally easily, as the underlying architecture principles are the same. Statelessness The application must not persist state to the server. We ensure things like cookies, cache files, logfiles, UGC etc are backed by external services like memcached, Amazon S3 and Papertail. Database Migrations We use a shared database between the Blue / Green production applications. All database schema changes are stored in migrations, which can be rolled forwards and backwards. Forward rolling DB migrations should be written in such a way as to not affect the currently live application. Backward rolling migrations should reverse the effect of the forward migration. Non-Destructive Migrations If you need to perform destructive actions against the database, defer them until neither Green nor Blue has code relying on the destroyed tables. Releasing small and often helps to achieve this. Dark Launches When features are not ready for consumers to see, we favour dark launching them over delaying a deployment. We find this is a safer way to get code into production and minimises the amount of code changes being deployed, when the feature actually goes live. Infrastructure Automation All server configurations must be stored in code, so they are repeatable and automated. We primarily use a hosted PaaS called Cloud Foundry, which allows us to define a particular buildpack in application manifests. However, whether we're provisioning our own Cloud Foundry infrastructure or custom infrastructure on AWS, we'll make sure we use tools like Chef or Puppet, so infrastructure is identical every time. Small Changeset Mindset Commit early and often. Deploy as frequently as possible. These are our main guidelines to our team and customers. If a change has been sitting in a staging environment for more than a few days, then the developers are likely to have forgotten the implementation details and you're increasing the risk around a production deploy. The more fear there is around deployments, the less likely they are to happen. We've got scripts which look at days since deploy and send alerts to our Hipchat room if we exceed a threshold. Feature Toggling Where possible, implement feature toggling so you can get features out to a subset of users (such as the customer and engineering team), and give yourself an opportunity to address any bugs and exceptions before they make their way through to the full user base. This approach increases the engineering effort, but is a useful technique if the application is of sufficient scale. Rollback If You Can't Roll Forwards If something goes badly wrong with a Blue-Green deploy, you've got the option of a full rollback. In our experience, this is rarely needed, but it's nice to know it's available, for those occasions. We tend to favour rolling changes forward where possible, as it's easier to deal with problems when they crop up, than defer them to later. Independent Caches Ensuring the Blue & Green environments have separate cache servers is an important lesson that we learnt. You should to be able to clear individual cache keys or entire application caches, without impacting the other environment. Monitoring Production Deploys Once an application is deployed to live, it's important that it's monitored closely for potential problems. We've tried a number of different techniques and tools and currently favour doing the following: Application Performance Monitoring We watch New Relic's key performance metrics in real-time and have alerts setup to notify us if they exceed certain thresholds whilst we're not in front of our screens. If we've deployed a brand-new feature, we'll closely monitor the controller actions to see where the application is spending time and quickly detect any potential problems, such as slow running SQL calls or unnecessarily complex Ruby. Exception Notices We always look at the exception notifications. These are an early indicator of significant problems, so we like to see exceptions infrequently. If the exception notifications increase to a significant volume we'll investigate further and decide if they warrant a rollback or corrective action. Application Logging We also monitor the application logs closely. We setup alerts within tools like Papertrail and Loggly to report any unexpected behaviour we notice. This becomes a useful technique for trying to resolve issues that happen infrequently. Whats Next? Our deployment process is great, though we feel like there is room for improvement. We're currently exploring the following techniques to help us improve it further: Canary Releases We see Canary Releases as the evolution of Blue-Green deploys and the obvious next step in improving our production deployment process. If you're not familiar with Canary Releases, the idea behind them is that you deploy a set of changes to a small number of end users, before rolling this out to an incrementally larger userbase, until all users have access. It's the technique that very large scale sites, like Facebook use. If you're interested in learning more, you should watch this video of Chuck Rossi from Facebook talking about their release process. . Smoke Tests We are looking to introduce a Smoke Test that runs against Blue-Green and verifies that the build has deployed successfully. At present, we manually check the latest Blue-Green deploy before routing live traffic through. We see potential value in automating this process by having a test suite that runs in the production environment and verifies key features are working. If the Smoke Tests fail, we would prevent this build from being routed into live. Further Reading If you're looking for further information on Blue-Green deploys, check-out Martin Fowler's introduction to Blue-Green Deploys here or this article by Transport For London on how they utilise Blue-Green deploys on tfl.gov.uk. You could also check out a RubyGem we've written called CF:Deploy Gem , which handles Blue-Green deploys to a Cloud Foundry environment and flipping applications into live and rolling back (if necessary!). Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-07-14"},
{"website": "Made-Tech", "title": "Javascript of the Future", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/javascript-of-the-future/", "abstract": "The history of Javascript, formally known as ECMAScript and originally Mocha, is a strange one. Originally developed by Brendan Eich at Netscape in ten days with the intention of creating a scripting language for the Web which could be picked up by new programmers, it has been praised and maligned in every corner of the Internet, by the same people in equal measure. The little language that could, or could at least try its hardest, has now spread past the browser to runtime environments like Node.js, game engines like Unity and \"hybrid\" mobile apps . This ubiquity is forcing the language to grow in scope and style dramatically. The next, better equipped, better looking version, ECMAScript 6, is finally here. And in light of its upcoming integration with Sprockets and its final specification being released , now is a good time to get familiar with it. Sugar coating Much has been made about the influence CoffeeScript has had on ES6, and the language is certainly starting to look more like its sister. New syntactic sugar for Object constructors, 'fat arrow' functions and destructured assignment go a long way towards making Javascript feel fresh. There are better resources for actually learning ES6, but here's a quick look at what New Javascript looks like. // A new Class syntax.\nclass Person {\n  // Destructured object arguments, with defaults\n  constructor({ job = 'Unemployed', age = random(1, 100) } = {}) {}\n\n  // Default arguments  speak(message = 'Hello') {\n    // Template strings have their own `special quotes`\n    console.log(`${this.name} said ${message}`);\n  }}\n\n// Fat arrow functions maintain context, just like Coffeescript\ndoubler = (num) => { num * 2 };\n\n// Constants cannot be redefined\nconst x = 1;\n\n// Lets have block specific variable scope\nlet y = 2; There is a new native module loader syntax which manages to fit in with the CommonJS spec used by Node and Browserify, and the RequireJS spec popular elsewhere. The following are equivalent statements. // CommonJS\nvar t = require('tape');\nvar inherits = require('util').inherits;\n\n// ES6import * as t from 'tape';\nimport { inherits } from 'util'; More excitingly, but less visibly, ES6 has borrowed more liberally from CoffeeScript's inspiration, Ruby, and the concurrency craving Go. With new features like Proxies, Promises and extended Prototype under its skin, Javascript is becoming a more mature scripting language, without sacrificing the ultra object orientation and transparency that makes it unique. And with the accelerated release schedule ECMAScript is adopting, we should expect it to quickly adopt even more from the 'programmer happy' scripting languages out there, with async/await functions coming in ES7 and Macros proposed for ES8, allowing the creation of custom DSLs à la sweet.js . Metaprogramming Metaprogramming means writing code which is introspective and can modify itself, or other programs. Javascript historically has limited support for advanced metaprogramming as developers have been bound to using the high level APIs of Objects, but that has changed with the introduction of Proxies. Proxies utilise 'traps' to intercept language level object events. There are 14 traps available , including get, set and setPrototypeOf and each can be used to observe or even replace a native manipulation of an object. It allows deeper access to an object's natural functionality than getters or setters, function methods like Function#call and Function#apply or Object methods likeObject#keys have in the past. Although their compatibility is limited at the moment, their potential going forward is great. In Ruby method_missing can be defined on an object, and any subsequent calls which do not match existing methods will be run through it. It allows Ruby developers to create dynamic functionality like Rails' find_by_* methods. Proxies provide a much lower level ability to observe and intercept objects, but defining a get trap could lead to similar functionality in JS objects. var methodMissing = function(target, handler){\n  return new Proxy(target, {\n    // Intercept 'get' actions, like input.value\n    get(ref, prop){\n      let skip = function(){\n        // The Reflect API provides access to native functionality,\n        // so we can fallback if necessary.\n        return Reflect.get(target, prop);\n      }\n\n      // Fallback if the property already exists.\n      if(target.hasOwnProperty(prop))\n        return skip();\n\n      // Pass the functionality to the handler defined.\n      return handler.call(this, prop, target, skip);\n    }\n  });\n} Of course, this could be used for a wide variety of interactions. For example we could proxy Backbone Collections to ape functionality similar to ActiveRecord's dynamic finder methods. library.findByName('Into Thin Air');\nlibrary.findByNameAndAuthor('Supergods', 'Grant Morrison'); Directly proxying function calls on an object to HTTP requests to an external API could make building RESTful API wrappers simple, and futureproof. facebook.user({ id: 12345 }, (err, user) => {  });\ntwitter.tweet({ message: 'Test message' }, (err, response) => {  }); Promises and the future Those of you familiar with Node.js or libraries like Bluebird or Q will be aware of Promises, and how useful they are for avoiding the callback hell which can easily plague asynchronous Javascript with the best of intentions. The concept behind Promises is that instead of an asynchronous function accepting a callback – a common Javascript paradigm – it returns an object which will later be fulfilled. var p = $.getJSON('api.dev.battle.net/whatever')\n  .then((err, response) => { console.log(response.results); })\n  .error(handleErrors); Splitting your asynchronous code up like this allows for chainability, code organisation and consolidated error handling. Generators Generators are a new type of function which yield a value, returning it and temporarily stopping function execution. It won't start up again until the next value is requested. The following shows an example fibonacci sequence generator which could run infinitely, but will only run as many times as is needed. var fibonacci = function*(){\n  var last = 0\n    , current = 1;\n\n  while(true){\n    let old = last;\n    last = current;\n    current += old;\n\n    // Yield halts the function and returns a value.\n    yield current;\n  }}\n\n// Generators can be iterated through with the new 'of' syntax\nfor(var num of fibonacci()){\n  if(num >= 1000) break;\n  console.log(num);\n} Using it now You can try the language online with the Babel REPL . The language isn't production ready by itself. Compatiblity still isn't there and it won't be for a while. However, I've been writing ECMAScript 6 for a while now and transpiling it into ES5 using, originally, Traceur and now Babel.js . They translate as much of the new language features and syntax as they can into a version of Javascript with high compatibility, which can be used in existing environments. New features like Proxies which weren't possible before, and therefore cannot be transpiled, are available in new releases of Firefox and, somewhat surprisingly, Microsoft's Edge browser. Naturally, as time goes on, compatibility in current browsers and runtimes will increase, but for the time being these are a good way to get writing ES6 today. I've been using Gulp tasks similar to this to translate my source folder to an ES5 folder, which I use as the index of my Node projects. Node tip: Ignore the build folder in Git, add a \"prepublish\" script to your package.json to run the Gulp task. Going forward Perhaps the most interesting aspect of ES6, and the greatest testimony to the Javascript community at large, is that the features on the surface of this update aren't that exciting. A lot of this functionality became part of the spec and got introduced early by eager developers, or has been 'pulled into master' after exploding in the Javascript community. In a way Javascript often feels like 'build-a-language', as the community is so vibrant and passionate about extending the language as they see fit. For all intents and purposes, JavaScript has merged CoffeeScript into master. I call that a victory for making things and trying them out. – Reginald Braithwaite The greatest parts of ES6 are the parts you miss at first glance. The combination of Generators and Promises are changing everything. There are already complete libraries like Co for manageable flow control built with this combination. It is the cornerstone of async/await functionality in ES7 and essential to the next few official releases. // One day.\nvar users = async function(){\n  var response = await request('/users');\n  return response.data;\n} The future for Javascript is unclear , but the language is getting stronger every day. Should the language continue to provide building blocks to its enthusiastic developers and then standarise their creations natively, the future of Javascript will continue to be up to the people using it. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-07-07"},
{"website": "Made-Tech", "title": "Lessons from the Trenches", "author": [" Nick Wood"], "link": "https://www.madetech.com/blog/lessons-from-the-trenches/", "abstract": "I've worked with organisations at various stages of their Agile transition, during which I've seen Agile implemented well, but I've also seen it done terribly. In a break from my usual tradition of myth-busting, I wanted to share some hard won lessons from my years as a Scrum Master, and give you some tips to help your Agile transformation succeed where many fail. Dazed and Confused I'll admit, starting out on your Agile journey is a daunting prospect; recent years have seen an explosion in the number of Agile methodologies available. Picking your first framework is a lot like buying your first car, you go into it with a very strong feeling of what you think you need. However, what you think you need, and what you actually need are very different and it's only possible to truly understand which is which until you've driven it around for a few months. For a first step into Agile, my best advice is to read a good primer on Scrum (I recommend this one ) and just jump in. All or Nothing When you're getting started it's tempting to want to do it gradually, pick out the easy bits, do those first, and then gradually introduce new concepts until you're 'fully Agile'. I'd strongly recommend not to use this approach. The transition to Agile is a tough one, and by spreading it out over many months, you're elongating how long this pain lasts, whilst also delaying the time when you can really start reaping the benefits of being an Agile organisation. Humans are also creatures of habit: every opportunity to cling to the old ways that you give them, they will take; and as the pool of what they are familiar with dries up, they will cling ever tighter to the small details from the old ways. By this point you're trying to tackle the hardest bits of the transition, and this sort of emotional investment in how things used to be only makes it tougher. You Can't Always Get What You Want I'll occasionally run across people who sell Agile methodologies as some magic bullet that will fix all organisational problems. In a sense this is true, but to an extent – take a saw for example, saws are great for cutting wood with, but the best saw in the world still needs a skilled carpenter to use it. Agile methodologies are tools, and they are only as good as the people using them. Crucially, going Agile won't magically fix all of your problems. What it will do is paint huge neon targets over every problem your business has, but you need a great team of people to step in and solve them. Go Your Own Way Once you've been Agile for few months, and I highly recommend doing it by the book for at least your first 3 months, you'll get a sense of what doesn't work for you and where things can be tweaked. This is a great time to look at other Agile techniques, and gradually introduce them into your process. This seems to contradict my 'all or nothing' advice, but there's a subtle distinction here. All or nothing is the best way to get people out of their old mindsets – throwing them out of their comfort zone will force them to adapt initially. However, no Agile framework is going to tick all your boxes off the shelf; once you've got some core Agile values in place, you can start tailoring things to better suit your needs. As we're fond of saying around here – prototype fast, then iterate. Don't Stop me Now Most companies want to adopt a form of Agile in order to produce better software quicker – don't get me wrong, that's a great ambition. My problem is that most stop there. A part of me feels like this is a bit of a waste. Agile principles can be applied to a much wider spectrum than most people realise, but it's rare to hear stories of an Agile finance team or HR departments using Scrum, though I'm sure they must exist somewhere. Once you're comfortable speaking the Agile language, do your colleagues a favour and see if there are others around who might benefit from the lessons you've learned along the way. For my part, I'm planning my upcoming wedding with a nifty looking Kanban board: Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-06-29"},
{"website": "Made-Tech", "title": "There is more to code quality than coverage", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/there-is-more-to-code-quality-than-coverage/", "abstract": "As developers we always strive to write the best code possible and, while we test for it, coverage doesn’t always tell the full story of the quality of your code output. Firstly, what is coverage? Within a coding context coverage refers to the number of lines of code covered by tests. Once calculated, code coverage is a way of seeing how much of your source code is covered by tests. The higher your code coverage, the better tested your source code is, and the less likely it is to suffer from any bugs or regressions during its life time. Previously we’ve shown you our complete deployment pipeline . In this article I am going to talk about the tooling we currently use during the first step of our deployment pipeline – the build step – to keep our code at a consistently high standard. The majority of our code quality testing happens before any unit, or feature tests are run. The exception to this being security testing, as this static code analysis takes a lot less time to run than an entire test suite does, so you don’t want to waste a build slave running \"bad code”. The areas we focus on can be broadly separated into three categories: Complexity, Style, and Security. Complexity More specifically, the complexity of Assignments, Branches and Conditionals (ABC) within a method. This metric can quickly identify potential “code smell” and overly complex methods. ABC metrics are not language specific, but the way you calculate them does vary between each language to allow for syntax differences. Broadly speaking though the ABC metric is calculated in a similar fashion. The ABC metric of methods (or functions) is calculated by evaluating the number of variable assignments, calls to other methods or classes, and number of conditionals within said method. Our build will fail if any method exceeds the industry standard score of 15. A more in-depth explanation of the ABC metric can be found in this article . Style Language style guides aren’t anything new, but the enforcement of a consistent code style within the team aids readability, and understanding. We run style analysis on both our Ruby and SCSS files, and fail the build for anything deemed a serious violation. For example, missing spaces around a brace will fail the build, while an unnecessary double quote will just get you a warning. Security Writing secure code has to be a top priority, so identifying any potential vulnerabilities in your codebase as early as possible is a must. Some of the common vulnerabilities you should be looking for are: Remote code execution Cross site scripting SQL injection String formating We run our security analysis after feature and unit testing to guard against these common pitfalls, the thinking being that this build could reach production. In addition to this security analysis, you could run any number of security scans on your application once it is deployed out. Self-Discipline All the previous areas I have discussed are easily automated, but there is an important human aspect to writing good code. Without self-discipline everything else could be in vain. It is down to the individual to write clean, and readable code. But this responsibility to be disciplined also rests on the team too. As a team you should be spending time looking at each other's commits, to keep yourselves honest. When you find an area of code that could be tidier, consider pair-programming with the author of the code. Testing To test for complexity, style, and security we (at time of writing) use the following gems: Tailor – Style analysis tool for Ruby that, like its peers, is configurable based on the Ruby community style guide . Cane – Code analysis tool for Ruby that enforces the ABC metric and some basic styles like line length, trailing whitespace, and new line at the end of a file. SCSS-Lint – Style analysis for SCSS which, like Tailor, can be configured to your SCSS Brakeman – A vulnerability scanner for Ruby on Rails that will find security issues. A comprehensive list of the vulnerabilities it identifies can be found here . Because all of these gems come bundled with Rake tasks, we can easily switch them out if we find a better gem for the job. This enables our code analysis to evolve from project to project. Looking forward A code analysis tool that I’d like to implement is Rubocop. Rubocop would effectively replace Tailor and Cane in our current set up as it provides an ABC metric, and style analysis. It can even rectify some issues automatically for you, which would be great, because there is nothing more annoying than a failed build because of a missing new line! Links Tailor – https://github.com/turboladen/tailor Cane – https://github.com/square/cane SCSS-Lint – https://github.com/brigade/scss-lint Brakeman – http://brakemanscanner.org/ Rubocop – https://github.com/bbatsov/rubocop Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-09-17"},
{"website": "Made-Tech", "title": "Making Multiple Browserify Bundles with Gulp", "author": [" Fareed Dudhia"], "link": "https://www.madetech.com/blog/making-multiple-browserify-bundles-with-gulp/", "abstract": "Recently I was writing an npm module, and I wanted to include some examples of how to use the module in the repo. For me, this required having a gulp watch function which compiled several different files that all included the same module into several different bundles in several different places. Finding examples of doing this were very rare and/or incomplete online, so I'm writing a full breakdown of what I did for anyone having the same issues that I was. So, I wanted the structure to look like this. src/\n  _ index.es6\n  _ my_lib.es6\n  _ other_lib_file.es6\n  _ a_third_lib_file.es6\nexamples/\n  _ basic.es6\n  _ square.es6\n  _ matrices.es6dist/\n  _ examples/\n       _ basic/\n            _ index.html\n            _ bundle.js\n       _ square/\n            _ index.html\n            _ bundle.js\n       _ matrices/\n            _ index.html\n            _ bundle.js\n\n...(The lib/ and test/ folders, etc., aren't important here.) Where each of the .es6 files in examples/ will include my module (in src/), and be bundled into the corresponding directory in dist/examples/. This would mean that my gulp watch command would have to watch several different files individually, and compile them to corresponding bundle.js files. I'm going to go through my gulpfile and describe how it works. Please also note at this point that I'm using babelify to convert the es6 to commonJS, but this example should work fine without babelify. Also, as of yet I'm not doing any uglifying. The gulpfile.js var gulp = require('gulp');\nvar babel = require('gulp-babel');\nvar rename = require('gulp-rename');\nvar sourcemaps = require('gulp-sourcemaps');\n\nvar watchify = require('watchify');\nvar babelify = require('babelify');\nvar browserify = require('browserify');\n\nvar source = require('vinyl-source-stream');\nvar buffer = require('vinyl-buffer');\n\nvar merge = require('utils-merge');\nvar glob = require('glob');\n\n// Makes the bundle, logs errors, and saves to the destination.\nfunction makeBundle(src, watcher, dst) {\n  // This must return a function for watcher.on('update').\n  return function() {\n\n    // Logs the compilation.\n    console.log('Compiling ' + src + ' -> ' + dst)\n\n    // Bundles the example!, which then:\n    return watcher.bundle()\n\n      // Logs errors\n      .on('error', function(err){\n        console.log(err.message);\n        this.emit('end');\n      })\n\n      // Uses our new bundle as the source for the sourcemaps.\n      .pipe(source(dst))\n      .pipe(buffer())\n\n      // Creates the sourcemaps.\n      .pipe(sourcemaps.init({ loadMaps: true }))\n      .pipe(sourcemaps.write('.'))\n\n      // And writes them to the destination too.\n      .pipe(gulp.dest(''))\n  }}\n\n// Watchifies the examples and their local import trees for bundling.\nfunction makeWatcher(src, dst) {\n  var args = merge(watchify.args, { entries: [src],\n                                    debug: true,\n                                    fullPaths: true,\n                                    extensions: [\".es6\", \".js\"] });\n\n  // The `watcher` watches, compiles from es6, and browserifies the entries given in `args`.\n  var watcher = watchify(browserify(args)).transform(babelify)\n\n  // `bundle` becomes a function that will be called on update.\n  var bundle = makeBundle(src, watcher, dst);\n\n  // Listens for updates.\n  watcher.on('update', bundle);\n  return bundle();\n}\n\n// Watches the example\n files.gulp.task('watch', function (done) {\n\n  // Find all source files in the `examples/` directory.\n  var files = glob.sync('examples*.es6');\n\n  // filesWithWatchers will be an array of simple objects that each contain a\n  // filename and a boolean that determines whether the file is currently being watched.\n  var filesWithWatchers = [];\n\n  for (var i = 0; i < files.length; i++) {\n    filesWithWatchers.push({ file: files[i], watching: false });\n  }\n\n  // Loop over all the files in the directory.\n  filesWithWatchers.forEach(function (entry, i, entries) {\n    // Don't let this loop finish.\n    entries.remaining = entries.remaining || entries.length;\n\n    // Get the destination for this bundle.\n    var bundleDest = ('dist/' + entry.file).split('.')[0] + '/bundle.js';\n\n    // Make a watcher unless the entry already has one.\n    if (!entry.watching) {\n      makeWatcher(entry.file, bundleDest);\n      entry.watching = true;\n    }\n  });\n\n  return;\n}); How it works The gulp.task('watch') function is what we call with gulp watch on the command line. For each of the files in examples/, we call the makeWatcher function which goes away and makes a watchify/browserify/babelify bundler (called watcher) for that file. The makeBundle function returns a function which has the watcher make the bundle. It also then checks for errors and compiles our sourcemaps. The makeWatcher function saves the function returned by the makeBundle function into a variable called bundle. This is then passed to the watcher.on('update'), so that the example is rebundled whenever a file in the example's import tree is changed. makeWatcher then calls bundle() to make an initial bundle for the example regardless of whether anything has changed. Hopefully that wasn't too bad! I'm still relatively new to this way of working, so any questions or comments would be much appreciated. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-07-18"},
{"website": "Made-Tech", "title": "3 Ways to Reduce Page Load Times on Your E-Commerce Website", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/3-ways-to-reduce-page-load-times-on-your-e-commerce-website/", "abstract": "As Benjamin Franklin once said, time is money, and for an e-commerce website every millisecond counts. Page speed is a common sticking point for most solutions out there, so in this article I’m going to describe 3 practical ways you can decrease your page load time. Image Optimisation Images are by far the single largest group of resources a visitor to your site will download. When creating image assets, both designers and developers are used to either “Saving for Web” from Photoshop or running site assets through apps like TinyPNG or SmushIt, all of which compress and optimise your images (hopefully) without loss of detail. Another technique that has been around for a while, yet still has a positive effect on your page load, is CSS sprites. There's a modern SVG twist on this technique which I won’t go do into detail on here, but for those wishing to read up Chris Colier has a great tutorial over on CSS Tricks . These are great ways of saving bytes on images you create, but what about assets uploaded through a CMS or by a user? Thoughtbot maintain a great Ruby gem called Paperclip which leverages imagemagick to crop, resize, and optimise your uploaded assets. You could then take this one step further, by implementing the picture element and serving the correct image for the correct viewport. Domain sharding Out of the box web browsers limit the number of concurrent connections per domain, which is usually two, and any further requests get placed in one long queue, thus slowing down our page load. However, we can side step this queue if we load different types of content from different subdomains, which creates more possible connections, allowing the requests to be processed faster, and therefore decreasing the page load! Rails users can achieve this leveraging Amazon S3s ability to host static websites and then point their DNS toward the bucket. We use CloudFlare for this as we then get the added bonus that our assets get cached by their CDN. We then use Asset Sync , which precompiles our static assets to one bucket, and Paperclip, which transfers all our uploaded content to a separate bucket. Critical CSS Second to images, the CSS for highly stylised eCommerce websites can be the biggest sink hole for page load times, so separating your styling into critical rendering path and non-critical styling is essential. So what should go into your critical CSS? The short answer is anything that is required to make the above the fold content look premium. After all it is the first thing a visitor sees on your website. A basis critical CSS file could just contain your grid structure, header, and hero image styles all of which would be above the fold. The Filament Group have open sourced a great NodeJS package for extracting the critical CSS of a webpage into a single file cunningly called criticalCSS , which is worth checking out. Rails bonus: Turbolinks Turbolinks is not really a Rails bonus as it's just a JavaScript file you can include in any project, but you do get an awful lot more from it in a Rails app, especially a Rails 5 app. Essentially it speeds up your site by AJAXing your internal site links and only replacing the body, meaning your visitor (and their browser) only has to download your static assets (JavaScript and CSS) once. It does some other tricks too like changing the pushState, and page titles. If you are working on a Ruby project, installing Turbolinks is easy: just bundle install the Ruby gem and require Turbolinks in your application. However, if you aren’t coding in Ruby, Turbolinks has been ported over to many other languages/frameworks, and a quick search on libraries.io will hopefully turn up the results you are after. Failing that you can grab the CoffeeScript from GitHub and include it in your project. Once you have implemented some of these suggestions, you’ll see the positive effects in page load time, which can be measured from within Google Webmaster page speed tools, or using services like webpagetest . Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-11-11"},
{"website": "Made-Tech", "title": "5 Software Anti Patterns I Love To Hate", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/5-software-anti-patterns-i-love-to-hate/", "abstract": "Software anti-patterns are a well covered topic, but I thought I would highlight some of the ones I've encountered most frequently. These may seem obvious and at times innocent looking but make no mistake, they are sinister and will sabotage your efforts to add features to a codebase. I've gone ahead and made up my own names for some of the more specific anti-patterns. Accidental complexity I would consider this a blanket anti-pattern which hosts an array of other anti-patterns. What's special about accidental complexity is its ability to grow exponentially. One bad decision early on could trigger a bunch of (incorrect) defensive decisions later on. New anti-patterns start surfacing like 'Reinventing the square wheel'. A popular way for this to happen is when developers make the decision to solve a certain problem using their own code, as opposed to using tried and tested solutions that already exist, i.e. \"Let's create our own CMS!\" You will be too focussed on trying to fix your code to consider the bigger picture. Dependency hell We've all been there, when you try and add a specific version of a gem to your project, run bundle install and are then met with a wall of red text complaining about versions being incompatible. Also, when dealing with external systems (which has become ever more popular as systems become decentralised), they're liable to get into a tug of war over what they consider correctness of data. Your app sits in the middle and has to keep both happy. Accidental complexity lies in wait, ready to complicate your code. An example of this that I ran into recently is where I had Salesforce and Carmen expecting different names for the same country. Salesforce would only accept 'Korea', and Carmen would only accept 'Korea, Republic of'. It is not hard to solve without inheriting Accidental Complexity. One way to solve this is to create an intermediate representation of the data housed in your application (good luck to the next developer to inherit the codebase in figuring out what exactly is going on!). Catch me if you can Have you ever looked at a codebase to try and find the place to make your change, only to be thrown down a never ending rabbit hole of abstractions? I was unable to find an existing anti-pattern that describes this situation, so I just named it myself. It also falls under the all encompassing Accidental Complexity anti-pattern, like most others do. I am a huge fan of abstractions and small methods, but have hunted through some frameworks where the search just never seems to stop. This can be a result of premature optimisation, or just making things too general before it is needed. It may have been designed to deal with a huge amount of complexity, but will never be needed. Without some sort of IDE functionality to navigate between the files you are left stranded. Blind faith Friday afternoon, and everything is going great. You push your commit to production, check New Relic, and all hell breaks loose. Your fix has broken a bunch of other code, and your customer's shop is now offline. All you can hear is money falling on the floor. It takes 30 minutes to run your test suite locally. That is a lot of time for the website to be down. You cannot roll back using your awesome blue/green deployment system because your commit has changed the database schema (doh!). You frantically revert your commit, and push it back into the master branch. Jenkins starts building your project and your test suite FAILS. You have introduced more bugs than there used to be in the first place. You are not moving forward, you are moving backwards at a rapid pace. The only advice I have in this situation is to remain calm. Your ability to solve the problem only decreases when you start getting stressed. Formulate a plan before just trying to solve the problem on the spot. Abstractions of deceit Also not a formal anti-pattern but definitely a descendant of Accidental Complexity. There is a huge difference between reading: convert_price and convert_base_price_to_requested_currency It is good to introduce code seams as a way to add new functionality if your abstraction is not accurately named, it can be worse than having no abstraction at all. On the other side of this is the \"names that are too long and verbose\" anti-pattern which I consider way less harmful this anti-pattern. It may hinder your ability to scan the codebase quickly, but at least once you read it, you will have a clear understanding of what it aims to achieve. Conclusion Naming is hard but it is good to label things so that they can be easily discussed. Be on the lookout for anti-patterns that you may come across on your travels, and flag them by name as soon as you recognise them. It also facilitates discussions when you can reference something by name, as opposed to explaining what you mean every time. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-11-05"},
{"website": "Made-Tech", "title": "Analysis Paralysis – Stop slowing down your enterprise!", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/analysis-paralysis-stop-slowing-down-your-enterprise/", "abstract": "So you are trying to build a fast moving digital organisation, one that responds to market opportunities, is decisive and can get things delivered. One of the challenges you may encounter is a thing we call Analysis Paralysis. It's a problem that is prevalent in large enterprises and governments, where lack of trust, office politics, and siloing can cause over-analysis, leading to indecision, resulting in things grinding to a halt. We see the most significant Analysis Paralysis occurring in the time that elapses between somebody trying to boot up a new initiative (Hey, we should do X!) and putting the proverbial shovel in the sand and making a start. People working within organisations that face this problem often come up with reasons to justify the lack of progress e.g., \"We need to get Enterprise Architects involved, and they are too busy at the moment\", \"The business people haven't made a decision yet\", \"We've got a meeting in four weeks to discuss it further. That's the earliest all stakeholders could do\" etc. These sort of excuses don't sit well with us, as we believe any organisation should be able to go from having a business need to having something in production within a few weeks. So why does this problem exist in nearly every big organisation we have seen? Why can some facilitate progress, while others cannot? We believe it comes down to a few things: Strategy: The team over at GDS coined the phrase \"The Strategy Is Delivery\", and we like this. We see many enterprises investing vast amounts of time into upfront strategy and extensive analysis before any actual work can begin. Although there is clearly value in doing some upfront thinking, much of the time organisations invest into strategy could be better spent on delivery and getting something to market quicker. UX / UI: We still come across organisations who invest significant time into BDUF (Big Design Up Front) at the UX layer. Iterating through rounds-and-rounds of wireframes and then rounds-and-rounds of UI design before moving into Templating (HTML/CSS/JS etc.). Although this can give the illusion of progress, without shipping (e.g. launching into a production environment), they are creating artefacts and not delivering actual value. It's just another form of analysis paralysis. Chunking: We often see decision makers that lack enough technical knowledge to chunk work sensibly. Booting up large programmes which invariably require much closer scrutiny due to increased expenditure and risk. Architecture: Another area where we see analysis paralysis taking place is in system architecture. Much time can be spent working out the perfect system design, from overthinking API structures to planning all-encompassing Common Platforms and general Gold Plating. Lean Inception One of the techniques we've used to help businesses get through Analysis Paralysis is Lean Inception. It's an approach which is useful when there is uncertainty within the organisation about how to move forward with a particular topic. Our inception sessions will nearly always happen face-to-face and include the cross-functional delivery team. They will be chunked correctly, so something small enough to be delivered in a reasonable period of time, but significant enough to provide business value. The session will typically be structured something like this: 1. Problem Statement: Person driving the project and team agree on the problem that you're trying to solve. 2. Actors: Agree the actors involved and gain a high-level understanding of their needs. Ideally, have somebody present who represents each actor. 3. Process Map: Draw out the existing process that you are trying to improve. Doing this will provide clarity to everyone in the room. 4. Problem Areas: Highlight the problem areas and prioritise these by business impact. 5. Real Actors: Now go and sit with the actors in the system, to see the problems you're solving first hand. 6. Revisit Process Map: We often find the sitting with real actors changes the process. Up until this point, we'll have focused on the problem. We'll now turn our attention to potential solutions. 7. MVP Definition: As a team, agree on a small slice that can be delivered quickly and can be used to test assumptions with the actors. Depending on the problem you're solving, this could be a spike/prototype or a real thing that an actor starts using daily. 8. Delivery: Spend a few days (or maximum a few weeks!) working on delivering the first slice. Get it into the hands of the actor to get some feedback as soon as possible. You should have a cross-functional team, so delivering things that are UX'd, built, tested and in a production like environment. 9. Capture Feedback & Iterate: Keep refining your MVP definition and learning more about the solution as you showcase to the actors.Within a few weeks, you should have learnt lots and know whether the path you're taking is going to meet the business needs. You may decide to continue investing time into the solution, move onto something else or even shut the delivery down. Whichever route is taken, you'll have minimised the investment, developed learnings and avoided a lot of the upfront analysis which can paralyse many organisations. Give this lightweight approach a go and let us know how you get on! Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-05-31"},
{"website": "Made-Tech", "title": "React and MVC", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/react-and-mvc/", "abstract": "React is improving the way we build frontends, but I find common patterns are making our apps more complex to write and manage, and more difficult to understand. I believe this is complicated by the capabilities you give a React component. React components need data to render, like any other view component, and rendering a React component with some data is a simple, synchronous process. This is easy to do on the server: However, a common practice is to fetch this initial data in the component itself, before the render, making the rendering process asynchronous and introducing external API dependencies. For example: This HomePage component is not only aware of how to render data, but also aware of how to fetch the data itself. It's dependent on a HTTP API on the same domain, so in order for this component to be rendered, this API has to either be available or mocked out, making testing it more difficult. Traditionally, rendering a view is a synchronous process, the view is given all the data it needs, then is rendered to HTML. This asynchronous approach means defining API endpoints to return data for individual pages, for example an endpoint to return order data for an OrderShowPage component. This is reasonable enough, but it's often tempting to reuse multiple existing API endpoints to retrieve data for a page. For example, on a basket page you might make an API call to get data on the current order, and a second to get data on related products. Additionally, all the pages in your application may depend on common data. For example, if you display information on the current logged in user in your navigation bar, you need to fetch this on every page render, or fetch it conditionally depending on whether it's already been fetched, and cache on the clientside. This increases overhead for a developer, and means page rendering becomes dependent not only on a HTTP API, but on a local cache store. Alternatively, common data could be included in every API response, but that leads to non-semantic REST usage. React is a view layer, it shouldn't force a particular architecture on the rest of your application. By organising a React app this way, you have to build much of your backend as an API. It's a shame to lose the benefits of patterns like MVC because you want to use React on the frontend. To use an MVC metaphor, I've found complex React page components end up acting as both markup and page controllers. This is a confusing mix and doesn't scale well. The more requests you need to put in the page, the more your page grows, and the slower it renders. You effectively end up reimplementing MVC within the view layer, but with significant overhead. In MVC frameworks that use HTML, for example Phoenix or Rails, you define routes to map requests to controller actions, where you retrieve all the data needed to render the view. Then you render the HTML using that data and a templating language. It's a simple, synchronous process. Data is retrieved, passed into the view, and HTML is returned. On the server, this all happens in a single request, and the developer doesn't have to make many performance considerations based on where the data is coming from. We should be able to build React apps that render the same way. We should be able to drop React in as a replacement renderer in any controller, in any language, rather than having to build Node APIs for everything. MVC In recent experiments with this approach, we've been using 'polymorphic' routes which can serve HTML or JSON conditionally. The idea is, when the first page request is made we want to respond with HTML and the JavaScript necessary to take over rendering in the browser. On subsequent requests to the same URLs the client requests JSON instead by passing an Accept header. An example controller action in Node might look like this: On an initial page load, so when the Accept header is text/html, res.component.render would use server side react rendering to render the page as HTML, including the JS client bundle. When the Accept header is application/json, it would respond with a JSON payload containing all the data the client needs to render. The res.component.render function might look something like this: An example JSON response would look like this: Notice that the single response body includes the data required for the page, no more, no less. This results in a good performance boost over making more than one API call and taking what you need. The page value is the name of a component which was bundled into the client JS application. We define a components index file which exports these, making it simple to match the component name in the JSON payload to the component itself. All the client has to do is render that component with the given props. We use Redux state to store what component is rendered at the moment, injecting the initialState on the initial HTML render using a JavaScript snippet in the HTML, and then updating it with Redux reducers on page transitions and history events. Other languages Now that we're able to drop React in as an efficient page renderer, separated from the API request paradigm, we'd ideally not have it dependant on a Node.js backend either. Given React is a JavaScript library, server side rendering is only available in Node.js at the moment, but there's no reason rendering has to happen in the same process as the rest of your backend logic. On some recent and ongoing web applications, we've been running small Node API web servers which expose a single endpoint, something like POST /render, which will respond with HTML or JSON conditionally as discussed above. We run this on the same server as our primary backend, whether that's Elixir or Rails, or anything else, and post page data to it in a format identical to the page response JSON above. The node apps look something like this: By splitting rendering out into a separate program, we're able to use it like any other renderer in any other language. For example, we've built custom renderers in both Phoenix and Rails which call the node app over HTTP. For example, you can write Rails controller actions like this: Which would POST data like the following to the node app: The node app then responds with HTML or JSON, which the Rails app can render as normal. This way we're able to get the benefits of any language or framework with the benefits of a full featured React frontend. You can simulate framework view features by passing certain props through to the node app by default. For example, on a recent project, we passed flash data and a CSRF token to each render call by merging them in as default props, so every page level component had access to this data. The biggest downside is React rendering isn't as fast as HTML rendering yet, but it's not that slow either. When to do this Smaller applications won't see most of the benefits of a bespoke approach like this. If you're making a website that's a couple of pages, you'll get the most bang for your buck with an existing framework like Next.js consuming an API. I think React becomes a more appealing proposition for larger applications when it can be more comfortably dropped into existing stacks. Having to re-architect everything with brand new tools and brand new tech, solely to get the benefits of a frontend library, is often too high risk to be reasonable. By introducing this abstraction, I think you find a pretty clean separation between your frontend and backend, while remaining pragmatic about their relationship. Your backend is aware of what it's rendering, and where to get the data. Your frontend is aware of where the backend is, and what data it'll give. We've used this to success on some customer websites, and are keen to see where we can take it in future. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-06-14"},
{"website": "Made-Tech", "title": "Doing dumb things with Postgres", "author": [" Ryan MacGillivray"], "link": "https://www.madetech.com/blog/doing-dumb-things-with-postgres/", "abstract": "A couple of months ago I gave a talk about PostgreSQL, and specifically using the Array datatype, at London Ruby User Group and I wanted to take some time to dive into this in more detail. I've been fascinated with the Array datatype from Postgres and have been using it in Rails since it became usable in 2014. At the time I worked for a company that used Postgres for all of the new projects and several came up where it fitted in nicely with the work we were doing. Based on this I decided to give it a go and see what benefits we could gain and also try to work out any major pitfalls. The array datatype works exactly as expected, instead of storing a string or integer it instead allows you to save arrays of these types. We used these for all sorts of things, from storing article tags and CSS classes to storing multiple associations without the use of a joining table. The latter usage is the area I became most involved and interested in. Given the product we worked on would be mainly used in a read heavy manner, the additional overhead of joining tables could occassionally cause bottlenecks, especially on more complicated projects. By leveraging arrays, Rails caching and some code that I've come to regret, we were able to mitigate these. Example: You have a series of news articles that can have associated tags that we use to find other articles, normally you would do this by implementing this: Articles Table -> Taggings joining table -> Tags Table Using the array datatype you could instead have: Articles Table Doing this provides us with a couple of advantages: with caching this can be noticeably faster than joining the 3 separate tables and provides us a clean data-structure that we don't have to manipulate or pluck from to get the thing we care about, in this case the tags. It also allows us to easily execute some very basic SQL to find articles that have the same tags as the current one being shown to the site user. If you've been using Rails/Postgres over the past couple of years this might be very run of the mill so lets dive into some of the madness you can cause. Example: A customer already has a series of one to one relations like Products to Product Categories, but they now want this to be a many to many relation. They've already input lots of data and if possible they want to keep this. The obvious solution would be to create a joining table, iterate over the existing products and create entries for each of the associations and then drop the columns. I instead decided to do something a bit more out there. What if we could convert this integer column over to an array column while keeping all the existing data and not have to iterate over everything that's already there? To do this you'll need a really weird migration like this: There's a lot going on in this migration so I'll break it down a bit. This line takes our existing foreign key column and converts it to an Array type, the SQL being executed recasts the existing data in the correct format so 4 would become {4}. The curly braces are how Postgres denotes an array, when returned by ActiveRecord the data would be represented as a standard array. This is clearly an index, the important part however is that it's using \"GIN\" (full-text indexing) as its index type. GIN is lossless versus GiST, the other index type available, which is lossy, introducing the chance for incorrect results slightly. In practice it probably won't make a huge amount of difference to you which one you use but if you want to read more on these types I'd recommend the Postgres documentation . One thing to watch out for is that GIN indexes can have long build times but there are ways to mitigate that. It's interesting to note that by choosing not to sort this array and restore it would allow the potential of using SELECT unnest(arr[1:array_length(arr, 1)][1]) as id from data or similar to get the first item to create a reversible migration. This is only advisable as a thought exercise and I'd recommend you never do that. The first snag we'll hit here is that we no longer have support for any of the Rails niceties around associations like has_many. You can solve this by using code similar to the example above: This will return all the Product Categories that have been saved in the Products product_category_ids column in the same way as has_many would. One of the biggest issues I have with using arrays like this is the amount of boiler plate code you end up writing due to losing the ActiveRecord association behaviours, but like I said sometimes the speed benefits on a read heavy site outweigh this. If you use the collection based form helpers you'll also have to flatten or discard empty arrays on a save since they send an empty array item each time. There's also the concern that when this is shown to the end user it can introduce a lot of delay to loading if there are lots of associations. The main way I've found around this was to update a cache in Redis or similar every time a Product is updated. You can also make this even less blocking if you pass it off to a queuing system rather than blocking the update/create actions. I'd also like to throw out an honourable mention to the PostgresExt gem for providing native support for querying the array datatype (and others) natively via Arel. Overall I think there are some amazing things you can do with PostgreSQL's array datatype in association with Rails but it's a very sharp and dangerous knife at times and truly requires you to be aware of what you're trying to do at all times. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-06-08"},
{"website": "Made-Tech", "title": "Vim Considered Harmful", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/vim-considered-harmful/", "abstract": "Oh no, not another Vim article! No I don't use Emacs and no, I don't hate Vim. This title is purposefully crafted to invoke some sort of emotion in order to convey what I think is an important message. I've been using Vim for around 7 years, and I still use it now. It's true, whether you like it or not, Vim can be harmful in certain situations. There's no denying that Vim is a power tool, and as with any power tool, it commands respect. You can use a chainsaw to cut down a tree but if the only thing you are used to is a manual saw, you will probably try cut down the tree without activating the blade. While you may feel like you are being productive, the fact is that you are wasting your time. I weirdly fell in love with the process of creating code When I was studying, some of my first programming was done in Actionscript on Windows PCs. My teammate was fast when it came to typing, and modifying the code. He used Alt-Tab to switch between programs and I watched him in awe. He had no need to reach for the mouse. I tried to emulate him, and I would switch between programs quickly using Alt-Tab, to the point where I started developing RSI. I enjoyed the process creating the code, and noticed that everyone has a unique style. I became obsessed with the process and lost focus on what actually matters. Getting things done. I tried to convert everyone in my company to use Vim The first time I heard of Vim, I couldn't understand what all the hype was about. As with so many things, after you hear about it for the first time, it seems to just pop up everywhere. I heard that only the best programmers could use Vim and that you could be so much faster in it, ultimately making you a better programmer. This felt like a natural progression from my Alt-Tab shortcuts, and I could have it all. My mind was made up. I decided to convert to Vim from GEdit. I would not shut up about it. I was very annoying. I was slow My obsession with a customised development environment grew to the point where I was typing on a loud mechanical keyboard, in Colemak, on Arch Linux with Xmonad and no real graphical interface. To interact with the database, I would do it in a Vim buffer, obviously. Half the applications on my computer didn't work correctly and I refused to use anything that wasn't on the command line. My obsession with tools and tinkering had spun out of control. I was becoming frustrated. Not to say that this development environment was wrong, but I didn't put in enough work to be proficient in it. Which is what this all boils down to. How many balls can you really juggle at once? If you haven't seen Rich Hickey's talk titled Simple Made Easy , I would highly recommend that you watch it. In it he talks about the mental capacity that programmers have to solve complex problems. He compares it to juggling balls, where even the best juggler can only handle a certain amount before it becomes impossible. With poorly designed programs, you might end up with a few balls unrelated to the problem that you are trying to solve. You end up having to keep unimportant things in your head, robbing you of your capacity to think clearly about the actual problem you are trying to solve. Vim could end up throwing even more balls into the mix. Many more. With a maximum of 8 mental juggling balls available, Vim could end up using 5, 6 or 7 of them. What's worse is that you may be oblivious to the fact that this is even happening. I've seen new programmers excited about using Vim, so enthusiastic, only to forget what method signature they were updating because their entire thought process suddenly became dedicated to figuring how to modify the text up until the 3rd t character. You have successfully robbed yourself of your ability to think. Typing isn’t the bottleneck Typing isn't the bottleneck but you surely have the option to make it the bottleneck. Having to think about what and how to type will drain your energy. It is counterproductive. EVERYTHING I do in Vim now is in my muscle memory. If I learn something new, usually by practicing with Vimgolf, I make a point of understanding and committing this new key combination to muscle memory. Once it's in your fingers, there is no need to actively think about it. Pragmatism is stylish I hate the word pragmatism. I've heard it thrown around so many times and truly it's quite soggy. However, when I see someone approach a problem with their entire cognitive ability dedicated to it with no distractions, I feel that same feeling that I used to feel when I saw people type fast or use a lot of shortcuts. Given a problem, the way you approach a spike to get a result as quickly as possible can save you a lot of time moving forward. You have become a power tool. What now? Unfortunately Vim is not a text editor that you can have a part time relationship with. It's all or nothing. If you believe that using the arrow keys or mouse is wrong and this adds mental overhead, then what's the point? Should you stop using Vim? Should you surrender and follow the path of least resistance? Most certainly not. That's not the point I'm trying to make here. When adopting this text editor, you need to accept the fact that you are required to put in your 10,000 hours. Figuratively speaking of course. If it's not in your muscle memory, it's not worth using it. Practice using Vimgolf , watch Vimcasts and read the built in documentation. There is a fine line between Vim being your ally or your enemy, silently draining your energy and making you worse off than if you were using a simpler text editor. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-06-29"},
{"website": "Made-Tech", "title": "Cost of delay", "author": [" Adrian Clay"], "link": "https://www.madetech.com/blog/cost-of-delay/", "abstract": "So, you’ve decided that responding to change over following a plan is going to be one of your guiding values. You look at your plans and think “Gosh a lot of decisions went into creating this roadmap, I don’t want to be changing these without good reason”. Then you start getting conflicted over what good reason is, “How can I be confident that rearranging the roadmap is for the best?”. Before you get too worked up, I suggest pouring a glass of wine and familiarising yourself with a technique known as cost of delay modelling. Definition Cost of delay is a modelling technique that examines how the accumulated cost of not completing something changes over time. Examples Super mega urgent tasks If you host an ecommerce website with lots of traffic, the simplest cost of delay model would be the total lost sales over the time the site is down. Improvement tasks If your ecommerce website isn’t compatible with the latest bit of technology sweeping the nation, you could estimate the cost of lost sales. External deadline tasks Black Friday is coming, and you estimate that you could generate an extra £5K worth of sales on that day if you were able to send an email campaign out beforehand. Roadmap So, let’s imagine a typical roadmap with 3 improvements. You’ve just hired a team for 10 days which has capacity to work on 1 improvement at a time and you want to minimise the overall cost of delay. Improvement A you estimate to increase profits by £100/day taking 7 days Improvement B you estimate to reduce operating costs by £15/day taking 1 day Improvement C you estimate to generate a one time £1000 windfall taking 2 days For each improvement we will model the accumulated cost of delay as the number of days without the improvement multiplied by its cost per day. Given there is no cost associated with delaying Improvement C, we’ll model its cost per day as £0. The accumulated cost of delay for C will be £0 no matter when it happens, but scheduling it before other improvements will impact the completion date of them. To maximise the overall accumulated cost of delay we should leave it til last. So let’s calculate the cost of delay when scheduling the improvements in the order they appear above AB. For A starting on the 1st day, the cost of delay is 7 days * £100 = £700 For B starting on the 8th day, the cost of delay is 8 days * £15 = £120 A grand sum of £820 Let’s calculate the cost of delay when scheduling the improvements BA. For B starting on the 1st day, the cost of delay is 1 day * £15 = £15 For A starting on the 2nd day, the cost of delay is 8 days * £100 = £800 A grand sum of £815 For this small example, we can see that the difference between scheduling the two ends up being small. When factoring in the original estimation error, we conclude there is no strong evidence that starting A first or B first is more cost effective. We might decide to schedule B first to allow more time to do research for improvement A. Conclusion If you are able to place estimates on the value and duration of a piece of work, cost of delay can be a useful tool which gives quantitative backing to prioritisation decisions. From the example above you should be able to formulate an argument for why one schedule is more expensive than another. If you are interested in finding the most optimal schedule given a set of tasks, I suggest researching “cost of delay divided by duration”. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-07-06"},
{"website": "Made-Tech", "title": "Homeless Hackday London", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/homeless-hackday-london/", "abstract": "At Made Tech, we run an internal Hack Day or Learning Day every month. We take time off our paid work to focus on learning something new, like a new programming language, or to hack together some tools that solve a problem we're encountering. Some of our recent hack days have covered: Learning Kotlin Uncle Bob's Clean Architecture Creating a Smart Office Hardening Security Making Video Games Creating Dashboards / Information Radiators Historically these days have been internally facing, with someone pitching an idea and just our team in attendance. This month we decided to try an alternate format, and used the time allocated to attend a public hack day called Homeless Hack . I thought I'd share some of the thinking behind this and take you through what we created at the hack. Hopefully this will inspire some other organisations to follow suit and use hack days for good! **Default to open** We have always been an open company and publishing our company handbook , frequently blogging , mentoring new programmers or writing books were steps towards being even more open. It felt at odds with our culture to run hack days that were closed events. Homeless Hack was a good opportunity to rectify this. **Be a socially responsible SME** Historically we've never given much thought to things like Corporate Social Responsibility (CSR). We're an SME, we've got fewer than 30 people working for us and we like to think we run the business ethically and fairly. It's always felt like the world of CSR didn't really apply to us. e.g. it's for the big enterprises, who do bad things and need to offset the bad things by doing some good things. This was a good opportunity to course correct some of our thinking around this and invest some time and money into something that would do some social good. **What are you doing on Saturday?** We invited everyone in the company to attend HomelessHack, though only 7 people were ultimately able to attend. The event fell on a Friday and Saturday and many people couldn't give up a day of their weekend to attend. Interestingly, we had quite a few people who originally said they would attend, but as the event got closer, had to drop out. From a cost perspective, it will have cost us around £6,000 in lost opportunity costs (as our billings on Friday will be down) and approximately 50% of this in direct costs. So it's a fairly significant investment for an SME, but one which we think is justifiable and worthwhile. Friday – Learn We kicked off Homeless Hack on Friday morning with an introduction followed by talks by Burcu Borysik, Gaia Marcus, Sam Thomas & Samantha Dorney-Smith. These talks really helped to set the scene and get a deeper understanding of the problem and potential areas for focus. Following the talks, a load of people presented ideas for discussion in the afternoon. We pitched a couple of things: the first was to look at Property Guardianship and whether there was a potential source of additional housing and what policy / social / changes would be needed. The second idea was around data and sources of insight which could be used to drive policy change. Following afternoon discussions, we had a short session to aggregate ideas and then split into teams to work on particular problems. There were around 5 teams in total and we were part of the Shared Accommodation Rate and the Volunteer Matcher teams. The objective for the SAR team was to: Use real pricing data to create a campaign to highlight the difference between LHA and real market rates We collectively agreed on what we wanted to try and showcase on Saturday and then broke the work down into a number of independent chunks. We collaborated mostly through verbal communication, but also had a Trello board and Slack room for bits too. We must have spent about 5 hours on Friday afternoon / evening working on the problem before calling it a night at approximately 9pm. Saturday – We’re Live We reconvened on Saturday morning at 10am and continued working on the problem. We knew the showcase was going to be at 4pm, so we didn't have a huge amount of time to get everything working. We managed to pull it all together approximately 1 hour before the showcase, so we could prepare a short presentation . The source code is here: https://github.com/homelesshack/sar , though it's obviously very rough code (No TDD'ing, linting, optimisation etc!) We've also shared what we've produced with the team at Centrepoint & Crisis , so they can take this forward and use it for their policy work. What’s Next It looks like Homeless Hack is gaining momentum. I've seen Homeless Hack events organised in Leeds, Manchester and Oxford. It's great to see the good work being picked up by other cities. As a company, we'll definitely be attending more public hack days. We're already talking about the next one, potentially an NHS Hack Day , and we're hoping to get a larger turnout from Made Tech next time! Finally, a huge thank you to Janet Hughes , James Cattell , Gaia Marcus & Richard Barton . They organised the London event and made it a fantastic success. These sorts of events wouldn't happen without their hard work, so top marks to them all. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-08-18"},
{"website": "Made-Tech", "title": "Modern APIs are Pushing Innovation in Retail", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/modern-apis-are-pushing-innovation-in-retail/", "abstract": "An increasing number of retailers are exposing core parts of their business through APIs, delivering a more cohesive customer experience across a variety of touchpoints, making it easier to streamline internal operations, and opening opportunities to interface with external channels. What is an API? First off, it's worth recapping what we mean by API, or Application Programming Interface, to use its full name. An API allows systems to easily talk to one another. APIs can allow an e-commerce store to send an order to the warehouse for fulfilment, or an EPOS system to understand what stock is available in the store. More modern applications are often API ready out of the box, but legacy applications can sometimes cause sticking points. Legacy Applications and Proprietary Protocols Historically, applications connected via proprietary protocols. Different vendors had their own standards, and connecting different systems was often a laborious exercise that involved licensing more proprietary software. Worse is that some software systems are \"walled gardens\", where they, by design or through lack of investment, won't interoperate with other systems. This isolates their data and functionality from the rest of the organisation. In today's world, customers are demanding a cohesive experience that this isolationist approach can hinder. Customers Increasingly Expect a More Connected Experience Customers are interacting with retailers through an increasing array of touchpoints, from web, to mobile, and in-store, perhaps even third party marketplaces. An organisation that has adopted an API ready approach will find integrating new channels a much easier experience. If the e-commerce platform already exposes an API for accepting payments, half the job would already be done in allowing a new mobile application the same functionality. Likewise, if the warehouse system already provides an API for stock levels, showing live stock data on the e-commerce storefront is largely solved. Centralising customer data into a CRM system that can be accessed via APIs allows a customer to be more easily recognised across the retail touch points and can enable the shopping experience to be tailored to them. A salesperson who can see a customer's recent browsing habits and online purchases is much better able to make recommendations. Likewise, if the data feeds both ways, customers can be retargeted with relevant advertising online based on their offline behaviour. Many retailers already have significant maturity in providing APIs for core parts of their operation and are realising the benefits. Existing API-enabled Retailers As noted, we observe many organisations, particularly lead by those with a strong online presence, adopting an API enabled strategy. Perhaps the most notable moves Jeff Bezos made at Amazon was to mandate that every service must expose an API (and you'll be fired if you don't!). That mandate was responsible for sewing the seeds of Amazon Web Services, now itself a $12 billion a year business. On a smaller scale, startups are entering many industries and adopting or building modern, API enabled software to do so. Payment processor Stripe has significantly lowered the barrier to entry in processing online payments, in part by how easy they've made it to integrate with your application. Algolia offers a search platform bettering many existing enterprise offerings, enabling developers to be up and running within an hour. Offering APIs can open opportunities outside of the organisation too. Retail APIs and Third Party Services Integrating with third party services easily can be a compelling benefit. Enabling commerce anywhere (think \"shop the look\" on Instagram), or exposing product catalogues to price comparison or aggregation services without the need for a standalone channel manager to power an integration. Opportunities can be opened to work with partner organisations with complementary offerings, or to encourage third party developers to interface with your systems. Reportedly 60% of eBay's revenue is driven by API calls, and for Expedia the number is said to be as high as 90%. Adoption Challenges in Existing Environments Established environments always offer up additional challenges in adopting modern technologies. The need to continue to run Business as Usual, and a legacy of platforms from various technology eras, can put existing players at a disadvantage compared to newer players in their ability to innovate quickly. Technology purchasing decisions in the past have often found comfort in procuring large \"all in one\" solutions that try to provide functionality for many areas of the business wrapped up in one oversize shrink-wrapped box. Too many of us have observed the multi-year SAP rollout that is already considered legacy by the time it launches. One of the biggest opportunities for established retailers comes when looking to replace all or part of an existing system. Opportunities When Purchasing New Technology When purchasing or commissioning the build of new technology platforms, future integrations should be a primary consideration. Ensuring platforms have existing capability, or can be easily extended to provide the capability to expose their data and functionality via an easy to consume API can offer a strong commercial advantage. As the technology landscape continues to quickly evolve, there's benefit in having the ability to easily integrate with and leverage as yet unknown platforms. Summary Many retailers have been actively pursuing an API enabled strategy for some time, aware of the benefits it offers, both in connecting systems within the business, and in opening opportunities to connect to external systems. There are often additional challenges when introducing an API strategy into an established retail environment, as legacy systems, and Business as Usual needs can slow down any new technology adoption. The commercial benefits, both in operational efficiency and in opening new commercial opportunities should at least provide encouragement to explore this strategy. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-08-07"},
{"website": "Made-Tech", "title": "James Stewart Joins Made Tech Advisory Board", "author": [" James Stewart"], "link": "https://www.madetech.com/blog/james-stewart-gds-joins-advisory-board/", "abstract": "Every organisation considering digital transformation (or even just a new technology project) worries about access to skills. Bringing change needs understanding of new tools and technologies. They can be learned, if you have the time. Significant change usually also requires new working practices, new ways of organising teams and new relationships with your users and the rest of your organisation. You'll get moving faster if you can draw on hands-on experience of different ways of working, and bring to bear perspectives not constrained by organisational silos and ingrained practices. Bootstrapping skills I've spent the last six years working in government, helping to found the Government Digital Service and embed a more digital way of thinking across the civil service. During that time I developed huge respect for many of the long term civil servants who brought deep domain knowledge and a commitment to public service, but was also regularly confronted with structures and practices not set up for digital ways of working. Changes to those structures and practices always start with people experiencing a different way. It's encouraging to see people quickly picking up new skills and adapting to new ways of working. We saw dramatic results when we were able to bring together blended, inquisitive teams: existing staff who understood the organisation, alongside experienced agile practitioners who would challenge the status quo. Everyone has their own way of learning, but everyone needs to try out new approaches in a supportive environment delivering real value. That's one of the reasons it's so important to find good partners who will come in, understand what you value, challenge your preconceptions and deliver alongside you. Changing the environment Effective ownership of products and services requires teams that are in it for the long-haul. However much training and collaboration you do, you’re very likely to also need to hire in some staff who can bring some deep specialisms. For organisations who have relied on off the shelf products or outsourcing approaches to technology, that usually means hiring technical architects, operations engineers and developers (and commonly this is also the time when user research, delivery management, design and other skills are also introduced). The new styles of teams that result thrive in highly collaborative environments. There’s a lot of communication. Teams need physical space and digital tools that can facilitate that and which they can make their own. They probably need to be able to install and trial open source or free software, and probably won’t be able to work with the heavily locked-down IT common in many corporate environments. Starting to shift your environment to be more accommodating takes time and needs to be done iteratively as you work out what needs to change. It should be based on specific user needs and challenges, and shouldn’t be done speculatively. As you grow your own team it’s their needs you should focus on, but a good partner that’s experienced a variety of environments can help get you started on the journey. Deepening the supply chain During my time in government we were fortunate to be able to draw on some fantastic freelancers and a handful of forward-thinking suppliers, but it was always clear that there weren’t enough to go round. Now that I’ve left the civil service I’m really keen to continue to do what I can to address that: supporting work that builds up the skills that are needed for new services and deepening the supply chain so there are more options of where to find them. That’s part of why I’m pleased to be working with Made Tech. Their commitment to partnering with and strengthening their clients is exactly what so many organisations need, and even in the few months I’ve been working with them I’ve seen them learning and adapting as they find new ways to help a wider range of clients. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-04-01"},
{"website": "Made-Tech", "title": "Polyglot disciplines", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/polyglot-disciplines/", "abstract": "As a polyglot software engineer, I have discovered some disciplines which I have found beneficial to me. My hope is that readers of this article will find the experience I share here profitable to their endeavours as polyglots (or indeed as monoglots). Problems One of the biggest problems with switching between programming languages is the context switching. Remembering what operator is used to perform a particular language essential function e.g. ; and . for statement termination or . or -> for object member access. The paradigm shift doesn't end with essential operators. A successful polyglot must also have an awareness of some basic tooling within each ecosystem. For example, a test framework, a web framework, how to represent time, how to represent currencies, concurrency gotchas. Syntax Differences in syntax are the first hurdle. As an example, some languages favour parentheses for function calls, some require them only in certain contexts, and others have special operators that allow you to avoid parentheses completely, and others have no support for parentheses at all! These differences in syntax can be hard on the eyes and cause deterioration in mental focus, especially when the language is extremely unfamiliar. Standard Library All languages come with a standard library and very rarely are these even remotely similar from one language to another. Every application needs to make some use of standard library operations. As examples: how do you concatenate strings? How do you open a file? What is the best way to create a JSON string? Learning your way around these core operations is the second thing I find myself learning after picking up the necessary syntax. Paradigm Ruby, Java, Python and C# share more in common with each other than they do with Erlang, Elixir, Haskell and Prolog. There are languages which straddle these paradigm borders, for example, OCaml and ECMAScript. It's important to be well versed in the reasons these languages are different and embrace their strengths, and avoid their weaknesses. A language which predominantly favours writing in an Object-Oriented style should be used as an Object-Oriented language, rather than attempting to go against the grain and using potentially more esoteric language features. Remember; as a polyglot, your code should be understandable by a monoglot programmer that only knows the language you are working in currently. Culture Every language, every stack within a language, and every paradigm approach applied to a stack and language have a sub-culture and associated idiosyncrasies. As a polyglot, it is more important to be idiomatic within a sub-culture, than it is to attempt to harmonise the various quirks between the languages that you are familiar with personally. If you are working with a team on a project which has a particular culture and approach to their code, you should attempt to work within the style that the rest of the team finds comfortable. Only at this point is it wise to begin to influence changes and gain support from the team to adopt those changes. Learning new cultures is by far the trickiest and requires a high degree of humility. My polyglot toolbox Remembering all the differences between languages is hard work. Being a \"specialist\" in a language requires serious dedication, internalising knowledge about every corner of a language requires a lot of practical experience. So how do I remember all of this? The secret is: I don't. Here, I'll tell you how I get by without being a specialist. Familiarise yourself with new language paradigms I have found it is important to learn new languages (but not just any new language, as I will cover). The biggest piece of advice to those looking to become a polyglot is to pick languages that are vastly different to what you currently find comfortable. Working in Ruby? Choose a statically-typed OO language. Know a statically typed OO language? Learn a functional language. Know a functional language? Learn a logical programming language. Know a logical programing language? Learn an old programming language. Know an old programming language? Learn a new programming language. Know a new language? Learn a systems programming language. And so on. The point here is comfortability – it is important to recognise that the difference between C# and Java (or Ruby and Python) is not significant enough to provide paradigm-changing learnings. Pick languages far outside your comfortable norm. Find a mentor The importance of advice from someone who has worked with a language, in a team, has absolutely no comparison. There are tiny details of how \"normal code\" looks within a particular language that you can only pick up through osmosis and direct feedback from a mentor. I have found that meetups e.g. London Code Dojo or London Software Craftsmanship are full of individuals who can provide mentoring over a kata. The subtle flow of knowledge that takes place in less than 2 hours is substantial enough to be worth your time. TDD The discipline with the biggest impact on my polyglot workflow is TDD. When I say TDD, I mean full on, disciplined TDD. TDD is a strict discipline where a programmer is not able to write a single line of production code without writing a failing test. Given the tests must be written, and written first, when TDD'ing a programmer is also only allowed to write the simplest assertion to produce a failing test. When a programmer has a failing test, they are also only allowed to write the simplest production code to make that test pass. What this yields in practice are watertight assertions in your test suite. As an aside; there are a few situations where TDD isn't appropriate e.g. testing CSS, testing UIs, testing FSM's, etc. (which I'll cover in another blog post). However, for the vast majority of code, IMHO, there are zero excuses not to apply a rigorous, disciplined TDD approach. The reason why TDD aids a Polyglot is that the feedback loop between an assumption (some theory about how a language feature works) is quickly backed up by a passing test or proven to be utterly false by a failing test. My life as a polyglot entails small, easy to digest struggles with language features which I treat as little learning opportunities, and those learning opportunities keep value flowing without getting \"stuck\". Through leveraging these feedback loops, I have replaced specialist knowledge in a particular language, with a rigorous scientific approach to writing software. The flipside of this is that I don't need to remember how a language works, I can \"re-learn\" the finer details every single time I run my test suite, and I get a confirmation of my understanding fast. On top of this, I can fake having an in-depth knowledge in a variety of tech stacks while under pressure of a customer engagement. Cultural Paradox TDD is a team skill. You do not get the full benefits of it unless your whole team is regularly practising it. As such, it is also a cultural thing, and some teams do not have the belief in it, or may not practice it in a disciplined way. As a polyglot, when a team does not practice TDD it puts more pressure on me to become increasingly specialised in a particular tech stack. My advice as a polyglot is to introduce TDD as early as possible to teams who lack this disciplined approach. IDEs IDE's are an underestimated source of learning opportunities. For example, IntelliJ gives real-time insights into better, equivalent ways of writing blocks of code in a range of different languages. My IDE also gives me hints on when I can simplify code in certain situations. Is string.equals(\"a\") the simplest or is string == \"a\" going to work? Defer Technology Decisions As an agilist one thing I want to be delivering is a continuous stream of value. One thing that I find to be of high friction is new technologies. The most common decision that I would suggest deferring is the answer to this one question: \"What is the database schema?\". Some Engineers will gasp at this point, but hold back any disbelief in this for a moment. When decisions like what the database schema is and when you architect your code to be truly agnostic of a particular web framework are deferred, something interesting happens, you can worry about what your customer needs without learning anything but a test framework and the language at hand. You can worry about learning how to solve those details later. Not only that, but you'll be more informed to make the right decision about the framework or the database as you'll have more knowledge about the shape of the solution. Maybe a lightweight database is enough (or even flat files!), or maybe Sinatra is sufficient to get the job done. The amount of time I've spent in the past attempting to get to grips with Spring, Rails, or GraphQL or some fancy new impressive stack before I've even begun learning and feeding back with customers is argument enough for me to defer these decisions. It prevents me from becoming distracted with the mission to solve the customer's goal simply. In most business systems, Customers do not need to see a UI to provide feedback and for you to be generating useful learnings about their domain. Considering the flow of data to and from the UI through the behaviours of the system will produce a multitude of questions for your customer and domain experts. Obviously, a UI is inevitable, and not something you should hold off forever, but you can throw that on top as icing on the cake after the first iteration. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-08-16"},
{"website": "Made-Tech", "title": "Introducing: The Made Tech Academy", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/introducing-the-made-tech-academy/", "abstract": "We're excited to announce the launch of a brand new initiative for our company: The Made Tech Academy. What is the Academy? The Academy is a yearly intake of fresh-faced software engineers looking to break into the industry, from university graduates through to self-taught individuals, and from all walks of life. Running for 12 weeks each summer, Academy members will be tutored by one of the Made Tech team in what we believe are the skills needed to deliver great software, such as Test Driven Development, Agile and Continuous Delivery (you can get a head start on this with our book ). Whilst there's a fair amount of focussed learning during the 12 week period, Academy members will also have the opportunity to join project teams and gain experience working with customers. All positions in the Academy are paid, and at the end of the 12 week period members will join one of our delivery teams. Meet our first students Joining the Academy in its first year are Tasmin Steer and Dan Burnley. Having been with us for a number of weeks already, they were kind enough to answer a few questions about their experiences within the company so far: What’s your background in software engineering? Tasmin: I learned to program during my Master's degree in Computer Science. As part of the course I did a Software Engineering unit – we worked in teams to make a game. I really enjoyed this unit, so for my dissertation, I decided to develop a web application for sports teams. The dissertation was challenging at first, but it gave me a good insight into software engineering and really improved my programming skills. Dan: Within education – Computing A level and Computer Science at university. Taught myself basic programming/sysops stuff in my spare time, but learned things in more detail through university and my placement. Outside of education, I've done minor freelance work setting up a website using a CMS, and I did a year's placement at a cloud accounting software company as a developer. I was in charge of their CI (Jenkins), ensuring it ran smoothy, and I worked on multiple projects during my year there. On my placement I learned a lot about TDD, Clean Architecture, Continuous Development/Integration, feature toggles, agile/kanban. How did you first hear about The Academy? Dan: A former colleague began working here, and he recommended the job for me as he believed it would be a good fit. Tasmin: I heard about the Academy at my university Career Fair. I knew I wanted to pursue a career in software engineering and talking to Luke at the fair gave me a great insight into what working at Made Tech was like! Can you tell us a bit about the project you’re working on? Dan: We're creating a statistics dashboard for the directors at Made Tech, in order to help visualise their goals and their progress towards achieving them. Tasmin: The dashboard will contain current data on Made Tech Marketing, Sales and Support. I’m really enjoying this project as it's giving me a chance to learn more about software engineering and develop my Ruby skills. How are you finding working at Made Tech? Dan: Enjoying it! I find that the team and environment are very supportive, and is a great environment to learn more about software engineering and professional development. Tasmin: Working at Made Tech is great! I particularly like being part of the Academy, as it's giving me a strong grounding in software engineering (and Ryan is a great mentor). Everyone here is always willing to help each other out and there’s a strong focus on giving each other feedback and personal development. Join us for our next intake With the new academic year fast approaching, we'll soon be taking applications for positions in summer 2018. Learn more about what you can expect during your 12 weeks as a student and submit an application here . Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-08-17"},
{"website": "Made-Tech", "title": "6 Ways to Embrace Startup Agility in Retail", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/6-ways-to-embrace-startup-agility-in-retail/", "abstract": "The retail industry has been squeezed on many fronts over the past decade, from Internet giants flexing their technology muscle (and willingness to not always turn a profit) to disrupt markets to fast-moving start-ups who are unencumbered by big company process, and who are able to attract strong technology talent. As companies grow, it is an almost inevitable rite of passage that stricter processes are implemented, decision making becomes more centralised, and increasing layers added to the hierarchy. While in some measure these things can help an organisation to scale, they can significantly reduce an organisation's agility when it comes to embracing changing commercial landscapes. Established retailers can take conscious steps toward adopting and embracing startup levels of agility, ideally without the relentless late nights. We've observed 6 ways retailers can embrace start-up agility: 1: Empower and incentivise teams Particularly in large organisations, individuals and teams may not be fully empowered to deliver their program of work. Teams that are reliant on outside stakeholders for decisions or for additional support typically show a much lower level of performance than a team that is empowered and incentivised for end-to-end delivery. Often this way of working involves pulling together cross functional teams – people from different areas of the business who are committed to delivering the same common goal. The team as a unit should be incentivised for successful delivery of the goal. Aligning individuals personal development goals to the overall success of the delivery, and keeping the team small can ensure accountability levels are high. We'd suggest a maximum of 5 or 6 to a team. 2: Hire intrapreneurial people When hiring, targeting people who have an entrepreneurial streak can be a real benefit. People who are not afraid to challenge the ideas of their seniors, and who are proactively looking to improve themselves can inject some additional energy into a team. Examining the profile of a typical early-stage startup hire, you'd likely be looking at someone who wears many hats – a bit of marketing, a bit of operations, and hopefully a whole load of hustle. Even if they've never done it before, someone who's going to figure out what needs to be done to make important things happen. 3: Reward innovation Many established businesses are focused heavily on risk mitigation, with individuals rewarded for following safe strategies rather than challenging and innovating. Where desirable, we see benefit in incentivising individuals and teams to take risks, to test new markets, and to help the organisation learn from this risk taking. An organisation that primarily rewards people based on taking the safe, tried and tested path needs to be confident that they're not at risk from outside disruption. 4: Adopt a test-and-learn strategy Most retailers have long since built out a data capability. Extending this a step further, startups often approach their entire business model with a test-and-learn strategy: Come up with a hypothesis Figure out the smallest experiment that will prove this Run the experiment Repeat This can be as micro as comparing the effectiveness of two POS promotions, the ordering of products on an e-commerce listing page, to testing an entirely new channel. 5: Buy smaller, modern technology services Too often, large organisations default to long purchasing cycles of \"enterprise\" software solutions. On top of the long purchase process, these solutions can also have long implementation cycles, leading to slow adoption of new technology. In the startup world, new technology can often just be a credit card purchase away, and at the most extreme, involving just a small copy-and-paste to integrate with existing systems. As a starting point, when looking to explore and prove new technology platforms, organisations could be encouraged to trial leaner Software as a Service packages in the first instance. Being able to launch a new platform in minutes or hours can offer an obvious advantage over waiting months or years for a global rollout. 6: Use rogue teams If all else fails, retailers can resort to letting rogue teams loose in the organisation. Usually backed by a senior executive, the rogue team can be given a specific remit, often in exploring new markets and opportunities, and empowered to bypass usual corporate policies and processes. While rogue teams can be a great way to deliver results in the short term, it could be argued that they're not a long term fix for more deep rooted organisation red tape and inefficiencies that exist for the wider company. While these techniques are unlikely to turn an established organisations into a venture that can \"turn on a dime\", they can be steps towards adapting to change in a fast-changing landscape. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-08-31"},
{"website": "Made-Tech", "title": "Continuous Feedback", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/continuous-feedback/", "abstract": "We believe it's important to foster an environment of continuous improvement, whereby the performance of every aspect of the organisation is encouraged to be on an upward trajectory. This is especially true in service and knowledge-based businesses, where, to throw in an early cliché, people are generally your most valuable, or only asset. What is Continuous Feedback? The goal of Continuous Feedback is to significantly shorten the personal feedback loop in your teams. When delivering software, lean and agile tell us to value things like short feedback cycles and regular retrospectives and course corrections. However, for many organisations, the individuals seem to have been sidelined, and the more traditional annual review continues to prevail. With Continuous Feedback, instead of having a touchpoint every 6 or 12 months, individuals have one every couple of weeks. By focusing on events from the last fortnight, the feedback is more current, and because the next review is only two weeks away, it's an ideal forum for regularly tracking progress on smaller, more incremental goals. What’s wrong with traditional performance reviews? We perceive a number of downsides to more traditional performance reviews: Feedback is not current If a feedback session happens once every 6 months, or worse once a year, the time between any event that warrants discussion or course correction can be significant, missing the opportunity to course correct or reinforce that behaviour sooner. There's also a natural bias to focus more on recent events, rather than potentially more important things that happened 6+ months earlier. Goals are not tracked regularly On a similar thread, setting goals to be achieved over the course of a year without more regular opportunities to share progress seldom yields the desired outcome. In the days running up to an annual review you may see a flurry of activity against last year's goals. If there is an opportunity to share progress in the next couple of weeks, it's likely to be a much more current concern. Feedback season is overly time consuming For managers with many direct reports, feedback season can be a particularly time consuming period as they try to recall the pertinent events of the previous year. In addition, if the organisation performs '360 feedback' team members need to be badgered, and possibly reminded on how to give good quality feedback. While it's true that Continuous Feedback likely adds up to more time spent on feedback activity over the course of the year, each feedback window is a less daunting prospect. Feedback is not owned by the individual In a traditional review format, where the manager authors or collects the feedback, and it's delivered to the individual, there's little feeling of ownership over the feedback. The individual is almost a passenger in the process. If the feedback is from a third party, the individual may also miss the ability to talk to the feedback giver to better understand its impact. With these downsides in mind, and with what we can learn from modern software delivery practices, we can consider another approach. How does Continuous Feedback work? With the primary goal to shorten feedback loops, Continuous Delivery offers up far more frequent review sessions. For us, once every 2 weeks has proven to be a sweet spot. These sessions are typically much lighter than a more traditional review, running for somewhere in the region of 15 – 25 minutes per session. Individual responsible for collecting feedback With Continuous Feedback, it's the individual who should take responsibility for their own feedback, and bringing it to the session. This gives the individual more ownership of the process, and also provides them a forum to discuss the feedback with the giver ahead of the session. We'd generally steer away from anonymising the feedback process, instead trying to encourage a culture where the team is able to provide well thought feedback, and where people are open to receiving such feedback to aid with their personal development. Short sessions and short-term goals As earlier mentioned, because Continuous Feedback sessions typically happen much more regularly than more traditional review processes, the sessions themselves are generally much shorter. Because the next session should be booked in for a couple of weeks time, Continuous Feedback makes short-term goals far more relevant. Individuals can be encouraged to think about the changes they'd like to make in the immediate term based on the feedback they've just received, as well as what incremental steps they can take towards their longer term goals. The sessions should also provide a forum to add some check-up, and holding to account for goals that were set in the previous session. Individuals should be coached to set achievable goals, and there should become an expectation that the majority of these goals will be achieved. Understand individual dissatisfaction sooner Review sessions should provide for two-way traffic (though in many organisations they may not!). If an individual is dissatisfied in their role through lack of progress, a lack of enjoyment for the work, having relationship issues with others in their team etc., it's generally far better to hear about this sooner, so it can be rectified before becoming a larger problem. Sessions can be facilitated by a peer To encourage more autonomy in teams, it's possible to have peers facilitate each other's review sessions, and further, facilitate sessions for their managers. It can be a useful tool to level out an organisations hierarchy, and to offer another forum for individuals to further develop their softer skills. In adopting a peer-led structure, organisations may want to consider how any individual dissatisfaction can be fed back to people in the organisation who are able to make changes, if for whatever reason the peer who is facilitating the session is not able to act on it. So now you've heard about the benefits of Continuous Feedback, how can this be rolled out to your organisation? How to adopt Continuous Feedback Continuous Feedback is likely to be a significant change in format for many organisations. The scale of the organisation and how open to change it is will be a large factor in your path to adoption. That said, one approach almost always provides greatest traction: Start with a small group If possible, steer away from launching a 'big bang' change on any sizeable group of people. Our recommendation would be to take a group of 3 or 4 people initially, and introduce the process to them, ideally in the form of a short face-to-face discussion on how you see things working, and why you're keen on giving this a go. Hand-picking your first cohort can be a good idea – people who are generally receptive and enthused by new ways of working, and people who you think may be good allies to evangelise this process to a wider audience. After facilitating a Continuous Feedback session for everyone in this group, take the next group of people, and introduce the process in much the same way to them. If you've chosen to adopt peer facilitation, it's a good opportunity to have some of the first group induct the second. Coach on how to give and receive effective feedback People in many organisations seldom have the chance to provide feedback, and so may not be well skilled in it. As the team will be expected to deliver feedback regularly, and because they'll be delivering that feedback direct to the recipient, it's a good idea to provide some guidance on how to give constructive and actionable feedback. Equally important is coaching people to receive feedback well. It is likely to be the case that some feedback will talk about areas for improvement for the individual, and so helping people to graciously accept feedback, and ask insightful questions when they don't fully understand the impact can help encourage a more open culture. Once the feedback has been collated, we need to do something with it. Encourage ultra-timely feedback Even with a window of a couple of weeks, it's sometimes hard to keep track of the most pertinent events on which to provide feedback. It can be worth encouraging the giving of feedback as close to the event as possible, so the individual can bring it to their next session. Givers of feedback may also choose to keep their own journal of feedback they have for other people, so they can quickly recall some recent feedback when asked for it. A word of caution here: if feedback is of a more critical nature, it may be wise to encourage the giver of that feedback to 'sleep on it'. Feedback of such a nature should never be given in the heat of the moment. Provide a framework for making feedback actionable During the review session, the individual should bring their collected feedback along. It's a good idea to provide a framework that encourages the individual to document what impact the event in question had, and what the key points to take-away should be. The role of the facilitator should be to bounce ideas off, and to ask questions to help the individual to think more deeply about the feedback. Once the feedback has been discussed, it's a good idea to look at some goals. Discussing progress towards the previous goals, closing out wherever possible, and identifying any new goals, either based on course corrections from the feedback, or as smaller increments towards the individuals longer-term goals. The facilitator should be doing what they can to hold the individual to account in achieving these short term goals. Keep momentum up As with any organisational initiative, keeping momentum remains a challenge. In the hustle of day-to-day work, it can be easy to allow commitments such as this to drop. You can consider rewarding buy-in from those who perform best at keeping their commitment high, offering public recognition, a company lunch or some other soft benefit. These people can be good allies in encouraging similar from their peers. Encouraging individuals, when asking for feedback, to highlight particular areas they're focusing on can help people provide more relevant and actionable feedback. In much the same way as retrospective exercises provide different formats with generally similar outcomes, you can occassionally switch up the format of the review session to keep things fresh. Overcoming resistance When rolling out any change to an organisation, you're likely to meet some pockets of resistance. These are some of the common arguments we've seen: – HR won't let me do this In larger organisations, HR may be a large skills silo with autonomy over how such things work. If you're unable or unwilling to fight this battle, you can run Continuous Feedback in parallel in your own team. You might have to continue to adhere to the more traditional approach provided by your HR team, but at least you'll have generated and documented plenty of feedback throughout the year to feed in to that process. – Shouldn't managers manage? Some organisations are precious over the hierarchy of managers managing people. It's possible to pick and choose how you want your implementation of Continuous Feedback to work – it's entirely possible for managers to facilitate all of the sessions, for example. As a principle, we'd be encouraging devolution of everything reasonably possible to the team, but that's a discussion for another chapter. – How can we let people go? Without the paper-trail of objective setting and measurement that more traditional approach offers, Continuous Feedback could be said to be a poor means by which to manage business exits. We believe such events aren't the norm, and so should be managed by other means. Our Observations We've been practising Continuous Delivery for some time. There are a number of hurdles or downsides that we've observed. More employees buy into it than others As with many initiatives, it's likely that some people will be more enthused than others. Some people can become anxious when expecting to receive overly-negative feedback (which turns out to seldom be the case), while others value the opportunity to have regular personal course correction discussions. Poor forum for larger issues If there are larger issues afoot with an individual, Continuous Feedback is not a good forum for dealing with them. This remains a good forum in which to involve a HR specialist. Without monitoring it's easy for people to duck under the radar Particularly where the program is largely delivered by peers, without some sort of tooling in place to increase visibility on the review schedule, some people may go for longer periods without a review. Struggle to keep accountability around short term goals A potential downside to delivering the sessions with peers, is that some accountability on goals can be lost. Some people are less comfortable in holding individuals to account on their achievement of their goals. Summary We believe Continuous Feedback to be a logical next step for teams who have been working hard to shorten feedback loops around their software delivery, and who have been practising team retrospectives as a part of their development cycle. If nothing else, it forces teams to have more conversation about individuals growth and development objectives, and provides a regular forum for people to vent their frustrations with a co-worker. The agile manifesto reminds us to value individuals and interactions, after all. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-11-24"},
{"website": "Made-Tech", "title": "Creating Environments That Promote Communication", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/creating-environments-that-promote-communication/", "abstract": "In any organisation, one of the most powerful ways you can empower your team is to give them an environment that allows them to communicate freely at all levels. At Made Tech we actively encourage every member of the team to initiate or join any discussion that interests them, whether it be giving their opinion on how a part of the business runs, or introducing a new way of approaching how we work. As a result, we've had valuable contributions from the entire team that have gone on to improve the way we work, build camaraderie and keep everyone up to speed on the goings on across the entire business. The benefits of good communication Visibility As organisations grow, it's often easy for, as the saying goes, the left hand to have no idea what the right hand is doing. Project teams may not be aware of what each other is working on, it may not be clear to the wider team what pieces of work management are trying to win, and it's especially difficult for peers to know who is trying to develop a particular skill they might be able to help with. As well as that, such a lack of visibility allows silos to develop naturally, where one or more developers wall themselves off to outside interference in order to concentrate on work. Whilst this is not a bad trait in moderation, left unchecked it can lead to those individual developers having exclusive domain knowledge, which then leads to bigger problems if there's a problem with the project they've been working on and they're unavailable for any reason. By increasing visibility, via the methods described later in this article, teams gain a shared understanding of the direction the company is headed in, who's working on what, and individual projects. Team members are also actively able to help each other grow, which ultimately benefits the entire company. Team cohesion A good company culture is important, and so giving your team ways to flex their tech muscles outside of obligations to customers is a great way to promote that. Whether it's an hour tackling a small, fun coding challenge , an entire day learning about a new technology, or a few days away on a company retreat, the time spent will go a long way to both ensuring your team are getting exposure to technologies and practices that interest and excite them, and that they're becoming a stronger and more collaborative team for it. Provide open channels and opportunities to share knowledge The first steps to creating a communicative environment for your team is establishing open forums in which everyone is invited to contribute as much as they'd like. We've got a few in place in Made Tech, all of which have had a positive impact on the way we work. Continuous feedback Chris has written about Continuous Feedback and its benefits at length, but in a nutshell: Continuous Feedback is the practice we use to encourage and address regular feedback from everyone about everyone. We found that annual reviews fell short in helping individuals learn and grow, and switching over to Continuous Feedback has allowed us to get a better sense of the goals people are trying to reach, and the areas in which they need improvement. Importantly, it also allows us to act quickly to help them reach those goals, and give them the support they need in the areas in which they need improving. Share progress and achievements with a weekly email One of the problems we were having was that it often wasn't clear what other people outside of your team was working on week to week. They may have been on a new project, running with a new initiative, or on a recruiting drive. Our solution to this problem is what we've lovingly dubbed \"TGIFs\". A TGIF is simply an email sent at the end of every Friday, with a short breakdown of what each of the various teams and departments within the company have achieved that week. Each team sends their breakdown to one person, and that person collates them all and sends out the email. TGIFs are a great way of giving your entire team visibility on what's happening within the company, and also reminding people of important events coming up the following week. Comradrospectives Retrospectives are a key component of the Agile process, occurring at the end of a period of work and allowing teams to reflect on what went well, what didn't go so well, and what they can learn from that. We've reappropriated that idea and applied it to the entire company, so that the entire team gathers together every few weeks to discuss our biggest sources of frustration, share our achievements, and figure out what we need to focus on in order to continue to improve the experience we provide to our customers, and the way we work. Prior to each comradrospective, each team and department have a brief retrospective, and then bring the notes from that to the larger meeting. A Facilitator hosts the comradrospective, going around the room and prompting discussions about what each team found in their retrospectives. The Facilitator then looks for common threads between those discussions, which often leads to the highlighting of a wider company issue that can be focussed on over the next few weeks. It should be noted that the role of Facilitator doesn't belong to one person, we actively encourage everyone on the team to take up the mantle for two reasons. One: each person will approach the role differently, ensuring comradrospectives never feel old, and two: it gives everyone a little bit of exposure to standing up in front of people and leading the conversation, which is valuable experience to have when your team is expected to present things like showcases to clients . Presentations No two individuals are alike, and within every organisation you're going to have an incredibly diverse set of interests and opinions. New technologies and frameworks are released to the public almost daily and, if your team are passionate about their craft, occasionally one of those technologies will resonate with someone on the team. They'll spend their own time researching it, decide whether it's for them and, if it is, spend even more time mastering it and becoming excited about the possibilities it provides. As someone cultivating a communicative environment, it's important that everyone on your team knows they have a platform for sharing their excitement with everybody. Giving them the freedom to host presentations on topics they're enthusiastic about means everybody is regularly exposed to ideas and concepts that could potentially offer real value to your organisation. This sort of exposure to new ideas and technologies then leads to a willingness to implement them in future projects, meaning your organisation is constantly evolving, rather than sticking with practices that will ultimately end up outmoded or obsolete. Hack days Hack days are another great way of exposing your team to new technologies, as well as being a way to mix things up and potentially even produce products you or your customers will use for a long time. As an example, a customer of ours wasn't aware that their SSL certificate was due to expire, leading to browsers designating their website as insecure. This is a problem we've seen in organisations as big as Instagram , so we decided to hold a hack day dedicated to solving the problem. The result was SSLCatch , a relatively small application that simply alerts domain owners in the days, weeks and months before an SSL certificate is due to expire. The whole team worked together over eight hours, regularly communicating with each other to get the clearest sense of what everybody was working on, solve blocking issues and ultimately get the application live as quickly as possible. We now have hack days every one to two months, and they've often resulted in new internal tools we use regularly. More than that, they're always a great opportunity to get the whole team swarming on a single project, solving problems together. Regular stand-ups Daily stand-ups, also known as scrum meetings, are an important tool for getting everyone up to speed on what people will be working on that day, as well as for highlighting any issues they have, or how they can help others facing their own issues. Daily stand-ups should be short, around ten minutes max, and should only involve team members working on the same project or in the same department. We've taken that concept and extended it into what we're calling \"Scrum of Scrums\", where a representative from each team takes part in two or three meetings spread across a week to state what their team's goal for the week is, and how they're progressing with it. The objective of these meetings differs to that of a regular scrum meeting, in that it serves only to give even more visibility over what other teams within the company are doing. Extreme programming Extreme programming is a popular methodology that encompasses many different programming practices, most of which are beyond the scope of this article. That said, two practices in particular are very useful when it comes to promoting communication within your team: pair programming and \"whole team\". Pair programming We've discussed pair programming at length , but, in short, pair programming leads to stronger communication by encouraging each programmer to articulate their thought processes and engage in conversation about the best possible solution for the problem they're facing. Whole team The phrase \"whole team\" refers to a way of working in which teams are given the freedom to organise themselves , and whose team members can, between them and their various sets of skills, solve any technical challenge a project might present. As well as that, when building software for customers, one of the biggest sources of frustration is often not being able to get hold of the customer when a critical question about the work arises. \"Whole team\", then, means having the customer, at the beginning of the engagement, designate one of their own team to be the Product Owner throughout. The role of the Product Owner is to be available at all times to your team, so that they can give guidance and answers to your team as and when they need it. In our experience with Product Owners, we've found that having the Product Owner on site leads to an even better experience, as they're instantly more approachable and, from a wider business perspective, it becomes very easy to build a relationship with the customer. Challenges in maintaining good communication Tuning out Every programmer knows the feeling of being in flow, when they're highly focussed and incredibly productive, and it's very important not to make that state harder to achieve, as it can lead to frustration. That said, left unchecked, it can be tempting for some to almost permanently block everything else out, forging ahead on their own path and rarely communicating with the rest of their team. This then leads to either the programmer having exclusive domain knowledge over a significant part of the application and becoming a silo, or spending time going down a path of work without getting more information from others, potentially producing something that can't be used. Too many cooks On the flip side, too much communication is A Bad Thing. Making time to communicate is important, but when it comes down to it you need to make sure the work you're communicating about is getting done. Pushing communication too much can lead to discussions or debates where there are so many opinions that finding an actionable outcome to them proves impossible and time consuming. While it's important to make sure everyone has a voice within your organisation, there will inevitably be times when action needs to be taken, but you're debilitated by too much discussion. Judge these situations carefully, and don't be afraid to take the lead if it means resolving a conversation that's gone on for too long. Conclusion By giving your team an environment in which they're able to communicate freely about whatever excites them, challenges they're facing, how your team could be doing things differently and ways they can help each other improve, you're ensuring your organisation is able to constantly grow and meet any challenge an ever changing industry throws at you. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-11-28"},
{"website": "Made-Tech", "title": "Beware of solution problems", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/beware-of-solution-problems/", "abstract": "A solution problem is an emergent property of a solution. Experienced Software Engineers avoid creating solution-problems, they create simple solutions for the root problem. A few years back I was working closely with designer and musician Martin J. Thompson. We found it difficult to get non-technical individuals to provide the right information about the problem they were facing. So we coined a phrase “don’t give us your solution-problem, give us your problem-problem.” We never actually said this to a client, but maybe we should have. Stakeholders tend to want to solve problems. It is only natural as human beings, especially entrepreneurs, love solving problems. The problem with not communicating the root problem across to Software Engineers, which you work in close collaboration with, is that it is unlikely that you can see the entire solution space. The building blocks As Software Engineers we see building blocks that we can use, all around us. Some we use often, some we use infrequently, but the one thing in common is that most don’t show up in the end product in any meaningful way. The problem with these building blocks is that they only allow you to build solutions in certain ways. For example if a client said I think we could build this skyscraper entirely with wood, the architect would laugh. When customers specifically ask for things such as Web 2.0, AJAX, REST, or Java without understanding what problems they are useful for solving we can, and we should ignore those requirements until we are aware that it is suitable. Think of these demands as being the equivalent of saying to a carpenter that she must use saws and chisels. These are tools and materials, and we are the experts. It is our job to advise on what is the most appropriate. Understanding the business The second thing we see are systems of people, people who follow processes. To elaborate a little Bob’s job might require him to prepare 50 spreadsheets per week manually. After Bob has finished these spreadsheets, he sends them to Jane, who, then sends to Lisa a shortlist of those spreadsheets. Lisa is responsible for ensuring another department is kept up to date. However, this other department needs to use a program only available on a 30-year-old mainframe, so she prints them off and gets Joe to input the data manually. This process may seem like a perfectly reasonable one to the people involved. Bob's managers are getting the impression that this is not the case, but they are not quite aware of all the details. So managers get together and decide that Bob should be responsible for the delivery of some modernisation, he is seemingly the reasonable choice for the business analyst as he produces the spreadsheets. They hire some software engineers and make Bob responsible for giving them requirements. The trouble is, Bob does not know that Joe exists. He is aware that the system Lisa needs to get the data in is old, but has never seen it. When the software engineers speak to Lisa, she is very busy and just informs them that she wants to continue receiving that spreadsheet. When they talk to Jane, she explains in detail how she creates the shortlists, and tells them that she wants an Excel macro built for her. The software engineers are left thinking that perhaps this is a good place to start. Similarly, Bob is looking for a promotion from successful delivery of this automation project and does not want to be manually creating spreadsheets anymore. It is tempting to deliver all of these people what they want. Seems complex right? UIs need well thought out UX. That is many spreadsheets that need emailing; perhaps that can be automated? There is lots of room for human error in the entire process. As Software Engineers, we can have a larger solution space. If the software engineers were able to get access to Joe, they might begin to ask lots of valuable technical feasibility questions: Can we emulate that mainframe on a modern computer? Is it worth the effort right now? Maybe we could emulate a keyboard? Does Jane need to be involved? Does Lisa use the spreadsheet for anything herself? The result could be to automate away this entire flow and have it running entirely on Amazon Web Services with no manual human input required. Without fully understanding the root problems, Software Engineers are unable to implement economic, long-lasting, valuable software solutions. When presented with a solution-problem it is important that software professionals dig beneath the surface and ensure they understand where their solution fits into the business environment. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-12-01"},
{"website": "Made-Tech", "title": "Learning From Mistakes", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/learning-from-mistakes/", "abstract": "As software engineers, we're faced with new problems and challenges every day. No matter how well we know a programming language, how many projects we've worked on throughout our careers or how much time we've spent creating repeatable solutions to common problems, there will always be something new that requires critical thought. Inevitably, then, we will make mistakes. Deadlines will not always be met, solutions may not always be correct, and critical tasks may be overlooked. We may even accidentally break the software. These are not Bad Things. When we're walking an unfamiliar path, we're bound to make the occasional wrong turn. What's important is that we know that those mistakes have a lot to teach us, and that we're in an environment where it's safe to make those mistakes. Creating a safe environment to fail Avoiding Blame From an early age, we're taught that when we do something wrong, there are negative consequences. We learn to associate those consequences with the action that caused them, and we actively avoid it in future. We're also encouraged to assign blame when we see others doing something wrong, if only to again avoid negative consequences directed at us. This continues into adulthood and our careers, with the assumption being that your employees work as hard as they do to, in part, avoid making mistakes and being made an example of. The belief is that without negative consequences to failure, your employees will be less engaged and less motivated. This attitude is counterproductive to a healthy working environment. Giving your team a space in which mistakes and failures can be accepted and learned from doesn't mean encouraging lower standards, but ensuring your team and your organisation as a whole can continue to evolve and grow. It's also important to recognise that mistakes and failures are not necessarily the result of wilfully deviant behaviour. Tolerating mistakes, and recognising that they are opportunities for everyone to learn rather than for one person to be blamed, is a skill. Retrospectives Your team needs to know that mistakes can be tolerated, and the best way to convey this is to have open discussions about problems that have arisen, without playing the blame game. Take the time to talk about why a mistake happened. Once you've discovered the mistake, you need to find out what it can teach you and how it can help you in the future. Regardless of what we discover, we understand and truly believe that everyone did the best job they could, given what they knew at the time, their skills and abilities, the resources available, and the situation at hand. – Norm Kerth This quote, known as the Prime Directive of retrospectives, illustrates perfectly the attitude that should be taken when trying to create an environment that looks at mistakes and failures in a positive light. Retrospectives happen at the end of a sprint, usually every week or two, and give everyone a platform on which to highlight things that went well, and things that didn't go so well. Providing you've built an environment in which it's safe to be honest about shortcomings and mistakes, retrospectives are a great way to uncover process failures and to voice concerns about the work that may lead to avoidable mistakes. An example we've encountered is realising that our mistake was not getting enough detail on requirements during the planning stage. This led to us moving down a path of work that we ultimately discovered was incorrect when we presented it to the customer. Within the subsequent retrospective, we were able to safely discuss this as a team, and to admit that there were things we could have done but didn't. We spent time figuring out why this had happened, and then deciding on actionable steps we could take to prevent it happening again in future. We came away from the retrospective having realised we needed to spend more time early on discussing requirements with the customer, and then making sure that information was disseminated across the entire team. Embracing mistakes We’re in the discovery business, and the faster we fail, the faster we’ll succeed. – Amy Edmondson The best way to deal with mistakes and failures is to treat them as opportunities to learn, both individually and as a team, after all, while it's our job to design solutions that meet requirements, we're not infallible. When we're embarking on a new challenge, making mistakes is a crucial part of the discovery and experimentation process. Our solutions may fail in unforeseen ways, or we may need to revisit the task and find that we've made more work for ourselves by creating something inefficient. Either way, you have the opportunity to reflect on what went wrong, why, and how you can simplify the process to either reduce or eliminate mistakes. Big mistakes are easy to spot and discuss. In software, you know something's gone wrong if, for example, a build fails, critical data is lost or a website goes offline. Steps are immediately taken to fix those mistakes and resume normal service. The trick is being able to identify and learn from smaller mistakes, as they're much more easily hidden, both passively and actively. The earlier these are discovered, the better. This mindset of actively discussing and learning from mistakes, rather than blaming anybody for them, doesn't mean you're encouraging your team to slack off and take shortcuts to the detriment of the project. Even in situations where a mistake can be attributed to an individual's lack of care or inattentiveness, there's the chance to dig deeper and discover what led to that behaviour, and what you can do to improve the situation for the individual and your team. Using mistakes to uncover requirements In most software teams, strides are taken at the beginning of a cycle of work to gather as much data and information as possible from the customer to understand requirements as completely as possible. Nevertheless, it's not unheard of for a seemingly unimportant detail to be overlooked during this phase of the project, only to either become a blocking problem midway through development, or to go completely unnoticed and later be revealed as a key requirement whilst you're showcasing your work. Use these situations as opportunities to figure out how you can improve next time: what information didn't you have that you wish you'd had? How could you have elicited that information from the customer? Could you have broken tasks down further to discover hidden requirements? Questions such as these will help your team improve with each new project, and ultimately you'll deliver better work and make your customers even happier. One way to try to discover hidden requirements is to carry out research spikes. On a software team, this would typically involve one engineer dedicating a small but significant amount of time, such as half a day or a whole day, to investigating whether a potential solution is worth spending more time on. It's a long enough period of time that some thorough research can be done, but short enough that, if it doesn't pan out, the loss of time isn't too much to bear. The researcher is also safe in the knowledge that, should the research lead nowhere, the team won't consider it a failure. Using mistakes and mentoring to help teach new skills Less experienced members of the team may struggle with tasks other engineers find simple. As we've said, engineers at every level are constantly facing new challenges, and mistakes are bound to happen. However, when you have junior engineers working alongside senior engineers, the environment you're creating should allow juniors to feel safe enough to approach their more experienced peers for guidance. To take it even further, encouraging your senior engineers to take an active interest in mentoring is a great way to quickly upskill newer members of the team. Using mistakes to analyse common problems and automate them away Software engineers love to automate all the things, but there'll always be the occasional process that's still being performed manually and, no matter how often the process is performed, the more convoluted it is, the more likely it is that a crucial step is overlooked, leading to a failure. Back in the day, before the advent of source control, something as fundamental as deploying changes to a production environment was a manual process, and involved massive amounts of risk. If the deploy broke anything on production, you had to cross your fingers and hope that someone on the team had a historical copy of the offending file. That problem was solved with a combination of solutions such as Git and Jenkins, which give us the ability to easily deploy and move software through various testing environments all the way to a production environment at the touch of a button. If anything is broken, we can then easily roll the latest changes back. Within your organisation, there are likely several risky and complex processes that are causing your team frustration. By allowing your team to identify and discuss these problems, you're giving them the ability to work together to find a solution that reduces the risk and transforms the process from one that causes frustration to one that is almost mundane. Conclusion Mistakes and failures are not something to be feared, in fact, celebrating them is perhaps more appropriate. That statement sounds a little ridiculous but, when you consider how much a failure can teach you about the work you're doing, the way you're doing it and how you can help others do it, there's too much valuable knowledge to be garnered from a mistake to set about reprimanding someone for making it. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-12-14"},
{"website": "Made-Tech", "title": "Give Teams Ownership of Delivery", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/give-teams-ownership-of-delivery/", "abstract": "It's a poorly kept secret that increasing levels of responsibility, particularly with knowledge workers, often correlates to an increase in performance. We strive to have the team dedicated to a software product responsible and accountable for the end-to-end delivery: from initial requirements capture, right through to launching and supporting the application. In devolving responsibility to teams, there are a number of areas worth consideration, the first of which is what skills you'll need on the team to best achieve this. Shaping the Team When pulling together a new team, it's important to consider the blend of skills that you need. We tend to shy away from specialist roles, such as database administrators or dedicated testers, though we do see value in ensuring a healthy mix of experience levels on a team. We package desirable behaviours in to documented traits, describing a series of attributes people can work towards. One of these traits is delivery, which encompasses process improvements, understanding commercial objectives, and ensuring customer needs are met. Other organisations may choose to recognise related attributes in other ways, such as through team lead or similar titles. Whatever your flavour, it's sensible to consider having an experienced pair of hands in the team with the nous to nudge things in the right direction. On the flipside, it's wise to be wary of conflict in teams where everyone is chasing the same personal objective. If you have multiple people on the team who are used to playing a delivery lead role, the team may not easily settle. Having a mix of people skilled in a number of different areas who can coach and upskill other members in their various strengths is a positive place to be. Once you're confident you have the right mix of people together, the next step is to consider how to better empower them. Empowering the Team Successful team empowerment typically comes from blending expectations, responsibility, and accountability. Without these forces working in balance, you're likely headed for frustration. It's firstly important to instil a sense of responsibility that everything from understanding what the customer needs, right through to launching the thing in production is down to the team. Depending on the culture in your team and wider organisation, you may need to do some work to set expectations on what you expect the team to be achieving. This should shy away from task-level goals, but be a higher level goal – perhaps something along the lines of: increasing customer happiness through delivering valuable, working software regularly. Once you've done this, your next step is to not interfere. For many managers, this part is particularly tricky. Your only responsibility from here on in should be to hold the team to account for meeting their commitments and for delivering to the higher-level expectations that you've agreed. We cover accountability in a little more detail later. An empowered team should be focusing its efforts on problems, not on implementing perceived solutions provided by an external architect or other higher being. Task Teams with Problems, not Solutions Ensure that the team are being engaged at the correct level. If you're looking to encourage strategic, rather than just tactical skills, don't ask the team to deliver a pre-defined solution to the problem. Instead, ensure the team are empowered with, and over time, are actively seeking out an understanding of the commercial goals behind each and every feature. Have the team take responsibility for identifying and then delivering solutions to these commercial problems, rather than executing against a pre-baked task list. This ownership of the fuller problem should help accelerate a feeling of empowerment. When relinquishing any kind of control, or when trying to increase the empowerment of others, it's highly likely that many things will not be done as you would have done so yourself, and it's important to quickly come to terms with this. Be Prepared to Accept Mistakes If you were managing a task-list for a team and having them execute it, likely you're going to be getting your own way a lot of the time. When a team moves to working toward higher order goals, this is likely to not be the case. It's important to be prepared to accept that mistakes will be made. Wherever possible, you'll need to allow the team the freedom to do this, even where there's a cost involved, and where you believe you can clearly foresee the problem. If you're overly keen to jump back in and provide input, you'll too easily undo the work involved in imparting a true sense of responsibility and empowerment, and before you know it, members of the team will be deferring decision making outside of the team. You should try to see your role as preventing the team from jumping off a cliff, but not much more. Opening yourself to the right conversations can help a seasoned leader more easily accept this transfer of decision making. This topic is discussed more fully in our article on Accepting Mistakes . Ensure You’re Being Asked the Right Questions Particularly with newly empowered teams, you should make yourself open to discussing strategies with the team. However, you should be careful to avoid allowing the team to too easily devolve lower-level decision making your way. A quick and easy technique is when asked a question, first ask the other person what they think the best course of action is. Even if you'd consider doing things differently, unless you believe what's being proposed is significantly detrimental, let the team run with it without providing input. You should move to a mindset of providing advice from outside of the team. One easy tactic to accelerate this shift is to ensure the day-to-day point of contact is inside the team. Point of Contact In most engineering teams, it's natural for there to be a primary point of contact for the Product Owner, Customer, or similar role. It's of paramount importance that this primary day-to-day contact be someone in the team, rather than having communication fed through intermediaries, particularly those more senior within the organisation, and those not full-time committed to the engagement. Having communication come from a conduit outside the team disempowers the team from building direct relationships with those commissioning the software, and it hinders open conversations that help engineers better understand the true commercial objectives and pressures. The point of contact is unquestionably an important role within the team, particularly if you provide consultancy-like services to other organisations. You should be conscious to invest in upskilling, nurturing, and providing regular support to those new to this role. Upskilling the Team There are a number of areas in which you could consider upskilling the team, particularly if increased responsibility and empowerment is new for them. Delivery Mindset If someone has spent much of their time being assigned tasks, and then executing them, it's likely some work is needed to coach more of a delivery mindset. This involves thinking at a higher level about how to best move from a stated problem, to working, released software. Likely this will entail higher level of communication with customers, helping shape priorities, identifying and articulating solutions, and ensuring there's an appropriate framework in place to facilitate fast delivery and fast feedback. Customer Service Many organisations make the mistake of hiding their engineers away from the customer, instead relying on a middleman to collate requirements, showcase the software, and generally keep the customer happy. You should encourage a solutions-focused culture, where engineers see their role as facilitating the customer, be it an internal or external stakeholder, in achieving their commercial goals. Ensuring regular and open communication, and regular showcasing of the software can go a long way. In many organisations, if software engineers have historically been engaged later in the process, after pre-conceived solutions have been devised, there can be a pervasive 'no culture', where engineers will not be used to engaging in the process of helping to solve higher order problems. In these cases, additional effort will be required to shift the culture to one of more open, helpful dialogue. Understanding the Problem If your team have historically been focused more on solving task-based problems, coaching is likely to be necessary to help in how to better gain an understanding of the higher level problem to be solved. Consider encouraging communication with the end user, support in navigating organisations to reach the true stakeholders, and relentless questioning of why requirements are important. Commercial Understanding In some organisations, you may observe engineers shielded or otherwise isolated from the commercial drivers behind software deliveries. In the worst cases, which are thankfully a little less common nowadays, engineers can be tasked with building libraries with specific interfaces, with no understanding of what's going to be interacting with this interface, even at the software level, let alone the higher-level commercial goal it's designed to solve. Instead, we believe people perform best when they understand the domain in which they're working, and when they understand the commercial goals that the software is supposed to solve. Empowered with this understanding, coupled with their software engineering know-how, engineers can often propose easier and cheaper solutions to achieve the same aim. To ascend a team to true autonomy, healthy mechanisms need to be in place to keep accountability levels high. Without them, the temptation for managers to interfere often become too great. Keeping Teams Accountable An important, and sometimes overlooked trait when increasing ownership within teams is putting in place appropriate mechanisms to hold the team to account for unacceptable deliveries. As a consequence of empowering the team with higher level responsibilities, you'll often see teams naturally holding themselves to higher account. This is the ideal culture to be building. That said, it's likely that, given enough iterations, even amongst teams who have plenty of experience with higher levels of empowerment, there will be occasions where less desirable performance will be observed and should be surfaced. To avoid disempowering the team, it is suggested to give higher level feedback, and to allow the time to digest and work out solutions without taking involvement. Describe the situation that you observed as undesirable, and explain the reasons you feel this way. You should use this mechanism sparingly, as its effectiveness is likely to be reduced the more you lean on it. If you find it necessary to follow this recourse every week or so, it's likely you've got larger root problems that need to be addressed, or that you're not truly comfortable in allowing the team to be fully empowered. This feedback should be given at an arms length, avoiding the temptation to roll your sleeves up and become a part-time team lead. Supporting Teams from the Outside While a large part of encouraging ownership is allowing teams to operate more autonomously, it's important to also offer an appropriate level of support. You should keep front of mind that you need to provide this support from outside of the team to avoid undermining the team's autonomy. Ideally, you should encourage a culture where a team will proactively ask for help or input when they feel they need it. Offering up too much assistance can have an undermining influence, even when well intended. You should consciously aim to always ask the other person, or the team, how they think to best handle the situation before offering up your own solution, further nurturing independent thinking. The nature of the team and the individual will dictate the best way to provide support. For some people, carving out a small dedicated amount of time each week may provide a good forum for supporting, for others, you may find you naturally have more informal conversations throughout the day. Next steps Depending on where your teams are currently at on the spectrum of empowerment dictates how much effort will be needed to give teams full ownership of their deliveries. As with rolling out any organisational change, we'd recommend to introduce change progressively and subtly, beginning by exhibiting and coaching the behaviours yourself. If you're currently integral to the delivery of software projects, particularly in cases where you're not truly a part of the full-time team, you should consciously look for opportunities where you can transfer these responsibilities to the team. With teams taking on more autonomy, not only are you likely to reap the benefits of people feeling more responsible, more valued, and more productive, you're also building a foundation that allows you to scale engineering efforts across many autonomous teams. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-01-04"},
{"website": "Made-Tech", "title": "Agile Planning", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/agile-planning/", "abstract": "Both words, \"agile\" and \"planning\", mean different things to different people. In this article I hope to provide an overview of agile planning without going into specific implementations like Scrum or Kanban whilst still providing practical advice for any implementation. For the purpose of this article, agile planning refers to the organisation of teams and work in order to deliver value iteratively. You might consider it a strategy where planning is not a phase and is not completed ahead of implementation but instead happens frequently during a delivery, enabling teams to adapt to change. Start with the value Every project will want to provide value to an organisation or person. If we choose agile we may not plan and document everything up front but we'll always be working towards a known, or partially known, value. It makes sense then to understand the shapes that value can take and work out what high level value we want to achieve. We can group our value into 5 abstractions. Each deal with a different unit, each with their own stakeholders to whom they provide value. Vision is the central mass that holds the project together Goals are quantifiable measurements of value Capabilities are provided in order to reach goals Features implement capabilities Tasks are the units of work that produce features You can read more about these units in Liz Keogh's article on estimating complexity. She first introduced me to this idea and it's stuck with me ever since. https://lizkeogh.com/2014/06/06/goals-vs-capabilities/ https://lizkeogh.com/2013/09/05/capability-based-planning-and-lightweight-analysis/ Even if you haven't knowingly defined these units before, your project will likely have them. They are natural artefacts of software engineering projects. Your team may not be aware of the vision, but your product owner or the manager ensuring the projects completion should do. Your team may be asked to complete a set of features without being given the freedom to consider alternative ways of providing capabilities. Start with a vision and goals that work towards it I hope those reading this article, those that haven't always identified vision and goals, capabilities and features, will be able to identify them in future. Even better, have the team understand and help define them. If you do not have a vision and set of related goals defined for your product or project, grab your team and stakeholders and define them now. Layers of agile delivery You could see the project's vision, goals, capabilities, features and tasks each as interdependent layers. Each layer helps achieve the layer above them. Goals implement a vision, capabilities implement goals, features capabilities and so on. This is the direction of the dependence. Never assume a feature is the only way to provide a capability When you think about a layer's dependencies as travelling in a single direction, as a single direction of authority, you begin to think less in solutions and more in problems. You start with a vision, this is the highest authority. Your vision defines its goals. Goals then require capabilities. Only then do our capabilities need features to implement them. Why then do we march into new projects with a bunch of features in our heads? Probably because they're the most tangible output of a project, but that doesn't make it sensible, or even agile. A charity might have a vision to provide free access to travel for the elderly. A goal to measure the success of this vision might be to provide 10 elderly people free access to travel a week. Now in order to provide 10 elderly people free access to travel, we'll need a way of transporting them. Transportation is our capability. Only then do we consider a feature that provides this capability. We might choose taxi, bus, volunteer driver, spaceship or a mixture of them. The point is we haven't locked ourselves to a particular solution. We've left our options open. When features become subservient to our capabilities, goals and vision we are no longer so attached to them. When we realise a feature is going to take too long to deliver we can consider our alternatives. When a goal needs to change, we are more inclined to let go of the capabilities and features as we know the success of our project is related to its goals and not how those goals are implemented. Moving through the layers with iterations Agile teams will use iterations to complete tasks. They might have several iterations for one task, or complete many tasks in a single iteration. What is common is the use of an iteration to receive feedback on work completed to ensure it's meeting expectations and providing value. Iterations enable us to move down through the layers understanding the problem all the way down to individual tasks, and then we travel all the way back up solving problems at each abstraction. Be clear on your purpose, bring it back to the vision During an iteration we travel through each layer in order to understand the problem. In order to decide on a feature we must understand what capability we are trying to provide; the vision and goals can help us make decisions. We might only track tasks on our board and capabilities on our roadmap but it's always useful to keep the parent abstractions in mind. Write up your vision and goals above your boards and maps. At the beginning of every stand up, reiterate the feature you're working towards before describing the task you're stuck on, don't stick to the minutae, try and remember why you're doing the task in the first place. Focus on high quality output At the end of every iteration you'll want something usable that you can put in front of stakeholders. It's the job of your team and stakeholders to be able to judge the output of the iteration against the projects goals. You therefore want a finished product in front of you at the end of every iteration. What do we mean by finished product though? Well, if you are iterating over the design of an interface, then that finished product would most likely be something more visual than functional. It would need to be finished though, you wouldn't want the text on a button missing, or some graphics being unrelated placeholders, you want the stakeholders to be able to buy into the vision and therefore everything needs to make sense. As for web applications, they need to be working and all known bugs fixed. If you get to the end of an iteration and that isn't the case, your teams first job is to admit this and communicate their challenges. They can then pick this up later in their retrospective. Keep iterations short If you're focussing on high quality output then keeping to shorter iterations can really help with that focus. By working on a small slice your team can focus on getting that done done. You could choose to tackle the entire checkout in one iteration. However that's a lot of work to complete without receiving feedback. It's also a lot of work to ensure it's done done. It might be easier to take each step in the checkout as an iteration. You might even spend a couple of iterations on each checkout to ensure they're exactly what you need. Communication is key Before, during and after an iteration communication will ideally be maintained between the team and stakeholders. Planning the next chunk of work, giving status updates and presenting a final showcase can all help with communication. The main purpose for communication is to ensure everyone knows they are working towards the same goals. The team need to coordinate with each other. Stakeholders need to know when the team are blocked so they can help as necessary. Stakeholders also need to give feedback and answer the teams questions. Stakeholders won't always understand every layer You may have product owners or other stakeholders in your business join your stand ups. It is the team's responsibility to remember that the stakeholders may have decreasing levels of understanding as you go down the layers. Sometimes it might make sense to split your stand up into two or more based on your audience. You don't need to spend any more time than usual, just exclude stakeholders from the really technical implementation details. You can go into the gory details of your deployment failure with the colleagues who understand your pain and spare the Head of Finance. In order to be respectful to your audience and also provide them understanding, only go into as much detail as they will understand. Using ceremony to get things done Many agile practices introduce ceremony. Scrum often includes estimation planning sessions. We already mentioned stand ups. Hopefully everyone is doing retrospectives. Use showcases to encourage communication, pace and achievement We like to use showcases during and at the end of iterations. Typically occurring every 2-5 days the showcase will bring stakeholders and teams together. The team will have a polished showcase that has been well rehearsed and is aimed at explaining the value of the work they're producing. The team will end with any questions or blockers they have. Explaining goals along with the actual changes made allows the audience to understand the context in which the team have worked and assumptions the team have made. Communication is the main aim and feedback is always desired. Another benefit of the showcase is encouraging pace. When the team knows a showcase is looming, they will learn to ensure they ready their current changes ahead of time into a polished state. It encourages them to think about the goal and the organisation rather than just cutting code. Finally, a showcase is a time for the team and stakeholders to celebrate the new value and learnings that the last iteration provided. When everyone champions the process, the process thrives. Ever changing landscapes Change is acknowledged and accepted by agile practices. At the end of an iteration we do not necessarily have to continue working with the same set of assumptions in the the next. When we build out a prototype and realise it's not going to do the job, we can reprioritise our roadmap and adapt for change. Prioritise by value Never assume that a roadmap has to be set in stone. It should be fluid and adapt as you work through it. Items in the roadmap will need reordering. Sometimes you'll find a capability isn't even required after achieving a goal sooner than expected. Sometimes a problem you thought you had isn't a problem at all. If a team sets out to deliver a particular feature over a number of iterations, but realises after the first iteration that what they've produced is releaseable, release it! Get users using it. You may find that it quells a need and you might find a different goal or capability to work towards after realising you're getting enough value already. Acknowledge scope will change Another thing to remember is that scope can and should be variable. If your project has a hard deadline, you're going to end up reducing your initial scope. That's almost certain. Acknowledge that the image in your head of the final picture at the beginning of a project will never be the same as what you finally produce. Have the destination in sight Not every delivery team will have a project with a deadline. We prefer thinking in a product mindset where delivery is continuous, visions and goals evolving as we go. That said, having a destination in sight is still important. Always be deploying The initial phases of product development may result in UI/UX artefacts like wireframes or component libraries and these won't necessarily be deployable to end users but should be of high quality. You should aim to get through these phases within a few days or a few weeks at most. You want to be producing software that end users can use and provide you feedback in the form of real data whether that be automatically collect metrics or user surveys. In a back office environment we aim to be handing over changes to end users every few days, a week at most. Getting things shipped means you can adapt your plan based on feedback. You'll never go too far down a rabbit hole, at most wasting an iteration or two which is a matter of days. Learn from mistakes with retrospectives Moving fast, your team will make mistakes and that's okay. You'll also have great successes too. No matter your flavour of agile, make sure you have regular retrospectives. You might have one at the end of your sprint if you're practicing Scrum. If you're using Kanban you might have one every 2 weeks. Make sure you hold yourselves to account with your actions too. Keep a log of all retrospectives and their actions, always recap on all actions every retrospective until they no longer have value. Measure success against goals and vision Finally, always measure your success against the vision and goals of your delivery. Yes, shipping a new feature means you've put effort and completed something, but the real success lies in whether that feature provides the desired value. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-01-10"},
{"website": "Made-Tech", "title": "Morale, Recognition & Reward", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/morale-recognition-and-reward/", "abstract": "Morale is closely related to job satisfaction. When morale is high, your team is happier, more productive, and more likely to believe in your organisation's vision. On the flip side, not enough (or any) praise for a job well done, dealing with a difficult clients, or heavy workloads can significantly lessen morale, and sometimes lead to higher employee turnover. In this article we'll discuss the importance of morale and how you can identify when levels are moving in an unfavourable direction, as well as how to give teams a boost by taking the time to recognise, and possibly even reward, their efforts. Recognising low morale Identifying the root cause of low morale can be complex, and there can be a number of contributing factors. It is a situation that no organisation wants to be in as it has significant costs down the line: quality of the product suffers, clients become unhappy and there is no energy within the team. Reasons morale drops There are various reasons for morale dropping within a company, and the following are some of the more common causes. Poor leadership Teams should be given the opportunity to self direct and self organise, so that they're better able to complete tasks, engage with customers and work together. That said, teams still need direction from a leader, and without that, teams can begin to feel unimportant, as though what they're doing has little worth. Poor communication Everyone needs to be kept in the loop with what's going on in the current project. Goals, praise, performance or personal gripes will always arise and need to be communicated within the team. Team members also need to be upfront with each other. The last thing you want is a disengaged workforce where individuals feel left out and undesirable social hierarchies start to form. There's a lot you can do to help promote communication amongst teams, and we've discussed it at length here . Unresolved conflict situations The act of creating software is a very subjective topic and, given the speed at which the software industry moves and which new technologies are introduced, conflict situations are sure to arise as opinions clash. Dealing with these conflicts and others is critical to maintaining morale, as lingering resentment over unresolved conflicts leads to a break down of trust between team mates, and can also lead to a fear of any kind of workplace conflict. Lack of empowerment or autonomy If you do not allow your team to take ownership of a feature, they will take less pride in it. Here at Made we believe that programmers should be responsible for delivering features end to end. This means dealing with all aspects of it from communication with clients to infrastructure to programming. If you only deliver a very small slice of this, you will not appreciate the positive impact that you are having on the problem, which may lead to dissatisfaction. Heavy workloads Bad estimates and poor planning can result in significant workloads on individuals. No one wants to work overtime, especially if they are not being compensated for it. Failing to understand the underlying requirements of a piece of work can have drastic consequences on the amount of work that is required to be completed. An overburdened workforce may also be a sign of significant understaffing, and as such is a problem that needs to be rectified sooner than later. Poor working conditions A team cannot do their best work in an environment not equipped to handle their needs. Without a space that allows them to collaborate, to communicate, to focus and to relax, your team will become increasingly dissatisfied and more prone to distraction. Knowing how and why a team's morale drops is the first step in making their happiness a priority, but beyond simply trying to prevent such a negative outcome, there are plenty of positive moves you can make to raise morale, and it starts with recognising the effort your team is putting in. Recognition A downside of only having individual recognition is that it can introduce competition. To receive recognition, you must excel compared to your colleagues. Unfortunately, this can lead to unhealthy environments and slow degradation of teamwork. By contrast, cultures with only full team recognition lead to a marked increase in amounts of cooperation and collaboration, as this is the only way to achieve success. The downside of team-only recognition is that it can go awry when individuals begin to feel that underperformers are receiving just as much reward for their actions. Another solution seen in other cultures is never to recognise anyone for fear of causing these problems. It is important to consider that this will begin to cause individuals to feel unvalued. Although we value individual recognition, we favour team recognition. We also believe that people should be recognised primarily through ad-hoc channels, by their peers, not by whoever sits above them in the hierarchy. We use continuous feedback as a platform for this recognition. It's important to recognise the highly positive impact that recognising teams and individuals has on morale. Also, and potentially more importantly, the disturbing implications of poorly handling mistakes as well. Handle mistakes in a positive way In a high safety, high trust environment, with practices such as continuous feedback and retrospectives, the detection of a mistake should not be seen as an opportunity for a chastising or otherwise attacking the individual(s) responsible for that mistake. Morale can be boosted during periods of adversity too, through robust, mature methods for picking up the pieces after a mistake has been made. Using mistakes as opportunities to engage in positive learning and improvement experiences, rather than downtrodden experiences, makes people feel both happier and supported in their role. Not to mention that as a leader, you can be more effective in your role when individuals feel they can share their mistakes openly and freely with everyone else. The reason is simple: the entire team can learn how to avoid making that mistake in future. By contrast, in environments where individuals are incentivised to cover their mistakes, from fear of retribution, then the wider team misses out on the learnings gained. Communicate openly Agile values communication with customers highly. A team should be driven to do the things that build customer happiness, with the hope that they are delighted by the team's efforts. Since this is the case, teams that work in close collaboration with customers have the benefit of receiving that praise directly. This praise, when received as part of a tight feedback loop with the customer, can be used as an early warning sign that something is not quite right when the amount of praise decreases. Peer recognition Recognition has a half-life. One consideration of solely using annual reviews as an opportunity for recognition and praise is that these cultures risk leading to large dips in morale. As an alternative, spreading recognition throughout the year, with a platform for peer-to-peer recognition in place ensures there is never the opportunity for such dips. At Made Tech, we have \"Made Merits\", a form of Karma system that is used to reward good deeds on a peer-to-peer basis. A consideration of using such a platform is that it can lead to a dip in intrinsic motivation, this can be initially manifested by \"will you give me a Merit if I do that\" but also demotivation when a Merit isn't received. While we believe that \"Made Merits\" are a harmless part of our culture, it is possible to draw parallels between Merits and other extrinsic motivators. Recognise dependence on recognition, an extrinsic motivator, and ensuring they are also finding their work intrinsically rewarding is something that only the individual team member can do by themselves. Reward Having recognised and acknowledged the great work your team has done, it's time to talk about the ways in which teams and individuals can be rewarded. As we see it, rewards can be either extrinsic, something that either is or costs money, or intrinsic, something that lends itself to helping the people being rewarded feeling fulfilled and happy. Intrinsic Arguably the more powerful type of reward is the intrinsic reward. It's also the trickier to give, since these rewards are something teams need to feel, rather than be given, and you can't force people to feel a certain way. Purpose By making a point of monitoring and keeping morale up, along with creating communicative environments where their hard work can be recognised, allowing your team to structure the way they work, giving them ownership of delivery and many of the other practices we've discussed, the members of your team will feel a sense of purpose, a sense that they, and the work they're doing, matters. With a sense of purpose, a team knows why what they're doing is important, they're driven to do it, and to do it well. Understanding how their work feeds into the wider company objectives plays a big part of instilling a sense of purpose, and we found it particularly useful to define a company mission in order to clarify why the company exists and what we're trying to achieve. Each of our teams knows our company mission is to improve software delivery in every organisation, meaning every member of those teams believes in our mission, and anybody joining our team wants to help us achieve it. Freedom to learn We're proud of the work we do with our customers, but every software engineer loves having the freedom to go off and get stuck into a technology that interests them, and the two things don't always align. Rather than snuffing that thirst for knowledge out, giving your team the space to pursue and share their interests is an excellent way to keep morale up, and may even yield benefits for the wider organisation down the line. At Made Tech, the entire team often engages in a number of non customer focussed activities, such as code dojos and hack days, where we set aside an hour a week or an entire day every month or so for everyone to do something fun and interesting that doesn't necessarily have to have commercial benefits. This creates an environment where every member of the team knows they have the freedom to suggest other ways in which to promote learning, and that their team will respond positively to their suggestions. A recent example is the \"Code Roast\", a variation on the traditional practice of code reviews, whereby a team member will pull out a piece of code they're particularly not proud of, present it to the rest of the team and then spend an hour pointing out its flaws, and then working together to improve it. Fresh challenges For organisations like ours, who work on a variety of different projects for different customers, having a team stay on one project for months on end can become tiresome, leading to a loss of productivity and enthusiasm. While it's beneficial to have somebody on the project throughout the engagement, if only to help build and maintain a relationship with the customer, we've found it important to let our teams know that, if they have a burning desire to do so, they have the opportunity to change things up at semi-regular intervals throughout the year. A team member can choose to either stick, and stay on the project they're on, or twist, so that they can pursue a fresh challenge. Knowing that you're not shackled to a particular project for what could be years is liberating, and keeps things feeling fresh. Extrinsic On the other side of the coin are extrinsic rewards. These are things the company can give to teams to both celebrate their work, and also to maintain a healthy level of morale. While we're not advocating showering them with gifts, we do see benefits to treating your team beyond just material gains. The following are two examples of situations we've felt it was important to hand out extrinsic reward; there are many other appropriate ways to reward your organisation, you just need to find what works best for you, your team and your organisation. Company Retreat Late last year, having had a particularly successful year which saw the company grow in many positive ways, thanks to the combined efforts of the entire Made Tech team, it was decided that, for the first time since the company was founded, everybody would be taken on a company retreat. This took the shape of a 5 day break in Spain, where we'd spend a few hours each day building a product we'd use internally, and then hanging out in the evenings. This could be seen as both an extrinisic and intrinsic reward, as everything was paid for by the company, but we used the building of the project as another opportunity to learn, and the entire experience was a great team building activity. Celebrating success Whether it's successfully launching a project, completing an engagement with a customer, or your team has reached a landmark point during the engagement, it's important to recognise these moments and celebrate them. These moments only come about because the entire team pulled together to produce the best work they possibly could, and letting those efforts go unnoticed is a sure fire way to leave your team feeling deflated. Whether it's something as simple as a trip to the pub for a few team drinks, or something more extravagant, celebrate the achievements your team is responsible for. Conclusion It goes without saying that your team is vitally important. Without them, nothing gets done, so making sure your team feels happy, fulfilled and committed to their goals should be near the top of your organisation's list of priorities. By taking the time to check in with your team and gauge how they're doing, listening to what they have to say and recognising the efforts they're putting in, you're creating a positive environment for everyone within the organisation. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-01-19"},
{"website": "Made-Tech", "title": "9 Techniques to Support and Improve Software Quality", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/9-techniques-to-support-and-improve-software-quality/", "abstract": "Most software systems will suffer from a deterioration of quality over time. Codebases become bloated, software is changed to solve problems nobody knew existed when it was initially written, and the cost of change keeps increasing. Of course, it doesn't need to be this way, though conscious action is necessary to avoid software systems from deteriorating over time. Here we provide 9 techniques that we've implemented in a number of organisations to help ensure that the code quality of software applications increases over their lifespans. 1: Beware the bit rot Bit rot, also known as code rot, software rot, software decay, and other similar terms, is the observation that software appears to rot over time, even when no changes are made to it. Software doesn't run in isolation, it's reliant on physical hardware, an operating system, libraries, and often, some third party services that it talks to. If it's a web application, it's going to be delivered via a web browser on a client machine, of which you often have little or no control over. The environment in which the software runs is constantly evolving. It is therefore realistic to expect that some ongoing efforts will be needed to ensure the software keeps pace with its environment. 2: Don’t throw software over the wall to a ‘maintenance team’ Many organisations split their engineering teams, choosing to have more experienced engineers working on the tricky greenfield products, who then throw their wares over the wall to a less experience maintenance team. A structure such as this is likely to result in a reduction of quality over time, both in staffing a team with less experienced engineers, and in the removal of any sort of ownership. A model we've found to work better involves pulling together teams with differing levels of skills. More experienced engineers often enjoy the opportunity to mentor less experienced team members, and it ensures that experience is spread equally among teams. If the nature of the maintenance work on your product suite means that you feel more experienced engineers aren't delivering enough value in delivering this work, it's worth considering how the work can be changed. If there's lots of manual, repetitive work in maintenance – can your maintenance team build tools to automate this? 3: Peer review all the things This applies not just in relation to software in maintenance mode, but software delivery in general. We'd heartily recommend moving to a pull request type workflow where changes through the whole lifecycle of a software product are reviewed by another team member. Ensuring another set of eyes is present on every change can help catch issues sooner, and can encourage healthy discussion on the best way to achieve code-level objectives. See also: pair programming . 4: Use automated code quality tools Using automated quality tools to keep an eye on every change ever made to the software: Linting tools are useful to keep styles consistent. Code with a consistent style looks better maintained, and can help steer developers from being tempted to hack a quick fix in Automated testing can be used to spot regressions in existing features as soon as a change is made- Code coverage tools can provide a caveated metric as to whether your testing efforts are tailing off Duplication detectors can help to identify where the same block of code is used in multiple places in the application, hinting that a refactor may be warranted 5: Think of Products and not Projects Too often people consider software delivery as a project. At the end of the project the software system will be 'done' and barring the odd bug fix, no further work will be necessary. Seldom is this the case. There are a number of studies that show that more often the bulk of the cost of a software application is borne in 'maintenance' – the period after the initial launch. We'd recommend adopting a Product mindset to your software deliveries – shipping small, incremental improvements often, rather than fixating on a drop-dead shipping date for Gold Master Version 1.0. 6: Be prepared to invest Building on the Product vs. Project point, you should be prepared to invest in maintaining your software through its life. Not just in adding new features and fixing visible bugs, but in providing enough capacity to allow engineers to continue to evolve the technical architecture of the software system to meet its current needs. While this may result in increased operational expenditure in the short term, it removes the need for larger capital expenditure events when software systems reach a point where they need to be replaced rather than evolved, due to a lack of upkeep. 7: Know when to refactor As an application evolves, you'll often see new features added, the introduction of entirely new capabilities, and improvements to existing functions. Often the scope of an application can change far beyond its original goal. With this in mind, it's important to be able to make well informed decisions as to when to refactor areas of the application, over forcing new behaviour on top of existing features. Refactoring may involve extracting out parts of a software system in to a new component or new software system, or replacing a part of the system that is no longer fit for purpose. 8: Keep on top of technical debt Technical debt is a normal occurrence of just about every software development project. At times it is the right course of action to cut a corner to achieve a short term aim. That said, in aiming to improve the quality of an application over time, it's important to stay on top of the debt. Engineering teams should be comfortable in prioritising the pay-off of technical debt as often as necessary. And Product Owners should be open to debate in pushing down the priority of new feature development to facilitate this – understanding that too much unchecked short term thinking causes significant harm in the long term. 9: Monitor the application in production Monitoring the application in production can provide useful insights in to how the application performs in the real world. It can help identify common error conditions, and can highlight areas of the application that are less performant. Correlating a deterioration in either of these metrics to a change in the software can be a useful thing to catch early, allowing a fix to be issued. Summary Avoiding deterioration of software quality over time requires a conscious effort, both on the part of the Product Owner and the engineering team. The mindset should be shifted toward an ongoing product investment, rather than an upfront big-bang project delivery. It's important to ensure a sufficiently experienced team takes responsibility for ongoing engineering efforts, and to take a mature approach to paying off technical debt and refactoring as often as possible. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-01-24"},
{"website": "Made-Tech", "title": "Keeping code quality high", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/keeping-code-quality-high/", "abstract": "Code quality is a term that is often thrown around in the software engineering industry. And like the art of coding itself, it is very subjective and its true meaning will differ depending on an individual, or a team's beliefs. But at its heart most engineers and teams would agree that good quality code is easy to read, well tested, and maintainable in the long term. But how do we achieve this? It starts with a feature Often a client will present what they see as the solution to a problem, guised as a feature request. However when a team takes their first slice into this feature, they should not do so blindly. If there are big questions about functionality, or end-user requirements, these unknowns should be uncovered. This will enable the team to build a better solution, with the least amont of code possible. Ultimately, this alone will increase code quality; less code means a smaller codebase which leads to a more managable system in the long term. From the outset understanding the shape of a feature keeps the team laser focused, meaning everyone has a clear definition of success for a task. Although an initial slice may have been defined, teams should not be in fear of changing it's requirements if during the course of an iteration they find a cleaner approach. Next we commit At Made Tech, we use pull requests on all of our projects, meaning we work on new code in a separate branch and request review before merging it and releasing it. We deliberately keep changesets small, releasing small increments of the feature early and often. We work this into our version control and code review workflow in a practice we call \"Single Responsibility Pull Requests\". This practice avoids \"Big Bang\" deployments, where many changes have been made since the last production deploy, introducing many points of potential failure, rather than just one. Releasing continuously hopefully means our pathway to production is always clear. This is a fundamental principle behind continuous delivery . Furthermore, smaller changesets are easier to digest, so when we request code review, reviewers are able to do a better job. They have more room to comment on particular areas, asking questions or suggesting improvements, if they only have to look over 10 lines, rather than 100. We can be sure that they properly understand the code we've implemented if they only have to look at a small chunk. Many changes over many files with many concerns can get confusing. The reviewer doesn't necessarily know what all the parts do and making sense of the whole thing can be intimidating. Massive cognitive overload leads to poor reviews. Poor reviews leads to bad code getting into the codebase, reducing quality. Make sure you break large features down into smaller slices. Write these in a way that they can be deployed individually rather than as a massive chunk whenever possible, and don't block the route to production. Request code reviews continuously during development to ensure reviewers can understand the feature throughout, rather than trying to make sense of it at the end, and always make sure they're satisfied before merging it. Raising the bar Keeping code quality high through manual code review is great and a really worthwhile practice, but it is often not so good for static code analysis. Static code analysis is another vital way we at Made Tech keep our code quality high, and machines are much better at doing this than humans. Static analysis can be broken down into a number of areas like complexity, style and security, and there are many tools that deliver this functionality. For raising the standard of your codebase the primary focus should be ABC complexity. ABC complexity is counting the number if assignments, branches and conditions within a method or function. A high ABC metric is a good indicator that the code is doing too much and should be broken down in to smaller, easier to understand chunks. It's important to stress that keeping ABC complexity low doesn't always mean you have an easy to understand codebase. The metric also doesn't reward terse code, instead it prefers simple code. Arguably our most favoured practice at Made Tech is TDD, test driven development, which is writing tests before writing code. This has broad benefits across code, but especially code quality. Like having a defined objective for a task, TDD keeps you focussed on accomplishing it, and ensures you're not writing perfunctory code. Every line you write is necessary to bring the feature closer to completion. This not only ensures you're writing the right code, it ensures the code you write is maintainable over time, preventing regressions down the line. Engineers in future don't need to be afraid when making changes the codebase if they're confident that it's well tested. They can trust that if they break anything, the test suite will fail, and they'll be able to fix it before it gets into a production environment. To accomplish this we include an accompanying test in every PR. Keeping it high We are committed to our projects and maintain them diligently over their lifetime, so it is in our best interests to invest time in quality. Although ensuring projects start off properly is paramount, making sure they stay in good shape and quality remains high is even more important, so we employ a number of practices and tools to enforce standards over time. One way we do this is through style linting. Style linting is another form of static analysis that requires your code to be written in a certain way, as predefined by either a lauguage, or an opinionated community standard. For example at Made Tech we use StyleLint, ESLint, and Rubocop among others. This means over the lifetime of a project the code can only be written in a certain way, so it will stay consistent. Linters will often suggest better ways of doing things, progressively upskilling the team in a language. This is extremely helpful when onboarding team members who are less familiar with a language. It also makes people feel they are delivering high quality code whilst still learning. However, style linting should not be seen as a silver bullet, as it doesn't always lead to high performant code, sometimes aesthetics have to be sacrificed for efficiency. Additionally, style configuration is highly subjective. At the end of the day it comes down to an individual's beliefs. People in the same team may have different ideas about how code should look, and you may even rely on third party defaults for code style rather than defining your own. What's important is that code remains consistent. Forcing developers to run all these code quality tools themselves can be a drag, but we want to ensure they're always run before code is merged in, so we automate their inclusion in our workflow. We use PaaS services like CircleCI to automatically run our test suites, static analysis tools and security testing against commits and pull requests in Github. This means code reviewers don't need to worry about these elements, as they have to succeed in order for the PR to be eligible for merging. Additionally, running these quality assurances means an engineers approach to code continuously improves. Spotting failures due to bad code soon becomes second nature. This then has the benefit that, as they maintain the project, when they encounter new areas of the project they are able to bring that code up to scratch, or at least leave it cleaner than they found it . Essentially, code should not be seen as sacred and while an engineer can be protective of code they have written they should also not be afraid of deleting or even replacing code when the time comes. As with code bases, so should teams be ever changing, since this also leads to greater code quality. A fresh set of eyes will spot problems an incumbent engineer will just live with, if they have noticed them at all. Conclusion First and foremost, a well tested, easy to read codebase is easy to maintain. Employ automated quality tools to ensure the code your team writes today and the code they write next year is excellent. Deliver features one chunk at a time to ensure code reviews are diligent and comprehensive, without demanding high cognitive load. Write automated tests and run them on every change to defend against regressions and let your tests document functionality, rather than trying to maintain documentation that can go stale. Engineers often consider code they wrote last month to be the worst in the world. That will probably never change, because as engineers we're always learning and improving. However, if quality is always kept high, whilst beliefs may change, we can always be proud of the code we have written. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-01-31"},
{"website": "Made-Tech", "title": "How to give your application the support it needs", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/how-to-give-your-application-the-support-it-needs/", "abstract": "For many organisations with their own software application, whether it be a website, an internal tool or something else entirely, managing said application can be a source of frustration, particularly in the absence of an internal software team. For applications built by external teams, what typically happens is the supplier and the customer part ways when the project is deemed complete, and this is where problems begin to arise. The accepted wisdom is that no software application is ever \"complete\" since, in the long term, both software and hardware technology evolves at a constantly increasing pace, forcing the need for applications to either keep up or become stagnant. On a shorter scale, no matter the application, unforeseen issues will inevitably make themselves known and require addressing. Many suppliers will offer a limited amount of after sales support, either through a retainer or on a case by case basis but, as new projects come in, those suppliers may find their focus being pulled elsewhere, leading to frustration on both sides. If the application has been built internally, focus, and the connotations of the word \"complete\", can again be the primary issues in maintaining the software. As the project draws to a close, teams begin to shrink as resources are diverted to concentrate on other business objectives, leaving, at best, a skeleton crew behind to manage things. The application is then at risk of the team either being too short staffed to cope with emergencies in a timely fashion, or becoming apathetic to the project. One way to reduce the risk of these scenarios occurring is to outsource the management of your application to a dedicated team who'll work with you to keep your application healthy and help it grow. Reasons to outsource Application improvements The most common reason to outsource is to ensure you have a team ready to help improve your application in a number of ways, from adding new functionality and features, to dealing with bugs and issues as they crop up. Perhaps less well known, but just as important, is ensuring you don't fall prey to \"bit rot\" . Ultimately, dealing with bit rot simply means dedicating maintenance time to making sure the environment your application runs in isn't negatively affecting it, whether it's the browser your users are using to interface with the application, or the software running on the machine that houses your application. Lack of internal resources As mentioned above, when an application is built internally and reaches a point it could be considered complete, the team that built it is then typically given another project important to their organisation's goals, leaving the application to grow stale. Break in case of emergency Over the lifetime of an application, it will likely encounter a handful of crises that will either severely limit its ability to function, or make it completely unavailable to its users. Whether it's a data center going down, or breaking changes made by a recent update, you need a team to be able to respond to and deal with these situations quickly. Parted ways with a previous team When reaching the end of an engagement with the team who've built your application, it's not a given that they'll be able to support it to the level you require. If they don't have the manpower to dedicate to your application, it's likely they'll already be committed to other projects. On the flip side, if you've outsourced maintenance to an external team, you may find that things simply aren't working out and they're not providing the value you expect. If this is the case, it's time to find a new team. Finding the right team So how do you go about finding the right team? There are a number of factors to consider when researching potential suppliers: Experience There are a lot of businesses out there that offer support services, and it can be tempting to hand responsibility over to a cheaper option. What you need to be wary of is handing it over to a team who'll simply keep your application in a perpetual state of maintenance, and never evolves it into something better. You want a skilled team who have experience dealing with applications like yours, and who can help do more than just maintain it. Level of service Find out what level of service you can expect from a potential team, you need to know the number of hours each week your budget will cover, and whether the agreement covers 24/7 emergency support. Additionally, look for teams who communicate well and often – you're handing responsibility for the wellbeing of your application to a third party, but for the sake of transparency you want to remain an active part of development. Availability One of the benefits of working in the software industry is the greater freedom it provides in allowing teams of developers to work remotely from anywhere with a stable internet connection, whether that's from a coffee shop, another city or even another country. Unfortunately, this isn't the case in many other industries so, when bringing in a support team, it's useful to know that physical location, or even time zones, won't be a barrier to getting good work done. Communication Though the team you find will be working with you in a support capacity, it's likely there'll be numerous times when you need to get hold of them at a moment's notice. More than that, you need to know that the team, over time, are delivering results. Find teams who provide an open and active channel for you to engage with them in (Slack, Skype etc) on a daily basis, and who take the time to showcase the work they've done for you every couple of weeks. Collaborators When you've zeroed in on a few likely candidates and have begun making initial enquiries, you ought to be looking for a team you can build a relationship with, who'll do more than simply the bare minimum to keep your application functional. In our experience, getting to know an organisation and understanding their wider objectives lets us make more informed decisions around the work we do with them. For example, in a situation where an organisation wants to add a new feature, we seek to understand what the driving force behind the decision to implement that feature is, and what we can do to better achieve that goal. Summary Making the decision to outsource management of your application to a third party is an important one, so it's crucial to find the right team to support it. Look for a team who can meet the needs of your organisation and work with you to evolve your application. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-02-02"},
{"website": "Made-Tech", "title": "Overcoming Common Challenges with Offshore Development", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/overcoming-common-challenges-with-offshore-development-and-support/", "abstract": "It's not a particularly well kept secret that there can be challenges organisations face when offshoring software development, both for greenfield builds and for products running in support and maintenance. While the headline cost savings for organisations based in expensive locations can look particularly attractive, it's important to understand the hidden costs and overheads involved in offloading a workload far afield. Overlap in Working Hours One of the most obvious challenges can be the overlap in working hours. Particularly when crossing several timezones, it's possible to end up with only an hour or two a day of overlapping working hours. We've often observed customers on particularly early and late calls to coordinate work. Some offshoring partners purport to alter their working hours to more closely match that of the customer. One thing to check here is that this applies to the whole team who will be delivering your application. There have been cases where the more seasoned team members work a regular local working day, with the more entry-level members manning the fort during the more unsociable local hours. Tips: Ensure there's at least a couple of hours overlap in regular working hours. Be aware that you will need to adapt your workflow to accommodate a small overlap in time. Consider favouring vendors in near timezones. Increased Communication Overhead A common source of day-to-day frustration can be the increased communication overhead, manifested both in poor means of communication (e.g. you can only talk to a team via a ticketing system) and language and cultural barriers. Particularly for organisations that promote high-communication environments, effort is likely to be needed to discover processes and ways of working to promote good communication with off-shored teams. Being aware of cultural differences and working with both teams to encourage a positive approach to embracing and learning from the cultural differences. Tips: Ensure the vendor is comfortable using common communication tools. Ensure the vendor is able to provide a team with sufficient communication skills. Insist on meeting with the team who will be servicing you, rather than just the sales people. Be aware that communication is likely to have more barriers when compared to working with your local team. Look for ways to 'bond' the local and remote teams through more social activities. Consider things like remote code dojos. Understanding the Domain It's important to not underestimate the value of domain knowledge in delivering software. Having engineers working in isolation of commercial objectives seldom leads to smart solutions that really deliver value back to the organisation. Transferring knowledge of a business domain across a far-reaching organisational boundary can be particularly challenging. Accept it will take time for people to gain a good understanding of your domain, particularly if they're not working hand-in-hand with your team. Tips: Try to engage at a strategic level with any development partner. Work with them to ensure they understand the commercial problems you're trying to solve. Take every opportunity to talk to people at all levels about your strategic and commercial goals. Avoid solution problems . Steer clear of 'ticket factory' type vendors who focus more on how quickly they can mark a request as resolved over understanding the business goal they're solving. Quality of Work It's certainly not a problem that's isolated to offshore development – there are also plenty of nearshore vendors who offer dubious levels of quality in their software delivery. That said, quality is one of the most commonly cited problems with offshoring. It's important to understand the cause of the quality issues, whether they're purely rooted in a vendor not having a team who is sufficiently mature in modern software delivery, or, as is often the case, it's an upstream problem, perhaps rooted in poor communication and understanding. Tips: Look for vendors who can demonstrate good credentials in modern software delivery practices such as Test-Driven Development and Continuous Delivery. Ensure you have sufficient skills to evaluate a vendor from an engineering perspective. Consider leveraging a trusted contact to help evaluate if you don't have the skills in-house. If you experience quality issues, try to understand where they're rooted. The root cause is not always in engineering capability. Shared Understanding of Expectations When crossing any organisational boundary, different expectations may exist for what 'good' looks like, particularly things such as quality, security, and user experience. A common answer to this can be to try to document everything in very fine-grained detail, passing large specification documents over the boundary, which in-turn can have many knock-on effects to the speed of delivery, ownership, and subsequently quality. Tips: Ensure expectations are well understood by looking to share values rather than large specification documents. Encourage teams to collaborate with one another, and work together to reach shared solutions. Cost Savings not Easily Realised With offshoring sometimes cutting local rates ten-fold, it provides some particularly compelling headline cost reductions.That said, it may be naive to assume that this cost saving will be directly realised. Strong software engineers bring far more to the table than just pure programming horsepower: engaging with stakeholders, interpreting commercial goals, improving software architectures etc. It's hard to perform a direct day rate comparison. It's important to note that while day rates can provide a seemingly reasonable cost comparison, the output, both in quality and ultimately quantity can vary hugely between teams. We've observed offshore teams staffed with 40+ people delivering a comparable output to a team of perhaps 5-strong local team. Particularly where day rates are much lower, it's a sadly common pattern to see more bodies being thrown at an offshored software project to speed things up, something which has been shown to often have the opposite effect . As well as eroding cost savings, this can also bring about many additional challenges that aggressively throwing people at a problem can bring. Tips: Be sure the work is a good fit for offshoring. Some work that requires higher levels of interaction may not be easily achieved. Don't be tempted to just throw people at a problem to speed things up. Shared Engineering Values Where local teams have strong engineering values, there can be challenges in sharing these values with remote teams. For example, local organisations who particularly value TDD, or a particular style of TDD, may be subject to frustrations when working with a remote team who don't fully embrace a test-first workflow, or who have dramatically different styles. Likewise for larger differences in approach to work. Teams who value releasing regularly, or working in small iterations, are likely to have tension with teams who don't equally embrace Continuous Delivery and Agile workflows. Even if a potential supplier promises that a remote team can work in whatever way the local team prefers, there is likely to be at best an onboarding process as the team learn different ways of working. Teams who are particularly mature at approaches such as Continuous Delivery have likely been finessing their craft and embedding their beliefs for a number of years. Tips: Look for ways to align cultural and engineering values across the teams. Look to work with teams who come from a similar engineering viewpoint. Ownership of Application Architecture It can be disempowering for a team to have another team to dictate an architecture on them. Without the team delivering the code being empowered to make architectural decisions, their engagement and feeling of ownership of what they're delivering is likely to be lowered. When this is the case, you're likely to observe a decrease in performance and quality. Tips: Look to work with teams who are technically strong enough to own the architecture of what they're delivering. Empower teams to own their end-to-end delivery. Subservient Teams We've often observed that the client/offshore supplier relationship can demonstrate result in the offshore supplier team becoming highly subservient. This can be a dangerous situation. If the team who are delivering your software system aren't able to have open and honest conversations, and are afraid to push a strong engineering agenda, it's likely that undesirable architectural decisions will be made to appease seemingly immovable requirements.In the short team, some business folk may see it as desirable to work with a delivery team who don't question anything. However, this team is likely to be quietly accumulating technical debt under the hood, and may not be engaging to their full capacity if they're not comfortable to ask challenging questions. Tips: To keep an application's codebase maintainable and in good health, it's important to work with teams who are able to drive a strong engineering agenda. Look for teams who collaborate with the stakeholders, rather than blindly execute against requirements. Demotivation of Local Team We've seen cases where a local team who were initially hired for their engineering expertise have their role shifted to now effectively become Managers / Product Owners for an offshored development team. Additionally, due to communication overheads, speed and quality of delivery, and an inability to enact change, we've observed wider local teams become frustrated when working with offshore partners. This can erode confidence in the organisation's ability to delivery technology products, which can in turn lead to people seeking ways to bypass the organisation's engineering capability. Tips: Ensure people across the organisation who will be affected by a decision to offshore are bought in to, or at least understand the commercial strategy behind the decision. Be aware that many people's roles may have additional overhead placed on them to make the offshoring partnership work. Keep a close eye on measures such as cycle time and internal satisfaction with engineering delivery. Offshoring remains an attractive proposition, particularly for the headline cost savings. And there are certainly examples of organisations reaping benefits from offshoring part of their workload. Some organisations who have made a real success from offshoring have built their own capability in a country with lower labour rates. This has allowed them to bring their own values in building the team, ensuring the primary challenges they're dealing with are the communication overheads, rather than a lack of alignment. For organisations to make a success in offshoring across organisational boundaries requires a strong evaluation process, perhaps involving moving a discrete piece of work as a trial, and a strong commitment to invest in the longer term to continue to nurture and grow the relationship with the remote team. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-02-23"},
{"website": "Made-Tech", "title": "Recruiting a tech team", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/recruiting-a-tech-team/", "abstract": "Recruiting the right group of people is one of the most important parts of building a top software delivery team. In this article, we take a look at some things you should consider whilst recruiting, and a few things that you should try to avoid. Create a cohesive culture A key component of building an effective software delivery team is creating a cohesive culture. A cohesive culture is achieved when a team feels empowered to participate, can collaborate effectively and gets recognition for its successes. To achieve this you need the right leadership, the right mix of experience, and people with character traits that encourage cohesion. When recruiting, you should be thinking about how to achieve a cohesive culture. Often people recruit with specific technical skills in mind, but we've found that it's just as important to consider softer skills such as: Is this person a team player? Are they able to communicate effectively? Do they take responsibility or blame others? Do they have a positive outlook? Although there isn't a 'one size fits all' approach to creating a cohesive culture, there are certainly signs that you can look out for during an interview process and, when you see them, question whether the candidate is somebody who will help encourage cohesion, or fight against it. Don’t hire rockstars, ninjas or self-proclaimed superstars Everybody wants to hire incredibly talented people, but never hire anyone who considers themselves a 'rockstar' engineer. It's a sign of arrogance and an inflated ego, and they're a sure fire way to create a divided team. Of course, you want exceptional people on your team, just make sure they're grounded and able to put their skills to good use, improving the team. Leverage network It's nearly always better to recruit through your existing network. Either through people you have worked with previously, or through contacts your team have within the industry. Your team should understand the needs of the organisation and what it's like to work there. Their relationship with a candidate will mean they already know a lot more than can be discovered during an interview process. These kind of network referrals are the best indication you'll get as to whether or not a candidate will be successful, so favour them over other recruitment sources. Referral bonuses It has become common for organisations to offer incentives to staff to help encourage recruitment referrals. In our experience, awesome people will automatically want to bring their most talented friends and acquaintances along. Offering incentives can help and may result in a few additional hires, but it's questionable as to whether carrot and stick motivation ultimately leads to stronger teams. Our advice would be to focus on building a great culture, that your team are proud of, and their network of acquaintances will be eager to join. You can still thank people for referrals but, hopefully, you'll find that by flipping the motivation model from extrinsic to intrinsic, you'll end up in a happier place. Strive for diversity It’s no secret that many software teams struggle with diversity. While there are some factors limiting progress, diversity is something that needs to be taken into account when recruiting. There are steps that you can take, such as: Helping people to reduce unconscious bias Capturing and reviewing data that highlights diversity challenges within your organisation Scrutinising job advertisements and internal practices to ensure they don't unintentionally discourage certain demographics. We've made significant progress in recent years, but our industry has a long way to go in terms of diversity, and there's a lot we can do to create an environment that is welcoming of people from any walk of life. Bear this in mind when recruiting! Keep it interesting, increase retention One of the challenges you'll face after you've recruited some good people is keeping them around. If they are talented and based in a decent location, then they are likely to have many opportunities open to them. It's common to see companies offer perks and incentives to help encourage employees to stick around. In our experience, the most important factor is ensuring they can keep on learning, improving and getting better. This means having work that is challenging, and is going to push them to improve every week. If this dries up and day-to-day becomes mundane, you'll often find people won't stick around. Above-market salaries Where possible, you should look to take money off the table and pay above industry wages. This often helps to attract a higher calibre of individual (providing you're recruiting well!) and shows your team that you value them. This approach is particularly relevant in fields which are highly leveraged and where software delivery is generating a good ROI. When this is the case, it can make sense to pay well above market rates and get above-average performers, as it can be a win for the organisation and a win for the employee. Of course, this approach is dependent on an organisation being in a very strong and stable financial position, which tends to limit the number of organisations that can do this. Don’t use contractors The software industry has a large number of people who work on a contract basis. In our experience, it's inadvisable to build software teams comprised primarily of contractors. We have found the most efficient teams to be those who have worked together for extended periods of time. This typically means full-time staff. We tend to find contract staff can take a short-term view, and this can mean your delivery teams are in a constant state of flux. When you have to use contractors, try to use them sparingly. We've seen technology teams 100+ strong, comprised of over 90% contractors and struggling to deliver. We would advise keeping contractor levels to a minimum at all times. Firing underperformers It's human nature to avoid making difficult decisions. Nobody wants to be fired or to have to tell someone that they no longer have a job. However, when faced with this scenario, it's vital that you act quickly and let them go as soon as possible. It's a sad truth that many organisations end up carrying underperforming team members for years. This can have a big impact on morale and wider team performance, so it's important that the situation is dealt with. Embrace juniors If you're hiring junior team members, it's important that you factor in some of the challenges that this can bring. Juniors often require significantly more support than expected, which can impede the team and impact ability to ship quickly. Sometimes you need to slow down in order to speed up, and we think this is particularly relevant when onboarding junior members into a team. Make sure your environment provides plenty of support for juniors and opportunities to learn. Things like mentors, pair programming, regular code dojos or code katas can all help, and over time the level of support and guidance required should drop significantly. Colocate or remote Nowadays it's common to see companies offering remote roles. We see some significant benefits to this, such as a much wider talent pool, ability to get more focused time and a better work-life balance. However, there are trade offs. One example is that it can be far more difficult to achieve a cohesive culture team when you've got people working remotely. We've found a good balance is to have people working remotely part-time, so maybe two or three days per week. This has given us some of the benefits of remote working, with some of the upsides of colocation and has worked well. Onboard effectively The first few weeks with a new hire is a crucial time. They are going to decide during those weeks whether the company is actually a good fit and whether they have made the right choice. Ensure you set aside ample time to onboard new starters properly. Make them feel welcome and help them to get to know the team and understand how things work. Set very clear objectives, so they know what they are working towards and what criteria you've got for reviewing their performance during their probation period. Talk with them regularly and provide them with lots of encouragement and direction around areas they are doing well and could be doing better. Retrospecting the interview process As with any process, it's important to frequently assess performance and find ways to do things better. Recruitment is no different. Set aside time to talk to candidates about the recruitment process, to find out what has worked well and not so well for them. We've explored running facilitated retrospectives with candidates we've employed and rejected. This has helped us understand what the next evolution of an interview process might look like and steps we would need to take to get there. Conclusion Recruiting a strong software delivery team is tough. It requires a huge amount of effort, starting with finding the right people, then convincing them to join and moulding them into a cohesive team that works well together. If you can achieve this, then you've done extremely well, and you are well on your way to building a high-performance software team. You'll find that once you've got the first batch of great people onboard, it should start to get easier. Talented and motivated individuals tend to attract other talented and motivated individuals. With strong management, your team should continue to grow and improve for years to come. Good luck! Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-02-23"},
{"website": "Made-Tech", "title": "The importance of software support and maintenance for all organisations", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/the-importance-of-software-support-and-maintenance-for-all-organisations/", "abstract": "It's rare to encounter an organisation where software isn't an important aspect of their day-to-day operation. Whether it's a small business with a simple website, an international retailer with an e-commerce store and a Warehouse Management System, or a charity organisation collecting, storing and reporting environmental data, at some point, each of those organisations will need to engage on some level with a piece of software in order to ensure its smooth operation. Organisations with a software team More straightforward than the alternative is for an organisation to already have an internal software team. Their application may have been built entirely by that team, or with the help of an external supplier. In either case, the internal team has domain knowledge of the application, which will be invaluable when investigating issues. Problems begin to arise when the building of the application can be considered \"complete\". What we've seen happen is for organisations to put their software team to work on a project in another area of the business, whilst recognising that the original application needs maintaining. One or two developers from the project are then either left behind for this purpose or, worse, it's another item on their growing list of responsibilities. For those developers left holding the baby, it can be demotivating to see colleagues go off and begin work on an exciting new project. When it's just another responsibility, split focus is an issue; a developer will naturally prioritise the things they think need the most attention, and that will often be something other than the continued running of an application. In both cases, knowledge silos are being created, making it much harder to maintain the project in the absence of those developers. Organisations without a software team In many instances, software applications will have been built by an external supplier over the course of a lengthy engagement, and said supplier will often provide some degree of post-engagement support. However, there are a number of ways in which this type of relationship can break down and lead to the organisation and the supplier parting ways. For example, a lack of capacity. The supplier may find they ultimately don't have the capacity to offer reliable support. Their workforce will be committed to other projects and they'll only have a minimal amount of time to handle requests from the organisation. This then leads to urgent requests going unanswered for hours or even days at a time, and increases the risk that work done will end up being rushed or sub par. Other examples include prohibitive cost, or simply the sense that the supplier is apathetic towards the needs of the organisation and isn't providing enough value. Whatever the situation, the organisation now finds themselves in need of another software team. Challenges in building a software team from the ground up -A particular challenge for many organisations, especially those who've never had an internal software team, is that their business is likely not closely related to the software industry. Such organisations have applications that need to be actively maintained in order to keep it running smoothly and up-to-date (at least in terms of security), but where do you start if you need people with experience in an area your organisation currently has none in? Hiring good developers is tricky even in the software industry, but is especially so for organisations not known for having a strong foundation in software and technology. A lot of developers look for positions that excite them, and in which they'll be working with like-minded people. This is not to say that working for your organisation won't be exciting! However, joining an organisation where they may wind up being solely responsible for an application built long before they arrived is a daunting prospect. On top of that, you need to be able to discern whether any developers you interview have the skills you need, and whether they're going to be able to push your software team forward. You don't want to hire someone who sees the role as a stepping stone to an opportunity they're more excited about in another organisation. One solution is to ask for advice from a third party. If your application was built by an external supplier and you parted on good terms, you'll often find they're willing to help you recruit the right people for senior software positions, like tech leads. They'll look at your applicants, give interviews and technical tests, and feedback to you with their thoughts. By hiring a technical lead first, your previous supplier can then step back and give your new hire room to start building the team internally. If talking to a previous supplier is not an option, it's worth hiring a freelance Chief Technical Officer (CTO) to recruit and build your team. A quick LinkedIn search will show there are many out there for you to consider, and there are also directory services specifically for freelance CTOs who'll be able to match you up with someone who can help you lay strong foundations for your software team. Once you have that first, capable new hire, building the rest of the team should be a lot more straightforward. You'll be able to work with your new hire to fill out the team you need, and they'll dedicate themselves to getting to know your application. A good team will then look at ways they can evolve and improve your application, beyond simply maintaining it, and they'll communicate with you constantly to ensure they're heading in a direction beneficial to your organisation. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-02-28"},
{"website": "Made-Tech", "title": "How to run a successful retrospective", "author": [" Ryan MacGillivray"], "link": "https://www.madetech.com/blog/how-to-run-a-successful-retrospective/", "abstract": "At the end of an iteration it's good to take some time to reflect as a team to assess what worked, what didn't work, and what could be improved upon. This can result in future iterations being more efficient and productive, as well as increasing happiness in the team. At its most basic level, a retrospective is simply having the team sit together and allowing them to voice their opinions on went well and what didn't. This provides increased insight throughout the team. Teams list out each of the points made and then take a vote on which of these are the highest priority. Keeping this to a small list is most prudent as it won't overload the team, and any points that didn't make the list will show up in future retrospectives if they're a persistent problem. From that list, action points are agreed upon to tackle each item, and the team bears them in mind when going about their day to day business. In future retrospectives, any action points that led to big improvements within a team should be shared back to the organisation as a whole, so that other teams can potentially benefit from learnings. The Prime Directive Regardless of what we discover, we understand and truly believe that everyone did the best job they could, given what they knew at the time, their skills and abilities, the resources available, and the situation at hand – Norm Kerth It's vital that a retrospective is run in a way where all members feel safe to discuss their viewpoints. Mistakes are something to be celebrated, as they present opportunities to learn , so when describing something that went wrong, be sure not to place the blame at the feet of an individual team member. If someone feels unsafe they're unlikely to be able to fully engage with the retrospective. Violations of The Prime Directive, deliberate or otherwise, should be seen as a team smell and may lead to the team going into the next iteration with a negative mindset. There are some steps you can take to help maintain safety, such as reminding everyone of The Prime Directive before beginning the retrospective proper, or having facilitators conduct safety/pressure checks. Safety checks It's worth running safety checks to gauge how comfortable everyone feels before tackling larger issues. The facilitator should ask each team member to write a number between 0 and 5 on a post-it or similar, to mean one of the following: 5 – I’ll talk about anything 4 – I’ll talk about almost anything, but one or two few things might be hard 3 – I’ll talk about some things, but others will be hard to say 2 – I’m not going to say much, I’ll let others bring up issues 1 – I’ll smile, claim everything is great and agree with authority figures 0 – I'm not comfortable talking / I don't want to do this / I want to leave Once these have been anonymously collected, the facilitator should tally the count. If there are more low numbers than high they should ask if the team would like the retrospective to continue. Either way there should be a brief discussion as to why the team thinks the numbers are low. If it's decided that the retrospective shouldn't run, then it's worth running this quick exercise to determine why people are feeling unsafe: The Facilitator asks everyone to put themselves in the shoes of someone who might not feel safe, then note down what could make them feel that way Based on the submitted notes, have the team work together to list reasons that might cause these issues Based on the potential causes, ask the team to present and discuss potential solutions Run the safety check again If the safety level has increased after this point, then you can run the retrospective. Facilitators A key role in a retrospective is that of the Facilitator. They contribute feedback along with the rest of the team, but they're responsible for: Outlining the retrospective exercise to the team Keeping the exercise on track, with a balance of strict time keeping while trying to keep the atmosphere relaxed and informal Ensuring the Prime Directive is adhered to Helping and prompting thoughts and ideas if the team is struggling Reiterating and sharing any agreed upon action points after the retrospective Trying to ensure everyone on the team contributes to the discussion To keep retrospectives fresh and different, try occasionally rotating the Facilitator role so that different members of the team get a chance to define how the session will run. When deciding upon a retrospective activity, you can choose to go with a tried and tested idea, or something wildly different to help spice things up. We'll discuss some different ideas below, but there are numerous websites out there that provide plenty of ideas to take advantage of. Scheduling and frequency We derive the most value from retros when we run them at the end of each iteration (for us this is weekly) directly after a showcase. By running them before the next iteration begins, the team is able to take the learnings and outcomes directly into the following week. If for whatever reason the team is unable to run one, we've found it best to wait until the next scheduled retrospective, so as to not disrupt the current iteration. Ideally these shouldn't be missed barring illness or holidays. If a team member has contributions to make but can't attend they can choose to provide these to the facilitator ahead of time. Exercises To keep energy up in a retrospective and help provide focus to the areas the team aims to cover it's common to use exercises. These are some of our favourite exercises: The happiness graph Ask each team member to draw a graph of how happy they were over the duration of the sprint. The X-axis denotes days, and the Y-axis denotes mood. As moods, we use: happy, indifferent, sad and angry, but you may find a different scale works better for you. Have the team chart a line representing their mood over the course of the sprint. When everyone has finished, take the time to discuss any notable peaks and troughs, and make a note of the reasons for each. As well as being a good way to see how everyone is feeling, with those notes, you can discuss ways you can make sure the things that made people happy continue to happen, and ways to prevent the things that made people unhappy from ever happening. Hot air balloon The hot air balloon exercise is popular within our team, with the format being: Draw a hot air balloon on a whiteboard or other canvas, with a sun to the right of the balloon, and a storm to the left. The Facilitator provides a prompt like \"Looking back, what was our hot air, taking us higher, and what were our sand bags, bringing us down?\" The team then writes these down above and below the balloon as appropriate. The team discusses each point as they go to make sure there is a shared understanding. The facilitator then offers another prompt \"Looking ahead, what stormy weather can we see making the route to our goal difficult, and what steps can we take to move towards sunny days and an easier path?\"\" These suggestions are again mapped to their appropriate area on the drawing, and discussed among the team. By approaching the retrospective in this way we can reflect on the previous iteration, as well as look ahead to try to avoid any potential pitfalls. Both of these exercises follow the same outline as the basic idea mentioned earlier. Most exercises work to elicit the following from team members: What went badly? What went well? How can we improve going forward? Don't let retrospectives become monotonous; mixing these exercises up is an ideal way to keep things fresh and provide different perspectives and prompts to these questions. Action points The outcomes of a retrospective can be used to identify action points that can be used to try and enact improvements to the next iteration. In the example of the Hot Air Balloon these would be the items listed featured in the sunny area. Ensuring that these action points are focussed and achievable is essential to their success. Comradrospectives Where a standard retrospective is for smaller teams, we've taken to holding a company wide retrospective, known as a Comradrospective. They're an hour long, and we hold them at regular intervals, often two weeks, after each team has had a retrospective. They have the following in common with retrospectives: The Prime Directive is read out at the beginning A safety check is held before hand A Facilitator is assigned Action points are decided on at the end of the session These sessions can be focused on a particular topic, or each team can bring discussion points from their retrospectives, to share with the wider company. For example: things that have been holding them back, and their biggest successes. The facilitator of the team retrospective can collate these in advance and propose them to the wider group. Through sharing and discussing these points as a group the company can identify issues affecting all teams and create further action points to address them. We make a point of revisiting agreed upon action points in future sessions to discuss whether we've been actively focussed on them, and whether they're still a priority for the company. Customer inclusive retrospectives Involving stakeholders from the customer side in a retrospective can be a valuable way to both improve communication and provide greater understanding for both sides. This allows for clear feedback from the customer, beyond the scope of a showcase. As well as that, because they can see you're actively taking steps to address any issues raised previously, it strengthens your working relationship. Conclusion Retrospectives, in any of their many forms, are an invaluable tool for maintaining healthy, happy software teams. By giving everyone a platform on which to voice their concerns, learn from their mistakes and celebrate their victories, you're ensuring your team will continue to evolve. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-03-08"},
{"website": "Made-Tech", "title": "Dismantling Silos", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/dismantling-silos/", "abstract": "A silo exists in an organisation when one group within the organisation has differing goals to another. In most organisations there are groups of people that, usually, have an objective to fulfil by an agreed upon date. For example, the Sales team is set a mandate to increase the number of customers of the company by 10% every month, whereas the Support team has internal performance goals, and one of them is to deliver support within a fixed budget. These teams have the freedom to innovate to achieve their goals. The Sales team choose to slash the price of their product, and immediately it is flying off the shelves; the Sales team are celebrating! The Support team however are now under pressure, and cannot offer a quality service to all their new customers without new members of staff. Due to the product’s newly discounted price tag, the company cannot afford this extra operational cost. Before long, the company gets a reputation for poor customer support. These two silos, sales and support, have potentially compromised the future of the enterprise. What happened here is that this organisation had failed to see the broader picture. Although this example is fictitious, it is easy to imagine happening in the real world. Software Teams At Made Tech, we commonly encounter silos within our client organisations, so one of our top priorities on a new engagement is the alignment of our customer’s goals. Cutting through these silos is critical to avoiding scenarios like the one discussed above, and to ensure that our software delivers the greatest value to the whole business. In situations where our point of contact has a silo-goal, failure to identify this quickly means software can be designed and delivered which does not benefit the wider company aims. Aside from making it difficult to deliver simple solutions for the organisation in the future, this can also have an impact on working relationships. Goal misalignments are usually unintentional, so it is important to mitigate these behaviours with activities designed to aid delivering great solutions for the whole business. Communicating with Stakeholders To understand an organisation's aims in detail, gathering a list of stakeholders and understanding their goals and how they fit into the bigger picture is often useful. One technique that we have found works very well is the use of regular client showcases . These showcases ensure that the parties concerned with the delivery of solutions are kept involved in the evolution of the plan, and feel a sense of ownership of the direction in which the project is going. Communicating with specialists A common way teams organise themselves is to group individuals according to their specialisms. Within the tech world, this has resulted in teams such as Backend, Frontend and QA, each of which have different yardsticks by which to measure success, and are therefore at risk not communicating effectively with other departments. A traditional silo is that of an IT department, viewed as a cost centre. Meanwhile, profit-building software delivery teams are depending on this IT Department for both hardware and software. It is not in the best interest of a company attempting to deploy a new software artefact to production, to be held up by another department that holds the keys. Uncovering hidden goals Hidden goals are those goals which are usually found within silos and not shared with the broader business. Inadequate communication is often the cause for these hidden goals, and although it is not reasonable to expect them to all be discovered early on in the development process, it is worth actively seeking them out. We have found that practising Continuous Delivery is a useful tool for uncovering such goals, given that it allows for a frequent and detailed feedback loop with stakeholders. We can showcase potential solutions regularly, giving stakeholers an effective forum to guide the direction of the project. Despite this, goals can remain hidden even after showcases. When stakeholders offer feedback on your solution, that feedback may be the product of a hidden goal. It is therefore important to test assumptions about why these changes are being made. Be sure to ask questions about any feedback, even if the answer appears obvious. Detecting silos Observation is key to discovering silos, and in most companies, it isn't immediately obvious where they are. To give you a feel for what type of activities are indicative of silos we have compiled a list from the trenches to help you in your endeavours. Communication go-betweens Communication go-betweens operating as a proxy between engineering and customers, can lead to situations where software engineers are unable to get meaningful feedback on the solutions that they are delivering. Useful and accurate feedback is critical to shipping well-built and well-designed software. Prescribed solutions Controlling every detail of a solution in a top-down fashion removes freedom from software delivery teams. Lack of liberty makes it less desirable or necessary for cross-functional teams to emerge (through interdepartmental collaboration). Moreover, this freedom is key to the production of truly innovative and simple solutions. Managers should not need to be involved in every single minutia of a solution, only that the software delivery teams are solving the most pressing strategic issues. Passing the buck When no department can deal with an unexpected critical issue and instead passes the buck to another, it can indicate that no team or individual holds true accountability for resolving that issue. The underlying problem is that it shows that people are not aligned with the goals of the company itself. This conflict with the business indicates that those departments passing the buck have formed a silo against the organisation itself. Lack of robust goal setting How the evaluation of each team's performance is measured can be a reliable indicator of a silo. In the most extreme case, every department has a balance sheet. This financially-led approach can promote unhealthy internal competition and discourage collaboration between teams. The symptom here is that the goals of two organisational units are at odds with each other. This is compounded if there is no clear process available to staff, through which they can resolve their differences. Organisational separation Allocating departmental budgets, or in extreme cases, spinning out separately registered companies, can introduce difficult to solve organisational impediments that make it difficult to deliver on company goals. Collaborating across balance sheets becomes politically risky, and can result in the onset of \"officially political cultures\". Competitive individual KPIs When the performance evaluation of individuals prevents or discourages collaboration and knowledge-sharing there is no incentive to behave strategically. This friction can create problems internally for a single department just as much as cross-departmentally. For example, engineers may decide it is not in their best interest interest to behave collaboratively, potentially reducing the quality and speed of delivery of solutions. Hiring The hiring policy is a good indicator of silos within departments; departments can fight for expansion which can prevent hires elsewhere in critical areas of the business. Hiring without a clear and mutually agreed reason in mind could lead to hiring in the wrong places or for the wrong reasons. Many companies encourage departments to compete over recruiting budgets, but this doesn't solve real business problems. The interview process itself might highlight the competition between teams, such as when other teams are unaware of the recruitment but are still expected to work with the new hire. How to break down silo mentality Now that you have a better understanding of how to spot a silo, it's possible to begin breaking them down by doing the following: Establish shared goals to ensure that departmental goals have organisational alignment. Ensure the organisation has high-level goals, above all departments to which departmental goals help meet. Ensure organisational goals are directly related to the vision. Once this framework is in place, it becomes trivial for anyone within an organisation to set highly aligned personal, project, or product goals that directly impact the business achieving its targets. It is important to get departments to speak to each other on important matters, such as how they will attain their goals collaboratively. For example, a digital design department should meet regularly with a software engineering department to determine how they will ship an online software product together. Departments should not communicate solely through hierarchies; a junior software engineer could and should be in direct contact with designers. As this evolves, it becomes clearer that it is more efficient to have departments working alongside each other to deliver a product. When people with different skillsets need to collaborate with the same goal, they should attend the same meetings, be aware of the same problems, and have a clear sense of shared direction and purpose. Requiring formality or involving convoluted chains of command at this stage will only hinder collaboration. At this point, management shifts to a broader view of the company rather than micromanaging all daily details. Instead, the freedom granted to the team allows them to deliver continuously and ship products more quickly. Tighter collaboration between skillsets leads to better product quality thanks to a better understanding of the issues. Formal long meetings can be a symptom of having too much bureaucracy around collaboration, which can make it less desirable for employees to break down silos. The really innovative companies are creating shared offices where teams with diverse skillsets (potentially cross-departmental) can collaborate. A psychological silo can manifest itself into a physical one with real walls and closed spaces inside the company. An analysis on how the office space is laid out can help to uncover potential silos. Leaders should encourage everybody to work with anybody to solve company problems. Employees should be able to move freely to another team if needed for the current objectives, and the decision to attend a meeting should be made by anyone who believes it will achieve progress towards the higher company goals. Summary Silos create organisational bottlenecks. In any organisation, this can feel like arbitrary red tape, slowing progress for no real reason. For software teams, an individual having exclusive domain knowledge means the team's ability to develop and release software is hampered when said individual is unavailable. Look for ways to ensure such knowledge is shared amongst the whole team by making everyone aware of the goals your organisation has, and the role the software they're building plays in pursuit of those goals. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-03-15"},
{"website": "Made-Tech", "title": "Things We Learned Building a Complex React App", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/things-we-learned-building-a-complex-react-app/", "abstract": "React is a fantastic tool for building frontends. We've been using it for small applications for a while, and recently used it for a more complex app. Given the library, and the mass of libraries you'll use with it, are still brand new, we encountered many architectural and code challenges throughout development which haven't been solved before. Making these decisions was a lot of fun, but also created a fair share of headaches. Assumptions we made at the start of the project were challenged strongly later in development. Following are some decisions we've regretted, and a few sins we committed. Use Smart Components where you need them A common pattern in React apps is having top level \"smart components\", often referred to as \"containers\", which are the only components which make state changes, through HTTP calls, dispatching Redux actions, or whatever. The benefits of this approach are that very few of your components have to be aware of receiving data and sending data changes. This simplifies the rest of the components, \"dumb components\", which are just rendering markup based on their props. Separating \"smart\" and \"dumb\" components is essential, but I've found prop delegation becomes confusing to read, tedious to write and difficult to manage if you only place your smart components at the top. Page level containers have to be aware of everything every component beneath them could possibly want, and have to provide deep dependency injection to allow all of these components to make data changes, and use sourced data. You end up passing a huge number of props up and down the \"tree\". For example, consider a ContactForm component inside a Sidebar component, which in turn is inside a HomePagecontainer. When submitting ContactForm an AJAX request should be made to an endpoint on the server. If HomePage is the only smart container, then it's the only component capable of making this request. However, the form button is inside the ContactForm, so registering the button click event happens there. In practice, it means function props for handling the button click have to be passed from the HomePage container to the ContactForm component, through all intermediary props. For (a contrived and trimmed) example: I strongly believe separating components into \"those which can access and manipulate state\" and \"those which can only receive props and only contain markup\" is the most sensible approach, but since working on this project I'm against keeping all the data manipulation at the top. I much prefer a structure like this: Now, the ContactForm widget is entirely responsible for its own state manipulation, meaning HomePage and Sidebar don't have to take responsibility for any part of its internal implementation. Not having to wire every component all the way to the top of the application greatly improves development speed, increases the portability of ContactForm, makes it simpler to test, and improves the speed of refactoring across all the components involved. Don’t split your backend into multiple servers Splitting your API server out from your rendering server is a reasonably common pattern in React development at the moment. The most popular React boilerplate does it. It sounded pretty good in theory, one server would be a pure RESTful API written in Node, proxying requests to our Rails e-commerce backend. The other would match routes for any request that came in and render React server side, so we get the benefits of server side rendering. This meant the rendering server consumes the API server for data on the initial page load, and from that point the client would make requests to the API directly. We wanted that separation of concerns, it sounded awesome. The separation of concerns got pretty much destroyed when we came to authentication though. We stored user data in signed cookies, so naturally weren't able to use this to form API routes. Instead to allow authorised requests we needed the API to be stateful, and able to send signed cookies to it. We could have proxied those API requests through the rendering server, added authentication to that instead and created proxy routes for all of our API routes. This would have meant pretty much doubling up on all the API code and making two requests for every one we needed. Having two separate servers meant both had to make compromises for the sake of security and development speed, and really complicated our error handling code. The backend would have been much better off as a single stateful application. Don’t do too much inside your components We have both client and server side rendering in the application. We dispatch HTTP requests through Redux actions when rendering a page level component, to fetch the data required to render the page. We used redux-connect to ensure all of these requests were complete before actually rendering, so page renders feel fast and don't have any pop in as it populates. This worked really well, but meant having complex code to manage it in both the server and the client. Additionally, it made testing page level components needlessly complex. Since HTTP requests are being made, we had to either mock these or run them against a dev or staging server in our test suite, when all we really wanted to do was test that the component had certain contents when given certain data. It would have been much simpler to test if we had changed the rendering process on the server and the client, so that HTTP requests were made before the containers themselves were instantiated, rather than making the container gather their own data. This would have given us a straightforward, synchronous component rendering process. You should use feature testing for integration and interaction, and container tests with a tool like Enzyme to test contents. CSS probably isn’t worth it We used CSS in this project rather than inline styles, wanting to try out CSS Modules and PostCSS and thinking it would allow us greater flexibility over our styles. We included several style loaders in our webpack config so we could import the CSS files within our components and mix them in with the react-css-modules decorator, and used webpack-isomorphic-tools to serve chunked assets in development and bundled assets in production. It worked well enough throughout most of the development, it wasn't until we had to introduce multiple themes for different markets that we encountered a ton of issues with the approach. For example, you'd have to define multiple webpack configurations for each theme and embed conditionals relating to webpack-isomorphic-tools all across your application in order to switch the theme used when rendering the page, and this behaviour would be very awkward to test. We ended up spinning up multiple versions of the app in both development and production, each of which served a different theme. Additionally, we had to litter our CSS with selectors like :global(.theme-blue) & {} in order to tweak the designs of different pages differently. It would have been preferable to define CSS in JS inside our React components instead, using a library like Radium, Glamor, or a litany of others. This would have allowed us to easily switch out values based on props, or even define multiple presentational components for each theme, which would have not only made our code a lot cleaner, but easier to run and deploy. You should use React The user experience is phenomenal in a well built React app, the development experience at its best is too. The community hasn't grown enough to have good, conventional solutions to plenty of problems yet, but it's getting better. There are well known pain points, and plenty of niche ones you'll discover yourself, but this leaves you room for innovation. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-03-29"},
{"website": "Made-Tech", "title": "Pair Programming", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/pair-programming/", "abstract": "We've helped a number of organisations successfully adopt pair programming, giving their teams the ability to increase productivity, improve knowledge sharing and enhance the quality of their software. As a company, we've been using pair programming for around eighteen months, and we've discovered it brings a significant number of benefits, along with one or two challenges we've had to overcome. With this article, we'll be sharing the experiences we've had when introducing pair programming to software teams, and we'll take you through the techniques you'll need to apply what we've learnt to your organisation. Background Pair programming was first introduced as part of the Extreme Programming (XP) software development methodology, as an 'extreme' way to practice regular code reviews. Conceived of by Kent Beck in 1999 , XP is a collection of software principles which help teams to deliver higher quality software. It places value on communication, simplicity, feedback, courage and respect, all of which, as you'll discover, lead to a positive pair programming experience. How It Works Pair programming involves two developers sitting at one computer, with one driving, and the other navigating. The driver types out the code, whilst the navigator constantly reviews what is being typed and, at regular intervals, the developers switch roles. Throughout their time together, the pair constantly communicate their thought processes, allowing the other developer to collaborate and help shape the direction of the code. There are a few guidelines that you should follow when pair programming. These will help you to get the most out of your pair programming experience, and sidestep some of the more common pitfalls we've seen teams experience. Driving The driver is solely responsible for typing and controlling the screen. They should externalise their thoughts as they type, and be sure to constantly communicate with their navigator, discussing ideas and clarifying where necessary. One of the more frustrating aspects of being the driver is that their navigator often has more time to think, meaning they're able to convey their ideas faster than the driver, who is more concerned with typing out code with the right syntax. The driver will often feel clumsy or slow, as the navigator will be able to spot things more quickly than they can. This is OK and to be expected, and the roles switch so regularly that both developers experience the situation from both sides. Navigating The navigator is responsible for reviewing everything the driver types, suggesting improvements to the code being written, alternative ways to think about the problem at hand. The navigator should be considerate and careful to minimise unnecessary interruptions when the driver is in flow. Much like a conversation, choose the right moments to point out trivial errors, like spelling mistakes. Switching The pair should switch roles to allow each person to get a mix of driving and navigating. There are a couple of methods that are often used to determine switching frequency. One technique is time based, where the developers switch at regular short intervals. Another technique is Ping Pong , where developers take turns writing a failing test case, that the other developer then has to make pass. Taking Breaks Pair programming is intensive, especially over the course of a few hours or a whole day. It's important that pairs don't burn themselves out, so they must make time for regular breaks away from pairing throughout the day. These breaks provide good opportunities to do things that might otherwise distract a developer during pair programming, such as checking emails, instant messaging, or making coffee. Pairing, Not Coaching Pair programming involves two peers of a similar skill level working together, but it's not uncommon to see some organisations use pair programming as a form of coaching, where a more experienced developer will sit with a less experienced developer and attempt to upskill and explain their rationale around particular design solutions. There are definite benefits to coaching, but it's important not to confuse it with pair programming, as it can lead to backseat driving, and the less experienced developer becoming demotivated. Why It Works Modern software delivery has shown us that shorter feedback cycles, frequent communication and regular displays of progress are techniques valuable to any organisation. They help to minimise the risk of a project failing, and they are all inherent in good pair programming. Programming Is Hard We should all know that the most time consuming aspect of software engineering isn't typing, it's the time spent thinking about how to solve the current problem and to design a solution that works well. There are a huge number of choices that need to be considered with every line of code a developer writes, and pair programming helps share that responsibilty. Increased Brainpower Two heads are better than one and, in a pair, both developers will have knowledge in areas the other doesn't, meaning their ability to find good solutions is much better than it would be were they working alone. Additionally, because each idea a developer has needs to meet the approval of their partner, they're forced think a lot more critically about solutions, ultimately leading to better code. Validation Of Ideas Pairing encourages you to explain your thought process as you go, whether you're the driver or the navigator, and in a lot of cases there are several ways to solve a particular problem. By having a partner there to constantly bounce ideas off, you can quickly weed out sub optimal ideas and concentrate on the best solution. Benefits Of Pairing There are many business benefits to pair programming, such as improving the quality of software delivered, building collaboration within a team and helping to share domain knowledge across multiple people. Productivity & Focus Working so closely with someone else means you've got no other option but to double down on the thing you're working on. The quality of the code becomes a shared responsibility, and means that, when pairing is done properly, you're both driven to produce the best code possible. The back and forth nature of pair programming also helps to keep you engaged; whether you're switching roles every fifteen minutes, or you've gone the Ping Pong route, navigators are often eager to get back on the keyboard and keep momentum going. Higher Quality A developer working alone can often be tempted to take shortcuts, either in the interest of time or because they're not quite sure what the best solution is and don't want to feel stuck. It's much more difficult to let that happen when you're working as a pair, because everything you do is being evaluated by another person and you're both responsible for the quality of the code. Domain Understanding Pairing is a great way to share domain knowledge within an organisation, and makes it much easier for developers to move back and forth across different codebases. By working this way, it means that if a particular developer is absent for any reason, whether it be holiday, sickness, or they've decided to leave, there's at least one other person in the team who has a good understanding of the project and can easily continue working on it. Improved Resiliency Whatever environment you're working in, the number of things that can interrupt or distract a developer working alone is huge. When you're part of a pair, you become much more resilient to those interruptions because you're both committed to the task at hand. You're less likely to be interrupted by colleagues because they can clearly see that you and your partner are in flow, and you'll more easily resist the temptation to check emails because you're both committed to completing the task at hand. Team Building & Cohesion The stereotypical view of software developers is that they work alone and are poor at communicating with others. Whether or not this stereotype is true, it's not uncommon to find developers who favour working alone and for teams, this is not ideal. Pair programming encourages a much more social and collaborative way of working, which helps to build rapport between team members and a culture where your team solves problems collectively. Learning The number of technologies and languages within the field of software engineering is vast, so it's inevitable that there'll be areas where one developer is stronger than another. With pair programming, it becomes much easier for developers to learn from one another as they're being introduced to new concepts as they go. Continuous Code Review Code reviews are a valuable part of any organisation's software development lifecycle. Pair programming takes that concept to an extreme, by having the navigator constantly reviewing what the driver is typing, suggesting improvements or alternate approaches that might help catch edge cases. Challenges In Pairing Moving from programming alone to becoming a strong pair programmer is not always an easy transition. We've encountered a number of specific challenges which you should watch out for. Switching Mindsets Programming has historically been a solo activity, so the mindset shift required to work on code together is significant. Developers often struggle to take onboard others ideas and externalise thought processes. This can be particularly challenging if you've worked solo for a long time and are not accustomed to social coding and working closely with others. People & Time As a business, you shouldn't expect two developers to complete a task in half the time. Often it will take a little longer than if a developer was working alone, so there is a short term cost associated with adopting pair programming. This additional cost is offset by the long term benefits of having higher code quality, less technical debt, and shared domain knowledge. Exhaustion Pairing is exhausting. When you've spent the entire day thinking critically about every line of code, justifying and communicating those thoughts and decisions, it's likely that you'll feel drained, but that you've achieved a lot. It's critical that you take regular breaks, that you have some flexibility, and choose appropriate times to pair. Skill Disparity You'll often find pairing works less well when you've got signifiant skill disparity between a pair. If one engineer is clearly more experienced and is making all the decisions, then this can lead to the less experienced feeling overwhelmed and demotivated, whch can lead to disengagement. Pairing With The Wrong People It's important that developers pair with someone they're well suited to. The most beneficial pairing will be two people who can learn from one another, share similar philosophies and balance each other out. Backseat Driving When a pair of developers have significant skill disparity, or one member of the pair is louder then the other, then you may encounter backseat driving. This will typically manifest itself in the navigator telling the driver exactly what to type, and the driver following their instructions. This is a problem, and you should encourage the backseat driver to be more considerate, or consider switching the pair members so they are working with people who have similar skill levels. Communication (Your Baby Is Ugly) It's important that you are able to communicate honestly with your pair. If one has an idea that the other knows to be bad, they must be capable of saying this and able to present a case for an alternate approach. Disengagement Some developers will be resistant to pairing or will spend their time less engaged in the activity. Developers checking their phone constantly, sitting back in their chair or not communicating actively are signs of an individual that is disengaged. This is detrimental to the team, and it's important to try to understand why it's happening. It may be that the developer really does work better alone, or it's a sign that they're exhausted, there's disparity in skill between them and their partner, or that they simply don't get on with their partner. Introducing Pairing When introducing pairing to your team, there's a number of things to consider, such as the practicalities, and the typical objections you might face from the business. Getting Started Start small, take two developers who are keen on the idea of pairing, set them a task to complete and give them the space they need. Get them to champion it, so others can get a sense of how it's working for them. Have them pair together for a short while, perhaps a couple of weeks, so that they have time to address any teething problems. Don't Assign Pairs It's important not to force people to pair together, let pairs form naturally. Keep an eye out for signs of the challenges mentioned above, and provide guidance if it seems like a pair is less effective than it could be. When Not To Pair Not every task requires two developers to tackle it. Having two developers working on the same thing may not be the best use of your team's time; a task may be so straightforward to complete that it relies more on muscle memory than critical thinking, or one developer is tired of pairing and in need of a timeout. Find some examples of good and bad times to pair here. Blocker: \"Double The Hours\" A popular argument for not adopting pairing is that you're doubling the man hours needed to complete a task, which is simply not true. You're likely to find that the number of man hours does increase, but you'll also find they produce better quality code and spend less time stuck on problems or dealing with technical debt. Blocker: Management Buy In Software projects can fail and the most common reason for failure is poor communications or mounting technical debt. Pair programming embeds collaboration into the process, which results in a software product that is of higher quality, and has fewer defects. When teams engage in pair programming, it means there is less risk when a developer leaves, or is away, as domain knowledge has been spread across the team. Cynics It's a hot button topic in software and you'll frequently find people opposed to pairing. You can't force a cynic to engage in pair programming, but you can get the people who are interested in pair programming to evangelise it. Over time, we've found this helps to bring some people around. Review As with any agile process, it's valuable to regularly review progress and see if it's working well for your organisation. We've found Agile retrospectives to be a handy tool in helping to understand what has worked well and what may not have worked so well. Pairing Environments Workspace & Equipment We see the ideal pairing workstation as having two mirrored screens, plus a shared keyboard and computer. The pairs should be able to comfortably sit side by side with one another in an environment which is ergonomically friendly. At the bare minimum, a pair should have a single computer which they pass back-and-forth. Remote Pairing Setup & Tools As remote working has become more popular, tools such as Screenhero have gained traction in the pair programming community, and are a great way to collaborate from different locations. Unless you're Ping Pong pairing, a Pomodoro app is a must. We use Pomodoro One , but there are hundreds out there that could be used for pair programming. Conclusion Through the increased adoption of open source and a plethora of online tools that help software teams to collaborate, programming has become more social and, with communication being one of the biggest problems in software delivery, we see pair programming as being the natural evolution of this. In fact, many forward-thinking organisations have already adopted the practice, some to the extent that their developers program exclusively in pairs. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-09-22"},
{"website": "Made-Tech", "title": "8 ways to get things done in big organisations", "author": [" David Winter"], "link": "https://www.madetech.com/blog/8-ways-to-get-things-done-in-big-organisations/", "abstract": "Over the past year I've worked on a number of projects that have involved some large organisations. By large organisations, I mean, big hefty things that have been around for a while, have lots of cogs and yet still achieve a lot. At Made, our mission is to improve software delivery in every organisation, so working with these lovely organisations is a dream, as there are so many opportunities to help make processes and employees lives so much easier as a result of our engineering and methodologies. It gets us excited, and gets us out of bed every day. We are a very small company in comparison to these organisations, so we've often felt frustrated when we're unable to deliver software at the speed we like to work. Deploying to production via our continuous delivery pipelines happens many times each day, so when things get in the way of that, we call it a smell, and go sniffing out the issues that are clogging things up. 1: Get into their office At the earliest opportunity, make sure you're able to visit an organisation's office or offices. This is the only real way to see all of the cogs in motion, and hopefully avoids you having to trawl through epic email thread trails where you've no idea who anyone is, or what direction the communication is flowing (see point 8, chat applications over email threads). We were developing a system for a client which sends out completed orders to a warehouse management system, where the staff in the warehouse physically ship goods, and then send status messages back to our systems. It wasn't working. Email wasn't the best way to communicate the issues, and it was a slow process day-to-day waiting for replies. Being at the client's office for a day turned out to be one of the most productive moves we made for the project. We started with our main contact and then followed the process via people in the UK office, at each stage checking what happened and if there were any issues, until we got to a point where something wasn't right, and then someone was able to resolve it for us to unblock our path to getting our code into a production ready state. 2: Shadow staff When working on feature improvements for a product, request to shadow staff working with the systems day-to-day. See what things are blocking them doing their work, and where real business value can be delivered. For another client we worked with, by shadowing the staff, we found out that for two days of every work week, the team were having to manually copy and paste orders that had been submitted over the weekend into their sales system. Two days of copying and pasting. Instantly, our brains were kicking into action figuring out solutions we'd be able to implement within a few days to help make this process a lot simpler for the staff. If you're unable to work on improvements there and then, be sure to capture and log improvements that could be made and implemented in the future. We setup a Trello board for things like this. 3: Reduce blockers, own the problem This isn't always feasible and completely depends on the size of your team, and the remit you have with the client. But when working with large organisations, there are often multiple layers to getting simple things done. Often we work in a more agile way to how large organisations have been running for the past decade, so it's not always easy to fit our peg in their hole. In these cases, we try and take ownership of these things, so that the problem is then our responsibility. An example being infrastructure. When we setup continuous delivery pipelines for projects, we often setup multiple servers, and want to be able to create and destroy these as and when needed. Most of the large organisation we've worked with, servers that they use are often snowflakes (read our blog post: Your Server Is Not A Pet ). They have no automated provisioning, and you can't easily request to have new ones setup, let alone torn down via their IT departments. When we hit the infrastructure hurdles, we try to set up the resources we require in a separate cloud account, and provide the client with the code that provisions these servers (Terraform/Ansible). The code serves as documentation, and means that we can then reproduce anything we create. If we want to destroy a server, we can create a new one when we need. 4: Daily standups For each project we organise standups that happen at the same time each day. These standups include our team working on the project, as well as key people from the organisation that we're working with. It's a great way to show the progress we're making, while at the same time, serves as an opportunity to highlight any blockers to those in the organisation. Sometimes, these can be unblocked easily once raised. Othertimes, not so easy, but if every day you mention the same blocker, it becomes something the organisation can't ignore. It's in these situations people within the organisation are able to put pressure on others to fix it, or the organisation is more willing to accept alternatives they wouldn't normally. These standups don't have to have the same fixed attendees list throughout a project. For example, if you start working on a different set of features for the project. It's OK to un-invite people from standups! Ensure people present are able to contribute in some manner. Try and prevent these standups from deviating from their original scrum purpose, which is to keep the team working on the project in the loop with one another. This isn't an opportunity to report back to management. That is what the mini-sprint demo is for. 5: Small kanban-esque sprints With so many cogs in large organisations, it becomes really hard to estimate how long implementing some features will take. If you have to integrate with lots of systems, it becomes near pointless to estimate these things. You just don't know what hurdles you'll encounter day-to-day. Don't estimate all the work. You'll be wasting time on just guessing. Instead focus on smaller sprints that consist of a few days work. Focus on a specific area of functionallity you want to tackle. Setup a Trello board for this sprint. And demo your progress after a day or two to the client, and then at the end of the small sprint. When starting these sprints, always set a demo date before you begin. The demo must always go ahead, even if you don't get the functionality in a state you wanted to. In the demo, you can still talk through the progress you've made, and the things that are blocking it from finishing. Have another Trello board for the more high level epic/roadmap of features, and always be asking the client to assess the priorities of these items. New mini-sprints can then be decided based on this prioritisation. As with standups, these small sprint demos allow you to feedback to the client, and in the event things couldn't be completed due to blockers, it gives the organisation a chance to re-prioritise based on business decisions. Is this feature worth the time? Or is there something else that would have better business value? A faster feedback cycle is only ever a good thing. 6: Isolate legacy systems We often have to work with existing legacy systems within big companies. It seems that over the years, layers of different solutions are placed on top of one another to fix problems as they're discovered. We're trying to cut away these layers and replace multiple systems with more modern, well engineered, easily extendible replacements. To do that, figure out the interfaces that these systems interact with. Place logging at each of the entry points to understand what communicates with these systems. Then build a very simple application that can receive these requests and respond in the manner it expects. Then, swap the system out. If you're able to take control of a few of these layers, eventually you should get to a point where you can then cut out some of the middle layers and merge them into one. 7: Chat applications over email threads Imagine having a meeting to decide something that spanned over a week in drips and drabs. Terrifying! That's exactly what important conversations over email turn out to be. Nightmares. Favour chat applications over email. You're able to get answers and make decisions far quicker. If you have a question, ask there and then. Via email, you never know how soon you'll get a response back. Most chat applications these days have web versions (Slack/Hipchat) that run in the browser, so there is no need to battle those big IT departments that fear any new piece of software will let loose an armada of viruses on their corporate network. Ensure that you invite everyone who could have valuable insight or knowledge into the chat group. Chat rooms are cheap/free with these tools, so be sure to create multiple ones to try and group conversations together rather than having a single room with a free-for-all, which will not be productive. Big businesses often use systems like Lync, so they will probably not be familiar with the Slack/Hipchat style of chatting. Be sure that you explain: How to enable notifications; especially if employees have to run the chat in a web browser Mentions, the @person syntax to direct questions (and notifications) to specific people Where to find different rooms in the group How to attach files How to set themselves as away/do-not-disturb. Sometimes people need to get their head down or attend meetings, don't let this turn into a negative experience for them Encourage people to have these chat applications open throughout the day. 8: Align services before new features This next point depends on the project you're working on, but for another client, we're rolling out a new ecommerce system across four different European markets. The system we're replacing had different codebases per market, and the goal is to get them all on the same system, so that new features can benefit all markets, while also allowing for a single codebase that is easier to maintain and support. The approach that has worked really well for us is to get all of the markets onto this new platform, with just the basics of what they currently have to allow them to continue their day-to-day work, and to resist the urge to develop new features. Our new platform is far easier to work with when implementing these things, but getting all markets on the system first means when we come to design new features, we'll consult with all markets so that we engineer a solution that works with them all, either out of the box, via configuration per market, or just disable for markets that don't require it. Be sure to share with the client (or individual markets) the roadmap. That way they will understand the benefits of holding out in the short term while you align everyone, and then the massive positives that will follow when new features start to appear. Working with big organisations doesn't need to be a David vs Goliath battle. The biggest theme from all of the above points is communication. Second is isolating systems and blockers so that you're in control of them. Always push the agile agenda. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-06-30"},
{"website": "Made-Tech", "title": "Products not Projects", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/products-not-projects/", "abstract": "Traditionally, software is delivered as a project: a thing with a start and an end date, and probably a list of desired features. The software is (hopefully!) deemed complete by the end date, the project is closed, and the project team move on to their next project. A Product on the other hand is something that lives for an indefinite period of time, is hopefully seen as an asset to the organisation, and is something that should evolve and improve over time, addressing the needs of real users. Too many organisations are still focusing their efforts on delivering software platforms with a Project rather than a Product mindset. We observe that this harms organisations in a number of ways: Encourages big bang releases We often see that the project approach encourages big bang releases, where significant chunks of work are stored up and released close to the project end date. This leaves little room for iteration and improvement, as the software spends a small amount of time in the hands of end-users before the budget is exhausted and the team are disbanded. We favour instead looking to adopt an MVP* approach, where the smallest increment is delivered to real users as soon as possible, so that their feedback can help to shape the most valuable direction for future investment. The investment strategy for a product would usually look different to that of a project. Typically projects look to maximise their budget at the start, and would expect to run the budget dry by the time the first release is out. *A Minimum Viable Product (MVP) is: \"the version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort.\" Measuring success on project delivery Encouraging teams to think of success in terms of \"on time, on budget\" delivery removes the focus of a meaningful commercial goal, such as how useful the product being delivered actually is to customers. Instead, a team can become focused on delivering a set of features to a deadline, often regardless of how useful those features might be once the software is in the hands of real users. We see much greater success where teams are more closely aligned with delivering against the commercial objectives of the product launch. Nobody owns the direction of the product There are distinct differences between the roles of Product Owners and Project Managers – though we often see the latter assume the surrogate role of PO during a more typical project engagement. Empowering a stakeholder in your organisation to take ownership of a product, set its direction, and assume overall responsibility for its commercial effectiveness is a great motivator. Lack of ongoing ownership It can often be observed that teams who are responsible for running a software platform in the longer term have a stronger vested interest in building something more maintainable and easier to run. A project team ceasing development and handing over to an operations team to run as an ongoing concern is a surefire way to promote organisational silos and a lack of responsibility. It's not true that the cost of change in software systems needs to increase over time. With a mature team having ownership of a product, a sensible approach to evolving and improving a system can be attained. Ignoring Software Rot It's a common misconception that once a software system is built, providing you don't make changes, it can run in perpetuity without further investment. Software seldom sits in true isolation. It's likely to be dependent on third party libraries that may require patching and updating, it may have dependencies on third party services, the interfaces to which can change, and it's likely to increase the size of dataset that it deals with. All of these factors can change how a software system behaves and requires a level of ongoing investment. Software rot is a challenge for most software products. Repeat cycle of replatforming Too often we see customers stuck in a cycle of replatforming every few years. Almost as soon as one project reaches its conclusion, another project is spun up to replace it with the latest, greatest new tech. By making use of appropriate software architectures, a software platform can improve and evolve over time, replacing smaller components as they reach end of life, but avoiding the baby with the bathwater scenario of throwing everything away and starting from a blank sheet. We'd encourage organisations to shift their thinking from freeing up big budgets every few years to embark on yet-another-big-project (\"we'll get it right this time!\"), and instead look to set a reasonable ongoing budget to maintain, improve and evolve their software systems as a continual effort. This approach also rings true for bringing new products to market. We don't believe investment strategies that encourage a big team upfront which is scaled down to a smaller skeleton maintenance crew is generally the best route to market. Delivery strategies like this too often burn their investment without getting anything in to the hands of end users; lacking a true feedback loop that can be acted on. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-07-06"},
{"website": "Made-Tech", "title": "Code fragmentation: The modern day goto", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/code-fragmentation-the-modern-day-goto/", "abstract": "Programs must be written for people to read, and only incidentally for machines to execute. – Harold Abelson Keeping your code simple and easy to change is one of the hardest challenges when writing programs. One obvious aspect of this is the amount of code that you have to parse to understand what it aims to achieve. All code can be divided into two categories: Boilerplate: Code that helps you, the developer.  It facilitates things like information hiding, type checking and encapsulation. Domain: The actual useful part of the program that is visible from the outside. More fragmentation means that you are going to have more boilerplate to support it. Code Fragmentation I primarily work with object oriented languages and I am always pleasantly surprised when I can achieve the same result in a fraction of the code by using a functional language.  It just has a smaller surface area, is more compact and generally more expressive due to the underlying philosophies about how to model a domain. Object oriented programs break up the data into small chunks and these are accompanied by the code that operate on them. Most functional programming languages put the data first and have lightweight functions operate on it to create new representations. This often has the nice side effect of the code being in a more central location.  The code is closer to the data. If you practice object oriented programming correctly you are going to inherit a lot of fragmentation by default. On top of this you can make it worse by prematurely optimising your code to accommodate potential new features. Some people just write fancy code because their language makes it easy for them to do so. What people forget is that a code is much easier to understand when you are writing it than when you read back over it, so they have no worries writing complex code.  Mix in a bit of meta programming and you are dealing with an absolute disaster situation. Code mixed in via modules, engines and gems to name but a few are some of the biggest pain points that I have discovered in my day to day work. Unneeded indirection is a classic form of incidental complexity. It only means that you are going to have 20 files open in your editor to try and solve one problem. Even the most advanced IDE cannot save you from this. Your cognitive overhead has increased from understanding the code to mapping the code to where it is located. YAGNI and premature optimisation A lot of the fragmentation comes when you prematurely optimise your system to be flexible and cater for edge cases that just never seem to occur. Prematurely optimising for performance is another form of incidental complexity but not what I am referring to here. Don't move the code to a different location unless you have established a clear boundary or separation of concerns that makes sense. Do not extract code simply because you think that the class is too large.  If you can identify and extract a role from the class that is cohesive then, and only then it is a good idea. Be kind to your fellow developers.  Don't make them go on the hunt every time they want to write some code. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-07-25"},
{"website": "Made-Tech", "title": "Let your teams plan for themselves", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/let-your-teams-plan-for-themselves/", "abstract": "As an industry of tinkerers, optimisers and perfectionists we occasionally miss the beauty of unorganisation and human instinct. Our obsession to be more efficient and productive can sometimes have some undesired consequences. At Made we often adapt our processes in the name of efficiency but lately we've started experimenting by taking away some of these processes with surprising results. In a recent Clojure Gazette , Eric (hello Eric!) wrote about management trying to protect the time of engineers so they can do their job of programming without distractions: You start to protect their time. You think \"I'll make all of these decisions for them. I'll write them up and divide them into little pieces and prioritize them. All they'll have to do is pull one down and start working on it. I can even ask the designers myself for some work. I won't bother the programmers with it until the designers are finished.\" – Eric Normand I can relate to this as it can often be tempting to setup a Trello board, calculate (guesstimate) a deadline and hand it to a team. Programmers want to program right? I've sat through hours of backlog grooming sessions and sprint planning meetings so I know how often they can feel both boring and useless. So why not keep the majority of the team out of planning? The issue with keeping your team shielded from planning is that they lose the context in which they are trying to solve problems. By spoon feeding a team you are taking responsibility from them, you are removing their ownership of the problem. This can be a big demotivating factor, after setting out to increase efficiency you can end up with a disinterested team coding features to a specification and fixing a list of bugs with no context. We decided recently to take a different approach; instead of removing developers from planning we decided to leave planning up to developers. We hoped this would allow teams to take more ownership of delivery. By having a higher level of understanding behind the problem we thought teams would be able to solve them better. We recognise that an increase in responsibility usually correlates to an increase in happiness and therefore performance. Instead of maintaining a backlog of stories along with grooming sessions and sprint planning meetings we tried an alternative agile approach. We moved to maintaining a high level roadmap of problems or goals. Once a team became available we would pull the highest priority goal off the roadmap and handed it over to the team for an iteration. Briefing: The team is introduced to stakeholders of the goal and then given a walkthrough of the problem domain for around 15 minutes. Discovery: The team then goes away to think about how they might solve the problem. Sometimes this might require a day or two of spiking. Planning: The team then plans in whatever way they prefer. Most favour a simple Trello board with columns for \"Must\", \"Should\", \"Could\", \"Doing\" and then \"Done\". The team would announce a date for initial and final showcases to stakeholders along with the scope they are expecting to tackle. Development: The team begins development on the problem. This is the \"programming\" bit most of the time. Showcase: The team will first give an initial showcase within a day or two of starting development. This allows them to collect feedback from stakeholders based on assumptions they've been making. The team then repeats steps 4. and 5. every couple of days until the final showcase. The final showcase will demo a production deployed solution. Monitoring: The team will typically monitor any deployments for the next few days whilst starting their next iteration. Sometimes a day or two will be dedicated to monitoring if we expect a bunch of tasks to fall out though this sometimes will itself turn into a subsequent iteration. With this workflow, our full stack engineers own each iteration and determine how it is run all the way to production. In a Signal versus Noise article on Permission , Jason Fried talks about the benefits of handing over responsibility: When the person doing the work is the person that has to live with the consequences, they tend to think more completely about what they’re about to do. They see it from more angles, they consider it differently, they’re more thoughtful about it because it’s ultimately on them. – Jason Fried We really agree with this sentiment and have found every time we hand more responsibility to teams the better the results. We're advocates for small teams of full stack developers rather than building larger teams with specialised roles. We typically do not have non-technical roles within teams. We want our engineers to act as project managers, agile coaches, customer liason, testers and more. We want a team to be able to fully engage with a problem from start to finish and find full immersion into a problem provides this. We've been using this workflow on a number of engagements for over 6 months now and have not missed traditional scrum one bit! Scrum originally had the benefit of allowing us to provide estimates for when features could be delivered. Even though we now work with high level roadmap items that aren't estimated, we still work to a timeline. Clients have an idea of a deadline for goals and teams will have to negotiate the scope of their iteration. In planning, teams will grade work into \"Must\", \"Should\" or \"Could\" by having conversations with stakeholders. If a client requires something by a certain date the team will accommodate this by limiting scope further, perhaps into shorter iterations so we can have finer grained control over what is delivered. If an iteration ends with unfinished tasks, we often find they are in the \"Could\" column and end up being dropped altogether. In larger organisations our practices would likely be met with adversity and this sometimes does happen with our customers. We often try out practices such as #noestimates and #noplanning in a small trial project with the aim to using them more widely. In reality we often find a balance between how we'd prefer to work and what our customers are comfortable with whilst still pushing new ideas where we can. In letting go of more traditional management of team workloads and planning of delivery we have found deadlines are still met and teams appear happier to be in more control of their work. We've even implemented this strategy in our customer's engineering teams to much success. Has your team recently tried reducing the bureaucracy around teams and delivery? Perhaps your team wants to try some aspects of #noestimates and #noplanning? Get in touch with us on Twitter @madetech_com ! Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-07-07"},
{"website": "Made-Tech", "title": "Should I Build or Buy Software?", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/should-i-build-or-buy-software/", "abstract": "Often customers, or potential customers, will come to us with a pre-supposed view that they need to build a piece of software to solve their business need. We believe this is true only in a handful of cases. In the first instance, organisations should look to buy existing software to meet their need. Only in cases where the problem the organisation is aiming to solve is sufficiently unique, and where the expected commercial advantage in doing so justifies it, should an organisation look to build a bespoke software system. Is your problem completely unique? We often observe that organisations can overestimate how truly unique their problem-space is. The software industry has been churning out software at a good rate for about thirty years now. The odds of the problem you're trying to solve having not already been solved are becoming less likely. As a first port of call, try to understand what your competitors and peers are using to solve similar issues in their organisations. Unless you identify at this stage a likely significant commercial advantage in building something bespoke that your competitors won't have access to, it is generally inadvisable to build a new software system. Be prepared to accept compromise The nature of buying software off the shelf often means it may not work exactly as desired, or provide every feature that an organisation believes it needs. We'd encourage a pragmatic view in looking to accept compromise when purchasing off the shelf software, understanding that particularly the cost benefits can outweigh these downsides. Choose smaller, focused applications that can interoperate As a general guide, we would push customers to prefer several smaller pieces of software that achieve a smaller number of goals well, over an all-in-one behemoth that can be bent a little bit to meet most, or all, of an organisation's needs. Where the software provides good APIs to allow it to play nicely with others, you often find a better user experience is provided by these smaller, lightweight applications, that choose to do one job, but do it particularly well. For example, many organisations combine invoicing and time tracking software Harvest with accounting software Xero to allow Xero to automatically reconcile payments with invoices. Focus on middleware For many organisations, the valuable software to consider building bespoke is around the 'middleware' – that is the software that stitches together their various software systems. This can often be the truly unique part of a businesses software platform – the point where various systems converge to solve the problems of that organisation. Maintaining your own software is costly Maintaining software that you build bespoke can often be much more expensive than that of off the shelf software. Depending on the licensing model for bought software, you may receive updates for a known period of time, or pay for upgrades and support on an annual basis. If the software has a large number of customers, you'd hope to benefit from economies of scale, with the cost of maintaining the software borne by the many customers of the software. It's almost never correct to expect that software you build will require zero maintenance. It's close to certain that bugs will crop up during the running of the application based on parameters that weren't covered in test. Software seldom runs in isolation, so it's likely external changes will influence how the software behaves, and changes in the devices that customers use to access or run the software can be a constantly moving goalpost. When you build your own software, you'll also need to be aware of the costs to run or distribute the software. Many examples of modern software are delivered on an \" as a Service \" basis, which offers a turnkey solution to adopting software systems. Buyer beware It's very much a case of buyer beware when commissioning your own custom software to be built. Too often we see customers embarking on the journey because they've assumed there is no other sensible course of action. Sometimes that can be the case, though we'd like potential customers for custom software to be sure that they're clear of the business case for building something bespoke, and that case shows there's a clear commercial advantage for doing so. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-07-19"},
{"website": "Made-Tech", "title": "The benefits of a pull request workflow", "author": [" Alex Minnette"], "link": "https://www.madetech.com/blog/deployment-by-pull-requests/", "abstract": "Note: Article edited on the 4/12/2018 At Made Tech, we’ve experimented with a number of approaches to improve our code workflow, for example we’ve been using continuous delivery extensively to bring code closer to our users. We’ve also switched to a pull request-based (or merge request on Gitlab / Bitbucket terminology) development method for our projects and have seen many benefits from it. If you are considering adopting this development method, here are the most compelling benefits plus some tips we want to share with you to help you implement it: How the workflow works The master branch becomes a production-ready branch and the only commits on master come from pull requests. Instead of everyone pushing directly to master, branches are used for every pull request. The pull request branch should be rebased often against master to make sure that branch remains unaffected by merge conflicts and breaking changes. Benefits Peer Review This pull request based deployment enables easy peer-review of the code. Instead of pushing directly through to master, everyone can comment on the changes and ask questions before merging any piece of work. This approach has helped to solve numerous misunderstandings during our epics at Made Tech and greatly improved code quality. Feedback is easier to gather if the pull request is small, so some effort should be spent to make more concise pull requests. Sufficient testing and better stability Every time a commit is pushed to the branch, all the continuous integration tests are executed against the codebase. This ensures that every piece of code reaching master is usable and production ready. The master branch, instead of being continuously broken on every commit, can now be used locally to ensure that the changes made are valid. No pull request is merged without waiting for the build to finish, which verifies that the work being done corresponds to the project standards in feature/unit tests and code quality. The codebase has better coverage and the stability of the code is greatly improved. Having more rigorous testing helps to reduce the work upfront to fix the codebase whenever a problem occurs. Reducing conflicts The pull requests are kept open just for a small amount of time, to get enough reviews and merged as quickly as possible. In order to support this workflow, the pull requests are kept as small as possible on purpose so they can be reviewed easily and merged quicker. Multiple pull requests can be opened if the work to be carried is too large to fit into a single one. This workflow of staying close to master helps to reduce the amount of conflicts. Team members are encouraged to work on smaller, individual tasks that are easily committed, which also helps with this problem. Developers, thanks to peer-review, are also increasingly aware of what is happening on the codebase, which means there is less of a need to dig into individual commits since requests for external review come quite often. Continuous Delivery Because the master branch is now much more stable and production-ready, deployment to production can be triggered at any time without any additional effort. Frequent production deployments are contributing to reduce the friction between the users and the code being produced through quicker development cycles, and more frequent user testing. Clearer responsibility Each developer is now responsible for the creation of the pull request and can address the peer-reviewed feedback given directly on the pull request itself. Developers have a greater sense of code ownership and responsibility since they are the only one giving the final green light to merge and deploy their code. Conclusion While this pull request based development also has some disadvantages (mainly slower merging due to the peer-review), we have witnessed a lot of positive improvements in our projects ever since we implemented it. As a result, we have rolled out this development method to all our projects within Made Tech. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-07-27"},
{"website": "Made-Tech", "title": "Design Patterns: Strategy", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/design-patterns-strategy/", "abstract": "Design patterns are solutions to software design problems that are presented in an almost conceptual way. That is to say, a given design pattern has the potential to be applied to a piece of software written in any number of languages but, at a code level, it's up to the developer to interpret that idea and make it work for them. In my last post on design patterns, I discussed the Observer pattern , which is basically a way to have a class notify other classes (called observers) of a change, without the original class needing to have explicit knowledge of its observers. In this post, I'll be discussing the Strategy pattern, another of the behavioural design patterns. I'm particularly comfortable using the Ruby language, so the examples here will reflect that. Problem: Using a video game, Overwatch, as an example (because I've been playing a lot of it recently), let's say you have a Player class, and it has many attributes, such as primary_ability, secondary_ability and ultimate_ability, that will be used throughout the game. People playing the game have the choice of 21 different characters to play as, and each of them have different values associated to those three attributes, so we need to be able to efficiently map those values to the Player class each time they select a character. A user is able to switch characters as many times as they like during play, so it's important that this runs as smoothly as possible. Solution: Without thinking about it too much, it might be tempting to plough ahead and create a Player class that looks like this: class Player\n  def initialize(character)\n    @character = character\n  end\n  ...\n\n  def primary_ability\n    if @character == 'D.Va'\n      'Boosters'\n    elsif @character == 'Lucio'\n      'Crossfade'\n    elsif @character == 'Reaper'\n      'Wraith Form'\n    ...\n    end\n  end\n\n  def secondary_ability\n    if @character == 'D.Va'\n      'Defense Matrix'\n    elsif @character == 'Lucio'\n      'Amp It Up'\n    elsif @character == 'Reaper'\n      'Shadow Step'\n    ...\n    end\n  end\n\n  def ultimate_ability\n    if @character == 'D.Va'\n      'Self-Destruct'\n    elsif @character == 'Lucio'\n      'Sound Barrier'\n    elsif @character == 'Reaper'\n      'Death Blossom'\n    ...\n    end\n  end\nend\n\nplayer = Player.new('Lucio') Obviously, this already looks incredibly unweildy, and I've only listed three characters. Expand this out to all 21 characters and every attribute unique to each of them (speed, health, shields etc), and you'd have something that would make anyone cry. Additionally, if Blizzard were to add a new character to the game, they'd have the unenviable task of needing to go through and modify each of those if/else blocks to accommodate said character's values. This is a big smell, and a violation of the open/closed principle which states that: Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. – Bertrand Mayer So what we can do is create a series of classes known as strategies, one per unique character, that defines how each of those characters implement their various abilities: class Dva\n  def name\n    'D.Va'\n  end\n\n  def primary_ability\n    'Boosters'\n  end\n\n  def secondary_ability\n    'Defense Matrix'\n  end\n\n  def ultimate_ability\n    'Self Destruct'\n  end\nend\n\nclass Lucio\n  def name\n    'Lucio'\n  end\n\n  def primary_ability\n    'Crossfade'\n  end\n\n  def secondary_ability\n    'Amp It Up'\n  end\n\n  def ultimate_ability\n    'Sound Barrier'\n  end\nend\n\nclass Reaper\n  def name\n    'Reaper'\n  end\n\n  def primary_ability\n    'Wraith Form'\n  end\n\n  def secondary_ability\n    'Shadow Step'\n  end\n\n  def ultimate_ability\n    'Death Blossom'\n  end\nend As a side note, if most characters had a common attribute, say jump_height, rather than implement it in each character's class, we could create a Character super class, which each unique character class could then inherit from, and override it only in exceptional circumstances: class Character\n  def jump_height\n    0.5\n  end\nend\n\nclass Dva < Character\n  # No need to override jump_height\n  ...\nend\n\nclass Genji < Character\n  # Allow Genji to jump real high\n  def jump_height\n    1\n  end\nend Either way, now, the Player class doesn't need to know about the specifics of how each character implements each of their abilities, we can just pass a particular character to it and be on our way: class Player\n  attr_accessor :character\n\n  def initialize(character)\n    @character = character\n  end\n\n  def character_name\n    \"Current character: #{character.name}\"\n  end\n  ...\nend\n\nplayer = Player.new(Lucio.new)\nplayer.character_name # 'Current character: Lucio'\nplayer.character.primary_ability # 'Crossfade' Now I'm playing as Lucio, but he's not being particularly effective and I want to switch characters, so all I have to do is call: player.character = Dva.new\nplayer.character_name # 'Current character: D.Va'\nplayer.character.primary_ability # 'Boosters' We've cleaned things up so much so that new characters can be added with ease by creating another strategy with the same methods (similar to the Template Method Pattern ), while the Player class no longer needs to be altered to take advantage of the abilities they bring with them. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-07-28"},
{"website": "Made-Tech", "title": "The Best and Worst Times To Pair Program", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/the-best-and-worst-times-to-pair-program/", "abstract": "\"Pair Programming\" is two developers focussing on one task and taking turns to \"drive\" the development. Normally this means sitting down together and passing a keyboard back and forth in ten or fifteen minute intervals, but can also mean screen-sharing remotely. We believe that pair programming produces better work for a number of reasons: because two minds are better than one, because the additional attention on your work increases your focus and because you learn new things as you go. However, there are times to pair and there are times to go alone. Best: When you’re starting something new together This is the absolute best possible time to pair. When you're starting a new project, whether you're a team of two or more, sitting down together and spiking the project as a team is extremely beneficial. It means you all start work with a common understanding of how the entire project works. It provides a good opportunity for team members who aren't familiar with new technology or code to learn in the shallow pool, rather than being dumped in the deep end months later. It means everybody is able to get involved in style and philosophical discussions before the code thats laid down inevitably becomes the project standard. This has the same relative benefit when working on a smaller scale, whether it's starting a sprint or epic, or even implementing a new feature in an existing project. Spreading knowledge and discussing the problem will help you a lot in the long run. Pairing in these situations almost always means starting something off in the best way you can. Best: When you don’t fully understand the work or understand the business motivation Although developers should be involved in iteration planning, sometimes you won't be aware of what an individual task entails. Given it's reasonably complex, you may feel you'll take a long time trying to understand and complete it yourself. In these cases pairing with somebody more knowledgeable of the given task can help get it off the ground and ease your discomfort quickly. In fact, pairing is extremely good for knowledge sharing in all cases, even when you're working on something you're already knowledgable about or comfortable with. A senior developer pairing to upskill a junior, for example, will often learn just as much throughout the session. They'll commonly pick up language features or idioms which they haven't seen before, and even teach themselves things as they are forced to focus on the code they've written in much more depth. Explaining your decisions to somebody is a great way to notice flaws, potential edge cases or unnecessary complexity. Worst: When there’s too much work to divide your time The truth about pairing is that, in the worst case, you more than halve your time when you do it, and even in the best cases you're not getting 100% of it. Any reasonably skilled developer can figure out and complete a task by themselves. Putting two developers on a task will generally reduce the time it takes to complete the task, while providing the other benefits that come along with pairing, but they will rarely complete it in half the time it would take a single developer. You have to be aware that no matter what a positive impact pairing may have on the project, in many cases your time would have been better spent doing separate tasks. When there's a lot of work to do and not a lot of time, don't spend your time pairing unless you need to. Worst: When you know exactly how you’d do something Sometimes as developers we envision exactly, in great detail, how we'd develop a feature. We're excited to build it and we know the steps we'd take and the techniques we'd employ. In these cases I've found that it's better to go alone, although it's certainly worth talking through your implementation details with somebody else beforehand as a sanity check and for inspiration. When you pair on features like this, it's difficult to resist the urge to grab the keyboard and type it yourself, or to \"backseat program\" and narrate what you'd type to them. Pairing is about sharing the development and the responsibility, not about following the leader. A pair partner should never behave like this so if you think you will, and you won't be teaching them anything in the process, either stop and try to get over it, or just take the ticket yourself and see it through alone. Worst: When you’re tired of pairing Pairing is a great way to get you out of your rhythm and it forces you to think differently. It encourages innovation, a high quality of code and learning new things. But it gets tiring. Talking all day is tiring. Breaks are encouraged every hour or so, but not on your personal schedule, and the intense degree of focus you have to keep up all day can burn you out by the end of it. It's fun and extremely valuable, but leaving a pairing session and getting back to working on your own can be just as refreshing as pairing is in the first place. Plus, I've noticed that relying on somebody else for too long can make developers lazy. I try and pair about 40% of my time, an amount I'd recommend. I've gone on longer stretches and enjoyed them, but \"too much of a good thing\" can apply here. Use your best judgement: When you’re doing something you always do There are some things you know better than anyone else you work with. The things you work on a lot become muscle memory to you, and you can achieve all those common tasks quickly. They become so much slower when you're pairing. If you know exactly how to accomplish something because you do it a lot, your ten minutes on the clock will either be spent racing through, with your pairing partner struggling to follow along, or crawling along while you explain every action you're making. Their ten minutes will be spent making incremental changes as they learn, while you sit impatiently. You're not enjoying this, but only because the reason you're pairing matters a lot in these situations. When the work is easy for you, pairing to get things done is rarely a good idea. Instead, in these circumstances, you should be pairing to upskill your colleague, or to share understanding of some part of the codebase you're responsible for alone. Storing the shared understanding of a problem or business area in a single person means things can fall apart when they're away. We don't accept single points of failure in our infrastructure, nor should we in our team. Pairing in these situations can an extremely beneficial way to teach team members new things. Ultimately, pairing is nearly always valuable. There's rarely a situation where the work suffers from being developed by multiple people, as you're getting constant peer review throughout development. Although it can sometimes take longer, when it does it's generally because you or your pair are learning something new and being upskilled in the process. This will give you the ability to pick up similar tasks in future and complete those quicker. I learn something new in every pairing session and would greatly encourage it. Just make sure you go in with the right task and the right attitude. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-08-03"},
{"website": "Made-Tech", "title": "How to survive your first tech talk", "author": [" David Winter"], "link": "https://www.madetech.com/blog/how-to-survive-your-first-tech-talk/", "abstract": "I hate public speaking. It's one of those things that I avoid at all costs. Though when my Brother made me his best man, there was no getting out of delivering a speech. And when my colleague signed me up to a do a talk without my consent (thanks Chris!), there was no getting out of delivering a tech talk. My first ever tech talk was to be about how to use Terraform, Ansible and a Makefile to create a Continuous Delivery pipeline by running a single command. It was to be half talk, half live demo. This gave me three pressure points. Live demo The foundation of the whole talk was the code I was going to write so that I was able to demo the construction of a working continuous delivery pipeline in the form of Jenkins installed on brand new AWS infrastructure. I was comfortable with using the tools, but the tooling or using a language is never the issue. It's the end product that is always the difficult, most time consuming part. In hindsight, trying to slot this in with my day-to-day work was very tricky, and took far longer than I expected. I think if I was to ever do another talk, I would avoid having to prep so much code for a talk. Being able to write code in time for a talk is not the only thing that is risky, but a live demo of any form can go wrong. Unless your demo is completely offline, if you can't be guaranteed a decent internet connection, or your talk doesn't depend on any third party services, there are many points of failure that could occur. I was very lucky, and I think most people are. But imagine if it was your talk where everything fell apart?! An emergency demo plan is something sensible to have. Make sure you have a PDF copy of your slides, and presenter notes, and preferably store these on a USB stick. Also, if possible, have a screen recording of your live demo just in case . It won't be live, but at least you can present how things should have been. What am I actually going to talk about? The second pressure of my talk was figuring out what exactly I was going to talk about, how it should flow, and my slide deck. When my colleague Chris signed me up, I think he was quite vague and told the organisers it would be a talk on Terraform and Ansible. I had quite a lot of flexibility with that. The high level tools had been decided for me, so all I had to come up with was something that used these tools. We're big advocates of Continuous Delivery, so using these tools to get a pipeline up and running as easily as possible seemed like a good choice of talk subject. My plan of action for this was similar to how I write my blog posts. Start with a very rough, high level, bullet pointed list of the things I wanted to cover. In Keynote this formed my main break points during the talk. I set out by creating these slides in Keynote as just brief title slides. Then beneath each of these I would create further title slides with minimal words on them. Dragging slides inwards in the outline viewed gave my slides a form of hierarchy so I could see which slides came under which topic of my talk. If you're able to take advantage of another pair of eyes and ears during this stage, then grab them! My good friend Luke had those eyes and ears on offer, so I managed to get some really valuable input from him on slides and things that I might have missed, or assumed the audience might know. When it comes down to your slide deck, keep in mind that slides are cheap, and bullet point lists are boring . Whenever you're tempted to use a bullet point list, break down each point into it's own slide. This keeps things moving, keeps the pace consistent. I'm always haunted by my University days where I had lectures on data structures, and slides of bullet points where the Lecturer would just read off of the slides about how to create a Linked List, with slide transition animations. shudder Keep your slides simple. The fewer words on them the better. And use many slides. Don't be afraid to use the Presenter Notes feature to add extra information to help prompt you while speaking, and definitely use the Presenter Display feature. But, do try not to just read off of your Presenter Notes (more on that below.) Talking out loud The final pressure is the talk itself. The big day has arrived. Make sure you have water to hand. Don't drink too much of it beforhand, it's mainly there so you can clear a dry throat while talking. If needed, have a beer to calm your nerves. Mindfulness is all the rage these days, try and keep a focused of your breathing, relax, and keep calm. While focusing on your breathing, try and keep calm, and try not to talk really fast. Focus on your pace while also delivering your talk. Talk slowly on purpose. Don't worry about the audience. Everyone says it, but people aren't there to try and pull you up on any mistakes you may or may not make. Sometimes people might ask questions during your talk, don't let this off-put you. Unless you claim to be an expert at the very start of your talk, don't be afraid to say you don't know the answer to a question. This was one of my original fears for giving talks, not knowing how to respond to questions. People often ask questions that have very specific scenarios and are usually going to ask how they can use your topic to fit into these. People can't expect you to know the answers, or know how to apply the thing you're talking about to all scenarios. When in doubt, defer, and tell people to come and catch you after your talk for a drink to talk about it further. Something I found really difficult was trying not to stare at my laptop/read off of my slides the whole time. Try to make eye contact with your audience. Try not to look at the same person all the time. To anyone reading who is about to do their first talk, keep calm. Breathe. Talk slowly. It won't be as bad as you think it will be, and once you have one under your belt, the next one will be a breeze. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-08-10"},
{"website": "Made-Tech", "title": "Planning is hard, can we do Kanban instead?", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/planning-is-hard-can-we-do-kanban-instead/", "abstract": "Planning is an activity that usually results in some emotional response, however it can generally be said that teams avoiding planning will also be avoiding thinking about the future in general. A lack of robust planning the detail in your organisation can bubble up to impact plans at the high level portfolio, causing serious consequences on Lead Time and Delivery Rate. Whether your experience with planning is a positive or a negative one, it is a skill that must be learnt and mastered. When teams start out in Agile they will typically venture towards no planning or Scrum. Implementing Scrum first will generally be an easier task. A good litmus test for when you might be ready to try out more lean/advanced practices is when you feel the need to pivot more frequently than Scrum typically allows you to. Ceremonious planning Traditional software project management usually centers around some kind of ceremony when it comes to planning. This can result in situations where the granularity of planning can be too much, or the decision framework it provides is there to control change to a plan made in a previous planning session. This type of environment leads to progressively decreasing productivity. What the business wants to receive is prevented by the same framework that is supposed to aid it. If a lot of your planning artefacts are going unused during development or need to be reworked entirely, then it is likely that you are over-planning. Typically ceremonious planning leads to an environment where teams only communicate once per week (or month/quarter), and they are attempting to plan the entire week ahead. No planning An answer to ceremonious planning is to remove planning from the equation entirely! Meeting once a week? Who needs that? This can work to reduce planning overhead massively, but this introduces the risk that business objectives wont be met. Highly performant teams will want to ensure they have appropriate robustness in their planning activities, and this is very rarely achieved by having no planning. One solution is to go back to meeting weekly, right? Scrum Scrum answers this with a combination of sprint planning and daily standups, both of which have some amount of ceremony attached to them. Sprint planning The purpose of sprint planning is to put a timebox on a set of deliverables that the team has committed to, and offer that as an iteration cadence to the customer. A sprint is usually planned every 1-3 weeks, and this can be a fairly heavy-weight session. It is hard for a Scrum team to change course in the middle of a Sprint, which may feel helpful for teams new to Agile, but highly-performant teams want to be able to change direction on a daily basis! Daily coordination The purpose of the daily standup is to improve team communication while in flight and there are three basic questions that create a standup meeting: What did you do yesterday? What are your plans for today? Do you have any impediments? This certainly helps teams plan their day within the context of the already planned Sprint. It is common for team members new to Agile to be unable to grasp the importance of the daily standup, and to add to the potential pain points it can be time consuming when facilitated badly. Inexperienced teams tend to focus on what they are going to say during their turn in the standup. As a result of this they forget to listen to each other which defeats the purpose of having them, and can contribute to the feeling that they are boring or time consuming. The general suggestion is to ensure that these meetings are to strictly take less than 15 minutes. Scrum requires team members to attend standup at the same time every day, for geographically non-colocated teams, or teams in different time-zones this can be logistically very difficult. This makes Scrum potentially inappropriate for agile teams with these scaling factors. Lean-coordination (Kanban) What is Kanban? At the very basic level applying Kanban is as simple as having a Kanban board (e.g. on Trello) to visualise the flow of cards (Kanbans) through a work system, where each piece of work exists as a single item on the board. As Kanbans progress they typically individually move through a series of states, represented as columns from left to right, until they are complete. The prevelence of such tools has enabled teams to create Kanban-like flow systems, dubbed Protokanban. What does mature Kanban look like? 1: Defined commitment points It is quite common for Protokanban boards to comprise of To Do, In Progress and Done columns. This is a potential sign that there is no defined commitment points, especially if requesters have write access to the board itself. Risks associated with not having defined commitment points: Perception that your team isn't performant Perception that Jira/Trello is a black hole for their tasks. Knock on effects could be that requesters stop requesting tasks (which can lead to high value items not being ever looked at) or, Requesters lose fundamental faith in the ability of the Service Delivery Team (SD Team) 2: Work in progress limits To prevent SD Teams committing to all the work, Kanban imposes strict WiP limits. Work in progress includes work that has been committed to! As soon as the SD Team has committed to a work item, the clock is ticking until that work item is considered Done. This forces teams to meet often with the Service Request Manager (SRM), who is roughly equivalent to the Product Owner in Scrum, whenever they have run out of work items to complete. 3: Prioritisation Kanban coordination generally relies on the SRM pre-prioritising (and pruning) Kanbans before any coordination meetings. Typically the prioritisation approach used by the SRM is centered around urgency modeling. Kanbans with higher urgency make their way to the top of the priority. The main purpose of coordination is to apply weighted-shortest-job-first (WSJF), and select the tasks which have the highest Urgency associated with them with the lowest Cost-to-complete. Weight is provided by a model of Urgency (which gets mapped onto cost in $ per week) Shortest-job is estimated by team members (potentially ranged estimate) Highly efficient Kanban SD Teams tackle the shortest-to-deliver most costly-items-to-delay first. 4: Frequent coordination meetings This changes the focus of a Kanban coordination from the standard Scrum three questions, to a discussion about the work. In other terms – which task am I doing next? As a result of work in progress limits, Kanban SD Team members will generally meet as-and-when with the SRM. This means the SRM is a role that needs to be highly available. Due to the heavy work focus of the coordination meetings, Kanban provides less of a feeling of team comradery when compared to Scrum. Adoption of Kanban works best for teams that have already proved that they gel really well. This is partly the cause of another one of the downsides of the Kanban approach: it may fail to discover collaboration issues and/or dependencies. As the focus is on delivery to an external stakeholder to the SD Team, Kanban can also hide expectations between SD Team members. Extra effort must be placed on ensuring that internal expectations are voiced. Scaling It is recommended in Scrum to generally not scale a single team beyond 9-12 members (with even 9 considered extreme by most), by comparison it is generally possible to scale a Kanban flow-system beyond that (with teams seen in the wild at 40-50 members). So a Scrum-approach or Kanban-approach? In our experience Scrum is best suited for environments where the product development team is defining both the problem and the solution, and it needs to interact with the rest of the organisation to collect feedback so that it can locate problems. By contrast we have found Kanban to be extremely fitting in both this scenario and for steady streams of work-items, like support requests, that come from an external requestor. The benefit of Kanban is that it can be used internally within a product development team to pivot quickly, as in a lean-startup model, in this situation Scrum would feel like change-control beaurocracy. One consideration with the Kanban approach is that it works best in highly-experienced Agile teams. The reason for this is that it is a more lightweight governance approach, team members less familiar with Agile will struggle to keep focus. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-09-19"},
{"website": "Made-Tech", "title": "Semantically Stable Test Suites", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/semantically-stable-test-suites/", "abstract": "When we run a test suite in most languages, we can also generate reports with percentage of code coverage. These reports aren’t all they are cracked up to be. The way that testing tools generally create these reports is by analysing which lines of production code were executed while the test suite was running. It then can, if you want it to, output a report on how many lines/methods/files were covered by your test suite. This coverage percentage is generally used as a reliability metric of a given test suite. Great! Since we have a metric, we can now fail the build when the code coverage is too low… Unfortunately it is not as simple as that. The problem with code coverage is that it is a lie. This lie forms the reason why you cannot trust this metric. It is also the reason why you should not fail the build because of this metric. Uncovering the lie Remove all the assertions from your test suite, now rerun it. Your code coverage metric will be unchanged . What does this mean? Simply put, it means you cannot trust the Code Coverage metric. Semantic Stability A semantically stable test suite is one that fails (given that there are two possible final states: pass or fail) when any semantics of the production code that it is covering is changed. What this means in practice is that every aspect of the production code has a reason to be there and that changing any aspect of the production code will cause the test suite to fail. Of course, the production code could be modified to retain its semantics, but be implemented in a different way. As an example it should be possible to swap out a merge sort for a quick sort, and not impact the semantics (the fundamental useful behaviour) of the system under test. As a potentially more complicated example, one could also swap the disk scheduling algorithm used by a disk driver from first-come first-serve (FCFS) to shortest seek time first. However it could also be much more subtle: for example, a semantically equivalent version could be one which makes it a little easier for the next programmer to understand, or easier to extend. Semantic stability as a metric Luckily there are tools available to measure semantic stability, called mutation testing tools. Mutation testing tools generate many versions (mutants) of your production code, with very small changes (mutations) made in order to attempt to make your test suite not catch those changes. Similar to code coverage, these tools provide a % of lines which when mutated cause the test suite to fail; this is a good thing! When this happens we have semantic stability. The terminology for this in the mutation testing world is that the test suite kills the mutants. Achieving 100% semantic stability Test Driven Development Following the test-first discipline of TDD strictly, whereby no single line of production code is written without first watching a test fail for that change to be made, produces semantically stable test suites by default. Mutation testing tools Alternatively it is possible to use mutation testing tools to create semantically stable test suites. The method behind this approach would be to write assertions in your test suite, for all elements of your code that can be mutated without being killed by the test suite. The problem with this approach is that it is time consuming, due to the fact that the tooling is slow. Who needs semantic stability anyway? Both the percentage of semantic stability of your test suite, and the percentage of code coverage are asymptotic in nature with respect to the cost. In that it is increasingly more difficult to achieve as you approach 100%, and it is not usually practically feasible to achieve 100%. In well-designed systems uncovered lines tend to not change, they tend to be calls to the system, external systems, libraries or general IO. They tend to be low-level details. It is possible to push the areas of low code coverage to the outermost extremes of your application. Treating them instead as details that get in the way of shipping working, high-value software. The customer cares about high-level policy High-level policy is the business logic. It is the logic which in an accounting system calculates tax on an invoice, and knows about double-entry accounting. As such it is only this application code that both you and the customer care about. In this world of high-level policy, we do not concern ourselves with low-level detail like databases, and web-frameworks. These are things that only programmers care about, these details only get in the way of describing the problem domain at hand. Really your customer cares about two things: A: That changes to high-level policy can be made cheaply. B: That the system works the way they currently have described they want it to. The best way to make it possible to cheaply change high-level policy is to ensure you have tests. However it becomes significantly cheaper to ship changes to high-level policy when you have a semantically stable test suite. Secondly, the best way to ensure that the system works is, surprisingly, to write tests. You cannot be certain that a system works 100% as you expect it to, unless your test suite is semantically stable. Software should be soft Fear holds back practices like refactoring to remove technical debt. Fear of changing the production code ultimately stems from distrust of the test suite. The fundamental role of software is to be soft. It is supposed to be cheap to change and remould to perform new behaviours while maintaining the old behaviours that are already depended upon. To that end I would say it is very rare for a customer to ask to remove a feature from a system. The common use case is to extend and build upon the features that are already there. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-09-27"},
{"website": "Made-Tech", "title": "Code reviews using the pull request workflow", "author": [" David Winter"], "link": "https://www.madetech.com/blog/code-reviews-using-the-pull-request-workflow/", "abstract": "As developers we always appreciate a second pair of eyes and an extra brain. The eyes are really helpful for catching that extra whitespace you might have missed. The additional brain power might help you solve a problem in your code with 5 fewer lines. All of this results in better code and more collaboration. A way of formalising code reviews within your organisation can be the pull request workflow , which aims to encourage regular code reviews with minimal disruption to your productivity, while gaining tremendous value. Why code review? **Knowledge Share** If you're new to an existing project, what better way to get valuable insight to the workings of it than getting someone familiar to have a look over your changes. It's really difficult to sit and read through the many lines of existing code to fully understand what is available for you to use. They'll have used existing functions a lot more than you, and will help to reduce code duplication, between you there might be a more efficient combination. The code review encourages the start of conversations that lead to improvement of the overall codebase, sharing of best practices and experience from both the reviewee and reviewer. It's important that reviews are treated as a positive tool. While it's easy to be defensive of your work there's probably a reason a reviewer is suggesting an alternative. However the reviewee should feel comfortable to start a discussion about suggestions provided, it's a good chance to learn. Developers shouldn't fear having their code picked over as comments provided should be constructive and it allows to them to gain real insight from their peers. Likewise, a reviewer should always feel comfortable providing constructive criticism if they feel it will lead to improvements. **Visibility** You may not just be a new developer joining an existing project, but in fact a new starter to the organisation. Having frequent code reviews is a great onboarding mechanism to get new starters involved in the process early on, helping them become familiar with alien codebases. More often than not, they'll also have new ideas, or other experiences that you can benefit from. Making the code review process as transparent and as open as possible will only encourage this. It also doesn't restrict the conversation to single teams, but the wider organisation can always input too. **Standards** Code standards aid in readability and maintainability of code. Sometimes standards can come in written form—a large set of rules to follow—but other times they can be unwritten rules that you'll only really learn the more you develop within an organisation. This is where that extra set of eyes come in handy. Ensuring that standards are followed doesn't require much brain power, but they are often easy to miss, especially if you're unfamiliar with them. Catching these violations early on save time in the long run and allows everyone to be on the same page, ensuring good readability. Also the opportunity to open conversations around the standards themselves. Whitespace vs tabs…ducks **Testing** While a reviewer is normally checking over implementation code, a review offers the opportunity to ensure good practices have been followed while developing code, for example Test Driven Development (TDD). It's important to make sure tests are present as part of a review if this is a practice your organisation adheres to. Tests, hopefully, allow the reviewer to follow the design of the implementation while also opening up another area for improvement. Reviews are a great way to ensure that the test provided are valuable and efficient. An important question to ask here; do the tests cover all the changes? **Catching bugs** Having another developer look over your work also provides an opportunity to catch any bugs you may not have noticed. While you might have a wonderful green test suite, a peer might be aware of another edge-case within the project that would otherwise has slipped by. **Readability of code (comments not required…)** At Made, we feel that if code isn't understandable without comments then this represents a smell. While it's common to see this crop up as a suggestion, is it really required if your functions and variables are clearly and consistently named throughout and have obvious and sensible data returned. Comments can provide value in some cases but they should never be a hard requirement for a code review for us. **Checklists not required?** While some of the above may look like a checklist it's not. We're trying more to present some best practices for code reviews, not rules that you have to follow. We'll dive deeper into this idea as we expand on the value we derive from having adopted a Pull Request Workflow. Some of the things we've covered above can be easily automated to make code reviewing more valuable as a result of this. Pull request workflow Pull Requests (PRs) allow for a standard and efficient way of doing code reviews within an organisation. Most popular tools these days, such as Github or Bitbucket offer features that allow for easy adoption of the pull request workflow. Adopting the flow PRs revolve around the idea of using dedicated branches for small feature sets. The branches of work are then submitted to your source control tool (we'll use Github from here on in our examples), and are opened up for review amongst your team. Only when the majority of people involved are happy with the work, will it then be merged into your master branch. Branches Working in isolated branches reduces the risk of conflicting with other developer's work. By not working in master, you can remain focused on your goal rather than constantly having to pull in others code. Single Responsibility Pull Requests The core idea behind this area is that the less code there is to review the more valuable the review will most likely be. Small features covering only a single area allow for a hyper-focused review and clear understanding of what's trying to be achieved. A reviewer can easily tell if the tests are present, valuable and covering these small chunks. Working in this style makes it easier for the reviewee themselves to write the tests and feature. Short lifespans A great way to avoid merge conflicts with other features and stale code is to impose a completely artificial lifespan on a PR. Whether it's a day or just 15 minutes making sure these don't hang around in limbo is an efficient way to maintain momentum on a project. If more work arrises out of a review don't just stop the conversation, move it out to an issue or multiple issues and assuming everything has been signed off merge the PR. This allows for more time to be spent on the conversation and potentially more opinions to be provided and more thought around the area. Sign off While it can be tempting to review your own work if others are busy… don't. This would render the whole process pointless, as remember that it's the extra eyes and brain power you were after in the first place. It would be equivalent to working directly on the master branch. Try and encourage a culture within the organisation where people are available to comment on PRs as they're submitted, even if they're working on a different project. This encourages having all repositories on Github, and their PRs open to the organisation. Tooling the flow We've talked about code standards and tests as part of your review process. These usually follow codified rules that can easily be automated with modern tools and services. Using your platform fully Knowing how to leverage your platform for the easiest adoption of this work style is a lot simpler than it may seem at first. Minor things such as having a clear and brief title and an accompanying description explaining the feature can make it much easier for the reviewer to quickly grasp the purpose of the PR. This can allow them to assess if they have relevant input or want to involve others and also that the feature matches the description provided. Keeping the conversations around reviews and PRs within the PR itself is the best way to ensure you don't lose any knowledge that surfaces. While it can be easy to take the conversation offline, to email or to Slack anyone who comes along after will be missing potentially vital context. Tests and code standards Github, Bitbucket and other platforms allow integration with 3rd party services or your own Continuous Integration server. These can be used for automatically running your test suite whenever a pull request is created and updated. This gives constant feedback to the reviewers of a PR, helping them know tests are passing, meaning they don't need to pull down your code and run the tests themselves. The same can be done for code standards. Linting services are available that can be integrated directly into Github. All of these integrations mean the reviewers can focus on the feature being developed and best practices around the implementation of that code, which is difficult, or impossible to automate. Notifications Github has notifications built into their PR functionality. In its simplest form it sends emails for when a PR is created and also when comments are made. Integration with chat applications, such as Slack, can be added to your Github organisation to give even faster notifications when PRs are ready for review. This can help you have short lived PRs. When PRs are merged in notifications are sent too. Optional extras Github allows protecting branches meaning you can lock down master to avoid anyone pushing to it accidentally. More usefully however, it also allows for PRs to only be merged if the tests and/or linting is in a passing state. While not essential, adopting these extras can help with keeping master safe and secure from accidental commits. Visibility Having the ability to look back and see who introduced a feature and the conversation surrounding reviewing it is a valuable way to foster knowledge sharing. If someone is looking to develop a new similar feature or improve upon the original, they can see from the description and conversation why the feature was implemented in this fashion. We believe using code reviews and pull requests in tandem gives you the most value in terms of time, knowledge shared and potential cost to clients. The buy-in to adopt this workflow is far less these days because the tools make it far more accessible to the majority. It can be rolled out across a team, a project or the entire organisation. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-10-04"},
{"website": "Made-Tech", "title": "Effective Client Showcases", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/client-showcases/", "abstract": "At Made we host regular client showcases, this is an opportunity to sit down with the client to discuss how the iteration and the project as a whole are progressing. Who should be involved with a showcase? The most important people to be involved in a showcase are the developers working on the iteration and the key stakeholders on the clients side. A showcase cannot happen without these people. Additionally, any members of the client's business whose lives will be improved by the work carried out ought to be involved, such as a customer service representative or an e-commerce manager. These people will have valuable, on the ground insights into existing problems it should solve and potential challenges the work may introduce before it's too late. An alternative showcase can be one made internally, to educate other teams in your organisation on things you've learned, the project you've been working on, any reusable software you've built and practices you'd like others to adopt. It's important to do this formally as a showcase, because it allows people the opportunity to comment and give feedback. You don't get this kind of honest technical feedback from clients, so it can be more than a chance to share, it can be invaluable peer review. Why do good software teams showcase? A showcase is a prime time to get your work in front of all key stakeholders to demonstrate how your work is progressing, and how this impacts the project and their business. It is extremely important to have these regularly, to ensure client and developer expectations are kept in sync. Without this regular contact point, delivered work may fall short in some areas and over-deliver in other areas, both of which are a waste of developer time and the clients resources. By instead ensuring you and your client are on the same page, everybody will be happy with the delivered iteration, and any potential problems will surface before it's too late, again saving time and money. Additionally, by involving people in the showcase who will actually personally benefit from the work being done, this gives you an opportunity to demonstrate and teach processes and functionality to these end users. We notice this in particular when building new tools for businesses. Overall it reduces the time spent up-skilling these people later on, and gives them a good opportunity to recommend improvements. Furthermore, a showcase is an excellent forum to openly discuss any feedback the client may have on the past iteration and the shape of the project in general, and to line up the goals of the next iteration if relevant. We've found it useful to give internal showcases to people outside of the project team, so that we can update everyone across the company on what the team have achieved, what they've learned, any practices the wider team should adopt or avoid, and any technical achievements that could be co-opted into other projects. Introducing better showcases into your team Things which are important to consider when introducing showcases in a project are structure and frequency. Enforcing good structure in your showcases will ensure the time is well spent for all parties. Make sure the first and most important part of the showcase is demonstrating what you've been up to, discussing any problems you faced and solved and any problems you still have, whilst not allowing interruptions or tangents to take place until the end. The benefit of this is clients have time to consider, condense and prioritise feedback, and those leading the showcase are able to make all of their points undistracted. Additionally, showcases provide a personal growth opportunity for developers. We believe showcases should be led by developers. This puts a certain amount of pressure on them, but we think it's beneficial pressure to have, because they are the people with the best understanding of the work, the people who can most easily gauge and accept feedback and the people who will have to carry out the rest of the work. Additionally, it encourages their best work as they know they will have to stand up and be accountable for it in front of clients. We've found this kind of growth is often best achieved by pushing people in at the deep end and providing them with a float, rather than wading them in gently. This sink or swim approach sounds brutal, but encourages autonomy and personal confidence in the individual. Preparation During an iteration we find it useful to keep note of all the wins and challenges we've had, on top of the tasks we've completed. This enables us to easily prepare a run down of the iteration. These run down documents then get stored inside the project, similar to a changelog to provide a contextual history of a project. After the run down has been prepared it is often valuable to run through it with other members of the project team to ensure smooth delivery, and catch any shortcomings, complexity, and omissions. This is particularly important when finishing an iteration where it is important to put your best foot forward and deliver a slick demo. Additionally, a practiced showcase enables you to efficiently time-box the demonstration, as an ideal showcase lasts a maximum of 10-15 minutes. If you find your showcases are longer than this, consider shortening your iteration times and focusing on smaller chunks of work. Keeping it concise also means you also allow time for questions, feedback, and thoughtful discussions about the work presented. Mid point showcases A mid point showcase is an opportunity to present work in progress and ensure the client's expectations are being met before the end of the iteration, where it may be too late to change, and certainly more costly. Having frequent showcases allows them more opportunity to give feedback and have more of a say in the way a feature is developed. Developers may also discover areas of improvement over the original specification of the feature, having built and used it. Features are often not built exactly to specification; UX and UI changes are made throughout the build and, as web professionals, we're well placed to make this kind of feedback to our clients. Secondly, as this is work in progress, it's important you make this clear to the client beforehand, so they're confident you'll be able to work on the fine details together, and focus on whether the high priority, wider feature is being completed correctly. For example, frontend tweaks can be made, but the backend logic is coming together as they wanted. If you do receive any feedback, it is important to keep it focused on the demonstrated functionality, not inflate the scope of the iteration, and to do your best to only capture top level information as more detailed discussions should be made with the client when you come to address it. You want enough to be able to triage it later, but not so much that the showcase goes on too long or anybody loses focus. It's also an excellent opportunity to set expectations early if you feel you won't complete the piece of work by the end of the iteration, and give clear honest reasons behind this, for example if a \"quick win\" task which we didn't expect to take long ended up taking a day. This will prevent the team from having to break bad news at the end of the iteration when work is incomplete, when the client was expecting fully featured work. Being honest about these problems builds a better relationship with the client and ensures they understand that you faced an unforeseen problem, and not that the team were slacking off. It's positive to build this relationship of trust, and that comes from openness. Additionally, bringing up adversity early gives the client an opportunity to back out early rather than wasting further time. For example, if the team was to discover the work would take twice as long as expected, the client should have an opportunity to re-evaluate their business priorities if they have other high priority features that may now be considered more valuable. Furthermore in this situation they may want to park the other work if they were depending on the time staying fixed, as they have other time sensitive work upcoming. End of iteration showcases The primary purpose of the end of iteration showcase is for the team to present all the work they've agreed to deliver, excluding any work previously removed from the scope of the iteration, which should have happened at the mid point showcase. Not to mention, a perfect time to show off. During the showcase you should make a point to highlight unseen wins, for example shining a light on complexity within the iteration where it existed. This builds your client's confidence in your teams ability to deliver technical work and solve problems. Also point out areas where the team have gone above and beyond to deliver unexpected value. You can often find easy opportunities to improve a feature by chance, that is a quick win to implement, and highlighting these value adds delights the client. This promotes a more trusting \"Adult to Adult\" relationship. It will be beneficial to the ongoing project if you are seen as partners rather than contractors. If you've taken the time to implement additional functionality to help an end user, which you believe will save the client time and money in the long run, you should take the time to point these out as well. Although ideally the work your team has delivered will be as expected by the client, often additional requirements fall out of new features, which should be captured at the end of your showcase. If they are minor tweaks, these new requirements should be stored in your project backlog and then prioritised by the client. However you will sometimes begin to pad out the next iteration off the back of a final showcase. Post Showcase The time after a showcase, before the next iteration begins, is a good chance to reflect on the iteration. It's worth congratulating everybody on a job well done by highlighting things individuals, and the team as a whole, did well, and which you'd like to see happen more. You should also take the time to evaluate areas of the delivery and development of the iteration which didn't go so well, which should be learned from for all future iterations. To name a few examples, if the iteration was slowed by interactions with a third party which could have been mitigated beforehand by starting conversations earlier, if you began work before the design was finalised, or if you waited too long to get code into a production environment. These are all lessons we've learned from over the years. Furthermore, when celebrating triumphs and learning from shortcomings, it is important to share this with your wider organisation. For example, if you've solved a recurring or complex technical problem and produced a reusable solution which other teams could use, or discovered a flaw in a company process which you'd like to evolve together. Keeping these learnings to yourself can cause fragmentation in the knowledge of different teams, and prevent you from raising everybody up together. A rising tide lifts all boats. Conclusion To reiterate, hosting frequent structured showcases, is an excellent way to keep lines of communication open, and ensure client expectations always meet your own. Having developers run these makes them accountable, promotes autonomy, and boosts confidence. You should use them to share your work, capture constructive feedback, and backlog new requirements. Don't be timid when you encounter adversity, and involve the customer when you do. This ensures they are able to make informed, timely business decisions. After your showcase learn from your strides, and miss-steps, and take the time to spread this knowledge amongst the wider organisation. A perfect showcase involves a minimal number of people for brevity, but these need to be the right people. You should invite key stakeholders along, like the project manager, as well as a relevant end user. Most importantly keep your client happy, and keep the work flowing! Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-10-20"},
{"website": "Made-Tech", "title": "Cohesion, coupling and viscosity", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/cohesion-coupling-and-viscosity/", "abstract": "One of the most important goals of a software engineer is to craft highly cohesive code. Cohesion refers to the grouping of code in a software system. Code is highly cohesive when the code contained within functions, methods, classes, or modules has much in common. Conversely, low cohesion occurs when elements of the code do not belong together and currently are. A code-base is said to be viscous when it is quicker to add a hack than to preserve the existing design. To qualify this statement, evolving an application's architecture, through a series of small refactorings, while keeping tests passing, is a form of design preserving activity. Another problem to contend with is unnecessary fragmentation of the code-base. Fragmentation does not result in viscosity specifically, but it can make it hard to build a mental map of the application, especially in languages that do not have static types. Fragmentation leads to friction, which is a feeling of lack of productivity that programmers get from a codebase as they attempt to make changes to it. Fragmentation is one of many sources of friction, others include lack of knowledge or experience in a particular paradigm. Systems can be highly cohesive when they are loosely coupled, but it is a simplistic assumption to assume that loosely coupling everything has desirable outcomes. Extreme Programming (XP) practitioners are aware of the practice of Simple Design; amongst other things, minimising the number of moving parts is highly critical when building a software solution. An outcome of having high cohesion and appropriate loose coupling is that it reduces class/method/file/library churn. Remember: code-churn introduces defects; ergo, reducing code-churn reduces the introduction of defects. Tight coupling can be good To discover why it is not always beneficial for areas of a system to be loosely coupled, we first must examine the \"gold plated\" architecture. Not everything needs to be dependency injected or mocked! A consideration of using test doubles is that it creates a test setup overhead. Due to this increased complexity, avoiding mocking is good unless we need to take advantage of class composition. When we do not care about loose coupling is usually when a class provides very generic functionality or very specific functionality. A good example of generic functionality is the Array standard library. We can refer to this library as being very adult, as it has many dependants. As such, it needs to be very stable and unlikely to change. To be tightly coupled to it is okay for this reason. Tightly coupling to specific functionality is useful when a class provides useful behaviour in a very specific domain. When a dependency's purpose is for specific functionality, and we are considering whether to tightly couple to it, care should be taken to ensure that the dependency has few dependents (it is child-like). An ideal number is one or two dependents, but this is not a hard rule. Depending on an unstable class in only a small surface area of the codebase allows the impact of changing it to be small. When tight coupling is bad Exposing unstable dependencies illustrates the problem with tight coupling. Passing instability along a chain of dependencies spreads instability throughout application code, resulting in more viscous code. The strongest form of decoupling exists when communication occurs using simple data structures only. When an area of the system communicates using simple data structures only, I call this a boundary. In object-oriented languages like Java, it is possible to use objects as data structures so long as they do not house any behaviour. Typically I tend to use objects with public fields in Java, data classes in Kotlin, and hash maps in non-statically typed languages. Lack of boundaries, to ensure loose coupling, creates codebases with low cohesion and high viscosity, termed \"ball of mud\" codebases that are hard to change. In the real world, it is common for business rules to depend on unstable child-like dependencies. The easiest way to ensure that the User Interface (UI) does not become coupled to these unstable dependencies is to have a boundary between the Business Logic and the UI. Another angle to look at this problem from is that the UI tends to evolve at a different rate to the business rules. For most applications, though not all, the UI is more likely to change, which is understandable, as this is the most visible part of an application and can be influenced by fashions. Business rules tend to change less frequently and, in fact, many core business rules can be reused across many different end users if the abstraction is both loosely coupled and highly cohesive. Regardless of whether the UI or the Business Logic is changing more frequently, having them decoupled is beneficial to the continuous delivery of working software. An example used by Robert C. Martin is that it allows the business to see the cost of changes for each component individually, rather than as a summed figure. For example, if a client has requested functionality that causes the cost of the UI to be 10x more than the Business Rules, it may question why exactly that is. Would it be better to deliver a cheaper UI now? To bring us back to approaches for decoupled software: one of the most powerful object-oriented techniques is class composition. It is also one of the most convenient and easy to understand ways to achieve decoupling of behaviours. It is wise to use class composition to invert your dependencies around areas which have different reasons to change. As an aside, employing dependency inversion allows the use of test doubles, which keeps architectural options open with regard to things such as the database, caching and authentication. For reasons described above, it is desirable to decouple your business rules from your database and use class composition around this dependency, so that it is possible to: Build a contract that we can create a test double for, which allows the business rules to be tested without the presence of the database, resulting in a fast test suite. To allow the business rules to control the database in highly complex ways. To ensure we can defer decisions about how we are going to deal with things like databases, schemas and ORMs. Database data structures are a very unstable aspect of a software application and are very likely to change. As such, it is bad for cohesion when your UI knows about the database or your ORM. Furthermore, it is bad for continuous delivery when you must change features that also depend on those ActiveRecord objects. Features themselves, housed inside what I call use case classes, should be decoupled from the saving and retrieving of business objects from the database. The database interface expected by the business rules should return business objects. It is perfectly fine for these business objects to house some shared behaviour. These objects should not be ActiveRecord objects, as this would tightly couple the business rules with the highly unstable and very likely to change database structures. To add to the problems of being tightly coupled to the database is that it may force either: The representation of data in the database to be unoptimised for the storage engine. The business rules to be aware of business objects that it should not care about. Naming conventions to be unclear. One common theme I see is paralysis during the software development cycle. Instead of worrying about the behaviour of the software, programmers expend enormous effort attempting to think ahead about what tables, columns, indexes and relations should exist before a line of business logic even exists. It is a much more natural and frictionless process to build business logic first, and then figure out how to store and retrieve these business objects later. Let's be clear. You have tight coupling and a viscous code-base when: Views call methods on ActiveRecord models. ActiveRecord directly handles your HTTP post requests. Rails controllers call methods on ActiveRecord objects. Use case classes call ActiveRecord methods. Fragmentation A good way to detect fragmentation is when related functionality is scattered across many classes, or even multiple libraries. The resulting pain point is that changing the structure of a system requires a mental map of multiple files, with a structure that does not reveal its intent easily. Fragmentation can feel like viscosity, but it is a different problem entirely. In my experience, it is the most common result of gold plated architectures, which is a result of not adhering to Simple Design. SOLID The reason that the Single Responsibility, Dependency Inversion and Interface Segregation Principles exist is to solidify this point about coupling and cohesion. If a class has one reason to change, then it is highly cohesive. If the dependencies are inverted, and the interfaces are segregated, then it is exhibiting some form of loose coupling. I do not believe ignoring these very well explored topics in the field of software programming in the attempt to speed up delivery is acceptable. The main mantra here is maintainability, both short and long term. Maintainability is the primary cost of software over its lifetime (unless its only purpose is to be deleted). Ergo, the primary value of software is its ability to tolerate and facilitate ongoing change. Customers want changes to their software to be cheap, and it is surprising how quickly viscous code can accumulate and begin eating into the budget. Contrary to popular belief, it is only the secondary value of software that it meets the current needs of its users. As mentioned above when referring to XP's practice of Simple Design, high cohesion and loose coupling is not synonymous with architecture gold plating or somehow unnecessary. It is the basic requirement for highly maintainable software. Another way to separate these concepts is to think of two classes of code defect: behavioural and structural. Most programmers, when referring to code defects, think solely of behavioural defects i.e. the code does not function correctly. Structural defects are defects that affect maintainability. Structural defects are worse, as they affect the ability to add features, fix behavioural defects and therefore arguably have a higher cost over the life cycle of a codebase. I also do not accept that once well versed in building decoupled software, practising these techniques slows down delivery of software in the short-term. In fact, in the words of Robert C. Martin, “the only way to go fast, is to go well”. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-11-03"},
{"website": "Made-Tech", "title": "Giving teams freedom to structure their own time", "author": [" David Winter"], "link": "https://www.madetech.com/blog/giving-teams-freedom-to-structure-their-own-time/", "abstract": "It can be scary to devolve a lot of managerial and planning responsibility to teams but we've found lots of positives in changing the way we approach time management. Allowing teams the ability to plan their workloads, holidays, working location and client engagement has resulted in a greater sense of ownership on projects. As a result of these changes we've noticed: Better quality of work through sense of ownership Better value in engagements for our clients Better relationships with the clients A reduction in the overhead around planning for holidays, etc. It's essential however that management are confident enough in their teams to place this trust in them. Communicating with clients Before a team is about to start development, having introductions with the client will prove beneficial during the development period. Traditional approaches have been for there to be a project manager to act as the go between from developers to clients and vice versa. This has always added extra time into a project – usually time spent waiting on responses. This can lead to stop-start interruptions which normally result in developers downing tools, and starting other work to fill the gaps. What harm is there in empowering the team to communicate directly with the client and enhance the relationship? For starters it'll save a bunch of time. When a developer is unsure of something, rather than risking development time that will potentially need to be backtracked later, give the client a call or IM them to get faster feedback or clarification around the work. It's essential that the team and client can create a fast feedback cycle to reduce the potential for interruptions to the development process. Developers will better understand client priorities if they're communicating directly, rather than being shielded unnecessarily from them. It helps that a team is conscious of these, so that they can make decisions that add better business value for the client. When holding standups and meetings with clients the team should keep in mind what the client's level of technical understanding is and make sure not to just use lots of technical speak. If people walk away from a meeting not understanding what's happened then that's a big failing in the communication process. Potentially this can cause misunderstandings where clients and developers aren't on the same page. Through identifying the client's priorities, and the constant communication, the team is well placed to shape the work for each iteration without outside guidance. Establishing work process Early on in building the engagement with the client, the intended work process should be discussed and explained to the client so that they'll be on the same page as the developers when they're introduced. Clients need to be sold on the advantages of devolving management to the team while also knowing they can reach out to others in the business if they need to. One of the main advantages gained through this process is the ability for the team and client to plan short iterations that allow for showcases of the work throughout development. The team should be able to identify the priority for each iteration through this close relationship with the client and be able to depend on the client throughout iterations to provide feedback and guidance around any ambiguous areas. On longer running engagements this should mean the stakeholder on the client side participates in the team's standup and is available via the communication formats outlined above throughout the day. It's important that developers can contact them quickly with any queries or concerns. This also helps foster a closer relationship between the client and the business. Ideally showcases ought to be run by the development team, extra points for rotating chair duties, who should focus on the work that was defined as being part of the iteration following the last showcase. Any feedback that comes from this showcase should be part of the next iteration rather than being added to a backlog where possible. If feedback is left open you run the risk of it getting lost, the client feeling ignored, or the original issue becoming unusable and not providing any value to the client. Establishing the business process Clients are onboard with this new way of working. But is the team? Do they really have the freedom to do all they need to self-organise? Do they feel trusted? We have a handbook that the entire world can see, and allows us to keep our processes repeatable and transparent. The handbook outlines how the business expects individuals to organise things like holidays and remote work (to be discussed later on in more detail) so that the team can effectively shape their workload. By making holidays and remote work visible to everyone via shared calendars, these decisions can be easily made by the team, and also allow the individual to make a quick judgement call on the viability of a request. For example while we might let everyone know we want to take a week off or work from home, we only require our team mates to ok it. Let team members communicate with one another It's equally important for team members to be able to communicate amongst themselves without barriers. Where a team member is shouldn't increase friction in communicating with the rest of the team. Our office is spread over three floors, and people are often spread out over the entirety of it and others are working remotely. Team communication needs to work for all these scenarios. This is where tooling is important for it to be a success. Day-to-day the team need to be able to attend standup no matter where they are. Them being out of the office shouldn't be a barrier to participation. After all, a client is going to call into these, so the team needs to too. Test out the various tools to see what works best for your team and clients and use this wherever possible. Remote work Lots of business are against remote work, as they're worried they can't see people working but it's important when switching to self-organising teams to trust your workers to do their jobs without being watched. When working remotely it's key that team mates and the clients are unaffected, the individual assumes the responsibility or maintaining the everyday standard these people are accustomed to. This means both the quality of work and the actual process, you don't want to be calling your client from a noisy place where the communication will suffer. We feel we've got the correct toolset to allow for the entire company to be remote on a given day. Recently we had to contend with constant construction noise next to the office and all worked elsewhere for a day or two with no impact to the clients, workload or our day-to-day process. One of the things we do to increase people's presence when out of the office is to drop a quick message into our chat when we're going to be away for a period of time like at lunch. Holidays Beyond what we've outlined above the main thing with holidays is ensuring that enough of the team is still available to maintain momentum. As the team is scheduling their own holiday, make sure you don't forget to let the client know too if there are any changes to the team. In some cases other developers can be brought in to cover. Holiday time must be taken into consideration when planning iterations so that they can set realistic goals/targets. On the flip side, it's as important that the client lets the team know if they'll be away on holiday. In these cases an alternative stakeholder should be introduced to the team who'll be a point of contact. Hackdays & learning days As with holidays, since these are usually at least a day in length, the client needs to be made aware of these well in advance and reminded the day before during standup. These normally involve the whole company, so whoever organises them is conscious that the company as a whole needs quite a bit of advance notice, at least a month. We try and balance the frequency of these days. While we see them as valuable to the company as a whole, we don't want clients to feel like they're constantly taking a back seat. Dojos These are often recurring hourly sessions that happen weekly. These are self organised each time and if no one has an idea we don't run them as they're encouraged, but not mandatory. If there is one scheduled, anyone participating is conscious of their current work load, and will only take part if it's not going to affect any the work due to be showcased. We also try to avoid scheduling client interactions when these are taking place to encourage maximum option to participate across all teams. Knowledge sharing It's essential to foster knowledge sharing as a cultural norm when enabling teams to self-organise. When people go on holiday, are ill or otherwise unavailable the team need to be able to keep functioning. To promote this we encourage frequent pairing throughout the company. The constant switching of roles during pairing greatly facilitates technical, domain, and business knowledge to be quickly shared between teammates. It's also a great way to bring a new team member up-to-speed with an ongoing engagement. Before a team member goes on holiday they should be sure to handover anything that only they have had sight of, although try not to let that happen. If this information isn't recent it's worth looking at how communication is working across the team. On the client side, it can be difficult to fully envision and grasp the use cases without going on site and working alongside the end users and stakeholders to understand their goals. Teams should feel empowered and encouraged to go to the client or bring them into the office to facilitate this transfer of knowledge. At the end of the day our clients know their business better than we do and that value should be leveraged. Testing and phasing of processes Depending on your business, introducing self-organising teams might be quite a drastic, big change to the business. There is no harm in trialling the introduction of this to a single team to begin with. We tested this process with a single team, giving full visibility, control over holidays, scheduling iterations etc. to see how effectively it worked. This allowed us to make changes to the process, building confidence in it, before rolling it out across the company. With processes that have been rolled out company wide, we still experiment and make changes. We're never afraid to change processes if we think they can be better. Don't be against something until you've tried it! Management Devolving this power to teams doesn't mean that management can't be there to assist and guide teams. There is still a lot of valuable knowledge and insight that the teams can benefit from. With the teams directly communicating with clients, it's still important for them to know the long term roadmap of work. It's just nice to know so there are fewer surprises for developers day-to-day. If a manager sees that a team is struggling, then this is a good time to assist, not taking control, but offering guidance on how to proceed and communicating with the client if need be. Hopefully the points we've covered will help you apply this to your teams. We've learned it's essential to be open to change, realising what works for you and what doesn't will be an important part of forming your own processes and norms around self-organising. We've found these to be a good basis for Made but don't feel that any of these are hard rules. As part of empowering your teams, allow them a say in shaping how best to find what works for them. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-11-10"},
{"website": "Made-Tech", "title": "Finery awarded Website of the Year at Retail Systems 2016", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/finery-awarded-website-of-the-year-at-retail-systems-2016/", "abstract": "We're incredibly proud to announce that the website we built with the team at Finery has been named Website of the Year at the Retail Systems 2016 award show . Finery , an online womenswear retailer, first came to us in 2014 with the goal of launching their ecommerce platform. Over the next year or so, we worked with them to develop an online store that would meet the needs of their customers, whilst maintaining the level of class the Finery brand represents. After delivering an initial, invite-only release within three months, which allowed us to gather data from real users without opening the floodgates, we launched the Finery store to the public in February 2015. The Finery team have since gone on to achieve massive success, with online sales reaching an incredible £5 million in their first year . Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-11-11"},
{"website": "Made-Tech", "title": "Trusting teams to deploy at any time", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/giving-teams-an-environment-where-everyone-can-deploy-safely-any-time/", "abstract": "Developers should be allowed to deploy at any time. Many find this a scary prospect since it makes traditional release management and QA very hard. We have found that empowering developers to own the responsibility of deployment allows you to ship software much faster whilst maintaining or even improving the safety of releasing changes when compared to more traditional processes. As part of our mission to improve software delivery in every organisation we seek to bring our customers on a journey from deploying once a quarter to a few times a day. What do we mean by deploying safely at any time? Anyone who introduces a new feature, makes an improvement or fixes a bug should be allowed to push that change into production and should take responsibility for that change. The engineer will not only make a change to the code but deploy it, ensure that it functions as expected in production and even be in touch with the stakeholders of the feature whether they be customers or colleagues to announce the changes. Should anything go wrong, they are responsible for fixing it. There are many safe guards that accompany a continuous deployment practice. We for example have a production-like environment that we test against before putting changes live. We also have automated testing and code reviews where other engineers are asked to review work before it's allow to be released. Why should everyone deploy their own changes? Before going into the details of explaining how to create a safe environment it's probably best we answer the \"why\" first. The biggest benefit when empowering teams to deploy more often is that change becomes less risky. By making more frequent changes they will naturally be smaller. If our deployments are smaller they will be easier to test and easier to fix in the case of any issues. Deployments should become mundane. Over time your team will become better at testing their own changes and fixing issues when they arise. When a developer releases a change they will learn to become responsible for testing and monitoring it. This creates a proactive culture where developers can quickly react to problems often allowing them to spot defects before many users encounter them. Releasing more often also means changes get into the hands of your users faster. Removing overheads such as QA processes means that we can make changes quickly, reacting to market changes and the metrics we collect. Empowered developers are happier developers. – Ancient proverb Not only do the business and customers benefit from more frequent releases but we've found empowered developers are happier developers. By being responsible for a change, from start to finish, developers will feel a sense of pride and ownership over their work. We've found more traditional release strategies lead to developers passing responsibility onto QA or the deployment teams, simply throwing their work over the fence. When developers own their changes, they will put care into their work. One common argument that may come up is that developers will be swapping depth of knowledge in a particular field for breadth which spans many fields. While this is a reasonable concern we have found that this is not as drastic as it may seem and almost always engineers prefer the responsibility of owning the whole problem. How do you provide a safe deployment environment? From our experiences we have found the following 6 steps to greatly improve the way in which software is deployed. These can be expanded on but having these in place will greatly benefit your releases. Create a safe environment where it is ok to fail Make deployments easier by automating them Ensure the deployment pipeline is fast Deploy to a production-like environment for testing before going live Get used to deploying small changes Tell everyone about your deployments when they happen Set up monitoring so you know when you deploy a breaking change Use blue green deployments so rolling back is easy We'll now briefly go into each one of these subjects although each could, and in some cases do, have entire blog posts about them. Encourage a culture where it is ok to fail In order to benefit from deploying faster, you first need a culture where failure is ok. It is rare in software engineering that changes are perfect first time round. Optimising for fixing failures quickly provides more value than getting things right the first time round. This is sometimes called optimising for MTTR (Mean Time To Repair) rather than MTBF (Mean Time Between Failures). Failure can often lead to blame. Instead of making failure a negative situation everyone in your team should understand that failure happens and is in fact a great opportunity to learn from and develop. When failure happens, the team should stand together, jump on the problem as a team and then discuss what happened afterwards with the aim of improving things for the future. When you have a friendly environment for people to work in, they will produce better work. Don't punish failure, reward recovery. Make deployments easier by automating them Humans aren't great at repeating processes, and let's face it, repeating yourself can be boring too. A typical deployment will include building, testing and releasing the software to the public. Each of these steps are themselves made up of a series of smaller steps. Use scripts for each step in your pipeline so each step can be executed with one command. You can use services like Travis CI, Circle CI and Codeship, or self-hosted solutions like Jenkins to run scripts automatically for you. For example when a new change has been peer reviewed and accepted by merging the change into the main codebase, code hosting platforms like GitHub can automatically trigger your build and testing scripts for you. Deployment scripts can be triggered manually or automatically when the previous steps are completed. Even if an engineer has to click a button to put a change live, that's a lot less error prone than running scripts, or a sequence of commands manually. Ensure the deployment pipeline is fast When a problem occurs in production, you'll want to fix it as quick as you can. Once diagnosis of the problem has occurred, and a fix applied locally, you'll want to ship that change out fast. In order to do this your pipeline needs to be quick too. Even larger projects should only be taking 10-20 minutes to go through the pipeline with the ideal speed being much lower than that. Being able to react fast can often mean rather than needing to roll back, you can in fact roll forward. In reality this means rather than removing a new feature when you find an issue you can instead fix it quickly. Of course, if it's a more serious problem, rolling back or disabling the feature would most likely be the correct course of action. Deploy to a production-like environment for testing before going live Before putting a change live that has only been run on an developer or two's laptop, you'll want an environment that you can test it on that mimics production. Often local development configuration will use different settings and modes, particularly when it comes to what type of databases it uses and debugging settings. A change in these conditions like when an application moves from development to production can be a source of errors. You'll want a production-like setup in order to discover these errors before things go live. In order to facilitate a production-like environment ideally everything from server setup, database configuration, data stored in the database should be nearly identical to production. One consideration with data is that you may want to copy data from production into your production-like environment but you'll want to replace customer emails with example ones otherwise you may end up sending emails to customers from your production-like environment. Get used to deploying small changes When deploys are kept small, to something like 100-200 lines of code or smaller, risk is limited. It's fairly obvious that when your changes only impact a smaller surface area of a system, when something goes wrong, there will be a smaller area to search within to find the problem. Smaller changes will also mean that peer reviewing and testing are a quicker process. Again a smaller surface area is easier to look over, easier to test the various pathways through it. To reduce risk, instead of deploying whole features, deploy tens of times before the feature is complete. You do not need to make the feature publicly accessible until the last release but by dark launching it into production you will be uncovering a lot of problems early, avoiding the big bang release and the problems that come with it. Tell everyone about your deploys when they happen If you have automated your deploys, you'll also be able to automate the broadcasting of this deploy. It is important to let everyone know about change when it happens so everyone can have a look at the new functionality, be alert and ready for when any issues occur, and also to celebrate yet another release. You could consider automatically emailing the team and wider business when changes go live. If you use chat applications like Slack you could set up alerts within appropriate channels. You could even start emailing your customers automatically if you're brave. Set up monitoring so you know when you deploy a breaking change You need good monitoring in your application so that when an error occurs, or page load of a web application slows down, you are alerted to the problem. After any deploy to production, the developer who pushed it up should keep an eye on the monitoring to see if any defects had been introduced but should also be notified automatically of any such issues. Since the nature of change does carry some risk the goal is to spot potential defects before the issue affects a greater number of people. Tools such as NewRelic allow you to set up alerts when certain performance thresholds are exceeded along with notifying of errors that happen to applications. Use blue green deployments so rolling back is easy If you deploy frequently and monitor the system after each deploy, you should have a pretty clear picture on which deploy had adverse effects on the system. If you have a fast pipeline, you'll likely be able to roll forward if the issue isn't too great. But what if you do need to rollback? Using blue green deployments is a safe way to deploy a new production. Essentially when you have a new release ready you deploy it to a new server rather than immediately replacing the old production server. You can then visit this version of the application, make sure it's okay, then point the domain name of production at the new server thereby switching web traffic over to the new version. Blue green deployments have the benefit that if something goes wrong, you can point the domain back at the old version again in order to rollback. Of course it also gives you another opportunity to test your changes before showing them to the world. Trust your teams It all comes down to trust. Developers should be allowed to deploy at any time in most cases. We should learn to recover from failure fast, and learn all the lessons failure can teach us. Let us know what you think on Twitter: @LukeMorton and @EmileSwarts 🙂 Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-11-16"},
{"website": "Made-Tech", "title": "Should Customers Pay for Bugfixes?", "author": [" Fareed Dudhia"], "link": "https://www.madetech.com/blog/should-customers-pay-for-bugfixes/", "abstract": "Whether or not you agree with the basic premise of this article, I'm sure you'll be able to agree that any software project of a reasonable size will have bugs. The way these bugs are dealt with can often become an obstacle to forming a healthy relationship with the customer, and can even impede the software development process itself. There's many ways to approach this issue, and I'm going to start with a common one. “We paid for a feature. The feature has bugs. Why should we pay again for bugfixes?” Many moons ago (at the first software company I worked at), if a customer came to us with a buggy feature, we would fix that feature free of charge. Customers liked this because they received the features that they had asked for, and everyone went on their merry way. The flipside to this was that features were defined in 50+ page specifications that would take days of meetings to draft. If the customer was expecting a feature that was not on the specification, the company would charge to add this new feature. Once, the specification dictated that the user should be able to \"add events\". The customer was obviously expecting that the app would allow for editing and removing these events, but as the functionality was not on the spec, they had to pay for it. That was obviously an extreme scenario, but when there is no trust between you and your customer, that's what ends up happening; it comes down to quibbling about the verbiage of specifications or support agreements. Of course, at that company, the conversations around whether or not something was covered by the specification were billable. The customer ended up paying a lot more than if we had simply charged for the time required to fix the bug. “This feature is not what we asked for. Why should we pay for something that you failed to understand?” This is where the conversation around \"bugs\" really opens up. To what extent can we consider a feature which is working correctly a bug? If there was a miscommunication, or something was not adequately described, or we took something for granted, or they took something for granted, or the full complexities of a feature weren't investigated in the planning phase, should the customer have to pay? In a perfect world, this would be judged on a case-by-case basis, by a neutral third party with no stake in either outcome. Of course, this is never going to happen, and in the past I've seen conversations about \"the reasonable assumptions that someone should be able to draw from a passage of text in an email\" stretch out of long periods of time. Conversations like these can cause a breakdown of the relationship with the customer. When naming something a \"bug\" means that it's free, the customer has a vested interest in naming as many things as possible \"bugs\", even if the \"bug\" is simply functionality that was glossed over, implied, or taken for granted in planning. These conversations take a long time to have, and they are rarely productive for either party. They slow things down. “This functionality was working fine, but is no longer working. Why should we pay for you to fix your own ‘code rot’?” Sometimes, features that were fine mysteriously stop working. Third-party APIs change. Infrastructure stops running smoothly. Regressions. Why should the customer have to pay for unforeseen problems with the continuing health of the app? Much like how the Golden Gate Bridge needs to be repeatedly repainted, apps need maintenance and support to keep running. If there has to be a conversation with the client around the nature of an issue and who should pay for it, especially while the app is not functioning as it should, this again indicates a real problem in the relationship. Also, this is kind of what support agreements are for. So what’s the solution? At Made Tech, we bill for all of our time. We usually have an agreed-upon amount of time dedicated to a sprint, and then an additional ongoing support budget if necessary. Our plan is always to wow the customer so much with our initial sprint that any misgivings about having bugs paid for in sprint time disappear. When we don't have to have any of the conversations I've listed above, we can get everything done much faster, and trust grows. Any miscommunications around what a feature should have been are resolved with both parties working towards a solution, rather than pointing fingers; issues are figured out much more quickly when everyone's on the same page. We pride ourselves on the fact that our customers trust us to get on with the job. Our test suites are sound, and we have frequent, open communication with product owners, so true bugs are rare to begin with. When they do appear, they are treated as a natural part of the software development process. Which they are. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-03-29"},
{"website": "Made-Tech", "title": "How To Build An Agile Team", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/how-to-build-an-agile-team/", "abstract": "There are many challenges in building an agile team. We hear about self organising teams, but how can a CTO ensure a roadmap is kept? We hear about #noestimates , but how can we plan anything without estimates ? Failure is an important part of agile, but how can we accept failure in production? Communication is key, how can we encourage it? In this article I will attempt to address these questions and more. Delivering great product takes great teams but putting them together is challenging. It is easy enough to stick a group of people in a room, give them a list of tasks and ask them to complete them by a date. What is not guaranteed is that any of the tasks will be completed when you expect. Start with a definition Before we build a team we must first come up with a definition of a team. What does a team mean to us? Who is it comprised of? What is its purpose? Let us just say a team is a group of people brought together for a purpose. It will be made up of everyone required to realise its purpose. In the world of software development, teams are brought together to implement new functionality, to bring improvements to systems and to solve issues. The purpose of a team is also important. We can define a team's purpose or goal as something that will be reached within one short iteration, perhaps two weeks at most. A team must begin with a goal and end with the delivery of the goal. We can take this definition a bit further and apply it to building a team. To build a great team is to engage a group of people around a particular goal with enough energy to see it through to a conclusion. Virtues of an agile team Great agile teams deliver often. They get something out in the wild and respond to feedback. Agile teams win trust from their successes and recovery from failure. In order to build a unit that can win trust you must first build a framework around a team that enables them. When pulling together teams you are shaping your delivery process. There are three virtues I believe your process should embody: autonomy, craft and focus. Autonomy Working towards a common goal requires buy-in from a team. They need to be interested enough in working together in order to achieve the desired outcome. The feeling of responsibility and ownership will strengthen their resolve to achieve a goal. They must be allowed to make mistakes so that they can learn from the pain of failure. A team must have permission to solve a problem how they see fit. This is autonomy. Thriving on responsibility Given to a team, autonomy is powerful in that the responsibility for delivery is felt strongly. When they can make their own decisions there are fewer reasons to not deliver and fewer blockers. A software engineering team who can push their code directly to production as often as they like have no process to hide behind. When you can QA yourself there is no need to wait for someone else to check your work and you can therefore move faster. When such barriers are lifted a team will know that the buck stops with them and more often than not teams thrive with such responsibility. Realisation of responsibility Transitioning a team into a self organising structure can take time. Sometimes we find a team isn't used to such responsibility. They can be unaware of it and even actively try to avoid it. We have noticed that process can be introduced by team members who are not ready to move so fast. We find some teams feature creep or change their goal completely mid-iteration. Such sabotage comes from an engrained sense that work is continuous, without a start or a finish. Introducing ceremony around delivery cycles such as a showcase during and at the end of the iteration can help. Getting the team to present their work when they reach a milestone can really help a team self organise. The key is to not allow showcase dates to change so that a team has to explain to stakeholders when they haven't achieved their goal. This drives them to make changes to avoid the embarrassment in future. Ask forgiveness, not permission When we work with businesses where teams don't have so much autonomy as we're used to, frustration can soon ensue. Autonomous engineers quickly get frustrated at being unable to solve problems themselves. A team must have permission to carry out their purpose. When deployments of certain applications must be performed by a particular person, especially someone outside the team, then those used to being able to deploy anything will soon be saddened. Same goes for not having permission to create new taskboards, email addresses, etc. Teams must be empowered to do all of the above. A team must have permission to carry out their purpose. Making mistakes When teams are autonomous they will make mistakes. They will often make more mistakes but they will also be fixing them faster. Making mistakes early means the team can learn and adapt. Autonomous teams may be fast to break things but also fast to fix them. We have heard horror stories of a fast moving autonomous team having a 2 month deploy freeze on their project after breaking stuff that they could have fixed in 5 minutes. This is not the way to deal with failure, a team must have the autonomy to fail and correct such failure. Empower your teams A team becomes empowered when they can self organise and complete their purpose with minimal blockers from the outside world. They should be allowed the chance to show off their agility when making mistakes. Their autonomy will strengthen their resolve when they feel the responsibility and power to deliver is with themselves. Craft A culture of craftship is important to an agile team. A team who are excited by their ability to solve a problem the way they want will enjoy working towards a common goal. Practices such as Test Driven Development and Continuous Delivery are both indicators of excellence and also improve the ability to achieve a goal. A team who are excited to discuss their work will be having the conversations necessary to solve a problem as a team. Love for the job When an individual loves their craft they will be excited to solve problems. A group of excited people solving problems have a collective energy that can be felt when you walk into the room they occupy. A good sign of a team that love their job include seeing two people sitting at one computer discussing a problem. Another perhaps when they're standing by a story board discussing their next task. Positive energy goes a long way and can keep a team interested and undistracted for longer. Best practice A software team practicing Test Driven Development and Continuous Delivery will be exposed to industry best practices enough to know about these methodologies and more. Expertise within a team will accelerate the speed at which problems can be solved. If they've been practicing them for a while you may well have yourself an expert team. It can also help up skill other members although there will always be a cost to productivity when expert members divert attention to assisting others. Thrill of craftship The buzz in the air when you walk into a team's room is a good sign of healthy communication. Those excited and caring of their craft will be happy to discuss their techniques with colleagues who may in turn offer up even better solutions. Walking into a room and everyone has their headphones on is a sign you may have a problem. You want the communication to be flowing. Of course communication and craftship aren't the same thing but good communication does come with a team who are good at their craft. Cultivate your craft Bringing together a group of people who care for their work is key to achieving goals. The positive energy from a team's excitement is the fuel to going the extra mile and to achieving goals more efficiently. With care of craft comes best practices that will help ensure the quality of work delivered. Healthy communication means problems will be solved collaboratively with the combined expertise of a team rather than by siloed individuals. Craftship is a must for solving problems. Focus Bringing a team together to solve a problem is hard. The bigger the goal, the more likely you are not to succeed, so it's best keep those goals small. A team built of members who aren't always allocated to that team is sometimes a necessity but is to be avoided whenever possible as context switching isn't fun. Same goes to a team working towards multiple goals, context switching isn't fun so don't do it. Defining a goal Carefully defining a team's goal is crucial to their success. We find the smaller the goal the more likely we can accurately predict a completion time. Smaller goals are easier to put live since a goal affecting a smaller surface area means there is less risk associated with change. Undiscovered complexity is also less likely to disrupt an iteration. The smaller the problem we are solving the less likely it will explode into a massive piece of work that affects our deadlines. 100% commitment A common problem we've found working with clients is that team members are often not committed 100% to a particular team. Often team members will be pulled from one team to another to help out with a problem. Sometimes team members are actually committed to both teams. We try to stop this as soon as we can as it completely breaks focus. We try to ensure goals are broken down enough so that we can finish one goal before switching focus onto another. This means a team member will be at most busy for a couple of days before being able to move to another team to provide their expertise. Ronin engineers A similar problem is that of the ronin engineer, or more simply, an engineer without a team. They are usually in charge of a piece of technology rather than working within a team. Service desk type system operations is a variant of the ronin. This is where you have a team dedicated to a technology rather than a team such as infrastructure or deployments. We try to move these engineers into a single team. We also try and knowledge share their expertise as soon as possible so they do not become a blocker for other teams when they are busy. Solve one goal at a time Teams should have a single goal when they are brought together. A single goal means they can focus on solving that problem properly before starting another one. We often setup taskboards per goal and only let a team work from that board. If other work emerges we ask teams to make note of it and bring it up in the showcase of their current goal. When goals change Sometimes mid-iteration a goal will change based on some learnings. You may find that another change must be made before your original goal can be achieved. At this point you cancel the iteration and pull stakeholders together to share learnings. Goals may sometimes actually be two goals in disguise. It is not always obvious until you start work. This realisation is again a good time to stop and pull together stakeholders. After debriefing everyone you can redefine the goal you want the team to solve first. Remember, solve one goal at a time. Focus is paramount Focus within a team is paramount to getting a problem solved. Just like breaking big stories down in traditional Scrum, goals should be broken down into their smallest possible incarnation. Team members should be allocated to a single team and that team to a single goal. Team work should be fun The framework on top of which you form teams should make team work fun. Autonomy, craft and focus are all virtues that can help you achieve this. A team needs to know they have the power and responsibility to solve a challenge. A team needs to be excited about their solutions, conversating and learning from each other. A team needs small focussed goals in order to deliver on time. I'm interested to hear what your experience is with building agile teams. Does the above ring true for you? What virtues do you foster when building teams? Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-03-10"},
{"website": "Made-Tech", "title": "What makes a great Tech Lead?", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/what-makes-a-great-tech-lead/", "abstract": "If you're a developer trying to understand what may be required to take a step up, then you've come to the right place. In this article, we'll discuss a few of the traits that we look for when hiring or promoting into Tech leadership roles: Full Stack over Specialist Modern Tech Leads are expected to know the full stack. For us, this means front-to-back development: front end (Coffeescript, Sass, HTML) through to server side development (Ruby, Python etc) and onto Devops (Chef, Docker, Terraform etc). It's ok to have areas where you feel more confident, but as a Tech Lead, you're expected to be shaping an application across the entire stack. Takes Responsibility over Waits For Tasks Modern Tech Leads don't wait for responsibility. They identify a job that needs to be done and they act. 'Responsibility is taken, not given' is a good mantra, and this couldn't be more true then when taking the step up into a Tech Leadership type of role. If you are waiting for someone to promote you or to give you more responsibility, then you're going to wait for a long time. Breaks New Ground over Sticks To Tried & Tested The software industry moves incredibly fast. New frameworks, techniques and tools are cropping up every single day. The modern Tech Lead needs to be on the cutting-edge, taking the appropriate opportunities to introduce new techniques and tooling into deliveries. It's no good continuing to roll out the same 'tried and tested' approach to every project. If you do this, you'll be left behind. Author over Translator In years gone by, a big specification document would land on your desk and you'd translate that into some working software. Nowadays, your job as Tech Lead is to be an Author, not a Translator. You're providing the narrative and deciding the journey in which the project should take. This requires creativity and imagination combined with a rigorous understanding of process. Active Communicator over Wallflower Modern Tech Leads need to be active communicators. Gone are the days of hiding away in the corner, waiting for somebody else to start an important conversation. You should be the one encouraging ongoing active communication within a team, as communication is key to making great software. Reliable over Unreliable The Modern Tech Lead is reliable. Their team can always count on them to be there and to be available. It will be unusual for them to be sick, running late or missing a deadline. It's this reliability which means they can take on the responsibility to run projects. We've seen a close correlation between an increased level of responsibility and reliability improving. Mentor over Mentee The Modern Tech Lead providers mentorship and looks to nurture and support their colleagues. They move from primarily asking for help to providing this help to others. They recognise challenges colleagues are facing and suggest ways of improving. They recognise that anything they do to improve an individual will help the overall team. Embraces Customers over Avoids Customers The Modern Tech Lead is comfortable interfacing with customers. They move from letting others deal with customers, to feeling confident and able to have conversations about anything that will impact delivery. Solves The Problem over Flags The Problem The modern Tech Lead is a problem solver. They don't sit around waiting for somebody to remove a blocker. They poke, they prod and they find novel ways to move things forwards, so they can continue shipping code. They find a smell and they fix it, rather than letting the smell impede progress. Always Be Learning As the business of software continues to get increasingly complex, the skills required will continue to evolve. To keep up, you'll need to be a great learner and you'll need to set aside time to learn new things. If you can do this, the rest should fall into place. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-03-31"},
{"website": "Made-Tech", "title": "Introducing our new monthly meetup: QuizBuzz!", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/introducing-our-new-monthly-meetup-quizbuzz/", "abstract": "We've been running internal Code Dojo sessions for a couple of years now, and last year our newest member, Andrew, suggested putting a new spin on the traditional format. We enjoyed it so much that we had to share it with the wider community so, to help celebrate our move to a swanky new office, we'll be opening our doors to the public and launching Made's first monthly event: QuizBuzz! What is QuizBuzz!? In the style of a pub quiz, QuizBuzz! is a chance for teams to show just how much they know about all things software related, from old school programming languages to modern video gaming, with a few pop culture curveballs thrown in for good measure. Also, since no pub quiz is complete without a drink or two, we'll be keeping you watered and fed with beers and pizzas (with non-alcoholic and vegetarian friendly options, of course). A game of two halves and five rounds, with myself as your quiz master, each team will need to work together to come out on top and claim victory. If trivia isn't your strong suit, then maybe, despite what Phil Karlton says , naming things is! At the end of the night, our very own Seb Ashton will be handing out a prize for the best team name. When the victors have been declared, we'll then head to the nearby Prince William Henry pub for another drink or two, where everyone will have a chance to get to know the other teams a little better. There are limited places, so claim a spot here on meetup.com , and we'll see you all on the night! Essential Info When: 18:30 on Wednesday, 20th April – register for a place here Where: 136 Southwark Street, London, SE1 0SW ( get directions ). We're a short, walkable distance from Waterloo, London Bridge, Southwark, Borough and Blackfriars stations. Afterwards, we'll be heading to the Prince William Henry pub, located just a couple of minutes away from Made HQ . What: Five rounds of ten questions, each designed to test your knowledge of code, programming, and pop culture. There'll be a prize for the winning team, and another prize for best team name! Who: Teams of four – if you already have a team, great. If not, we'll find you one! Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-04-05"},
{"website": "Made-Tech", "title": "Why be defensive about programming?", "author": [" Alex Minnette"], "link": "https://www.madetech.com/blog/why-be-defensive-about-programming/", "abstract": "Programming style is often categorised by language or architecture, but the code style also can differ a lot within the same kind of architecture. Defensive programming is derived from the notion that very few things can be trusted when running a program, and adding more checks help to improve the stability of it. It's a clear defence against Murphy’s famous law that whatever can go wrong will go wrong. Defensive programming, as with most things, is not always black or white; the code style can be defensive in some critical parts and have fewer checks in other parts. Let's stop discussing theory for now and have a look at some common defensive patterns. Null checks The most common trait of defensive programming is to add null checks for just about everything. Here is a small example in Javascript: if (product\n     && product.groups\n     && product.groups.length\n     && product.groups[0]\n     && typeof product.groups[0].id === 'number'\n     && product.groups[0].name) {\n      // do something with the product group\n    } I'm sure this code looks familiar to you in one way or another! The conventional wisdom here is basically that we should check everything because it's difficult to know what to expect. Here are some of the things that were probably going on in your head when you wrote that code: What if the product is not formed properly? What if the group on the product does not exist? How will the code behave if another kind of object is passed? (Welcome to Javascript!) These are all valid reasons to write the code above, but unknown scenarios still need to be dealt with. Maybe the product itself is simply invalid and if the code does not crash, you would never know about it. Consider the following product being sent to this method : {\n    \"name\": \"Red Carpet\",\n    \"Groups\": [\n        {\n            \"id\":3,\n            \"name\":\"carpet\"\n        }\n    ]\n} Here we can see that this particular product lacks a proper \"groups\" property and instead has a \"Groups\" property with a capital G. This error was introduced by another system, and will likely never be discovered and raised properly. The user probably just won't see any group associated in the interface and these data errors might never be fixed. Checks which are always true Sometimes, we just want to be sure that everything is in order and the world still works as it should, so we add checks directly inside the class methods. Here is another example in JavaScript : User.prototype.getGroup() {\n  'use strict';\n  // this should never happen...\n\n  if (this !== null && typeof this !== 'undefined') {\n    return this.group;\n  }\n\n  return null;\n} This is a terrible example, but it illustrates my point and finding similar examples in real life is not unheard of. What is the point of checking for the 'this' keyword here ? It's obviously going to be defined there since the function should be called with the context from the object itself. Take the following code: User.prototype.getGroup.call(null) // this useless call only works in strict mode If someone is trying to call your code like that, what is even the point of trying to check anything? Since the code is not called with a User object, nothing is going to work in any of the User calls, whatever checks are in place. Main issues with defensive programming Maintainability Maintaining code when every function starts with a ten line null check on every object is not an easy task. Every time an object changes, the code needs to be updated everywhere on every method. Because of all the checks, even if some part of the code is not updated, nobody will know about it until it's too late. Because errors are hidden and not being raised properly, it becomes difficult to know if the software is working as intended. Good testing and test driven development are the keys to building reliable software, though the advantages of proper testing cannot be fully explored in the context of this article. Unit tests are exploiting the fact that the program will raise errors if a problem occurs to cancel the tests. In some Unit tests, it's difficult to make the difference between \"no data returned\" or \"no data returned because the data is invalid\". Proper error tracebacks are also difficult to get since no errors are actually raised, debugging becomes then a harder task where every piece of code need to be checked. Readability Defensive programming makes it much harder to read the actual logic of the code. The checks themselves are taking a lot of space on every part of the program and it might become complicated to figure out what goal the programmer had in mind when writing the function. Readability is one of the main requirements for building reliable software . As a side effect, adding a lot of checks makes it more difficult for other team members to dig into the code to figure out how the code works. Conclusion Defensive programming is far from a new concept; the C standard library is full of NULL checks, overflow and memory checks on almost every function. While this concept can be useful at a lower level to detect major errors, preventing the program to crash can sometimes only mean allowing the program to continue to malfunction, which can lead to incorrect results. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-04-06"},
{"website": "Made-Tech", "title": "Building a Continuous Delivery Pipeline in 5 minutes", "author": [" David Winter"], "link": "https://www.madetech.com/blog/building-a-continuous-delivery-pipeline-in-5-minutes/", "abstract": "At Made, the majority of our projects use Continuous Delivery pipelines to provide a clear path for deploying to production. It's common for these to be setup on the first day of a project kick-off. In the past we've always setup these manually, and as with all things you do manually, subtle differences creep in over time. We needed a way to automate this process so that firstly it would standardise our setups, and secondly, save us a lot of time. What used to take at least a couple of hours can now be accomplished in a matter of minutes. Using Terraform and Ansible, and with a Makefile acting as glue, we have a command that we can run, configurable with some variables, to get a Continuous Delivery Pipeline setup for a git repository by running make jenkins . The basic flow of the tool is: Create an SSH keypair used on AWS EC2 instance Using Terraform, create AWS resources Have Ansible connect to EC2 instances and install and configure Jenkins Configure a Jenkins build pipeline using common pipeline steps, as described below, for a specified git project repository. We use a standard pipeline structure which consists of a few steps: Build to runs tests and code checks Continuous environment deploy Staging environment deploy Production environment deploy These steps are of one of two types: a build or deploy step. The tool we've made expects projects to have two files: bin/build – used for the build step bin/deploy – used for the deploy steps, which is one of continuous, staging or production. By using this interface for projects, Jenkins needs to know nothing about how to build or deploy. It just triggers these scripts at each step. That's a brief overview of the tool and what it achieves. Now lets break it down into more detail for each of the tools we use. SSH Key Setting up an SSH key for the instances that Terraform will create is an essential first step. Using ssh-keygen we generate a new key and store it on the local filesystem. Our Terraform configuration will use this to setup a keypair on AWS, which will then be used on the new instance it'll create. This adds the public key to the authorized_keys file on the new EC2 instance, allowing Ansible to connect further on to install and configure Jenkins. When you normally run ssh-keygen it gives you an interactive prompt to complete the setup. We want to automate that, so we use two different flags. Using -f allows us to specify the location where the key will be saved. And -P allows us to specify an empty passphrase. Terraform Terraform , by Hashicorp, the Vagrant people, allows you to write a configuration file containing a bunch of resources which represent infrastructure. AWS is the best supported provider with Terraform, so we're using that for this. Terraform resources can be things such as EC2 Instances, Elastic IP's, security groups or key pairs. Defining these resources in code means it can be automated, and these configuration files also serve as a form of documentation. At the very heart of a Terraform configuration file is the provider. This is where your infrastructure will be. You specify it with the following: provider \"aws\" { } You can specify a bunch of options in this block, such as the region you want to use, and API credentials, however, Terraform supports the use of environment variables for defining these, allowing us to have the most simple of declarations, and then let the user configure these by prefixing the command we run with environment variables such as AWS_REGION=eu-west-1 make jenkins . Our configuration file sets up a single EC2 instance to run Jenkins: resource \"aws_instance\" \"jenkins_master\" {\n    ami = \"ami-abc579d8\"\n    instance_type = \"t2.micro\"\n    key_name = \"${aws_key_pair.provisioner.key_name}\"\n    security_groups = [\"${aws_security_group.ssh_and_http.name}\"]\n    tags {\n        Name = \"jenkins_master\"\n        role = \"jenkins_master\"\n    }\n} This resource references a few others defined in the file, such as a key pair, and security groups, but the important thing to note here is the use of the tags block. We give the instance a Name, but also a custom tag called role . tags {\n    Name = \"jenkins_master\"\n    role = \"jenkins_master\"\n} The keypair that Terraform will use is defined in the following resource: resource \"aws_key_pair\" \"provisioner\" {\n  key_name = \"terransible_provisioner\"\n  public_key = \"${file(\"keys/terransible_provisioner.pub\")}\"\n} You'll notice that ${file(\"keys/terransible_provisioner.pub\")} is used to get the contents of the public key file that we generated with ssh-keygen . Ansible Once Terraform has run, we'll have an instance on AWS, but with nothing installed or configured on it, besides the SSH key. Ansible is a provisioning tool, similar to Puppet or Chef, that allows us to install and configure software on the instance. Ansible is configured to use the same SSH key that we created in the first step, so that it's able to connect to the instance. But how does Ansible know which instances to connect to? Normally, you specify an inventory file for Ansible, which lists the hosts that are to be targetted. This is normally a manual process, of grabbing the assigned IP address of an EC2 instance from AWS and then pasting it in the inventory file, and managing it completely by hand. Ansible supports a Dynamic Inventory script . This is a small Python script that connects to AWS and pulls out the EC2 instances you have available. The important thing it does is group and assign these instances to Ansible groups based on the AWS tags you defined when creating them via Terraform. In our Ansible playbook, we can specify the hosts to target, with the following: - hosts: tag_role_jenkins_master All roles and tasks under this will only be run on EC2 instances with the tag role and value of jenkins_master . Now we're dynamically targetting the instances, we use Ansible to say what should be installed. We install a bunch of dependencies so that we can install Jenkins. We use Ansible templates so that we can generate the XML files that Jenkins uses to create jobs for the git repository of our project, and to configure the Build Pipeline plugin to create our Continuous Delivery pipeline based on the steps mentioned earlier. Makefile Gluing all of this together is the Makefile. This allows us to define the dependencies of our tool by defining tasks and their dependencies. Breaking our Makefile down, we have a few tasks: keys : uses ssh-keygen to generate an SSH key build : this runs terraform for us. In our terraform file we make a reference to a file that contains the SSH public key that we want added to the EC2 instance when it's created. This is then used by Ansible to SSH into the box to provision it. Therefore build has a dependency on the task keys. ansible : downloads the dynamic inventory script and also some Ansible roles from Ansible Galaxy provision : this wraps up the ansible-playbook command that we need to run to have Ansible install our software. It prefixes the command with a bunch of configurable environment variables, as well as additional variables that will be used by the Ansible playbook when it's run. This includes the git repository URL of the project we want to use in our pipeline. jenkins : this just ties our creation of infrastructure with the task build to our Ansible provisioning of provision . It defines those dependencies in that order so that Terraform is always run before Ansible. Creating your pipeline make jenkins \nproject_name='TestProject' \njenkins_auth_user=jenkins \njenkins_auth_password=testing \nrepo='https://OAUTH_TOKEN_HERE:x-oauth-basic@github.com/YOUR_USERNAME/YOUR_REPO.git' You can prefix this command with any additional AWS environment variables as mentioned above. The Makefile itself takes four arguments: project_name : This is used by the Jenkins pipeline view to give it a name visible from the Jenkins interface jenkins_auth_user : When Ansible sets up Jenkins, it enables HTTP auth to protect it from the outside world, and this is the username that will be used jenkins_auth_password : Along with the username, this is the password used for the HTTP auth security repo : This is the project that will be used for the pipeline Currently, to make setup easier, we use HTTPS to clone the git repository, using an OAuth token, which means we don't need to worry about SSH key setup to connect to Github. After the make jenkins command has finished, in the Ansible output, there'll be an IP address. You can then visit that in a browser, where you'll be prompted with an authentication prompt, and there you can supply the HTTP auth credentials to login to your pipeline. Taking it further We're still developing this tool, and in its current form, which was a proof-of-concept, we have plans to build on top of it support for the following: Support multiple projects/repos. Currently this tool only accepts one git repository. We'd like to be able to support multiple, in turn creating multiple pipelines on a single Jenkins. Ability to add Jenkins slaves rather than everything run on one instance. DNS support to configure Jenkins to be served at a domain. Project build dependencies. Currently, we only support Ruby based apps with common Ruby dependencies. We'd like to isolate these within Docker, or something similar so that any type of project can be built/deployed. Over the coming months, we hope to introduce these and other features. We've open sourced all of the code on our Github and welcome feedback, feature requests and PRs! Infracoders talk I presented this tool at Infracoders on the 16th of March and you can find the video here and view my slides on Speakerdeck . Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-04-07"},
{"website": "Made-Tech", "title": "7 Problems to Avoid When Building a Software Team", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/7-problems-to-avoid-when-building-a-software-team/", "abstract": "Building and maintaining a high performance software team is an ongoing effort. The challenges range from attracting great people in a highly competitive market, to providing interesting and challenging workloads, and putting together team structures and practices in which people can thrive. We're lucky to have worked with a number of software teams who have been looking to improve the quality and frequency of their delivery, and we've noticed a few recurring structures and embedded practices which hinder a team's ability to ship good quality software fast: 1: “DevOps” silos Particularly as a team grows, or perhaps to fill perceived gaps in the current team's skill set, the temptation can be to build separate functions in or around the team to perform specific roles. The most common manifestation we've seen is that of operations (often referred to as DevOps or infrastructure), where any infrastructure related tasks need to be performed by someone in this unit. We see this as adding an unnecessary boundary around a critical part of software delivery – deploying and running the thing. We'd much prefer to see true DevOps skills embedded in the software delivery teams, empowering these teams to deliver their application end-to-end, and making them responsible for running it. 2: Lack of empowerment We often observe a high correlation between a lack of empowerment and poor performance. A team needs to be able to manage their own workload day-to-day, be able to make technical decisions and, if necessary, to make changes to the way they work. Where a team is fed small units of highly specified work, and where decisions are made top-down, is likely where you'll find apathy. We've found that teams perform best when they're given a clear, commercially focused brief, and then empowered to figure out the best way to deliver it. 3: Isolation from stakeholders In some organisations there can be structures or practices in place that discourage or make it impossible for the delivery team to have contact with stakeholders. A high performance team needs to have regular, open communication with those for whom they're delivering the software. On top of the usual forums for facilitating conversation such as kick-off sessions and showcases, we encourage the use of communication tools like Slack to foster ongoing discussion between stakeholders and developers. 4: Lone soldiers and large teams We've found an optimal team size to be between 2 and 4 people. For most people, working in a team of 1 lacks the accountability and social interaction gained from working with others. When a team size starts exceeding around 4 people, communication can become harder and team accountability reduces. 5: Quality belongs to everyone An all too common response to challenges with quality is to try to solve this by introducing a dedicated role, or worse still, a testing silo. Where there's a perception of a safety net between the team and the software running in production, responsibility levels drop and quality soon follows. We've seen better success in encouraging quality as a team responsibility, embedding practices such as peer review and increasing adoption of automated test techniques. 6: Prioritising features over technical debt There's a balance to be had between delivering to commercial deadlines and keeping on top of technical debt. When not kept in balance, technical debt quickly hinders a team's ability to deliver. Teams being happy to accrue technical debt, or leaders being happy to turn a blind eye to it, are behavioural patterns that we immediately try to identify and improve when we start working with a software team. A team needs to feel empowered and encouraged to sell the benefits of paying off technical debt to their Product Owner, such that technical debt can be dealt with alongside feature development. 7: Underinvesting in team building It's important to not forget the basics in building a cohesive team. Facilitating an array of social activities to provide forums for teams to enjoy one another's company outside of a work setting, alongside opportunities for individuals to learn and better themselves remains ever important. Improving the happiness, productivity and cohesiveness of any team remains an ongoing effort, and requires regular course correction. If you're aiming to build a high performance software delivery team, we'd encourage hiring militantly well, and investing in practices that provide a regular feedback loop to help you embed a culture of regular introspection and improvement. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-04-12"},
{"website": "Made-Tech", "title": "Technical challenges with adopting Continuous Delivery", "author": [" Nick Wood"], "link": "https://www.madetech.com/blog/technical-challenges-with-adopting-continuous-delivery/", "abstract": "I spoke last time about the organisational challenges of adopting Continuous Delivery. One of the key takeaways was the importance of blurring the boundaries between team ownership in order to facilitate better adoption. This time around I want to zoom in a bit and focus specifically on the challenges faced by specific parts of the pipeline. Depending on how far through adoption you are, you likely no longer have dedicated teams for each of these functions, though the problems outlined here can still exist and derail even the most cross functional of teams. Most applications, unless specifically developed with Continuous Delivery principles in mind, are a long way away from being built and deployed many times per day – a core tenet of Continuous Delivery. This is generally the first issue teams should tackle and there are many components to it. Automated testing If you aspire to deploy code to production multiple times per day, or even to a UAT environment, being reliant on a manual test phase is impractical. If the application doesn't have automated tests, particularly for the most critical paths, your ability to deploy a working application regularly is going to be significantly compromised. Even if you have automated most of your tests, it's also important to consider the reliability of your tests. One symptom of this is frequent test 'flickers' (where the same test seems to pass or fail randomly between different test runs); another is where your tests aren't of sufficient quality to provide meaningful coverage across the breadth of you application. These both contribute to a gradual erosion of trust in your own tests and, over time, an increase in manual test steps. The end result being a much slower (and more manual) deployment pipeline than necessary. The final big testing headache is a long running test suite. If you need to wait more than 10 minutes or so for feedback (and we've seen cases of close to an hour!), not only is your workflow is likely to be impacted, but subsequently, your ability to deploy fast is going to be compromised – because every change needs to be pushed through the pipeline, starting at the build step. Automated testing in the absence of incredible amounts of hardware is a trade off. For an application of any meaningful size, having a test suite which covers 100% of your application, whilst also running in a few minutes isn't a realistic goal. It's important to analyze your test suite to ensure it's delivering you the most possible value, especially for the longer running tests. My final word on testing is to trust your tests. I've worked with teams who had great automated test coverage, but after a deployment the team insisted on testing user journeys manually. Some call this superstition, I call it madness! Why spend all that time writing tests if you're going to ignore what they tell you? Automated deployments Long build times are a huge blocker to Continuous Delivery adoption. If your deployment currently takes 8 hours, then it's obviously impractical to do this multiple times per day! Considered another way, your cycle time per feature is 8 hours longer than it needs to be. In order to build 4 features per week and deploy them individually (at Made we strive for 4 features per day), leaves 2 hours to build and test each feature – in other words, it's not going to happen. Are your servers cattle or pets? Much as we like to think every deploy is safe, you can't control everything and some deploys are just going to fail no matter what. This risk is compounded when your deployment methodology is to deploy sequentially to the same infrastructure. When a bad deploy happens your infrastructure can end up in a weird state and rather than trying to unpick everything it's far easier and quicker to blow it all away and start again. This is only possible with automated build scripts, and when all you configuration exists entirely within source control. A canary server is a great way to detect issues early in a deployment. The idea behind these is to have a single instance running the new version of the application and gradually introduce it to live traffic. By watching error rates and tracking key consumer metrics (using automated tools of course) you can quickly flag problems which might prompt you to pull out of a deploy, or even fail automatically. In practice this is much easier than having to roll back a whole deployment. Over-reliance on brittle third-party dependencies Seldom is a development team entirely self contained. At some point you will have some sort of reliance on an external team. Particularly where an external team isn't able to work using a rapid delivery method, your team's ability to move fast and release often can easily be compromised. Where possible, we try to isolate high-cadence teams from any external factors that may slow down releases. We use techniques such as mocking and API integration tests, to define the contract between our application and external dependencies. Such tests become not only the documentation for the APIs, but also notify us if and when a third party changes their integration, so we can quickly respond. We usually deploy these changes behind a feature toggle , so we can deploy the changes to production and move onto the next. Once the third party has caught up, as long as the integration tests are all green, turning on the new feature then just requires a configuration change within the application. Clean up after yourself Too often Continuous Delivery is seen as an excuse to make rash decisions and never come back to clean them up later. This is something we're all guilty of to some extent. Things like: Didn't address a couple of the comments from code review because we had to get the feature out today Duplicate an existing class and modify it slightly, rather then refactor the existing code. Deployed a feature without adequate tests. Found an edge case but didn't have time to fix it. Activated a feature via a feature toggle, but never removing the code for the feature toggle which is no longer needed. I'm not saying the rationalisation of any of these things are good, but in a rapidly evolving code base technical debt can spiral out of control fast. It's important to realise that building and deploying features rapidly, does not mean doing things half-arsed. There are times when cutting a corner is necessary, my rule of thumb is that this is OK only if you have a good reason (which you are comfortable justifying to others) and a plan to make it good within the next few days. Conclusion Going from a cycle time of a few days to a few hours isn't going to happen overnight and deciding what to focus on first is an important step on your Continuous Delivery journey. Analysing cycle time of features – the time from when an idea was conceived to when it was successfully deployed – will help you quickly identify bottlenecks in your process. If 50% of cycle time is spent on deploying features, it's fair to say there's real value to be had speeding that up. Likewise, if there's idle time waiting for stakeholders to feedback on requirements, then whilst there's value in speeding up automated tests, there's probably more value ensuring your engineers are getting everything the need from the rest of the business. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-04-14"},
{"website": "Made-Tech", "title": "When to avoid the DRY principle", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/when-to-avoid-the-dry-principle/", "abstract": "The Don't Repeat Yourself principle is probably one of the most widely recognised software design patterns out there; most beginners in the industry will have heard of it, and more seasoned engineers will have taken it further and see its use in other design patterns such as Service-Oriented Architecture , Inversion of Control and Composability over inheritance . It's easy to see why DRY is so well known as it's self explanatory, or at least it is on the surface. Put simply, don't repeat yourself. If you see a block of code that looks similar or the same as a block of code in another file you'll likely think to yourself \"how can I abstract that?\". DRY is all about abstraction, which is the use of interfaces such as functions and classes to hide away implementation details. It's the method by which you keep related bits of code together. Instead of writing 5 lines of code to print a web page when a button is clicked, you may well place those 5 lines in a function called printPage(). Abstraction is related to DRY in that when you place related pieces of code together, they are easier to reuse and you end up repeating yourself less. Spotting and abstracting repetition isn't always obvious, it can get a little nuanced at times. The more experienced engineer may spot more subtle repetition, such as that in an e-commerce application there are multiple places where products are selected from a collection based on some criteria and then some action is performed on those products. They might choose to abstract the filter and map operations on the collection of products into a reusable class. Someone less experienced might not see this abstraction straight away. DRY might not always be the best idea So we have this principle which is understood right from the start. You move repetition into methods. It turns out a lot of code is repetitive. In some languages you might use a for loop in nearly every file, so should you abstract a loop? The answer is: sometimes. Like all things, too much can be a bad thing. Sometimes the principle can be taken too far, you can get too clever and end up with something that is harder to understand or work with. I'd like to suggest a few situations where the principle might not be so useful. Unnecessary abstraction Repetitive code can often be abstracted unnecessarily. Feature testing in RSpec is a good example of this. A lot of us in the world of Rails test features using capybara, a library for interacting with web pages. Often you will visit a page and then perform an action on the page. Let's look at an example: feature 'Adding product to cart' do\n  scenario 'Adding in stock product' do\n    when_a_shopper_adds_an_in_stock_product_to_cart\n    then_they_should_see_the_product_in_the_cart\n  end\n\n  def when_a_shopper_adds_an_in_stock_product_to_cart\n    visit product_path(product)\n    click_button 'Add to cart'\n  end\n\n  def then_they_should_see_the_product_in_the_cart\n    visit cart_path\n    expect(page).to have_content(product.name)\n  end\n\n  private\n  let(:product) { create(:product) }\nend This is a feature for adding a product to cart. The when step contains some capybara functions to add the product. Let's implement adding to cart from another page. Seeing as it's still about adding to cart there must be something that will want DRYing up. feature 'Adding product to cart' do\n  scenario 'Adding in stock product' do\n    when_a_shopper_adds_an_in_stock_product_to_cart\n    then_they_should_see_the_in_stock_product_in_the_cart\n  end\n\n  scenario 'Adding product from listing page' do\n    when_a_shopper_adds_a_product_to_cart_from_listing_page\n    then_they_should_see_the_product_in_the_cart\n  end\n\n  def when_a_shopper_adds_an_in_stock_product_to_cart\n    add_to_cart_from product_path(product)\n  end\n\n  def then_they_should_see_the_in_stock_product_in_the_cart\n    assert_product_in_cart\n  end\n\n  def when_a_shopper_adds_a_product_to_cart_from_listing_page\n    add_to_cart_from products_path(category: product.category)\n  end\n\n  def then_they_should_see_the_product_in_the_cart\n    assert_product_in_cart\n  end\n\n  private\n  let(:product) { create(:product) }\n\n  def add_to_cart_from(path)\n    visit path\n    click_button 'Add to cart'\n  end\n\n  def assert_product_in_cart\n    visit cart_path\n    expect(page).to have_content(product.name)\n  end\nend Okay so there are two methods that we've abstracted here #add_to_cart_from and #assert_product_in_cart. In this case I would say that #add_to_cart_from is bordering on being unnecessary since we're just saving one line and the method body could even be perhaps more descriptive than the method name. That said, let's keep it for now. The #assert_product_in_cart is a common pattern in my feature specs. I like assertions to be clear in their nature so I find abstracting them even if they aren't used in multiple scenarios useful. Now let's say we're testing a related feature: recommended products. We want to test that when we add a product to cart products are recommended on the cart page. feature 'Cart product recommendations' do\n  scenario 'Recommending products' do\n    when_a_shopper_visits_their_cart\n    then_they_should_be_recommended_products\n  end\n\n  def when_a_shopper_visits_their_cart\n    add_to_cart_from product_path(product)\n    visit cart_part\n  end\n\n  def then_they_should_be_recommended_products\n    product.recommendations.each do |recommendation|\n      expect(page).to have_content(recommendation.product.name)\n    end\n  end\n\n  private\n  let(:product) { create(:product, :with_recommendations) }\n\n  def add_to_cart_from(path)\n    visit path\n    click_button 'Add to cart'\n  end\nend You'll notice #add_to_cart_from is duplicated in this feature. We could have placed this method in a module that could be included in both features but why? What real benefit does this give us? Okay we don't repeat the definition of the function. The costs? Well you need to look in multiple files instead of one to understand and work with two features. For me I'd say it isn't worth it. Clever abstraction Now let's say we want to add another scenario to our add to cart feature but this time for a sized product. Let's be a little clever and be DRY up front. Both the when and then logic will be similar so lets reuse them. feature 'Adding product to cart' do\n  scenario 'Adding in stock product' do\n    when_a_shopper_adds_an_in_stock_product_to_cart\n    then_they_should_see_the_product_in_the_cart\n  end\n\n  scenario 'Adding in stock product with size' do\n    when_a_shopper_adds_an_in_stock_product_to_cart(sized: true)\n    then_they_should_see_the_product_in_the_cart(sized: true)\n  end\n\n  def when_a_shopper_adds_an_in_stock_product_to_cart(options = {})\n    if options[:sized]\n      visit product_path(sized_product)\n      select sized_product.sizes.first, from: :cart_product_size\n    else\n      visit product_path(product)\n    end\n\n    click_button 'Add to cart'\n  end\n\n  def then_they_should_see_the_product_in_the_cart(options = {})\n    visit cart_path\n    expect(page).to have_content(options[:sized] ? sized_product.name : product.name)\n  end\n\n  private\n  let(:product) { create(:product) }\n  let(:sized_product) { create(:product, :with_sizes) }\nend We've added an options argument to both methods and added some if logic into them. With the use of if statements we've handled both scenarios from within one set of functions. What have we really gained here though? This is basically being clever for the sake of it. Instead we could write out more specific when and then methods that are easier to understand. Instead of providing arguments to the when and then why don't we create unique methods per scenario. feature 'Adding product to cart' do\n  scenario 'Adding in stock product' do\n    when_a_shopper_adds_an_in_stock_product_to_cart\n    then_they_should_see_the_product_in_the_cart\n  end\n\n  scenario 'Adding in stock product with size' do\n    when_a_shopper_adds_an_in_stock_sized_product_to_cart\n    then_they_should_see_the_sized_product_in_the_cart\n  end\n\n  def when_a_shopper_adds_an_in_stock_product_to_cart\n    visit product_path(product)\n    click_button 'Add to cart'\n  end\n\n  def then_they_should_see_the_product_in_the_cart\n    visit cart_path\n    expect(page).to have_content(product.name)\n  end\n\n  def when_a_shopper_adds_an_in_stock_sized_product_to_cart\n    visit product_path(sized_product)\n    select sized_product.sizes.first, from: :cart_product_size\n    click_button 'Add to cart'\n  end\n\n  def then_they_should_see_the_sized_product_in_the_cart\n    visit cart_path\n    expect(page).to have_content(sized_product.name)\n  end\n\n  private\n  let(:product) { create(:product) }\n  let(:sized_product) { create(:product, :with_sizes) }\nend Okay so we've got a few more lines of code and it isn't as clever. We gain easier to read method names, simpler methods that contain no conditional logic and do what they say on the tin. Sometimes abstraction can compromise readability, so following DRY blindly, or getting excited by writing less lines of code can often mean your code is a little harder to read and understand. These examples are simple but I've seen some pretty bad cucumber steps with nested if statements in the past. Simple and stupid is good enough for me! Wrong abstraction Okay, time to write a feature for the checkout. We want to run through every step as a single scenario. The later steps are going to need the previous steps to have already run, however like good developers we do not share state between scenarios. Let's be naughty and reuse our steps in subsequent scenarios. feature 'Checkout' do\n  scenario 'Adding address' do\n    when_a_shopper_is_ready_to_checkout\n    then_they_should_have_to_add_their_address_details\n  end\n\n  scenario 'Choosing delivery method' do\n    given_a_shopper_has_added_their_address\n    when_they_want_a_fast_delivery\n    then_they_should_be_able_to_choose_next_day\n  end\n\n  scenario 'Paying for order' do\n    given_a_shopper_is_ready_to_pay\n    when_they_want_to_pay_by_paypal\n    then_they_should_be_redirected_to_paypal\n  end\n\n  def when_a_shopper_is_ready_to_checkout\n    # add product to cart\n  end\n\n  def then_they_should_have_to_add_their_address_details\n    # fill in address details\n  end\n\n  def given_a_shopper_has_added_their_address\n    when_a_shopper_is_ready_to_checkout\n    then_they_should_have_to_add_their_address_details\n  end\n\n  def when_they_want_a_fast_delivery\n  end\n\n  def then_they_should_be_able_to_choose_next_day\n    # choose next day\n  end\n\n  def given_a_shopper_is_ready_to_pay\n    when_a_shopper_is_ready_to_checkout\n    then_they_should_have_to_add_their_address_details\n    when_they_want_a_fast_delivery\n    then_they_should_be_able_to_choose_next_day\n  end\n\n  def when_they_want_to_pay_by_paypal\n    # select paypal\n  end\n\n  def then_they_should_be_redirected_to_paypal\n    # assert current url is paypal\n  end\nend I've left out the implementation of the steps but you get the picture. The given methods of subsequent scenarios reuse previous steps. Now this may be DRY but we've missed a couple of issues. Firstly, running capybara steps is much slower than operating directly on a DB. Instead of running through the browser in given steps, we could use factories that setup the DB in the right state. This means capybara only tests each step once. The alternative would be just to have one large scenario, which I've seen done before. I prefer splitting them out so that I can just run the one test when working on a checkout step. Second to that we are using given/when/then steps inside of other given steps. Reading through the code makes sense but it could be a little terser and more specific to the current test. With these points in mind we can reimplement the feature: feature 'Checkout' do\n  scenario 'Adding address' do\n    when_a_shopper_is_ready_to_checkout\n    then_they_should_have_to_add_their_address_details\n  end\n\n  scenario 'Choosing delivery method' do\n    given_a_shopper_has_added_their_address\n    when_they_want_a_fast_delivery\n    then_they_should_be_able_to_choose_next_day\n  end\n\n  scenario 'Paying for order' do\n    given_a_shopper_is_ready_to_pay\n    when_they_want_to_pay_by_paypal\n    then_they_should_be_redirected_to_paypal\n  end\n\n  def when_a_shopper_is_ready_to_checkout\n    # add product to cart\n  end\n\n  def then_they_should_have_to_add_their_address_details\n    # fill in address details\n  end\n\n  def given_a_shopper_has_added_their_address\n    jump_to_checkout(:delivery_step)\n  end\n\n  def when_they_want_a_fast_delivery\n  end\n\n  def then_they_should_be_able_to_choose_next_day\n    # choose next day\n  end\n\n  def given_a_shopper_is_ready_to_pay\n    jump_to_checkout(:payment_step)\n  end\n\n  def when_they_want_to_pay_by_paypal\n    # select paypal\n  end\n\n  def then_they_should_be_redirected_to_paypal\n    # assert current url is paypal\n  end\n\n  private\n  def jump_to_checkout(step)\n    order = create(:order, :jump_to_step, step: step)\n    visit checkout_path(order_token: order.token)\n  end\nend We have now moved the order creation into a factory, which means we can get more performant tests. I'd argue that the DRY principle helped us here in seeing that we were repeating ourselves. The solution was not to reduce the repetition by reusing steps, but to gain performance by using the factory. Not only was the reuse of capybara steps the wrong abstraction due to performance, it was also the wrong level of abstraction. I see three levels of abstraction in a feature spec: the scenario, the steps and then the helpers. The scenario is isolated inside the do block by RSpec, and I separate steps and helpers with the private keyword. Each level of the abstraction should never use other methods at the same level of abstraction, they should only call lower levels of abstraction. The fact we had given steps calling other steps broke this abstraction layering. Conclusion I use abstraction layering as a way of detecting problems in code. I can and will write a whole article on it eventually. It's a method for structuring your code so that related methods are found together, and that the various layers of your application do not cross concerns. Until I write my post, take me at my word that it's a good idea to use this abstraction layering when thinking about the DRY principle. Hopefully I haven't put you off DRY. It's an awesome tool. You just need to be careful on how you solve it and sometimes it isn't a problem that needs solving. Let me know what you think on twitter @LukeMorton . Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-04-19"},
{"website": "Made-Tech", "title": "8 Ways to Foster a Vibrant Company Culture", "author": [" Andrew Scott"], "link": "https://www.madetech.com/blog/8-ways-to-foster-a-vibrant-company-culture/", "abstract": "Culture is a word that is thrown around so much within our industry that it has become a parody of itself. From bizarro world news stories such as Dropbox’s cafeteria gaining a Michelin star , to the ping pong tables that are eternally associated with a tech office, it is often assumed that tech companies, and in particular startups, are innovative by default. This is reinforced by sweeping proclamations from people such as Paul Graham, who seem to believe that there is nothing better to do with your twenties than to spend them hunched over a keyboard eating Ramen noodles . But, as workers, we know that this is a fallacy. The startup industry to a large degree thrives on taking young, idealistic twenty-somethings and overworking them, to turn them into husks by the age of 30, at which point they rinse and repeat. This idea of culture is not sustainable so here are some things that have worked for us as a company to develop the culture to which we attribute much of both our personal and commercial success. 1: Flat Structure Whilst this may seem obvious, and go hand in hand with much of what the agile manifesto dictates, it is all too easy to end up in an Animal Farm type scenario where within a flat structure, everyone is equal, but some are more equal than others. We strive to avoid this by maintaining parity across teams, hot desking and equal privileges across the board (more on that later). **2: Camaraderie** There’s more to life than work. Contrary to the Gospel of Y-C, chapter 3:16, fun can be had away from adding value to a proposition. We go for a team lunch every Friday, regular drinks, and go to social events and meet ups together. This engenders a sense of brother/sister/humanhood, and has a commercial benefit too, since people perhaps give a little extra when they are working with and for friends. **3: Communication** With the flat structure also comes an equality of communication. Everyone has “equal access” to everyone else, everyone helps everyone else out, and crucially there is no social red line where perhaps one is scared to go and ask the founder something as opposed to someone else. This then naturally leads to… **4: Transparency** There is no faster way of making someone feel like a wage slave than having barriers to knowledge where there really is no need. Obviously there is some information that needs to be kept private, but giving people access to all the company documents on a shared Google Docs account, access to a company credit card for expenses and as much transparency about business decisions as possible all contributes to helping to build a sense of partnership as opposed to a hierarchy. All of this is part of… **5: Treating people like adults** When I grew up I was told that you never really grow up, school is just like university, which is just like a job, all one endless system of which you are constantly climbing the greasy pole. I don’t know whether I had a particularly dystopian childhood, but the company trusting people with the aforementioned company credit cards, the ability to work from wherever they feel comfortable and unlimited holiday (within reason), all help to make it feel like less of an obligation and far more like a proactive choice to come into work, and to be treated with respect. I guess you could say that this sounds a bit like… **6: Compassion** In a profession that is often viewed as a relatively brutal meritocracy, it is important to have compassion for those working alongside you. The Kindergarten doctrine is true, we all have our strengths and weaknesses and a company that enforces some kind of brutal Kafka-eque rank and yank system generally just serves as an engine for resentment and discontent. We have an excellent retention rate, and a key part of this is that people don’t feel as though they are in a nasty, competitive environment; they instead feel that they are on a team where we can all learn from each other. Whilst the John Lennon plays in the background we should move on to…. **7: Knowledge Sharing** In order for developers to become better developers it is key for work to be an educational pursuit as much as a financial one. We are blessed to work in a profession where there is always something new to learn and as I alluded to in my previous message, there are always some people who are better at some things than others. Some examples of ways to share knowledge and learn new things would be regular pairing, mentor relationships with more experienced developers and regular code dojos/mob programming where we explore a new language or a block of code together. All of this adds up to… **8: Fulfilment** At the end of the day, whether you are a bleeding heart liberal who wants everyone to get along, or a shrewd free marketeer, fulfilled employees are productive employees. It really doesn’t matter how you get there, but you need to get there. When I look at some of the jobs in Silicon Valley in particular, it seems almost like companies go out of their way to provide their employees with ridiculous benefits that they don’t need. Developers on $120k a year do not need free food, but it’s a race to offer the most outlandish thing as “perks”. This list has shown that you can cultivate and foster a great culture without needing to break the bank hiring Michelin star chefs, or a plethora of table tennis bats, and by focussing instead on improving the common things that already are there. Whilst burning VC money to hire yoga therapists may seem like a great use of your money at the time, I think that many of the things here are almost counter intuitive. Offering people unlimited holiday doesn’t mean people go crazy and take a year long sabbatical, they tend to take the normal amount, it just doesn’t feel forced in the same way it does when you’re extracting from your allowance. The essence of this post is that your work-life should be based around people, not the other way around. You don’t need to spend millions or provide fancy perks to create a great culture that scales with your company. You just need to really, truly care about your workforce, and it’s somewhat sad how rare that is. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-04-21"},
{"website": "Made-Tech", "title": "Automating infrastructure provisioning with Terraform and Packer", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/automating-infrastructure-provisioning-with-terraform-and-packer/", "abstract": "Here at Made we're always trying out new technologies that will automate repetitive tasks we need to perform on each new project. We’ve already discussed how we do this with our build pipeline , but until now the only manual task we’ve had to do repeatedly across all projects is provision infrastructure. We've been using Chef to set up provision our infrastructure for a while now, but we have still had to log in to the AWS console and spin up the EC2 instances to run it against, alongside all the other infrastructure requirements like RDS, S3, and load balancers. For this we can now use two Hashicorp tools – Packer and Terraform . These tools enable us to codify all of the configuration and commit it into source control. Having our infrastructure as code enables us to create identical server environments all the way up the deployment pipeline. Getting started? Before we get started there are a couple of prerequisites we need in place: Login to AWS to create a new user in IAM for Terraform and Packer to use, be sure to download the credentials file, and store it in ~/.aws/credentials. This is the standard location for AWS, and its also the default path both Packer, and Terraform. Its what I shall be using throughout to omit that configuration. It's worth mentioning that these credentials are what is used behind the scenes by the AWS libraries to create/destroy all the infrastructure. Generate a new SSH key pair. This SSH key will be used by Terraform, and added to the new EC2 instances. It will also allow us to SSH in and configure the newly created instances with Chef. When you generate the key pair you will need to specify a few additional attributes. -f chef-provisioner to specify a custom filename, -P to create a key pair without a passphrase, and -m pem to specify PEM format. The full command being ssh-keygen -f chef-provisioner -e -m pem Install Packer , and Terraform . Packer up Now, I'm going to go out on a limb here and assume you have a working knowledge of Chef-Solo, and how it works. If you are new to Chef, here is a video intro and the chef-solo docs , both of which are great starting points. We now have Chef set up, and have confirmed it works. Provisioning a machine image with Packer is really simple, all we have to do is specify a provisioner of type chef-solo and configure it . {\n  “provisioners”: [{\n    \"type\": \"chef-solo\",    “chef_environment”: “production”,\n    “cookbook_paths”: “./cookbooks”,\n    “data_bags_path”: “./data_bags”,\n    “environments_path”: “./environments”,\n    “roles_path”: “./roles”,\n    \"run_list\": [\"role[app]\", \"role[production]\"],\n    \"staging_directory\": \"/home/ubuntu/cookbooks”,\n    “skip_install”: false\n  }]\n} Now that we have the configuration for machine image, how can we make it available for use within Terraform? Packer provides a number of builders. In our use case we want to use the amazon-eps builder. This takes an existing Amazon AMI (ubuntu 14.04) and provisions on top of it. \"builders\": [{\n  \"type\": \"amazon-ebs\",\n  \"region\": “eu-west-1\",\n  \"source_ami\": \"ami-f95ef58a\",\n  \"instance_type\": “t2.nano\",\n  \"ssh_username\": \"ubuntu\",\n  “ssh_private_key_file”: “path/to/generated/private_key\",\n  \"ami_name\": “made-project-ami {{timestamp}}\"\n}] With both the provisioner and builder combined into one json all that is left to do is run packer build path/to/packer.json and Packer does the rest! Once packer has finished it will spit out the generated AMI reference. Keep a note of this. Let's Terraform this… provider \"aws\" {\n  region = “eu-west-1\"\n}\n\nresource \"aws_key_pair” \"chef\" {\n  key_name = “chef-provisioner\"\n  public_key = “your public key file\"\n} You can further dry up the aws_key_pair definition using the file function which is one of the interpolation functions built into Terraform. variable “azs\" {\n  default = {\n      zone0 = “eu-west-1a\"\n      zone1 = “eu-west-1b\"\n  }\n}\n\nresource \"aws_security_group” \"app” {\n  name = \"app_sg\"\n  # here be security rules\n}\n\nresource \"aws_instance” “app\" {\n  ami = “ami-m4d3am1” # Not a real AMI\n  availability_zone = \"${lookup(var.azs, concat(\"zone\", count.index))}\"\n  count = 2\n  instance_type = “t2.nano” # Can be any Amazon instance size\n  key_name = \"${aws_key_pair.chef.key_name}\"\n  security_groups = [\"${aws_security_group.app.name}\"]\n  tags {\n    Name = “${concat(“web_app_\", count.index)}\"\n  }\n}\n\nresource “aws_eip\" \"app_ip\" {\n  count = 2\n  instance = \"${element(aws_instance.app.*.id, count.index)}\"\n  vpc = true\n} You’ll notice we have specified the availability zones for the instances above; this is to guarantee maximum availability of our app, by ensuring we are spinning up instances in multiple availability zones within our AWS region. In order to open up our instances to the world, alongside the aws_instance resource declaration, there is an elastic IP being defined, and to secure the whole set up there is a security group defined. Taking this further A great iteration on this would be to get Packer to build an AMI of each successful deploy to production using the base box we built earlier on. To do this all we’d need to do would be to add an additional provisioner to checkout the latest stable production build and execute the necessary commands, e.g. bundle install. What benefit would this have? It would mean our application could scale up very quickly and we would always have a current production (or staging) AMI ready to go. Another direction to take this would be add the Atlas post processor (another Hashicorp tool) to the Packer json to register for Atlas Enterprise and leverage its artefact directory to always serve the “latest” production artefact to Terraform . This would also hopefully take care of housekeeping all the older production machine images. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-04-26"},
{"website": "Made-Tech", "title": "Experimenting with Clojure for a day", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/experimenting-with-clojure-for-a-day/", "abstract": "Ruby really is my favourite programming language, I love the clean syntax and the freedom that it gives you. Beyond debugging meta programming in gems far far away, I have no real trouble with solving problems in it. Many people say that it is too slow, but I have found that effective caching and background processing negates this for the majority of typical web apps. So why Clojure? We all know the awesomeness that makes Lisp unique: Homoiconic Function composition Macros Advanced interactive development Data first Simplicity One thing that got me interested in it was that there was some sort of natural progression in style of coding over the years. I started placing more value in data and started seeing stateful and stateless code as a separation of concerns. Lisp seemed to have all these principles baked into its core, yet I had to work so hard to get them in procedural languages. Made Learning Day We recently had a day dedicated to learning / exploring something that may benefit to our daily work, whether it be a programming language, library, service or whatever. This would be the perfect opportunity to share my interest in the language. We were divided into 3 teams with one person at the head of each team to lead the way. We all pitched our ideas and people joined the team with the idea they were most interested in. The 3 winning suggestions were: Clojure Elixir Swift Leading Team Clojure, I was far from a professional but knew enough to get us started. Some of my previous Lisp experience included: Configuring Emacs Smashing as much of the Wizard book into my brain as possible Clojure for the Brave Random katas My two team mates were Chris and Nick, and they were pretty enthusiastic about it. London Google Campus would be our location. I did a quick introduction to Lisp including history, features like the reader, macros, data structures, and why I think this language was worth researching. Our approach wasn't going to be to zone in on any one aspect of the language, I instead opted for building something useful as quickly as possible. At the end of the day I hoped to achieve the following: A CRUD product system, backed by MySQL Have at least one unit test Make it presentable for demo Stretch goals Caching I18n Image uploading Deployment to Heroku We used Luminus (whoops nearly said framework there) to create the program. The documentation has an awesome example of building a xxx system, and we were going to adapt this to build our own product CRUD system. So we started hacking away at it, picking random tickets that we had created in Trello. The bad Having to reboot the JVM for changes to be picked up Plain SQL, we missed our ORM's! (as imperative programmers do) Java stacktraces; the volume of data made it feel like we were not escaping any complexity Testing culture in general was different, and we struggled to get into our red / green / refactor cycle Less convention over configuration than we were used to; we had to actually think about glueing stuff together The good We were able to make significant progress in 5 hours and had a lovely \"stable\" website to demo Fun talking about the origins of Lisp and realising how different it is from object oriented programming Beautiful maps without commas Variety of interesting datatypes Preview of how awesome the community is; because the barrier to entry is higher than Ruby I find that there are a lot more smart academic types The safety that immutable data structures bring; you have to be explicit about shooting yourself in the foot Rainbow parentheses! Nick wrote what I would consider the best function of the day. (defn slug-from-name [name]\n    (clojure.string/lower-case (clojure.string/replace name #\"s+\" \"-\"))) What next? It is optimistic to assume that you will be sold on a language in 5 hours of experimenting with it. To be honest I felt quite a bit of pain as I was trying to write Clojure in my default imperative way of thinking, but I will continue to study Clojure and bring it up again as a research topic at the next MLD! Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-04-28"},
{"website": "Made-Tech", "title": "11 Developers Who Said It Best", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/11-developers-who-said-it-best/", "abstract": "Those who forget the past are condemned to repeat it. Writing code is an art form. There is no one correct way to do things, so we strive to solidify our understanding of what works and what doesn't. This makes things interesting.  In my quest to understand how to write good programs, some quotes have resonated with me. Below are some of my favourites, which pop up in my head every so often when I am in doubt. For each desired change, make the change easy (warning: this may be hard), then make the easy change – Kent Beck Also known as Preparatory Refactoring , you sometimes need to take a step back to gain agility in implementing your new feature. Refactoring should be included as part of the feature, and should not be treated as a separate task in my opinion. It may feel counter intuitive especially when there are tight deadlines, but this usually results in a more robust solution. Bad programmers worry about the code. Good programmers worry about data structures and their relationships. – Linus Torvalds I find this quote so important that I have written an entire blog post about this. Smart data structures lead to dumb code which is always desirable. You are pushing the decision making down into the primitives, and end up with a system that has suffers from less cyclomatic complexity. The trouble with programmers is that you can never tell what a programmer is doing until it’s too late. – Seymour Cray A bit harsh but it is easy for even the most experienced developers to head down the wrong path when trying to solve a problem. This is usually a result of poor communication. There needs to be constant discussions between the developer and the stakeholders or expectations can go out of sync. Good design is obvious. Great design is transparent. – Joe Sparano I feel like this is very much related to the Principle of least astonishment . If making a change is boring because you only needed to navigate to where you expected the logic to be contained, you will not find it very interesting. If however you have to absorb some of the obstacles that the previous author of the code wrote, it can be more challenging. Besides a mathematical inclination, a [..] mastery of one's native tongue is the most vital asset of a competent programmer. – Edsger Dijkstra In order to express yourself well through code, you must master the language that you write / speak in. Naming concepts in a program can make it understandable or a total mess. I often open a thesaurus to double check that the name that I have selected is as expressive as it can be. Code should be written to be understood by humans first and computers second. Functions that create values are easier to combine in new ways than functions that directly perform side effects – Marijn Haverbeke Pure functions are easy to test and faster than functions with side effects. I always try create as many pure functions as possible and have an imperative layer wrap these to do the interacting with the volatile outside world. Haskell has taken this concept so far that it does not allow side effects apart from ones wrapped in the IO monad. The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be absolutely precise – Edsger Dijkstra Definitely one of my favourite quotes of all time. When you create a code seam, and name it correctly it can house a specific piece of knowledge and can be thought about in isolation. Class and method extraction refactorings are some of the tools used to accomplish this. When in doubt, use brute force. – Ken Thompson Quite often we encounter bugs that seem impossible to comprehend. You need to eliminate all the variables in order to get to the root cause of the problem. You need to cut through the layers of abstraction to get to the actual piece of buggy code. Strace, Tcpdump and Wireshark exist for when you have exhausted all your options but hopefully will not come to that. A language that doesn’t affect the way you think about programming, is not worth knowing. – Alan Perlis There are so many interesting programming languages that aim to solve problems in different ways. Don't waste time learning new languages that follow the same paradigm but only differ in syntax, you probably understand the underlying concepts anyway. All code in any code-base should look like a single person typed it, no matter how many people contributed. – Rick Waldron It is very important to keep consistency throughout the codebase. Even if you think a style of solving problems can be improved upon, stick to the existing way of doing so. If you want to refactor it to be better, do so to all the code at the same time. Given enough eyeballs, all bugs are shallow. – Eric S Raymond Here at Made we pair quite frequently on writing code. It is incredible how much can go undetected if you are doing things on your own. Pull requests is another excellent form of keeping code quality high. Got a favourite quote? Share it below! Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-06-10"},
{"website": "Made-Tech", "title": "Overstacked? The journey to becoming a full stack web developer", "author": [" Andrew Scott"], "link": "https://www.madetech.com/blog/overstacked-the-journey-to-becoming-a-full-stack-web-developer/", "abstract": "At first glance, it is easy to believe that programming as a profession is one which is both in rude health, and for which the future is incredibly bright. Increased automation, the mind bending world of machine learning, and the ever more intuitive ways in which software impacts our lives all suggest that programming is the career to be in, and one of the few careers which one can safely guarantee will still be around in 50 years irrespective of automation or many of the other issues that threaten the future workforce. Many thousands of people have heeded the call. An entire industry has been rapidly built around getting budding young developers “job ready” in 12 weeks. The idea, in my experience as a student of an early self driven remote course , is to provide fertile young minds with just enough Rails/JS knowledge to get them through a technical interview, and that’s about it. These businesses thrive on selling dreams of working for Google and Facebook and usually profit handsomely both at point of enrolment and graduation (when finding their students jobs). But this is not yet another blog post criticising the commoditisation of young programmers, as that topic has been explored to a large degree already. The problem comes when these budding young developers hit the jobs board. The halcyon days where a developer could simply get stuck in with a particular language, whether on the front or the backend, are gone. The definition of a full stack developer is somewhat vague and the requirements for the role depends entirely on who you talk to. A better place to start may well be to ask the question: what makes a good professional developer, full stop? The short answer is clear: a professional developer produces good quality code on a regular basis. It is a much more complex question as to how said developer can achieve this. It's not enough to be a savant about a given language or framework, due to the fact that this does not help with strategic decisions and technology moves at such a pace that such knowledge in and of itself may soon be useless (who's hiring Flash developers?). Knowing the plethora of buzzwords of design and architecture is all well and good in a theoretical sense, but it does little to help with a concrete implementation. Understanding design patterns is frequently cited as good advice for budding developers, but that again is not sufficient, both due to the fact that they focus on particular challenges and are frequently misused but also due to the fact that the framework that you use often makes virtually all of these decisions for you. Instead, a good professional developer must have an understanding on all of these areas (in their mother tongue), alongside many others. The code which appears on the IDE is simply the culmination of this work and the considerations of dozens of technical details, which are often interconnected in tenuous ways. And then you add to this the full stack, beyond one's mother tongue: The phrase \"a jack of all trades is master of none\" can ring true here. Whilst nobody would be expected to know all of the items in this list, understanding one from each row would certainly be a prerequisite to being considered a \"senior\" full stack developer. This is the stack as of today, and every year there are additional prerequisites added. There is also a clear opportunity cost when you deviate from your core competency to learn something new. Whilst some of these skills are trivial to learn, fully appreciating the idiosyncracies of any major programming language or framework can take years. So where did this idea of a full stack developer come from? Facebook of all places seems to have provided the genesis of this idea, or more specifically a Facebook engineer named Carlos Bueno . At the time this was written, Facebook only employed full stack developers which makes a lot more sense when you view it within the context of its time. It had a relatively simple PHP backend and did not have the massive technical demands that it has now. Early iterations of Facebook certainly did not require 2-3 years of professional front end design skills. Personally I think the idea of the full stack developer comes from the age old idea of the 10x developer , who have come to represent the Ark of the Covenant for startups and smaller businesses that cannot afford to hire specialised developers for every aspect of the delivery of a web application. The two terms seem to be used as synonyms for each other, but I think the idea that is hidden beneath all of the advertising is that companies want to hire super effective engineers. I would argue that it is significantly counter productive to developing such engineers by hiring them for these roles. Excellent full stack developers do exist (I work with several), but very few of them started out as such. The conventional wisdom is that a good programmer is a good programmer irrespective of language, but as the programming world splinters into ever more complex language combinations, frameworks and even programming paradigms (functional anyone?), it is perhaps pertinent to take a moment to sit and consider the best way of acquiring good problem solving skills. Whilst critical thinking and problem solving skills are developed, are we helped by having to learn to understand the intricacies of Chef? What makes this even worse is that often the companies who are seeking full stack developers most ardently are startups where you can add the specs for a project manager to the list of requirements. But what is happening in programming is symptomatic of a wider cultural shift within the workforce. Employers want full stack employees , because why hire dozens of people if you can get one person to do all of their jobs to a higher degree? If a brilliant full stack developer exists, as defined in Bueno's piece, are they achieving maximum utility by even writing code? Someone of that talent, with that level of expertise across the stack should sit in a CTO role rather than making rudimentary code changes in the trenches. For the avoidance of doubt, I am not advocating siloed programming where everyone is ignorant of the rest of the stack and what the rest of the team is working on. The path to being successful at anything is in knowing what you know, knowing it well, and more importantly knowing what you don't know and knowing where to improve on this. Being thrown into the deep end of development by having to learn 8 (possibly more) disciplines simulataneously is not the way to do this. The underlying truth is that there are not enough unicorns in the forest to fill all of these roles to the level at which they have been advertised and this makes a large amount of job \"vacancies\" permanently unfilled, which in turn pushes more people into the cycle of thinking there are millions of tech jobs available. Instead we should focus on allowing our junior developers to grow and to develop their core competencies before branching out into the various layers of the stack. A key part of a developer's growth is confidence, generated via a series of \"wins\", but that is difficult to achieve when you are seeking wins in sometimes drastically different areas. As a case in point, Chef is written almost entirely in Ruby, but trying to navigate the source code as a novice programmer is a project in and of itself. The most important skill in programming is learning to learn, and that can only be improved upon by learning one thing and learning it well. The best programmers I know have a rapacious thirst for knowledge, which is built on a solid bedrock of understanding of the fundamentals they are using. It's difficult to have that when you are standing on 8 separate bedrocks each made of sand. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-05-05"},
{"website": "Made-Tech", "title": "Let’s Encrypt setup on Nginx", "author": [" Alex Minnette"], "link": "https://www.madetech.com/blog/lets-encrypt-setup-on-nginx/", "abstract": "Let's Encrypt is a new certificate authority which provides free and automated certificates. It's sponsored by many of major players including the EFF, Mozilla and Google. The service is now considered stable and just left beta recently. As I write these lines, over two million certificates have been issued in less than five months following an exponential growth. The statistics can openly be checked at https://letsencrypt.org/stats/ . How does it work ? The Let's encrypt client is communicating to the certificate server through a protocol called ACME . This protocol is handling all the operations related to the certificate and is used by the letsencrypt client to register or revoke certificates. Installing the client I'm going to assume you are using Ubuntu here for the installation but it can be adapted really easily to other distributions / operating systems. Please run these commands as root ( sudo su on Ubuntu). # apt-get update\n# apt-get -y install git\n# git clone https://github.com/letsencrypt/letsencrypt /opt/letsencrypt\n# /opt/letsencrypt/letsencrypt-auto /usr/local/bin/letsencrypt-auto The let's encrypt command line tool can now be used! It probably still needs to install more dependencies to work properly, just call letsencrypt with the –help flag and it will install the remaining dependencies. # letsencrypt-auto --help Everything is now ready to get the certificate! I'm going to assume here the domain is already registered, if it's not, just add it from your registrar. Nginx configuration Preparing for let's encrypt First, the dhparam.pem file needs to be generated. This file is used by the Diffie-Hellman key exchange algorithm in order to achieve Perfect Forward Secrecy . In other words, even if a third-party gets access to your private key in the future, no previous communication which used HTTPS can be decrypted. The file can be generated by using this command (this can take a long time): # openssl dhparam -out /etc/ssl/certs/dhparam.pem 2048 Now we can create the Nginx configuration! First we need to enable read access to the .well-known folder. This folder is used by let's encrypt to verify that you have ownership of the domain and that you are not generating a certificate for someone else. Let's encrypt is going to create a file in this directory with a secret and verify its access before creating the certificate. The configuration is normally located in /etc/nginx/site-enabled/example.com.conf. server {\n       # this is the basic nginx configuration for a static site\n       # you might have more depending on your site!\n       listen 80;\n       server_name example.com;\n       location / { autoindex on; }\n\n       root /var/www/example.com;\n\n       # enabling access to Let's encrypt\n       # this is the line we need to add\n       location ~ /.well-known { allow all; }\n} Then Nginx should be reloaded to take into account those changes. With systemd, this is how you do it: # systemctl reload nginx If you are not using systemd, you might use service nginx reload or /etc/init.d/nginx reload to achieve the same effect. Generating the certificate Everything is ready to get the certificate! Now let's encrypt should be called to verify the domain and retrieve the certificate for us. # letsencrypt-auto certonly -a webroot --agree-tos --renew-by-default --webroot-path=/var/www/example.com -d example.com If everything is configured properly, this is what should appear on the screen: Requesting root privileges to run with virtualenv: /root/.local/share/letsencrypt/bin/letsencrypt certonly -a webroot --agree-tos --renew-by-default --webroot-path=/var/www/example.com -d example.com\n\nIMPORTANT NOTES:\n - Congratulations! Your certificate and chain have been saved at\n   /etc/letsencrypt/live/example.com/fullchain.pem. Your\n   cert will expire on 2016-08-04. To obtain a new version of the\n   certificate in the future, simply run Let's Encrypt again.\n - If you like Let's Encrypt, please consider supporting our work by:\n\n   Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate\n   Donating to EFF:                    https://eff.org/donate-le The certificate has now been retrieved and can be seen in the /etc/letsencrypt/live/example.com folder. # ls -l /etc/letsencrypt/live/example.com/\ntotal 0l\nrwxrwxrwx 1 root root 45 May  6 09:20 cert.pem -> ../../archive/example.com/cert1.peml\nrwxrwxrwx 1 root root 46 May  6 09:20 chain.pem -> ../../archive/example.com/chain1.peml\nrwxrwxrwx 1 root root 50 May  6 09:20 fullchain.pem -> ../../archive/example.com/fullchain1.peml\nrwxrwxrwx 1 root root 48 May  6 09:20 privkey.pem -> ../../archive/example.com/privkey1.pem HTTPS Configuration Here is the configuration I use for HTTPS, I've added comments to the relevant sections. You can test if everything is configured properly by using the Qualys SSL Server Test , this Nginx configuration is normally getting A+. The first part of the configuration is the same as the HTTP part, the HTTPS configuration can be added directly after the HTTP one in the exact same file. server {\n   listen 443;\n\n   # This is copied from the HTTP section\n   server_name example.com;\n   location / { autoindex on; }\n   location ~ /.well-known { allow all; }\n   root /var/www/example.com;\n\n   # ----- SSL configuration -------\n\n   # certificate location\n   ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;\n   ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;\n\n   ssl_protocols TLSv1 TLSv1.1 TLSv1.2;\n\n   # cipher list\n   ssl_prefer_server_ciphers on;\n   ssl_ciphers \"EECDH+AESGCM:EDH+AESGCM:ECDHE-RSA-AES128-GCM-SHA256:AES256+EECDH:DHE-RSA-AES128-GCM-SHA256:AES256+EDH:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4\";\n   ssl_session_cache shared:SSL:10m;\n   ssl_dhparam /etc/ssl/certs/dhparam.pem;\n   ssl_session_timeout 1d;\n   ssl_session_tickets off;\n   ssl_stapling on;\n   ssl_stapling_verify on;\n\n   # security HTTP headers\n   add_header X-Frame-Options DENY;\n   add_header Strict-Transport-Security max-age=15768000;\n   add_header X-Xss-Protection \"1; mode=block\";\n   add_header X-Content-Type-Options nosniff;\n\n   # allowing lets encrypt to access the well-known folder\n   location ~ /.well-known { allow all; }} Everything is now ready to disable HTTP completely! The HTTP part of the configuration can now be changed to redirect any HTTP connection to HTTPS by doing a standard redirect. location / { return 301 https://example.com$request_uri; } Renewing the certificate For security reasons, let's encrypt certificates have a low validity duration, the certificate needs to be renewed regularly. To renew a certificate, the same command used to generate it in the first place can be executed: # letsencrypt-auto certonly -a webroot --agree-tos --renew-by-default --webroot-path=/var/www/example.com -d example.com A crontab makes it easier to renew automatically the certificate: # crontab -e 5 8 * * Sat letsencrypt-auto certonly -a webroot --agree-tos --renew-by-default --webroot-path=/var/www/example.com -d example.com In this case, the certificate gets renewed every week on Saturday morning. Because it's easy to miss if a certificate is not renewed, SSL Catch is a small service which is going to send you an email if the certificate is about to expire, this is definitely going to save you some time if part of the configuration is wrong. Conclusion Even if the Nginx configuration is not built-in into the lets-encrypt client like Apache, the let's encrypt tool makes it easy to create new new certificates. With over two million certificates already issued, the HTTPS web is growing stronger and stronger every month and making the web a more secure place. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-05-10"},
{"website": "Made-Tech", "title": "JavaScript Fatigue", "author": [" Alex Minnette"], "link": "https://www.madetech.com/blog/javascript-fatigue/", "abstract": "In the last 10 years, the web has grown quickly from a document-only platform to full-scale applications. A good chunk of the applications which are now being developed are no longer native, and are instead relying on the web. This massive ecosystem change has not been painless and a lot of developers in the JavaScript community experience what could be called 'JavaScript Fatigue'. There are now more than 350 different APIs listed on Can I use with various levels of browser support and an ever-expanding scope of JavaScript. JavaScript is also now commonly used on the server side with Node.js and on desktop with Electron or Node Webkit . The tooling is complex Keeping up with all these different technologies comes at a cost. Browsers are slower to update than the language and as a consequence, lots of tools have emerged to circumvent this problem. ES6 , the current JavaScript standard is partially incompatible with existing ES5 implementations so a transpiler must be used in production to make sure the code actually runs on most browsers. The next iterations of JavaScript, ES7 and ES8 are currently being specified. A modern tooling can include ES6 and JSX transpilers, boilerplates, linters, minifiyers, Bower and NPM package managers. In order to generate and run all of this reliably, some scripting must be used, Grunt or gulp are currently the most popular ones. All of these tools are very complex to setup together so some project generators like Yeoman have appeared. Even the summary itself of the most popular React generator on JavaScript for Yeoman clearly shows the problem by showing a large list of included tools: React Starter Kit — isomorphic web app boilerplate (Node.js, Express, GraphQL, React.js, Babel 6, PostCSS, Webpack, Browsersync) For a JavaScript novice or JavaScript developer coming from 2005, all of these technologies are new and understanding how each of them interact with each other can take a long time. Poor debuggability Because the end result on the browser is very far from the developer source code, it becomes harder and harder to debug JavaScript applications. After passing through dozens of transpilers, minifiers and other code generators, the end source code actually executed is very different from what the developer sees. Event debugging has also not improved very well in the past few years so understanding what triggered an action is now even more complex. Sourcemaps might not work very well JavaScript sourcemaps are an attempt to provide some better debug stacktrace to heavily transformed JavaScript but sourcemaps also have their own shares of problems. While all major browser now support sourcemaps, not every browser supports them to the same degree and, depending on the browser, the resulting stacktrace can sometimes be largely unhelpful. The sourcemaps also need to be applied in the right order during all the tooling transformations, otherwise the resulting sourcemap might be incomplete or invalid. The environment is changing too quickly Client-side frameworks are constantly changing Client side frameworks are appearing and disapearing very quickly. As an example, on the front-end side, we have: Backbone Knockout Batman – Now deprecated Ember Angular – Angular 2 has now very different semantics than the original Angular React – The current trendy framework, made by Facebook. All of them were popular at some point and then decreased in popularity, the current framework du jour is at the moment React.js, but for how long ? The codebase still needs to be maintained, sometimes years after developing the code, but having a new trendy JavaScript framework every two years is expensive to maintain. The tooling is constantly changing The tooling itself has also changed a lot in the past few years, here is a small list of technologies which have been experiencing changes: CoffeeScript : A language wrapper to make JavaScript nicer, since most of the improvements are now available in ES6, CoffeeScript seems to die slowly. Typescript is a new transpiler adding types to JavaScript which seems popular at the moment (for how long?). Require.js / Webpack: Tools to bring the power of package managers to the browser, even if both are still popular, the new trend is now to use Browserify . Grunt : An equivalent of make for Node.js , the new tool du jour is now Gulp.js and a lot of repositories are converting their codebase to the latter. Babel , Traceur , Esnext …: The war for ES6 transpilers is still on and no clear winner can currently be observed. SCSS / SASS / Less / PostCSS : The tooling war does not stop at the borders of the JavaScript world but also extends to CSS. SASS seems to be slightly more popular than Less at the moment but it's difficult to predict what will happen there. The language itself is constantly changing ES6, the current addition to the language is not itself adopted, but talks are already focusing on ES7 and ES8. The ES6 compability table is now very large. This table is just the additions to the language itself, not considering the new Web APIs appearing almost every other week. The UNIX philosophy seems a practice from a distant past. Every new API is now added to an ever-ending global window Object along with prefixes and checking API support is quite complex. What counts on the browsers is now the speed of delivery, stability is only considered secondly. IndexDB barely works reliably between different browser implementations ( PouchDb as an example is using a lot of workarounds to make it work) and the HTML5 Offline Cache offers so little control that it's barely used in production despite having a lot of potential. Conclusion As the JavaScript language is now properly maturing, the speed of change of the ecosystem should hopefully slow down a little bit and enable developers to actually let their projects age. I hope for a future where coding JavaScript applications is simpler and choosing tools much easier, while competition is a worthwhile goal, it is often at the expense of stability and simplicity. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-06-15"},
{"website": "Made-Tech", "title": "The Pros and Cons of React + Redux", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/the-pros-and-cons-of-react-plus-redux/", "abstract": "React is a JavaScript library which brings a declarative class driven approach to defining UI components. Redux is a state management layer which allows you to write events as simple 'action' objects, and centralises their storage and all change processing. Action objects which look like the following are dispatched in a fire-and-forget fashion and trigger an efficient re-render if necessary. Pros Easy to start writing (if you can get over the syntax) Writing React templates is extremely similar to writing HTML with interpolation thanks to JSX. It ends up looking similar to Mustache templates, except the markup is directly in your JavaScript component code. For example, a simple \"call to action\" button might look like this: You'd use it like this: It's unusual, and many developers have a hard time getting used to it, but once you get over your preconceptions it becomes extremely familiar. Notice that in the above code the markup is inside your JS class, and your css is import ed at the top as well. Naturally, your JavaScript code would go here too. React is extremely unusual in that it encourages you to put all your markup, style and functionality in the same place, but this gives you convenience and extreme modularity in the long run, which we know is a good thing. Total separation of data and presentation React provides little more than a presentation layer. Although React components do have a concept of 'state', this is best used for ephemeral storage. Anything you could lose on a new render can go in React state, although when combined with Redux, putting as much of your data in your Redux store as possible generally gives the best results, as it allows complete separation of state (Redux's concern) and presentation (React's concern). You hook top level React components (think 'views' in an MVC framework, often referred to in the React world as 'containers') to a subset of the data in your Redux store. For example, a 'TweetDash' container might depend on the 'tweets' key in a Redux store with a state like this: Never think about re-rendering! The benefit of this separation of concerns is you don't have to concern yourself with whether something has been rendered before, or whether it's the first time. Since React rendering is immutable, the first render and the 100th render of the same component are handled the exact same way. When state changes, Redux re-renders relevant components on the page. Users of frameworks like Backbone or Angular may have experience with re-rendering on data changes on the front-end. It's not a new concept. The reason React is much more successful in this regard is because it applies changes to a 'Virtual DOM' and creates a diff of the smallest changeset possible, which it then applies to the DOM. Since updating the DOM is the lengthiest part of the render process, this massive reduction in DOM updates greatly improves performance. It means you really can forget about the rendering process, which has never been true before in front-end. DOM binding isn't our concern If you've written any front-end component, with or without a framework, in the past five years then you know the pain of binding DOM elements to functionality. Imagine a simple comment list with a form. When you add a comment using the form, it appears in the list. Behind the scenes in most implementations, this means listening to forms with a certain class. When one submits you retrieve the content of the form and make an AJAX request to your server with that payload. When it responds you either render the whole form again or render another '' with your templates and append it to the list. This is a very tightly coupled and complex series of operations. Although as web developers, we're quite used to writing them, we write it differently to most of the code we write. In other areas we strive for DRY, reusable code with single responsibilities. But in front-end, we write highly specialised, tightly coupled, multi-responsibility code to achieve simple tasks. Although React would handle this in much the same way, it would be split across multiple areas of the code with single responsibilities. From the React developers perspective, we have the form element selection (React does this), the form event listening (our Component does this), the AJAX request (a Redux Action), the state updates (a Redux Reducer) and the rendering (React-Redux does this). No more tightly coupled code. No more defining awkward, un-semantic classes in your HTML to suit your JavaScript. Elements are bound directly to functionality. Server-side rendering Some of the most touted problems with Single Page Applications (SPAs) are SEO optimisation and page load times. If the client does all the heavy lifting then the user has to wait for initial page load, then for rendering. This is slow. What about accessiblity concerns? There are still users with JavaScript disabled! These issues have always plagued SPAs and there have never been great solutions. Although many have allowed some presentation reuse between server and client, generally using shared template files, the construction around them has always differed. Having to write separate views for the server and the client is unreasonable these days. Since React is just a component library, it can be rendered both in the browser using the DOM, and to a string of HTML which the server can deliver. Using component routing libraries (namely react-router or react-router-redux) means you can even use the same routing on the server and the client. Naturally this is easiest with a Node server, as it's in the same language, but there is support in most other backend frameworks for doing the same, often by running a Node server on the side exclusively for React rendering. React isn't a framework React is a library which provides a declarative method of defining UI components. ReactDOM is an associated library which provides rendering and DOM diffing. Redux is a library which provides a data store, and React-Redux provides the glue between React and Redux. There's a philosophy around building React apps, which I've covered, but there isn't a formal framework to stop you doing what you like with the rest of the app. This reduces lock-in, as you can swap out the presentation layer of your front-end if you change your mind without affecting the rest of your codebase. Changing the data layer is harder, but doable. Conversely, it means React components can be gradually introduced to a codebase, rather than thrown in whole cloth. Facebook did this, replacing existing UI components on the Facebook Wall with React components one by one. Cons React isn't a framework Philosophy is great, but when you need to get something done quickly, the React Way can be frustrating. If you have clients and projects and pressing deadlines and the first page of your React handbook no longer works (I've actually seen this), you can get frustrated. Fast change generally means growth and improvement, which is great in plenty of ways. Despite React being a relatively new tool in the front-end toolbox, it's achieved dramatic improvement since it was revealed. You can use it stably in production and your speed of development can flourish, but with a serious caveat. Getting started is slow unless you know what you're doing. React (and JavaScript in general) doesn't have \"stable\" the same way most languages, libraries or frameworks have \"stable\". Stability in React is all too often \"working well but we better keep these versions locked down because it's all going to change soon.\" Be prepared to watch libraries grow from gists to large codebases quickly, and for them to change into something just as confusing by the time you've finally gotten your head around them. Take react-router, which handles client-side routing. It has seen dramatic, unwieldy changes from v0.13.1, v1.0.0, v2.0.0, it's fork rrtr to its child library react-router-redux in the past six months, and coming in fresh it's still hard to pick up where to start. What's the right way to write react-router? What's the write way to write React in general? The looseness with which React can be employed is great for experimentation, but challenging when you're trying to do things the right way. Knowing there isn't one yet will save you some time here. Community conventions are still developing How do I structure this? How do people handle that? I won't say library developers don't have strong opinions on how their libraries should be used, because they certainly do. The problem is turnover and change is so rapid, these don't have time to solidify into common practices. Only those really paying attention to monthly, weekly, daily changes in the React community could tell you the best way to use X library. This is endemic of a larger problem in the JavaScript community. There's so much going on and so many problems to solve, the community hasn't had time to settle on solid, fundamental JavaScript project practices. Some of this is again philosophical, the community certainly favours the *nixy 'simple functions and common interfaces over rigid structure and adherence to whats popular' standard more than most web developers. Build tools are necessary (or strongly encouraged) For all front-end apps but the absolute simplest, decent build tools are highly recommended. For simple apps you can often get away with shell scripts in your 'package.json' file. For more complex apps, you might want to use a tool like Gulp to manage multiple tasks. These build tools are useful but remain unnecessarily complex. I would strongly recommend sticking to npm scripts like above and using the command line interfaces of your build tools, for example 'browserify' or 'webpack-dev-server'. Using build tools isn't too much of a pain, we generally use them when we work in other languages anyway. The only real issue with using them in JavaScript is there aren't standard, reusable solutions for all of your projects. This can unfortunately seriously increase the amount of time it takes to get the idea in your head into code. Conclusion React is an unbelievable way to write front-end code. Once your project is set up and you have a decent idea of what you're doing, developers are able to create stable, snappy and sane applications in a fraction of the time it normally takes. This is especially true when creating single page applications, which have always been a bit of a pain. React introduces a few headaches, learning it up front is challenging and keeping up is more so, but it will save you many, many headaches in the long run. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-06-22"},
{"website": "Made-Tech", "title": "What is Continuous Delivery?", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/what-is-continuous-delivery/", "abstract": "In today’s world of instant communication, businesses need to be able to move quickly to meet the ever changing needs of their customers, especially so when it comes to building, maintaining and evolving the online platforms of those businesses. Past development practices didn’t always lend themselves to this fact, with releases taking place weeks or even months apart, and huge changesets being the norm. At its inception, the goal of Continuous Delivery was to find a way to optimise those practices to the point where small, iterative releases could happen, and that those releases are be safely deployable to a production environment at any time, and are deployed as often as several times a day. For businesses, this is incredibly valuable, in that they are then able to get features to, and feedback from, their customers at a much faster rate. For developers, they can be confident that what used to be a very painful process is now simple, mundane even, and that the code they release is stable. Where did Continuous Delivery come from? In a nutshell, Continuous Delivery was borne, many eons ago (i.e. within the last twenty years), out of the frustration of developers stuck in what was known as ‘Integration Hell’, the point at which the code you’ve been working on for the last few weeks meets the code that the rest of your team has been working on in that period, and the point at which you realise those two code bases are incompatible in subtle and sometimes devastating ways. It used to be that a project would be worked on by a team of developers who each went away work on their own area of code for periods of weeks or even months at a time, after which all of the work would be brought together, and everyone would knowingly walk into Integration Hell. Developers are problem solvers by nature and, as a huge pain point, this was a problem that very much needed to be solved. Automation For The People One such solution was Steve McConnell’s ‘Daily Build and Smoke Test’ , from 1996. His idea was for all code to be tested at the end of every day and run as a ‘smoke test’, an automated process that immediately ‘broke the build’ should it find any fundamental problems. His reasoning was that if anything did break, chances were that it would be much simpler to fix something that had cropped up in the last 24 hours than it would be to fix something that had occurred at some point within the last month. The idea struck a chord, and later, over at Thoughtworks , Matt Foemmel, along with Martin Fowler, took a similar concept introduced by the Extreme Programming methodology and developed it into something much bigger. This concept was called Continuous Integration (not to be confused with Continuous Delivery ). The pair wrote about their experiences in this popular article , the broad strokes being that developers should be contributing code to a single repository under source control, which then, automatically, runs a series of tests, followed by a build on success. While the build and testing steps are automated, there is still (and most likely always will be) a necessarily manual step to all of this, in that individual developers are themselves responsible for reintegrating several times a day. Integration problems will still occasionally arise, it just means that when they do occur, they occur early, and are usually small enough to prevent another trip into Integration Hell, making the entire process demonstrably less painful for everyone. As an added bonus, the fact that all of the code is committed to a single repository means that new developers can join the project without needing to spend days setting up a development environment, again increasing the speed at which a project can move. So, what is Continuous Delivery? Where Continuous Integration is about making it easier for multiple developers to work on the same project together, Continuous Delivery is about how we make it easy to get that work quickly and safely to production, and out into the wild, actually delivering value to Product Owners and their users. This transition from development to production is facilitated by what’s known as a build pipeline, which houses a number of different ‘environments’. The pipeline handles the process of advancing our code through each of these environments whereby, upon a successful build in a given environment (which includes our automated tests), we are then able to trigger a build in the next environment, either automatically or manually. Each environment serves a different purpose, and different development teams will create pipelines with any number of environments to meet their needs. What’s important is that the pipeline is set up in such a way that developers can road test their code in a production-like environment, with production-like data, and that new features can be previewed and tested by the Product Owner. The ultimate destination for our code is, of course, the production environment. What Continuous Delivery is, then, is the ability to push that code through the build pipeline all the way to production, with confidence, at a moment’s notice. While the Product Owner may not want those new features to go live just yet, by getting the code successfully all the way through to the staging environment and having both you and the Product Owner review it, you can be sure that when you are given the go ahead, the push to production will be a painless one. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-11-26"},
{"website": "Made-Tech", "title": "The Benefits of Continuous Delivery", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/the-benefits-of-continuous-delivery/", "abstract": "Continuous Delivery is a technique that grew roots in the IT department, but is very much focused on delivering value, in the shape of shipped software, back to the business more frequently and more reliably. We, as most modern software companies do, focus on delivering Software as a Service over the Internet. While we expect that you're able to realise many of these benefits if you're delivering software via another mechanism such as application installs, there may be some areas where your ability to move at an 'as a Service pace' might be reduced. There are a number of reasons we believe organisations who take software delivery seriously should be investing in Continuous Delivery. Faster to market The primary business drivers we see behind Continuous Delivery adoption is that it both facilitates and encourages shorter delivery cycles. Continuous Delivery practitioners must be integrating their work with the rest of the team at least once per day, and ideally, a couple of times an hour. Continuous Delivery can encourage stakeholders across the organisation to start thinking of software delivery as a series of smaller features. The demands of keeping a single software product release-ready without making use of patterns such as feature branching, are made trickier when delivering particularly big changes. By releasing small features more regularly, software and the value that it's intended to deliver gets in to the hands of customers faster. We see that the sooner you can get software to customers, the sooner you can start getting feedback, and the less time you spend working on unnecessary or less valuable features. As more organisations adopt fast release cycles, we're seeing customers expecting this as the norm. Businesses such as Slack, the team communication tool, make much of their \"What's New\" feature, which highlights the changes that have been released in the past day. Improved engineering practices We've observed Continuous Delivery as the catalyst for improving development practices through the delivery cycle, from project inception through to maintenance. The need to keep software release ready encourages, and some corners may argue, demands the adoption of good software engineering practices, such as Test- and Behaviour-Driven Development, Pair Programming , techniques that we believe, over the course of a software project, will increase your ability to move quickly with confidence. When the business demands more regular release cycles, technology teams are incentivised to leverage higher levels of automation. With more regular builds, resource intensive manual QA processes should be replaced in-part (or ideally, in full) with automated UAT suites. Performance testing and the associated provisioning of infrastructure can be automated, similarly with security testing. Reduced risk Big releases containing many changesets, perhaps in extreme cases, the culmination of many months work, offer significant risk. Our experience tells us, the larger the deployment, the more likely something will go wrong. By contrast, we see that releasing little and often keep changesets small, and accordingly, the risk of the change introducing issue is reduced. In addition, by practising deployments more regularly, we become better at them. We feel the pain of bad deployment practices more regularly, we're better incentivised to replace manual steps with automation, and so the risk associated with human error becomes a less likely occurrence, and releases become a less resource intensive practice. Where it allows, our general principal would be not to fear acceptable production bugs to the detriment of moving quickly. Where software is released close to it being built, resolving issues is generally made easier as memory of the implementation is fresh. When coupled with a fast, automated pipeline that allows followup fixes to be released quickly, issues can often be squashed before they're even noticed. Releasing is a business decision We see too often that releasing software updates is a technology-lead decision, rather than a commercial decision. In many organisations, releasing is a process encumbered with risk and involves a highly orchestrated series of manual steps, each of which offers an opportunity for error. A good Continuous Delivery environment replaces these manual steps with automated tooling. By encouraging discipline within the team to ensure software is always in a release-ready state, the decision to release new features and updates can be moved back to where we believe it belongs: with the commercial stakeholders. In particularly mature Continuous Delivery teams, an organisation can go as far as empowering Product Owners to physically push the button to make a release whenever they're happy with the changeset, in a complete absence of operations folk standing by in support. Beyond this, teams can move a step further to practicing Continuous Deployment. In a typical Continuous Delivery setup, full automation usally happens as far as a staging/pre-production environment, with a manual button-push to trigger the production release. Continuous Deployment fully automates this final step, so that a code check-in that passes all previous steps in the pipeline will be automatically deployed to the production environment. Expose inefficiencies We often find the adoption of Continuous Delivery to be a strong tool in exposing inefficiencies through the software delivery process, from insufficient requirement description, to unexplainable change control processes. By tightening up discipline and tooling around the build and deployment process, and particularly by focusing on delivering incremental release-ready features, the true barriers preventing an organisation from moving fast are highlighted with a big red pen. In many cases we see that outdated technology practices can be a strong force in preventing software being delivered in an acceptable or ideal timeframe for an organisation. However, as more businesses realise that they're actually software companies in disguise, there are many factors outside of actually building the software that can hold businesses back. By getting the software delivery process right, an organisation loses the most obvious reason for slow software delivery, allowing it to focus on fixing wider issues. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-12-01"},
{"website": "Made-Tech", "title": "Preparing your team for Continuous Delivery", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/preparing-your-team-for-continuous-delivery/", "abstract": "It can be challenging to get sufficient infrastructure set up to enable you to practice Continuous Delivery , but the biggest challenge may be changing the way you (and your team) think about releasing software. The goal is to evolve software in small increments, not to drastically change it all at once. This has been proven to make releasing software faster, more reliable and easier to debug . Each contribution/commit has to be releasable, no two ways about it. This may sound unreasonably optimistic but it is possible; people are doing it, and you should be doing it. Releasing to production should be boring. So boring in fact that there should be no need to notify anyone that you are doing so. If someone has a commit in the pipeline that should not be deployed to production without further work, then you are not doing Continuous Delivery. If a team member has introduced a bad commit, it is up to that developer to fix it. If this starts to block the deployment pipeline, the commit may be reverted. Developers can be reluctant to adopt such a drastic change of pace, but once you have experienced the benefits you, and your team will never look back. It is vital that every single member in the team be on board with this style of working or it will fail. I personally fell in love with Continuous Delivery even before I knew what the term meant. It happened naturally in an attempt to deal with the anxiety of massive deploys. Below are some guidelines that must be followed by the entire team in order to successfully practice continuous delivery. The art of sneaking it into production undetected Production is the one and only source of truth. Proving that something works in a non-production environment does not necessarily mean that it works. When you work on a feature, aim to get 99% of it deployed into production undetected by the customers, users, and the rest of the code in the system. This is known as dark launching, and you will be doing a lot of this. Sometimes dark launching can be achieved by something as simple as omitting a hyperlink to a page, sometimes it is more complex, but it is never impossible. Feature flags may be used when dark launching becomes too complicated, but should be treated as a last resort. Some of the software giants such as Google and Facebook (to name but a few) have proven that these techniques work. These companies move at a rapid pace, and their developers commit straight into the master branch. Your new feature needs to be designed in such a way that it can be activated through a simple trigger. Decouple it from your system as much as possible. This should be one of your highest priorities in producing modular decoupled code anyway. The last 1% of code that you push up is usually just the trigger that makes an entire feature live to the public. By getting your code into production early, it has proved to some degree that it can co-exist with the rest of the system, and that none of the other existing functionality has regressed. You will find that your definition of done shifts from some pre-production verification to actual production, because it has become so easy. Communicate with commit messages When writing software, having fast descriptive feedback can make all the difference. Continuous Delivery demands that you integrate your changes with the master branch frequently. I see no reason that you cannot commit to the master branch at least once every hour if you intend to keep the code. If a developer has not committed all day, there is no way of telling how their work will fit in with the rest of the system. Frequent commits mean that you have considered the bigger picture and that you have broken this down into smaller, more manageable sub-problems. A positive side-effect of this is that team members get a lot of feedback on the current state of the project. You quickly discover miscommunications or people heading in the wrong direction with their intended solutions. In big teams this becomes vital, since merge conflicts become less frequent and less complicated to solve. Publish your commits to your chat room, or put them up on a big screen for all to see. Good commit messages matter, and your master branch must remain pristine at all times. A good understanding of your version control system is necessary. Confidence obstacles As soon as you decide on a direction to take when solving a problem, you should make sure there are no dead ends down the line. There can be a lot of pressure around making these decisions early on and being confident that they are the correct ones. At times there may be missing or incomplete specifications that will require your system to adapt at a moment's notice. This prevents some people from pushing small commits often, as they're uncertain whether their intended solution is indeed the correct one. Practicing continuous delivery generally encourages you to clarify your intentions before you start writing large amounts code. As soon as the first commit goes in, everyone can see it and it can generate feedback which would benefit the overall project. I have found that this also promotes the quality of code as small chunks are easier to analyse by other team members, and there is no hiding bad code in thousand line diffs. Production awareness When developing new features it is vital that you have considered all the factors for releasing it into production as soon as you start work on it. This may even change the way you approach the problem at hand, which is a good thing as the only end goal is your production system. Sometimes this means pulling down real production data and running your code against it for verification. Beyond that I am going to assume that you have battle tested your feature as much as possible in your staging environment and your automated tests prove, to a certain degree, that it is correct. If you have something like a blue/green or canary release system , it makes it much easier to verify the actual production system by running the actual code. Assuming you do have a zero downtime deployment system, you generally are forced to share resources between the different versions of your application, the obvious one being the database. When we implement new features, we are required to clean up tables and alter schemas. We keep our codebase tidy and do not allow dead code to exist. At times when practicing Continuous Delivery, it makes sense to hold off on certain cleanup tasks until we are absolutely certain that no previous (potentially live) systems depend on this data. You must develop the foresight to never negatively affect your running production system. Big bang deploys are your enemy Big bang deploys mean releasing a large amount of code into production at a time. This usually happens on a Friday afternoon because a deadline has forced you to do this. If it were not for the deadline who knows how much code would have accumulated for the release. It can be complicated, and developers can experience a great deal of anxiety when deploying features to production. Introducing thousands of lines of code into a running system can have unexpected consequences. It's hard to know exactly what went wrong, and it can be difficult to diagnose. Large faulty production deploys are also difficult to remedy as you cannot roll back easily. Small commits are easy to reason about. You get instant feedback on the quality of your small change as soon as it is released. Should something go wrong, you only have a handful of potential suspects to debug. Summary Whenever you write a new piece of code for a feature, consider the earliest point at which it makes sense to commit it in. Even if it's a small schema change, send it up to production and be done with it. Constantly evolve the current system, and don't try and introduce large amounts of new functionality at once. Above all, practising Continuous Delivery means having the discipline to stick to these principles even when it is difficult to do so. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-12-09"},
{"website": "Made-Tech", "title": "Made Named as Leading Enterprise Software Developer in the UK", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/made-named-as-leading-enterprise-software-developer-in-the-uk/", "abstract": "After a packed year working on some fantastic projects, we are proud to announce that we’ve been named as a leading UK software developer for 2015 ! Clutch is a Washington based research firm that specialises in identifying leading services and software partners around the world. They have conducted an evaluation of Made and the services we provide, and found that we are a company with a strong focus on enterprise software, with an ability to deliver on and exceed client expectations. “Developing an enterprise solution requires a huge investment of time and resources and yet there is no guarantee of success,” explained Joshua Margolin, Senior Analyst at Clutch. “That’s why it’s crucial that organizations find partners that can perform exceptionally well under pressure and have a wealth of experience to draw from. The companies showcased in this report fit that bill and come highly recommended.” Through 2015 we've been lucky enough to work with a range of customers on software and IT transformation projects, including CDP , Surfdome , Finery , On , Kjus , SHOWstudio , The Keyholding Company , and most recently Akzonobel . Clutch have been in contact with a number of these customers and published their feedback . Thanks to everyone who helped make 2015 a great year for us all at Made! Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-12-11"},
{"website": "Made-Tech", "title": "Migrations, seeds and pipelines", "author": [" David Winter"], "link": "https://www.madetech.com/blog/migrations-seeds-and-pipelines/", "abstract": "Most applications you write rely on certain data stored in a database that is essential for it to work. That data defines your business logic as much as the code. That data should be represented in code. In this post I'll discuss how we handle migrations and seed data through our Continuous Delivery pipeline at Made. If you're currently practising this correctly, you'd be able to delete your database and migration files, and be in a production-like state in less than a minute with a couple of commands. You don't want to have to manually setup—or try and remember—what data you need so that you're in a position to code if you're starting afresh. You should only need to run rake db:setup or rake db:reset if you've already got a database created. These commands will handle the setting up of the structure (or schema) of your database, and the data that it contains. Schema As you're building your app, you write migrations incrementally as you go along. You run rake db:migrate so that your database structure is altered and for the changes to be reflected in your db/schema.rb file. This is the single source of truth for the current structure of your database. When you or another developer wants to work on the project, you should be able to just run rake db:setup in order for the database to be created, and for the current version of your schema to be setup. There's no need to run the migrations for a fresh project clone. Why run through each migration sequentially when db/schema.rb has the end result? As you continue to develop the project in a team, add more migrations, you'll want to run rake db:migrate to get the latest changes that differ from what you already have. Data Migrations are a means to alter your database schema, but also can potentially be used to alter the data contained within it. You might have used a migration to populate your app data, or have done it manually via an admin interface or the Rails console. But the thing to bear in mind is that your db/schema.rb file only contains your database structure. The data you add to the database isn't persisted anywhere in code. So if you have to reset your database, or someone new works on the project, if they run a rake db:setup they'll get a database with the correct structure and zero data. An example Rails migration: class AddPreferencesToSpreeOrders < ActiveRecord::Migration\n  def change\n    add_column :spree_orders, :preferences, :text\n  end\nend The changes in this file will be reflected in the db/schema.rb file. This is where db/seeds.rb comes in handy. This file is to data what db/schema.rb is for the structure. The difference though is that you have to manage this file yourself. When you create a migration that alters the data somehow, Rails won't be updating the db/seeds.rb file for you. An example Rails seed file: Spree::ReturnAuthorizationReason.create!(name: 'Not satisfied with the item')\nSpree::ReturnAuthorizationReason.create!(name: 'Colour appearance')\nSpree::ReturnAuthorizationReason.create!(name: 'Wrong size') And corresponding migration file to include it: class SeedDefaultReturnAuthorizationReasons < ActiveRecord::Migration\n  def change\n    Spree::ReturnAuthorizationReason.delete_all\n    require_relative('../seeds/return_authorization_reasons')\n  end\nend Anything altered in the database as a result of this migration will not be reflected in the db/schema.rb file. When you run rake db:setup or rake db:reset Rails will create/empty your database, load your current schema, and then run the db/seeds.rb file to populate the database. By mirroring the data your app requires in code in the db/seeds.rb file, you can get yourself or others up to speed much quicker. That may be a set of categories, tags or users, for example. Breaking down your seed data into multiple files helps you organise it into logical pieces. require_relative is your friend to load in additional files. Setting up a bunch of users? Throw that all in a db/seeds/users.rb file. Then require_relative('seeds/users.rb') in your db/seeds.rb file. Up the pipeline This isn't to say that you should stop writing migrations and use seeds solely as a means to get your data into your database. It's not practical to reset your database everytime, nor can you expect other developers on your team to do this. You'll want to get the data into your local environment via a migration as you want to be sure it works fine for others. Also, because you probably have a Continuous Delivery pipeline (right?) where you need to get seed data up to your production environment, it's completely unrealistic to reset the production database. Migrations are the only solution. This is why it's important that the data in your migrations and seed data mirror one another so that if someone were to do a db:reset they'd have that same data as if it were added via a migration. Graveyarding Once your migrations have been pushed through your pipeline, and your db/seeds.rb file is current, it's safe to delete your old migrations. Though I'd recommend keeping the last 5 just in case you're developing, trying something out, and may need to revert back. Summary Discipline is essential. And the best way to make sure you're disciplined is to not be afraid to run a rake db:reset every so often. If there's something you find yourself manually adding to the data, it probably belongs in a seed. If you're ever tempted to add something to the db that is essential and should be in code, then: Write a migration with the change Ensure that the data the migration is changing is reflected in db/seeds.rb so that if you rake db:reset you'd have the same state of data Push the migration through your pipeline Remove all but the last 5 migrations in your project to keep things clean Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-12-10"},
{"website": "Made-Tech", "title": "Continuous Delivery: Feature Toggles", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/continuous-delivery-feature-toggles/", "abstract": "When practising Continuous Delivery, it's important that your application is deployable at all times. This can introduce some challenges, especially when you have features that span multiple builds, or bug fixes that need to get into production quickly. In this article, we're going to look at some of the techniques that we use to keep our applications deployable. We'll look at why branches (particularly long-lived ones) can be the kiss-of-death for a continuous integration project, why techniques like feature toggles are useful but should be used with caution, and why you should focus on actually delivering small incremental changes to production. Decoupling deployment and release I'm sure we've all run into this problem before: You've got a set of changes that have been in development for weeks. For whatever reason, they cannot be deployed through to production. Suddenly there is a small bug fix that is urgently required and which needs to be deployed right away. What do you do? It is time to start looking at ways in which you can decouple the deployment and release of your code changes. If you had used feature toggles and were able to enable/disable features, you could disable the offending feature in production and deploy your bug fix through your pipeline. Without this, you're probably going to need to perform a revert, implement the bug fix and then deploy this through your pipeline, before re-applying the bug fix to master. This is risky, potentially complex, and the sort of thing you really should be trying to avoid. Prevent long-lived branches In this scenario, some people would advocate developing the feature in its own branch and merging this back into master once the feature is complete. We have found this creates an additional set of challenges and goes against many of the principles of Continuous Integration (where you should be integrating into a master branch multiple times per day). Branches create merge nightmares If you've ever tried to merge a long-lived branch, then you probably know how difficult complex merges can become. You may have experienced that sinking feeling when confronted with hundreds of merge conflicts needing to be addressed. The time and cognitive effort required to handle the merge, plus the risk that some important changes might get missed, makes manual merges something we think should be avoided. Feature branches and pull requests The one type of branch that we would use is a very short-lived feature branch, as part of a pull request type workflow. My colleague Fareed has written an article about this and how we use these , and explains some of the techniques that we try to use to avoid merge difficulties. Feature Toggles The basic idea behind them is that you have a configuration system that allows you to toggle features on or off depending on some factors, such as environment, user profile or market. The running application then uses these toggles to decide whether or not to show the feature. The actual implementation can take many forms. At its simplest, you will have a static configuration file within your application, which you deploy to production for the toggled changes to take effect. Some companies use more complex feature toggling systems, to allow runtime feature toggling via web interfaces or for toggling features within a canary build or quality assurance environment. There are obviously a number of benefits to feature toggles, such as the ability to gradually roll out new features to users, proper decoupling of the deployment and release of code, and the huge benefits around keeping your continuous delivery process working. Branching in code vs. branching in source control One of the big criticisms of feature toggles is that they can introduce unnecessary code branching into your application. You are effectively moving the branches a level down, so out of version control and into your application.You're effectively putting features into conditionals and therefore increasing the cyclomatic complexity of your application. This code branching needs to be managed or you are fairly quickly going to start accruing technical debt. Jim Bird wrote an interesting article on this and described some techniques that he recommends using to prevent the accrual of too much debt in your application. As a general rule, the quicker you can remove a feature toggle, the better. Toggles should be a short-term solution and not code that remains within your application for years to come. Added test complexity Another area in which you need to consider the impact of feature toggles, is within your test suite. There is a significant overhead to testing all toggle combinations, and frankly, it can be difficult to justify the impact this has on your test suite performance. Martin Fowler recommends having tests that cover: All the toggles on that are expected to be on in the next release All toggles on We believe this is a sensible default, but it is also important that you consider the best approach for your application. If you are dealing in high-stakes software, like the team at Knight Capital Group then it may be worth accepting this performance hit. Small incremental releases Feature toggles are great, but before introducing them, we believe you should first try and break features down into small incremental chunks, which are releasable. By doing this, you're minimising risk and more quickly realising value from your development effort. Conclusion As a developer, it's your responsibility to manage application complexity. We would recommend you try and adopt a 'small and incremental' mindset as your default course-of-action, as this has the lowest complexity overhead. Only when this becomes impossible to achieve should you look towards feature toggles (or any other technique), as they add complexity and ideally you want to minimise this as much as possible. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-12-16"},
{"website": "Made-Tech", "title": "Continuous Delivery: Building a Pipeline", "author": [" David Winter"], "link": "https://www.madetech.com/blog/continuous-delivery-building-a-pipeline/", "abstract": "A pipeline is a set of steps that your code takes to get from a developer's local machine through to a production environment. This pipeline is managed by a tool that lets you define these steps, what they do, and how and when it proceeds onto the next one. The pipeline is the tool used to deliver Continuous Delivery. The steps in the pipeline you create is completely dependent on the application you want to deliver, and any processes you might have to get it verified into a production state. Common pipeline steps Most commonly, the first step you'd have in the pipeline is one to collect all code that your team of developers are working on and to run automated tests. This practice is known as Continous Integration. All developers are pushing into a central code repository and comitting early and often. The pipeline will then run your automated test suites to make sure new code doesn't break. If any tests fail, then that code goes no further through the pipeline until it's fixed. Once you have a good build, steps that follow in the pipeline can involve deployments to various staging environments where your team, and perhaps your product owner can preview the code changes. At the end of the pipeline is always your end product. Either a production deploy, or release of your application that is ready to be released to the end audience. For the purpose of this article, we'll use an example pipeline structure based on a web application, consisting of the following steps: Build Continous Staging Production Production flip (for Blue/Green deployments) Lets now talk about how we progress a new set of code changes through a pipeline. Triggers Each step in your pipeline needs to be initiated by some means. These can be manual or automatic triggers. New builds through your pipeline should be triggered automatically when new code is pushed into your central code repository. This involves either periodic polling of the repository by your tool, or push notifications from the repository, either of which will trigger a new build. This gets the ball rolling. Proceeding steps are then triggered automatically on the success of the current step, and you continue to trigger steps in this manner, until your processes require you to have human verification of your application before it progresses through the pipeline. If your pipeline consists of nothing but automated steps, then you're also practising Continuous Deployment. However, you might require a product owner to sign off on new changes before they are progressed, or released. So a manual trigger is required for the pipeline to progress past a point, and these are normally a human clicking a button from within your tool of choice. Once clicked, the pipeline continues as normal until the next manual step (if any). Repository representation of the pipeline With branches and tags in your source code control, you can easily mirror the state of your pipeline in your repository. Each of your pipeline steps can be configured to push the code into a branch of a matching name if it succeeds. Each step after will then pull from that branch. That branch represents the state of the pipeline after each has successfully completed. When a build is triggered for a pipeline it's given a number. Making use of this number and a repository tag will give a good historical reference and make browsing different releases easier in tools such as GitHub. Or if you have a bug for a previous release, you can easily checkout based on a tag name (or branch) rather than trying to find a specific commit based on a date. Build step commands Besides a step leading to another step, how do we get them to actually do something? Such as running tests, or deploying code? All tools come with a mechanism to execute an action or actions for a particular step. Depending on which you're using, it might be an integration with another service online, or one that allows you to specify custom commands. These give you the most flexibiltiy if your needs are more complex. You could list the various commands that your step requires to complete, one after the other in the pipeline tool configuration. However, it's good practice to use a shell script as that allows you to group those multiple commands into one, and to keep the script in your repository. Be descriptive with the names of your shell scripts. A step that needs to run tests? Why not have a script called pipeline/tests. Performance? pipeline/performance. Your pipeline tool then only needs to know of one command to run per step, and you've then got all the benefits of source code control for your pipeline logic, with the added benefit of being able to run these locally too. It's worth noting that there might be limitations with what you execute at each step. If you have a self hosted tool (for example, Jenkins), then anything your scripts are running, you need to ensure those dependencies are on the system that Jenkins is running on. For example, if you're using rspec for tests, your system would need Ruby, Rubygems and rspec installed. If you're using a hosted tool, then build steps are usually run within virtual containers, and often the services allow you to install any of your step dependencies before they are run, or they have common virtual containers already that you can specify. For example, there might be a Ruby container that would already have most common dependencies in place. Breaking down our pipeline Lets go into a bit more detail with our example pipeline. For each of the steps, we'll talk about the trigger, how it's linked to our repository, what it executes and what step happens afterwards. Build Purpose: To ensure the latest code passes our tests and code standards. Trigger: Automatic, by someone committing to the master branch of our repository. Pulls from branch: master Executes: pipeline/tests, pipeline/code-standards Success push branch: build Success trigger: Trigger the continuous step to run automatically. Continuous Purpose: To deploy the code to a staging area for the development team to preview and test. This allows us to check that all the code our developers have integrated together works as expected. Trigger: Automatic, if the previous build step succeeded. Pulls from branch: build Executes: pipeline/deploy/continuous Success push branch: continuous Success trigger: Create a manual trigger for the staging step as the next in the pipeline. Staging Purpose: To deploy code to a staging area for a client to preview. Trigger: Manual, if the development team are happy with the continuous deploy. Pulls from branch: continuous Executes: pipeline/deploy/staging Success push branch: staging Success trigger: Create a manual trigger for the production step as the next in the pipeline. Production Purpose: To deploy to the production environment when the client is satisfied with the staging deploy. Trigger: Manual, if the client is happy with the staging deploy. Pulls from branch: staging Executes: pipeline/deploy/production Success push branch: production Success trigger: Create a manual trigger for the production-flip step as the next in the pipeline. Production flip Purpose: After a final verification of the production in a production environment, flip live traffic to the new release of the application. Trigger: Manual, from the production deploy. Pulls from branch: production Executes: pipeline/deploy/production-flip Success push branch: n/a Success trigger: n/a – you've reached the end of the pipeline! The pipeline and your team Even with an automated pipeline, it's important to retain communication within your team throughout the development of features. You're more than likely using a chat application, so setting up integrations with your pipeline to notify users when a new build has passed or failed, is a great aid. Before deploying between your various environments, it's always a good idea to communicate with the team to see if that's ok. For example, if you have clients using your staging environment to preview changes before they go to production, you might want to check these yourselves on the continuous environment before deploying. You may be happy with your own work, but your colleagues might want some time to check over their work before it continues up. Most pipeline tools have good visibility on what changes triggered builds, and with manual steps, who triggered the build. This isn't to associate blame in the event something goes wrong, but it brings ownership to a deploy by having this visibility. Variations Even though the example we've used in this article is linear, not all pipelines need to be. A pipeline step could trigger multiple other steps allowing for different tasks to run in parallel. Unit tests, code standard checks and performance metrics are all things that could be run independently of one another. The benefit of running tasks in parallel is a reduced feedback loop. This is the time it takes to know whether a pipeline build can continue or whether it fails. The quicker you have the result of a build failing, the quicker a resolution can be made. Other variations to pipelines could include post-deploy tasks that run smoke tests on a deploy to ensure it was successful, or to run security checks. These would save the need for you having to do this manually, saving you time and letting you know sooner when things aren't as they should be. Improvements to your pipeline Don't let your pipeline remain static and fixed once it has been setup. Be on the look out for tasks that you or your team perform manually that could be included in the pipeline process to automate instead, because including that in one of your build steps might take an hour or less, but think of the time that will save in the long run. Each time you manually perform that task, multiply it by the average number of builds you have each month, and that'll be your saving over the same period. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-01-05"},
{"website": "Made-Tech", "title": "Continuous Delivery: Tools", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/continuous-delivery-tools/", "abstract": "Finding the right platform to form the basis of your Continuous Delivery is key, and you really need a solution that is going to fit into your existing way of working with minimal effort. In this article, we'll discuss the key features which are fundamental to running a successful Continuous Delivery pipeline , and some of the open source, self-hosted and Software as a Service (SaaS) solutions on the market. Selecting your solution When choosing your solution, be it SaaS or self-hosted, you need to have confidence that it will offer you and your team a pain free Continuous Delivery work flow. Factors that can influence this decision will range from the amount of work needed for initial setup, to the time it will take to undertake any ongoing maintenance, and finally how easily it integrates with your PaaS or server infrastructure. Whilst it's great to shop about you really want to get it right first time, as you don't want to invest project time on an \"Ok\" solution that you will have to migrate away from 6 months down the line. Essential Features Whilst every SaaS or self-hosted solution on the market boasts a myriad of different feature sets, there are a handful of core features that are essential to operating a successful pipeline. Integration with version control Version control integration is the most crucial feature your chosen solution should possess. When I say integration, I mean it should poll your repository, or use web hooks to detect changes, which should then initiate a new build and subsequently trigger the rest of your CD pipeline process. Custom script execution Custom script execution within a pipeline step is a key feature, especially if you deliver a diverse number of projects. Thanks to buildpacks , many common deployments are very simple, but often the need arises to run custom deploy scripts. In these instances the ability to use tools like Capistrano to execute deployment tasks is vital. While it is written in Ruby it can be used to deploy almost any project, thanks to the open source community behind it. A Pipeline A pipeline view is a visual representation of all of your deployment steps. These steps should all be linear. A single step, e.g. unit testing, could fan out to run multiple tests in parallel. Then any dependent step should be executed automatically once the previous steps have passed. The final step in the process should be a manual one, in order for one of the team to make certain that the build is in a good state before releasing to any publicly facing environment. Notifications As a software engineer, switching contexts can be really distracting so your Continuous Delivery solution should offer an alerting system to inform your team to any successful or failed deploys without them needing to check a web interface. In a basic form, this can manifest itself as an email, however many Continuous Delivery platforms will also integrate with the popular team chat programs, like HipChat and Slack. Self Hosted vs SaaS The argument for self-hosted or SaaS solutions will not be resolved in this article, as both have their flaws and their advantages. A self-hosted solution for example will require a lot more initial set up than a SaaS solution, as most of these are one click installs, and simple to configure using a YAML file (or similar). However ,the up front investment in infrastructure, and other set up required with a self-hosted solution, will be netted out over a number of projects, and will allow you flexibility in the long term as you are free to add features provided from community plugins and extensions. This is flexibility you just don't get with a SaaS solution, where you are tied to their feature set and development cycle of adding new features. Having discussed the recommended features (and pitfalls) of both options, below is a selection of solutions that provide said features, which the market currently offers: Self Hosted Jenkins CI Whilst technically a Continuous Integration platform, with the addition of the build pipeline plugin it is easily configured to be a complete Continuous Integration and Continuous Delivery solution. Go Go is a Continuous Delivery platform written and maintained by the folks at ThoughtWorks, who literally wrote the book on Continuous Delivery, so as you can imagine Go is based on a lot of the principles outlined in it. Like Jenkins (with the build pipeline plugins) it will perform both Continuous Integration and Continuous Delivery. Spinnaker The newest option in the list – Spinnaker – differs from the previous two as it is only a Continuous Delivery platform. Spinnaker will take a deployable asset (e.g. a Docker image) and deploy it out though the pipeline steps, but only once it has received trigger from a separate CI platform like Jenkins. SaaS Cloudbees Cloudbees is Jenkins in the cloud. Cloudbees uses a Workflow plugin – which you could implement on your self-hosted Jenkins instance – to add Continuous Delivery functionality. So if you like your self-hosted Jenkins but no longer want to maintain the infrastructure, then this could be an option for you. Snap CI Snap like many SaaS offerings is tied to GitHub (at time of writing). Snap enables you to build both simple linear pipelines, and advanced branched pipelines from either a single, or multiple repositories. Harrow IO Harrow IO is a SaaS solution from the folks that maintain Capistrano. Whilst you can use any script to run your integration and delivery steps, Harrow provides simple integration if you are already using Capistrano scripts to execute deployments. Summary I don't think its possible to ever be definitive and say this is the platform you should be using, or that is the correct solution for you to choose. Different teams will have different requirements for their Continuous Delivery tooling. However if you are maintaining a single product, a SaaS solution will most likely be your best bet, as you wont have to worry about the additional infrastructure. On the flip side of the coin, if you deliver a large number of client projects, a self-hosted solution that can be tailored to your needs will most likely be a better fit. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-01-07"},
{"website": "Made-Tech", "title": "Continuous Delivery: Keeping Quality High", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/continuous-delivery-keeping-quality-high/", "abstract": "Practicing Continuous Delivery is worthless if it's not to facilitate the delivery of high-quality code. In this article I am going to cover some techniques, tools and best practices we employ at Made Tech to keep our pipelines moving, and how you can compel developers to push quality code often by rewarding them for attention to detail, rather than punishing them for making mistakes. One change at a time The most typical development process is making a change and testing it, making another and testing again. Even the most complex features of an application grow from simple code. Employing this technique of incremental development is rewarded in many areas of software development. Best version control practices typically involve breaking large changesets into a series of small updates. These atomic, self contained commits are readable by themselves, and thus easily understood, so shared understanding of a project doesn't get lost in a slew of massive isolated updates. Like a grand painting, complex mechanics and fine details emerge from small additions and deletions. A 1000 line changeset is difficult to fully comprehend and give feedback on, whereas a few lines of good code rarely is. Large changes result in less shared understanding of a feature, and a lower chance of being improved. Small changes are easy to store in your head and thinking of ways to improve them is simple. If people can easily understand other people's changes, they are more likely to make suggestions. If they can't, they'll leave it and assume it works fine. Small changes are largely beneficial in continuous delivery. If you push one small commit to master and the build breaks, it's easy to work out where the bug was introduced. Better yet, if your change is just a few lines of code, it will likely be trivial to work out which bit went wrong. Long lived branches with large changes, on the other hand, can easily become out of sync with master, creating conflicts which can lead to hairy merges. Continuously pushing to master means your new code doesn't lose the context of the project as a whole. It's embedded in the branch that's going to production, the more regularly the better. Keeping it together with tests You've made a change in one part of your application to fix a bug, and it's caused a bug somewhere else to spring up. Maybe an old bug you thought you'd squashed, or something entirely new. You've probably experienced this before, this is very common when developing complex applications, and can unfortunately be easily missed. Without a robust test suite, including unit tests which ensure your business logic is sound and feature tests which ensures the user can actually use every part of it, mistakes like these can easily be missed, and buggy code can leak out to production. At Made we believe testing should come early and often. Not only do we normally employ TDD, meaning continuously testing throughout development, but we run tests in the 'build' step of our pipelines, meaning it has to be run before we can deploy any code. If tests fail, the code can't be deployed. The primary benefit of this when practicing continuous delivery, and small atomic commits, is that problems are discovered very quickly. Having a robust test suite which ensures the reliability of your product after every change means you can develop without fear of breaking anything. Running tests after 40-50 commits and having them fail may give you hundreds of lines of code to parse through to find where the bug was introduced. If every commit to master is tested, then bugs introduced in a commit of a few lines can be found quickly. Better yet, fixing them is cheaper early in development than it is when the feature has reached production, and quicker fixes means more time for the fun parts of software development. Testing so frequently has its consequences, however. Test suites are slow. As projects grow, you can see your test step grow from seconds, to minutes, even to hours on very large projects, which is clearly a frustrating impediment to shipping continuously. You want the time between development and production to be as short as possible. There is no canonical 'fix' for this, for many it is what it is. Some developers, however, advocate only testing new code and crucial areas of your codebase in build steps, for example the checkout process of an e-commerce store. This means bugs can still appear and get missed, but it also dramatically reduces the amount of code which needs to be tested, speeding up your build step and reducing your time to production. Pushing frequently can alleviate some of the pain of all this, however. You can push code and continue working, and only stop when you notice the build failing. The tests can just run in the background, frequently enough to alert you when something goes wrong, but not demanding so much of your attention that you can't get other work done asynchronously. Stay beautiful The easiest projects you've ever worked on are the projects which had consistent, readable code. Any developer can work faster by writing ugly code, and still have it work beautifully and efficiently, but as soon as somebody else comes along the work suffers. The pace is slowed as they come to grips with it. Finding a bug, or the right places to implement something new, becomes much more time consuming. Maintaining readability of a project is paramount because projects which are easily understood are easy to work on. Developers can become familiar quickly and ship faster. An app's source code is a manual for future developers, the code you write will become the standard you and others will follow when working on that project – and even future projects – so it behooves you to follow best practices at all times. So if we use automated tools to codify conventions we like, and test every revision of a project against them, sloppy work is more difficult to get through the pipeline. At Made, we use a number of tools on every 'build' step of our deployment pipelines to ensure the code passing through is of a high quality. For example, as one of the first steps of a Ruby project's build step, we run Tailor and Cane , which are static analysis tools that review the style of every Ruby file in the codebase against a set of rules we define. Tailor checks the cosmetics. Is the whitespace correct? Are method names all in lower-snakecase? It checks that the code in our project matches our own guidelines. Cane checks the complexity of code. For example, by using an \"ABC\" metric which analyses how much a single method is trying to achieve, we can ensure the methods we're writing and using make sense by themselves and follow DRY conventions. In Javascript we use ESLint , a newcomer to the linting world. Like Tailor, it checks our Javascript code against defined rules, and supports languages which transpile to Javascript, for example React's JSX syntax. Fixing these syntax issues and accidental complexities as they come up is really helpful. Even if you're just pushing a hotfix which is a few lines long, ignoring the quality of that code to speed up development would allow the codebase quality as a whole to suffer. Your code will crumble brick by brick unless diligently maintained. Preventing bad code from entering production forces the codebase to maintain a good shape, and although this may slow developers down in the short term it improves productivity in the long term. Brakeman , a Ruby 'vulnerability scanner', is the fourth tool in our static analysis arsenal. We use it in our builds to check for potential exploits our applications may be open to, so we can cut them off before they reach the wild. Although a tool which relies solely on static analysis and generic assumptions can't catch every possible point of attack, knowing that we're safe from common or simple exploits after a short build time is good for peace of mind. See our post on code quality for more on tools you can use to keep your codebase happy. Fail like a pro \"Breaking the build\" is often dreaded, but if we consider all of the ideas discussed in this article to be valuable, then actually preventing code which is broken, unsafe, or just plain ugly from getting into production is a very good thing. Fixing problems quickly as they come up actually speeds up development as a whole and ultimately encourages developers to get it right the first time. To take their time in developing new features and make sure they're as good as they can be when they're pushed. By developing with this framework around your deployment process, there is little fear of accidentally pushing code which could harm the customers experience, the experience of future developers, or our own experience working on a codebase. Code which isn't good enough gets rejected, and you fix it. This has a positive side effect of preventing other developers from stacking code upon broken code. The team is all aware that some part of the codebase is currently unreliable and leaves it up to the original developer to sort out, before making changes to the same things. Be happy Ultimately, many of these tools and use cases focus on ways to get developers working well together, following agreed best practices and conventions, taking time on their work and continuously receiving feedback on what they've achieved. This attention to detail makes us proud of our work. This process encourages developers to shepherd their changes to production, rather than throwing them in the wind. Giving responsibility to developers makes them care about the quality of the code. They care about helping the customer, they want to see it being used and appreciated. Furthermore, by deploying regularly while keeping quality high, any small fixes needed once a feature hits production can be picked up by the developer best suited for the job. If they have been waiting days or weeks for a deployment, they may no longer be able to get around to fixing small issues quite as quickly. A back and forth discourse emerges whereby changes can be requested and implemented quickly. Tests, QA, feedback and linters all on their surface slow down development. But in the long run, they provide a safety net for creativity and experimentation, which results in better, more stable code and happier developers and customers. They are an important part of the virtuous cycle continuous delivery tries to achieve. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-01-12"},
{"website": "Made-Tech", "title": "Continuous Delivery: Keeping A Clear Path To Production", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/continuous-delivery-keeping-a-clear-path-to-production/", "abstract": "Note: Article edited on the 4/12/2018 In order to enjoy the benefits of continuous delivery it is imperative to be able to deploy all the way to production at any time. Anything that prevents a deploy to production is a blocker, and keeping that pathway clear is the hardest part of implementing and maintaining a continuous delivery of software. If you have been doing CI for a while, are not having success yet and are looking to improve, this article is for you! We are sharing the lessons we learnt from the mistakes we made first implementing this, many moons ago. Broken builds The first step of a Continuous Delivery pipeline is the build step. A typical build step will run your tests, check the style of your code, point out overly complex methods and more. This means your build step has a lot of ways to fail. This is A Good Thing because it catches issues early, but it also means it’s a prime candidate for blocker status. The solution to broken builds is obvious: fix them and fix them fast. We have a rule at Made Tech that you must own your build failure. As soon as the build breaks you tell your colleagues in Slack that you are looking into it. You drop whatever you are doing and try to fix it immediately. If you cannot fix the problem quickly you back your changes out with a git revert, or something similar. Preventive measures can be taken to reduce the likelihood of broken builds, such as your developers running the build script locally. We usually have a rake task that our build server runs so this can be done easily. Parity between environments is also important. Running your tests locally and seeing them pass to then see them fail when they run on the build server is never fun. We have found bugs can arise from using different databases in development from test, for example running SQLite in test but MySQL everywhere else. Where possible use the same database management system everywhere. Same goes for asset storage, if you use Amazon S3 for file storage in production you should use it in development and test too. On flickering builds In the past we have suffered from flickering tests, tests which sometimes pass and sometimes fail. Often, the cause will be a test that sets up state and does not clear it afterwards, therefore affecting the result of a subsequent test run. Another cause we’ve found are tests that run in headless browsers like Selenium, PhantomJS and the like. Sometimes expectations on page content will fail because an asynchronous request has not completed yet and therefore hasn’t changed the page content as expected yet. The nature of flickering tests is corrosive to Continuous Delivery discipline. You will often find engineers just rerun tests several times in the hope they will pass, which obviously affects the clear path to production. Also, if your build could fail at any time, and if your build time is upwards of more than a few minutes (another problem for another time), a flickering test can really slow you down. The cure to flickering tests is to either resolve their flickering nature as soon as you notice them or delete them. Removing the test is better than having it flicker due to its ability to disparage your engineers. Work In Progress We encourage our engineers to push early and often. This means committing small chunks of work and pushing them to your project’s master branch several times a day. We encourage this to avoid merge hell and also to encourage limited work in progress. The problem with your engineers pushing code is that the tests may pass but the feature is not necessarily complete enough to be released to users. Incomplete work in your master branch that would affect a user is a blocker. We practice dark launching of code regularly to circumvent this. Dark launching simply means the code is able to be deployed out to production but it is never accessed by users. The simplest way to dark launch a new feature is to release it under a new URL and not link to the URL from anywhere. This might seem crude, but it has worked very well for us. Feature toggles are a more advanced way of dark launching. Simply wrap up your feature in a toggle and ensure it is disabled in production. We tend to use feature toggles if a feature is a change to an existing feature or is provided by a URL already accessible to users. Unreviewed work We have found that sometimes features can sit complete on staging, undeployable to production since the feature requires verification from a Customer, Product Owner or a colleague that simply hasn’t had time. The external dependency on a Customer can really block your pipeline. If you have limited reviews by Customers before deployment to production you are in luck. In these situations, we encourage engineers to not move onto another task until the one they have just completed is reviewed. In fact, we consider it incomplete until it’s reviewed and deployed to production. This will lead the engineer to get up and get a colleague to review their work as soon as possible. If the feature must be checked by a Customer then the solution is more nuanced. We try and send features over to clients via instant messaging as soon as they are ready to be reviewed. This is similar to our internal approach above. Failing that, if we need to deploy a hotfix to production we will back the changes out with git revert and deploy our changes up. This is cheeky, we know, but it often encourages you to change your policy of review or open up better lines of communication with your clients. Dependency on other releases We work jointly with other teams to deliver software on a regular basis. We try and integrate into a single team where possible, to work from the same codebase. However we have been in situations where the other team is delivering an API that our team will have to interact with. Often we know the spec of the API before it is built so we can mock it out. Problems arise when a change to an API occurs and we update our code, but the API side has not implemented the change yet. A deployment to production would mean miscommunication with the API. This is a blocker. Another variation of this is migrations not being run at the same time of deployment. If you push your code and it tries interacting with fields that do not exist yet, you’re going to have a bad time. Another blocker. The first solution we jump to is hacking the setup. We try and get both our team to deliver the API in the same sprint and codebase as us, so we don’t deploy them separately. This way we can always keep in sync. We would always recommend running migrations on deployment and not having a siloed database administrator ( DBA ) run them instead. You can often convince the Ops Person or DBA to allow you to run migrations if you promise (and prove you are able) to only commit non-destructive migrations or get them to lock down DB permissions so you cannot drop tables, remove columns, etc. Alternatively, if we cannot deliver as a single team, we get smart. Getting smart means using if statements. If this API call fails, fall back to using the older API call. If this field doesn’t exist in the database, don’t render it. This can get messy and should be cleaned up as soon as you can but it does unblock you. Conclusion The success of your Continuous Delivery practice depends on your ability to react and unblock your pipeline whenever issues arise. Broken builds occur and that’s okay, your team needs to be drilled to respond and fix them as soon as possible. Work in progress and work ready for review is part of the software delivery process, but must be dealt with sensibly. Dependency on other releases should be avoided where possible but you can get smart too. Ultimately, if you develop a culture of responsibility for keeping the pathway clear, then you will truly start to benefit from Continuous Delivery. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-01-14"},
{"website": "Made-Tech", "title": "Continuous Delivery: Organisational Challenges", "author": [" Nick Wood"], "link": "https://www.madetech.com/blog/continuous-delivery-organisational-challenges/", "abstract": "Adopting continuous delivery for a single team is tough, adopting it across a whole organisation exponentially more so. It's hard to catalogue all the issues a business may face during a transition, but in this post I'll discuss the common pitfalls. Giving teams ownership In a traditional software setup, with separate teams (product, architecture, development, QA, Ops, to name a few) a move to Continuous Delivery is likely impossible unless you have exceptional levels of communication between all teams. When each team is only responsible for their own output, without strong discipline it's hard to stave off a culture of throwing things over the wall, to the next team in line. You feature pipeline might look something like this – it's ominously Waterfall: On close inspection it's apparent that this pipeline isn't very fault tolerant. A failure in any single team prevents anyone else from doing their job! The other challenge is how to ensure that each of the five teams are working at the exact same rate. If the Product Team are defining features faster than they can be architected, then we have a bottleneck. Items are sitting idle in the Architecture queue, waiting for someone to pick them up. Over time this leads to a massive Architecture backlog which becomes impossible to prioritise, and business goals may have changed by the time Architecture get around to tackling them. The opposite problem is equally likely, and is probably happening simultaneously elsewhere in your process. Let's imagine the development team can build features faster than the architects can specify them, and also faster than QA can test them. There's now a very real chance that your developers will run out of stuff to do, so those bums on seats are costing you money and you're getting no return on that investment. This is not to say that Architecture is always the problem, in practice your bottleneck could be anywhere. In the example described above your pipeline looks like this. With the width of the pipe in some sense indicating the rate of work: In such a system the flow of the whole is ultimately determined by the flow of the slowest team. Any team capacity above this is essentially wasted. As a stop gap you could expand your slower team to improve throughput, but ultimately all you will do is move the bottleneck elsewhere. Bottlenecks are a disfunction and they'll lead to frustration of the team, and a culture of blaming others when they inevitably slow the system down. Fixing the problem needs a complete rework of the plumbing. Rather than each team being responsible for their part of the pipe, the team as a whole need to be responsible for the entire throughput of the system, this can only happen when the team is equally responsible for the whole: Any team, regardless of scope, will have its inputs and outputs; the adoption of Continuous Delivery is fundamentally about teaching teams to group up and share their dependencies, rather than being co-dependent on each other. Such a transformation won't happen overnight though. Adopting this change in thinking can usually be accomplished in several discrete steps. You might start for example by joining the Dev & Test functions, by having testers and developers work closely with Architecture to define technical requirements. Testers can then write their tests whilst the developers write their code, and as they are both working on the same thing it's much easier for them to talk through issues to reach a common understanding. The team should also share ownership of their output stream, ensuring that nothing goes to the Ops team until developers and testers alike are both happy. No more arguments along the lines of \"testing should have caught the bug!\" – the team either succeed together, or fail together. Ensuring everyone is utilised effectively will likely require some element of cross skilling, ensuring that your devs know how to test code, and testers understand the system architecture; valuable side effects of the transition. That's not to say that there's no room for specialisation though – our goal isn't to have everyone equally skilled in all areas, just to ensure that everyone understands and is responsible for how the whole team works. Measuring success A key difficulty with Continuous Delivery, is changing how to measure success/progress. Many Agile methodologies start to fall down when faced with genuine Continuous Delivery. Take Scrum as one example, what value does a Sprint Demo have in such an environment? In a two week sprint we'll have close to a dozen features in production by the end, so is there value sitting stakeholders down and running through features they've likely already seen? How does measuring a team's velocity over two weeks help us figure out where bottlenecks are so that we can improve? It's likely in such an environment that average cycle time, i.e. how long it takes to go from inception to production, will be a much more meaningful metric. Delivering value to customers is the ultimate goal of any software project, though actually measuring this is generally quite difficult with a large change set. Once you are delivering atomic units of value in a Continuous Delivery fashion, it becomes much easier to measure this. By tracking key performance metrics from build to build you can easily see which builds had the biggest impact on value (conversion rate, or average session time being two common examples). Combine this with cycle time and you have two very powerful tools for determining the most valuable work for the team to focus on next. Setting deadlines Broadly speaking, there are two kinds of deadlines: hard, and soft. I would define them as follows: Hard deadlines: a fixed date in the calendar by which something must be delivered, and failure to do so will result in significant business harm, such as going bust or suffering severe reputational damage. Soft deadlines: everything else. Most deadlines are soft deadlines, but a common disfunction is for soft deadlines to be treated as hard ones. How your business treats deadlines is a useful indicator of how successfully they might adopt continuous delivery. The following are all smells which are going to make CD difficult: Soft deadlines frequently masquerading as hard deadlines: \"We promised customer X this feature by March\" is a soft deadline, not a hard one. Teams have no control over their soft deadlines: Customers are promised feature by a given date with no involvement of the team. Teams are punished for failing to meet soft deadlines In an ideal world there are no hard deadlines, and the team are able to set their own deadlines. Reaching this ideal is a tough slog though, and requires lengthy periods of building trust between engineers and business people. Developers need to trust the business to communicate estimates, as estimates , to stakeholders, and not use them as tools to make teams to work extra hours. Similarly, sales people need to trust developers to set realistic expectations of what they can deliver, and also to meet the majority of the deadlines they set. Managing technical debt It's relatively uncommon for the concepts of technical debt and code health/quality to be well understood outside of your immediate tech team, which is a shame, as these have a massive impact on your teams ability to perform or, as we defined it above, cycle time. Exposing these metrics as loudly as you can is a great way to encourage the business to take more of an interest in them, and to prioritise improving them. It's much easier to make the business care about these metrics when faced with concrete data such as: \"We spent a week on increasing test coverage in this section of our application by 50%, and sped up build times by 15% – this has removed a day from our mean cycle time for this component meaning we can get new features in front of customers 10-20% faster.\" The reverse is also true: \"Test coverage has dropped this month 20% and as a result we've seen more build failures and an increased cycle time\" is a great way to make financially incentivised people care about your code health, and give teams space to take remedial action. That's not to say our aim is to remove technical debt completely; consider treating technical debt like a financial one. Having lots of debt is probably a bad thing, but if you have a good rate of interest, debt becomes manageable, and in some cases even desired (lots of companies run at a deficit, and most homeowners have a mortgage for example). Paying off the entire debt in one go is probably not practical, but in order to get the debt under control it's important to keep up with interest (make sure you are not making your technical debt worse) and make regular down payments (take active steps towards improving the state of your codebase). Technical debt is hard to measure, but using the metaphor above it can be understood by imagining our interest as a tax on every release. The more releases we do, the more important it is to keep this under control. Helping customers embrace change Arguably the hardest relationship to maintain as a software company is the one with your most important resource: your customers. They can also be one of the biggest challenges to Continuous Delivery. Changing your application frequently has both the ability to empower or frustrate your entire user base. As with many things, the devil is in the details. Unless you're working in the rarified sector of delivering software to other developers, then it's highly likely that your customers don't understand how software is made at anything more than a superficial level. The first question you must ask yourself when adopting CD is \"can our users handle a constantly evolving product?\". If the answer to this question is 'no' then it's possible that Continuous Delivery is not for you. An alternative approach might be to split your product into different 'release streams', such as a stable branch for customers who don't want change, and an alpha/beta channel for those who embrace it. This in some sense gives you the best of both worlds, as you can readily get feedback from your alpha channel customers. They feel like they are being listened to, and have a very real impact on shaping the product which they use. Multiple release channels do add an overhead to every release, however, so this approach should be used with caution. It also probably wouldn't strictly be defined as Continuous Delivery, but if it's your only barrier to Continuous Delivery then you might consider it worthwhile. Continuous Delivery has a multitude of downsides if done poorly, but it also has tremendous potential upside if done well. If you're planning on adopting some Continuous Delivery principles I hope this article has helped you see ahead to where your organisation might run into problems. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-01-19"},
{"website": "Made-Tech", "title": "Continuous Delivery: Continuous Improvement", "author": [" Fareed Dudhia"], "link": "https://www.madetech.com/blog/continuous-delivery-continuous-improvement/", "abstract": "We've discussed what Continuous Delivery is , the benefits , how to prepare your team for it , the challenges you may face adopting it , the tools you can use , how to build your pipeline and what you can do to make sure quality remains high , but how do you stay on top of the advances in Continuous Delivery? Here at Made Tech, we're always trying to improve. We continuously question our own processes and implement (or remove) processes to eliminate pain points. This isn't about using the newest tech you read about on Hacker News. It's about letting people do their jobs better in order to deliver software faster. So, how do we continuously improve? Retrospectives Sprint retrospectives are an important part of not only a project lifecycle, but also how we continue to improve and iterate on our processes. After every sprint, we sit down with the product owner and our scrum master for about half an hour and look back over the sprint. What went well? What went badly? How could we improve? Our scrum master comes up with novel ways for people involved in the project to vent about pain points in the previous sprint in a constructive, solution-orientated atmosphere. Each retrospective ends with a list of actionable ways to improve, and if previous improvements from earlier retrospectives were found to be particularly helpful, these can be rolled to other project teams. Retrospectives are a great way to find processes that end up doing more harm than good, too. As with everything, it's about finding a balance between structuring developer's output to maximise efficiency, while also making sure the processes aren't getting in the way of delivery. It's only by spending time sitting down and talking about what's working and what isn't that we can continue to improve delivery. Dojos, Katas, Mobs and Talks It's important that your developers continuously improve, too. After a certain amount of time working on a project, even the best developers have a tendency to settle into their own ways of doing things. Some developers can also be resistant to change. In order to improve going into the future, it's important to make sure that developers are all on the same page in regards to best practices, techniques, and maybe even languages and frameworks. It's also important to get the most out of your developers. People who develop software for a living often have a lot of things to bring to the table outside of their project work. Looking into the future, talking about and honestly evaluating new technologies as they mature is an important part of staying afloat in the quickly moving technical world. So what do we do about this? Every Friday, everyone who can spare the time is encouraged to join one of our group programming sessions. These are always organised organically, by people who want to talk about something with the team. These are usually organised loosely into one of the following categories: Code Katas A Code Kata is simply a programming exercise wherein a problem is taken, solved, and iterated over until it is of production standard. We usually use TDD to solve these problems. Everyone in the group has a chance to provide input. The goal of the exercise is to get everyone on the same page, programming-wise. Code Katas are a big help for onboarding and demonstrating the standard of programming required at the company. No-one writes bad code when there's other people watching, especially when the problem is designed to encourage good programming practices. Code Dojos Our Code Dojos are usually long-running projects which are completed over a series of weeks. We divide an hour up roughly into chunks depending on how many people are attending. The laptop is then passed around so each person has about 5 minutes to contribute to the project. The goal is usually to get everyone on the same page in regards to how we deliver software. Contributors are forced to commit early and often, lest their chunk of time go to waste. A good Code Dojo shows how quickly a project can take form without a long planning period, and helps developers get over their fear of uncertainty. Mob Programming A Mob Programming exercise is similar to a Code Dojo, but it's with everyone around the same laptop, making cases for their ideas and then implementing them. Everyone is around one laptop, so the team is forced to work together, building on each others ideas. We often use Mob Programming as a way to refactor some of our existing code. Talks and interactive demonstrations One of the most valuable things that our developers do is give talks and interactive demonstrations on new techniques or technologies they think we should be using. Our modern npm-based javascript stack was introduced to us by one of our more recent hires by a talk he gave on a Friday. He made a strong enough case that we should be using better javascript in production, and now we are. We recently deployed our first projects in React, which was introduced to the team at a similar talk. Talks and demonstrations give the team a chance to air concerns about new technologies. We can decide if they're right for us. Gentle introductions to new techniques and technologies can help to ease any resistance that team members might feel. In our opinion, organically improving beats company-mandated improvement, hands down. Thirty Thursdays So how do we find time to eliminate pain points? Where do we get the time to try out new ideas? If our sprint retrospectives bring up pains in process, when can we research tools that might help us, or develop our own? The answer, for us, are 'Thirty Thursdays'*. Our developers, barring emergencies that need immediate attention, have Thursdays to develop ideas of their own choosing outside of project work. Usually there's a Thirty Thursday pitch meeting, where people pitch their ideas and recruit other team members to help them. We've used the time to produce learning materials, canary builds, implement automated visual regression testing, and even make what you're reading right now. State of the Art New technology is always a big part of looking to the future. Technologies are increasingly born, hyped, and killed in ever-quicker cycles. It's often difficult to keep up with what the \"state of the art\" actually is. The best techonologies still stand the test of time, but often the level of corporate hype around certain companies' offerings can make it difficult to distinguish the soon-to-be-replaced tools from the tech that will stand the test of time. We use a few techniques to figure out what technology is worth trying out and what is worth leaving. Firstly, check the source code! Have a look (or have a developer look) and see what you're using actually is. See how it works. See how easy it would be to fit into your existing workflow. Secondly, are your favourite high-profile developers talking about it? Are they excited about it? Most importantly, are they using it in production? Has anyone written about the use cases that the technology is best suited to, and are you sure that you aren't trying to shoehorn it into your workflow simply because it's the \"hot new thing\"? Thirdly, check out Technology Radar . They publish a monthly list of techniques and technologies to adopt. They're very level-headed about new tech; they'll advise to \"trial\" certain technologies and reassess them as they mature. It's a great resource. In conclusion, there's no foolproof method for ensuring that your company will adapt perfectly in this ever-changing industry. Hopefully the ideas outlined here will have given you some ideas, but ultimately every company works best in its own culture, and the points above are best treated as ideas to help inspire techniques that could prove useful to your company in their own way. In other words, Your Mileage May Vary. That said, we believe that the general concepts we've outlined should stand the test of time. We're very proud of what we do here at Made Tech, and we believe strongly in our culture. A big part of our culture is about sharing what we believe makes us effective, and our series on Continuous Delivery has been our biggest example of this principle so far. Of course, delivering quality software is a business with innumerable facets, but we hope you have found it valuable. Thank you. * the name 'Thirty Thursday' comes from a fleeting moment of terrible maths. Somewhere along the way, it was declared that one day out of a five day working week is equivalent to thirty percent of a working week, and, more importantly, ' Twenty Thursday' doesn't roll off the tongue nearly so well. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-01-21"},
{"website": "Made-Tech", "title": "How To Create Cross-Browser Font Styles That Match The Design", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/how-to-create-font-styles-that-match-the-design/", "abstract": "Longstanding was the battle between designer and front end engineer in regards to having fonts render perfectly as per the design, and consistently between different browsers and operating systems. In even darker days, designers were lucky to get a custom font face. Thankfully those days are behind us. This is largely because browser vendors (Google, Mozilla, Microsoft, and Opera) have started implementing the working draft of CSS Level 3 (CSS3). This, in turn, has lead to the release of a number of great JavaScript plugins that can enable us to create the right typographic layouts; there is a new hope for pixel perfect web typography. So what exactly are the these new features? Chances are you've already been using them, albeit unknowingly, but in this article I'll cover some of these new features, how to implement them, what impact they can have, and I'll maybe even share a rendering fix for Internet Explorer (because, Spoiler Alert, there is one). In addition to the new CSS3 features we'll also look at some existing CSS2 attributes which, when used in the correct manner, can also be just what you need. The CSS Font weight synthesis AKA \"Mystery Emboldening\" The font-face rule is now quite commonplace, but have you ever noticed that in some browsers – such as Chrome – if you don't have a matching font-weight (e.g. bold) for your webfont, the browser will help you out by artificially emboldening your regular type face, usually with varying degrees of success? Well, this is the browser synthesising the missing typeface. You'll see the difference below in these examples. Missing typeface: Typeface present: Some might see this as desirable behaviour, as it's better to have a bold weight than none at all. However, I am of the opinion that this font rendering isn't correct, and we as developers should stop this from happening. The addition of the font-synthesis attribute at a global level will suffice in solving this rendering quirk. Which means you'll notice when you've not selected the right weight. * { font-synthesis: none; } Font synthesis off (Firefox only): Tracking Often we are handed a design where the designer has tweaked the tracking – or letter spacing as it's referred to in CSS – of the font, in order to \"tighten up\" the design. As long as this change is across the whole body of text, it is quite straight forward to match the letter-spacing. In the past I've used this mixin to convert the PSD tracking value to ems. It's always best to specify letter spacing in ems – especially for responsive layouts – as it means you can redefine the font size and the letter-spacing will scale. @mixin tracking($spacing) {\n  letter-spacing: ($spacing / 1000) * 1em;\n} Differently tracked text: However if the design does require more in-depth and accurate kerning, as opposed to tracking (the difference between the two is explained here ) then you'll need to employ some JavaScript, but I'll cover more on that later. Leading and line height Leading isn't the same as line-height in CSS. Leading is in fact the half difference between the font-size and line-height. Line-height isn't something new, but worth briefly mentioning for an accessibility reason. For big blocks of text (e.g. an article or a blog post) line-height should be at least 1.5 times the font size according to WCAG rule 1.4.8.4 , which is easily achieved using ems. Em based line heights: Font features One of the biggest typography features to arrive with CSS3 is the ability to enable OpenType font (OTF) features, of which there are a lot. Typekit have an awesome article explaining them all in great detail, which I'll refrain from repeating here. I will, however, demonstrate how to use a common font feature: enabling ligatures. Ligatures in Lato: Typically OTF WebFonts with font features are a bigger download for a user, so this might put you off using them. However, using them generally makes your long form copy look a lot nicer, and will deliver a better aesthetic. It's worth noting that specifying text-rendering: optimizeLegibility will also add ligatures to your text. As a side note, this is one reason why you should only add this property in areas where you definitely want ligatures. Kerning Kerning is defined as \"the process of adjusting the spacing between characters in a proportional font, usually to achieve a visually pleasing result\" by Wikipedia , which is pretty accurate. In CSS there are a couple of ways to enable kerning, the first being text-rendering which can have mixed results, and the second being font-kerning. Due to browser support it's probably worth specifying both, and (as noted above) you'll get the added bonus of enabling ligatures too! Un-kerned and kerned text: And also like ligatures, text kerning can be enabled using font-features. Font smoothing There are a lot of articles in support of using font-smoothing in your CSS and quite a few against it, with the arguments centering, respectively, around making the text more attractive, and usability. Personally, I always specify it in my CSS, as it does make the fonts look a lot crisper, especially at low font weights. Font smoothing at low font weight: However, this is a Mac only feature. To get your fonts rendering nicely on Windows you'll need to employ ClearType . ClearType is enabled using a meta tag in the head of your html. Also, while we're on on the subject of font rendering and Internet Explorer, there is another meta tag that could make your life easier if you are having issues with the appearance of your font on older versions of the browser we love to hate. This meta tag will turn off the inter-pixel spacing in IE9+ and uses GDI metrics instead. JavaScript Solutions We can't talk about web typography and not mention a few of the JavaScript solutions available on the Internet. Two such solutions are letteringjs and kerningjs , and both achieve their tweaks in a similar way, with the former being a touch more manual than the latter The method they both use to achieve this is to wrap each character of the string in a . This will make your generated markup look ugly, but it allows you to fine tune the kerning and vertical positioning. It's worth noting that if accessibility is a priority for your site, screen readers will pause on each span, which isn't ideal. This can be overcome by adding aria-* attributes on the spans, which is something letteringjs does do. Useful Links OpenType Font Features in CSS State of Web Type CSS Fonts 3 CSS Fonts 4 Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-02-11"},
{"website": "Made-Tech", "title": "An APPLE a Day: The Key to a Successful Presentation", "author": [" Pedro Martín Valera"], "link": "https://www.madetech.com/blog/an-apple-a-day-the-key-to-a-successful-presentation/", "abstract": "My name is Pedro Martin, and I began my career as a programmer in late April 2015, when I started the Web Development Immersive course at General Assembly. During the course, in addition to classes, we were given talks by experts in different areas. While some of them were able to express domain concepts in simple words, others couldn't, making it very difficult to understand the subject. When listening to experts discuss technology, I have often found that they are less able to express themselves in simple terms. Being an expert in something does not necessarily mean that knowledge can be conveyed by such a person in a concise manner. Before becoming a programmer, I was a teacher of Environmental Sciences in secondary schools in Venezuela, and part of my job was to explain in simple words how complex systems rule in natural environments. One of my inspirations for becoming a teacher was the late Carl Sagan who, for me, was the perfect example of how to explain complex scientific or technical subjects into little pieces of knowledge. As a teacher, I found the following model, known by the acronym APPLE, to be a great approach to conveying information in a way my students could understand. It is a practical guide to breaking a complex subject down into simple pieces of knowledge, and was first introduced by Karl Ronkhe, one of the founders of Adventure education . APPLE Assess: Who is the group? What do they want to accomplish after the talk? How long will the presentation last? Where will the presentation take place? Plan: How much information do they need? How much time do you have? What sequence of activities will produce the best results? Be prepared to alter your plan on the fly How can you bring fun to the presentation? Prepare: Preparing, different from planning, begins with the implementation Have a plan B if something goes wrong Ensure location has sufficient facilities to accommodate your presentation Lead: Set the tone and make people feel comfortable Style: Be clear and simple, be enthusiastic, use humour. Be creative Observe and listen Ask questions, check if your audience is with you Have fun. Do DDADA (see below). Evaluate: During the program. Monitor the group. DDADA Describe: Present the feature you would like to describe as simply as possible. You can be creative in your presentation, but try not to confuse people with too much detail or jargon. Try also to keep it light, using humour when appropriate helps engage your audience. Demonstrate: As the saying goes, a picture is worth a thousand words. No matter how clear your explanation was, a brief demonstration would clarify even more. As another saying goes, “I hear and I forget, I see it and I remembered, I do, and I understand\" . Ask questions: Before moving from one topic to another, ask if anybody in the audience needs clarification, and give them the opportunity to ask questions. If you find you're inundated with questions, it may be worth describing and demonstrating the topic again. Do: Play it!, even if certain people don’t get it at the beginning. If you still notice confused looks, stop, describe or demonstrate again. Adapt: Check if people are having fun; a big part of the learning experience concerns your audience's feelings and emotions. If your audience is having fun, their learning experience will the better for it. Carl Sagan may not have known of these models, but if you were to watch him talk about the 4th dimension, you would see that he takes a similar approach to explaining complex topics: These guides are here to help give you structure in your presentation, but as you are learning and developing your own style of giving presentations, you may find some parts more useful than others, in which case you shape it to suit your needs. I hope you will find this as useful as I did. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-01-26"},
{"website": "Made-Tech", "title": "9 Benefits of Test Driven Development", "author": [" David Winter"], "link": "https://www.madetech.com/blog/9-benefits-of-test-driven-development/", "abstract": "Test Driven Development is the practice of writing a test for a piece of required functionality, before writing any implementation code. This test should fail when first run, and then, you write the code to get it to pass. It doesn't have to be the most perfect code, just so long as the test passes. Once it does, you can then safely refactor your code. TDD is a discipline, and as such you must try your best to not be tempted to write tests after you've written code. Pressure can come from clients or employers to delay or not bother with the writing the tests at all. Firstly, they need to understand the benefits of tests, and then the additional benefits of TDD on top so you can secure that time to practice TDD correctly. If your application has tests that were written after the code that implements them, that means TDD wasn't followed. It does mean however that your project has test coverage, which is a good thing that has many benefits (and is definitely better than not having any tests), but practising TDD brings additional benefits on top of those. Often, articles on the internet claim to write about those additional benefits but instead seem to end up focusing on the benefits of tests in general. This post will try and focus strictly on the benefits of TDD. You shouldn't assume either that TDD doesn't mix with BDD . The key is writing the tests before the code. When writing feature specs that define the behaviour of something, if you're writing those specifications before implementing them, you're TDD'ing too. At Made, we use TDD when writing our feature specs. We use tools most commonly used for unit tests for our feature tests too, such as RSpec with Capybara helpers , rather than things like Cucumber, which are often associated with BDD (that said, you can still use Cucumber for feature specs and TDD). In all of the time that we've been using TDD, these are the biggest benefits we've noticed along the way: Benefits 1: Acceptance Criteria When writing some new code, you usually have a list of features that are required, or acceptance criteria that needs to be met. You can use either of these as a means to know what you need to test and then, once you've got that list in the form of test code, you can rest safely in the knowledge that you haven't missed any work. 2: Focus You're more productive while coding, and TDD helps keep that productivity high by narrowing your focus. You'll write one failing test, and focus solely on that to get it passing. It forces you to think about smaller chunks of functionality at a time rather than the application as a whole, and you can then incrementally build on a passing test, rather than trying to tackle the bigger picture from the get-go, which will probably result in more bugs, and therefore a longer development time. 3: Interfaces Because you're writing a test for a single piece of functionality, writing a test first means you have to think about the public interface that other code in your application needs to integrate with. You don't think about the private methods or inner workings of what you're about to work on. From the perspective of the test, you're only writing method calls to test the public methods. This means that code will read well and make more sense. 4: Tidier Code Continuing on from the point above, your tests are only interfacing with public methods, so you have a much better idea of what can be made private, meaning you don't accidentally expose methods that don't need to be public. If you weren't TDD'ing, and you made a method public, you'd then possibly have to support that in the future, meaning you've created extra work for yourself over a method that was only intended to be used internally in a class. 5: Dependencies Will your new code have any dependencies? When writing your tests, you'll be able to mock these out without really worrying about what they are doing behind the scenes, which lets you focus on the logic within the class you're writing. An additional benefit is that the dependencies you mock would potentially be faster when running the tests, and not bring additional dependencies to your test suite, in the form of filesystems, networks, databases etc. 6: Safer Refactoring Once you've got a test passing, it's then safe to refactor it, secure in the knowledge that the test cases will have your back. If you're having to work with legacy code, or code that someone else has written, and no tests have been written, you can still practice TDD. You needn't have authored the original code in order for you to TDD. Rather than thinking you can only TDD code that you have written, think of it more as you can only TDD any code you are about to write. So if you inherit someone else's untested code, before you start work, write a test that covers as much as you can. That puts you in a better position to refactor, or even to add new functionality to that code, whilst being confident that you won't break anything. 7: Fewer Bugs TDD results in more tests, which can often result in longer test run times. However, with better code coverage, you save time down the line that would be spent fixing bugs that have popped up and need time to figure out. This is not to say that you might be able to think of every test case, but if a bug does come up, you can still write a test first before attempting to fix the problem, to ensure that the bug won't come up again. This also helps define what the bug actually is, as you always need reproducible steps. 8: Increasing Returns The cost to TDD is higher at first, when compared to not writing any tests, though projects that don't have tests written first usually end up costing more. This stems from them not having decent test code coverage, or any at all, making them more susceptible to bugs and issues, which means more time is spent in the long run fixing those. More time equals more money, which makes the project more expensive overall. Not only does TDD save time on fixing bugs, it also means that the cost to change functionality is less, because the tests act as a safety net that ensure your changes won't break existing functionality. 9: Living Documentation Tests can serve as documentation to a developer. If you're unsure of how a class or library works, go and have a read through the tests. With TDD, tests usually get written for different scenarios, one of which is probably how you want to use the class. So you can see the expected inputs a method requires and what you can expect as outcome, all based on the assertions made in the test. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-02-09"},
{"website": "Made-Tech", "title": "7 Reasons to Adopt #NoEstimates in Software Delivery", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/7-reasons-to-adopt-number-noestimates-in-software-delivery/", "abstract": "In this first of a pair of articles about software project estimations, I'll be arguing in favour of foregoing estimations. The #NoEstimates movement/hashtag has been gaining much press over the past couple of years, particularly when tied to a Kanban-style workflow. It takes the viewpoint that instead of expending effort estimating, or some may argue, guessing, as to how long the implementation of a feature may take, you're better to spend that effort delivering the feature instead. Popular delivery methodologies such as Scrum are heavily underpinned by estimations. Planning sessions typically precede the start of every iteration, during which the team will estimate the complexity involved in a set of features, drawing a line in the backlog when the estimates say that the team is at capacity. Other contemporary methodologies such as XP focus more militantly on delivering the most valuable feature at any given time, rather than estimating and committing to larger bodies of work. Here are our 7 top reasons to consider adopting #NoEstimates on your next software project, or indeed transitioning estimates out of your workflow on your current project: 1: Humans are bad at estimation Humans are inherently bad at estimating the time it will take the complete a task, particularly in complex domains such as software development, where it can be unrealistic to foresee fine-grained implementation details or every case in which unexpected complexity could arise. 2: Estimates used as deadlines In many organisations, it's common practice that estimates are quickly turned in to deadlines. If a team estimates that a feature can be completed in one week, there can be an expectation that it will be delivered in one week. This can lead to scenarios where teams inflate their estimates to ensure they have adequate buffer to deliver the feature to the estimate, ultimately reducing your team's throughput as every feature includes potentially unnecessary padding. 3: You estimate when you know least By nature, estimations happen before the work has started. You might have chosen to spike out an implementation to remove some uncertainty or to understand the feasibility of a proposed solution, but you still know least about the feature before you've written any code. 4: Time can be better spent delivering value In most scenarios, we believe that you'd be hard-pressed to argue that spending time estimating (guessing?!) is a true value adding exercise. Estimates can seldom speed up delivery, and so the time spent estimating is time not spent delivering working software. We often espouse that generating heavy documentation, or designing interfaces using tools such as Photoshop is ultimately an exercise in producing artefacts. In many cases, producing time estimates can be viewed in a similar light. 5: Adapting to change becomes harder By the time you've invested in pulling together your estimations, you feel attached to them, and have a hard time throwing them away. Value is attached to the estimate because of the effort involved in creating it, and so even where it's desirable to follow a different course, there may be resistance, conscious or otherwise, to doing so. 6: Adopting #NoEstimates doesn’t mean #NoDeliveryDates One of the common themes of resistance to abandoning estimates is that it also removes delivery dates. This doesn't have to be the case. By continuing to work towards a delivery date, and making regular go/no-go calls based on the very latest understanding, the team can remain focused on delivering the highest value increment by the agreed delivery date. 7: Focus on delivering smaller increments Instead of big planning up front, the team can shift their focus to break features down on a just-in-time basis. In addition, with discipline, each of these features can be delivered as an incremental delivery out in to the hands of customers on a far more regular basis. This shortens feedback loops and empowers the Product Owner to make more frequent prioritisation calls. Where the team's discipline allows, adopting increasingly agile workflows that don't rely on estimation, can, in our experience, often result in reduced waste when compared to methodologies that encourage longer-term commitments. There are a number of drawbacks, or at least areas that require closer full-team collaboration and discipline when removing some of the structure that estimates can help to provide. We'll be covering these in a follow-up post that looks at the pros behind retaining estimates in your software deliveries. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-02-23"},
{"website": "Made-Tech", "title": "One Week of Remote Working: The Good & The Bad", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/one-week-of-remote-working-the-good-and-the-bad/", "abstract": "Released in late 2013, Remote: Office Not Required by David Heinemeier Hansson and Jason Fried (of Basecamp, formerly 37signals) immediately struck a chord, particularly in our industry. Suddenly it had become acceptable to want to work in places other than the office! The work/life balance benefits of remote working are pretty obvious; it affords you the ability to do a job you love within a great company, whilst also giving you the opportunity to place greater focus on your home life by simply allowing you to be there, or somewhere very close, more often. Also, no arduous commutes! As a company, we've been indulging in remote working for quite a while now. We're not completely remote by any means, but in a typical week we normally find that around half of us take a day or two to work remotely, and the rest of the time we're in the office. Because we're a team of people who get on really well, have a lot of fun and put a lot of passion and enthusiasm into our work, we know that we have a great environment to work in already, and as a result there hasn't been the desire to fully embrace the idea of remote working. Even so, we wanted to see if we could make the switch to being completely remote, so we recently ran an experiment to see if we'd be able to cope with an entire week of everyone in the company working outside of the office. The one caveat to the experiment was that, obviously, clients are the priority. Remote work had to take a back seat if there was a meeting with a client to attend or we'd arranged to, as we've done on a number of our projects, work on-site with a client. The goal of the experiment was to find out how it would affect the company culture, whether we were more productive, and whether it was something we could actually see ourselves doing more of in the long term. What We Learned Be Seen AND Heard Remote work makes being literally face to face impossible, unless you happen to be working in the same location. It's important to do the next best thing and talk via video chat, you want to be able to see the people you're working with, rather than have them be a disembodied voice emanating from your laptop. We recommend checking in with your team at least once a day for a quick daily catchup in the morning, not just so everyone knows what others have done and are planning to do, but so you're also getting some human contact. Big Group Meetings Don't Work We used both Skype and Google Hangouts for our meetings, with varying degrees of success. On the one hand, Skype is great for smaller team meetings of around four or five people. On the other, we had a meeting with the entire company via Google Hangouts and everybody hated the experience. There's an energy you just don't get when the meeting room is virtual and not physical, making it hard to bounce off each other. Because the technology made it so difficult to hear what was being said when two or more people were talking at the same time, a lot of us became reluctant to talk at all, and led to huge periods of silence while we were each waiting for somebody else to chip in. Research Remote Work Locations You may already be familiar with a number of places you know are good for working in. If you decide you want to branch out, it's useful to know ahead of time whether a place can offer you the fundamental things you'll need if you want to work uninterrupted. Look for reliable wi-fi connections (some coffee shops restrict you to an hour), power sockets, find out what the noise level is like, and how busy the place can get. If you can remain focussed and productive whilst working there, great. We found Work Hard Anywhere to be a useful tool in looking for potential places to work. Beware Of Spiralling Expenses When working from places such as coffee shops, pubs and bars, there's a creeping obligation to buy something every so often to justify your setting up camp on their premises. Keep an eye on this as, left unchecked for a few days at a time, you'll find your wallet has taken a big hit on what is likely several cups of overpriced coffee. Libraries Are Your Best Friend Libraries tick a lot of boxes when it comes to remote work. People go there to study, so the atmosphere encourages you to focus, making it easy to be productive. Wi-fi and power sockets are in ample supply, though be warned that libraries blacklist a LOT of websites. That said, blocked sites are all along the lines of Reddit et al, so you could say they're doing a favour by removing access to distractions. Also, don't be the one who forgets to put their phone on silent. The reaction is not pleasant: \"Remote Work\" Doesn't Have To Mean \"Work From Home\" Working remotely means you're given the freedom to work wherever you like and, for most of the team, even before we began this experiment, that often meant just not leaving the house in the morning. Working from home can be a great, refreshing experience in short bursts. The novelty of not needing to commute, being able to work in a place you feel truly comfortable (and can get the laundry done) is enticing, but a big problem in this scenario is finding that separation between work and life. It becomes easy to start working earlier and finishing later, because you don't have that ritual of getting to and from work at a similar time as your colleagues. You don't get those same cues at the end of the day that it is the end of the day, so you never close your laptop. Similarly, because you know you're working these longer hours, it's tempting to work at a less productive pace, on the understanding that you'll get the same amount of work done anyway, despite this being the opposite of what remote work wants to achieve: a healthy work/life balance. Put A Shirt On During the experiment, when we absolutely had to work from home, this was one of our mantras. Though it started out as a bit of a joke, there's truth to it in the sense that there should be some sort of ritual you engage in before starting work at home, to try and create that separation. Get dressed as you would for work, go to an area in your home you don't necessarily associate with relaxing (i.e. somewhere there's no TV you can put on in the background), and check in with your colleagues. Not Everyone Wants To Work Remotely It's so easy to get swept up in the excitement that the concept of remote work offers, that it's also easy to overlook that there are people who genuinely prefer to work in an office environment. At the end of the day, remote work boils down to you being able to choose the location in which you're the most happy and productive, and in some cases that is the office. Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-02-17"},
{"website": "Made-Tech", "title": "The Building Blocks of Reliable Software", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/the-building-blocks-of-reliable-software/", "abstract": "When solving requirements for a system, you should extract specific roles out into service objects. The lazy path is to solve problems directly where you encounter them such as in the controller, model or view (given you are using MVC of course). You are missing a \"Code Seam\" (a term coined by Michael Feathers in his book Working Effectively with Legacy Code ) that you can hook into to get the desired functionality from. Instead you should create very simple, small objects (generally less than a hundred lines), with verbose names describing exactly what problem they solve. In Ruby these are known as Plain Old Ruby Objects (POROs). State as a separation of concerns A referentially transparent (or pure) function will always return the same output given the same input. These are considered dependable because they are not affected by the outside world beyond what they are given as arguments. In this setup I propose having at least a pair of objects for accomplishing tasks. One which deals with stateful things and the other operates only on its arguments. I will call the non-stateful object a Value Object. The stateful object is a Service Object. The value object exists solely to represent the data needed by the stateful object. It acts as a filter that transforms the shape of the data. In a perfect world your data would conform to your program exactly, meaning that you have to write almost no code to utilise it. This is rarely the case though. Stateful objects coordinate actions like writing to a database, sending emails, uploading CSVs and all those other volatile things that are undependable might fail. We need to separate the dependable from the potentially volatile, and the likely to change from the more static code. One could argue that the stateless object may change more frequently due to changing requirements, where the stateful object is less likely to. In other words, if you were sending an email, the means by which you do this is less likely to change than the data contained within the email over a period of time. Single public method and its anatomy Apart from the constructor, there should be only one public method that you can call. This method is special and should be comprised of only methods that you have defined. It should be a sequence of steps and by simply reading it you should be able to understand everything that the object does. Notice that below every method sits at the same level of abstraction. A layer of abstraction that we have defined ourselves in plain English. Good: def signup_employee\n  find_or_create_user\n  set_user_as_employee\n  notify_via_email\nend Bad: def signup_employee\n  find_or_create_user\n  set_user_as_employee\n\n  mail('foo@example.com', 'Welcome new employee')\nend The mail method above is an abstraction that sits at a lower level than the rest of the methods. This applies to both the value objects that are instantiated in the service object and the service object itself. In fact I would advise that all methods you write adhere to this. Adopting this as a discipline will lead to more cohesive code. Hashes as the transport mechanism Hashes/Maps are primitives that are composed of keys and values and are perfect for packaging up data to be sent to another object. They are descriptive enough to make sense on their own just by reading the contents. I instantiate both the service objects and value objects with hashes. A benefit of using hashes is that you sidestep ordering complexity that you get with positional arguments. It does not matter what the order of the contents of your hash is as they are referenced explicitly by key. Always favour using the .fetch method over square bracket syntax to access values that you expect to be present in your hash. This is important because it will throw a key not found error, rather than having a nil value percolate through your program undetected. The Principle of Least Surprise Be wary of writing fancy code, even if it saves you from writing an extra line or two. It may not be worth it in the long run. Write boring, predictable, simplified code. Write it as if a 5 year old was going to inherit the codebase from you. Long descriptive names and no abbreviations are the law. You will be surprised how hard a piece of code may become to understand after 2 months of not working on it. If something could be given a name, do it. first_name is much more intention revealing than person[0]. Good: if first_name == 'Sally'\n\ndef first_name\n  @person[0]\nend Bad: if @person[0] == 'Sally' You could argue that person[0] is less code but first_name has more meaning and is far superior. Easy testing When testing these objects, you only assert the outcome of the one public method that the object has. With the Service Object / Value Object pattern, pair testing becomes much easier. For testing the value object it need simply be instantiated with a hash of parameters to assert that it has transformed the data into the values you require for your service object. This type of test will be fast and isolated. Testing the service object itself has also become easier as there is less setup required and it's been untangled from the data it needs. I have no problem with stubs and mocks for testing this at the unit level. Show me The Stateful Service Object class CaptureUserPdf\n  def initialize(params)\n    @user = params.fetch(:user)\n  end\n\n  def run\n    write_pdf\n    mark_as_exported\n  end\n\n  private\n  def write_pdf\n    SomePdfClass.new.write(pdf_user)\n  end\n\n  def mark_as_exported\n    @user.mark_as_exported\n  end\n\n  def pdf_user\n    PdfUser.new(@user).to_pdf\n  end\nend The Non-Stateful Value Object class PdfUser\n  def initialize(params)\n    @user = params.fetch(:user)\n  end\n\n  def to_pdf\n    {\n      name:   name,\n      status: status\n    }\n  end\n\n  private\n  def name\n    @user.name.capitalize\n  end\n\n  def status\n    if @user.roles.include?(:manager?)\n      'manager'\n    else\n      'worker'\n    end\n  end\nend Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-02-19"},
{"website": "Made-Tech", "title": "4 Reasons Not To Adopt #NoEstimates In Software Delivery", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/4-reasons-not-to-adopt-number-noestimates-in-software-delivery/", "abstract": "The #noestimates movement is a subject that has generated a fair amount of controversy in the software development community since its inception, including within the team here at Made. Last week, my colleague Chris discussed several reasons why #noestimates is A Good Thing . In this article, I'll be arguing why completely foregoing estimations in project delivery is a bad idea, and why there may be a compromise. Within the world of Agile, estimations are part of a number of methodologies, particularly Scrum, with practitioners meeting at the beginning of each sprint to plan out and estimate the work to be done in that period. The meeting gives everyone a chance to raise concerns, ask questions and get a very high level overview of the work that needs doing. Although we've been experimenting with #noestimates on some of our more recent projects, we're still fans of practicing Scrum, and the following are four reasons why we feel those estimation sessions are important: 1: They’re Estimates, Not Exactimates Estimates, by definition, are rough guesses, and are ultimately very likely to be off the mark. A piece of work may take longer than expected, and that's fine . Trouble only sets in when developers get attached to estimates and treat them as absolute, immovable deadlines. The onus is on you, the developer handling a given task, to communicate to your Scrum Master that you feel something is going to take either a lot more time to complete, or a lot less. Your Scrum Master then has the info he or she needs to make the best decision for the sprint at large, which will likely involve discussing the situation with the client, and then either reducing the scope of the sprint, or pulling more tasks in. 2: Chance Favours The Prepared Mind It's true that we estimate when we know least, but planning and preparation is an important process in any project, in all walks of life. By spending time estimating, with as much knowledge as we have at the time, we give ourselves the best possible plan for the sprint ahead as we discuss and discover the tasks that will likely need the most time spent on them. This allows us to either prioritise those tasks within the parameters of the current sprint cycle or, if we know there are more pressing tasks to complete beforehand, push it to the next sprint in order to fill the current sprint with tasks we're confident we complete. 3: Trust Issues The #noestimates movement requires a massive amount of trust between you and your client, as you're effectively removing the one thing that gives them a sense of how long the project will take, and how much it will cost them. For most clients, that's a huge risk. Without an estimate of how long it will take, how can they be confident that their project will come in at a reasonable cost? What grounds do they have to take you at your word that you'll demo progress at regular intervals? Existing, flexible clients with whom you've built a significant amount of trust may be willing to experiment with the idea, but you'll struggle with asking a new client to throw money into the unknown. 4: Being Bad At Something Doesn’t Mean We Quit Yes, people are typically bad at estimating, but with experience we learn to spot where pain points are likely to arise and adjust future estimates accordingly. We'll still get it wrong from time to time, maybe even drastically so, but estimation remains an essential tool for figuring out a roadmap for the upcoming sprint. Can’t We All Just Get Along? As hotly contested as #estimates vs #noestimates is, a more level headed approach may be to only estimate when absolutely necessary. Mike Cohn , a prominent figure in the Agile community, put it best: \"A team should estimate and plan only to the extent that further investment in estimating and planning will lead to different actions. If you will do the same thing even if you estimate or plan more, stop.\" Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-03-04"},
{"website": "Made-Tech", "title": "Continuous Feedback – Peer Review for the 21st Century", "author": [" Andrew Scott"], "link": "https://www.madetech.com/blog/continuous-feedback-peer-review-for-the-21st-century/", "abstract": "One of the hardest things for any company to do is to foster an environment in which employees feel motivated and that they have the ability to improve, both as skilled employees and as people. For the last few years a fierce debate has emerged over the effectiveness of annual performance reviews and the merit they have in the 21st century workplace. A key point before we start here is that companies focus on what value they add to customers, but the value they add to employees is often an afterthought at best. The Annual Performance Review The era of annual performance reviews is over and, before we get on to the matter at hand, it makes sense to illustrate why. Conventional performance reviews have a negative connotation right off the bat. They have been traditionally used by companies who use the so called “rank and yank” system, whereby employees are ranked according to their output and promoted/fired accordingly. A performance review is based on the core tenet that one’s manager or one’s direct colleagues cannot be trusted to give an accurate assessment of an employee’s worth or potential and therefore layers of bureaucracy and managers have to weigh in with their suggestions, all being weighted in a way to get a manufactured output. The system relies heavily on internal politics and the company, where having friendly relationships with one’s managers and superiors is far more important than how you work. This ferments an atmosphere of a) fear and b) political bureaucracy which is not conducive to a happy work environment and punishes people who just work hard and don’t “play the game”. Additionally, some employees have underlying performance problems and therefore are likely to be poor employees, issues which should be addressed immediately, not at the end of the year. Nobody can accurately assess someone’s work over the space of a year. Any feedback you receive is most likely going to be out of date. Thinking of how much one can improve as a developer in the space of a year makes the feedback from March completely extraneous by October. People respond best to pertinent and actionable feedback, and the “appraisal” process is the antithesis of this. Often the only time that employees ever receive feedback is when they're in a pay review, being disciplined or discussing promotions. The reasons for large companies with this structure in place to not want to change the status quo are manifold: It gives the impression of fairness when it comes to promotions or salary increases as opposed to it being one person’s decision. Hedges against employment litigation so you have a record of poor performance when letting someone go. Helps to build “the file” on an employee, giving HR the massive amounts of data that they feel they need to do their jobs effectively. The problem is that, as you can see, these reasons are defensive and reactive instead of being proactive. The company is put firmly before the employee and this is obviously not in the best interests of the employee. The Feedback Loop We Need As developers, we all understand the concept and importance of a feedback loop. It helps to guide us as to where problems lie in our code and how we can improve it, both to get it working and then to optimise it. It makes sense to extend this logic to employees and to have a regular feedback loop which can help us to optimise our workflow and improve constantly. At Made, we implemented continuous feedback 6 months ago. The thinking behind this was clear: we have a motivated group of developers who are autodidactic in nature, and it makes sense to provide a consistent, constant stream of feedback, both positive and negative, to allow employees to have clarity of mind as to which areas require improvement and in which areas they are excelling. In order to excel as an employee and have personal growth, it is crucial to have a feedback-rich culture. The way we do that is through a series of ‘1-2-1s’. Everyone meets with a colleague in an informal setting once every 2 weeks (or sooner) and discusses feedback which has been shared with the rest of the team. The feedback-ee can choose which feedback to take on board and which to discard. In general we have found that most people are fairly aware of their strengths and weaknesses, so much of the feedback is self guided. The feedback is split into three different categories. They are as follows: Feedback from others This covers what others have said, concrete examples of said feedback (which forces people to back up their feedback), and what the impact/take away from this is. The benefit of this is clear; it helps us to gauge where we stand within our peer group, how the work we are doing has been perceived and where we are falling short. It also helps to remove the politics since it encourages us to be up front and honest with our peers. The person giving the feedback and the feedback-ee can discuss the feedback and whether they feel that feedback is fair and whether it is actionable. Long Term Goals and Objectives This covers where we want to be in 3-5 years, how the company can help us to achieve this and to track the status as we progress towards these goals. It is important that these goals are not tied directly to wider company objectives, since personal development is undoubtedly as important, if not more so, than development within the company. Short Term Measurable Goals This covers what we can do to action the feedback received from others. An example of this would be to give a talk at a meet up or work on a different part of the tech stack. When we put in a short term measurable goal it is important to set a time when this can be reviewed to ensure that these objectives are at the forefront of our mind. In order to stop these changes from being ephemeral, we keep them noted down after they are done to remind us to keep doing them. The Argument For Continuous Feedback In the workplace, a large proportion of problems are caused by a lack of communication. Regular feedback clears up miscommunications, gives you as an employee a sense of self worth and understanding of where you are and how you can improve. Knowledge is power and as an employee, the more you know about your strengths and weaknesses, the better equipped you are to improve and to head off problems before they become a critical issue. However there are several much more deep rooted needs for continuous feedback. In Branham’s book The Seven Hidden Reasons Why Employees Leave , he explains that one of the primary reasons why employees leave is because of a lack of feedback. If you have employees who want to improve, then they want feedback and to be pushed by others. If you have employees who do not want to improve or are merely happy treading water, then you probably don’t want them at your company. On an even more fundamental level, it encourages a flatness of opinion that is refreshing. There is nobody in the continuous feedback system who is beyond reproach. Some of the bluntest feedback falls on the shoulders of the most senior members of the team, and this helps battle against complacency. This builds morale amongst employees and in turn helps them to be more productive. Much has been written about the psychological contract at work. This could be another blog post in and of itself, but in essence it is comprised of our perceptions and trust concerning the agreement that exists between ourselves and our employer. Maintaining the health of the psychological contract at work leads to less broken promises, less opportunities for misunderstandings and a healthier work environment. Introducing Continuous Feedback to your organisation In introducing Continuous Feedback at Made, we've found a couple of documents useful. During the feedback session itself, we use a sheet to track feedback (with some hints to delve a bit deeper in to it), and our long and short term goals. We also produced an introduction document to get the process kicked off. If you're feeling encouraged to give Continuous Feedback a try in your organisation, we hope you might find these useful. Individual Feedback Sheet Introduction Document Tweet 0 LinkedIn 0 Facebook 0", "date": "2016-03-01"},
{"website": "Made-Tech", "title": "Pull requests and Continuous Integration", "author": [" Fareed Dudhia"], "link": "https://www.madetech.com/blog/pull-requests-and-continuous-integration/", "abstract": "A few months ago at Made Tech the Finery team switched to GitHub . Before the move we pushed commits directly into the master branch. Commit notifications with links to the diffs would then come up on the Finery channel on our HipChat server, and members of the team could review the commits at their own leisure. There was no commitment to code review. When we switched to GitHub, we put in two rules for committing into the Finery project: 1: Features and solutions to tickets go into pull requests. 2: You can’t merge your own pull requests, another member of the team has to do it for you. As with any new process, this has its cons. Pull requests slow the process down. Code doesn’t get merged in straight away if all members of the team are busy. Branches have the capacity to go stale. The pros, on the other hand, have been pretty huge. You’d think that the biggest pro would be that it helps keep the build pipeline clear if important fixes have to be pushed through quickly, but in reality pushing into master just means that the team couldn’t push unfinished commits, or groups of commits that didn’t constitute full features. The biggest pro has actually been the ability to have a conversation around the code, inline with the actual code itself. ‘Is there a better way of doing this?’, ‘I didn’t know you could do it like this!’, ‘Would using X work better here?’, etc. Having an extra pair of eyes on a group of commits almost always highlights things worth looking at: bugs, style issues, possible simplifications, the list goes on. It also helps to form conversations around the way we write code. It helps form culture. Knowledge is shared quicker. Bus factor is lower. Having someone else approve the code means that there is a shared responsibility for the feature. No single person can be said to have developed it in a vacuum. If there is an issue with the feature that needs to be fixed, both the programmer and the reviewer share the knowledge gained from fixing the issue. Both of them are less likely to make the mistake again. The other, slightly less widely-cited benefit of compulsory code review is that if you know that someone else in the team is going to have to sign off on the code, you’ll probably write it better the first time round. You can’t cut corners. You can’t sneak stuff in. Let’s face it, this happens even in the best software teams. The very idea that a code review is going to happen on a pull request ensures a reduction of technical debt. I’ll end with a nice little pro that I’m personally the happiest about since switching to continuous integration with pull requests on GitHub: it’s easy to leave someone an encouraging note on their pull request. ‘This is great.’ ‘Nice job here.’ Sometimes it’s the little things that keep you loving the work. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-09-22"},
{"website": "Made-Tech", "title": "Running a Retrospective", "author": [" Nick Wood"], "link": "https://www.madetech.com/blog/running-a-retrospective/", "abstract": "A few weeks ago, Fareed wrote about his favourite retrospectives . I wanted to follow up on this and outline what I'd consider essentials for a good retrospective. Planning You get out of a retrospective what you put into it. Before even stepping into the room I like to spend some time trying to anticipate what people are going to say: are there any thorny issues dogging us that I can tailor an activity to flush out into the open? How is team morale? Is anyone particularly upset/frustrated and might this disrupt things? Structure Much like every good story needs a beginning, middle and an end, so too does a good retrospective need structure. The details of each section can and should vary, but I like to try and keep the overall format familiar. I try and stick to something along these lines: 1: Icebreaker – something to get people talking. It could be summarising the sprint in a tweet, writing notes that you wish you'd received at the start of the sprint. Keep it short but make sure it's something everyone can do. 2: Review actions from the last retrospective – two weeks is a long time, so it's important to remind people where we were back then. Did the ideas we came up with fix our problems? 3: Data gathering – gather up the wins and losses for the last sprint, and group things into themes. We're not trying to solve anything just yet, but give everyone a chance to raise issues for the group to consider. We'd then use a vote to decide which are our big issues for discussion. 4: Identify objectives – This is the meat of most retrospectives, so it's important to keep it focussed. The aim is to discuss the 'big issues' in turn. As a team we're trying to figure out why we think they're happening, why this is a problem, and what ideas we have that might fix or improve the situation. 5: Wrap-up – Depending on how many issues we've talked about, there may be anywhere from 5 to 15 improvements that the team has come up with. This is too many for most teams to adopt in a single sprint. I tend to have them pick out no more than 3 which, as a team, we will will try out for the next sprint, and review at the start of the next retrospective. Activities Here's where I like to get creative, and you should too. Doing the exact same thing every week gets very repetitive, especially if you're looking after multiple teams. Even something as simple as changing the questions asked can subtly shape the responses differently. Consider the following questions: What's slowing us down? What could we have done better in this last sprint? What's frustrating you right now? These are all slight reframings of a similar question, but even this subtle shift can force people to think about things in slightly different ways. This alone can shift the mood of the retrospective considerably to keep new ideas surfacing every sprint. General Guidelines Some people love to talk in a small group environment, whilst others find it hard to contribute. Your job as a facilitator is to make sure every has their say and that everyone's opinion is valued. Doing so can be tricky as often you don't want to shine a spotlight on the quiet ones. Choosing activities which favour writing down ideas before reading them out can help in such situations though. As can going round the table one at time for ideas. A lot will happen in the two weeks following the retrospective, so it's important the outcomes are captured somewhere, and that the team remembers them. If the team decided to do something new in planning, remind them of this when they go into planning. Maybe get the team to make posters for the stand-up board so they see them every morning. Of all the Sprint ceremonies I think the retrospective has the worst reputation as, done poorly, they can be seen as a waste of time. Done well however, a well executed regular retrospective will transform any ragtag bunch of developers into a self-governing engineering powerhouse. They're not always easy, but they are the single most important hour you can spend in your sprint if continuous improvement is important to you. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-09-24"},
{"website": "Made-Tech", "title": "Internal vs External Quality of Software", "author": [" Emile Swarts"], "link": "https://www.madetech.com/blog/internal-vs-external-quality-of-software/", "abstract": "Many studies have been conducted in an attempt to formalise the quality of software. Some quality models have been established, like SQuaRE by Consortium for IT Software Quality , which takes into consideration 5 key points: Reliability, Efficiency, Security, Maintainability and (adequate) Size. The difficulty in measuring the quality of software is that software is very rarely at the end of its lifecycle. Systems keep growing and evolving to meet new demands. Eventually these systems will be replaced or discarded, but they will fight to survive for as long as they can. One cannot measure quality based solely on what has already been completed. You have to also consider how your system will cope with the next inevitable change request. We will see how internal and external quality are tightly coupled, and how external quality dictates internal quality (obviously). External Quality (Functional) External quality is the usefulness of the system as perceived from outside. It provides customer value and meets the product owner's specifications. This quality can be measured through feature tests, QA and customer feedback. This is the quality that affects your clients directly, as opposed to internal quality which affects them indirectly. Internal Quality (Structural) Internal quality has to do with the way that the system has been constructed. It is a much more granular measurement and considers things like clean code, complexity, duplication, component reuse. This quality can be measured through predefined standards, linting tools, unit tests etc. Internal quality affects your ability to manage and reason about the program. Is your program able to cope with new requirements easily moving forward? Is your program efficient enough to deal with an inevitable increase in data volume? Is your domain logic decoupled from the framework so that it can be updated without breaking the system? Do you have tests to guard existing functionality? These are some of the questions that need to be answered in order understand internal quality. There are two common aspects of quality: one of them has to do with the consideration of the quality of a thing as an objective reality independent of the existence of man. The other has to do with what we think, feel or sense as a result of the objective reality. In other words, there is a subjective side of quality. – W. A. Shewhart Example Suppose you receive a new requirement, that a product can have a main header image. The code for optimal internal quality would be something simple like this: @product.gallery.header_image It is the least amount of work you can do to provide some external quality. This code works until the user creates a product without a gallery and the program blows up on a nil error. You are forced to increase the Cyclomatic complexity of the code to increase its external quality. if @product.gallery.present? and @product.gallery.header_image.present?\n  @product.gallery.header_image\nend Next you realise that the frontend design does not accommodate a missing header image, and you compensate for this. if @product.gallery.present? and @product.gallery.header_image.present?\n  @product.gallery.header_image\nelse\n  @product.gallery.default_image\nend As you can see, the cyclomatic complexity of this code has increased drastically in order to facilitate the external quality. Unless proper care is taken, this can grow into a mess that has a very low internal quality, inadvertently sabotaging efforts to add external quality in the future. Ensure that this functionality has Black box tests in place so you can refactor its internals without changing its behaviour. Constraints on agility Most clients are unaware of the internal quality constraints on a project. Cost goes up and it can be hard for software companies to justify exactly why that is. It is usually due to low internal quality, manifested in overly complex code. The best thing you can do is to keep your clients informed. Allow them to join the daily standup remotely and share your concerns from an internal quality perspective. We have been doing this for a while, and have even had a client come in once a week to work in the office with us. Needles to say, we have found this to be very productive. Transparency is crucial. Regressions from two angles When refactoring code you have to be aware of both internal and external quality regressions. We have started manually reviewing our features before they are deployed to production . Usually by developers that are not familiar with the project. The author of the code will take the other person through what has been done. First by looking at the code / tests, and then by exercising the system (usually in a browser) in a way that demonstrates its usefulness. At this point you should have sufficient feature and unit level tests to verify basic correctness. Over-testing as a form of low internal quality Even if you have a really comprehensive test suite, if it was designed in such a way that it heavily depends on the implementation details, it will hinder your ability to move forward quickly. Mock heavy tests tend to suffer from this. You cannot change much of the code without breaking a bunch of unrelated tests. Conclusion Obviously providing external quality is the reason why we build programs in the first place. We always cater for external quality first, but have to be concious of the state of the internal quality which will facilitate future growth. If your system is robust, with small flexible components that can be composed in different ways, you will find it easier to add or improve external quality. Good external quality however does not facilitate good internal quality. Both play a vital role in contributing to the overall quality of your program. References Growing Object Oriented Software Guided By Tests Software Quality Internal And External Software Quality Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-09-29"},
{"website": "Made-Tech", "title": "Practical Checkout Flow Enhancements", "author": [" Seb Ashton"], "link": "https://www.madetech.com/blog/practical-checkout-flow-enhancements/", "abstract": "At Made we pride ourselves on crafting websites that deliver a great aesthetic, and a rich user experience. Over the last few years I have spent the majority of my time building eCommerce sites, where some good UX can make or break conversion rates and profitability. In this post I am going to share a few ways to enhance your checkout flow that will have a positive effect. The Address Step Nothing daunts a user more than a page full of form inputs, and within your checkout flow the biggest form – I hope – is customer address entry. We can improve the customer's experience of this step in a couple of ways. The first, and most obvious, way to instantly halve the size of this form is to allow the customer to use the same address for both their shipping, and billing address. From personal and professional experience this is best achieved using a simple check box, which in most cases can be preselected. The second is to remove the address fields altogether and provide auto-complete suggestions based on postcode. There are a few services that can provide this for you. One that we've implemented a few times at Made is PCA (formerly Postcode Anywhere) . PCA provides multiple end points which can be queried to complete a customer's address. The Payment Step The taking of payment is arguably the most important part of your checkout process. Making it as quick and simple as possible for a potential customer will have positive effects on both conversion and customer satisfaction. The obvious way to speed a registered customer through the payment step is offering to save their card details, and using those details for all future purchases. But for those shoppers who aren't returning account holders we can improve the way we collect that data. Stripe have a great open source jQuery plugin which allows you to validate a customer's card type and number. It's up to you how you display this back to a customer, but this early visual indicator will enable them to correct any mistakes in card details before they even submit their payment. This only really relates to mobile users, but another way we can improve the customer experience of the payment step is by enhancing the input types we use in the form itself, and the keyboard that is presented to the user. Traditionally you would use input[type=\"text\"], but as LukeW talks about in this video specifying an input[type=\"text\"] and a [pattern=\"[0-9]\"] will show the number keyboard. But… Why reinvent the wheel? It's quite likely that your customers will already have an account with Amazon or PayPal – I mean, who doesn't? With some integration work, you can implement the checkout with PayPal (or Amazon) functionality. Yes, it completely bypasses your own checkout flow, but is that a bad thing? By leveraging the addresses and card details already stored on the customers preferred payment system, they can get from cart to confirmation in a few clicks. While these suggested enhancements aren't the most arduous to implement, and there are many other ways to enhance your checkout , they will positively impact the journey though your sites checkout flow. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-10-01"},
{"website": "Made-Tech", "title": "Componentisation and the Single Responsibility Principle", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/componentisation-and-the-single-responsibility-principle/", "abstract": "Keeping a good separation of concerns means writing code that only handles as much as it needs to. It's a concept that should affect every piece of code you write, from class definitions to database tables. Only store the data which is relevant. Only encapsulate the logic which is covered by the responsibility of your class. My colleague wrote about this recently when discussing Inheritance and Composition . Keeping a good separation of concerns has historically been tricky in front end code. Having to bind Javascript classes to window for example, and using classes to link together Javascript, HTML and CSS. These are loose ways to tie together disparate code. Ideally code should be tied directly to any relevant aspects. It's gotten a lot easier with client side Javascript; developers are reaping the benefits of small, open source modules more easily through package managers like NPM and Bower , and writing modular, DRY code by employing module loading systems like Browserify and AMD . As Single Page Apps get more popular, though, a lot of the weight of an application is shifting into Javascript, and the rest of the application can feel cumbersome by comparison. It has always been difficult, not to mention finicky, to keep HTML and CSS code modular. At Made we use best practices like BEM (Block, Element, Modifier) rules to 'namespace' DOM elements, allowing us to tie them logically enough to their respective CSS and Javascript. It's a loose system though, and having to operate so naively in a global namespace often leaves me craving the rigid control over dependencies and associations which imperative languages offer. Custom Elements Custom Elements are a major part of the Web Components spec which is making its way natively into browsers today. They allow developers to create custom HTML tags, with custom behaviour, isolated inside a \"shadow DOM\" – its own scope, invisible to any consumer. It involves the creation of an HTML element, and a registration of custom behaviour using document.registerElement. Once a component has been created, it allows you to use it with descriptive, semantic markup like this: When this  tag is instantiated, it creates a shadow DOM, and fills it with whatever content has been put in the gallery template. For example, the template markup may look like this: Notice how simple the selectors can be? No concern needs to be made for global leakage. The title and images would be filled in and controlled with Javascript. This freedom to create in a private space gives developers the same flexibility as developing in a closure, for example, when writing modular Javascript. It doesn't mean Custom Elements follow the 'Angular Approach' of moving all your code in HTML. Custom Elements are initialized in Javascript and have all the benefits of imperative, reactive code. By developing in a private shadow DOM, you are allowed the freedom of custom HTML, CSS and Javascript without the hassle of having to bind together complex classes, data attributes and IDs. Code becomes terse and more readable, and worrying about conflicts with other elements on the page – which is to some degree outside of your control as a developer – becomes a thing of the past. Custom Elements are a new feature in browsers, which recently made their way into Chrome stable. The Polymer project provides a great polyfill for them for other browsers. React React elements, similar to Custom Elements, are developed in a private scope, but in this case, a single language also – Javascript. Although alien, writing and using a simple React component may look like this: The HTML style tags are JSX – and can be compiled into regular Javascript objects, for example by using the reactify Browserify transform. But we all agreed inline styles and scripts were bad, right? I'd argue the benefits of directly, imperatively referencing any styles and UI events directly on a DOM element, rather than through the abstraction of finding, styling or listening to DOM elements by identifiers like classes, outweigh the negatives. React elements are Javascript objects first, HTML elements second. The React library takes care of converting them to HTML, rendering them on the page, and updating them on changes. It actually does this better than doing it manually would, as it diffs changed HTML against HTML which is already rendered on the page, and only changes as much as is necessary, rather than rerending the whole element. For example, it might just add or remove a child element, or alter a style. Aren’t we already close enough? There are hundreds of open source libraries out there which instantiate and control DOM elements for you. For example there are a myriad of jQuery libraries which allow the creation of custom elements like lightboxes, slideshows and validating forms. These have many drawbacks though. They are often dependent on DOM manipulation libraries, canonically jQuery, and are often too opinionated about how they should look and behave. At best you get a pluggable API, but generally you get verbose and unintuitive options objects. They also require manual creation in Javascript, rather than just adding tags to your HTML (or HTML-like) code. Custom Elements differ in that they allow you to use the three languages which you're already using as they were designed to be used. Using HTML Imports, another part of the Web Components spec, Custom Elements can be imported directly into your pages, rather than having to be instantiated with Javascript. Although obviously React demands a dependency, the HTML style syntax means third party components end up being used a similar way. What does this mean for open source? The goal is to have a standardised architecture for reusable, customisable DOM elements. I want to be able to import a component and put it in place as easily as I install a gem, or require an NPM module. I don't think the encapsulation of style is important here. In fact, I often feel UI libraries being particularly opinionated about looks slows me down as a developer. Of course sometimes this makes sense. For example, material-ui includes a huge amount of element styling, but creating elements which look and feel a certain way is the reason it was created. Libraries which offer elements such as mobile navigation menus or autocompleting text fields ought not tell me how they should look. Their ideas generally conflict with my own ideas, and are often troublesome to override. In these cases I'm far more concerned with having the functionality put in place for me, so I can style it to the look and feel of my site as I choose. We need to make a good separation of concerns in HTML and CSS the expectation in open source code, and our own code as well. Too many libraries are breaking the Single Responsibility Principle by pulling more than they can carry. The best UI Component libraries are going to be the ones which concern themselves with either functionality or style. Never both. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-10-06"},
{"website": "Made-Tech", "title": "Testing without QA", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/testing-without-qa/", "abstract": "We have an almost continual dialogue on how to improve the quality of the software that we're involved in delivering. One of the conscious decisions that we've made is that we don't make use of a dedicated QA or test role. That's not to say that we don't believe in testing software, or in the value that a strong tester can bring when embedded in a team. However, we do often see a number of common challenges when leveraging a dedicated test role. I'll share a handful of things to be mindful of, should you be considering bringing a tester on board, as well as some of the techniques we use to keep quality high without a test role. Common challenges with a dedicated test role Decrease in development quality We've often observed that there's a steady reduction in development quality when a tester is brought on to a team. By having the comfort of a safety net between a commit and production, there can be a subconscious relaxing of development standards – perhaps not considering edge cases, or bothering to fire up another browser or device to run a quick test. Increased cycle time There's a well cited cost of defect graph or matrix that has been doing the rounds for decades in software engineering circles. Effectively, it'll show you (very crudely) something like: As soon as you've got a second party involved in the feedback loop, the cost, or time, is increased by several multiples than in cases where the originating engineer resolved the issue as part of delivering the feature. Push the defect closer to production, and the cost keeps on multiplying. Externalised testing While it might be cheating to bring up externalised testing in a post that is predominantly dealing with embedded testing, we can observe cases where embedded testers can create externalisation issues. If the testers aren't involved in both feature planning and subsequent discussion that happens during implementation, a significant overhead can be added to your engineering team to explain how a feature is now supposed to work and to triage incorrectly opened defect reports. How else do we keep quality high? On most teams, we see peaks and troughs in quality. Typically you'll have a handful of particularly stable deliveries, after which it can be easy to become complacent, gently allowing quality to drop in the process. To avoid this, there are a number of behaviours that can be encouraged. Continuous Integration Regardless of how you're performing the more human element of quality assurance, we're going to need to assume that your team are practising some combination of Test and Behaviour Driven Development, and that these tests are being executed against the codebase every time a change is committed. That's to say, that as an inherent part of the software engineering process, you're cultivating a suite of automated test that are both helping to drive the design of your application, as well as providing a useful regression suite, to keep check that the features you're building today continue to work alongside the changes you make tomorrow. Pull requests Another code-level feature that we often find beneficial is to use pull request style development , particularly where the team are immature to pair programming, or where the nature of the workload doesn't necessarily lend itself well to pairing. Having another pair of eyes check and give 'approve' the change can be beneficial. If you adopt this workflow, you need to keep on top of pull requests: not letting them linger for more than 20 minutes or so. You'd also be well advised to discourage bikeshedding , focusing the team on truly valuable feedback. Peer review Where pull requests can provide a valuable second pair of eyes at the code level, having a second pair of eyes run through some more manual testing and verification can be equally valuable. Where we see this differ from the dedicated test role is a subtle one. By 'troubling' another engineer or team member to help validate a feature before it's moved to a pre-production or production environment, there should be an expectation that they're going to be able to give the feature a quick tick. We observe that teams should have a low tolerance for picking up sloppiness in one another's delivery. Pair programming We've seen pair programming offer up many benefits to teams. One of which is improving the quality of delivery. Having two pairs of eyes focused on solving the same problem, the differing perspectives can lead to more elegant and better thought through solutions, generally with more edge cases identified and mitigated against. While we can, and have, seen good value in properly embedded and experienced testers in teams, we often see that the drawbacks can outweigh the advantages. Cultivating a strong culture of discipline in software teams remains possibly the biggest challenge facing engineering leads. However, we see that the dividends it plays throughout the delivery, in quality and many other areas, is worth the ongoing investment. As a final piece of advice, we'd strongly suggest being wary of outsourcing any sort of test capability – particularly if you're trying to deliver software with any kind of speed. The overhead in slowing down your delivery cycles, and the communication overhead it can place on your engineering function are seldom worth the returns – particularly if you do observe any quality drop as part of the perceived safety net. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-10-08"},
{"website": "Made-Tech", "title": "Agile Problem Solving: Moving as a Team", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/agile-problem-solving-moving-as-a-team/", "abstract": "We're a company dedicated to producing quality software our clients love, and we're constantly striving to refine and improve the processes we use to do so. With that in mind, getting things \"Done done\" is a massive priority for us, and anything that hinders that ability is something we will always work to remove. On a recent project with a relatively large development team, we noticed that quite a few things were being \"done-ish\", but not \"done done\". Oftentimes those things were 95% of the way, but needed just a little more work to get it across the finish line. Recognising this, we knew we had to rectify the situation, in a potentially drastic way. A little bit of backstory As a software development company that practices the Agile methodologies, particularly Scrum , we continually work with clients to break their project down into stories describing a particular feature, and then we break those stories down further into subtasks. After assigning a story point value to each of those subtasks, we agree with the client on the total number of story points we will deliver in a sprint . Typically a sprint lasts for two weeks, but can be anywhere between one and four, depending on the project and the client. To monitor our progress during the course of the sprint, we use JIRA , an issue tracking tool. The stories, including their subtasks, are added to JIRA, ready for developers to come along and assign an issue to themselves to indicate that they are working on that thing. When we've begun work on a task, we move it into the 'In Development' column. On the project in question, we'd set JIRA up to have columns representing each environment in the build pipeline: development, continuous, staging and production. Once the developer working on an issue had verified the issue in each in environment, they could then drag the issue into the next column, ready to verify it again in the next environment. The Problem A few sprints into the project, what we found was that, despite progress going noticeably well, there were enough comments coming back from the product owner that we realised that certain tickets hadn't been thoroughly checked, that they hadn't been \"done done\". More seriously, we'd failed to complete a couple of sprints because we'd had to re-open issues and spend more time addressing comments that had been raised against them. The whole team sat down to discuss the issue, and to find out what we could do to prevent this from happening. Two things quickly became apparent: there was a lack of any kind of code review process (we hadn't yet adopted the pull request approach Fareed recently wrote about , in fact, our solution acts as an alternative to that), and the volume of issues to be completed in a sprint, along the free-for-all approach to picking those issues up, led to developers allowing themselves to become distracted from making sure the issue they were working on was 100% complete before trying to move on to the next one. The Solution Individual commitment to a group effort – that is what makes a team work, a company work, a society work, a civilization work. – Vince Lombardi The first thing that we picked up on was the arbitrary way issues were being picked out by developers: a developer would pick an issue from a story, and then maybe something would block their progress. They'd leave the story in the 'In Development' column and then, without much discussion, pick up another issue and begin work on that. This was clearly not useful. What we decided was to ensure at all times that our JIRA board reflected what was actually happening, so if we were working on something locally, it was in the 'In Development' column, if it was in one of the other environments, it was in the relevant column. On the other hand, if progress had been blocked for some reason, the developer was to discuss it with the others to try to resolve the blockage, and if that still didn't help (such as when we were being blocked by external services), it was moved back to the 'To Do' column and flagged for the attention of our Scrum Master. Another major turning point for us was to limit the amount of stories we could have \"In Development\" at any one time. The problem with our previous approach was that developers were tending to work in a siloed way, each of us cherry picking issues from a number of stories to the point where there could be as many stories in progress as there were developers. What we needed was for the whole team to take responsibility for each story, making sure that, when a story was started, we all go out of our way to get that story completed and verified, confidently knowing that it is ready to go to production. We did two things in JIRA to help us with this: The first was to limit the amount of stories that could be \"In Development\" to 2. If we went over this limit, JIRA would paint the column a nice deep red to make sure we were aware we'd crossed into the danger zone. Each story might have a number of subtasks, which usually meant there was plenty of work to go around when beginning a story. The second thing we did was to add a \"Verified\" column. On this project, deploying to production regularly wasn't a luxury we had, so we had to know that what was in staging was at least production ready. The rule with the \"Verified\" column was the developer assigned to the issue was not allowed to move the issue into this new column, they instead had to get independently verified by another developer. The benefits we noticed from implementing and strictly adhering to these two rules were immediate and massive. One of the first things to happen was that there would be a pair of stories (meaning we were at our 2 story limit) with several subtasks, enough for everyone to go away and work on. Some of those issues would naturally require less time/work than others, so the developer working on such an issue would now be unable to pick up another issue in a third story, as the initial two stories had yet to be completed. This directly led to a huge increase in the amount of pair programming we were doing; suddenly each developer was invested in getting stories they might never have worried about over to the \"Verified\" column, so that they could progress onto the next story. With a second set of eyes on the issue, pairing meant we were able to move much more swiftly through a story and whilst ensuring the quality of the code being written. Pairing wasn't always the solution though, and the other effect these rules had was on instilling a sense of urgency that wasn't necessarily present before. We now were communicating much more regularly to check where everyone was on a particular issue, with a view to getting that story completed. Skype/IM messages can be ignored, so we communicated verbally to make ourselves heard. It meant we had a very active forum in which to ask for help or raise concerns if we had them, and when we ran into a blocker out of our control, we would make the decision to put the whole story back in the \"To Do\" column until it could be resolved. Ultimately what happened was that we moved much faster, as a team. It bears mentioning that there were instances where the 2 story limit was simply too low, such that we had to begin work on a third story out of neccessity e.g. when one story's issues could only be handled by one particular developer (in this instance the only one of us with any knowledge of a specific .NET application). In exceptional circumstances like those, the limit must be broken for progress to continue to be made; the \"In Development\" column will turn red for all to see and feel pressured by. I'm not suggesting more pressure is a good thing, but there's a palpable sense of relief when the column turns back to its original colour and we know we're not breaking our own rules. Wrapping up The takeaway for you, the reader, is that it may be worth introducing these two rules to your projects: 1: Limit the amount of stories you have in development at any one time. The exact number will naturally vary according to how big the size of your team is, but we'd suggest no more than 4. 2: A story can't be closed until all the subtasks within it have been verified, and a subtask must be verified by someone other than the developer(s) that worked on it. Smaller development teams of 2 or 3 developers might not feel the benefits of this approach, in which case adopting the previously mentioned pull request method of code review might be the best solution. On the flip side, individuals in larger teams are likely to benefit from increase in productivity, communication and, most importantly, the sense that they are part of a team . Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-10-13"},
{"website": "Made-Tech", "title": "Comparing OOP and Component Based Design", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/comparing-oop-and-component-based-design/", "abstract": "In this article I'd like to discuss two concepts that you might not immediately think to compare. I've written previously on keeping your stylesheets modular and also on boundaries in object oriented design , so today I bring the two loosely together. Object Oriented Programming provides the ability to organise our programs or applications into objects. Objects represent real life things and also computationally important things. They can represent a product, a cart that holds products or the algorithm to find relevant products for a specific user. Object design is the drawing of boundaries between the logic of our application, and how we can then envisage the interaction of such logic. When designing with objects, we can visually imagine how they interact with each other. This broke with previous ideas of procedural programming where we were forced to think of programs as a continuous operation or rather, one big list of commands to be executed. If we wanted to devise yet another address book application, we would need to encapsulate the ideas of an address book in code. We would need to think about what real world objects physical address books have. We first of all have the idea of an address book. The address book will then have a number of entries. They have entries that may have one or more addresses. Each address could be made of smaller objects such as an address and phone number. Splitting out our code into objects that represent real world \"things\" allow us to discuss our code more easily. I can't imagine what it was like describing a piece of procedural code. \"Oh I think the if statement on line 45 for adding an item to the address book table is not quite working as expected\" versus \"The 'addentry' method of AddressBook isn't working as expected\" Abstraction is great for making things easier to understand whether you're reading a class or discussing one with a colleague. Let us leave objects for a moment… We now find ourselves in the world of user interface design; if you're a backend developer just imagine a wonderful world of vivid colours and shapes. More usefully, component based design is a concept of modular UI design. It's a bit like stepping from procedural to object oriented code. To explore this concept we first must describe the age before. Typically when building a new feature for a website a designer would provide us with a mock up of the web page containing the feature. As a developer I would then begin to build out that page within the app in my development environment to the design provided. This change would then be deployed out to production. Component based design changes this process, though a designer may still provide a mockup of the page with the feature. The difference is in implementation by the developer. Instead of building the feature directly into the application, the UI of the feature would first be built into a style guide, sometimes known as a pattern library. The design would be broken down into individual elements. For example, if a feature required a table, the table would be added to the pattern library as an individual module. Below is an example of Mailchimp's table from their pattern library . For each visual component in a feature the same process is applied until the entire UI for the feature is documented as a set of modular components. Some teams will then mock up the entire feature in a HTML page using something like pieces for rails . This is essentially a HTML and CSS prototype that can be sent over to the designer to get their take on the implementation. Once all parties are happy, the modules can be used within the application to implement the feature. Component based design is leaps and bounds ahead of the old page based design since a byproduct is a reusable set of UI elements. The pattern library can be reused in a number of ways to implement new features without necessarily requiring an entire new design. Designers often had a file in Photoshop or Illustrator without their UI widgets. The pattern library is essentially one step closer to implementation in the app since they are made from HTML and CSS. Very much in vogue at the moment, various conversations and solutions are gaining popularity such as Atomic design , BEM , styleguide driven development and Twitter's Bootstrap . Component libraries are themselves a resource and product for clients. By producing a library of components, clients can later chose to build their own pages reusing these resources. Such libraries are becoming a new currency in client relationships for UI and UX. Instead of the old Photoshop .psd we're selling the building blocks for unimagined structures. Open source component libraries are popular too. The aforementioned Bootstrap library by Twitter is perhaps the most predominant one finding its way into all kinds of admin interfaces, CMS's, documentation and open source sites. One of my own hopes is that in the future we can work even closer with designers to build these libraries of components by iterating on mockups with HTML and CSS. We'd get the first ever mockup from a designer, we'd implement this in HTML and CSS, then extract out into the library and by the time the next feature comes around the designer can consult the pattern library and produce mockups using them. If the designer could maintain such a library within the application, that would be a dream! There are so many benefits from this way of working, there's a reason pattern libraries / styleguides are getting more popular. Being able to describe UI components by name will make conversations with designers and developers much easier just like objects do for code. So too will be pulling these components together into mockups that can be iterated on without implementing any backend code at all. Having reusable UI modules that can be mixed and matched mean we can reuse components more regularly, and we can save time! Again this is just like OOP. We get the added benefit of our components being decoupled making the process of updating them less risky since they won't be leaking into other areas of the design. When we update a component we can simply interate within the pattern library ensuring all the states of the component still look okay. It's pretty much REPL driven development for UI! I'm rather excited about all this 🙂 Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-10-20"},
{"website": "Made-Tech", "title": "Choosing the right supplier post IR35 reform", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/choosing-the-right-supplier-post-ir35-reform/", "abstract": "With the latest reforms to IR35 having come into effect on April 6th, public sector bodies (PSBs) are facing the mounting challenge of keeping teams filled. This, coupled with the Civil Service-wide recruitment freeze introduced in 2010, is a particular problem for PSBs tasked with delivering digital projects, where software engineering teams were once largely made up of contractors who now have an incentive to look elsewhere for work. The reforms are here to stay, so what options do PSBs have in terms of continuing to deliver good digital products? The Story So Far Prior to the IR35 reforms , the recruitment freeze meant that PSBs couldn't take on new employees. However, there weren't such restrictions on bringing in contractors. Hiring an employee means incurring a certain amount of cost and risk, both in terms of onboarding them and hoping that they're a good fit for the organisation in the long term. Contractors don't come with that baggage, in the sense that they're typically not given the same benefits as an employee, and contracts can be as short as a couple of months. The IR35 reforms came about as a reaction to the increasing number of contractors who were having their contracts continually renewed, to the point that they were essentially full time employees who had the advantage of being able to pay less tax through various means. With IR35, end clients now decide how a long term contractor is taxed. For such contractors, that's not an appealing idea, resulting in many contractors pursuing new work . Now what? PSBs delivering digital products have the unenviable task of figuring out how to get a skilled team working on those products again. The recruitment freeze makes it difficult to bring new people in for the long term, and IR35 means skilled lone contractors will, at best, be around for a matter of a few short months. This leads to high team turnover, and such instability is a danger to software projects – without a consistent driving force leading development, software projects can become unwieldy, difficult to understand and, worse, much more costly to maintain and evolve in the long term. One option is to bring in a dedicated software supplier. Software suppliers provide teams of skilled engineers who can deliver high quality projects, whilst building and maintaining a successful working relationship. The question is: which software supplier is right for you? Choosing the right supplier Deciding which software supplier to go for is not a decision to make lightly. Ideally, you'll be working with the supplier you choose for months, possibly years. The worst case scenario is working with one who simply isn't a good fit. Such a situation may end up costing you more money and time in the long term, as you seek to replace them and repair any damage done to the project. It's crucial to find a supplier that you, to the best of your ability, are able to discern will be a benefit to your project and wider organisation. We believe there are a number of key areas to look at, to give you the best sense of what it will be like to work with a given supplier: Location Though there's often a lower upfront cost, we've spoken at length about the challenges of offshoring development . From differing working hours to problems in maintaining a good line of communication in order to ensure a consistently high level of quality, keeping the development of a project close has many benefits. With a local supplier, though there is admittedly likely to be a difference in cost, there are several benefits. For one, having them nearby makes it infinitely easier to feel them out over a period of time. As you're not incurring significant travel costs, you're able to make repeated visits to their offices, and vice versa, to really get a sense of the people you may end up working with. A good supplier will also allow and even encourage their team to work on site with you. This gives you the chance to strengthen your relationship with the supplier further, on an individual scale. For the team, it gives them the opportunity to become properly immersed in your organisation's culture, and become even more invested in the work they're doing. Questions to consider: Taking geographic distance into account, how simple is it to communicate with the supplier? Does the supplier endorse colocation, as a means to building a stronger working relationship? Communication Having a supplier close by means it ought never to be difficult to get real face time with the team building your project. Even so, you need to be sure a prospective supplier can communicate well., from initial meetings through to development and, ultimately, completion of the project and beyond. For instance, if you're working in different locations, you need to know you can quickly get hold of them via email, a phone call, or a video chat. Look for a supplier who places an emphasis on collaboration. You'll have goals for the project in mind, but a good supplier will discuss those goals with you and constantly try to find ways to improve upon them for the benefit of everyone involved. They'll include you in planning meetings to capture as many requirements as possible, and they'll regularly, every week or two, showcase the work they're doing to your organisation's stakeholders. You'll be able to give feedback during these sessions, and the supplier should be willing to discuss any concerns you have. Questions to consider: How easy is it to get in touch with the supplier? Are they quick to respond? How well does the supplier communicate with you during initial meetings? How do the suppliers intend to keep you involved and abreast of progress during development? Team consistency As mentioned earlier, keeping a team consistent on the development of a digital product is vital to its success. When the majority of the team have been involved from the beginning of the project, there is a clear, shared goal that is easily understood by each member. When bringing in a third party supplier, look for teams who can guarantee a sense of continuity throughout the engagement. Though you're working with a company, you'll ultimately be building relationships with the individuals within, and you need to know those individuals won't be rotated off to help their company deal with a new project. Questions to consider: Can the supplier guarantee you'll be working with the same people throughout the engagement? Is the supplier keen to build a strong relationship with your organisation, or do they just want another project on their books? Experience working with large organisations Digital transformation is currently a priority for the Civil Service , with the aim being to build services that are simple to use. However, the bigger the organisation, the harder it is to introduce the changes required to build such services. Agile practices can help, but how do you convince your team to adopt them? Any experienced supplier will be able to guide you towards the practices that will benefit your organisation most. There is a lot of material on the subject of Agile, but it's not a one-size-fits-all methodology so, for it to succeed in your organisation, it needs to be tailored to your needs. In smaller organisations, this is a lot simpler. For large organisations, it's a lot trickier as it will inevitably affect many more people and established processes, and could become quite political. You need to know that the supplier you choose is equipped to deal with this level of responsibility, that they've helped brought about such change in organisations of a similar size. Questions to consider: Does the supplier have experience working with large organisations? Can the supplier help your organisation adopt agile practices? The IR35 reform has presented new, large obstacles for PSBs building digital products to overcome, but they're not insurmountable. Small in-house teams and a rotating roster of short term contractor may get the job done, but the risk of producing something suboptimal is high in such situations. Though just one solution, working with a good software supplier ensures that the product being built has a great team developing and supporting it. Take the time to get to know potential candidates, they'll be working with you for a long time and you need to know it's going to be a productive and successful relationship. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-05-04"},
{"website": "Made-Tech", "title": "Privilege Bingo", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/privilege-bingo/", "abstract": "Intersectionality, BAME, cis, anonymous hiring, inclusion. These words are likely not on your ticket if you are playing privilege bingo. Like a canary in a coal mine, a lack of understanding of these concepts will likely mean your daily actions reinforce systemic discrimination. We aren’t good at diversity and that is something we need to work on as a business, as an industry, and as a collection of humans. Let me be a little clearer, Made Tech isn’t good at diversity. There are direct and indirect effects caused by our policies and our privilege that actively work against us building a more diverse team. We didn’t set out to avoid diversity, but our inaction and lack of ambition to tackle it have made it worse. We haven’t placed any proactive efforts into improving the situation, the fault is entirely ours. Awakening from ignorance Ignorance is bliss right? It’s certainly easier but we cannot plead ignorance here. I think ignorance has been the goto excuse for us but through slow osmosis, from forgiving friends offering words of advice, and by growing noise about diversity, we are slowly waking up as an organisation. The first step toward change is awareness. The second step is acceptance. – Nathaniel Brandon The first step has to be awareness of the fact we’re poor at diversity. I mean it’s hard to argue looking at the fact we’re almost all white guys. The next step is acceptance, a willingness to come to terms with reality without protest or reaction. Lastly, action. Identifying smells Historically we’ve used a combination of recruiters and adverts, along with referrals from our existing team to find our new hires. We’ve seen very few women and non-white candidates apply through these mediums. This in itself is a smell that hasn’t entirely gone unnoticed in the business. I’ve heard and have raised questions around why we see so few women applying but we’ve never taken it upon ourselves to try and find out the underlying problem. The smell was left to linger. Another smell that I became aware of from a blog post was that of language in job adverts. Some of our adverts have sounded like military job ads with phrases such as “come armed with an opinion” along with exclusionary phrases like “It’s probably not a Windows PC”, “work hard/play hard”, and “it’s likely Made isn’t the place for you”. These went unnoticed for a long time. Our refer a friend scheme is another smell. Why did we implement that scheme? We did it because we are always hiring, and hiring people recommended by the existing team is less risky than hiring someone unknown to the company. Unfortunately asking white guys to refer their friends and ex-colleagues leads to a lot more white guys applying for jobs. It’s not that we wanted to encourage more of the same ethnicity, gender, or other intersections, but the reality is that this kind of scheme does just that. Unknowingly we’ve been financially discouraging diversity, around 15% of our hires in the last 2 years have come from referrals and that figure is closer to 30% over the entire history of the business. It’s not that taking away this scheme will suddenly improve the lack of diversity at Made Tech but removing it is certainly a step in the right direction. In the future when we have a more diverse workforce, bringing this scheme back could in fact help maintain our diversity but right now it’s only working against it. What caused us to look at these areas through a diversity lens finally? An increasing internal and external pressure, from some members of our team and from our customers and advisors, has been drawing our attention to our poor levels of diversity. We’re slowly but surely becoming more and more introspective. Introspection has to be the place to start. Accepting failures The tougher part is to gain wider support for change. Coming back to privilege bingo, when a team isn’t educated in areas of diversity and are unsure what they can do about it, making diversity a high priority is difficult. Sure, diversity is gaining wider attention and people are talking the talk, but walking the walk is a whole other thing. One of the most challenging things is to avoid getting into ideological debates. I find it very easy to slip into an ideological argument that is then held immovable because of the moral foundations I’ve made the argument on. Arguing that it’s the right thing to do and that those not helping the cause of diversity are actively working against it will only make it harder for others to join you. If there’s one thing I’ve learnt it’s to step around hardline views and do not make any yourself. To be honest, I’m still learning this one. Gaining wider acceptance, I think, is a slow and subtle process. Having leaders, including but not limited to management, admit problems is a good starting point. Having these “early adopters” start a dialogue with the wider team helps increase awareness and is especially effective when it’s both peers and management talking about diversity and it’s challenges. Steps in the right direction If you don’t like something, change it. If you can’t change it, change your attitude. Don’t complain. – Maya Angelou Widening your hiring pool is a great place to start when trying to tackle diversity. We’ve started working with diversity recruiters to gain access to pools of people outside our usual networks. We’ve also begun using platforms like hired.com that provide anonymous modes where names and pictures of candidates are hidden from us when selecting candidates to stop our conscious and unconscious biases from impacting who we select. We have removed our refer a friend scheme and although it may not have an immediate impact on diversity we hope it shows a commitment to tackling it. Small actions like this help us show those inside and outside the business that we are pushing in the right direction, finally. Addressing diversity challenges can start to build more credibility around the fact it’s a problem and is solvable. It becomes harder for people to argue that the lack of diversity in the business is out of our control when we begin changing it by taking certain actions. Once you reach this point you are providing evidence that it’s a lack of action that causes such an industry wide phenomenon. We are in the early stages of addressing our diversity problems, still in the process of acknowledgement and acceptance, but with some small actions we are hopefully taking ourselves in a better direction. Helpful links Bytemarks description of their anonymous hiring process Why gender diversity isn’t enough Intersectionality Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-05-24"},
{"website": "Made-Tech", "title": "How to Create a Better Rails Development Environment with Docker Compose", "author": [" Fareed Dudhia"], "link": "https://www.madetech.com/blog/creating-a-better-rails-development-environment-with-docker-compose/", "abstract": "I've seen quite a few articles recently detailing the steps to creating a simple Ruby on Rails development environment without Vagrant. I've found a few issues with these that make the environment somewhat unfeasible for real use. Hopefully, by the end of this article you will have a Docker-based development environment that can actually be used for real development. Today we'll be using Docker and docker-compose to create a development environment around an existing Rails app. For this tutorial, we'll be linking the app's container to a separate db container to create a mini container cluster. This could then be extended to include memcached, redis, or other services so that your development environment can better replicate your production environment. Don't worry: it's not as hard as it sounds. Requirements A computer running a recent version of Mac OS X. I believe that the Docker toolbox works on Windows, but the steps may have to be adjusted if you are not running OS X. Git. This is needed to clone our Rails app repo. If you choose to use your own, you may have to modify the Dockerfile to apt-get more dependencies. Luckily, no previous experience with Docker is necessary. Setup Download and install the Docker Toolbox . Open your terminal of choice. Run the following command to bootstrap the required env vars: eval $(docker-machine env default) Clone our repository down and enter the folder: git clone https://github.com/madetech/rails-docker-dev.git\ncd rails-docker-dev Use docker-compose to build the app: docker-compose build This downloads the images the app needs from the Docker registry and creates the containers to run our app. Go and make a coffee. If you don't already have the images (which you probably don't), it will have to pull and build a LOT of stuff, which will take a quite a while. If this stuff hangs for a long time, try Ctrl+C'ing out and using docker-machine restart default to refresh the state of your VM. Now it's time to launch the app. This will pull the db image if you don't have it already. docker-compose up Once that's done, have a look at that pretty cluster output! NOTE There's an issue with docker-compose 1.4.1 that means this output might cut out at Attaching to. If you get this issue, run the following command to upgrade docker-compose: sudo pip install -U docker-compose Open a new terminal tab or window. Use the following command (again) to get the env vars you need. eval $(docker-compose env default) You can now use the following command to get the ip of your docker host: EXAMPLE:\n$ echo $DOCKER_HOST\ntcp://192.168.99.100:2376 Then you can visit that URL in the browser on port 3000 to see your app running! EXAMPLE:\nhttp://192.168.99.100:3000 It will tell you that you haven't created the database yet. So, in your second tab, cd into the same rails-docker-devfolder we've been working run the following command. docker-compose run web rake db:create Now refresh your browser window and you should see the app running! For future convenience, you can then add a line to your /etc/hosts file to access your app more easily. Example: 192.168.99.100 dockerdev Replacement Functions As we're developing within Docker containers using docker-compose, we'll need to use a few different functions instead of our usual ones. I've included a few examples below. Conclusion It's that easy! It's the quickest way I've found to set up a fully operational Rails app with database on a fresh laptop. No vagrant, no chef, no complicated build process, just pull and go. Next time we'll be diving into how this stuff works, and if there's any way to simplify this process even further. Stay tuned. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-10-15"},
{"website": "Made-Tech", "title": "The Strawman: TDD", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/the-strawman-tdd/", "abstract": "There have been a few rumblings recently between the subculture of TDD-lovers and the rest of the programming community, as always. I’ve heard reports that TDD doesn’t work, that it is snake oil, and that there are studies to suggest that TLD and TDD are no better than each other in a scientific study. So, let’s dig a little deeper. “TDD doesn’t work because you end up with more complicated solutions because you need to mock.” I’m going to use the phrase “Test Double” to refer to the concept of a stand in for the purpose of testing, as for me a mock is a “true mock”, which is only one type of test double. Test doubles are an interesting one for me – sure, there are times you need to use test doubles. I also will say using test doubles is bad. So, if I say that you should use test doubles, but it’s also bad then am I just making this up? Am I another one of those useless “TDD Consultants”? To answer this question you’ve got to understand the single responsibility principle, and specifically when you cross into a clearly different family of behaviour. For example, your business logic can use a stand in database component. Having your code separated allows your business logic to own business rules and your database logic to own database rules. “You can’t use a stand-in database component as it won’t work like the real thing!” Sure, it may not, but then I’d say this is a smell of something else. Those database components should construct entities (Domain objects in DDD lingo). The interface that these objects provide should not expose the underlying database. More often than not, they know nothing at all about the database. Sure, the database component can construct and receive them as a message, but that’s not the same as being ActiveRecord. What does this mean? It means your database tables and schema can be entirely different to the entities that you use to model the domain for your business logic. Decoupling each of these concerns from each other allows you to evolve them independently. Oh, and on the topic, that test doubles may not act like their real counterparts… Since the test double exists to allow you to check the behaviour of the code that uses the test double. If you made a test double that doesn’t match the production version, did you understand the production version fully? At this point it's probably worth mentioning that I rely on acceptance tests to as an integration suite to test against the real database, this happens without the UI. So here you should notice meaningful differences between production and your test doubles. Here lies the reason why you should limit the number of test doubles: they are a pain to manage. So only use them when crossing a boundary into a new concern that should be decoupled. This could be into a new family e.g. business logic is decoupled from database code, or into a new bounded context e.g. a use case for the payments area of the system should be decoupled from the user accounts area of the system. Another way to state this is that tests should exist to test for a single responsibility. As we know, responsibilities are reasons to change. Who requests changes? Hats (also known as actors). For example, in a financial system, there may be the following actors exerting influence on the software: accountants, tax authority, operations, sales, customers, to name just a few. If you want a modular, flexible system, it is important to separate tests (and indeed business logic) along these responsibility boundaries. I’ve seen systems with lots of unit tests and indeed feature tests, but with the boundaries and responsibilities willy-nilly scattered all over the place. These systems become complicated and hard to change very quickly. “TDD doesn’t work when we try to solve problem X so we can’t use it for problems A, B or C” I’ve heard that TDD cannot be used to write certain types of algorithms because you will end up with a brute force solution. I think TDDing into a brute force solution is a very real end game of TDD. Let's play this out. You have a brute force solution, a pretty decent test suite and probably quite a nice API around the code. Brute force can sometimes be enough: optimise last. There are tonnes of brute force algorithms that I’ve written that are running in production. Why? They’re fast enough. So I’m not going to spend effort making the production code unnecessarily complicated to make it faster for no good reason. In the cases where optimisation is necessary, you should be able to refactor the algorithm with the test suite acting as a safety net to verify you still have working code. You should be able to do this without touching your test suite. As a sidebar, though, under-performing algorithms can usually be avoided by being disciplined enough to apply the Transformation Priority Premise (TPP). Which, for example, when TDDing a sorting algorithm, means the difference between ending up in a bubble sort vs. a quick sort. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-05-18"},
{"website": "Made-Tech", "title": "Otto: truly a successor to Vagrant?", "author": [" David Winter"], "link": "https://www.madetech.com/blog/otto-truly-a-successor-to-vagrant/", "abstract": "Otto is marketed as the successor to Vagrant, the development tool that has been used by developers across the globe for the past five years. When Vagrant was first released it took the industry by storm because it allowed self-contained virtualised development environments that didn't require you to worry about installing tools on your computer's actual operating system. It also allowed you to match your production environments more closely by being able to mirror the operating system and by provisioning the Vagrant machines using the same tools you'd use for production servers (Ansible, Chef, Puppet etc). Five years is a long time in terms of technology. Docker has come along in this time and allowed for a similar concept of self-contained environments for your apps. With Docker, you could use the same container/image to deploy straight out to production to achieve better development/production parity. Tools like docker compose are helping to increase the rate of adoption of Docker. PaaS providers have also grown in this time. Heroku is still as popular as ever, and with tools such as Wercker gaining traction, the line between development and production environments is blurring even more. If you haven't already, go and read my colleague Chris's article on IaaS vs. PaaS Otto, as well as claiming to replace Vagrant, also intends to make the jump across the line between development and production a much easier one to tackle. Using a mixture of Hashicorp tooling , you can develop an application, create infrastructure and deploy your application all with one tool. It looks very promising, and as only a 0.1 version was released very recently, we can expect lots to come and changes to be made as the developers receive feedback. In this first look article, I'm going to focus on the development side of Otto and how it currently fares as a Vagrant replacement in developer's day-to-day work. Successor to Vagrant? Under the hood Otto is still using Vagrant. When you setup Otto using otto compile it will take a stab at detecting the type of application you're developing; Ruby, PHP, Node for example. I was playing with Otto using an existing Ruby application with the intention of being able to convert it to an Otto project. Otto will create a bunch of setup files within a .otto directory in your project. For the development side of things, this means it creates a Vagrantfile that uses Shell provisioning to install the tooling for the project, such as Ruby and bundler . If your project requires any packages or configuration other than those specified in the predefined application type, there is currently no easy way of changing this. The two options you have is to specify a custom application type that uses a Vagrantfile with your specific needs, or, *gulp* , ssh into the machine and install manually with apt. Specifying a custom Vagrantfile would also then give you the option to detail any other customisations, such as memory and CPU values, which can't currently be done via any of the Otto customisations. This was a frustration I ran into when I was trying to seed a database with data. Currently, if needing to specify a Vagrantfile for use with Otto, I don't see how it can currently replace Vagrant itself. Mitchell Hashimoto has commented on Hacker News regarding this point: Otto is a lot of magic, we're not trying to hide that. It is a magical tool. But we also give you access to the details (OS, memory, etc.) using \"customizations.\" We'll improve customizations and increase the number of knobs over time. – Mitchell Hashimoto So if your project uses MySQL rather than Postgres—which comes installed by default—then you're currently in for a rough ride and it might be best to hold out for the project to mature. Hopefully it's not a big deal, as it should be just a case of adding libmysqlclient-dev to the Otto application setup so that the mysql2 gem can communicate with a MySQL server. Assuming that the MySQL dependency does get included in the project, the next step would be for you to define MySQL as a dependency of the app. The Otto documentation has an example of using Mongo, so adapting that for MySQL was quite simple by creating the following Appfile inside a new directory I created at private/otto/mysql/Appfile: application {\n    name = \"mysql\"\n    type = \"docker-external\"\n}\n\ncustomization \"docker\" {\n    image = \"mysql\"\n    run_args = \"-e MYSQL_ROOT_PASSWORD=secretpassword -e MYSQL_DATABASE=localdev -p 3306:3306\"\n} This uses a docker MySQL image that Otto will configure with some environment variables to setup the machine, such as the root password and an initial database to create. Then back in our main Appfile we reference the dependency like so: application {\n  name = \"testproject\"\n  type = \"ruby\"\n\n  dependency { source = \"mysql\" }\n} You can then start the development environment with otto dev and updating your database.yml file to reference the MySQL host with the hostname of mysql.service.consul. This dependency configuration all seems very similar to the way docker-compose and wercker are setup. However, Otto seems currently less flexible, though it is early days. Otto currently lacks multiple environment support, where you might wish to have a staging version of your project, though this is being introduced in the upcoming 0.2 version . Making changes in Otto currently require a destroy and rebuild of the entire machine. So you don't currently get the speed of a Vagrant provision as you might be familiar with in your current setups. Once you have an Otto environment up and running, you are ready to otto dev ssh and run a bundle install and get your server running. Run otto dev address to get the IP address assigned to the machine and then you can visit it directly in your browser. Continue developing away at this point. Otto is currently a 0.1 release. It's nowhere near ready to replace Vagrant in most developers day-to-day work, especially if you need to deviate from the supported App types . But it's given us a good idea of what is in store. When plugins are introduced in the next release, a lot of the quirks and hurdles I encountered I'm sure will be ironed out. We'll be keeping a close eye on it as it develops. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-10-22"},
{"website": "Made-Tech", "title": "Canary Releases", "author": [" Richard Foster"], "link": "https://www.madetech.com/blog/canary-releases/", "abstract": "Continuous Delivery is an approach to software delivery which promotes small incremental releases rather than huge iterations. Every push to master which passes its test suite is considered to be production ready, and as a result our deployment pipeline is optimised to get it out to production websites as quickly as possible, without sacrificing security and safety. This benefits us as developers and our customers in the same ways. New features and bug fixes are deployed rapidly, and code is safer by virtue of having smaller changes. How much can you break in a day or two? We are all spared the anxiety of large, lengthy deploys where weeks of work can be released to production at once. What we’re doing now At the moment our process of Continuous Delivery is broken down into several steps: Development: We work on a new feature using a local copy of the app. Build: Pushing the new code to source control automatically triggers a build step, which runs our test suite, ensuring this code isn't going to break anything and we can safely release it. Continuous: The latest version of our app is hosted on a private server. This allows us to review it in an environment closer to the production one, and to have peer review from our colleagues. If we discover a bug here which our test suite didn't catch, we will go back to the first step and fix it. Staging: At this point we're happy with the state of our app. The feature is working as expected and any bugs we may have found are fixed. The latest version of the website is pushed to another private website where customers can review. Production Test: We currently employ \"blue green\" deploys , which means we can safely push changes to production without users seeing it. Instead a secondary production app is launched, connected to the same database as production. This is the final review step, if everything works here we're good to go live. Production Flip: We now point all users of the production website at the new production app, and switch off the old one. The new features are live! This is a fairly complex setup, but it allows us to employ that rapid Continuous Delivery we love. Since we host most of our apps on Pivotal now, we wrote a gem called cf-deploy to take care of most of these steps for us. Starting a new project is as simple installing cf-deploy, defining configuration for each stage, and hooking it into our deployment pipeline. In our case, Jenkins. We have been using this for a while now, and we encourage you to try out the gem yourself! The benefits of Canary releases Today, we're excited by the concept of 'canary releases,' which promise to offer us and our customers more secure deployments, at a faster pace. A 'canary release' is similar to a 'blue green' deployment in many key ways. New production candidate releases are pushed to a secondary production server, just as before. They share the same database, as before. But, significantly, a percentage of new users to the live website will instead be directed towards the canary release. This allows us to test a production candidate with a subset of real users before committing to showing to all users. If our new changes have a bad effect, for example if they introduce a new error, we have the ability to pull the canary out and cancel the deploy. The live website is in the same state as it was before, and the error has only been exposed to a reduced number of users. If the new app had instead been pushed directly to production, it would take longer to pull back and the damage could have potentially affected all of our users. The name, of course, comes from caged canaries which would be taken down into a mine by coal miners. If dangerous gases in the mine killed the canary, the miners would have sufficient warning to get out. In software delivery, canary production apps give us that same warning ahead of time. Of course, just using a canary release doesn't remove any of our prior security nets. We still have peer review, customer review and production review stages before allowing new code into the wild. Crucially, canary releases give us an intermediate production release step which is much more powerful than 'blue green' deploys. Nobody can test your app better than real users. Our Canary strategy Implementing canary releases means adjusting our deployment strategy. Our new process might look like this: Development: Same as before Continuous: Same as before Staging: Same as before Canary Deploy: A canary production release is spun up straight after staging, but not shown to any real users yet. This gives us an opportunity to check everything is okay first. Canary Trial: A percentage of traffic to our live website is directed to this canary instance, so we can test with a subset of users. Following a trial, we have two options. Canary Release: Everything went well, new users on the canary website are not experiencing any new errors. The canary app scales up and replaces the old production app, all users are now on the latest version of production. This is analagous to the 'flip' step from before. Canary Rollback: Users on the canary website are experiencing errors, so we turn off the canary instance, and they are (invisibly) sent back to the old production site. All users are now on the existing production site and we can fix the bugs we caught before pushing again. When connected to a Jenkins pipeline, the latter half of this deployment approach looks like this: Going forward We are excited by the possibility of integrating canary releases into all of our clients' websites, and have built proof of concept deployment pipelines using this new approach. We are still experimenting with this, and haven't refined our approach to a production ready standard yet. That said, we have integrated the steps necessary for a fairly opinionated canary production release into an experimental 'canary-releases' branch of our cf-deploy gem. Give it a try and please let us know what you think! Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-10-29"},
{"website": "Made-Tech", "title": "Introduction to Agile and Scrum", "author": [" Rory MacDonald"], "link": "https://www.madetech.com/blog/introduction-to-agile-and-scrum/", "abstract": "Over the past five years, Agile has gained significant traction and has been adopted by organisations of all shapes and sizes. At Made, we run all of our software delivery through an Agile methodology called Scrum. We've been using Scrum for the last three years and have had some great success with it. We're getting to the point where we would consider ourselves experienced practitioners of Agile, so in this post we'll share some of our learnings, provide an introduction to Agile and explain how we are using Scrum to deliver software at Made. History of Agile Agile started to gain traction in the early 90s as a reaction to the widespread failure of many large software projects. Back then, the software development process tended to be slow and documentation heavy. The first few months of a project would be spent detailing everything within a specification document, which would often end up being several hundred pages in length. Nobody ever read these documents, but when requirements changed, people ended up in dispute and claims of scope and cost adjustments ensued. People realised there must be a better solution, and so Agile was born. What is Agile? At its core, Agile is a set of principles that can be used to guide the delivery of a software project. It encourages communication, collaboration and working software over documentation and plans that cannot change. The Agile Manifesto While the Bible has The Ten Commandments and Asimov has his Three Laws of Robotics, Agile practitioners follow something called the The Agile Manifesto: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan The Manifesto is embraced by all good Agile teams. Teams value the items on the right hand side (the text that is not in bold), but favour the left hand side (text in bold). You'll often hear Agilistas citing items from the Manifesto as a rationale behind a decision or approach. In addition to the manifesto, Agile practitioners also follow a more granular set of principles, which guide the day-to-day running of a an agile project: The highest priority is to satisfy the customer through the early and continuous delivery of valuable software. Welcome changing requirements, even late in development. Agile processes harness change for the customer's competitive advantage. Deliver working software from couple of weeks to a couple of months, with a preference to the shorter timescale. Business people and developers must work together daily throughout the project. Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the job done. The most efficient and effective method of conveying information to and within a development team is face-to-face conversation. Working software is the primary measure of progress. Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a constant pace indefinitely. Continuous attention to technical excellence and good design enhances agility. Simplicity, the art of maximising the amount of work not done, is essential. The best architectures, requirements, and designs emerge from self-organizing teams. At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly. Scrum Scrum is an Agile methodology. Its name comes from the sport of rugby, where a team needs to work together to drive forward and achieve a common goal. It follows the principles set out in the Agile Manifesto, with some additional concepts that sit on top of it, such as Sprints, Product Backlogs and Daily Standups. Scrum is the most popular of the Agile methodologies, so it's often considered to be the same as Agile. It's not , and you should explain the difference when you hear people refer to Agile and Scrum as one. How does Scrum work? Terminology There is a lot of jargon within Scrum, which can make it difficult for newcomers to adopt and understand. For those who are not familiar with the Scrum practices, here are the main terms you're likely to encounter. User Story A User Story is a feature written from a user's perspective, for example: \"As a Customer, I would like you to remember my credit card details, so I don't have to enter them each time I purchase a product\". User stories are preferred to feature lists as they allow the team to more easily understand the business value. Product Backlog The Product Backlog is a list of user stories for a particular project. It is essentially a to-do list, which has been prioritised by the Product Owner according to the expected business value to be gained on a user story's completion. Story Points A Story Point is the estimated effort required to complete a User Story. It's a relative measure of complexity, rather than some number of hours. We have written a more detailed article which explains story points here . Sprint A sprint is a timeboxed period (typically between one and four weeks) where a team commits to completing a set of defined User Stories. A project is comprised of multiple sprints. Sprint Backlog A Sprint Backlog is a list of User Stories that the team will aim to complete during a sprint. All of the User Stories will have Story Point estimates alongside them, and this total should align with velocity achieved in the previous sprints. Spike A Spike is a fixed period used to investigate uncertainty. For example, you could run a 2-day spike to evaluate a new technology that the team hasn't used before. Velocity Velocity is the rate at which the team is completing user stories. Once a project is underway, and the team is in a rhythm, you would expect to see a consistent velocity between sprints. Planning Poker Estimating complexity is particularly difficult in software projects. Scrum uses Planning Poker to help with this. It's a card game in which the team is given a set of cards, each of which are numbered with a value from a modified version of the Fibonacci sequence: 1, 2, 3, 5, 8, 13, 20, 40 & 100. The numbers get further apart as you get higher to emphasise that bigger things are inherently harder to estimate.The Product Owner describes the User Story and each team member puts a card face down on the table. All cards are turned over simultaneously, and then the team discusses the differences in estimations. This technique encourages discussion and helps to provide fair estimates that the team has agreed upon. Blockers A blocker is an impediment that is holding up progress on User Story. For example, if a User Story was to setup Tax Rates for a particular country, but the engineer didn't have this data, the missing data would be considered a Blocker. Blockers should be flagged to the team and if the team cannot resolve them, then the Scrum Master should try to. Burndown The Burndown chart visualizes the progress against velocity and provides an early warning system if the team are tracking behind the intended target. If a sprint was ten days long, and the team had 100 Story Points to complete, then the team would need to average upwards of 10 points per day to track good progress and see a positive burndown chart. Potentially Shipping Increment A Potentially Shipping Increment (PSI) is a piece of functionality which can launch on completion. One of the big focuses in Agile is to limit work in progress and ship frequently. There is a lot of tooling, like Continuous Integration Environments , which help to support the delivery of PSIs. Backlog Grooming Once a project is underway, it's easy for the Product Backlog to grow and grow until it becomes a bit of a mess. One of the tasks for the Scrum Master and Product Owner (and sometimes the team) is to sit down together, go through the items in the backlog and remove duplicates, or those which are not a priority and are not going to deliver value to the business. Sprint Planning Before a sprint begins, it's important that the team are all aligned, and that we have up-to-date story points for the features. The role of the Sprint Planning session is to go through the highest priority items in the Product Backlog and agree which User Stories will be included within the next Sprint. Scrum Roles Within a Scrum project there are a number of key roles that are required: Product Owner The Product Owner (PO) is in charge of the project vision. This person is typically somebody senior within a business, who has the authority to make decisions about direction, features and understands the business objectives. Their primary role is to mediate between the internal stakeholders and the Scrum team delivering the project and ensure the team is delivering the most value to the business within each sprint. Scrum Master The Scrum Master (SM) handles day-to-day delivery of the sprint. They lead the ceremonies and work closely with the team to remove any blockers that may be impeding progress. The Scrum Master's job is to protect the delivery team and ensure that committed work will be completed for Sprint Showcase. In a well-functioning Scrum team, it's healthy to see a certain amount of tension between the PO and the SM. Naturally, the PO will want more from the scrum team than is feasible and it's the SM's responsibility to help the team deliver consistently and function well. Team A sprint team consists of people in various roles, such as Software Engineers, Designers, Business Analysts, User Experience and so forth. The team will be required to deliver the sprint and typically work full-time on the Sprint. Sprint teams tend to remain consistent but can change if the user stories being delivered necessitate the change. Ceremonies Within Scrum, there are ceremonies at key moments in the project. The three main ones being: Daily Standup, Sprint Showcase, and Retrospective. Daily Standup Each morning, the sprint team comes together to have their Daily Standup. The standup is a short and informal standing meeting, where the entire team attends. Each team member answers the following three questions: What did you complete yesterday? What are you going to complete today? Is there anything that is blocking you from achieving this?. Standups are a regular opportunity for the entire team to communicate progress, understand blockers, cross-pollinate information and evaluate risks. Sprint Showcase On sprint completion, a Sprint Showcase meeting takes place. In the Showcase, all the completed User Stories are showcased to the project stakeholders. The idea behind this session is that the team is sharing what should be a shipping increment, something that the business can release to end consumers and that the business can start seeing some return on their investment from. It's therefore critical that at the end of each sprint, the Scrum team are delivering features that are truly ready to launch. Retrospective Following Sprint Showcase, a Retrospective is run. This is a fun exercise used to capture feedback from the team on the things that what went well, things that didn't go so well and what learning can be taken into the next sprint. The Agile / Scrum methodologies heavily focus on team improvement and the goal is for the next sprint to be better than the last. This 'always be improving' mentality is key to Agile. My colleague Fareed has written about his Favourite Retrospectives here . Putting this into practice Now lets look at how this works at Made: Before the project starts – We'll typically run a story planning workshop. In this workshop we'll capture an initial product backlog from the attendees and work together to prioritise stories into an appropriate order. Before the first sprint – We'll run a high-level Planning Poker session with the team. We'll add Story Point estimations to the items in the Product Backlog and work with the Product Owner to agree a first sprint. Sprint starts – The team will work their way through the User Stories within the Sprint Backlog. Daily Standups – Each morning at 10am the team will get together to have their Daily Standup. Showcase – At the end of the Sprint, the team will organise a Showcase session, where the completed work is demoed to key stakeholders and feedback is captured. Sprint Planning & Retrospectives – Following the showcase, the team will get together to have the next sprint planning session. Any feedback will be included in the backlog and the next sprint will be agreed. If necessary, a sprint Retrospective will take place. Repeat – The next sprint starts and we repeat from step 3 onwards. Where next? In recent years, we've seen Agile principles being adopted by non-software teams such as by the Lonely Planet legal team , teachers of degree courses and by marketing teams . We find this really interesting and are looking forward to seeing Agile being used cross-functional departments and the impact this will bring to organisations and teams across the world. Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-10-27"},
{"website": "Made-Tech", "title": "Finding Ethics in Data Security", "author": [" Luke Bennellick"], "link": "https://www.madetech.com/blog/finding-ethics-in-data-security/", "abstract": "We live in a world increasingly intertwined with software; the Internet of Things allows companies to collect data relating to not only what we type, but on where we are, who we are with, how we drive our vehicles, and more intimate details like our sleeping patterns and heart rates. In the near future it will be trivial to sequence an individual's genomic information and make predictions about their personality traits and health. Modern computers have the space and power to analyze this data incredibly efficiently, and if Moore’s Law continues to hold up, we will start to see real time analysis of an individual's genetic constitution. The implications of this are far reaching; individuals could be discriminated against on a level far deeper than gender or ethnicity, and it has the potential to impact everything from loans, job prospects, insurance and even our interpersonal relationships – imagine a dating app based on genetic viability. Although 'genetic fortune telling’ may seem a way off, the way we handle privacy and data security now will build the foundations for the future, and engineers often play the role of gatekeeper to the systems which contain this sensitive information. To whom is their moral obligation? The client, the user, or perhaps the Government? The Home Office’s stance The terrorist attack on Westminster in March 2017 brought the issue to the fore when a WhatsApp message was sent a few minutes before the attack, which the government were unable to decipher due to the end to end encryption in place. Should the engineers responsible for this encryption allow the government access, or decrypt the messages for them? To the Home Secretary, Amber Rudd, the answer is clear;  WhatsApp have a moral obligation to allow the government ‘backdoor access’ to its messaging services. The rationale for this is that terrorists should not have a place to hide online, that encryption enables terrorists to communicate freely and that society would be a safer place if encryption were banned. Although it’s likely a difficult task to ban a branch of mathematics, it would be an even more difficult task to enable worldwide banking and e-commerce without encryption. The does not seem, to me, to be a viable solution. Perhaps had the government had access to WhatsApp’s data they could have intervened sooner, however it seems unlikely given the two minute time frame. More likely, they would have been able to analyze this information after the incident to glean a better insight into the cause, however this is not without its own risks. A backdoor for one individual means an exposed point for groups with less than ethical ideals. A ‘secure backdoor’ is not secure – people can and will access it and their intentions most likely won’t be good. Would every holder of personal data (almost every company operating on the web) be expected to have a backdoor? This would present incredibly difficult to solve problems for keeping personal data secure, and largely useless too; no organisation has the computing power to analyze all of this data fast enough to make use of it. Perhaps some time in the future, but not now. Practices for Ethical Engineering There is a solution, I believe, which is more balanced. It is the system currently in place; the government requests access to encrypted organisational data, and the organisation makes a choice to either release the data or refuse access. Ethically, I believe they have an obligation to support the government in investigations, but the government also have an ethical obligation not to put society's personal data at risk of exposure. So where does this leave the ethical obligations of the engineer? Should they make a stand and refuse to allow access to data if it is requested, if it conflicts with their moral principles? This is a far more difficult question to answer. It is so subjective that it cannot, I believe, be generalised broadly. There are however a few practices which can be applied broadly, and that would in my mind, mark both a professional and ethical engineer from a security perspective: An engineer should always stay on top of security developments and potential vulnerabilities in their chosen tools. They should understand the advantages of other technologies, and when using them offers their customers better security. They should have a good knowledge of cryptographic methods. Perhaps not at the implementation level, but at least a high level understanding of the advantages and disadvantages of each. They should have a good understanding of data backup methods to mitigate the risk of data loss, but to also ensure the data isn’t easily accessed by those who shouldn’t be able to. They should ensure that physical access to infrastructure is secure if it is kept on site. Failing that, a reliable infrastructure provider should be used. The geographic location of third party hardware should be considered carefully. Different countries have different data protection laws. Looking to the future Data plays an integral role in our day to day lives, and it is likely to play a more important role in the future. The data being collected is becoming more personal, more invasive and as a result, our responsibility to keep it secure has never been greater. Whatever the ethical implications of the particular situation, as long as an engineer does all they can to ensure the security and integrity of their customer's data, at a professional level I believe they have fulfilled their moral obligations. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-04-28"},
{"website": "Made-Tech", "title": "Planning an Agile backlog", "author": [" Nick Wood"], "link": "https://www.madetech.com/blog/planning-an-agile-backlog/", "abstract": "Most Agile methodologies work in small independent units of work, often called User Stories. Coming up with these is its own challenge, and one I plan to tackle in a future post. Today though, I want to discuss how to determine if a User Story is valuable, and from there I'll outline a method for turning a list of disparate stories into a working backlog for commencing an Agile project. Three types of value The Kano Model talks about five types of value. To keep things simple I'm only going to cover three of them: Must-have/Basic Needs: Users will think less of you product if you don't have these features. Performance needs: The more of these you have, and the better their implementation, the better your product is received. Exciters/Luxury needs: New/novel improvements or optimisations, the presence of which will exponentially improve user perception. Group by value Given our initial backlog of stories attained at a user workshop, we'll first aim to classify our User Stories into these three categories. In my experience about ninety percent should fit into these three groups (and the remainder likely into the remaining two). We can use these broad brush strokes to give us an initial sense of the project's size. In general: We should implement as many Basic Needs as is feasible We should aim to complete as many Performance Needs as we can afford We want to include a few Luxury needs, to increase user experience, and for market differentiation A similar method of prioritisation is to use MoSCoW (Must, Should, Could, Won't) to do this first pass, I Prefer to use Kano though. MoSCow can bring subtle cognitive biases to the table, as many people familiar with it have the mistaken notion that anything that's not a 'Must-have' won't get built. Bucketing by Theme Now we've bucketed your features by need, it's also useful to now do so by theme, e.g. in the context of an eCommerce store: checkout, user accounts, product information pages are three (of likely many more themes). The key thing to notice is that these are orthogonal to our needs column above, we might decide that being able to purchase at all is a must-have, whereas accepting the latest crypto-currency is probably an Exciter at best; both would probably fall into a checkout/payment theme though. It's helpful to group features in the same value and theme together, so that during development the engineering team will work on them sequentially. In practice this will make development more efficient as the team won't be constantly jumping between disparate parts of the codebase. Identify a walking skeleton Now you want to run through your basic needs, with the aim of pulling out a core 'Walking Skeleton' or MVP. This is the absolute minimum of features that ties your application together end to end, and it may be helpful to work with your engineering team for this excercise. The idea is that this is going to build a very rapid prototype for the first sprint or two. This forms a stable foundation which we can build on in later iterations – putting the meat on the bones as it were. Try and imagine the walking skeleton for a car – for the sake of argument let's assume we don't need to worry about passing any kind of legal/safety requirements – this is just a prototype. Our must haves are propulsion, steering, and somewhere to sit, plus the absolute minimum needed to attach this all together – a basic chassis. The gearbox can wait and windows are an unnecessary extravagence at this point. That's not to say that we won't add either of these things later, just that for now we'll be happy with a golf buggy, we'll use subsequent iterations to turn it into a Ferrari, or a Prius. Identifying biggest risks Every project has risk, and most of the time the risk is manageable. Most projects will have some inbuilt assumptions though, and the project outset is a good time to try and figure out what these are. Perhaps there's a new API you're relying on but have never used, or maybe you're unsure if there's a market for what you're selling. It's absolutely not worth the time to fully implement these things at the outset, as this may take many weeks, and we're more interested in building a proof of concept at the moment. However, it's definitely beneficial to set aside a day or two to get some reassurance on the biggest unknowns, the stuff that will sink the project if our assumptions prove false. Triage your remaining must haves These will form the main bulk of the application, and are things that you or your users would consider your product incomplete without. These are usually fairly obviously fundamental to function, and will include things like product listings on an ecommerce site (though likely not search). Reviewing your must-haves is a great opportunity to really challenge your assumptions; do we need this because the site won't function without it? Or because most of the competing solutions have this feature? Streamlining your feature set early on will give you the greatest flexibility for change later. Performance needs These are things which improve a user's experience, but your user demographic is going to massively impact what fits inside this category. If we're trying to build a novice image editing tool, we should be wary of building something too complicated for our users, as that will alienate our market. We don't want to build Photoshop if our users only needs MS Paint. Also, beware of diminishing returns with your performance features, adding your first will add a lot of perceived value to your product, but adding a twentieth will add much less. What are your key exciters? Similar to above, you don't wont to go too overboard with these, especially at the expense of core needs. However, exciters are what will differentiate you from your competitors, so it's defintely worth adding a couple of well considered such features. It's hard to think of a big tech success which hasn't had a well executed hook (note that 'first to market' is rarely a sufficient hook). Google had PageRank, Candy Crush added a lives system to a well trodden 'match-three' formula to keep users from burning out and shaving company Harry's had a killer referral scheme to drive explosive growth. Embrace change – but prepare for it The most important part of any project, and the hardest to account for in practice. At the project outset you know less about the project than you will at any point in the future, yet so often is it at the beginning when most expectations are set. If you've been though all the above steps, you hopefully have a pretty solid picture of what you're going to build, but this is all based on imperfect knowledge. I can guarantee that there are features and assumptions that you haven't thought of, and those which are necessary are going to take a huge bite out of your plan. Better to plan for this ahead of time – not by trying to identify every potential change, but by carving out a chunk of your timeline to afford an opportunity to pivot if required. To sum up This isn't intended to be a magic wand for all projects, nor is it a hard and fast rule, but I recommend that newcomers to Agile try and break up their first project using the following proportions. If you're a newcomer in a nascent market you might want to forego exciters in favour of getting the core journey right. On the other hand if you're entering a crowded space you might want to invest heavily in luxury features to better stand out from the crowd. Walking Skeleton: 5-10% of project Spike out the big risks: 0-10% Flesh out the core features: 30-70% Key performance features: 10-30% A couple of exciters: 10-30% Leave at least 25% for stuff you haven't thought of yet! Tweet 0 LinkedIn 0 Facebook 0", "date": "2015-11-03"},
{"website": "Made-Tech", "title": "Slicing code", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/slicing-code/", "abstract": "The primary goal of slicing software is to make it cheap to ship, and inexpensive to ship additional features. A poorly thought out slicing strategy leads to viscosity. More advanced teams notice this typically the greatest around their test suites. On the whole, if you find test suites are slowing down the introduction of additional functionality and the evolution of your domain model, it is a sign that the coupling between your test assertions and the production code are less than ideal. From this point on the following terminology is used: Refactor: the act of modifying the structure of the code, through a series of simple steps, while keeping all the tests passing. Rework: the act of rewriting sections of code, possibly guided by test suites, but potentially not in simple steps Transformation: the act of modifying code (production or test) for the purpose of changing its behaviour In the worst cases, such as in the absence of automated test suites, teams feel the joint pain in wrong code slicing greatest. In situations without test suites, it can be much harder to realign slices to suit the needs of the team better – this is rigidity. In situations with test suites, it may be necessary to undertake significant rework of those test suites to eliminate areas of rigidity. The danger with rework is that it has a significant risk of introducing behavioural defects. In the ideal case, simple paths are always present to allow refactoring, which has a lower risk of introducing behavioural defects. One of the most valuable lessons from DDD (Domain Driven Design) is that no \"model\" is perfect, but some models are more useful than others. I use the terminology \"model\" to refer to how objects, or functions in the case of functional programming, collaborate to model the real world within the programming language chosen. One of the reasons micro-services are incredibly useful is that they allow the models within each service to be radically different. Microservices are independently deployable, independently developable products within a sphere of a larger whole. Each microservice collaborates with other services, micro-services, and potentially third parties to fulfil a role. It is very rare for a micro-service to be isolated from other aspects of an ecosystem entirely. Microservice architecture is an example of an extreme slice between two or more areas of code. Of course, slicing code in this way comes with benefits and (expensive) drawbacks. Technical teams should not make the decision to rework their systems into micro-services without being acutely aware of the disadvantages. When beginning to build a system from scratch, the \"domain model\" is usually hard to determine up front. This \"hard to determine all details up front\" problem is especially the case if the intended approach is to build several different features over the course of multiple iterations. During those iterations, it may be necessary to go back and refactor/transform other features to be aware of new data within the domain model. AntiPattern: No Class has responsibility for the User One common slicing problem is related to domain objects themselves. Typically, these can be ORM classes or, they could be plain old objects with some behaviours. One simple approach is to embed actions related to user stories into these domain classes, with UI classes also taking responsibility for some of this behaviour as well. This design, seen commonly in the Rails-way, can work quite well for simple user stories; however, when the user stories become more advanced it can become harder to model the domain using a single ORM object, which means other ORM classes must also become aware of this action. One property of well-designed tests is that they provide a living, up-to-date documentation of the system. Agile is customer-focussed. However, this approach to not design around the customer causes the system design to not focus on the customer. To illustrate, consider unit test suites for each of these classes; they will each have knowledge of the user story. Another way to describe this problem is with cohesion: a system exhibits low cohesion if that system forces multiple unit tests and different aspects of the codebase to be aware of the same user story. While fully minimising this lack of cohesion this is unlikely; it is important to keep an eye on cohesion and to increase it wherever feasible. Focusing unit tests around user stories results in those unit tests describing customer expectations, rather than the expectations of another class operating in consort. AntiPattern: Black and White Mockist vs. Classical testing One common misconception is that \"unit\" refers to \"a single class\", but this leads to lots of fragmented test suites, and lots of usage test doubles. A strong coupling exists between test doubles and their production counterparts, and so can become a maintenance overhead. Every time a production counterpart changes, the test double must also be updated. When multiple test suites must be understood to understand a user story, we see a fragmentation of knowledge. Every piece of fragmentation makes the code much harder to navigate and understand. Unfortunately, the problem does not end here: when test suites are involved in too big a slice of the pie – in the extreme case of testing via the user interface, we see slow, brittle and incomplete test suites. Typically with this approach, it is also impossible to write the assertions that are needed to create complete coverage of behaviour. Classical testing favours testing large swathes of the codebase, and cause test suites to have large fan-outs. Mockist testing favours testing small amounts of code and using test doubles to reduce test-suite fan-out. The answer is just enough classical testing, and just enough mockist testing to keep things flexible in the right places. For example, a comprehensive test suite for the code that \"Creates an Order\" does not also need to exercise the database, but it does probably need to exercise some other objects hidden from it behind a public API. So, it provides some test doubles for the database aspect. However, the details of any objects not cared about by the test suites can be hidden by the system under test. By doing so, we create a highly customer-focussed test suite with all the intricacies related to the user story for \"Creating an Order\", without also needing to provide test cases for the database. Before we saw test doubles become a maintenance overhead, but when cohesion is high we see the test double serving as an example for how the real database aspect should work. By hiding what we want to be flexible (the model of creating an order) using classical testing, we can refactor the code related to \"Creating an order\" without changing the test suite or its assertions. We have created flexible code and a test suite that is largely agnostic to / protected from structural changes. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-05-10"},
{"website": "Made-Tech", "title": "Using Ansible for infrastructure", "author": [" David Winter"], "link": "https://www.madetech.com/blog/using-ansible-for-infrastructure/", "abstract": "For the last year or so, the majority of our new projects at Made Tech have used Ansible as our go-to tool for provisioning and configuring our servers with the software that runs on them. We've paired Terraform with Ansible and Chef previously for creating our cloud resources, but have recently been experimenting with using Ansible to see if the one tool was capable of both of these stages in our infrastructure setups. We've not been disappointed with our experiences so far. Ansible and Amazon Web Services Ansible has support for a variety of cloud services out of the box with the more recent releases. To name a few, AWS, Azure, Google and Rackspace. Full support of each of services API varies, however it appears as though AWS has the most supported set of features by Ansible. If you're familiar with using Ansible for provisioning your servers, then creating resources in the cloud is almost identical. You use AWS specific modules that represent the resources in AWS and, once defined in YAML, you're able to run an Ansible playbook to apply those resources to your AWS account. AWS credentials When using Ansible with AWS, when you run your playbook, Ansible will be running commands from your local machine using the AWS API. It uses a Python library behind the scenes called boto to do this. As with most AWS libraries, boto looks for a configuration file on your computer located at ~/.aws/credentials. In this file I find it useful to group credentials into separate profiles so that you can be explicit in which credentials to use when running a command. For example, your credentials file may look like: I'd be able to use these credentials by specifying the profile playground in my commands. Infrastructure playbook When setting up new resources for an infrastructure project, the first thing I usually do is setup the SSH keys that we'll be using in future to connect to any servers to provision them. I create an SSH on my machine, and then add it to AWS. To generate the key I use: The value for the -C flag is the comment that is included in the public key. This can be something descriptive related to the project you're working on. The -f value is the name of the file that the keypair will be saved to. The value you specify will be the exact name for the private key. The public key will have a .pub appended onto the filename automatically. Create another file called build-infra.yml and in this file include: The first three options, hosts , connection and gather_facts are setting up the Ansible playbook to use your local machine to run the subsequent modules that we define in it. As mentioned above, that is because behind the scenes we use our local computer to do the communication with the AWS API. We're then defining a standard Ansible task using one of the AWS modules, in this case, ec2_key which is used for managing AWS security keys. We tell AWS to use the name web for the key, and to load the content from the local file web.pub as the key content in AWS. To run this, we use the following command: We prefix the standard ansible-playbook command with two environment variables that are used behind the scenes by the boto library to connect to the AWS library. We tell it to use the playground profile from our ~/.aws/credentials file to connect to the API, and to run the AWS operations within the eu-west-1 AWS region. By building our AWS playbooks in this manner, it makes them far more reusable than if we were to hardcode these values directly into our playbooks. Lack of state file Running the above command, you should see some output with a [changed] value next to the Add web keypairtask. Run the command again, and the same task will have an [ok] flag next to it as nothing has changed. I see this as one of the benefits to using Ansible compared to Terraform. Ansible will check for state directly with AWS rather than a local state file that is updated each time a command is run. Using resources between tasks Some of the AWS Ansible modules require you to reference other AWS resources that you have created. You can use Ansible variables to manage these. To better demonstrate this, lets build the following resources; a set of three EC2 instances with Elastic IP addresses associated to them, that are sitting behind an Elastic Load Balancer. We'll setup a security group and assign the instances to these, and then attach each instance to the load balancer. In summary, the steps are: Create keypair for instances (already done above) Create a security group to assign instances to Create three instances Assign Elastic IP addresses to instances Create a security group for the load balancer Create a load balancer Assign instances to load balancer For each of the above steps, we'll add the following tasks to our YAML file: This is a standard security group permitting all outgoing traffic from the instances, and only allowing SSH and HTTP access inbound. The ec2 module creates, modifies and removes EC2 instances based on the count of instances that are present with a particular tag assigned to them. In this case, we will create the number of EC2 instances defined by the exact_count value that have the Name tag of web assigned to them. We store the details of these instances in a variable called web_instances once the command has completed. We'll then use this variable to assign Elastic IP addresses to them in the following command: In the above Ansible module, we're using the with_items option to loop over a variable and run the ec2_eip module on each value. The ec2_eip module will allocate an EIP and associate it to the instance specified by the device_idvalue . Similar to our instance security group, we create another that will be associated to the load balancer. Again we permit all outgoing traffic, but restrict access to HTTP only. For the specific instances, we'll connect directly to them over SSH, therefore this protocol doesn't need to be enabled on this group. The above creates our load balancer with our specified security group. We listen for traffic on HTTP port 80 only. We need to assign each of the instances we created in the task above to the load balancer. We loop over the instances variable again, as we did with the EIP task, and one by one attach the instances to the load balancer. Running the above playbook will in turn run each of the tasks within it, gradually building up our resources. If you change the count in the ec2 task, and re-run the playbook, you'll notice that instances are created or removed depending on the count that currently exist. Provisioning our instances with Ansible The best way to tie together this introduction is to provision our new EC2 instances with Ansible and set them up as web servers. For this, we'll create another Ansible playbook with the specifics for the software configuration to apply to the instances, as well as using the Ansible dynamic inventory script so that we don't have to manually specify any of the instances we want to apply the configuration to. Ansible will be able to do this by us specifying tags that are assigned to instances. In order for this to work, we need to download the dynamic inventory script to the same directoy as our playbook from here . We then need to give the file execute permissions: The dynamic inventory script depends on a configuration file, so we download the default to the same directory also: Copy the below playbook file to provision-infra.yml: And then provision the instances by running the following command: Once completed, the instances should have PHP configured with a running test script. As we added a health check to our load balancer, once it detects the presence of the script, it will start serving traffic to that instance. In the output of the above command you should see a line beginning with TASK [debug]. Within this, it'll have a msg key with a value that is the public DNS value of the new load balancer. So you can copy that and visit it in a browser. Each time you refresh the page, you should see the value of the hostname output change. This is traffic being routed to each of the different instances on each request. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-04-05"},
{"website": "Made-Tech", "title": "The Digital Classroom", "author": [" Igor de Alcântara Barroso"], "link": "https://www.madetech.com/blog/the-digital-classroom/", "abstract": "From keeping in touch with friends, to hailing a cab, and even allowing us to control the temperature of our houses from our phones, Technology has become increasingly pervasive, changing the way we do things and how we interact with one another. Contrary to what some would think, Education was not left out of this revolution. In recent years, an incredible number of digital tools and platforms have seen the light of day which try to improve how people learn, how teachers share their knowledge and allowing pedagogical resource providers to reach an ever increasing number of people. But in Education, things cannot change as rapidly as in other sectors because the cost of doing something wrong has long lasting consequences. Pedagogical practices have been refined over the years and introducing changes requires a lot of thought. In this post, we would like to first look at some areas where Technology could actually add value to existing practices, and how it can be used to solve particular pain points. Then we will move on to sometimes unforeseen challenges schools and universities face when going digital. Finally we will provide some thoughts on how to ensure the digital transition is as smooth and as successful as possible. Benefits Here are some areas where digital solutions could make sense, with the things they enable: Online First Probably one of the biggest challenges any teacher faces with a classroom is to ensure all students acquire the knowledge they are supposed to. But each person learns in a different way and at a different pace. Using digital tools can allow teachers to provide different teaching materials, such as text lessons, videos, exercises or even pedagogical games, that cater best to individual student's needs. Making those resources available online, opens up a world of possibilities such as Flipped Classrooms , Differentiation , or Gamification . Progress Reporting One important aspect of a student's academic success is the ability to self-assess, as this gives the student more ownership over their learning process. For parents, it is also important to make sense of their children's report in order for them to give better support. As such, crafting good user interfaces, with common components such as dashboards, that are adapted to both the student's age and the subject matter help make this information more understandable. For teachers, new developments in Docimology may impact the way they used to work and assess their students. A digital solution that helps them reuse their existing teaching material will make adoption of new assessment practices easier and more widespread across faculty members. Communication Improving communication between teachers, parents and students is an ongoing effort and digital tools have become the de facto solution for most communication channels. From email to forums, or even using Slack in the classroom, there is a plethora of tools that try to make communication more efficient or have information shared be more accessible. For institutions, this can raise important issues such as privacy, moderation or centralising communication channels. More often than not, existing products don't fulfil every requirements and bespoke solutions could make more sense rather than just falling back to email. Challenges After having glanced over possible use cases for technological tools in Education, we now will start looking at some usual problems that arise when it is time to actually implement those solutions. We will focus on three points: Costs, Security, and Tool/Practices fit. Costs Budgets in Education are notoriously constrained, that is why careful costs consideration are important. One common example is a school deciding to buy 30 computers or tablets for a room so that every student can have their own online access while in this classroom, only to find that 30 devices trying to watch the same video on Youtube can put some serious load on your internet bandwidth. Aside from the aforementioned not loading, it may impact the classroom next door or anyone currently trying to get some work done online. The naive approach to this issue is often to increase the bandwidth, which entails an unforeseen cost increase for the institution. To add insult to injury, schools usually find out that increasing the bandwidth does not fix the actual issue and they just end up with a pricier internet bill and expensive devices not being used any more. This scenario could easily be avoided if the institution had a technology partner, as this is a known and solved problem in IT. It is also good to keep in mind that financial costs are not the only ones involved in modernizing the classroom. Other kinds of costs come into play such as time, as educational institutions have hard deadlines, and people that either need to be trained or hired to ensure the success of the digital transition. Privacy, Security and Resilience One major source of concern for schools is the security aspect. Besides obvious authentication (who can access) and authorization issues (what can a user have access to), there are potential legal considerations to keep in mind with regards to what information is being stored, where it is being stored, and how it is encrypted. This is where off the shelf solutions usually fall short. Those considerations also lead to some more thoughts around the defence mechanisms against inside and outside threats. As the information stored is highly sensitive, both on site and in the cloud solutions have to mitigate external threats, but also prevent students from changing their own grades for example (yes this happens). One last note, maintenance, backup and overall availability are key in making sure services are always up when needed and that catastrophic scenarios can be recovered from. Tool/Practices Fit When it comes to adoption of new tools and technology, a few items can play a part, but the most important is that the proposed tool actually solves a use case for their users. On one hand, tools that are pushed from top to bottom usually face a decent amount of rejection from end users. On the other hand, trying to reach a consensus on what available tools to use can be hard. That is where custom built solutions usually shine, as a competent technological partner can help design a system for school users and integrate it tools users already love and use, bringing in the best of both worlds. Another area where custom built solutions have an advantage over off the shelf ones is that often schools have students of different age ranges. Having 6 year old using something like Excel or young adults using a Fisher Price like user interface is … less than ideal. Don't force a digital solution to your users if it does not meet your actual requirements. Making It Happen Considering the challenges that may arise, we'd recommend not trying to do it on your own, but instead to search for a partner to help you modernise your institution. Set Up Before even starting your search for a digital solution provider, make sure you and your team are in a good position to start the actual transition: Identify stake holders inside your institution and figure out where you all want to move towards to. Decide on where your initial efforts should focus. The idea is to start small, then reiterate on the solution to reach your end goal. If it doesn't exist yet, create a technical referent role in your institution. This role should preferably be filled by a faculty member as that person will be the main liaison between your external partner and your institution. Partner Here are a few of the things you should look out for in a potential partner: As obvious as this may sound, don't hire your nephew to create your website. Go for a company with experience in building tailored solutions for businesses. Keep in mind that paying for expertise will prevent you from spending more due to unforeseen problems. To protect you from high up front costs and to make sure you end up with the best fit for your institution, prefer potential partners who practice Continuous Delivery . Try to find a partner with previous experience dealing with your industry, as Education has a distinct set of requirement and practices. Implementation When it is time to start making the digital transformation a reality, it is important that it be done in a timely manner. Again, try to start small, provide frequent feedback to your technological partner so that they can iterate quickly on proposed solutions. With both parties doing their part, you can be confident in the quality, usefulness, and overall fit of the end product. Conclusion There are many valid use cases for introducing technological solutions to existing classrooms, providing value for all parties involved, improving existing workflows, and helping solve some new challenges. But deploying digital solutions is something that needs to be done with care so as to avoid unnecessary strains on budget, to ensure private data is being kept securely, and to make the digital transition the success story it should be. In order to achieve that, you will have to identify the key areas where you should start focusing, selecting a partner with the right technical skill set , and implement the solution in a gradual manner. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-04-19"},
{"website": "Made-Tech", "title": "Red Hat Forum UK 2018 – Come and meet us on stand 5", "author": [" Delphine Gattaz"], "link": "https://www.madetech.com/blog/red-hat-forum-uk-2018/", "abstract": "We’re excited to announce that we are a Bronze sponsor for this year’s Red Hat Forum UK , on October 9th. Come along and find answers to your questions. Get your ticket: https://red.ht/2Dz0Q3J About Red Hat Forum UK The Red Hat Forum UK is the premier open source event for all IT decision makers looking to gain first-hand knowledge of solutions such as hybrid cloud, DevOps, containers and more, that are driving innovation and digital transformation. You can look forward to: Keynote presentations on industry trends Detailed technological implementation sessions Conversational sessions with industry experts Learning from customer references Live demos Come and chat! As a sponsor of the event, you will find us at our stand: stand number 5. If you would like to find out more about how to get cloud ready by adopting and leveraging modern technology: come and meet our team, we’ll tell you how Made Tech can help. We’ll also be handing out free copies of our book, Building High Performance Agile Teams , so feel free to stop by and grab yours. Key info Date: 9 October Time: 08:15 – 19:00 Venue: Park Plaza Westminster Bridge Hotel, 200 Westminster Bridge Rd, London SE1 7UT You’ll be able to find us on stand number 5. We will be active on Twitter on the day too, drop us a message @madetech or just add #RedHatForumUK to your tweet. We look forward to seeing you there! Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-09-26"},
{"website": "Made-Tech", "title": "Made Tech achieves Advanced Consulting Partner Status in the Amazon Web Services Partner Network", "author": [" Delphine Gattaz"], "link": "https://www.madetech.com/blog/advanced-consulting-partner-status-in-aws-partner-network/", "abstract": "London, UK – 17 July 2018 – Made Tech, a leading UK-based agile software solution provider, is proud to announce the company has achieved Advanced Consulting Partner Status in the Amazon Web Services (AWS) Partner Network (APN) . APN Consulting Partners are professional services firms that help customers of all sizes design, architect, build, migrate, and manage their workloads and applications on AWS. In achieving Advanced Consulting Partner status in the APN, Made Tech satisfied the rigorous requirements by demonstrating expertise, capabilities, and engagement with AWS. “I'm delighted that we have attained Advanced Consulting Partner status in the AWS Partner Network. This is a significant milestone for Made Tech and corroborates our expertise with AWS and our commitment to helping customers modernise their software delivery,” – Made Tech’s Founder and Director, Rory MacDonald. Made Tech’s clients benefit from increased value through the company’s collective proficiency in modern technologies and agile delivery methods, and creating, training, and transforming high-performance software teams: Made Tech’s approach to projects is straightforward and refreshing when compared to other suppliers who may over complicate it to achieve the same goal. Achieving Advanced Partner Status in the AWS Partner Network gave me increased confidence in the expertise of the Made Tech team in managing our operational infrastructure within the platform. It will put them at the front of the queue when considering suppliers for future projects. – Lucas Gundry, Head of Technology at The Keyholding Company The relationship between Made Tech and AWS has strong foundations, going back over eight years. As early adopters of Amazon Elastic Cloud Compute (Amazon EC2) and Amazon Simple Storage Service (Amazon S3), Made Tech has been a long-time advocate of AWS to customers. The graduation to APN Advanced Consulting Partner status therefore comes during an exciting period of growth for the company and offers clients facing cloud challenges, a greater level of skill and support. For more information, please email delphine [at] madetech.com Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-08-09"},
{"website": "Made-Tech", "title": "Infographic – Pros and cons of Ruby on Rails", "author": [" Kyle Chapman"], "link": "https://www.madetech.com/blog/infographic-pros-and-cons-of-ruby-on-rails/", "abstract": "We don’t have to tell you that the world of technology is an ever-evolving landscape of ideas and concepts, and that even when you have a firm intellectual grasp of a technology this knowledge can quickly become outdated. With this in mind, we thought we’d readdress one of our blog posts from 2015 with a fresh, 2018 perspective. What are your thoughts on this? Let us know in the comments! Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-08-01"},
{"website": "Made-Tech", "title": "How do we solve a problem like lack of diversity?", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/how-do-we-solve-a-problem-like-lack-of-diversity/", "abstract": "Just over a year ago, I wrote about the lack of diversity at Made Tech and how it was something we needed to work on as a business. We’d recognised our ‘ignorance’ had led to inactivity in identifying problem areas, but once we’d accepted our failures, we had to stop being passive bystanders and actually take active steps to address them. We began by stripping everything back and looking at our core mission: “ Improving software delivery in every organisation ” and asking, how do we do that? One of the ways is by building software that meets user needs. But users are not one-size-fits-all and in order to understand what their many and varied needs are, we must have people that share them, working on each project. Like the seatbelt companies that failed to test the safety equipment on females or the infrared sensors that can’t identify dark skin , we didn’t want a lack of awareness – caused by lack of diversity – negatively affecting our clients and their customers. At the time of writing my last post, I had hoped that a year on, we would have a team that was better represented in terms of gender, race, ethnicity, age etc and that this would be positively affecting our delivery. However even after redirecting our attention and resources – and making diversity a priority focus – we haven’t made as much progress as expected. And I wanted to work out why. Institutionalised oppression is everywhere. Common misconceptions, (outdated) societal expectations and reinforcement of racial and gender stereotypes in the entertainment industry have contributed to the ‘white men’s club’ mentality in the tech industry. We too as an organisation have directly or indirectly created a number of obstacles to diverse hiring. Masculine, military-style wording in our job descriptions had been putting off women engineers, and our employee referral scheme had brought in like-for-like; often white males in their mid-thirties. Always more appealing to choose workplaces where we can identify with our peers. These well-intentioned but ill-advised strategies had increased disparity. So, rather than spend time and money trying to re-do them, we needed to change tactics entirely. Looking at other companies’ diversity schemes and undertaking further research, it became clear that not only are huge businesses like Google and Facebook – with access to unlimited resources and a wider pool of people – were also struggling when it came to gender and race bias, and under-representation. But a key part of this for them, and us, was the fact we were looking at the problem the wrong way. Diversity, rather than being the issue, is actually the consequence of a working environment that fosters inclusivity. So, was the problem that we weren’t being inclusive? Or are, but just not enough? It seemed neither. From the top down, everyone who works at Made Tech believes wholeheartedly in diversity and genuine moves have been made to foster a culture that actively encourages it. Our office has gender neutral facilities and offers a safe space for our LGBT+ and gender-fluid employees to be themselves without fear of being treated differently. We also advocate a hot desking policy so that everyday there’s a possibility to interact and collaborate with someone new and the opportunity to form and share ideas that may not have previously been aired. Similarly, to avoid hearing the same (unavoidably male) voices over again, we rotate who runs meetings to give everyone the chance to choose and direct topics and opinion and give feedback. Every Friday, the whole team lunches together and takes it in turns to choose the restaurant to widen our experience and cultural knowledge of cuisines. We also have a virtual ‘common room’ on Slack where people can share news, stories, jokes and inform others of the – important – weekly ice cream run. In addition, Made Tech hosts regular socials so we can get to know each other outside of work and avoid ‘talking shop’. As I discovered through research, “ It takes energy to go there and present the opportunity to people that would not otherwise hear about it.” The Made Tech Academy therefore provides us with the chance to find diverse people by ourselves; those who have a genuine interest in development and engineering, but no commercial experience, and upskill them. And although we’re haven’t quite reached gender parity as a company, I’m pleased that our new hiring efforts have seen an academy intake this year of 66% women. So why, despite our inclusive environment, are we still lacking diversity in the company itself? Put simply, it isn’t easy. To increase it, we have to look beyond hiring and put emphasis on and resources into developing, progressing, and retaining underrepresented employees. The steps we’ve made are progress, but the reality is, much of it is not visible to anyone who isn’t already working at Made Tech. It’s clear that we need to do much, much more, and then shout about it. Going forward, plans are in place to start promoting female voices within our company and the wider industry, by offering opportunities and a platform from which to speak. We’ll also be partnering with diversity based organisations, working together to bring new people into tech and we’re very excited to be volunteering some of our engineers to help Ada College students with their coursework in the next month. To paraphrase, David McQueen : ‘On equality…conflict will occur. Finding the right language to have the right conversations will be hard, [you] will make mistakes, you won’t necessarily feel comfortable. Embrace this.’ And we’re absolutely ready to. __________ Luke Morton runs a mentoring network providing skills, experience and advice relevant for finding a job in tech. If you’re interested in joining or want to find out more, please get in touch here . Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-07-12"},
{"website": "Made-Tech", "title": "How deep is your work?", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/how-deep-is-your-work/", "abstract": "Coined by Cal Newport in 2012 on his Study Hacks blog, deep work is the ability to reach a state of increased productivity when performing cognitively taxing tasks by minimising or ignoring external interferences. Making deep work the centrepoint of your knowledge work schedule generates three key benefits: 1. An increase in the quality of your output; 2. An increase in the quantity of your output; and 3. A passion that manifests as a deeper sense of satisfaction when undertaking and completing tasks. In a 24/7 world driven by technological change and innovation, distractions are everywhere and unavoidable. Emails, social media interaction and digitalisation of basic administrative tasks often require immediate attention and an instantaneous response. The workspace, whether office-based or remote, as a result, becomes a hot-bed of multi-tasking, interruption, and profound information overload. And when it’s been found that re-focusing on a task after being distracted takes a person on average, 23 minutes and 15 seconds, this just leads to more speed and more stress. Like a lock that eventually gives way after being repeatedly picked at, unconstructive, fragmented time reduces the ability to concentrate and therefore resist such distractions. If permitted to continue then over time, deeper working becomes harder to attain and you need to work for an increasing amount of time on complex tasks before reaching peak productivity. Consequently, at Made Tech, we’ve been trying a number of techniques to help teams commit to periods of deep working. Success of each varies from person-to-person as everyone responds differently to external stimuli and some are naturally better than others at zoning out distractions. While it’s worth taking time to work out what works on an individual level, these are some strategies we would recommend. Adapt your working environment Flexible working is embedded in our culture so people can choose where they work from and as much as possible when. We do however require most of our teams to have larger overlap with their customers' working times as well as try to discourage working in the evenings. This is to ensure work/life balance, and because it has the potential to prevent relaxation and risk negatively impacting work being done the following day. Time yourself During pairing activities, our teams use pomodoros – a time management method which breaks down work into intervals of deep work, separated by short breaks (25 minute pomodoros and 5 minute breaks for example). This technique is not limited to development or pairing environments – anything that requires periods of uninterrupted concentration can benefit. Our developers and engineers have started using the timer system while working on client projects with good outcomes, so we recently extended its usage to the marketing and sales teams as well. Make it a daily habit Planning periods of deep work and creating a ritual makes it easier to focus than when you try to transition from shallow work when the moment (or a deadline) takes you. Being in a quiet space at a set time every day gets you into a rhythm and stops energy from being expended unnecessarily by jumping between complex and simpler tasks. From experience, we’ve found that deep work is best practiced in the morning when distractions are less frequent, or the early afternoon prior to post-lunch slumps. Control your distractions Breaks aren’t a sign of laziness, they’re key to productivity both before and after a period of deep work. Allowing yourself to fully relax during such time is therefore crucial because the mind needs to wander rather than think about what you’ve just been working on or what you have to do next. To manage any potential interruptions during breaks and deep work, dedicate set times for shallow work and online activity i.e. checking emails and social media at 10am, 1pm and 5pm and stick to it. Extensions and apps like StayFocused and Freedom can help while you get used to working this way. Another good thing to do is to book meetings in blocks rather than spread across the day, potentially slicing it up. Eventually, it will all become second nature. Mastering intense focus isn’t easy, but if you want to perform at your peak, achieve set goals and successfully tackle complex tasks and projects, then it is absolutely necessary. Even undertaking a deep period of work for one-to-two hours a day, is sufficient to get significant results for beginners. It’s therefore worth taking these steps and putting in the effort to get there because you and your clients will reap the rewards. Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-06-27"},
{"website": "Made-Tech", "title": "Pair programming remotely: tools and tips that make it better", "author": [" Mark Sta Ana"], "link": "https://www.madetech.com/blog/pair-programming-remotely-tools-and-tips-that-make-it-better/", "abstract": "Pairing is a great way to boost productivity and help crack a particularly complex problem. At Made Tech we try to spend 50% of our development time pairing with colleagues. We have dedicated pairing workstations, which consist of two screens mirrored: one for the driver (the person who writes code) and the other for the navigator (the person who reviews the code being written). If you'd like to know more about pairing, we have written the following articles and books: Richard's excellent The Best and Worst Times To Pair Program There is an entire chapter dedicated to pair programming in our Building high performance  agile teams book Pair programming on-site is an awesome experience, but there are a number of challenges when one or more of you are working remotely. The problem For those of us who work remotely our current solution for code pairing is to use screen sharing via Slack or Google Meet. Both of these options have some disadvantages when it comes to productivity: If video quality degrades, you end up with a screen that is a mess of random pixels.- If the audio quality degrades, the flow of constant feedback (from the navigator) is interrupted, so is productivity. Tip: get headsets to reduce the amount of background noise. If you're swapping roles, you either stay on the shared screen and use your pairing partner's editor or reverse screen sharing. The solution Use real-time collaborative editing tools. Instead of relaying the entire screen (as used by screen sharing), these tools only send editor/keystroke telemetry, avoiding higher bandwidth required audio and visual. We also had additional requirements which were: The tool must work on our varied collection of editors: Sublime Text, (Neo)vim, Emacs, Atom, ItelliJ IDEA (pycharm et al) and Visual Studio Code. There should be a shared terminal to provide access to specialised testing, debugging or deployment workflows. The shortlist We evaluated the following solutions in the market: Floobits , Teletype and Visual Studio Live Share . Because of one of our requirements was to use native editors we immediately discounted any cloud-based solutions such as Cloud 9 IDE , CodeAnywhere and CodeEnvy who provide an integrated web based editor. Floobits Floobits is a paid service, although there is a trial version. This is the only tool, that is reliant on code repositories being imported into a workspace before collaboration can begin. The editor extensions/plugins allow you to do this from within your editor. On initial inspection Floobits appears to cover most of our requirements. They support a lot of editors (Atom, Sublime Text, Intellij, Neovim and Emacs) and provide shared terminals. How it works (how to start collaborating) Signup Install the extension Import your project into a workspace (this can be done in the editor) Share the name of your workspace with your collaborator Pros Baked in video chat – you could use this over a separate channel via Slack or Google Meet. Cons No support for Visual Studio Code.Workspaces could potentially leak code if you forget to set as private. The synchronisation story between local and remote is not clear. Picking the wrong option can result in data loss. The shared terminal feels janky, why prevent remote users from sending the Enter key, but them allow to type in the terminal? Teletype Teletype was created by the same GitHub team behind Atom. How it works Install the extension Generate a github oauth token In the command palette enter: Teletype: Share Portal Share the portal URL Pros Free Cons Atom only No shared terminal Visual Studio Live Share Microsoft have entered this space, allowing users of Visual Studio Code and Professional editions to collaborate. How it works Install the extensionSignin using Microsoft Live or Github Share the project you're currently in Send pairing partner the sharing URL Pros Free, but a paid tier is being evaluated for the future which would have more advanced features. Shared debugging – though not a requirement it was nice to be able to remotely inspect the contents of the code while it was running. Shared servers – think ngrok, but baked in Independent exploration – the navigator could look at other parts of the code base in anticipation of new or old problems without disrupting the driver. Cons Only supports Visual Studio Code and Professional editions Summary There's no clear winner, but Floobits has the advantage of supporting so many editors and terminal support. The user experience of the Visual Studio Live Share was exceptional and just worked. This is still a relatively new field, and improvements as well as new players will appear in the market. So we'll be revisiting this space periodically. It would be great if this market adopted an open standards protocol, in a similar fashion to how language support in code editors and IDEs has been simplifed with the advent of Language Support Protocol . This new protocol would be responsible for sending editor and shared terminal telemetry. The market leaders could provide their own features on the server implementation, but clients would be reduced to a single extension/plugin. Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-06-05"},
{"website": "Made-Tech", "title": "Game, skill set, match – How to develop your team using LearnTech", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/game-skill-set-match-how-to-develop-your-team-using-number-learntech/", "abstract": "2017 was a big year for us with a number of successful projects under our belt including writing our second book Building High Performance Agile Teams , adding more great people to our team and launching our new Made Tech Academy initiative. As exciting as everything was, it was also an incredibly busy time. Technology and software delivery moves so fast, we faced new problems and challenges every day and it can sometimes become impossible to follow everything going on. We realised one project in particular wouldn't have encountered specific pain points if the team’s skill set had been different and more engineers were React specialists. Once identified, we knew that while we could change the makeup of future teams to get better outcomes, it wasn’t enough. We already had Made Tech ‘learning days’ that were organised along customer engagements, and required deliverables to be produced in one day. These took place every six-to-eight weeks, but there remained a small, skills gap. So, towards the end of last year, we reviewed where things were going over the next six-to-twelve months and decided that we needed to be more ‘hands-on’ to get our team to perform better. LearnTech We therefore developed the LearnTech initiative – a weekly half-day internal training session where we look at gaining skills that would be of most value to the company. The sessions adhere to a learning pattern of five or six 25-minute pomodoros followed by five minute breaks; each team then presents a showcase demonstrating what they’ve learnt and teaches something small to everyone else. Every other week, three groups do a second showcase of customer-focused work, exhibiting what they learned in previous #LearnTech sessions and later applied, so that everyone understands and crucially, doesn’t fear practical application. Initially the focus of #LearnTech was on TDD and test design, however in January 2018 we added web application, React and blockchain (specifically how to test drive these technologies). We also intend to further add front-end development, Java and JavaScript to improve polyglotism within the team. We’re purist about delivering working software and our previous ‘Made Learning Day’ format made deliverables a key component as a result of hacking together. This worked well for learning new languages, new tools or trying out e.g. writing games software. However, we noticed that learning hard skills, such as Test Driven Development, which required teams to rewire their muscle memory meant that teams couldn’t always deliver and made people feel dejected. This ultimately detracted from us being able to make progress during the Learning Day. Switching to LearnTech, deliverables (in the form of software) are not the emphasis so the team can instead focus all their attention on upskilling. Results and the future Learntech began as a team-led initiative with materials being provided, however it progressed to a level of self-sufficiency where everyone is responsible for their own learning. We also use it as a way to onboard new staff by training them in our methodologies efficiently and effectively and integrate them into the Made Tech culture: LearnTech helped me understand the kinds of things people wanted to learn to use on projects and gave me access to something I don't normally work on (blockchain), which has been great for getting me to grips with the core concepts. Having my peers actively admit the things they didn't know – even simple things – helped me do the same. It helps us ‘learn to learn’, which is a very important skill in fast-moving tech. It also means we all get together and often socialise afterwards, so I don't feel distant from people. – Kyle Chapman, Software Engineer The life cycle of the programme ensures skills are gained quickly and regularly, improved through application on consumer projects and eventually mastered so engineers can pay it forward. This cohesion and collective responsibility means that within six months our team has already gained new skills, reduced technical debt and increased positive outcomes. #LearnTech has been such a success with the core team, we’re now looking into how it can be scaled and incorporated into the Made Tech Academy and accessed by clients. At this stage, we have a show-and-tell model with accountability loops. We run frequent team or client-based showcases to show progress and share what we’ve learnt, so we need to ensure this remains practical and achievable when projects and teams get bigger. There also needs to be some thought around how to prioritise topics, when to switch between skills and ongoing assessment of learning through beta testing. In the meantime however, #LearnTech has helped exponentially on client projects; smoother processes, reduced unplanned work, increased quality assurance and confidence of each of our teams has greatly improved customer satisfaction. Win win! Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-05-24"},
{"website": "Made-Tech", "title": "When to fold ’em – how to avoid the sunk cost fallacy", "author": [" Kyle Chapman"], "link": "https://www.madetech.com/blog/when-to-fold-em-how-to-avoid-the-sunk-cost-fallacy/", "abstract": "Some years ago, I worked as part of a development team on a project to upgrade a tangled legacy system managing a company’s payment systems. Two months in, and we had very little to show for it – pages of diagrams that looked like spiderwebs, a few outages caused by failed attempts to untangle the pile, and a rising level of frustration. That company had spent thousands of pounds, not to mention missed out on a bunch of other opportunities – and not got a good return on that investment. Stakeholders weren’t any happier than we were, and so we talked about what to do next: We’ve learned some things about what we’re trying to tackle here. Let’s give it another couple of months and see if we can make any progress. – Hopeful project manager Of course, a month later, nothing had changed – it was the same team, with the same tools, and the same level of support. But we didn’t look at how to make it better. Instead, we talked about “not giving up”: We’ve already sunk so much money into this. We can’t throw that away now. Let’s keep fighting! – Hopeful project manager This is the sunk cost fallacy in action – treating what you’ve already lost as a factor in the decision on what to do next. Our brains were compiled in caves, and some of the software is very out-of-date. Today, we’re going to do some bug-fixing. Let the project die – kill it, if you have to But what is the bug? Aren’t you right to care about wasting money? Sure, waste is bad. But if you’re looking at it in the past, it’s already wasted. Since the past loss is happening no matter what, only look at the future results if you want to make the right decision. Let’s say this half-finished project of ours has cost £100,000 so far, and the cost of finishing it will be another £50,000. When complete, it’ll reward us with £150,000 of business value. Alternately, we’ve got another opportunity that costs £20,000, but makes us £100,000. Option A: Spend £150,000 total, get £150,000 Option B: Spend £120,000 total, get £100,000 And that makes option A look appealing – at least you make your money back, right? Option B looks like it’s a loss! But you can’t change the past, so your current decision is actually… Option A: Spend £50,000, get £150,000 (x3 return) Option B: Spend £20,000, get £100,000 (x5 return) If you wanted to maximise return on investment – and you don’t have a time machine, option B is the rational choice . In real life, decisions aren’t this clear cut – outcomes can’t be exactly predicted – but the same rule holds. Decide based on the costs and benefits from now on, because the past cost is already sunk, in both cases. Lazy load those decisions This is all a more specific application of one of the key parts of iterative development – we’ll learn as we go, so we’ll make better decisions later on . It’s important to stop, breathe and reflect, even when you’re in the middle of work, or you’ll miss opportunities to make the right decisions. Once you’ve learnt this lesson, there’s still some work to do. Whether you’re an executive trying to encourage this in your culture, or a developer trying to make the right decisions, you need to understand the context . Remember, making the rational choice isn’t the only thing that motivates people. Don’t fall into the trap of explaining why abandoning a sunk cost might be the best decision, whilst neglecting the emotional attachment your colleagues may have to their work. To make the right decisions, you need a culture that accepts failure and change, and many organisations aren’t there yet. Be bold and explain yourself clearly, and don’t despair if it doesn’t go perfectly straight away – that’s part of the process of getting better! Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-05-11"},
{"website": "Made-Tech", "title": "Welcome to the Skills Matrix", "author": [" Delphine Gattaz"], "link": "https://www.madetech.com/blog/welcome-to-the-skills-matrix/", "abstract": "A new approach to employee development Made Tech is where we are because of our people. We see the importance and value in progress and self-improvement and encourage a culture of openness and sharing because it creates an enjoyable and productive environment. With learning at the crux of everything we do, we wanted to improve software delivery within our own organisation first and create capable mentors to help everyone in the team progress. We therefore decided a new approach to employee development was needed. The idea was to provide clear paths for improving ability both internally and in customer teams. While reviews and continuous feedback systems work on a general level, we wanted to find the most effective way of increasing skill parity across a company, so we trialled a skills matrix. From skills ‘awareness’ to ‘mastery’ A variety of strengths and weaknesses can lead to inconsistencies in the delivery and quality of software. With the sheer speed at which technology is changing, it can be difficult to know where to focus learning and personal development. Rather than relying solely on self-reporting, observing behaviour can actually assist with diagnosing issues with both individual and team capabilities. We created a skills matrix based on the properties held by our team in relation to core engineering skills, programming languages, agile processes, products and techniques. We then invited people to add to the property list and rate themselves against each on the following scale: I would like to, or am happy to teach people this skill/technology I consider myself proficient/expert at this thing I know about this thing, and can do some productive things with it I’d like to learn more about this I don’t really know this This helped identify areas of expertise versus inexperience among staff and create individual development plans that incorporated a mentor/mentee matching system so that everyone could work together to plug any gaps. Overcoming ego Generally, some are quicker to take up the self-evaluating bastion than others, and therefore traction may be gentle rather than hurried. Egos can make it difficult for people to be honest about where or what they might be lacking. Plus, marrying awareness of expertise can also be challenging when people view such expertise of each property i.e TDD, in a different way. This misalignment can cause further confusion when trying to improve specific skills and therefore requires isolated focus to reach uniformity, which takes additional time. The most crucial element to ensuring efficacy of any skills matrix, is creating a culture that promotes psychological safety – ‘ the shared belief that the team is safe for interpersonal risk-taking ’. People need to feel comfortable and have a ‘safe space’ to acknowledge their own fallibility and open up about the gaps in their skills set. By asking the right questions, you can encourage people to think about what they want from their job and particularly, which set of skills they want to acquire. A common performance platform is powerful, but does not always equal efficiency. People need to be free enough – and ‘enough’ is key – to share the things that they wish to improve, both in relation to skill set and their place within the team and wider (including limitations to collaboration and productivity) without fear of recrimination. It’s just as important however, that they feel heard and that changes will be made or support offered to help overcome any obstacles to progression. A failure to do so will undermine the psychological freedom if concerns are simply shared into a vacuum. All together now Having a list of essential skills mapped out has made the ongoing task of self-development more accessible and efficient. Approaching employee development in this way allows organisations to positively impact their teams’ innovation and speed. It ensures faster failure, discovery of creative solutions to problems and better results. Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-04-26"},
{"website": "Made-Tech", "title": "A tale in adopting Alpine Linux for Docker – problems we faced with RSpec testing", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/a-tale-in-adopting-alpine-linux-for-docker-problems-we-faced-with-rspec-testing/", "abstract": "If you read Wikipedia you will find that Alpine is a Linux distribution that is based on musl (more on this later) and BusyBox. With the rise of Docker, it has become a favoured distribution for Docker images due to the greatly reduced size of its images. At the time of writing, the Ruby docker images are 343MB in their standard Linux distribution format, but are 28MB in their Alpine flavour. The tooling We use RSpec to test our code, a common testing framework for the Ruby language. One of its most distinctive features is the ability to create “hierarchical test suites” by using context blocks. As an example: These context blocks can get more powerful, and indeed allow sharing of setup between multiple test suites (can be desirable when controlling simulators of external services in tests). We used Alpine Linux as our Docker image in our Continuous Integration pipeline (CI pipeline), but locally we were using our native Ruby installation at this point in development since there are performance benefits. What we noticed was our CI pipeline went red: This was extremely strange since we did not notice this error locally. Step 1: Tracing the error What we knew from this error message: – The stack size was being restricted within our CI pipeline. Increasing the stack size At first, we thought there was an environment configuration difference between the Ruby in our CI pipeline and our local machines. So we attempted to increase the stack-size within the CI pipeline via the RUBY_THREAD_VM_STACK_SIZE environment variable. This had no effect. We thought this was very odd, since we still could not reproduce locally. What we knew then: – The stack size was being restricted within our CI pipeline. – This was not a configuration issue backed into the Docker image, and changing Ruby configuration did not work (which it should). Decreasing the stack size We then decided to reduce the stack size on our local machines (again with the RUBY_THREAD_VM_STACK_SIZE environment variable). What we discovered was that we needed to have stack sizes <500KB in order to reproduce the issue. This got us no closer to a fix, but did enable us to see the error locally and trace it to a recursive algorithm within RSpec. What we learnt here: – The stack size was being restricted within our CI pipeline. – This was not a configuration issue backed into the Docker image, and changing Ruby configuration did not work (which it should). – The configuration had an affect locally. Running with Alpine locally Gaining some parity at this point made a lot of sense. While we didn’t expect there to be a runtime difference within Ruby between local and Alpine, it was definitely worth ruling it out. Immediately we noticed the error locally. What we took away: – The stack size was being restricted within the Alpine docker image – The RUBY_THREAD_VM_STACK_SIZE environment variable had no affect within this docker image Step 2: Analyse the problem. What was wrong? Doing some Google searching we ended up at musl-libc’s FAQs, specifically the “functional differences” i.e. incompatibilities with glibc. What we learned is that musl provides a default stack size of 80k, which is a significant difference to the stacksize provided by glibc (generally 2MB-10MB). Moreover, it requires programs to explicitly ask for more (or less) stack via pthread_attr_setstacksize, which MRI Ruby does not appear to do. There is an open Ruby bug here: https://bugs.ruby-lang.org/issues/14387 What is RSpec doing? RSpec prints out a list of errors, and helpfully includes a snippet of source code from your test suite. In the example above, the string expect(1).to eq(2) is extracted from your test suite source code. The way it does is that it parses your Ruby source code using Ruby ripper , and then searches for the relevant nodes in the Ruby language Abstract Syntax Tree (AST). The more deeply nested our context blocks were in our RSpec specs, the more recursion occurred during this process. This requires a Depth-First-Search algorithm, which is more elegant when expressed as a recursive algorithm. Step 3: How we solved the error We submitted a patch upstream to RSpec to change the implementation from a recursive algorithm to a loop-based algorithm that does not use the stack. Ultimately this is a temporary fix, as any recursive logic could be a source of errors in production. This should be solved at the Ruby language level. The whiff of a larger design smell This is a Liskov-substitution principle violation. Liskov substitution principle?? That’s an object-oriented principle? How does a stack overflow error caused by libc relate to Objects? The dependency of what libc implementation is used is controlled by the Linux distribution (the caller of your Ruby application). This means the dependency is inverted, and we are looking at an object-oriented design. If you consider musl-libc and glibc as implementing an interface called libc, then you will notice that they must also be Liskov-compatible to ensure smooth operation. The imperfection here is that their public APIs are slightly different, which in Alpine’s case is by choice. Alpine linux is derived from the LEAF Project , which specifically targets low-powered hardware. The sort of hardware that would not have much space for 2MB-10MB of memory allocated for a stack. The interesting implication here is the community has decided to use, by defacto choice, a Linux distribution that has historically made some design choices to achieve both low memory and disk-size footprint. In our modern cloud environments, where we can spare 2-10MB of memory allocation for a stack, these design choices don’t appear to apply. The impact is that the Ruby codebase must be aware of what version of libc it is running against in order to provide a stable programming language to us Ruby developers. This is undesirable to the Ruby language developers, as it creates a reverse dependency on musl-libc, which increases the total cost of maintaining Ruby (if musl changes, so must Ruby). Ideally, the community that maintain libc implementations need to work together to expose the same behaviour (and agree on what this behaviour looks like). In reality, it is very difficult to achieve this. In practical terms, musl-libc has proven to be a very good replacement for glibc, and certainly provides a desirable disk-size footprint. As software engineers, choosing Alpine over a fatter base image requires consideration of the functional differences that musl-libc has compared to glibc. It is very likely that you will encounter issues, and when you do it is time saving to know where to look first. Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-04-20"},
{"website": "Made-Tech", "title": "What’s green on the outside but red on the inside? Hopefully not your project.", "author": [" Delphine Gattaz"], "link": "https://www.madetech.com/blog/whats-green-on-the-outside-but-red-on-the-inside-hopefully-not-your-project/", "abstract": "The skin of a watermelon presents a paradox or simultaneous reality of its core; both perfectly ripe and disappointingly mushy, but you won’t know which until it’s cut open. So you just leave it be; looking green and fine until it’s too late and… disintegrates. We’ve seen agile projects following this pattern – reports and feedback to senior management suggest all is well (green on the outside) – but if you scratch the surface and and talk to the team with day-to-day involvement, it isn’t long before it’s apparent all is not what it seems. Instead, things are somewhere on the scale of just above lukewarm (pink) to actually on fire (red) on the inside. While it’s important to have strong leadership in place to protect a team and provide the space to deliver, there is often a risk that such leadership can end up dictating what is reported back to the business. A balance needs to be struck; rather than controlling and manipulating data to provide good commercial results, the strength needs to be exercised by acting as a ‘head cover’ while paving the way to achieving better results. This is especially necessary when working within a complex environment that has strict reporting requirements and regulations. Showing and not telling You’re working on a huge project with a number of stakeholders and halfway through, costs, timescales and stress levels are on the rise and motivation and sight of the end goal, are declining. Despite this, the delivery manager prioritises reaching each milestone and / or sticking to budget constraints (however unrealistic) and, rather than flagging blockers to senior management, they’re hidden. The project moves forward and gains additional investment yet for all intents and purposes its failing from the inside out. Just not everyone can see this. Whether it’s through fear of the consequences or a misplaced, but genuine belief that the difficulties are trivial and can be salvaged in due course, the project is presented as being on track simply because that’s what management wants or in fact, needs to hear. In some cases, the misreporting has been driven by a client pushing a project forward because they need to succeed at all costs and cannot be seen to be failing. Suppliers are then pressured to take a disingenuous approach to reporting and are unable to communicate the full story. Decisions are then made by those at the top that aren’t reflective of the reality and at the expense of developers’ workloads and quality of the deliverables. This in turn allows issues to build up until it’s simply impossible to fix everything, leaving the manager to explain how a seemingly ‘great’ project has failed and what you’re dealing with is a watermelon. Honesty is the best policy There lies a tension between perceived success and actual success, and delivery managers who won’t report anything but a green status – possibly through lack of experience, knowledge or the space / support to do so – ultimately do more damage than good. A judgment call needs to be made as to when (not if) an issue needs to be communicated to senior management and stakeholders, but sooner rather than later is usually the best course of action. It should however, always be informed by the frontline team, who should be permitted to review and feedback on the material being distributed to management, prior to its dissemination. We’ve found the best way to avoid projects developing into watermelons is to implement a number of key control procedures: Be proactive rather than reactive – Learn to spot warning signs and address smaller problems before they become insurmountable or threaten to derail a project entirely. The bigger the project, the bigger the pool of stakeholders, which on the one hand means more people to reassure and satisfy when it comes to results, but it also means access to a well of wisdom that can help with risk foresight, analysis and management when harnessed effectively; Be transparent – Get everyone on the same page by utilising agile project management rather than waterfall style. This way, you can visibly demonstrate your MVP or software (even with problems) so that senior managers – particularly those without a technical background – can see the end goal and understand that the desired outcome will be achieved regardless of any interim issues; Adopt a ‘monitoring’ rather than ‘reporting’ approach – Use tracking that is both qualitative and quantitative in measurement to get reliable KPIs. The process of monitoring analytics and focusing on performance metrics is much more hands-on and strategic than that of reporting as it removes subjectivity and shifts the focus towards data. This helps avoid misleading or unhelpful information that may result in decisions being made that are not in the best interests of the project or the team working on it; Use retrospectives – At the end of every project (and even every milestone where possible), teams should reflect on what worked well, what didn’t and how they could do better so that behaviour can be adjusted accordingly for any future operations.  The objectives identified during retrospectives should be communicated to leadership to promote transparency and encourage a culture of openness, and then actionned in the next iteration by pegging them during sprint-planning; Build psychological safety for key influencers – Removing the barriers to escalating problems can be key in achieving success because psychological safety promotes learning behaviour among teams and helps shape outcomes. Project managers and owners need to be empowered to make key decisions and when required, bold changes; and Work to get rid of the blame culture – Arguably the root cause of watermelon projects, a climate of understanding needs to exist to enable innovation and progression. While accountability (and its consequences) needs to be recognised, politics should be eschewed and moves made towards creating a purposeful, value-driven culture where failing fast is permitted and actually leads to greater outcomes. Ultimately, it’s a combination of delivery managers taking ownership and responsibility for transparent reporting and having the freedom to do so. By equating investment with success you’ll see a direct correlation between input and effective project management, allowing you to achieve key deliverables and constructive results. Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-04-05"},
{"website": "Made-Tech", "title": "Made Tech Awarded 3 Government Publishing Tenders", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/made-tech-awarded-3-government-publishing-tenders/", "abstract": "We're excited to announce that we've been chosen to deliver three projects on the GOV.UK platform. Over the next few months, we'll be working closely with the GDS teams to deliver: End To End Testing for Publishing Applications GDS have built a multitude of microservices within the GOV.UK platform, each of which are thoroughly tested. An improvement they're keen to make is to have a similar level of testing on a broader level. We'll be working with them to provide a set of end to end tests for six of their publishing applications, ensuring the most common routes through and between each microservice work when integrated. Broken Link Checking Tool Across the GOV.UK platform, there are thousands of links pointing to external domains. As those domains change, such links are liable to break or, worse, lead a user down a malicious path. Our delivery team will be working to build a tool that evaluates the quality of external links in a number of ways and helps GOV.UK publishers identify links that are either compromised or simply broken. Dependency upgrades for Ruby on Rails applications Many of the applications running on GOV.UK are built with Ruby on Rails which, along with Ruby itself, regularly receives security updates and bug fixes. This project will involve ensuring each application is running on up to date software, and will introduce the automatic updating of Ruby on Rails and its dependencies to GOV.UK's Continuous Integration workflow. Each project's dedicated team of engineers will be working to both deliver software that provides a robust solution to each problem, and introduce strong software delivery practices that will benefit the GDS team and the GOV.UK platform in the long term. Founded in 2010, Made Tech's mission is to improve software delivery in every organisation. We work with our customers to deliver modern applications and help them move to a faster, leaner, and more agile software delivery model. For more information on how Made Tech's services and how we can help your organisation, get in touch with us via email: hello@madetech.com Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-12-13"},
{"website": "Made-Tech", "title": "What Does a Modern Retail Technology Platform Look Like?", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/what-does-a-modern-retail-technology-platform-look-like/", "abstract": "Far from technology star gazing, we explore what a modern retail technology platform could look like. Established retailers are typically encumbered by a number of legacy technology choices that are deeply embedded in their business – from a creaking SAP rollout, to proprietary Warehouse Management Systems that are resistant to integration. Starting with the customer, and with the goal of delivering a highly cohesive brand experience through multiple touchpoints, more recent competitors have been able to quickly compete with more established players, in no small part due to their ability to make modern technology choices from the outset. A Collection of Best of Breed Systems Rather than trying to find an all-in-one solution to run their retailing operation, modern retailers look to combine a number of best of breed services, opting for discrete Data Warehousing, E-commerce, and Product Management packages, for example. Key to adopting and integrating disparate systems is choosing systems that have both open and high quality APIs that expose that systems capabilities to other applications. Beware of any software vendor that sells \"connectors\" to integrate with other third party systems. Open APIs allow the retailer to choose how to best integrate their systems. Amazon.com is famed for its adopting of a service oriented approach. It's reported that rendering the Amazon.com homepage involves in the region of 120 services, from recommendations to stock availability. A Common Messaging System To avoid tightly coupling software services, a common messaging system can be used to enable each service to publish new events and to subscribe to receive particular types of events emitted by other services. This avoids the services communicating directly, which is a pattern that often leads to tight coupling, which in turn can easily result in services becoming overly dependent on one another. There are examples of Zalando, Etsy, and Shopify using messaging systems to provide a means of communication between their services. Clear Boundaries Between Systems An important architectural mandate for modern retail platforms is to provide clearly defined boundaries between the various services. The responsibility of each service needs to be clearly defined, as well as how the service will interoperate with the other services in the retailer's landscape. The boundary of each service should provide a contract that specifies the format of the events that it will emit, as well as the type of events it will respond to from other services. Returning to the example of Amazon, one of the pivotal moments in the company's history was Bezos' 2002 mandate that every business unit must expose their data and functionality via API. As well as giving rise to Amazon Web Services (currently a $16 billion revenue stream), it allows business units autonomy in solving their business problems with technology, while mandating common standards. A Continuously Evolving Technology Landscape Many legacy retailers treat technology evolution as a decades apart expensive big bang upgrades and replatforms. Painful enough to avoid for as long as possible, and perhaps a little beyond. More modern retailers shift their mindset to think of technology investment as an ongoing operational expense over a one-off capital expense, investing to improve and evolve the technology landscape as an ongoing effort. As a modern retail platform will be comprised of a number of smaller systems, replacing individual components as they reach end of life becomes a less disruptive activity that doesn't have to necessitate large-scale organisation change. Providing the new service can deliver to the same contract as the outgoing service, other services or areas of the business shouldn't even need to be aware that the underlying software system has changed. Read more on how we helped online retailer Surfdome to evolve their retail monolith to microservices . Modern Technology Continuously Delivered A nod also needs to be made to how, and how often, forward-facing retailers are pushing changes to their platforms. Practising Continuous Delivery – the automation of the build, test, and deployment steps of software delivery – has been the norm in modern businesses for the best part of a decade. This allows more frequent and more reliable changes to be made to production software systems. Retailers such as Etsy have lead the way with this approach, though we observe many established retailers in various stages of adoption of Continuous Delivery and wider systems and development lifecycle automation. Sainsbury's, for example, are particularly active in the London Ansible scene . Where to Start For retailers who are carrying the burden of legacy systems and services, it can be hard to see a route forward to a more modern platform without ripping out the foundations and starting afresh. This is seldom a sensible choice when a business needs to continue trading in the short to mid-term, and seldom does the big bang replacement deliver the desired business value, or the promised technology nirvana. In almost all cases, we would suggest adopting an evolutionary approach. Identifying a system, or subset of a system that is approaching the end of its useful life, and look to extract those responsibilities out to a new, single purpose service. Techniques such as Anticorruption layers or the Autonomous Bubble Pattern to allow new services to exist alongside elements of the legacy platform. The evolutionary approach removes significant risk by focusing on a smaller area of the system as the candidate for replacement, allows uncertainty to be uncovered sooner, and delivers value back to the business by getting something working into the hands of end users faster. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-12-06"},
{"website": "Made-Tech", "title": "How we accidentally became cloud migrators", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/how-we-accidentally-became-cloud-migrators/", "abstract": "When coming up with an explanation of our cloud migration heritage for my talk at Red Hat Forum the phrase “accidental cloud migrators” came to mind. You see we didn’t set out to help organisations move workloads to the cloud but nevertheless we’ve found ourselves helping organisations modernise and migrate. On reflection, I think accidental cloud migrations are the best kind of migration, let me explain why. We are a company of software engineers of mostly web based apps, platforms and microservices. We wouldn’t dream of building a greenfield system on-premise or in a datacenter, it would take too long and it’s too costly. We can orchestrate and test cloud infrastructure with the tool we know, code. If you look at the world from our perspective for a moment, why would you physically go to a datacenter and install hardware when you can provision in the cloud and deliver value sooner? Going one step further, why would we use an admin interface to configure anything in AWS when we can use code instead? Manual steps lead to human error. From the early days of cloud we’ve been writing code to provision and orchestrate our infrastructure with tools like Ansible. We happen to be early users of Amazon Web Services with our team using it commercially even prior to Made Tech coming into existence 9 years ago. It’s just a default tool of choice for us whereas its a game of catch up for older organisations. I suppose this is why we hear about big 2 year programmes of work to lift and shift legacy infrastructure to the cloud. “Let’s get to the cloud and quick before we are disrupted,” businesses cry. Our journey into legacy applications When Made Tech started out we focussed mainly on greenfield custom applications. Sure, we integrated with some older systems but for the most part the issues of legacy weren’t something we had to deal with. This was because the majority of our customers back then were startups wanting to get a product to market in 100 days. This really helped shape our lean thinking and approach that we continue to use to this day. Our approach and mindset seemed to appeal to various CTOs and Heads of IT in more established startups where we began to work along side existing engineering teams. Whilst in-house engineers were bogged down fighting fires, we would be delivering value for the business. Helping the in-house engineers with their fires would usually end up becoming a priority. We would share practices like continuous integration and delivery, automating BAU chores, acceptance testing, pair programming, variants of agile, etc. to help improve effectiveness, quality, and in the end, happiness of teams. Legacy isn’t limited to applications. Process, culture and decisions are all legacy too. – The more established startups had their fair share of legacy applications albeit built with relatively new technology and still mostly with a cloud first approach. Good practice was still missing though with cloud infrastructure configured by hand, manual or semi-manual deployments, a lack of automated testing and so forth. As we began to work with larger and larger organisations, we began to realise a few of things: Legacy can be very complex and is often a political issue Our lean and agile approach could probably benefit larger and older organisations It would be tough job to adapt ourselves to those organisations We began to see on-premise and datacentre infrastructure more frequently. Mix that and the politics of global organisations and it was clear to see why things are so hard to change. It was at this point we were realising that technology is only part of the problem. We were learning that process and culture were hugely important factors that needed to change too. Legacy isn’t limited to applications. Process, culture and decisions are all legacy too. Cloud migration as a side effect The common theme among our customers nowadays is that their business is hindered by legacy systems and technologies. One such legacy system we see time and time again is the proprietary ERP at the centre of a business. It’s a common issue with ecommerce and supply chain businesses. Traditional ERPs are expensive, often requiring on-premise servers and they limit a business’ ability to change business process. The last point is a often a huge one, businesses will either force their processes to fit the ERP or will default back to paper solutions or other workarounds. We’ve helped a number of businesses move away from legacy ERP, purchasing and warehouse systems. Rather than focus simply eradicating legacy, which is a fallacy by the way, we put together a roadmap that explains the business value modernisation and migration will bring. We usually list 4-5 goals per roadmap to measure ourselves against: Onboard new clients easily and be able to handle more business Reduce manual work and paperwork for more efficiency Team and clients can trust the quality and granularity of data Work with a greater number of suppliers & partners These goals speak nothing of a cloud migration or removing legacy systems. They are goals, that if met, will provide new value to the business in the form of efficiency, trust and scale. All of these projects did end up being implemented in the cloud or at least in some hybrid form. Why continue to pay outsourced IT technicians to keep servers running in your office? Why require remote workers to VPN into your office connection rather than VPN directly into a cloud setup? Reliability is a huge issue when talking about on-premise, as well as it being slow to change and expensive. Cloud based solutions. Modern architectures and approaches. Change to culture and process. All must be implementation details of trying to provide more value to a business. Migrations driven by business value It’s in this sense that we are accidental cloud migrators. We’ve grown our skills around working with on-premise and datacentre legacy systems, working out how to transition businesses iteratively and therefore safely to the cloud without ever directly trying to sell cloud migration as a concept or set cloud migration as a goal. We continue to “accidentally” migrate our customers to modern infrastructure in the cloud but only where we can prove it will provide business value. We do it because we believe this is the best kind of cloud migration. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-11-27"},
{"website": "Made-Tech", "title": "3 Ways to Improve Your Microsoft Dynamics NAV User Experience", "author": [" Chris Blackburn"], "link": "https://www.madetech.com/blog/3-ways-to-improve-your-microsoft-dynamics-nav-user-experience/", "abstract": "We've worked with a number of organisations who make use of the Microsoft Dynamics NAV suite as their Enterprise Resource Planner (ERP). As is common with similar off the shelf tools such as SAP and FinancialForce for Salesforce, it can be hard to provide a productive and enjoyable user experience, often requiring a number of unituitive steps on several screens to perform basic tasks such as logging a sick day. At a recent conference a delegate recalled that they were sat on over £1000 of expense claims because they couldn't face trying to login to their ERP system! Poor enterprise software user experience has a long documented detrimental impact on organisations, including low user adoption, reduced job satisfaction, and significant productivity hits. Often these large ERP systems are procured because they provide a comprehensive list of features (payroll: tick, warehouse management: tick, purchasing: tick etc.), without providing sufficient attention to how these features actually work from the end-user perspective, or whether customisation will be needed to bend the way the features work to meet specific nuances in the organisation's workflow. Further, we often observe that the need for an organisation change program to successfully adopt a wide-reaching ERP system is underestimated or missing entirely. Particularly where these systems dictate very specific ways of working. There are three primary routes to improve your Microsoft Dynamics NAV experience: Adapt your processes Some of the more successful ERP rollouts are realised by giving up all resistance, and modifying the organisation's workflows to match how the ERP expects you to work. This can often be the most costly initial rollout, as it can involve lots of organisational change, and can sometimes result in processes that are less optimal for the tasks the organisation needs to achieve, but keeping the ERP system in a more standard state does provide some significant advantages when looking to upgrade to newer versions. Adapt the ERP Microsoft Dynamics NAV promises that it can be easily customised, which is true to an extent. It's possible to customise fields, reporting screens, and so forth. However, there are limitations to how far this customisation can stretch. It's not possible to provide a fully tailored user experience that matches complex local workflows. Often customisations result in complex multi-step workflows, fields with unintuitive names and inputs, and usually require highly paid specialist consultants to implement. There is also another significant downside to performing heavy customisation inside the ERP – that it makes upgrade paths much trickier. Often with each new major release, local customisations will need to be updated or rewritten to work with the latest version of Dynamics NAV. Build a bespoke experience outside of Dynamics The final option is to consider the role of the ERP as a data store and rules engine, and to build a bespoke user experience outside of the ERP that interacts with NAV. Microsoft Dynamics NAV provides an API that allows third party applications to read and write data to and from it. This enables a fully tailored and modern user experience to be delivered to users, which can be designed to closely match their needs, keeping them as productive as possible. This route also has the advantage that upgrades to the ERP system should have less of an impact on these customisations – providing the API remains stable. Or at the very least, it clearly isolates the areas of the customisation that require upgrade to the interface with the API. Summary As with the adoption of any large scale software platform, consideration needs to be taken not just in the specification sheet meeting your purchase criteria, but in how you plan to integrate it with your organisation – particularly in whether you're expecting to widely adapt your way of working to embrace what the software provides out of the box, or whether you're expecting to embark on a program of changing the software to meet nuances in the way your organisation operates. If you're looking to customise the product to meet your needs, which particularly in cases where it is likely to offer improved productivity or user engagement, can be a worthwhile endeavour, it's important to consider where these customisations take place – whether directly inside the software, or in wrapping an additional layer around the software. Particularly with Microsoft Dynamics NAV, we've seen the greatest success in building a lightweight bespoke interface that sits outside of the ERP. This allows the business to keep control of its custom business logic in its own application, rather than embedding it directly in the ERP. Consequently, it provides easier upgrade paths to newer versions of Dynamics and NAV, and in the most extreme cases, keeps future options open for migrating to other ERP platforms without disrupting the end user experience. Additionally, designing and building a tailored user experience around the specific tasks users need to achieve can often deliver a significant uptick in productivity, usability, and user adoption. Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-01-23"},
{"website": "Made-Tech", "title": "The Agile Path to the Cloud", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/the-agile-path-to-the-cloud/", "abstract": "Cloud is good for us. So too are practices like DevOps and Continuous Delivery. It's easy on the surface to read a few articles and think, \"yeah, that's a good idea,\" but putting them into practice on the other hand isn't so easy. The reason things aren't so simple is that technology is only a third of the problem. The other two thirds are culture and process in equal measure. As we adopt new technologies we have to make cultural and process change too. You'll often hear about how scalable the cloud is and how DevOps can lead to delivering value faster, but actually adopting a cloud or DevOps approach can often be glazed over. I think this is because the solutions are hard, not one size fits all, and not everyone has the answers. Gradual change When explaining the concept of gradual change I often use Henrik Kniberg's agile versus big bang diagram. It compares delivery value of both agile and big bang approach over time. Agile providing an increasing amount of value right from the start and big bang providing its value all at once. I'd argue that taking the gradual approach in software delivery is almost always the best option to take. The reason for this is that cost of change in software shouldn't increase over time or, at least, not as much as say architecting buildings. Analysing cost of change Once you've built a building it is costly to significantly change its structure or function. It is expensive to build a floor and then change your mind based on learnings. This is why up front planning and design is necessary. In software, there is also a cost to change but in comparison it is cheaper and it does not increase exponentially over time. In the above graph you can see traditional cost of change increases sharply over time. Agile software delivery on the other hand does not; while there is a cost associated, the cost does not continue to rise over time. The important takeaway from this is that, in software, it is attractive to adapt your plan after starting a project as the cost is not great. In fact you get many benefits from allowing learnings to change your plan. By not having a detailed plan up front, you save time, and can provide even greater value to your business as your learnings can be brought back into the delivery. Applying cost of change theory to cloud The world of data centres and on-premise are closer to building a building than delivering software. You need to plan in advance your requirements for new infrastructure, work out what capacity you are going to need when you put it live and predict growing demands on capacity over time. Getting this wrong, or changing later, has a significant cost associated with it. In the cloud, things are different. Spinning up new infrastructure is quick, does not require trips to data centres and finding more space for racks. You can make and provision enough machines you think you need, if you later realise you over provisioned you can ramp things down in minutes rather than days and weeks. It goes the other way too; if you need to scale out to more machines, that again is trivial in comparison to a data centre. Moving your operation to the cloud means you can – and in my opinion should – move to an agile delivery approach with less upfront planning. A changing world Not moving to an agile approach, in fact, is the underlying reason why cloud migrations fail. There isn't enough history and knowhow for organisations to be able to know up front what they'll need to provision in the cloud. In most cases it isn't as simple as lift and shift. Ideas need to be reworked. With a traditional hardware focussed operations team, going all in with cloud is going to be a rough ride. New skills need to be learned. Performance in cloud is a different beast too. Great performance can be achieved in the cloud but architectures need to be evolved for that to work. It's no surprise performance is one reason cited for failure. Organisations may recognise lack of skills in the organisation for cloud and end up outsourcing everything. Firstly, in doing this, you are ignoring a capable workforce that needs retraining. Perhaps more importantly for the bottom line, outsourcing to offshore companies doesn't necessarily lead to success any faster. You'll purchase a 2 year migration, it'll be extended by a year after \"unexpected delays\", and finally fail after millions are spent. I'm not saying that there aren't successes in lift and shift to the cloud nor am I saying you can't get it all done in one go. What I am saying is that it's a much riskier approach and unless you can validate in no uncertain terms why a lift and shift approach is a good idea, it's most likely smaller steps will be safer. Far better, embrace the fact that change in culture and process is necessary if you are to change the technology. Cloud is a new world where new techniques are required. Invest in your organisation's future by embracing them. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-10-13"},
{"website": "Made-Tech", "title": "Where to start with cloud migration?", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/where-to-start-with-cloud-migration/", "abstract": "Your organisation has decided it's ready to move to the cloud. Where do you start? Defining a roadmap isn't as simple as writing a list of servers that need migrating. Cloud migrations are very rarely a one to one lift and shift from on-premise to a cloud provider like Amazon Web Services. Not only are migrations not as simple as lift and shift, the prioritisation of what to move and when is fundamental to making a migration successful. Knowing how to phase the migration into many small steps is crucial for reducing risk. There are a number of questions that can help inform your roadmapping process. What are your mission critical applications? Changes to mission critical applications will have significant risks associated, will likely impact other applications, and will potentially be the most disruptive to the organisation. It is for these reasons that mission critical applications should be tackled first. If your business sells products and services to customers via an application then this system will likely be mission critical. Systems such as a trading platforms or bespoke software, perhaps even running in your customers premises, are also likely to be critical. If your business uses software in order to function, and let's face it, this is most organisations, which systems could cause serious damage to your business if they disappeared for a few hours, minutes, or perhaps even seconds? Breaking this analysis down into processes or workflows can also be useful. A single application may well provide multiple processes and workflows of which only a subset are mission critical. It's important to split these out. Now you have a list of mission critical functionality, you have the beginnings of your roadmap. What are the goals behind your migration? Modernisation and cost savings are great supporting arguments for moving to the cloud but they aren't the only goals that should be driving your migration. In order to decide how to break the migration into reasonable chunks, and in order to be able to prioritise what to move first, goals that focus on improving the business value provided by existing applications is essential. I'll illustrate what I mean with a couple of examples. Example one: Large monolithic business application Our first example is that of an application that is a business's central database, provides workflows for business processes, is the CRM, and provides accounting tools for an organisation that is currently running on-premise. Clearly the organisation is tied into the application and as such a migration to the cloud could prove quite disruptive. Lift and shift would be difficult because such systems often rely on running on the users machine, having access to local printers, accessing a database on the local network, shared storage, and likely more ties that make it hard to remove from on-premises. Not only this but it would also require switching the entire business over to the new system all at once seeing as everything is tied into a single system. A byproduct of understanding how locked in the business is to the single application, and understanding how risky an all-in-one migration might be, there may be significant business value to be gained from breaking down the monolith into smaller focussed applications. Breaking an application down into applications per process or workflow would mean the migration could happen process by process or department by department. This would be less risky than moving the whole business at once. Moving accounting and CRM to off-the-shelf SaaS products would mean two less pieces to move across and also mean you could benefit from using software that focusses on doing one thing and one thing well. From this analysis, 2 additional goals for the migration could be defined: Decouple business software and processes from one another in order to enable easier change in future Move to off the shelf software where possible so engineering efforts can be spent in more relevant areas (rather than reinventing the wheel for accounting and CRM) Example two: Focussed supply chain application In our second example, a business has a set of more focussed applications. One of their mission critical systems is their supply chain tool that manages stock ordering. The application is by all accounts ripe for the picking, however there have been many complaints from the business that due to the poor user experience of the application, it is a source for a lot of bad data within the business. Inconsistent product naming, many records in the product database for the same product with only subtle differences are among many data quality issues. Migrating the application as it currently is won't be providing any new business value and also will be extending the life of software that is currently a poor fit for the business. It might be wise to use this opportunity to improve the user experience of the software, running the new version in the cloud alongside the existing version means an easier transition for the business, and an opportunity to improve data quality before switching off the legacy system. From this analysis we can define a new goal for the migration: Improve quality of data being entered into system Both of these examples are real strategies we've applied to two separate customers successfully. Cloud migration must be tied to significant business value, otherwise the migration may perpetuate legacy problems. By defining business goals that provide new value you can make decisions on how to approach the migration. How cloud ready are your applications? Understanding how cloud ready your applications are is important as it will likely impact how you approach the migration. What kind of system dependencies do your applications have? Are they operating system dependant? For example, do they have to be run on Windows? Do they require other software to be installed on the systems on which they run? Answers to these questions can change how a migration might be approached. How is your application consumed? Do users access it via a web browser? Is it accessed via an intranet or VPN? Is it connected to via a remote login session? Again, answers to these questions will affect the roadmap. If your application backs onto a database or other service like SMTP for sending emails then these two will need to be migrated. If you can change where you application looks for these services via configuration, you're lucky. A lot of legacy applications will have hard dependencies that are not so easy to change. You may recognise some of these questions if you are familiar with \"The 12 Factor App\" . This seminal document defines 12 properties of a cloud ready application. If you plan to create new applications, especially if you're breaking a large legacy system into smaller pieces, then this is a great time to ensure your applications are cloud ready. How ready is your organisation for a cloud migration? Change in an organisation is always a challenge. The larger the organisation the harder change becomes. Migrations are both disruptive and technically demanding. By breaking down migrations into smaller steps, disruption will be mitigated somewhat, attention to timing with other milestones in the organisation is important. Changing e-commerce systems just before Black Friday and Christmas period is a risky approach. Replacing accounting software at year end is also likely to be more disruptive than necessary. Organisations that have traditionally used on-premise servers or datacenters for running their systems may be lacking skills required for a cloud migration. The toolsets for cloud can differ from more traditional approaches. The use of automation tools like Ansible, Capistrano and Fabric may be less familiar. Using such tools to create and scale servers will certainly be unfamiliar. Infrastructure as code techniques and knowledge will need to be gained for the migration to be successful. Architecting applications to run in the cloud differs too with scale out approaches being far more common than traditional scale up. Data warehousing techniques look a lot different, rather than large data crunching applications doing all the work, you're more likely to find smaller services being stitched together with queuing systems like Amazon Web Services Simple Queue Service or Kinesis. Skill gaps can be filled by external consultants and teams, however dragons can lie within this solution. System Integrators aren't always as familiar with modern cloud practices as they claim, and often upskilling an existing workforce will pay off in the longer run. By breaking down the migration into smaller pieces, your teams can build experience moving smaller pieces across to begin with, building confidence and knowledge as they go. Cloud migration isn’t a one size fits all Though it may be tempting to see a cloud migration as a lift and shift operation, moving your existing applications to the cloud, but in reality it's a little more nuanced than that. In fact, tackling a cloud migration in a large programme of work in such a fashion is risky, unlikely to provide business value, and perhaps even worse if the migration succeeds, you may end up extended the shelf life of legacy systems. Understanding which of your applications are mission critical and identifying goals to provide new business value through them will help you decide an approach to migrating that makes the most of cloud. Understanding the cloud readiness of your existing application estate, the readiness of your organisation, and technical readiness of your team will help you identify blockers that will need to be tackled before you begin migrating. We'd like to invite you to share your questions about cloud migration with us. Tweet us on @madetech , or get in touch with myself directly on luke@madetech.com . Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-09-26"},
{"website": "Made-Tech", "title": "No Uncle of Mine", "author": [" Igor de Alcântara Barroso"], "link": "https://www.madetech.com/blog/no-uncle-of-mine/", "abstract": "In August, one topic sparked a lot of debate on the internet in general, and in the Tech industry in particular: the so-called \"Manifesto\", written by James Damore, then engineer at Google. While this blog post won't be about the manifesto, some events that occurred after it came out prompted me to write this. But before looking into it, let's go over some of what has been said and written since then. Damore’s Manifesto and subsequent firing. That manifesto has definitely received more attention than it deserves, to the point of even having a lengthy Wikipedia entry . Since it became public, many have discussed this in more detail than I'll be able to do here, so to keep this short, here is some interesting reading on the subject. The manifesto relies on evolutionary psychology, which as a field is at least debatable . Even some scientists from that field raise questions on Damore's take. The manifesto does show sexism . And my personal favourite: Sharing such a document to the whole company turned the author into someone people could no longer work with . Teaching and punishing Now, amongst the multitude of blog posts, news articles, and tweets that followed the release of the memo, one influential person within the software industry, Robert C. Martin, came out with his take on the matter. Well, actually, Robert did not focus on the content of the memo, which he acknowledged was wrong, but instead focused on Google's decision to fire Damore. According to him: You never punish bad ideas. Instead, you counter bad ideas with better ideas. And as such, not only was the firing a wrong decision, but it would in turn make people uncomfortable sharing their ideas, which would endanger a company as Google that relies on innovation. While I agree that you should try to discuss things freely, and allow people and their ideas to evolve through debate, there surely is a limit on how much time and effort a company can invest into the debating process. In this particular case, it seems like Google did provide opportunities both for discussion and learning, as is mentioned in the manifesto itself. If after this, someone continues pushing on a subject with a rhetoric that hurts the company and its employees, surely the company can be forgiven for drawing a line and cutting its ties to that person. Furthermore, it is indeed important that people feel like they can share their ideas freely. But this raises the question: what should a company do when one of its employees says half of his coworkers are less suited for the job than him? If anything, letting this kind of thinking foster inside a company would make the people targeted by that memo less confident in themselves and hence less prone to share their thoughts. And while freedom of speech is important, one should try to remember that: The exercise of the natural rights of each [person] has only those borders which assure other members of the society the enjoyment of these same rights. – https://en.wikipedia.org/wiki/Declaration_of_the_Rights_of_Man_and_of_the_Citizen Finally, while a company like Google does rely on innovation to continue its business, there was nothing new in that manifesto. The points Damore made were old ones, and you would be excused for hoping these would be considered defeated in the 21st century. The dangers of missing the point In an industry where heterosexual cisgender white men are over-represented, I strongly believe we could be doing a better job of denouncing, refuting, and standing up against points of view and behaviours that make our industry less inclusive or less welcoming to members of underrepresented groups. And no, the issue of diversity cannot be reduced to a pipeline problem . Obviously, this is a personal belief, and I could not reasonably expect Robert to act accordingly. But what worries me, is that I find there is something toxic in Robert's take, that he failed to perceive, and that I believe could help him make sense of the sheer amount of negative feedback he received: he missed the point. Putting aside the tone of his original article, which could be seen as inappropriate to the situation, but was truthful to his style of communication, his focus was to try to make a seemingly reasonable point on an adjacent topic. As someone coming from what others will perceive as a position of privilege, his attempt to address the situation in a \"moderate\" manner brings to mind the image of a man in more sinister times who […] prefers a negative peace which is the absence of tension to a positive peace which is the presence of justice. – https://www.africa.upenn.edu/Articles_Gen/Letter_Birmingham.html Furthermore, no \"thought police\" were involved in the incident. The Ex-Googler did not get arrested because of what he thought , he was fired for probably many reasons including what he wrote and the subsequent publicity of that Manifesto. Giving an article the title \"The Thought Police\" makes it sound like its author believes persecution was at play, which would make Damore a victim. In turn, this starts to resemble Reverse Victimization , which is something that clearly appears in the Manifesto itself. Greater than code When trying to articulate my personal thoughts around what disturbed me in Robert's article, I also started to perceive something else about him. His importance in the software industry is largely due to the many and great contributions he made to what we call Software Engineering today. He has spent a considerable amount of time and energy sharing his thoughts on how we write software that delivers value, that can be depended upon, and that can adapt to change. But his focus revolves primarily around personal skills, and while there is definitely value in honing one's skills — or as Robert would say, one's craft, this is only part of our jobs. In order to deliver software, we need a team whose collective output is more than just the contribution of each member. This is a social challenge, not a technical one. Some simple examples include: Knowledge has to be shared amongst the team to prevent silos and to increase the bus factor . According to Conway's Law , we need good communication structures inside an organization to produce well designed systems. Diverse teams are more efficient and innovative . As such, so-called \"soft skills\", such as Communication, Creativity and Collaboration, could arguably be seen as more valuable to a company. Especially since technical abilities can be taught, whereas the previous ones are harder to develop. In conclusion, I realize I should not be surprised by, what is for me, Robert's \"faux-pas\". As he recognizes himself , such topics are not his strong suit. Personally, while I'll certainly continue to try and improve myself on the technical side thanks to his work and others , I will also be de-emphasizing Clean Code in favour of more Livable Code . Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-10-12"},
{"website": "Made-Tech", "title": "Red Hat Forum London: Agile Cloud Migration with Ansible", "author": [" Scott Mason"], "link": "https://www.madetech.com/blog/red-hat-forum-london-agile-cloud-migration-with-ansible/", "abstract": "We're excited to announce that our very own Luke Morton is going to be giving a talk at this year's Red Hat Forum London , on October 3rd. Entitled \"Agile Cloud Migration with Ansible\", Luke will be discussing our experiences using Ansible to perform cloud migrations in an agile way. Organisations of all sizes are finding themselves faced with the considerable challenge of migrating a number of legacy applications to the cloud, and often with a lack of migration experience. We've found that the answer lies in not treating a migration as a single, monolithic and risky task. By taking a number of smaller steps and prioritising the elements that need to be moved to the cloud, we've been able to reduce risk and build both confidence and experience within organisations that are trying to make such a change. If you'd like to hear more and throw some questions his way, Luke will be giving the talk at 13:45 in the Pepys Suite, as part of the Specialist Breakout sessions taking place on the day. About Red Hat Forum London Red Hat Forum London is the premier UK open source technology event to showcase the latest and greatest in cloud computing, platform, virtualization, middleware, storage, and systems management technologies. The event brings together IT decision makers, architects and developers (to name a few) to discuss hybrid cloud, devops, containers and many more topics that are driving innovation. Come and say hello! As a sponsor of the event, outside of the talk you'll find us at our stand, where we welcome the chance to speak with anyone who'd like to find out more about overcoming the challenges involved in migrating legacy applications to the cloud, and how Ansible can help. We'll also be handing out copies of our new book, Building High Performance Agile Teams , so feel free to stop by and grab yours. We'll be active on Twitter on the day too, drop us a message @madetech or just add #RedHatForumUK to your tweet. Key info Topic: Agile Cloud Migration with Ansible, by Luke Morton Time: 13:45 Location: Pepys Suite We look forward to seeing you there! Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-09-25"},
{"website": "Made-Tech", "title": "Enterprise lacking skills required for cloud migration", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/enterprise-lacking-skills-required-for-cloud-migration/", "abstract": "For all the excitement and talk of cloud it seems the reality, at least in enterprises, is a little less glamorous. With more and more executives backing the move to the cloud, more and more organisations are booting up large migration programmes. Unfortunately their aging IT departments lack the experience required and organisations are forced to depend on service providers to fill the gap. 35% have migrated applications and infrastructure to the cloud that were ultimately brought back on-premises – http://it-trends.solarwinds.com/ Worse yet, in one survey organisations reported a third of migrations as failed. The reasons for migrating back were: security and compliance, poor performance, cost, and technical challenges. These obstacles must be faced for large organisations to modernise. Security and compliance is not a technology issue The situation would be much improved if organisations had the skills necessary. Security and compliance is often blocked by a lack of knowledge regarding how to apply existing regulation to new technology. Cloud providers are working hard to provide far greater security than that of on-premise. Public cloud is making great progress with Capital One and Pacific Life being big case studies for AWS. Azure and Google are also building their own case studies in this space that has up till now been wary of public cloud. Researchers for Deutsche Bank are calling 2017 the year of cloud migration for financial institutions. If the use cases are supported by the technology now, it's surely more a battle of politics and resourcing for highly regulated industries. Cloud performance is achieved with new techniques An issue we see time and time again is the fitting of a square peg into a round hole. Whilst it's tempting to try and run your applications as is within a cloud environment, this will certainly lead to lots of head scratching. There are new fundamentals for building applications for the cloud. That's not to say everything needs to be rewritten but they must be adapted to operate in the cloud. Workloads need to be handled differently too. Gone are the days of \"scale up\" which is the technique of adding more RAM or CPU to your servers to handle larger amounts of work. \"Scale out\" is now the preferred mode of achieving performance at a lower cost. This often means breaking down larger workload applications into smaller ones which represent individual tasks rather than a whole data pipeline. Cloud offers a different model for seeking performance. Issues are encountered when trying to apply the monolithic application approach of the past to an infrastructure that is better utilised when broken down into simpler pieces. Again, at the core of this issue is the lack of knowledge on how to approach the opportunity provided by cloud. Migrations shouldn’t last years but months at most When organisations rightly recognise their lack of skills, purchasing a huge department of engineers (often 50+) from service providers becomes tempting. A huge amount of legacy requires a huge amount of engineering power right? It turns out the decision to purchase multi year migrations from oil tanker sized service providers suffers from a few problems too. Firstly, the old guard of service providers who still think throwing 50 engineers at a problem will be faster than a team of 5 are equally ill-equipped in skills to migrate to the cloud. Just as workloads need to be broken down, the approach to delivering the migration needs to be adapted too. Instead of migrating many applications at once, build a roadmap for migrating one. Commit to the spend of migrating only that one piece of the puzzle. Organisations need to work to improve cloud skills If organisations are lacking the experience required for migrations, that experience needs to be attained from somewhere. External help has to be in the mix but it may not necessarily mean getting a service provider to do all the work, especially if you already have IT / engineering teams in house. Having a single team who work in the same physical location be responsible for the end to end delivery of the migration of a single application is an approach we've had great success with. By reducing the complexity of the migration by limiting it to one application, and providing breathing room to learn, teams can thrive in these empowering environments. Bring in external experts in cloud to help train the team in new ways of working and eventually you will have a cloud ready team. When training the capability in house is not an option, an external service provider will have to be engaged. The migration of a single application should not need dozens, or even a dozen engineers. If your provider is selling you any more than that you need to walk away. Quickly. Build on successes and learn from failures A failed migration of a single application, or delays to that migration, will have a significantly lower cost than if you committed to a larger programme of work. This lean approach will provide many opportunities to learn, build confidence, gain new skills, and reduce risk significantly. It's important to provide a safe space for teams to make mistakes and learn from them. We do not build experience without them. The enterprise world is often full of fear of failure and who can blame them when failure has consequence in millions of pounds and dollars. Enterprises will find the less riskier and far cheaper path to the cloud by breaking the journey down in smaller steps. Tweet 0 LinkedIn 0 Facebook 0", "date": "2017-09-04"},
{"website": "Made-Tech", "title": "Learning to learn: Introducing the four stages of learning", "author": [" Tony Griffin"], "link": "https://www.madetech.com/blog/learning-to-learn-introducing-the-four-stages-of-learning/", "abstract": "Few skills have the power to hold such sway over the course of our lives as learning, but for such a vital skill, the steps we go through on the learning journey are not always greatly understood. Learning is an everyday occurance, it’s something we start on the day of our birth and we’re expected to continue for the rest of our lives. If it’s something we’ll be doing forever, then why are there no classes in schools on the subject? A semester dedicated to Learning 101? The ways in which to learn are many and often unique to the individual. Rather than analyse different learning approaches in depth, I hope to show you that there are recognised stages of learning that people generally experience regardless of their own learning style. Knowing which stage you’re currently in can help you stay calm, keep you motivated and allow you to persist when learning gets tough. The knowledge skyscraper Is it just me or is the pressure to learn increasing over time? If you think about it, our knowledge as humans only increases as our society continually advances. There’s an expectation to learn more, learn faster and learn better. Think about being a software engineer, new technologies are released every week, we are expected to keep up and keep learning to stay relevant. This is the pressure I’m talking about! Humans are the only creatures on the planet to be able to pass on knowledge, thoughts and feelings from one moment in time to another. The extraordinary benefit of this is that we have amassed a huge library of these thoughts, an ever increasing skyscraper of knowledge on which to base our assumptions, predictions and theories on. One drawback of this is that we actually have to learn from this library, to actually climb that skyscraper of knowledge! We don’t just have to learn what is in front of us, we’re expected to be aware of what has come before us too. That sounds like a lot of work. Learning, it seems, just got a whole lot harder, but remember the times when learning was easy? Passive versus active learning Some learning comes without effort. Whether it’s a new song you’ve learnt the lyrics to without even trying, or your favourite football teams scores over the last decade or two. This learning is almost unconscious. It’s fun, and without pressure to retain, we seemly retain it more easily. This learning is known as passive learning. Unfortunately, not all learning is passive. Some learning is required for work or for progressing in life. This adds a certain pressure to learning. Whether it’s learning to drive, or learn a new technology as a software engineer, the added pressure of needing to succeed doesn’t help. We call this type of learning active learning. So with all this pressure to actively learn from the ever growing skyscraper of knowledge, what are we to do? In part, we need to be aware that the journey of learning has recognisable stages. Four stages to be aware of as you learn, to keep you focussed, and help you adapt your learning as you progress up the ladder. Four stages to be aware of as you learn Stage one: Unconscious incompetence Firstly, please don’t be offended by the word incompetence, it is meant here without negativity but simply in the truest sense of being unable to do something. This stage occurs when a person is attempting to learn something brand new, perhaps even without any context on the subject at hand. A person is unable to do something and most likely doesn’t even realise that a challenge exists for which they have inadequate knowledge for. To move beyond this stage, a person must recognise there is a gap in their knowledge and it’s essential to have the nature of the challenge explained to them in a context that they can understand so they can see why this challenge exists. In the example of learning to drive a car, it is the stage before the first driving lesson. A person has no idea how little they know and how much they have to learn. They may ask what seems like basic questions, such as why can’t they start the car in the highest gear. Trainers often find resistance to learning from people at this stage who do not know that they do not know. Stage two: Conscious incompetence Here, a person understands that there is a challenge and although they still don’t know how to remedy it, they know there is a benefit in learning the skills to address and solve this challenge. When they understand that there is a challenge that they are expected to solve, but also understand that they are ill-equipped to do so, the natural response is to panic. Actually this is a good sign and shows that they’ve moved on from stage one. In the driving example, it is the stage when people realise that understanding the controls on a car and negotiating street conditions is much harder than they thought. Trying to reach that perfect biting point between clutch and accelerator without the engine cutting out. A seeming failure before they’ve even gotten the car out of the driveway. PANIC! They fail to do well more often than they succeed. PANIC! This stage is often accompanied by a feeling of hopelessness or “Am I ever going to learn this?” Did I mention PANIC? Stage three: conscious competence Hoorah! Progress. Here a person can understand a challenge and also how to remedy it. Although such actions are not yet innate to the person and they must devote much of their concentration in applying their new skills. With time and regular practice, the learner moves out of a place of getting things mostly wrong into a place of getting things mostly right. However, this competence comes with much effort. In the example of driving a car, it’s when the learner consciously practices the “rules of the road” that they have been taught. Reversing so that they can parallel park on a steep hill while there’s a car waiting for them to move and they’re being watched by that driver and the instructor… panic to some degree may still exist, they know they can do it but it takes all their focus. Stage Four: Unconscious competence At last, the person has had so much practice with these skills that they become innate to them, second nature or unconscious. They no longer go through the steps of performing those skills, they just do it. In-fact, they can often perform those skills while performing other tasks as-well. They’re driving in heavy traffic but checking the radio for their favourite station and drinking their Starbucks. Maybe they’re talking on their hands free device too. (I’m hoping I’m not in this car with them!) When people reach this stage, the skill has stuck. Even if it is not performed for long periods, people can go back to it without too much difficulty and perform again as well as if they had never been away, driving a car, riding a bike, you just know you can do it. So now you know that learning has scientifically recognised stages, a learning ladder with rungs that we all must climb on our journey to mastering a skill set. Whether you want to pursue a career in software, learn a new language or just like to understand things, understand that being able to recognise where you are in your journey of learning means you won’t get lost or disheartened on your climb of that ever increasing skyscraper of human knowledge. Like most things in life, learning is a journey, not an end point, so even though you may never reach the top of that skyscraper, you’re sure to enjoy the views as you climb. Go forth and learn! Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-11-06"},
{"website": "Made-Tech", "title": "Importance of information management in agile", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/importance-of-information-management-in-agile/", "abstract": "I risk boring people with this topic, as well as upsetting the folk who dislike most forms of documentation, and the folk who love the code to be the documentation. I think it’s important nonetheless to address a problem I see affecting most digital products and their development teams. I often ask these questions of product teams, but much rarer do I find the answers: What needs is your product currently meeting? What desired outcomes are you currently measuring yourself against? What impact has your product had to date? In this blog post, I address the importance of information management in agile software development. Bear with me on this one… Software development requirements In waterfall software development, requirements are written before design and implementation phases. This model meant information flowed in a single direction, like a waterfall, which has proven to be inflexible for software development where requirements change regularly and learning is continuous. With the adoption of agile, teams could instead incrementally develop and document requirements. What do we mean by “requirements” though? In software development, requirements are a way of expressing the needs and desired outcomes of a piece of software from various perspectives including business, user, architecture, functional, and non-functional requirements. Expressing requirements, or as I prefer, needs and desired outcomes, of a particular software product is important for making sure stakeholders, from budget sponsors through to users, are happy and satisfied when that software is delivered. We build products to have an impact after all, whether that’s enabling members of the public to apply for a passport online while reducing the public cost of providing passports for taxpayers, or enabling businesses to sell online while enabling shoppers to purchase without needing to physically visit a town centre. Requirements enable us to express these desired outcomes. Context throughout the lifecycle of a product Expressing needs and desired outcomes isn’t only important during development. Context is needed throughout the lifecycle of a product. Teams need to be able to continuously validate whether their products are having the desired outcomes, knowing what those desired outcomes are, is important. When new needs are required, we need to understand the current context of a product before suggesting changes. We should be able to retire functionality too, when a product is no longer having the impact it once was, and so understanding that impact is no longer being met requires past context. The current state of a product, in terms of the needs it is currently fulfilling and the outcomes and impact currently being worked towards, is an important asset. This information needs to be managed carefully and maintained throughout the products lifetime. Beyond behavioural automated tests and manual test plans, in agile software development terms, I rarely find up to date documentation for the current state of a product. This means that understanding the existing context of a product is often an expensive process that involves conversations with team members who have been on the team for a long time and know this inside out. Even then, memory can be inaccurate or differ between members of a team. This means onboarding new members, or providing an overview to stakeholders outside of the team, is inefficient. A product backlog is a delta You might think to yourself, hold on, isn’t the product backlog an up to date list of needs and desired outcomes? “The Product Backlog is an ordered list of everything that is known to be needed in the product. It is the single source of requirements for any changes to be made to the product.” The Scrum Guide™ It’s true that the product backlog is an up to date list, but it’s a list of “requirements of any changes to be made to the product,” rather than an artefact representing the existing state of a product. A product backlog is a delta from the current state, towards the desired state. It’s ephemeral and constantly changing. So then, what to do about managing information about the existing context of a product? Managing information – the agile way It should be a product manager’s concern to ensure that the team has an up to date list of: Needs being met by the product Changes made to the product, often expressed in (user) stories, so that as needs evolve you have a changelog that can provide context around why past decisions were made. The changes should be linked back to needs. Test plans for each story, or at least, every epic user story, for ensuring that core needs are being met Testable hypotheses or KPIs for tracking whether that story is having the desired outcomes and impact as intended Historic record of tasks, incidents, and technical debt related to each story Product managers need to manage this information about their product, ensuring that information is accurate and useful, while not burdensome for the team to maintain. How exactly this information should be managed or where is for your team to decide. Some teams keep this information documented on a wall, some use a tool like confluence, some keep this information in Trello or JIRA. Some prefer to keep this information in code. Let your team decide what is best for them. The important thing here is to be able to understand the current context of a product, and to maintain a historical record of important events relating to the product. It is this historical record that maintains institutional memory, and enables easier onboarding and decision making in the future. Future series on agile information management I’m planning on writing more on this topic in the future, including a deeper dive into product backlogs and their ephemeral nature, a guide on tools of better agile information management, and perhaps most importantly, a detailed example of product information architecture for product managers. Stay tuned! Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-09-23"},
{"website": "Made-Tech", "title": "Kubernetes: The Good Parts – Introducing Google Cloud Platform and GKE", "author": [" Mark Sta Ana"], "link": "https://www.madetech.com/blog/kubernetes-the-good-parts-introducing-google-cloud-platform-and-gke/", "abstract": "Continuing with the theme from our last post , we’ll continue to learn about Kubernetes, not by worrying about the moving parts that are required to make a cluster, but instead by using a managed service provided by Google Cloud Platform (GCP) called Google Kubernetes Engine (GKE). As with the main cloud providers (Azure, AWS and GCP), resources can be created through a web-based control panel (also referred to as the Console or Portal) or through the command line tool i.e. aws, az or gcloud. We’ll use the command line in our examples to avoid any screenshots becoming out of date. At the time of writing we were using version 260.0.0 of the Google Cloud SDK. The remainder of this blog post will involve practical hands-on experience. Installing the Cloud SDK The Cloud SDK provides a suite of command line tools that allow you to interact with services and resources on GCP. To install on a Mac with homebrew use: brew cask install google-cloud-sdk . If you’re not using a Mac, the quickstarts page can help you set up the SDK for your own computers. If you haven’t previously, now’s a good time to log into GCP via the command line using your Google Account. The console output has been edited for brevity Configuring GCP and our project Before we can create our cluster we need to do the following: Link our project to a billing account Set our default region and zone to provision our cluster Enable the compute service for our project Linking the project to billing To set up billing, we’re going to make use of gcloud CLI’s “beta” channel for commands to allow us to link our billing account with the project. There’s also an “alpha” channel and these are exactly what they sound like these are trial commands/features going through alpha and beta stages of a software release cycle. If commands are already in beta, there’s a good chance they’ll make it into general available (GA) which are the commands that come with the Google Cloud SDK by default. Additional features or channels are components, so to install the beta channel we’d type the following: gcloud components install beta Incidentally, if you want to see what components you have installed you can see them by using the list subcommand: We need two bits of information to enable billing for our project; the project ID which is k8s-good-parts (from our previous posts) and the billing account ID which you can get by running: gcloud beta billing accounts list Assuming our billing account Id is 00F0F0-F0F00F-0F0F00 we can now link our project with our billing account to our project: Tip: if you find the command line tough or you are a bit rusty, you may find using the interactive shell helpful. This provides a friendly interface with a list of auto-complete suggestions for all of the gcloud commands. You can enable it using: gcloud beta interactive Setting default region and zones Find the region nearest to you using the Regions and zones page on Google Cloud. Made Tech are based in the UK, so our closest region is “europe-west-2” and available zones are “a”, “b”, “c”. We’ll use zone a (“europe-west-2-a”) Enabling GKE in the project GKE is part of the “Kubernetes Engine API” service, so to set up the project we need to enable this service using: gcloud service enable containers.googleapis.com Incidentally, this will activate additional services that GKE is also dependant on like Compute Engine API, Container Registry API, IAM API etc. There may be a brief delay whilst the operation is being carried out. Tip: To see a list of services you can enable type: gcloud service list –available Creating our cluster After a lot of preparation and set-up, we finally get to the moment you’ve been waiting for to create our cluster. Let’s give it the imaginative name of “hello-k8s”: gcloud container clusters create hello-k8s There will be another brief delay as our cluster is provisioned. Setup Kubernetes Client In order to communicate with our new cluster, you’ll need a client. This is kubectl (pronounced “cube control”): gcloud components install kubectl If you’ve already got an up to date version of kubectl you can safely skip this step. If you’ve already played around with Kubernetes i.e. using Minikube or Docker, the chances are you will have a lot of contexts (connections to different clusters). To ensure we’re connecting to our new cluster let’s check our current list of context which is available through the config command of kubectl . We can see that we’re still connected to the Kubernetes cluster that you can enable with Docker. We’ll use the use-context subcommand to connect to our cluster by its name: kubectl config use-context gke_k8s-good-parts_europe-west2-a_hello-k8s If you run the get-context subcommand again you should see our new cluster is set as the current context. We’ve got two more commands we’re going to run to complete our “smoke test” The cluster-info command allows us to check the URLs and statuses of master and cluster services. The version command allows us to compare client (kubectl) and our server (our Kubernetes clusters) versions. Introducing Helm, the package manager for Kubernetes CLI tool setup To install Helm, we’ve got a handy homebrew package: brew install kubernetes-helm If you’re not using a Mac, the maintainers of Helm have provided instructions for installing on different platforms. Service setup In the early days of Kubernetes, security was pretty much a case of having the right certificates to access the cluster and default access was as root (admin). Nowadays access is more granular and starts off with the least amount of privileges. Role Based Access Control (RBAC) is enabled by default in our new Kubernetes cluster if provisioning it on GCP. There’s a good likelihood that by the time this is published the other cloud vendors will have adopted this too. Given the nature of what Helm can do i.e. provision various resources and setup network routing, it will need a service account. Let’s create a new service account, to do this we’ll need to to create YAML manifest (if you’ve never used Kubernetes, everything is expressed in YAML). We’ll call it rbac-config.yaml . Now we can run the manifest use the create command in kubectl: kubectl create -f rbac-config.yaml Kubernetes will confirm the creation of the service account and tiller (explain) has permissions to use the service account. We can install the cluster service element of Helm (Tiller) and limit command history by running helm init Let’s update the copy of package repository that’s stored in our cluster, by default, this means we’ll use the “stable” collection of Charts (packages in Helm parlance). helm repo update Let’s pick a Chart (packages in Helm parlance) that’s well known and install WordPress (here’s a link to Chart page): helm install stable/wordpress You’ll see a lot of information on the screen, include any setup instructions that require manual intervention. Let’s confirm our package has been installed by issuing the ls command: We should see just the one package installed with it’s an autogenerated name (this can be overridden), chart version use and the status. Tip: if you forget to make a note of the instructions that you saw during the installation of WordPress you can view them again by typing helm status <name of the deployed application> i.e. helm status cautious-giraffe. We’re going to monitor the deployment until we get an external IP address using get service ( get svc ) and the watch switch ( -w ). Once we’ve got an external IP address we can connect to our WordPress blog, to exit this command hit CTRL-C Entering the IP address in a browser will display the familiar WordPress starter blog. Teardown That’s our first foray into using GKE on GCP, we can tear down the whole cluster by performing gcloud container clusters delete hello-k8s Tip: if you wanted to keep the cluster, but just delete WordPress you just need to run helm rm <application name> i.e. helm rm cautious-giraffe. Summary To recap here’s some of the commands we used to interact with our cluster. kubectl (cube control) This is the Kubernetes client CLI tool, it’s also your primary means of running commands to manipulate the cluster. Here’s a reminder of the commands we used: config – switch between different clusters create – a service account to allow Tiller (service component of Helm) to have administrative access to the cluster get – the details around the application deployment of WordPress We also used the following commands to perform a smoke test against the cluster to verify it’s status: cluster-info – displays the status of components installed on the master node version – displays the client and server (cluster) versions There are a lot more commands to be found on the Kubernetes web site. helm This is the package manager for Kubernetes, just like apt, yum or brew. Since the advent of Role Based Access Control (RBACs), there’s been a bit more complexity (creating a service account for Tiller) introduced into the setup, but this is worth the security peace of mind as you’ll recall without RBACs access to the cluster mean you, or the service or applications you install have administrator i.e. root access. The commands we used: init – to install the Tiller service and configure the helm client repo – to update the local (to the cluster) copy of Charts repository. We also used the “stable” version of the Chart repository. The difference between “stable” and “incubator” versions of the Chart repository is quality assurance. There’s a considerable checklist to be achieved before a Chart is added to the “stable” repository. install – to install the WordPress Chart status – to view the status of the Chart installation and to also view setup instructions that appear after installing the chart. ls – to list any Charts we’ve installed on our cluster rm – to remove any installed Charts Just like kubectl, there are a lot more commands you can use with Helm (including ones to create your own Charts). Whilst there is a lot of set up involved around creating a Kubernetes cluster, it’s worth noting a lot of this was administrative: sign up to GCP, set up billing and create a project. Once you’ve done this, the process of connecting to your cluster, installing applications and accessing them is relatively quick. What next? We plan to work on a new series of blog posts around Kubernetes, going beyond this taster series. Some of the topics we plan to cover: Using Kubernetes in Production – Infrastructure as Code, Securing web traffic, Persistent Storage, Service Mesh, Continuous Delivery pipelines and; Alerting and Monitoring. We’ve got all the buzzwords covered. Creating your own kubectl commands using Custom Resource Definitions Observability as practised in Kubernetes Providing platforms on Kubernetes like OpenFaaS Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-10-02"},
{"website": "Made-Tech", "title": "Academy Week One: Mind Blown", "author": [" Bogdan Zaharia"], "link": "https://www.madetech.com/blog/academy-week-one-mind-blown/", "abstract": "It’s the end of our first week in the latest Made Tech Academy cohort. My mind has been truly blown. I was excited before joining Made Tech, in fact, when I first read about the Academy programme, it looked different from any other job advert I had seen for graduates. The culture stood out for me in particular, the emphasis on continued learning not just in the Academy but the fact every member of staff has Friday afternoons to learn and develop their careers. After joining, my expectations were exceeded. There’s always a fear that the job advert would be all marketing. I’m glad to report that’s not the case at Made Tech. Here’s why… Thrown into the deep end with mob programming During the first couple of days we were introduced to a new way of learning: mob programming. It’s a practice that involves a group of people working on a coding problem together. You switch the keyboard regularly (for us every 7 minutes), the person with the keyboard is the driver, but everyone in the group will be providing input as navigators. The idea of mob programming was intimidating at first. A whole group of people deciding what code to write, together! I’m no longer just coding in the privacy of my head, I’m now letting the whole team input into the code I am writing. The code problems we are solving are called code katas. They are problems specifically designed to enable deliberate practice. In particular we were using them to learn test-driven development. Discovering test-driven development Another new concept to me was that of test-driven development. Letting tests drive the evolution of my our code, where you write the next smallest test, then writing code to make that test pass, finally cleaning up (refactoring) and moving onto the next test. The mind-blowing thing about this approach is that it totally reversed my way of solving problems with code. Before coming to Made Tech, I was used to dealing with a problem with a top-down method, using test-driven development encourages you to deal with a problem from the bottom-up. While it’s hard to challenge the ways in which I’m used to my brain working when it comes to programming, it’s fun too. It’s strange to take a small step forward rather than picturing a full solution in your head, with a whole bunch of edge cases. It’s almost scary to let the tests drive your code, rather than your instincts, it’s like driving on a highway without knowing the destination. Even though it’s scary, I have trust in this method. I wouldn’t have done this if it weren’t for my expectations being exceeded right from the start of joining the Academy, and everyone’s enthusiasm and encouragement of following test-driven development. Team mindset The people here at Made Tech are yet another reason why I’m so excited to be a part of this company. A company’s culture is reflected in its people, and with all my interactions with my new colleagues so far, I’ve got a taste of how things will be. It was great to see everyone coming up to us being genuinely excited about the new Academy cohort. It was surprising to see this level of enthusiasm for newcomers, but very much welcome. We were assigned buddies from the previous Academy cohort and have regular 121 meetings. Having recently finished the Academy, they have lots of relevant experience to share with us and help us overcome challenges. Their experience is fresh which brings lots of empathy and advice. In terms of my Academy colleagues I’m pleased to find that we are all open minded, and very much able to work as a team. The Academy hiring process certainly has picked a group of people willing to grow as a team rather than as individuals. Everyone at Made Tech is willing to help, always up for a chat in the kitchen, and I can’t wait for my first Friday learning afternoon today. That, and moving from mob programming to pair programming next week! https://media.giphy.com/media/xT0xeJpnrWC4XWblEk/giphy.gif Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-08-23"},
{"website": "Made-Tech", "title": "Should we be treating software delivery mistakes like plane crashes?", "author": [" Tito Sarrionandia"], "link": "https://www.madetech.com/blog/should-we-be-treating-software-delivery-mistakes-like-plane-crashes/", "abstract": "In software development, things occasionally don’t go the way you expected them to: that’s not news to anyone who has tried to implement anything but the most trivial of applications. You can pick up a task that looks like a small tweak to the front-end of a web application, then after a few hours of development you learn that you need to replace a Javascript library, which means you need a new build tool, which doesn’t have that custom plugin you were using for dealing with static files. Maybe you didn’t anticipate that your fancy new NoSQL database wouldn’t be able to scale up to meet the needs of that new customer you just signed. Suddenly, you are spending two weeks on a problem that you told a stakeholder would probably be done on a Friday afternoon. As a team, there are two ways to manage this problem: you can treat them like car crashes, or you can treat them like plane crashes. Both can work, but it’s a choice you should make with your eyes open. Safety Models When an aeroplane crashes it is a big deal. We don’t expect it to happen, and so when it does it becomes a top priority to locate the cause and ensure that systems are changed so that it cannot happen again, even if that comes at great expense. Recently, the entire global fleet of the Boeing 737 MAX was grounded after two similar high profile failures. Returning the aircraft to service will require a thorough investigation, mitigation strategies, and sign off from the Federal Aviation Administration. In short, nobody expected this failure to happen, and everybody expects all parties to be diligent enough to ensure that it never happens again. This investigation is not confined to the aircraft themselves. Instead, we think about whole systems to identify and correct problems. Every aspect of the system is up for scrutinisation: pilot training, maintenance protocols, passenger behaviour, weather conditions, etc. Contrast this to a car crash. We expect these to happen. There are around 1,800 fatalities from car crashes per year in the UK, and we expect that to continue. While models of car are sometimes recalled for safety reasons, we rarely apply the same level of systems thinking to road travel as a whole. A car crash is unlikely to force a change to driving tests, road design, availability of cars, etc. It would take a truly enormous number of crashes before system-wide changes were made. These offer two contrasting approaches to getting things wrong. Are your mistakes more like car crashes or plane crashes? Blame won’t get you anywhere It’s important to remember that this whole process will be severely limited in its ability to teach you anything if you are working in an environment that blames individuals, and not systems, for mistakes. If you’re in the habit of firing junior team members, passing on blame to other teams, or making excuses to keep a clean performance record, then stop reading, focus on that problem, and come back later. Both of these models assume that you have built a culture where your teams have the safety and integrity to retrospectively acknowledge mistakes, accept collective responsibility, and move forward productively. The Car Crash Model If you haven’t given it any thought, this is probably the model you are using by default. That might be fine, it might work for your team. In short, this is the model where you: Acknowledge that we occasionally make mistakes, sometimes expensive mistakes Predict that the cost of occasionally messing up in the exact same way is less than the cost of systemic fixes Measure the negative impact on your velocity and accept it as a constant It isn’t the same as expecting everything to be perfect all the time and going into panic when it isn’t, that is unlikely to work for anyone. It is, for example, learning that your overly complex data model is costing you 50 hours per month in unexpected extra development effort, but agreeing that it is an acceptable price and moving on. The Plane Crash Model This is the model where we look to apply some amount of systems thinking as a matter of routine . That is, if a mistake hits a certain set of criteria, we will automatically resolve to see if there are system-wide corrections we can take to stop this category of error from happening again. Of course, we still need to allow some contingency time for new mistakes , a small amount of time to conduct investigations, and a velocity impact if we want to take an Andon approach. However, we predict that the investment into fixing systems will be less than the cost of repeatedly making a mistake. Why not both? In reality, we are going to treat our mistakes both ways. That is, there will be some mistakes that strongly feel like they are just part of the job. Unlike a pilot, part of our job is combining technologies in new and interesting ways, and there will inevitably be some stumbles upon the way. There will also, however, be some mistakes where we think there is probably a critical lesson to be learned. It’s important to agree how you will select which mistakes represent critical lessons for the team. The criteria for what counts as a critical mistake is going to be highly contextual. Here are a few that I’ve seen work in practice: In a time critical project where you are counting on consistently delivering according to some deadlines that are difficult to move, and where you were counting on consistent-ish sizing of tasks, it might make sense to choose any task that takes longer than a certain % over the average. For example, if you had 20 tasks that you expect to take roughly the same amount of time, most of them take between 3 and 6 working days, but 2 of them took 15 working days, those two could be good candidates. You could select tasks that unexpectedly required changing the tech stack: introducing a new library, replacing a third party service, etc. If you predicted that you could just add another CSS file but ended up having to introduce Sass and rewriting large amounts of styling, that could be a good candidate. If your tooling is able to support it, you might check for tasks that created a subsequent need to do some level of extra work because they introduced defects. You could select all tasks where extra work was identified at a late stage in your development cycle, such as at the user review stage. A simple fact finding ceremony A useful format I like to use is to spend 5 minutes answering each of three questions. This format is a fact-finding format , it isn’t supposed to generate solutions. The reason for this is that it’s easiest to find facts with a small number of people present — the people involved in the task, and a facilitator — but easier to make consensual decisions in a wider team format, such as a retrospective. Each question represents one section of the ceremony. Keep it brief, we aren’t here to agonise over mistakes, we want to acknowledge them and move on. The longer you spend dwelling, the more likely you are to start beating yourself up over things you couldn’t have predicted. Each section should last 5 minutes, giving an overall ceremony length of 15 minutes plus some time to give instructions at each stage. What happened? [5 minutes] At this stage, the facilitator explains that we are only interested in dealing in facts: not opinions or hypothetical alternatives. “Our build tool isn’t compatible with our Node plugin” is a fact, but “We picked the wrong build tool” or “We shouldn’t be using that Node plugin” is not. Let participants spend around 3 minutes writing down objective facts on blue sticky notes, one note per fact. Arrange them in a vertical line down the middle of a whiteboard, then spend two minutes sharing the output. Be strict on what counts as a fact at this stage. If in doubt, give it back to the person that wrote it. There will be plenty of space to express opinions, but you want everyone to agree on what is indisputably and unanimously true before exploring them. If we knew what we know now, what would we have done differently? [5 minutes] You don’t want to end up with more points than you can discuss here. Let participants add pink sticky note one at a time that says what they wish they’d have done differently, and ask them to spend about 30 seconds explaining it. Try to limit follow up questions if those questions are actually different opinions disguised as questions – it’s OK to generate contradictory ideas here. Arrange the pink sticky notes in a vertical line to the right of the blue ones. If they closely relate to one of the blue notes, keep them physically close on the board. How can we detect if this is about to happen again? [5 minutes] Allow participants 5 minutes to write ways of detecting this category of error on yellow sticky notes. We want to cover as much ground as possible here, so make sure they are added to the left of the blue notes as soon as they are written so participants know what hasn’t yet been covered. Again, where possible yellow notes should be placed closest to the cluster of pink and blue notes that they relate to. The only action item generated by this meeting should be to share the output with the rest of the team so that they have the context to propose a change if they feel it’s appropriate. This could be sharing a photo of the sticky notes produced, or a brief update after a daily stand-up. Don’t ignore mistakes Your planning process should explicitly acknowledge that the team will get things wrong. You can deal with these like car crashes: frequent, expected, accepted; or you can treat them like plane crashes: exceptional, studied as a system, and leading to process improvements. What does this process look like on your team? Continue the conversation on Twitter . Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-08-28"},
{"website": "Made-Tech", "title": "Developers joining user research: an interview", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/developers-joining-user-research-an-interview/", "abstract": "I see design and development as a continuous process and importantly, a single team effort. I wanted to capture some of the passion from our software engineers, the passion for blurring the boundaries of design and development, the passion for being a software engineer who cares about users and using researching efforts to inform the products we build. To this end, I could think of no one better than our very own, Elena Vilimaite. About Elena El joined us last year and has since worked across multiple customers delivering public sector digital services alongside civil servant multi-disciplinary teams. Currently El is working for central government organisation building tools that enable government departments to digitise paper forms, rapidly. What follows is an informal Q&A between myself and El. What excites you about being a software engineer? “I want to build products that people actually use. This is particularly exciting within the public sector. I love the challenge of government – it’s big and often stuck in the past so there is an opportunity to make a big impact. I love understanding users and gathering requirements. My parents used to ask me to fix issues with our computer, or help them to buy online, and I’m driven to empower users like my parents to do things themselves. That’s why as a developer I’m keen on making sure a product is fit for purpose through continued user research and I love to be a part of that process.” How does a developer get involved in user research? “I’ve actually been involved in user research before working at Made Tech, at a charity I previously worked for. We used focus groups to provide feedback on web applications. We would sit in another room watching videos of users attempting to use the products we’d built. We’d have to put our hands over our mouths to stop us shouting at the users for not using the software in the way we had intended. It was a really good lesson. We used those sessions to iterate on usability. More recently, for a local government organisation, we were building software for staff users. We would collect feedback on a weekly session, both with managers and day-to-day users, separately, to understand the varying needs of the product. We would also shadow staff as this was the most insightful way to really find out what people were thinking. Of course, it’s not about doing everything a user asks, it’s about distilling their feedback. As much work is needed in order to turn that feedback into something usable, if not more than the actual work itself to implement it. Lastly, we had showcases and demos with users on a regular basis to introduce new functionality in order to help with its adoption.” Why do you think it is important for a developer to join in with user research? “As a developer you need to understand how a product will be used. What you build should not require specialist training. We are building self-service software – it needs to be usable, for users to be autonomous. Software engineers can help uncover technical feasibility with technical discoveries. You can hear what users are asking for, what designers are recommending, and then a developer can provide feedback on what is realistic. Designers can interpret usability from research, but it’s the developers who can determine feasibility. Having first hand context of research is invaluable for this. If the whole team understand context, understand users, and have empathy for the user, the team can function better together, working towards a common goal, delivering successful outcomes for the user.” Can you tell me more about technical discoveries? “Technical discoveries are about researching technology to understand feasibility. You might do some in depth analysis of APIs you are considering integrating with. You might want to better understand non-functional requirements including security and maintainability. You might want to interview other developers who may consume an API or using a developer tool you are planning on building. Actually a lot more technical research should be carried out during discoveries, much more  than currently is at least.” Are there challenges having developers get involved with user research, or designers getting involved with development? “It’s positive if the whole team are aligned, and want to work with each other in this way. It wouldn’t be fair to burden a developer with user research unless they were keen. Problems can arise when taking on a role that isn’t fun to you. It can also lead to overworking, you need to understand that if you are partaking in user research, you won’t be able to have the same velocity as you would if just doing development work within a sprint. This needs to be factored into planning.” What advice would you give a developer joining in with user research? “Listen. What you hear might not always be pleasant or useful. There is meaning behind what people say. They are the users or future users of the software you a building. Build something you would want to use yourself, something you can be proud of. And be prepared to compromise.” Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-08-21"},
{"website": "Made-Tech", "title": "Investing in digital skills for the future", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/investing-in-digital-skills-for-the-future/", "abstract": "Many organisations have fallen behind in the digital race. They’ve been unable to keep up with the pace of change and therefore need to buy a digital transformation to catch up. Unfortunately the digital skills required for such a change are expensive and in limited supply, and buying such a transformation wholesale is expensive, slow and risky too. Couldn’t this whole transformation business have been avoided? It’s not like users, customers and IT are new concepts. I recently read that user-centred design was a concept the UK government was thinking about in 1999. I see digital transformation as a wake up call for organisations that allowed the world to continue evolving without them. The organisations failed to learn new skills required to keep up with the pace of change rapidly occurring around them. That’s okay, all is forgiven, we’ve always been trying our best, but now is the time to embrace change! The IT crowd is a thing of the past The antiquated view of the IT geeks in the basement is disappearing. Technology is cool, developers work directly with users to understand their needs, and hundreds of technology startup office listicles attest to the fact coding no longer happens in the basement. That said, some IT functions haven’t kept up with this changing image. For whatever reason technology evolved faster than some kept up with. While IT teams continued to capacity plan their data centres with their “new” hypervisors, technology startups had stopped thinking about servers – what a FaaS farce! Most IT departments are now looking at cloud technologies but they are quite behind the curve on that journey. What do disruptive technology startups have that traditional IT functions lack? Besides foosball machines, my personal view is that they fear, or regularly oppose for whatever reason, change. Hiring for the skills of today will hold you back from tomorrow This is further perpetuated by traditional HR processes that haven’t kept up either. CV sifting relies on technology buzzword bingo and is often optimising for technologies currently in use with a preference to senior hires having extensive experience with a particular technology. Unfortunately by focussing on experience for technologies in use today, rather than trying to hire people keen to learn current or future technologies, you may be attracting the wrong kind of folk to your organisation. You might just be hiring folk who are happy and in fact fixated with what they know now and are averse to change. The other mistake is requiring senior hires to have extensive experience in a particular technology. Some of the best seniors consider themselves polyglots, able to work across many programming languages and technology stacks. These seniors may not have the most extensive experience in your current range of technologies but are the kind to help you adapt to changing times – true technology leaders. What would happen if instead we hired for people who love to learn, keep up with technology trends and embrace change? Employers-as-Educators If we hire folk who love to learn, and seniors who have a track record of keeping up to date, we can begin to create cultures that adapt and evolve with the times. We can’t just hire learners though, we need to invest in learning as first class work. We need to shift our view to seeing the employers as educators. Sure school and university might be current enough to give you the technology chops for a modern technology practice for today. Technology changes though, and so we can’t expect folk to go back to education every time a new programming language is released. Instead employers need to create space in the work day to keep current, trial new technologies in throwaway experiments, and provide strong onboarding processes to help new joiners be productive as quickly as possible. There are great examples in the wild of organisations already taking this approach: GDS run an Academy for civil servants to get up to speed with digital skills ThoughtWorks runs their ThoughtWorks University Made Tech run a 12 week onboarding programme and invest 10% of the week (Friday afternoons) for all staff to spend learning skills Is your organisation investing in learning and optimising for change? Get in touch and let me know! Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-08-14"},
{"website": "Made-Tech", "title": "An inside perspective of the Made Tech Academy", "author": [" Dushan Despotovic"], "link": "https://www.madetech.com/blog/an-inside-perspective-of-the-made-tech-academy/", "abstract": "At the time of writing this, I’m in the latter half of the Made Tech academy process, so I figured I would write about the experience so far and why the scheme is invaluable for people who have some technical skills but are trying to get started within the tech industry. Getting you started The Made Tech academy provides people with an excellent opportunity to get their software engineering career started. Not everyone will have had the fortune of doing a placement year as a developer, and a surprising amount of graduate schemes seem to expect graduates to have a year or two of experience with a programming language. The result of this is that graduates can struggle to get their foot in the door, leaving them searching for an opportunity that will take them on and help them develop their skills. The Made Tech Academy doesn’t require any industry experience, just some knowledge of programming (which can be self-taught) to be considered for a position. The low barrier to entry means that people will feel encouraged to apply as the requirements are suitable for people looking to start out their careers. From the moment I read an advert for the Academy, I knew that I wanted to get an application in ASAP. Earning while learning The academy offers a great salary during the 12 week learning period, which is quite attractive to potential candidates. This differs from the approach taken by a lot of coding boot camps that require payment for their teaching. The result is that the high cost can deter potential applicants who might wish to start a career in software engineering. Made Tech’s approach is a refreshing difference and I can’t overstate how appreciated it is to not have to worry about having to support yourself while developing your skills. Learning isn’t job specific A lot of companies advertise mentoring as a key aspect of their junior roles. At Made Tech, mentoring is a value and it really shows in the Academy. You’re assigned a buddy from the previous Academy cohort and you have weekly 1-2-1’s with the Head of Learning. These two things are incredibly useful when you’re starting your career as you know that you will always receive help should you require it and it allows you to develop your career at your own pace. You don’t need to wait for a yearly performance review to find out what you need to do in order to progress, the areas you may need help improving in and the opportunities for you to get involved in. I think that this is extremely valuable as when you’re getting started you’re always looking for how you can improve so having help so readily available is fantastic. Some sacrifice is required The Made Tech Academy does require some sacrifice to be made. For example, during the 12 week training period you can’t really take any days of holiday, unless you absolutely need some time off, which differs in approach to other jobs that have a shorter training period and allow you to start booking holiday time after about a month. The benefits of the Academy outweigh this though and the benefits of working at Made Tech as a whole mean that you consider it to be a sacrifice worth making – I know I sure do! Final thoughts Overall, I think that the Made Tech Academy is brilliant. To any potential applicants reading this, I would strongly recommend applying here for all the reasons above and more. To anyone considering how they can improve their training for junior software engineers, I would urge you to take the Made Tech Academy as an example and look at what it does well to apply to your business. Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-05-13"},
{"website": "Made-Tech", "title": "Kubernetes: The Good Parts", "author": [" Mark Sta Ana"], "link": "https://www.madetech.com/blog/kubernetes-the-good-parts/", "abstract": "This is the start of a blog post series that will discuss the various aspects of Kubernetes. Inspired by Douglas Crockford’s cautionary tale ( JavaScript: the good parts ) we’ll try to guide you through the myriad of choices when choosing the type of Kubernetes installation, setting up a continuous pipeline to deploy your application to a Kubernetes cluster and how to secure said cluster. What’s Kubernetes? Kubernetes is a Platform as a Service ( PaaS ). This means we can automate our application deployments just like we can with Heroku and Cloud Foundry using a single YAML file to define how our application is configured and deployed. We can also scale and monitor these applications in a generic fashion i.e. no specialised knowledge is required about the application. We can run one off computational tasks or have them scheduled on a regular basis. Kubernetes is highly extensible, allowing you to add new functionality and provide APIs that other components within Kubernetes can interact with. The components required to set up a Kubernetes cluster A Kubernetes cluster consists of a Master and at least one Node (a worker machine, usually a virtual machine). The Master (control plane) component (there can be more than one for resiliency) makes decisions on scheduling and responding to cluster events like provisioning and autoscaling. The Node component carries out the actual work as instructed by the Master. If we zoom into a node, we can see that a pod which is the smallest unit that can be deployed to a Kubernetes cluster consists of one or more containers. Just like containers are analogous to those in Docker (the only exception is that with Kubernetes you can use alternative container runtimes like rkt , containerd , cri-o , etc). Pods can be thought of as a docker compose file , where you orchestrate one or more containers to build an application. One or more pods can form a deployment (often done to scale out an application). These pods are usually scheduled by the Master and assigned to nodes that have the capacity or specific capability. Typical uses and applications As we mentioned at the start, we can think of Kubernetes clusters as a personal Platform as a Server (PaaS). We can take this further and using a Kubernetes application like OpenFaaS , OpenWhisk and Fn turns our cluster into full-blown Serverless platform. Further reading kubernetes.io – The canonical definition of “What is Kubernetes” Cloud Native Computing is a one-stop shop for all Cloud Native related resources Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-05-24"},
{"website": "Made-Tech", "title": "Becoming a better engineer by becoming a better mentor", "author": [" Steven Leighton"], "link": "https://www.madetech.com/blog/becoming-a-better-engineer-by-becoming-a-better-mentor/", "abstract": "Mentor and mentee pairing. Some dos and don’ts for supporting and coaching less experienced engineers I’ve avoided the term ‘junior’ engineer here, as this post will be discussing thoughts that apply to many coaching situations. It also has a few connotations I want to avoid: when you’re pairing you’re going to learn things, even if you’re the mentor. Software engineers spend a lot of time teaching and coaching. This is true whether they realise it or not, and if they really don’t, they should. We’re also typically not teachers, and there are some things we can bear in mind to get the most out of that time. Helpful tips to use whether we’re teaching programming to someone learning from scratch, coaching less experienced engineers in a pairing situation, or teaching a new stack to a seasoned developer. I’m one of those engineers – I spend some time every week mentoring an up and coming engineer who started with no coding experience, I’ve volunteered as a teaching assistant at Ada National College for Digital Skills in London, and I pair and mentor daily here at Made Tech. Things to aim for Think about the desired goals of this mentoring/pairing session. A goal will help keep both focused. Usually, this will be the feature or task you’re pairing on, but could be a learning goal like practising a technique or gaining confidence in a new technology. Try to make mentees feel comfortable Nobody will learn if they’re nervous, out of their comfort zone or overwhelmed. Foster an atmosphere where either person can say ‘I don’t know’. Most people will readily understand the need to ask their mentor questions, but it’s also really important to understand that your mentor is also human and doesn’t already have every answer. Sometimes, you need to find the answer together. Foster an atmosphere where either person can ask questions or clarification . Seems a given, right? But comfortable mentoring means being able to ask, for anyone. Comfortable mentoring is productive mentoring. Foster an atmosphere where either person can ask for something to be explained again . This is something I’ve noticed, at some point many people feel they can’t ask something because it was already explained. Feeling comfortable means feeling like you can ask for a repeat answer. Seek feedback as a mentor. Don’t forget this part, this is how we improve. Being a better mentor and coach means being a better engineer, as being able to articulate and share your thoughts will serve in more than just mentoring. Communicate (especially important in a delivery setting). Feels like another given, but communication is key. In a delivery setting, this means talking about the task and the context as well as learning and writing code. Try to determine and work to preferred learning styles. Some people learn from books, or from watching someone else demonstrate. Some people thrive on pairing, and others need to pad pairing time with solo research. Some things to avoid Avoid saying ‘just some code’, it’s easy, obviously etc Another one that seems obvious, but I’ve caught myself slipping this one in at times. Takes practice to train yourself out of, but the single biggest thing you can do to improve the mentoring experience with programmers who are just starting out. It’s only obvious to you! Avoid the temptation to do it yourself There’s a couple of points I’ve seen here.I find it tempting to reach for the keyboard if someone I’m pairing with in a mentor capacity is really struggling with something chewy. Train yourself not to do it. A less obvious example is whipping up a quick PR to show what you mean, perhaps even to replace something someone else is working on. Often, I could spike something quicker, but this would be pretty demoralising, so it’s best to refrain. Don’t refuse to write a little code to get things rolling. I know what I just wrote is almost a contradiction, but something I’ve seen (especially with people very new to programming) is the hardest part is sometimes knowing where and how to start. I’ve had great success writing a few lines and letting momentum do the rest. Avoid rushing. This one is pretty applicable in all situations, but especially when working on a feature for production code. You can pair and rush, but nobody will get a lot of learning out of it. Some things that have worked Pivot! A big thing in my mentoring is to experiment and find an approach that works for the mentee. If something is too complex/not interesting just try something else. Share your favourite learning resources – it’s also great for your own experience to revisit these. Learn fundamentals – Something particularly challenging for someone learning to code is that many materials assume you know how to use a terminal emulator, even things like project setup and installing dependencies may need some command line know-how. Workflow is as important as code – share what you know and what works for you around source control, collaboration and etiquette. Review progress and reflect – this one is especially important in mentoring people into programming careers, reviewing progress is a major motivator to push the next difficult part. Try to develop something mentees can be invested in/work on production code in a delivery setting – sounds self-explanatory, but personal investment in what you’re building helps motivate. Take proper breaks and if pairing, stick to pomodoros – it makes pairing slightly less draining. Be on top of pairing discipline – this isn’t just breaks and pomodoros, but also things like driver/navigator pairing discipline or even more esoteric pairing styles. Draw diagrams/whiteboard – Another one that seems a given, but a visual learning aid will help when explaining new concepts or architecting code. Some things that haven’t worked Running before walking – Partly learn your fundamentals, partly don’t rush, but the final part is don’t dive into frameworks and complex programs before building and hosting a simple page! Remote pairing (for extended periods) – Hopefully my only controversial statement, it can work in a pinch and is fine for short periods of pairing on production code, but in a mentoring partnership try to get face to face interaction where possible! Conclusion I have written these points with pairing in mind for a reason – we believe pairing is the fastest route to knowledge transfer when mentoring, and effective mentoring is imperative to the health of a software delivery team. Being a great software engineer means making your team great, and sharing your skills effectively will strengthen that. A strong team delivers strong software! Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-04-25"},
{"website": "Made-Tech", "title": "Azure DevOps is not DevOps (yet)", "author": [" David Winter"], "link": "https://www.madetech.com/blog/azure-devops-is-not-devops-yet/", "abstract": "At Made Tech, our company mission is “ to improve software delivery in every organisation ” and recently while using Azure DevOps, I’ve been feeling that currently, these two things are mutually exclusive. Before I get started, it’s important to not confuse Azure DevOps with Azure’s cloud services offering. You can think of Azure DevOps as Github and CircleCI bundled into one package, whereas Azures cloud services is akin to AWS, or Google Cloud. Now, I’ve already made a comparison between Azure DevOps, and Github and CircleCI. The latter are the tools that I use day to day out of choice because I feel that they let me get my work done in a way that I’m accustomed to following best practices in our industry. It’s only to be expected that when starting to use Azure DevOps over the past month or so, I’m going to compare it to these. On the surface, besides a tricky UI to get used to, Azure DevOps appears to have all of the features you’d expect, and has good potential at being an all-in-one version control system (VCS) and continuous integration/delivery pipeline, similar to what Gitlab offers. However, there are a few niggles before I’ll be encouraging customers and others to use it. As a developer, I’m passionate about being able to codify “all the things”. It means you don’t need to document as much for people if the code is written well, and it also means you can review code before merging it into your repository. It also means that you can easily duplicate and reuse existing functionality. If we set aside the VCS element of Azure DevOps for now and focus solely on the CI/CD pipelines feature. CircleCI gives you a completely blank slate when you use the tool. You define your pipeline in code however you see fit. Out of the box, with pipelines in Azure, you get both Build and Release pipelines. They are clearly segregated in the navigation. It’s up to you to define the implementation of each, but by having these two defined separately, Azure are trying to encourage a good practice of separating the logic of the two. Build pipelines are responsible solely for building an artefact of your application, be this an executable or a docker image for example, and then running your tests and linting checks against this. It will then publish your artefact somewhere if everything passes, in effect, declaring it as fit for deployment. That is the end of the Build pipelines responsibility. Similar to CircleCI, your Build pipeline can be declared in YAML, or if you wish, a GUI. Being good developers, we obviously opt for keeping this defined in code so it can be committed into our repository. When it comes to deploying your code, the Azure DevOps mindset is to now move to a Release pipeline. It’s great in that it gives a clear separation of the two pipelines. The Release pipeline will take a published artefact from a build pipeline and figure out how to get that deployed out to your various environments. Here’s the bomb. Release pipelines can’t be codified. You have to build these using only the GUI. So for the reasons I’ve already alluded to, this is bad for a number of reasons: It’s really hard to re-use these pipelines across projects. You can’t just copy the pipeline from one project into another, and therefore; Setting up pipelines is manual, therefore time consuming and fiddly, which could; Lead to mistakes being made that may affect reliability or security I was quite surprised by how much I was affected by not being able to codify my deployment pipeline. It felt a little dirty and unclean for me as a developer. It hampers my ability to: work fast when setting up CD pipelines by being able to reuse existing pipeline code keep the pipeline reliable and reviewable before changes are merged into it, making it more secure not needing to document the pipeline separately with how it is set up and configured These practices, along with many others, are what we pride ourselves on at Made Tech – doing things the best way we possibly can, based on our experience over the years, and adopting best practices within the industry that we then do day-to-day without even thinking. It’s what helps us deliver good quality products to our customers. So with these downsides, I was trying to stay positive, and look for an alternate solution that could work, and knowing that the build pipelines could be codified, I started looking into perhaps utilising those for everything instead and ignore Release pipelines altogether. You can get quite far with this approach, as the build pipelines are highly scriptable to do whatever you wish. As with CircleCI, behind the scenes you just have an image of your choice, be it Linux or Windows, booted up for you, and you write scripts. The stumbling block was the inability to have manual intervention steps, whereby a release to production was guarded by an approval button. So very close. Word is, that Microsoft is working on the ability to codify and in fact unify build and release pipelines, however, I’ve been told that the feature has already been pushed back once previously, so there is no knowing when it will be ready to use. The current estimate is by the end of Q1 this year. Though we’re now in April, Q2… Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-04-10"},
{"website": "Made-Tech", "title": "Architecting Software for Extremely Fast Feedback-Loops part 2", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/architecting-software-for-extremely-fast-feedback-loops-part-2/", "abstract": "So you’ve read part one. You see the point of building Extremely Fast Feedback Loops and you want to understand what it costs to have such a test suite. Let’s explore some problems… The problem with testing in this way is that you must architect your system to support it. While I do not have the figures to back this up, I have observed that the number of teams that practice disciplined TDD is a small figure. This is something that we at Made Tech are actively trying to change. On top of this, the number of teams practising Acceptance Testing with TDD (ATDD) similar to how it is described in the book “Growing Object-Oriented Software Guided by Tests” (an awesome book by the way), is also a smaller number still. What this means is that systems built with ATDD, specifically one with Extremely Fast-Tests, will have an architecture that does not look like a “standard project”. This is the cost of Extremely Fast-Tests – the architecture will include aspects whose purpose will not be immediately clear to those unfamiliar with the challenges of building a system with Extremely Fast Acceptance Tests. That is not to say that the architecture is not simple, but a different simple. This is clearly an issue – it can cause confusion between groups of people with the same goals, but a different idea of what a good path to those goals looks like. It can cause tension in code reviews, it can cause distrust. I have heard comments such as “Why didn’t you just do it the way everyone else does it?”, “This could have been a few Rails controllers…”, “This seems way more complicated than it needs to be”. This kind of conflict is caused by only a slightly different perspective on how to approach testing software. Another anecdotal observation is that projects built without practising ATDD tend toward the more complex. This is an observation of teams attempting to build systems that have a Hexagonal Architecture, without practising ATDD (or even TDD). What usually occurs is that programmers introduce more code (and complexity) to cope with the lack of high-level tests, or they find that their unit tests solidify an architecture and hold back evolution. With Extremely Fast Tests: teams have the ability to quickly refactor anything. This frees up teams to discover new architectural designs, and really evolve codebases, without worrying about breaking functionality. If you want to picture a good architecture for this type of testing, imagine one where the Database Schema can be tackled last. Remember earlier that we mentioned testing below the Framework? Many programmers will be surprised by this, since the Framework, and also the Database, are both often seen as the core organising component. What ATDD does is make the Use Cases the core organising component, with the Framework, the Database and other external dependencies plugins to your Use Cases – the actual business logic. This type of architecture (out of the scope of this article) is known by many names including Boundary-Control-Entity by Ivar Jacobson, Hexagonal Architecture by Alistair Cockburn and Clean Architecture by Robert C. Martin. We isolate the framework because it is a dependency that will only make writing your acceptance tests more difficult. Since most teams do not build software using disciplined ATDD, you will find that the many frameworks that language communities love (including C#, Java, Ruby), will provide minimal help, or require you to delve into arcane features to achieve this type of testing. This phenomenon is often described as “fighting the framework” – what we do to win the fight, is to not fight at all. If you are using a compiled language – such as C# – your Acceptance Test assembly should not reference any MVC Framework. If you are using a dynamic language – such as Ruby, you should be able to execute all your Acceptance Tests without a single call to require ‘rails’ (or anything with similar effect) anywhere in your test suite. Once you isolate the framework, and the associated code – your controllers and views, you can begin to test this aspect in isolation (without any business rules). We call this aspect the “Delivery Mechanism” because it is what delivers your features to your customer. By decoupling this aspect you can write extremely fast unit tests for every aspect of your Delivery Mechanism. This is because you can swap out the Use Cases with test doubles! This is the reason we do not write Acceptance Tests to test “through the Framework”. We want to enforce this isolation. A neat trick is that now you can make your Acceptance Tests run at varying levels of abstraction e.g. you could have an Acceptance Test that is repeated twice: once “Subcutaneously” and once “through the UI”. What is happening are doing here is decoupling the steps of Arrange, Act and Assert from how they are executed within your system. Whether you choose to build a system in a way to practice all the techniques described in this article not… If you want Extremely Fast Feedback-Loops, you will need an architecture to support it. Don’t be surprised if this architecture leads down a path of doing things that aren’t well supported by the wider programming community. Remember that your ATDD discipline puts you in the minority, and always consider the pros and cons of making potentially surprising choices. If in doubt, write an Architectural Decision Record for future maintainers. If you experience resistance from fellow programmers, perhaps consider offering to pair program with them. We practice ATDD because we’re in it for the money – this is known as the “Money Premise” – we believe that ATDD enables us to deliver more value for less money than without. In reality, the Fast Feedback-Loops is what enables us to save money – anything that reduces the time it takes to go from an idea into being able to see the result of that idea (and crucially being able to validate it is a good idea) is usually worth trialling. Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-02-27"},
{"website": "Made-Tech", "title": "Digital skills shortage: what can we do?", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/digital-skills-shortage-what-can-we-do/", "abstract": "The evidence is all around us: there is a shortage of digital skills. You only need to look at the ever inflating tech salaries and the array of benefits companies offer employees. Such offers are made in an attempt to get access to the limited supply of tech talent available in the market. We’ve got a problem and it’s only going to increase unless we do something about it. What then can organisations do to address this shortage of digital skills? Digital skills are in demand within every industry With 55% of digital tech jobs falling outside the tech industry [1] , it’s clear the demand for digital skills is ubiquitous. Couple that with the Department for Digital, Culture, Media & Sport (DCMS) definition of technology literacy: 32% of UK jobs are classed as digitally-enabled and 13% are digital tech jobs [2] . These are big numbers. As each industry wakes up to the need to improve their technology game, to modernise and take advantage of digital, they struggle to compete in the talent market both in terms of salaries and the culture they have to offer. It’s no wonder tech startups are busy disrupting long established organisations, they have the venture capital to pay top salaries and they have awesome startup cultures and modern technology stacks that attract top talent. A polarised problem On one hand, you have venture capital-backed tech companies paying top money for seniors. On the other hand, you see other organisations trying to hire experienced developers for junior salaries. “Seeking junior developer with 5 years experience” It’s common to see job adverts for junior developer roles that want 5 years of experience or to have experience in many different programming languages, frameworks and tools. The organisations that do this often do because they are limited by salary budgets but have big needs for digital expertise. The opposite also occurs in venture capital funded startups and large organisations with more comfortable salary budgets. You see developers with minimal experience employed into senior roles. This is again caused because of a big need for digital expertise and compromises being made to meet hiring quotas. I can see why this happens and I can also see why this results in poorly delivered technology. It’s tough to rationalise why engineers need to be paid so much more than other skills within the business, governments too struggle with this especially. The result though is hiring poorly skilled engineers, settling for less experience, or failing to hire. It’s tough and fraught with disappointment all round. Organisations want and need experience but can struggle to attract people with the right expertise. Budding technologists can be chewed up and spat out by organisations as mismatches in experience and expectations are discovered. What can we do? Inverting the senior to junior ratio with coaching Use your investment in senior salaries wisely. Create a coaching and mentoring culture within your organisation. See seniors as a multiplier for junior productivity rather than seeing juniors as reducing productivity. By introducing coaching and mentoring expectations to the seniors within your business you create open learning cultures. Seniors become knowledge sharers rather than silos. One place to start with this is introducing onboarding plans and buddying up seniors with new joiners and hold seniors to account for the development and onboarding. We publish clear expectations for all of our roles in our Handbook and our seniors are responsible for helping new joiners meet them. We currently have an average of 1:1 ratio of seniors to juniors on our engineering teams. I aim to increase this over the next year, that is, increase it to a 1:2 ratio. Run your own academy A more advanced route to onboarding requires a little more investment up front. We’ve been running our own Academy since 2017 . This year will see us onboard two cohorts of six Academy Software Engineers. We provide 12 weeks of training, during which we pay our learners a £25k salary, while they learn. They then get a raise to £30k when they graduate. While running your own 12-week training programme may seem to be a big undertaking, particularly for SMEs, I can say from experience it works. We’ve managed to run our Academy with a mix of group workshops, a lot of pair and mob programming, some self-study and also a group project. We have actually run it to date with just one (very passionate) senior software engineer who has now taken the role of Head of Learning. We’ve even open-sourced our learning materials to make it easier for others to run their own academies for Software Engineering roles. We’re looking to add materials for other digital roles in the future such as Delivery Management. Provide inclusive access to digital knowledge Why do we pay our Academy Software Engineers a salary rather than charge them a fee? Inclusivity. It’s the same reason we do not offer unpaid internships. They exclude those who cannot afford to not be paid and with such a shortage of digital skills in our industry, it’s very shortsighted to be creating exclusionary access to digital knowledge. Do not exclusively hire from paid bootcamps. Not everyone can afford £8k to attend a 16 week training course. Okay, some provide scholarships and apprenticeships like Makers Apprenticeship, but not nearly as many as are needed. I was glad to see Makers introduce a financially inclusive option with their apprenticeship programme as they previously only had an academy programme that cost a flat £8k fee. Unfortunately the apprenticeship option is oversubscribed and I hope to see more organisations offer You can also run mentoring networks by encouraging your team to mentor those looking to get into the industry. This pay it forward mentality also pays dividends. You increase your network of budding technologists proactively doing what they can to get into the industry. What more can you ask for than tenacity? Train the digital skills you need Organisations need to invest in growing their own digital skills. The market isn’t going to magically change overnight. Sure, digital skills need to be delivered through education and they are increasingly so, but that doesn’t help us today. In lieu of society catching up with the demand for digital skills we have to address the supply of digital skills ourselves. I’d love to hear from those already investing in growing their own talent, and also those looking to start their own journey of training the digital skills they need. Catch me on Twitter @lukemorton . [1]: Source: ONS Annual Population Survey, Wave 4 2016, Waves 1-3 2017 [2]: Source: ONS Business Structure Database, 2017 Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-03-27"},
{"website": "Made-Tech", "title": "Architecting Software for Extremely Fast Feedback-Loops part 1", "author": [" Craig Bass"], "link": "https://www.madetech.com/blog/architecting-software-for-extremely-fast-feedback-loops-part-1/", "abstract": "When we deliver software, we need to ensure it meets the needs of end-users. The slowest feedback-loop is a “big-bang” release – this is essentially shipping all the features at once after developing for weeks, months or even years. We know this to lead to failure far more often than not… Yet, many teams release software this way, and no we don’t recommend it. The reason we don’t recommend you practice “big-bang” releases is not that we don’t want to release “perfect software” or because “we are lazy”… It is because we want to lock ourselves into a feedback cycle between the users, any new problems and the code. If you do not course correct often enough, you will build the wrong thing. The best way to achieve fast feedback loops is with tests, but not just any tests, Extremely Fast Tests . Unit Tests can be fast since they test isolated aspects of the code. You can isolate away anything that is slow – which is usually anything that involves Input-Output operations. They are also really helpful because they are able to pinpoint the exact location of breakage in functionality. However, this isolation comes with a cost. That cost is that while you know a Unit works, you do not know the Units work together correctly. They provide low-confidence in the correct behaviour of the whole system. Typically, most teams solve this by writing tests that simulate the act of using a keyboard and mouse to click buttons and fill out fields. We shall call this “Testing through the UI”. This has the benefit of providing high-confidence that the whole system works. These tests come with a cost – that cost is speed. Testing this way is slow. Really, really slow. So slow that you probably want to only have a handful (between 1 and 10) of this type of test. Many teams realising that these tests are slow, work up from Unit Tests to create so dubbed “Integration Tests”. More often what programmers writing these types of tests do is find load bearing integrations between individual Units, and write tests around those Units to provide increased confidence that they work together. The downside of Integration Tests is subtle, and we must explore “Acceptance Testing” first to be able to explain why. Acceptance Tests are what you can talk to your customer about, sometimes your customer can write them. You can have a conversation about them, you can collaborate. This makes them extremely valuable as a tool for answering questions about the Functional requirements of a system. Your customer can pose questions like “What happens to rounding of VAT when there are multiple line items with many different quantities?”, you can quickly answer this question and validate the correctness of the system. Acceptance Tests are also interesting because you can code them to test through the UI, to test through the HTTP API, or even to test through some other interface. As we know, the slow aspect of testing is I/O operations – so we should avoid that. This takes us into the world of “Subcutaneous Acceptance Testing” – here defined as testing below the network e.g. HTTP API (which we avoid because it will slow our tests down) and below the framework (we will come back to this later). This is different from “Integration Testing” in one important detail: acceptance tests are always specified from the perspective of the customer. This makes them valuable in two ways – you get all the benefits of “Integration Testing” plus you can have a conversation with the customer about them. The subtle downside of integration testing is that it’s not inherent in their nature to encode business rules from the perspective of the customer. That is not to say that you will never need Integration Tests, but you will likely minimize the number of them in favour of Acceptance Tests. By having a suite of Subcutaneous Acceptance Tests along with Unit Tests, that run in under 30 seconds, borne out of a team that practices disciplined ATDD; you will have Extremely Fast Feedback-Loops. You can ask questions about the functional correctness of the system by writing a single Acceptance Test and get the answer in seconds. The value of this is not to be trifled with, nor should you pass it up without carefully considering the downsides. So this is the ideal way to write tests. We will explore in a second article at what cost these come, and explore some of the downsides. Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-02-20"},
{"website": "Made-Tech", "title": "Made Tech’s Guide to Continuous Delivery Tools", "author": [" Mark Sta Ana"], "link": "https://www.madetech.com/blog/made-techs-guide-to-continuous-delivery-tools/", "abstract": "Since we wrote the original article just a little over two years ago, we’ve seen a fairly big shift away from self-hosted tools to feature rich Software as Service (SaaS). We’ve also seen nearly all Continuous Integration (CI) tools blur their lines with Continuous Delivery (CD) providing an all-in-one solution. That being said, finding the right platform to form the basis of your Continuous Delivery is a fundamental principle of the Agile Manifesto, and you really need a solution that is going to fit into *your* existing way of working with minimal effort. In this guide, we’ll talk about the features to look for to run a successful Continuous Delivery pipeline , and some of the open source, self-hosted and Software as a Service (SaaS) solutions on the market. Selecting your solution When choosing your solution, be it SaaS or self-hosted, you need to have confidence that it will offer you and your team a pain free Continuous Delivery workflow. Factors that can influence this decision will range from the amount of work needed for initial setup, to the time it will take to undertake any ongoing maintenance, and finally how easily it integrates with your PaaS or server infrastructure. Whilst it’s great to shop around, ideally you want to get it right first time, as you don’t want to invest project time on an “Ok” solution that you will have to migrate away from 6 months down the line. Let’s talk features Essential features Whilst every SaaS or self-hosted solution on the market boasts a myriad of different feature sets, there are a handful of core features that are essential to operating a successful pipeline. – Integration with version control Version control integration is the most crucial feature your chosen solution should possess. When I say integration, I mean it should poll your repository, or use webhooks to detect changes, which should then initiate a new build and subsequently trigger the rest of your CD pipeline process. – Custom script execution Custom script execution within a pipeline step is a key feature, especially if you deliver a diverse number of projects. Thanks to buildpacks , many common deployments are very simple, but often the need arises to run custom deploy scripts. In these instances the ability to use tools like Capistrano to execute deployment tasks is vital. While it is written in Ruby it can be used to deploy almost any project, thanks to the open source community behind it. – A Pipeline A pipeline view is a visual representation of all of your deployment steps. These steps should all be linear. A single step, e.g. unit testing, could fan out to run multiple tests in parallel. Then any dependent step should be executed automatically once the previous steps have passed. The final step in the process should be a manual one, in order for one of the team to make certain that the build is in a good state before releasing to any publicly facing environment. – Notifications As a software engineer, switching contexts can be really distracting so your Continuous Delivery solution should offer an alerting system to inform your team to any successful or failed deploys without them needing to check a web interface. In a basic form, this can manifest itself as an email, however many Continuous Delivery platforms will also integrate with the popular team chat programs, like HipChat and Slack. New Features – Docker support Container images have replaced buildpacks as the way to assure consistent versions of software releases. Look for baked in Docker support to reduce the complexity and overhead of your pipeline. We use docker images as a consistent way to provide base image for Rails. – Pipelines/Workflow This is the heart of continuous delivery and there has been a lot of innovation in this space. GitHub , who traditionally are the first step in building a CD pipeline now provides GitHub Actions which allow pipeline functionality from within your source code repository. It provides a similar “look and feel” to IFTTT allowing build your workflow visually. Circle CI have turned workflow steps into shareable packages called Orbs . The “deploy anywhere” approach of Microsoft Azure Pipelines (part of the Azure DevOps suite) means you can release software to any environment: Kubernetes (Azure Kubernetes Service), Serverless (Azure Functions) and Web. This is, of course, achievable because of the close integration with their Cloud platform Azure. Using Azure DevOps suite meant we could commit new features through source control (VSTS) and deploy onto Azure Kubenetes Service (AKS) with relative ease. Heroku Pipelines’ Review Apps allows us to review Pull Requests in a single-use version of the application. Self Hosted vs SaaS The argument for self-hosted or SaaS solutions will not be resolved in this article, as both have their advantages and their flaws. A self-hosted solution for example will require a lot more initial set up than a SaaS solution, as most of these are one click installs, and simple to configure using a YAML file (or similar). SaaS can also reduce the complexity of managing an exotic build farm if you are delivering to multiple operating systems (Windows, MacOS and Linux). A common combination used by Rust developers is to use Travis CI for Linux and MacOS and Appveyor for Windows when building and packaging software. However, the upfront investment in infrastructure, and other set up required with a self-hosted solution, will be netted out over a number of projects, and will allow you flexibility in the long term as you are free to add features provided from community plugins and extensions. This is flexibility you just don’t get with a SaaS solution, where you are tied to their feature set and development cycle of adding new features. Note that the self-hosted route is only feasible if you have a dedicated Ops team, as the chances are that if your CD goes down, your CI will be down too (nearly all self-hosted solution provide CI/CD as an all-in-one package). If both are down then your developers can’t release features. Having discussed the recommended features (and pitfalls) of both options, below is a selection of solutions that provide said features, which the market currently offers: Self hosted – Jenkins CI Whilst technically a Continuous Integration platform, with the addition of the build pipeline plugin it is easily configured to be a complete Continuous Integration and Continuous Delivery solution. We have been using it for a couple of years and it is still our goto tool for self-hosting and combined with AWS CodeBuild can allow you to dynamically scale your workers to meet with sudden demand in builds. – Go Go is a Continuous Delivery platform written and maintained by the folks at ThoughtWorks, who literally wrote the book on Continuous Delivery, so as you can imagine Go is based on a lot of the principles outlined in it. Like Jenkins (with the build pipeline plugins) it will perform both Continuous Integration and Continuous Delivery. – Spinnaker The newest option in the list – Spinnaker – differs from the previous two as it is only a Continuous Delivery platform. Spinnaker will take a deployable asset (e.g. a Docker image) and deploy it out though the pipeline steps, but only once it has received trigger from a separate CI platform like Jenkins. New players for Self-hosted GitLab aims to provide seamless integration between self-hosted and SaaS. With multiple ways to deploy your own CI/CD instance. The added bonus of using GitLab is that you can buy support. We have used this package microservices as Helm charts (Helm is Kubernetes’ package manager) to be deployed to the Kubernetes cluster. And yes we hosted our GitLab in our cluster. SaaS – Cloudbees Cloudbees is Jenkins in the cloud. Cloudbees uses a Workflow plugin – which you could implement on your self-hosted Jenkins instance – to add Continuous Delivery functionality. So if you like your self-hosted Jenkins but no longer want to maintain the infrastructure, then this could be an option for you. New players for SaaS There’s been a massive growth in this area with cloud independent solutions such as Travis CI, Circle CI. On the flip side, Microsoft’s Azure Pipeline has strong integration with their cloud offering Azure. – Travis CI Travis CI is a hosted, distributed continuous integration service used to build and test software projects hosted on GitHub. Open source projects may be tested for free via travis-ci.org, while private projects can be tested at travis-ci.com on a fee basis. They are mostly famous as the go to CI for open source projects. – Circle CI Circle CI is a platform for software teams looking to rapidly build quality projects, at scale. Their intelligent continuous integration and delivery tools are both simple and powerful. Circle CI allows for a connected development ecosystem where every team member is empowered to make technology decisions. This is our go-to solution within Made Tech unless we’re required to use our customer’s own CI. Summary So while the landscape has changed in the last two years, the new features we have seen have made building that all-important pipeline ergonomic and no longer the proprietary domain of the Ops team. We don’t think it’s possible to ever be definitive and say this is the platform you should be using, or that is the correct solution for you to choose. Different teams will have different requirements for their Continuous Delivery tooling. The trade-off between self-hosted and SaaS providers remains relatively unchanged. Although perhaps self-hosted has become easier to provision and scale. If you deliver a large number of client projects, a self-hosted solution that can be tailored to your needs will most likely be a better fit. On the flip side of the coin, if you are maintaining a single product, a SaaS solution will most likely be your best bet, as you won’t have to worry about the additional infrastructure. Read more on continuous delivery. Tweet 0 LinkedIn 0 Facebook 0", "date": "2018-12-04"},
{"website": "Made-Tech", "title": "Lessons from our Academy: How to teach a new programming language", "author": [" Maria Astakhova"], "link": "https://www.madetech.com/blog/lessons-from-our-academy-how-to-teach-a-new-programming-language/", "abstract": "In this blog post, I’d like to share some advice that I’ve learnt from the Made Tech Academy that might help you teach a new programming language as part of an onboarding process. As we’ve written in the past, organisations struggle to hire the digital skills they need. Made Tech is no different and that’s why we started our Academy . Before joining Made Tech, I knew C++ and Python. I had to quickly transfer my coding skills over to C# and Ruby. Hopefully, some of my learnings will be useful to you. 1. Dip your toe in with a koan Where to start? Well, every programming language has its approach to syntax and semantics. Koans are the perfect way to get a taste. If you’ve never heard of a koan before, it is a set of exercises to help you learn the features of the language, step by step. For example loop syntax, writing classes, and the standard library of the language. The koans will equip you with an awareness of what’s available in the language, and even some interesting edge cases. Most languages have koans that you can find online or on our learning resources . 2. Get into exercises quickly The most effective way to learn a new language is to start coding. Start with simple tasks; add more complexity as you get more comfortable with the language. This will help push your proficiency in the language. Good exercises will test your ability to address a range of problems – code katas are particularly great for this. Katas are challenges that you solve using code. The bowling kata was the first one I attempted at the Academy. The challenge is to implement the scoring system for a game of bowling. This sounds deceptively simple, however as you progress you learn that the scoring rules are difficult to describe in code. The great thing about katas is that you can re-attempt them. You can start applying practices such as test-driven development (TDD) and design patterns; along with advanced features of the language. If you want to try a kata you can find plenty online, or see our list of katas . 3. Guide your code with principles Some concepts and practices are transferable between languages, such as design patterns and TDD. These are designed to help you architect your code. Using a set of principles to guide your software development can reduce the fear of the unknown, leaving you with more energy to learn the new language. I started practicing TDD when I joined the Academy – at the same time that I started learning Ruby. At first writing test driven code seemed like more work, but over time it has become a useful practice. Working through test-driven exercises will help you to break down a challenge, and to refactor your code iteratively. Even if you are new to a concept like test-driven development, learning it will then help you in the future. 4. Learn the language, not the framework It might be tempting to jump straight into a framework, however, this can lead beginners to confuse framework features for the language. Learning the language on its own at first will enable you to know its features and idioms. This will help you develop your problem solving skills, without defaulting to a particular framework. My introduction to Ruby didn’t involve the Rails framework at all. This is different from what I hear about a lot of coding boot camps. Instead of building web applications in my first week, I was learning how to make small programs using Ruby. I can now integrate any Ruby framework into my code. 5. Don’t get bogged down with detail/syntax When faced with learning a new language there is a balance to be struck between reading the documentation and writing code. It is easy enough to get lost in several hours of reading, only to find that you don’t remember, or don’t know how to implement the thing you read about. Do spend time reading about the language, but do this in small chunks and practice as you go. If you’re working on an exercise, read just enough to get through the next step. When working on my first test-driven Ruby exercises, I could find approximations for expressions I already knew from C++ and Python, and quickly pass a test. Knowing that the test had passed, I could then research more Ruby-ist expressions as part of refactoring. 6. Learn with others One of the most effective ways to learn is to pair with someone else or to work in a group. This could involve solving a coding kata together or collaborating on a project. During the Academy, I have frequently paired with others or worked in a group to solve a kata. People will use the language in different ways to solve a problem, or they may be aware of methods and tools that you aren’t aware of. On many occasions I have learnt that several lines of Ruby code can be re-written in a single line – using a Ruby expression. 7. Everyone’s learning needs are different Adapt and mix all of the above to suit individual learning needs. Agree on an appropriate pace; experienced coders are likely to learn a new language faster, leveraging prior knowledge. Less experienced coders may need more time to get familiar with concepts, as well as support when completing exercises. An important aspect of the Academy experience has been working out how to collaborate on projects while accommodating our different learning styles. The way through this has been mixing up independent learning with group sessions, and balancing practical work and research time. Give it a go Whether you’re teaching yourself or onboarding a new colleague, give some of these learning tips a go. Is there a learning strategy that you would like to share? Let us know on Twitter ! Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-10-29"},
{"website": "Made-Tech", "title": "Why React Is a Game Changer For UI", "author": [" Faissal Bensefia"], "link": "https://www.madetech.com/blog/why-react-is-a-game-changer-for-ui/", "abstract": "React may be a library for building front-end web applications but the concepts it introduces are exciting and I’d love to see them applied to native user interface libraries. In this post we’ll be comparing the way React unifies behaviour and layout with the way traditional user interface libraries separate them, and the conceptual simplification this brings. The Traditional Way It is often the case in user interface libraries that entire chunks of the layout are defined in single files, detached from the code that controls them. ToDoList.layout <VerticalLayout id=\"layout\">\n  <ListBox id=\"myList\"/>\n  <Label id=\"helpLabel\" text=\"Click the add button!\"/>\n  <Button id=\"addButton\" onClick=\"addButtonClicked\"/>\n<VerticalLayout/> ToDoList.class class ToDoList of Widget {\n  constructor() {\n    setLayoutFromFile(\"ToDoList.layout\")\n  }\n\n  addButtonClicked(){\n    layout.hide(helpLabel)\n    myList.addItem()\n  }\n} This separation requires that the developer expend extra effort to understand how individual classes impact the displayed layout at any given time. In this paradigm behaviour and layout are very far from each other, meaning that additional software must be used to navigate and stitch together the sprawling files. This is to say that WYSIWYG editors in this instance are a symptom, not a solution. Some of these problems could be addressed by defining the layout with the behaviour code, but this comes at the cost of the clarity provided by the hierarchical approach found in XML-like languages. Below you can see how meaning is lost with the flattening of the code. ToDoList.class class ToDoList of Widget {\n  constructor() {\n    myList = ListBox()\n    addButton = Button(\"Add\")\n    helpLabel = Label(\"Click the add button!\")\n    layout = VerticalLayout()\n    layout.add(myList)\n    layout.add(addButton)\n    layout.add(helpLabel)\n    setLayout(layout)\n  }\n\n  addButtonClicked() {\n    layout.hide(helpLabel)\n    myList.addItem()\n  }\n} These libraries also splinter the definition of an interface across a class, potentially occurring at any point during the event loop, making it difficult to change the existing interface without being aware of its current state. Out of necessity some projects will often end up uniting user interface updates under a single method, similar to React. The New Way React on the other hand makes it very easy to see where your user interface begins and ends. It does this by splitting parts of the user interface into components that can be represented in a nested structure with your code and uniting all changes under one method, which cannot itself affect the state of the component. ToDoList.class class ToDoList of Widget {\n  constructor() {\n    state.labelHidden = false\n    state.listItems = []\n  }\n\n  addButtonClicked() {\n    state.labelHidden = true\n    state.listItems += ListItem()\n  }\n\n  render() {\n    return <VerticalLayout id=\"layout\">\n      <ListBox id=\"myList\" items=state.listItems/>\n        if state.labelHidden {\n          <Label id=\"helpLabel\" text=\"Click the add button!\"/>\n        }\n      <Button id=\"addButton\" onClick=\"addButtonClicked\"/>\n    <VerticalLayout/>\n  }\n} Rendering is done in the render() method, this ensures that any runtime changes made to the user interface are done in one place, rather than events being able to modify the layout directly. This means that you do not need to keep track of components, avoiding bugs caused by irrelevant elements of the layout persisting. Additionally, functionality attached to the layout is defined very close to the user interface itself, making it easy to navigate between the user interface and the associated functionality. React Native is sometimes dismissed as a fad fueled by the prevalence of Javascript but this simplistic view overlooks the clear demand for a better way to build user interfaces and for the time being at least, it is React Native that is fulfilling that demand. Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-09-06"},
{"website": "Made-Tech", "title": "Creating shared knowledge of Web APIs within Made Tech", "author": [" Lawrence Goldstien"], "link": "https://www.madetech.com/blog/creating-shared-knowledge-of-web-apis-within-made-tech/", "abstract": "A bit of background Made Tech run a learning session every Friday for everyone in the company called Learn Tech. The primary purpose of Learn Tech is to empower all employees to develop their knowledge and, in turn, work towards career progression goals. This time has also been used to discuss the need for new guides , core skills , or workshops. As a new starter, I’ve found this a welcome shift from my previous experience around learning in the workplace. In one of these sessions, a lead engineer asked: “Do we need to provide learning materials for API development?”. During this initial meeting, the attendees believed that there should be some learning materials, and this was an important subject to explore further. The first session ended with a goal for subsequent sessions: every engineer at Made Tech should have a shared understanding of what building awesome Web APIs looks like. We had also agreed that the next meeting should seek to validate and define our approach to achieving this goal. Validating our assumptions When we met again, we produced a survey to ascertain the level of understanding across the business, not just within the technology team. We sought to answer the following questions: What exposure do people across the company have to APIs? Do people think that having a shared knowledge would be of benefit? What makes an awesome Web API? Our survey showed that while every respondent knew what a Web API was, they did feel that they could have benefited from materials. 100% of the respondents said they knew what a Web API was, while 87.5% said that they could benefit from some introductory resources. We also found that the most common pain point when using a Web API was the lack of complete documentation. We used the results of this survey to ascertain the need, and want, for a shared understanding across the business when it comes to Web APIs. We also collected several resources and opinions around the experience of Web API features and pain points. We found that most people knew what a Web API was, but still felt that they could benefit from some form of introductory materials. For this post, we put together a version of the survey that can be used to run your own internal research. Ways of sharing knowledge We discussed the different methods we could use to facilitate this shared understanding. Workshop We thought that a workshop would be an ideal way to bring people across the company up to speed. Workshops cannot realistically be run on a regular basis due to the investment of time needed from attendees. The main advantage is the ability to bring a larger group of people up to speed at the same time. Workshops allow us to create a resource that is easy to present, and can be delivered repeatedly. Series of blog posts As a static resource, blog posts are great as they allow for self-directed study and do not require the same time investment as a workshop. Producing a series of posts benefits external readers, not just the team at Made Tech. Ultimately, our goal is to ensure a level of shared knowledge across the business and while a series of posts could achieve this, using a workshop format would allow for interactivity that a blog post would not. Guide or Core Skill By producing a guide or a core skill, we would ensure that our standards for what makes an awesome API are available as a learning resource for everyone. A core skill would also involve coming up with a curriculum of study for people to follow and is a significant investment of time. Summary We decided to produce a series of workshops with accompanying blog posts. The workshops would allow us to provide a shared knowledge of APIs to everyone at Made Tech, while the blog posts offer a static resource for further reading. We came up with the following structure to deliver on our goals: Workshop: What is a Web API? Post: What is a Web API? Workshop: Consuming APIs Post: Consuming APIs deep dive Workshop: Developing APIs Post: Developing APIs deep dive Post: Summary of outcomes and next steps We’ll see you next time with our post discussing what an API is, what value it can and cannot bring to your application development. Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-09-04"},
{"website": "Made-Tech", "title": "What does good technology look like?", "author": [" David Winter"], "link": "https://www.madetech.com/blog/what-does-good-technology-look-like/", "abstract": "Defining what good looks like and identifying appropriate metrics means that you’ll know whether the service is solving the problem it’s meant to solve. Collecting the right performance data means you’ll be alerted to potential problems with your service. And when you make a change to the service, you’ll be able to tell whether it had the effect you expected. GDS Service Manual Just as digital service teams are measuring the performance of their products for the reasons in the above quote from the GDS Service Manual, it’s important also for technology departments and development teams to decide on what good technology looks like so that we can ensure the choices and practices we’re implementing are having a positive outcome for our users. When it comes to technology: Teams may work in silos and rarely share knowledge which can lead to an increase in the time taken to onboard new team members, along with a loss of knowledge transfer Products might not adopt best practices and common standards from the rest of your organisation, or the wider technology community Development and ops tooling, as well as your pipelines to production, may be inefficient or not automated Code can be buggy, inconsistent, hard to debug, have slow tests or no tests Wouldn’t it be great if we could try and prevent these scenarios from happening as early as possible? Once we start to measure, we can start to see some important trends based on the data; both immediate shifts in those metrics for short term changes, but also overall a longer term picture. We’d like to see over that long term period that there is an upward trend where quality is improving. In the short term, if we see a downward trend, we can monitor that, but also try to proactively take action to remedy it and set it back on course. A technology department-wide score We work on multiple products for different customers and we need a centralised place where we can track these trends using metrics, so that we can compare between the product deliveries, to potentially identify areas where learnings can be shared between teams and as a result pull up lower scorers. With each team reporting on their individual metrics as discussed below, we can calculate an overall average for that product delivery team. Combining these product delivery scores into an average, we can then gain a technology department-wide score. All organisations with technology departments and/or any type of development team can benefit from recording a department-wide score, and by making it transparent, everyone can help and work towards constantly improving it. What and how should we measure? There are countless different metrics you could score your deliveries on, but we’ve focused on a few that are critical to delivering successful outcomes for our users, and that would help identify issues that could be easily remedied if caught early on, maximising impact and improvement. Technology NPS (Net Promoter Score) Why is it important? Teams should feel confident in the technology choices and practices that have been selected in order to have the best successful outcomes. Early on this can help to identify issues that could be resolved by a course correction, or in a longer period, uncover a degradation of quality with the product and it’s underlying technology. What it means: Collect a score of between 0 and 10 from each member of the team asking “How likely is it that you would recommend the technology choices and practices selected for the product you are working on to a friend or colleague?” Team capability support rating Why is it important? Individual team members should feel as though they are supported in being able to deliver towards a product by implementing technology choices and following selected practices, and this can be in the form of coaching, mentoring or pairing. Unhappiness can lead to bad retention, or poorer quality of technology. What it means: Collect a score of between 0 and 10 from each team member asking “How supported do you feel by the team in the ability to implement the technology choices and practices being used for the product?” Productionisation (p15n) score Why is it important? The p15n checklist is a collection of best practices that have been learned over the years, and have helped assist with consistency, supportability, maintainability and extensibility. There should be a good reason why the recommendations aren’t followed if they are applicable to a product. What it means: Teams should be baking in p15n from the start of a delivery, focusing on items relevant to the stage of their delivery, and this should always be improving over time. A low score would help to identify a risk to the future and longevity of the service. The score against the checklist can be reported on weekly. Test coverage Why is it important? This is often a controversial topic. However, this should never be treated as a score on the quality of tests written for an application, as that is impossible to measure objectively, and that is completely down to experience in writing good tests. It’s better to reverse this view on test coverage and to consider what production code has no coverage whatsoever and what code is at high risk of breaking and not being detected. Read it as “20% of this code is not covered,” as opposed to “80% of the code is covered.” What it means: Teams should be collecting code coverage metrics in their CI/CD pipelines when tests are run, and ideally a minimum should be set to reduce risks, but also to ensure if there is already high test code coverage, that it doesn’t start to slip. Deployment frequency (deploys per day) Why is it important? If deployment frequency is low, that could be a sign that there is not a clear path to production. This is one aspect of the Made Tech way. A reduction in deployment frequency can identify issues with the quality of a delivery, such as; a poor CD pipeline, too much WIP, backlog features not being owned and pushed through to production, organisational blockers, test failures, etc. All of these could eventually lead to unhappiness with both the team and users. What it means: Once a deployment has occured to the various different environments, for example, edge, staging and production, increase the count of deployments against those specific environments. Critical incidents (incidents per week) Why is it important? Incidents can pull the team away from delivering on their current sprint, which means a slowdown in delivery pace. But also it is potentially a sign that quality of code and tests is slipping. This can cause a risk of dissatisfaction with users. What it means: Teams need to take ownership of handling these incidents when they are reported and once resolved write an incident report. The number of these incidents can be reported on during fortnightly meetings with the wider organisation. Mean time to repair (MTTR) Why is it important? When an issue is detected, you want to have it resolved as soon as possible. Every second the issue is present, users are likely to be affected by it, resulting in a loss of confidence in the product, service and organisation. It can identify issues in team redundancy around getting issues resolved, CD pipelines, and deployment stages. What it means: Teams could implement simple uptime monitoring around critical user journeys, as well as more involved smoke tests. The MTTR can be tracked either via the uptime monitoring tool, or between when a smoke test started failing, and once it begins passing again after a fix has been deployed. Pipeline cycle time Why is it important? This provides a good indication of how quickly changes can be deployed out to production once they have been committed. It can help to raise alarms over a slow CD pipeline, or a test suite which can affect your feedback cycle and MTTR. What it means: The team could track this based on the commit hash within the repository; start and stop the timer at each stage of the pipeline once a commit has been merged into the integration branch in the CI/CD pipeline. After the commit has been successfully deployed out to production, calculate the total length of the timer in either seconds or minutes. Age of technology Why is it important? Ensuring that technical debt doesn’t accrue, and also keeping update paths easier, and as a result, things more secure, making applications more likely to be supportable and easily maintainable in the future. It’s a good way to ensure that some time in sprints is set aside to do a regular health check on age. What it means: Ideally, you want to aim for a technology age of around zero. One potential way to track this is by tracking version numbers of languages and packages that you use, and then compare the difference between the most recent versions based on major and minor version numbers. For example, if you’re using Ruby 2.5 and the latest version is 2.7 that contributes 0.2 to your Technology age. Still using Rails 4.2? Oh dear, you’ve just got older and are now 1.2 years! Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-08-07"},
{"website": "Made-Tech", "title": "Balancing discovery and development", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/balancing-discovery-and-development/", "abstract": "In an ideal world, ideas are transformed through discovery and development into desired outcomes and impact. Discovery tests our assumptions and provides a design to move forward with. Development takes this knowledge and delivers it to users. Discovery and development are both delivered by the same cross-functional team, often in parallel. In practice, balancing discovery and development isn’t always easy or a priority. In my experience, working in startups and technology companies, development didn’t have much involvement with discovery. In fact, in many startup environments, the CEO was the discovery team. Design was a phase that resulted in a Sketch or Photoshop file handed over from a branding and PR agency rather than a continuous process running alongside development. At worst it was a “build it and they will come” approach. The thing with this approach is that “they” don’t usually “come”. And at best? It was a “build, learn, measure” MVP approach. The problem here is that failing is expensive if you’re building comprehensive solutions, or in the case, your CEO will accept a ropey MVP, the solution won’t scale. There’s pretty much no analysis in the approaches I’ve described or at least none that was visible to me. Even if the research was carried out, the development team certainly weren’t involved. By the time developers were engaged, a shopping list of features were already defined as a product backlog. The backlog was signed, sealed and ready to be delivered to a deadline. Only the development team have no context of the problem they are solving or why the solution in the backlog is the right one. In my experience, ego-driven development is a good name for this “just get it done” style. Discovery phases that last years I see the opposite problem in many government organisations. Discovery phases that last years and end up being reset and started again due to staff turnover. I’d call this, “analysis paralysis”. I see similar issues with service design and user research consultancies, they naturally lean towards learning and insights, towards discoveries. Perhaps it’s a hammer and nail type of thing, developers naturally lean towards building while designers and researchers naturally lean towards designing and researching. Interestingly, the time I see discovery skipped or expedited in government is when a minister has a political reason to push forward. Does that remind you of the startup CEO I described earlier? It does me. Ego-driven development, or perhaps politics-driven? Either way, they aren’t backed by validated learning. Balancing discovery and development That’s the problem with spectrums, isn’t it? It’s hard to take a middle path, balance is difficult, you’re more likely to lean one way or the other. There’s no blame in what I’ve illustrated, in fact, heavy discovery can be a good thing, and just getting something shipped can also be a good thing. I’d just say that they aren’t always the best route. I have seen both startups and government organisations take the middle path and I’ve seen it be massively successful. HMPO’s transformed Passport Renewal service is a simple and easy to use digital journey but they still have a commitment to those digitally excluded by providing assisted-digital and paper-based services. Made Tech’s work with FutureGov and the London Borough of Hackney to deliver new services within Housing are cross-organisational collaborations to be proud of that had service design, user research and technology working hand-in-hand with real users. In the private sector, Spotify has been known for taking software engineering seriously but they put as much effort into design . I actually feel that team composition can have a big impact. I’ve noticed for example when our engineers are engaged during discovery, there’s a tendency for those discoveries to move into development faster. The engineering know-how gives the rest of the team confidence to move forward. I’ve also seen researchers engaged throughout deliveries who can help give pause to development, to stop and think for a moment, instead of building and building and building. Perhaps there’s a trick in that? If you look at the teams and consultancies who are more comfortable staying in discovery-mode or if you look at the teams and tech companies I’ve worked in who feel more comfortable building rather than thinking, it’s a lack of the other discipline that’s common. Do you see this in your organisation too? I’d be interested in the results if you’d ask yourself or your team honestly, how balanced is your approach to discovery and development? Let me know the results on Twitter or by email 🙂 Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-06-04"},
{"website": "Made-Tech", "title": "Kubernetes: The Good Parts – Should you build your own or use a managed service?", "author": [" Mark Sta Ana"], "link": "https://www.madetech.com/blog/kubernetes-the-good-parts-should-you-build-your-own-or-using-a-managed-service/", "abstract": "Now that we know what Kubernetes is , let’s see what options we have to create our own Kubernetes cluster. Rolling your own When you write software rather than use an existing off the shelf product, you define its form and functionality. The same goes for infrastructure: when you build it from the ground up, you gain a better understanding of how the parts fit. Kubernetes’ managed services are a relatively new option when it comes to provisioning a Kubernetes cluster, but they have stemmed from customers needs. Setting up Kubernetes from scratch is not easy. In the introduction, you’ll recall there are a lot of moving parts. To get a feel for the level of complexity we’ll cite Kelsey Hightower’s now legendary “ Kubernetes The Hard Way ” tutorial for creating a non-production Kubernetes cluster. The tutorial consists of lab sessions whose topics range from: Setting up client tools to create your cluster Provision the network resources i.e. VPCs, firewalls and public IP address Provision compute instances Provisioning Certificates to facilitate secure comms between clients and the clusters. Also, most services that form the cluster will also need their own certificates Bootstrapping services and servers in the cluster The chances are that you may know several of these areas extremely well, but not all of them. Can you afford to take on the risk of knowledge gaps in Production? There is help in the form of Kubernetes Operations (kops) which is a tool created by the Kubernetes group to provision and maintain Production grade Kubernetes clusters. There are some caveats to consider. Firstly the tool level of complexity varies between providers, in order of easiness: GCE Setup state storage (required by kops) Provision your cluster through kops AWS Set up an IAM user Choose a DNS strategy (there are at least four options to choose from) Setup state storage (required by kops) Provisioning your cluster through kops Prepare local environment Create and customise cluster configuration Build the cluster Whilst this automates the provisioning and maintenance of Kubernetes clusters, you’re still accountable for all the moving parts that we mentioned earlier. If you’re still interested in learning the ins and outs, there is a reward for this effort, by passing an exam to become a Certified Kubernetes Administrator (CKA). If your organisation has three CKAs and have met additional requirements you can apply to become Kubernetes Certified Service Provider . Managed service Just like other managed services (databases, DNS, mail, etc) we can also free ourselves from the implementation details and on-going maintenance of a Kubernetes cluster. The big three cloud vendors (AWS, Azure and Google) all have their own Kubernetes managed service offerings: Amazon Elastic Container Service for Kubernetes (Amazon EKS) Azure Kubernetes Service (AKS) Google Kubernetes Engine (GKE) Given the number of smaller cloud vendors who are also providing their own managed service, it would be fair to say that this is a good opportunity for vendors to upsell computing power and services. With the process of provisioning a managed Kubernetes cluster being very similar between the main cloud providers, it can be hard to know when to choose one over another. The most obvious decision is to stick with your existing cloud vendor if you already have an existing infrastructure in place. Beyond this, there are a few differentiators between the vendors and their managed service offerings: If you need specialised computing power (machine learning, high-performance computing, etc) then both AWS and Google’s can provide GPU capability. If there’s software that can only run on the Windows platform, Azure can provide a virtual node where Windows specific workloads can be allocated against. This same technology can also be used to provide a node that has burstable capacity rather than have to recreate the cluster with higher specification nodes. If you’re interested in evaluating new ARM-based servers, then AWS can provide clusters based around their new A1 ec2 instances. Whilst there are a lot of compelling reasons to use a managed service, there are of course some detractors Your Kubernetes cluster will be provisioned utilising services (compute instances, storage, load balancers, security groups, routers, etc) provided by your cloud vendor rather than the “best of breed” in the industry. Whilst it looks like most offerings are similar, you may find hidden costs, in particular, some cloud vendors will in addition to the infrastructure required to provision the cluster, will also charge you for running the cluster. Summary We’ve given a brief introduction to Kubernetes, the components that make it up and also the two routes you can choose to provision a Kubernetes cluster. We’ve highlighted the pros and cons of both approaches. Finally, we have this simple guidance: If you want cloud portability, use Kubernetes full stop. If you have the time (to set up and maintain) and inclination to gain a deeper understanding: build it yourself. If you don’t have the time, don’t want a larger area of responsibility or need specialised workloads: go with a managed service. Tweet 0 LinkedIn 0 Facebook 0", "date": "2019-07-19"},
{"website": "Made-Tech", "title": "Defining outcomes as a product team", "author": [" Andreas England"], "link": "https://www.madetech.com/blog/defining-outcomes-as-a-product-team/", "abstract": "Product teams should be focussed on delivering outcomes. In this post we will discuss how a product team can define outcomes and then ensure epics/features (outputs) are tied to outcomes. Hours taken is not an indicator of value As digital project delivery has developed from waterfall methodology, through the many flavours of agile to the current state of lean , so has how we measure the throughput, quality, and overall ‘fit-for-purpose’ of the digital products made. In traditional manufacturing processes , the final product was a tightly constrained and predictable output. This means that a simple method of measuring (admittedly a single dimension) output was the number of hours invested into production. With digital engineering, it is quite likely for the number of hours invested in a single aspect of a product to vary wildly due to the number of constraints, dependencies, and influences from elements that are sometimes out of the control of the project team. So without time as a measure, we need to view project outputs using different measurements. Outputs are meaningless without context Given free reign, most digital engineers will build what they like to. It’s part of human nature to operate within an environment where we feel safe, confident, and comfortable. Our ‘productivity’ will be as high as possible when we’re reproducing known solutions or re-purposing a product that’s proven, tested, and reliable. This looks great then, the digital engineers are beavering away producing lots of output, at pace, with few issues and it all works (doesn’t break). However, this output is directionless. We will, at the very least, be creating a product that will be cumbersome, onerous, or even unusable by the end-user. Moving from outputs to outcomes So far, I’ve postulated that time is taken and the amount of ‘stuff’ produced has far less impact on the success of a project than assumed. This doesn’t mean that they shouldn’t be measured during a project, however, they need to be defined against a far more important measurement: the value of the outcome. “Output is the amount of something produced, whereas an Outcome is the way a thing turns out.” A change in terminology enables us to shift the entire focus and direction of a project team from an outdated 18th-century manufacturing metaphor to a principle that is aligned with how digital engineering is best approached. Consider the review of a single component of a project that has just been completed. It is the output of a collaboration of designers, engineers, testers, etc. Thinking of the component as an output, we can determine that it has taken an amount of time to produce, it is of predictable quality, and that it will integrate into the intended environment with minimal issues. Now let’s consider the same piece of work as an outcome. By producing this component of the product, it’s now possible for an action to be achieved by the end user, an amount of effort to be reduced, or an issue we identified to be fixed. Why is this distinction valuable? In an engineering environment where anything, within reason, can be built, the stakeholders will have an aspirational view of what can be achieved. Put simply, teams will always be challenged to build more than they can within the time and/or budget available. Using outcomes to determine value allows the team to determine what should be built, in a way that outputs (time and volume of production) cannot. Outcomes are synonymous with value As mentioned before, the outcome is the way a thing turns out. So the team needs to know how a thing should turn out before they build it. I’m not asking for the team to predict the nuances of engineering detail that they will use to produce an outcome.  As an alternative, I’d ask them to be aware of how the component they will build will be used and how it will affect the end-user experience. Before anyone accuses me of only being able to consider ‘end-users’, anything we build that interfaces with another service (be it human or digital) can be referred to as a user. The value of an outcome is usually measured (ranked) against the value to the business (the client’s institution) and the value to the user. This moves us to some preparation that the entire project team must undertake to ensure the value of the outcomes and from the project as a whole. Proper planning prevents piss poor performance This is the core of user first/ outcome driven/ value-driven/ lean engineering methodology, or what you like to call it. Planning in this context is not the cumbersome collation of stale information into impenetrable documentation. Instead, we agree as a team to collaborate on a few exercises that will enable us to build the right thing quickly. North star/Business vision/Business goals Various schools of thought and workshop exercises use different terminology, but the key element for any project is an agreed direction of travel. This can be described as a value statement, a series of objectives, an elevator pitch, or a hypothesis statement. The format is less important than the overriding fact that the entire team knows where they are going and why. With a North star in place (my current favorite term), we now have a tool that enables us to measure the value of anything the team proposes to do with their time on the project. This has to be done at the beginning of any engagement. It’s a challenging outcome to craft, especially, when a team consists of strangers, trust is low and many will be exposed to new working practices. I’d like to be able to demonstrate a prescriptive workshop sequence that magically delivers this outcome. The reality is that we need them to be brutally frank and honest about their goals, and to share this openly with strangers. A pragmatic north star is a single measurement that we can use to rank all team effort. You’ll note I’m no longer expressing measurement in the abstract, I’m using it to rank project elements against one another. A reasonable North Star could be “the number of digital interactions per user.” What’s cool here is that we’re not dealing with the minutiae of user sign-ups, dwell time, journey, failure rate, etc. Instead, we’ll measure their interactions. Devil’s advocate could suggest that a frustrated user could endlessly ‘interact’ with the service without achieving anything of value, so I’m not suggesting the live service only has a single measurement. The user goals The end-user, whether human or other digital services, is the reason the product is being built. It may sit within a legal or regulatory requirement, but these are vehicles for encapsulating a user’s need. The craft of discovering and documenting user goals is beyond the scope of this post, but the outcomes from this exercise are the principal way we determine what we should build. We like to describe user goals as ‘user stories’. These are beautifully crafted works of art that Business Analysts spend decades perfecting on misty mountaintops as Haikus that conform to the meter of: negotiable, valuable, estimable, small and testable. At the coalface of a 20-week project working under COVID-19 restrictions, we’re more pragmatic. A user story fits on a rectangular post-it note when written with a blunt sharpie, preferably in legible handwriting. The value they have is that they describe a manageable chunk of value to the user. With enough of these written, we can prioritise them against the north star using the team’s knowledge, any domain experts we can persuade to attend. The end of the beginning We’ve not written a line of code yet, so many unknowns remain to be discovered. We’ve spent as little time as possible doing this, so we can optimistically reckon on having missed 20% ( Pareto principle ) of the user needs. We’re also aware that pivots are likely to occur, such as a global pandemic, which will affect the project. However, we have a strong direction of travel and a backlog of user stories that are prioritised. Given the cyclical nature of agile development principles, we’re able to return to our original assumptions regularly and evolve it to meet emerging needs. One legitimate criticism of agile development is that it can lose direction, as all goals are short term. However, with a North star in place and an understanding of how to use it, the direction is strongly defined. Final thoughts I’ve skipped over lots of details. There are dozens of workshop methods, diagrams, and structures that make a methodology like this work, so please believe that there are no ‘leaps of faith’ required. In summary, a project is an expression of a need. They’re normally couched in the language of the organization that identified the need. It can take a great deal of unpacking and discovery of the ‘world’ where the project resides, however by helping the project team to collaboratively discover, identify, collate and rank what they should do, enables them to perform effectively and happily. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-07-07"},
{"website": "Made-Tech", "title": "The world beyond the Academy ?", "author": [" Renny Fadoju"], "link": "https://www.madetech.com/blog/the-world-beyond-the-academy/", "abstract": "Several engineers, including myself, joined Made Tech via the Academy . It’s a 12-week programme that the company offers to teach passionate learners the best software engineering practices in the industry. Now we are working on customer deliveries, solving problems and addressing real user needs. I feel comfortable doing it, but, if I look back to when I first graduated, I remember feeling uneasy about leaving structured learning and familiar weekly routines. This got me thinking about whether my fellow graduates had thought the same. And whether, by speaking to them about their time at the Academy, I could uncover how the programme had helped to prepare us to transition into customer software delivery teams. But first, why the Made Tech Academy? A Habitat for Learning and Development The Academy was attractive because it positioned itself to be an atmosphere of continuous learning and development. It’s one thing for a company to pander to hopeful candidates in writing, but another thing for the company to be able to demonstrate its ethos sincerely, e.g. when you start working there. I believe the Made Tech Academy was sincere with its Academy engineers because it made clear what its responsibilities were and delivered on them. The main ones were: to sharpen our programming skills, to help us learn how to ship high-quality code in a commercial environment. I recently ran a survey amongst Academy alumni, asking them to highlight the topics or technologies they felt the Academy helped them increase their knowledge on: A bar chart displaying the percentage of responses from the survey identifying which technologies were learnt during the Academy The chart above paints a clear picture of the areas the Academy helped develop. It also confirms the spread of individual skills within each Academy intake.  For example, some individuals (e.g. me) might not have been familiar with the fundamentals of databases and SQL before the Academy, while others might have. From these responses, the Academy provided an environment for us to: Improve individual coding capabilities, regardless of previous experience. Learn and experiment with the latest technologies. Learn and experiment with popular programming languages. Learn and practice modern methodologies such as CI/CD, TDD. Learn leading software development practices such as User-centred design and Agile. It was where we developed our good programming habits and skills, but programming-related skills weren’t the only things we learnt. The Art of Valuable Delivery In the final weeks of the Academy, we focused on a simulated software delivery. The goals of each simulated delivery have varied over Academy sessions because not all software delivery teams outside the Academy would deliver the same outcomes. For example, one client delivery might require you to build a prototype application for users , along with formulating a strategy to develop its usability and longevity . In contrast, another delivery might want you to help convert a current legacy system into a modern system. The Academy simulated delivery taught me that: The client’s needs from the delivery are different from their wants and this difference needs to be noticed and considered throughout. Frequent and open communication of the delivery’s progress with the client is critical. User research is crucial and should be routinely done throughout the delivery. User research should always inform the next significant feature within the delivery timeline. Production code must be written bearing in mind the users of the service. e.g. Following service accessibility guidelines. Agile software techniques facilitate the smooth running of the delivery. So which delivery-related skills did the Academy help us develop? A bar chart displaying the percentage of responses from the survey identifying which delivery-relates skills were developed during the Academy As you can see from the chart above, a vast majority of individuals selected Agile software practices and productivity techniques. So why did these skills have the highest percentages? Notwithstanding the delivery scenario, every software delivery team benefits from its engineers being able to: adapt and respond quickly to changes, manage customer expectations and communicate these effectively, deliver outcomes frequently and in small slices, adhere to and work effectively towards agreed delivery timelines. Points 1 to 3 involve Agile software practices, and the final point requires effective productivity and time management techniques. In my opinion, the Academy prioritised fostering these two skills in each of its intakes and these skills are transferable across the types of client engagements that Made Tech specialises in. Continuous Improvement The Academy continually improves its curriculum and the structure of these simulated deliveries to give its students the best possible understanding of external customer software deliveries. For example, my set provided a solution to a fictional company but the subsequent sets have provided solutions that Made Tech currently uses. These include a Slack app that addresses the issue of people not filling their timesheets on time and one that allows people to view and book attendance for upcoming Learn Tech showcases and workshops. Still, as I’ve mentioned before, every delivery is slightly different. A Little Different than Expected It’s worth emphasising that, just because our time in the Academy ended, our learning did not come to a halt. Made Tech’s culture is one of continuous learning and development , and this continued after the Academy. But how do you prepare for deliveries that could vary in your familiarity with delivering their outcomes? “ Give me six hours to chop down a tree, and I will spend the first four sharpening the axe. “ Abraham Lincoln I understand this quote to mean that preparation isn’t finishing the task but being well equipped to proceed with said task with less resistance and more knowledge. Some Academy alumni commented on what skills introduced during the Academy were strengthened further during a client delivery: “Agile methodologies – user stories/needs specifically. AWS (Terraform), Sinatra, etc. Knowing Ruby helped me learn Sinatra quite quickly so this was a good foundation.” “Working as part of a multidisciplinary team. Pointing work. The Academy exercises on breaking down user stories and on being user focussed did help when working with URs and BAs.” “Senior/junior pairing. Detailed meanings of scrum, kanban, agile, etc., Had to learn how a team works in general, such as daily events and practices. Academy gave a good base level.” The above responses recognise the limits of the Academy but also recognise how useful it was for learning leading software practices and adapting to new technologies. World, Hello. When an Academy graduate moves into a customer delivery, they join an established software delivery team that has more experienced Made Tech engineers on it. This strategy allows us to learn from our more experienced colleagues continually. It is also clear right away that everyone in the company focuses on continuous development, not just those of us who have gone through the Academy. From what my fellow graduates told me, it seems the Academy had given them a great starting point for transitioning into customer deliveries: “It did give me the confidence to air my opinions about how much support is needed by juniors.” “The Academy gave me a really good overview of all components of a project that I definitely knew nothing of beforehand.” “The learning curve, with regards to technology, particularly, continued to be very steep after the Academy. However, I found a lot of things more manageable based on the topics I’d learnt in the Academy.” “As consultants, we are encouraged to share our expertise and transfer knowledge to our clients. We want to be professional and represent Made Tech well. We know how to problem solve from a theoretical perspective rather than how to solve a problem in a specific way and are encouraged to be polyglot software engineers…” So, how exactly did the Academy prepare us for the world of customer software deliveries? It introduced us to a breadth of best software development practices and ways of working that are transferable across different client engagements and technologies. It equipped us with the tools to get our foot in the door of software deliveries and cultivated the habit in us to develop our skills continuously. It also introduced us to a network of enthusiastic engineers who love to code, enjoy sharing their knowledge with others and are delivering services to users’ with their best interests in mind. The Academy prepared us for the world beyond by starting us off on the path to becoming tech generalists who can help clients solve their problems, not just write code. Shout out and a massive thank you to the Academy alumni who contributed to this blog post. Could this be your Academia? Would you like to become an Academy Alumni? As someone who has been through the process, I can assure you it’s a great way to kick-start your career in software development…oh, and there’s an ultra-rare laptop sticker in it for you too. If you are interested, applications for the are now open . Follow us on Twitter to learn more about Made Tech . P.S. At the time of writing this, I have just shy of 2 years experience in the software industry 🙂 Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-05-13"},
{"website": "Made-Tech", "title": "Don’t estimate without evidence", "author": [" Tito Sarrionandia"], "link": "https://www.madetech.com/blog/dont-estimate-without-evidence/", "abstract": "In software, estimating is controversial. Developers are afraid of being held to impossible deadlines, and managers are afraid of situations where they can’t forecast (and therefore can’t plan.) Blanket rejection of or insistence on estimating is too stubborn. Instead, you should focus on the situational evidence for and against estimating in your context, and pragmatically adjust based on what you learn. In this post, we’ll look at how to measure the quality of your estimates, and what to do when it doesn’t stack up. What is estimating? Estimating is when a team tries to figure out the size of a piece of work before starting the work. Different frameworks and teams will mean different things by size: time, complexity, novelty, etc. The theory is that if you assign a size to the pieces of work in your backlog, you should be able to do some forecasting and give your stakeholders a better idea of when they can expect to receive the value you are looking to deliver. Estimates are sometimes measured literally, in days or hours, and sometimes measured relative to other pieces of work (e.g. “This task is twice as big as that task”.) Teams who estimate with a relative measure often choose an abstract way to represent size, such as t-shirt sizes (small, medium, large) or “story points” (1, 2, 3, 4.) Endless variation on this is possible, many teams choose to use the Fibonacci sequence to represent the available options for estimating — a sensible compromise that recognises that the fidelity of an estimate is likely to decrease with its size. Teams that estimate often use some variation on “Planning Poker” – a ceremony where tasks are described, discussed, and team members give their estimates simultaneously to avoid being heavily influenced by each other. Outlier estimates are discussed, and a number is recorded against the upcoming task. There are plenty of reasons why teams might not estimate at all. It could be because they just haven’t got round to it yet, because they don’t believe that their estimation sessions were producing accurate data, because their task size is mostly consistent, or because their stakeholders and organisations don’t require forecasting. Measuring the accuracy of your estimates I’ve worked with many teams who have chosen not to estimate, a common scenario I see is this: The team is not estimating, and maybe not attempting to forecast. A new senior person joins the team: either as a stakeholder, a technical lead, a delivery manager, or a senior engineer. They are used to teams producing estimates, and have seen them used effectively in previous teams. They persuade the team to adopt estimating. Now burnup charts are produced, likely completion dates can be broadcast to stakeholders, and the team has a new meeting in the diary each fortnight to produce estimates. Several iterations pass. So what’s the problem? The team has no idea if it is working. Giving numbers and dates to stakeholders feels like a mitigation of risk. After all, if you spot that you are going to miss an important date early on based on your estimates, you can act to change course, or give stakeholders fair warning about what to expect. However, if you don’t have evidence that your numbers are accurate, you have massively increased risk . Sharing inaccurate data means that people will make bad decisions — false certainty destroys value. Checking for accuracy Before you do this, have an honest conversation about what an estimate is for. In my experience, the biggest motivator for producing estimates is forecasting: telling stakeholders what is likely to be ready by what date. Sometimes it is also used for cost benefit analysis of an upcoming feature. The thing these have in common is time: If your estimation process produces reliable indicators for the time it takes to do some work, your stakeholders will make better decisions. This means that you need to decide what you are going to benchmark your estimates against. Figure out how to measure Lead Time across your process, and make sure that measure is broad enough to capture time that the task spends in queues, analysis, testing, etc. It’s important to note here that if you aren’t using literal time as the basis of your estimation, then the thing we actually care about is consistency. Obviously we can’t truly say that we accurately estimated something to be “Medium” or “4 points”, because those are abstract measurements, only meaningful in relation to each other. We just care that “4 points” means roughly the same thing each time we see it. Next, pick an evaluation period. This needs to be long enough to cover a good range of the types of tasks that your team does. About a month often feels right. During this period, estimate all of your tasks and record the lead time for each of them. If you are using project management software, it will probably help you get this number automatically. You don’t need to overcomplicate things – it works just as well to add a small dot or sticker to each card on your story wall at every standup until it gets to Done, then count the dots. Next, plot the actual times taken on a stacked bar chart, and see how they stack up against your estimates. If things are going well, you’ll see something like this This chart shows that of the stories assigned “1 point” in estimation, 75% were completed in one day. Similarly, around 80% of the stories assigned “2 points” in estimation were completed in 3 days. This tells us that for any story that the team gives “2 points” during estimation, we can be pretty sure it will be completed within 3 days. In this example, our 3 point estimates are a little more shaky. It’s most likely that stories assigned “3 points” by the team will be completed in 4 days – but even though that is the most likely outcome, it’s only true 50% of the time. Still, most of the data is tight enough that you could do some forecasting. Here’s what it looks like when your estimates aren’t consistent. In this example, a story assigned “1 point” by the team is equally likely to take 1, 2, 4 or 5 days to complete. Stories assigned 3 or 4 points by the team have an almost identical profile. It’s hard to describe probability in a way that makes sense to human beings – so if you need a takeaway message from the above graph it is this: It would have been just as accurate to assign our estimates using a random number generator. In my experience, most teams that have never measured their estimates are in a position like this. Estimating consistently is really hard when part of what you are doing is solving puzzles. Unless your task is simple and something you’ve done before, then estimating the size of it is like estimating how long it will take you to complete a cryptic crossword. At this point you need to make a decision between trying to bring up the consistency of your estimates, or communicating to your stakeholders that you are no longer going to estimate. The one thing you should not do is continue forecasting with your dodgy data. As with all investments, if you choose to work on improving your estimation practises, think about what value it will give you and how much work it is worth doing to achieve it. Factors that influence accuracy Domain – Some domains are just more complicated than others. I’ve worked on teams where every new task starts with getting a professional opinion on the interpretation of a piece of legislation, estimating was pointless in this situation. Team maturity – The more times your team has “just added another rule”, or “just exposed a new piece of data on the API”, the more likely it is that they’ll be able to benchmark it consistently. Tech stack – Do you need to add a new library or framework to achieve this task? That adds a lot of uncertainty. Project phase – The early stages of development of a new product have more uncertainty than later, business-as-usual phases. If it is the very first time you have added an authenticated user session, for example, then you are going to need to do some learning, some puzzle solving, and some troubleshooting; this will be much harder to benchmark. Team wellbeing – Don’t be surprised if an unexpected event harms your consistency. For example, have you measured how your estimates changed when everyone transitioned from a co-located team to a remote team because of the COVID-19 pandemic? Are stakeholders putting extra pressure on your team because of a crisis elsewhere in the business? Estimates are made by humans, and so changes in team wellbeing will be reflected. Safety – This should go without saying, but if you incentivise your team to meet estimates, either through rewards or reprimands, your team will always estimate too high. If you get angry when your team tells you something will take longer than you are hoping for, your team will hide that data from you. Don’t be that person. Communicate certainty openly Most people agree with this in theory, but many find themselves in organisations where they aren’t actually empowered to change the way their team interfaces with stakeholders. In this situation, stakeholders might demand estimates for the sake of consistency between teams, regardless of the unique situation your team is in. When this happens, just make sure you always give the data about certainty alongside the estimates. For example: “Based on what we know about our estimation data, this epic is equally likely to take 10 or 90 days”, or “We gave this a 4, which in practice is the same as a 2, 3 or a 5”. This might sound facetious, but it’s reckless to communicate certainty when you have none. So should you be estimating? Maybe, but you should make that call with all of the evidence in front of you. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-06-24"},
{"website": "Made-Tech", "title": "Prioritising your application transformation", "author": [" David Winter"], "link": "https://www.madetech.com/blog/prioritising-your-application-transformation/", "abstract": "After you have mapped your application ecosystem as best as you can, you should begin work on prioritising which applications to modernise first. Lots of variables can help you to determine a priority order but it’s important to remember that it’s not rocket science. It’s mostly just a common sense approach to evaluating and balancing the various factors and needs of your organisation. Security and maintainability are important reasons to modernise legacy applications. If software ages and is not maintained, then security vulnerabilities can be discovered over time and it can become difficult to roll out the required upgrades to plug these holes. Security and maintainability come hand in hand. It’s also important to consider budget constraints. These may result in you choosing a particular transformation approach for an application or in deciding to modernise a different application altogether. In making these choices, you will need to weigh up the business value of transforming an application against the technical effort required to do so. What affects prioritisation? Some modernisation tasks may take longer than others or take longer than expected, so you might find yourself needing to juggle conflicting priorities. You may also find that a new piece of information surfaces when you are following up on your application mapping that has a knock on effect on your priority order. While it’s important to be aware that these issues might crop, you can get on with prioritisation by weighing up technical effort and business value. The diagram above serves as a simple guide to how these factors might steer your decision. Using the attributes you have collected during your application mapping, you can identify the technical effort an application may require and the business value it represents. As mentioned, the security and maintainability of an application are key considerations when prioritising. However, you might also have an urgent need to alter an application’s behaviour to add a new or adapt an existing feature based on changing user needs. If it is difficult or impossible to add or change the functionality of an application in its current state, the business value of the feature change may be significant enough for the organisation to trump all other considerations. Review priorities regularly Just as you needed to create a single source-of-truth when mapping your applications, you need to create a single list of applications to track your transformation priorities. This should be reviewed on a regular basis because priorities can change if, as mentioned, new information is gathered. Set a recurring review session in the calendar and remember that if no change in prioritisation has occurred, it’s not a waste of a review meeting – it just confirms that your current order is still correct. If priorities do change, it’s not anyone’s fault either. You should just accept that a change in priorities has occurred, that you’ve caught it early on and that you can therefore pivot. What are you trying to achieve? Without a priority order list, you’d be taking a gamble on which application to modernise first. What you actually want is a list of prioritised scores for each application, based on the attributes from the mapping work you’ve already done. This scorecard will help you to make an informed decision about which applications to work on first. The quadrant diagram above, paired with your mapping information, will allow you to rank applications into your prioritised list. You may need to do a few iterations of your priority list before deciding on an initial order because a priority for one application might affect another. Upcoming deadlines is one example of how applications may affect the priority of others, potentially around support contracts or licence renewals. Until you have done one iteration of all applications, you won’t be able to compare those deadlines against one another. However, this will be possible in a second iteration. This is why reviewing the priority list at regular intervals is important. You can check what progress is being made and, if slower than initially estimated, revise your priorities to account for changes. By doing this, you’ll ensure you are not left needing to renew a costly licence or contract for one application because the transformation of another took up all your time. Gather different viewpoints It is important to consider the people in your organisation that might use or be involved with the application. For example, the product manager and the service owner might see different parts of the picture within the organisation. Similarly, if different departments are involved in the support and running of an existing application, they might have different needs to factor in. Technical factors When considering the technical effort required to modernise an application, some questions worth asking are: How up-to-date is the application? If it is a bespoke application, you need to consider the versions of the programming language and application framework that it has been written in. How many versions behind the most recently released one is the application? What are the known security vulnerabilities of the version that your application is currently using? The difference between versions will steer the technical effort required, as upgrade paths are not always clear or simple. Plus, frameworks and other third-party dependencies might not just be drop-in replacements. If it is a Commercial Off The Shelf (COTS) application, how many versions behind the most recently released one are you? Is there an automated or clear upgrade path you can follow? Will an upgrade break any integrations that expect functionality or data to be available in a particular format or structure? How complex is the application? For bespoke applications where you have source code to maintain, is it written with hundreds of lines of code or hundreds of thousands of lines of code? Each line of code increases the complexity of maintenance and the burden on ensuring that functionality does not break. How well tested is the application? With the application complexity comes the question of what sort of automated tests are there for the code? If tests exist that can check if functionality works as expected, you can use those as a safety net to ensure it works as expected when upgrades need to happen. In this way, you can ensure other applications or users that interact with it won’t have any surprises. When was the application last released? If an application hasn’t been released in a while, there is the risk that instructions or even automated pipelines to deployment might be out-of-date or broken. This might be a hurdle that blocks you from putting a modernised application in front of users. Business/organisation factors When considering the business value of modernising an application, some questions worth asking are: How much are licences costing you? For COTS, how much do the licences cost for the application? Would it be worth saving the money for this license by replacing COTS with a bespoke application? Also, is there a renewal deadline? Would you like to save money by having the application replaced or decommissioned before that date? What support contracts exist? Depending on how COTS or bespoke applications are hosted, do you have to pay large support contracts that are squeezing budgets? If you want new functionality or to change existing functionality, do you have to pay large fees? Does the COTS feature roadmap align with organisation? Before making the decision to remain with a COTS application, does the future of that application match the predicted needs of your organisation? Might the company that develops the software take a radical new direction with the application? If so, have you considered that you might be stuck with the version you have or be forced onto a bespoke system later? Considering interdependencies It’s important to consider interdependencies between applications while you are prioritising. What you want to avoid is duplicated or wasted effort when modernising applications that affect others. If an application depends on another directly, or further down a chain of dependencies, do you need to consider which order you modernise those applications? If you modernise one of the applications, does that mean you’ll then need to alter the way that application communicates with another? You wouldn’t want to finish modernising one application and then start with others, to find these subsequent applications have changed in such a way that you need to revisit the first one to ensure it continues to communicate. Develop a few iterations of your priority list using the application mapping work you’ve done to identify interdependencies and work out the order for prioritising these dependency chains. After all, the destiny of one application’s modernisation may be tightly coupled to another. Choosing your top priority If we look back to our quadrant diagram: A workshop would be an ideal format to iterate through your list of applications a few times to work out your first priorities. The iterations of the list can be based on the factors we have discussed so far and listed below. After each iteration, the priority may change and should be reordered accordingly: Business value Technical effort Upcoming deadlines Interdependencies With the two axes in our quadrant diagram, technical effort and business value, you’ll need to ensure that you invite the correct people to give an opinion on each from their different perspectives. Some example prioritisation scenarios may be: A number of applications that have a number of dependency upgrades available that contain security fixes. So long as they pass automated tests highlighting that the upgrades will not break functionality—as this would increase the technical effort required—these could be updated rapidly with the benefit of increased security over a wide area of applications. These would be low technical effort but mid-high business value. An application that has an expensive support contract renewal looming but is not easy to change to meet new user needs. If a bespoke solution could be created to replace it, that would mean this application becomes a low business value item and could be decommissioned, so that the costly renewal costs can be avoided. A legacy bespoke application that is of high/critical business value, though also high technical effort to modernise. The risks of not being able to fix security vulnerabilities and make changes to meet user needs means this should be a top priority, as the risk to the organisation is too high if it were to fail. Applications that don’t pose a significant risk to an organisation due to low business value and low technical effort that can be dealt with at a later time once the higher risk items are handled. After the prioritisation workshop, you may have a clear priority list that allows you to decide on the best transformation strategy for the top application, before working your way through the other applications. If the top positions in your list are still hard to determine though, things may become clearer as you consider the transformation strategies for each application. Either way, let’s continue by looking at the various transformation strategies you could employ in the next section. This blog post has been co-authored by our CTO Luke Morton, Technical Consultant Ben Pirt, and Content Consultant Joe Carstairs. It is part of a series that covers modernising legacy applications . We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . If you are interested in learning more about this topic, our team has released our ‘Modernising Legacy Applications In The Public Sector’ e-book which will empower you to define and implement the right approach for your organisation to make legacy applications a thing of the past. Download a free copy here. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-06-10"},
{"website": "Made-Tech", "title": "Mapping your organisation and applications in legacy transformation", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/mapping-your-organisation-and-applications-in-legacy-transformation/", "abstract": "Mapping your organisation and applications is crucial because it enables you to decide which application to prioritise and which transformation strategy to adopt. The specific steps you should take, particularly in mapping your applications, are detailed in our book, ‘ Modernising legacy applications in the public sector ’. However, it’s worth exploring the key differences between mapping organisations and applications here, so that digital, data and technology leaders can get an idea of what’s required. The key difference between these two approaches is the level of detail you must go into. For your organisation, you can use broad brush strokes to sketch a landscape of how things work and interoperate. It’s also really important to do this organisational mapping first, so that your transformation is centred on user and organisational needs from the start. When it comes to mapping your applications, you need to be more analytical, data-driven and detailed, so you can develop a single-source of truth about all the applications in your organisation. Once you’ve completed both steps, you’ll have all the information you need to start modernising legacy applications. Mapping your organisation When you map your organisation, you do not need to identify and interconnect every service, system, touchpoint and individual within your organisation. Some lightweight mapping of what people’s daily routines are, how they use systems to complete tasks and why they do what they do is all that’s required. The most important thing to understand is how legacy applications affect the people that use them. This work will give you vital context when you dive deeper into specific applications, by allowing your team to see the real needs and frustrations of users that work with them everyday. Empathise with the user It’s important to take a design thinking approach to mapping by focusing in the first place on building empathy for users across your organisation. One of the best ways of doing this is by ‘walking the gemba’, which is a key part of the lean management philosophy. Walking the gemba involves immersing yourself in the places where the activity occurs in your organisation. For many organisations, it will mean engaging with everything from call centres to accounts departments to help desks, in order to observe, ask questions and record what happens. Ask the sort of questions that will uncover what really goes on. Try to keep your questions as open as possible and don’t be afraid to ask why something is done in a certain way. This approach might reveal something unexpected but vitally important about real user behaviour. Sketch the landscape You are just trying to get a broad understanding of how things work and interoperate, which you can use as a jumping off point for further technical analysis and a reference resource for your team. This information can then be organised relatively quickly in a half day mapping workshop with your whole team. If possible, you should aim to do this as part of setting up a ‘war room’ for your transformation project. This is where your team will work from and the information gathered during your organisational mapping can be laid out on the walls of the room to serve as a constant visual reminder for the team. The aim here is to review your research together and to gain a shared understanding as a team. The walls should serve as an opportunity to spark conversation and be kept up to date throughout your transformation, as the team learns more. Mapping the applications When you start mapping the applications in your organisation, you’re aiming for an up-to-date and comprehensive list of every application that can serve as a single source-of-truth for your ongoing transformation. Rather than just compiling a list of applications without much thought, you need to define a clear structure and outline the key attributes that you can compare and contrast in order to accurately account for user needs during prioritisation. You will start this journey with very little visibility. It’s an iterative process and, with each iteration, more of the landscape becomes visible. Along the way, you’ll find out about other people or teams that have knowledge to help direct you on your journey. What should be recorded? It’s essential to make a record of each application and its associated attributes, and also the relationships (or dependencies) that they have with one another. With attributes, there are two groupings to be considered; technical and organisational. In some situations, the technical attributes are easy to identify, such as the language and version that has been used to develop an application. Other examples of technical attributes might be whether it is off-the-shelf, SaaS or bespoke. Organisational attributes often need a little more investigation and require you to find the right people to talk to. It might involve interviewing teams to answer questions like which team is responsible for supporting this application? What are the boundaries and journeys that are used? Which applications interact with it? Every attribute will not necessarily be relevant for you, so pick and choose as you see fit. The full list of organisational and technical attributes we suggest are available in ‘Modernising legacy applications in the public sector’. Finding and recording information You will already have a high-level overview of the organisational landscape from the mapping you’ve done. The objective here is to investigate and dig deeper into the various applications. Invite teams or individuals to workshops if you believe they may have knowledge of specific applications. A good idea would be to run a whiteboard workshop where everyone can see the same mapping and point things out as a group. Also, if it’s possible to view the source code of applications, that’s always an option too. Roll up your sleeves and dive in. You should treat this whole exercise as a data-driven analysis and therefore record this information in a way that lets you easily extract, query and calculate various factors from it. This will allow you to score different applications and prioritise transformation. Ensure that when you record the information about an application, its attributes and its dependencies, these are stored individually, so they can be moved around on lists. The data should also be visible to all who need it. A central spreadsheet can work well or even a non-technical solution, such as a whiteboard or post-it notes with strings that show dependencies. Building a single source of truth As mentioned, this will be an iterative process and you may use multiple methods to collect this data. Act like an investigative journalist and keep seeking out answers by following up with questions to build an understanding of your application ecosystem that complements your organisational mapping. Remember that, with application mapping, you are aiming for a single source of truth, visible to everyone in your transformation team and anyone else in the wider organisation that needs to see it. It will be essential for the next stage of prioritisation but can also serve as a living record that helps to reduce technical debt and your reliance on legacy technology for years to come. This blog post has been co-authored by our Senior Technical Advisor David Winter, Technical Consultant Ben Pirt, and Content Consultant Joe Carstairs. It is part of a series that covers modernising legacy applications . Keep an eye on the blog for my next post discussing prioritising your application transformation . We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . If you are interested in learning more about this topic, our team has released our ‘Modernising Legacy Applications In The Public Sector’ e-book which will empower you to define and implement the right approach for your organisation to make legacy applications a thing of the past. Download a free copy here. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-05-27"},
{"website": "Made-Tech", "title": "Being an Introvert in an Agile working environment", "author": [" Emma Corbett"], "link": "https://www.madetech.com/blog/being-an-introvert-in-an-agile-working-environment/", "abstract": "Agile working environments often include open plan offices, close-knit teams and lots of impromptu discussions. Encouraging collaboration both between developers and with other business people is a great way to aid producing valuable software. But it’s important to remember that at the heart agile puts people over processes. Without an opportunity for some quiet time to recharge and focus, it can be extremely draining for introverts. I’m a big advocate of agile methodologies and really value the benefits of teamwork and collaboration however I’ve been in many meetings where I feel like I’ve not had the opportunity to contribute, and finished a lot of meeting-filled days feeling tired and unproductive. In this blog post I’m going to discuss some of the things within agile that I have found do and don’t work from an introvert’s perspective. What’s an introvert anyway? Carl Jung described introversion as an inward turning of energy and extroversion an outward turning of energy . More recently psychologists have speculated that instead of being turned inwards introverts are less driven by the prospect of reward than an extrovert . One common theme I noticed across all definitions is that it centred around energy – what energises you and what drains you? You may identify with some introverted personality traits and not others. It’s a spectrum and the majority of people are somewhere in the middle. In the words of Jung, “There is no such thing as a pure introvert or extrovert. Such a person would be in the lunatic asylum.” Some common traits that differentiate between an extrovert and introvert are: – Having large social networks vs valuing close one-on-one relationships – Thinking out loud vs thinking before speaking – Making quick decisions vs needing time to reflect and think – Thriving in team-oriented and open work setting vs preferring to work in quiet, independent environments – Enjoying being the center of attention vs finding it exhausting Neither is better than the other, they both have their advantages and your introversion/ extroversion inclination only makes up a small part of your personality. However, as Susan Cain argues , western culture can often seem to value extroverted qualities more than introverted. This encourages introverts to deplete their energy store quickly by trying to fit into an extroverted way of life. So how can you ensure that your team gets the most out of Agile ceremonies? It’s important to make sure no team member is too burnt out from meetings and ceremonies to be productive in their work. I’m going to discuss some ceremonies that agile teams commonly undertake (standups, planning and retrospective) and how they might feel from an introvert’s perspective, based on personal observations, discussions with other people and things I’ve previously read about. Agile is largely aimed at building more productive and effective teams that are individual-centric so it’s always important to consider what works best for your team. There’s no right way that works for every team. Stand Ups…. Standups can take a number of formats; for example, asking each team member the questions “What did you do yesterday?”, “What are you doing today?” and “Any blockers?”. This is great for introverts who don’t like to be put on the spot, it always follows a strict format which can be easily prepared for ahead of time, if needed. The key thing here is that there is a structured format, giving everyone a chance to prepare their contribution. So try and avoid stand ups that are just an open floor for having a chat, you might find that it is only a few members of the team that end up giving their updates. Another format I have found works well for the same reasons is “Walking the Wall”, which is where you go through all the stories or tasks which are “in progress” (i.e. out of the backlog but not yet done) and get an update from the owner of the story. Also try and avoid letting standups get hijacked into full blown discussions about things, you can always ask to “Take things offline”. Standups are great for surfacing problems/ blockers but then schedule a separate time to discuss them with the relevant people, giving everyone a chance to prepare for that separately. Planning…. Most planning sessions I have been a part of tend to consist of a story or task being presented and then an open discussion between the team about it followed by assigning points to the story. This unstructured format can be a challenge for some introverts. Generally, introverts like to have more time to think through ideas and problems to solidify thoughts before speaking out in front of a big group. If you haven’t been involved in preparing for planning, the discussion point will often be new information, rather than giving people this time. While introverts can take longer to process information they could come up with problems or insights around a task that others may not have considered and it would be a shame for your team to miss out on them. If the facilitator knows what will be discussed in advance and then sends a message to the team highlighting this, it gives introverts a chance to gather their thoughts and prepare questions beforehand. I personally always like to spend a bit of time preparing before a meeting, even if they are very casual and routine. I find I get the most out of them and can contribute more when I have this opportunity. Open discussions generally tend to lend to extroverts doing most of the talking. It is often said that introverts think before talking, whereas extroverts talk to think and some research claims that during a typical six person meeting, two people will do more than 60% of the talking. This can be self-perpetuating, if you are finding it hard to contribute you might feel disheartened and be put off. This can in turn lead to more extroverted people feeling like they need to talk more as other people are not, when in reality introverts may just need a bit more time to think things through before talking. Try to be aware if the conversation is being dominated by one or a couple of people and ask around the room for other opinions. Anyone can do this, not just the facilitator . You don’t always have to fill the silences, sometimes people are just thinking for a minute. Retrospectives… In my experience, retros have loads of good formats for introverts and extroverts alike. They often mix in quiet time to reflect and personally brainstorm and then some time for an open conversation to discuss the ideas people come up with. Mind writing is a great way to start off, this is where you take some time for everyone to come up with ideas or thoughts on post it notes (virtually or physically) and then share them. If there is a specific goal or theme of the retro it can also be helpful to share this around beforehand for people to think about. When it comes to an open discussion about some of these things, maybe give people a minute or so to think it through. As previously mentioned, extroverts tend to talk to think, so they might just go straight into the discussion leaving other people no opportunity to think. If you do find that only some people in your team seem to be contributing a lot in retros, you can try to have more structured discussions. Maybe go round the room and let everyone give their initial thoughts on a topic so that every team member has a chance to contribute if they want to. I have also found in the past that sometimes the conversation moves on before you have a chance to make your point and then it seems irrelevant. So as a facilitator I think it’s important to try and not to let the conversation move on without checking in with anyone else to see if they had anything they wanted to say. If some people haven’t had a chance to speak in a while and only a couple of people have been owning the conversation maybe cut them off and check in with everyone else. It’s always good to get feedback on how comfortable people felt contributing, especially with new formats. How often did you have a point to make but didn’t have an opportunity to say it? Everyone’s different – and that’s OK Everyone is different and we thrive in different situations, two introverts may respond totally differently to the same situation so it’s important to try and work out what works best for the individuals in your team. Be aware of your teammates and how they might be feeling, if someone isn’t contributing much or doesn’t seem engaged in a certain meeting maybe you could try mixing up the format then getting feedback from everyone to see if it works. It’s fine to be an introvert, you don’t need to change your personality or exhaust yourself trying to fit into an extroverted way of life. Just be open with your team if things aren’t working or if you want to mix things up. A retro can be a great opportunity to bring up if a particular part of your team’s process isn’t working for you . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-06-04"},
{"website": "Made-Tech", "title": "Legacy transformation is a team sport", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/legacy-transformation-is-a-team-sport/", "abstract": "When you consider the drivers of legacy transformation and the financial investment required for legacy transformation projects , it’s easy to see why going it alone is not an option. Legacy transformation is a team sport because it’s hard, expensive and can take a long time, so you need to have everyone ready to push in the same direction from the start. Organisations must involve digital teams, who can deliver new applications, technology teams, who are responsible for maintaining applications, and business or service teams, who are using applications on a daily basis. The needs and frustrations of all these groups must be considered if your legacy transformation strategy is to be successful. Building the right team from the start is also important so everyone understands how gradual changes will help to derisk the process. This can be hard when you don’t have a history of working in an agile, incremental way. However, by explaining your intentions early on, you will give yourself the best chance of achieving a worthwhile transformation. Digital vs Technology In many organisations, there is a divide between teams that build new user-centric applications and those who are responsible for maintaining the critical legacy systems that day-to-day services run off. In fact, one of the reasons legacy application issues occur is that these headline grabbing new services receive investment while the boring but critical core systems are left to gradually degrade. This is never a good strategy because new applications will invariably need to interact with core legacy systems in order to be useful. Failing to engage with the people who maintain these systems will therefore only cause issues down the line. Just as it’s important to involve those who understand the art of the possible, the people who can give you a true warts and all analysis of the ‘as is’ state must be heard. Your IT and application support staff will be the ones who are able to add the crucial feasibility angle to any desirable idea your developers want to jump in and build. One team, one dream Legacy transformation needs more than just digital and technology know-how though and every strategy must start from the same point – understanding and empathising with users. Empathy should be the driving force behind everything the team does. This will sound familiar to many, especially those who are responsible for developing new digital services with a user-first approach. What’s really important for legacy transformation is that this empathy is directed at all areas of the team, from the service or business users, to the IT maintainers and the digital developers. Each team member should understand and empathise with what drives and frustrates everyone else. It’s important to set out like this because service teams and IT maintenance teams are often overlooked in legacy application transformation. While digital teams might have an admirable urge to get on and replace legacy technology, not involving those who have been looking after these systems for 10 years or those who know how frustrating they are to use is a major mistake. Ignoring these insights is a particularly bad idea if an organisation chooses to follow a process of iterative legacy transformation. In this scenario, the organisation might find itself in a situation where one user group is moved onto the new application, while another is left interacting with a legacy system, waiting to be moved across at a later stage. To get this right, everyone in the organisation needs to be onboard. Who is in the perfect team? Building a multidisciplinary team to modernise legacy applications is important because you need to bring together people with a vision of the future and those that can actually make it happen. Service owner – responsible for all the touchpoints involved in a public sector organisation’s service, including digital and non-digital elements. Product owner – responsible for delivering a specific digital application as part of a public sector organisation’s service. Technical architect – responsible for defining the structure of systems and applications, and how data flows between them. Delivery manager – responsible for leading the agile and lean practices within a legacy transformation programme. Business analyst – responsible for understanding the business needs of a service and how these fit into an organisation’s strategy. Software engineer – responsible for building and delivering any new applications, as well as making changes to existing legacy ones. User researcher – responsible for understanding user needs and how the service might be designed to meet them. Service designer – responsible for designing all the touchpoints involved in a public sector organisation’s service, including digital and non digital elements. Interaction designer – responsible for researching and designing the content of the service, so it helps users to complete any tasks they need to. Support team – responsible for fixing an application when it breaks, so the service is always available for users. Other additions – every team is different and some other people you might include are content designers, subject matter experts and performance data analysts. As mentioned, an important driver behind building this sort of multidisciplinary team is empathising with a whole range of internal and external users from the start. Only by bringing all of these elements together at an early stage will you be able to develop a legacy transformation strategy that is desirable, feasible and viable. Don’t forget your senior stakeholders By building a multidisciplinary team that helps you to understand the ‘as is’ state and the art of possible, you are reducing the chance of unforeseen issues halting future progress. To reduce this risk further, you should also ensure your fellow senior stakeholders are on side too. Firstly, they need to be shown why such a wide range of employees must be involved in legacy transformation. Secondly, they need to be convinced why a derisked legacy transformation strategy based on small, incremental changes makes sense in the long run. Taking an agile approach to modernising applications involves failing fast but doing so in a way that can be rolled back quickly and easily. In this way, rapid progress can be made while derisking the sort of legacy technology disaster that others have experienced. The issue is that senior stakeholders are used to being presented with big bang transformations, where they have a deadline to aim for, after which (they are told) all their problems will be solved. Even though history tells us these are the legacy transformations that often fail and cost huge amounts of money to fix, you need to ensure senior stakeholders understand this early on. If not, your piecemeal strategy is in danger of getting to a stage where it looks like a mess of parallel changes being made to systems with different user groups and no end in sight. If you’re not careful, your sensible and derisked agile transformation could end up appearing like you are failing to plan ahead. Having head cover from a CEO, COO or Head of service who understands the bigger picture, and who can back you up when you’re in the midst of a complex transformation programme, might be the difference between failure and success. You should therefore be engaging and educating them on the risk reduction benefits of your approach as early as possible. Working together from the start Ensuring you have the right team in place to work on legacy application transformation will seem hard at first, especially if your organisation has traditionally been siloed into teams with specific, ring fenced responsibilities. However, it might also be the most important step you take in the whole project. It will allow different areas of the organisation to see how legacy transformation can benefit them and allow you to develop a strategy that ensures it does. It will also establish a collective momentum that can help you through the inevitable hiccups that occur along the way. Crucially though, it sets the tone for the way you will work. It establishes a foundation of empathy within the project and shows your organisation that previously overlooked areas will play a crucial role in legacy application transformation. This blog post has been co-authored by our CEO Rory MacDonald and Content Consultant Joe Carstairs. It is part of a series that covers modernising legacy applications . Keep an eye on the blog for my next post discussing the mapping of organisations and applications . We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . If you are interested in learning more about this topic, our team has released our ‘Modernising Legacy Applications In The Public Sector’ e-book which will empower you to define and implement the right approach for your organisation to make legacy applications a thing of the past. Download a free copy here. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-05-20"},
{"website": "Made-Tech", "title": "How financing affects legacy application transformation", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/how-financing-affects-legacy-application-transformation/", "abstract": "Cost is one of the most important drivers of legacy transformation and is usually the catalyst for a much wider discussion about how to finance such projects. From the start, it’s important to realise that legacy transformation requires investment, even if it results in significantly lower running costs over the long-term. Therefore you need to develop a strong business case to back up any changes you propose. Communicating the risks and identifying the drivers of legacy transformation are important steps for this. However, there’s no getting away from the fact that you’ll also need a busload of courage and commitment to convince senior colleagues that this crucial work must take place. This might require a new approach to funding that involves lots of areas being seed funded to kick start transformation, with the aim of following up on those that can prove further investment will be worthwhile. However, this is not a one-size-fits-all solution and you should always be pragmatic and flexible about how you build support within your organisation. Dealing with a double hit One of the first issues an organisation faces when developing a legacy transformation strategy is the parallel financial commitments that are required to begin with. This occurs because transformation doesn’t involve simply turning one legacy application off as you turn another modernised one on. There will be ongoing costs involved in running critical legacy applications in the short term while you create new, modernised applications. These could be significant if the cost of legacy technology was one of the main drivers of transformation in the first place. If an organisation has decided that legacy costs are unsustainable in the long-term and that modernisation is required, it will also need to accept that this work needs upfront investment and that it must be paid for on top of any legacy running costs. Public sector leaders should be aware of this double hit from the start and be ready to prepare other senior colleagues within their organisations. Unavoidable upfront costs Legacy application transformation will significantly reduce an organisation’s expenditure on data centres over the long-term. However, to get to this point, there is always a capital intensive period to begin with. Historically, funding data centres always involved a lump of capital expenditure upfront and this situation is also true of the start of a transformation project. In the case of data centre costs, the same capital expenditure will come around again and again as requirements change and hardware needs to be replaced. However, by transforming your legacy applications to be cloud-first, you can eliminate this future capital expenditure entirely and transform your financing model into “pay as you consume” expenditure. The result is a flexible architecture that continues to renew, without the need for physical infrastructure and the capital expenditure costs that come with it. Before that though, there’s still the not insignificant matter of convincing others that the upfront costs for transformation should be swallowed first. Convincing others to change Without exception, it takes time for organisations to move away from legacy technology. It requires senior leaders to accept the double financial hit of maintaining critical legacy systems whilst modernising applications to the point where old technology can be decommissioned. It also means they have to be patient to see the significant benefits that these changes bring about. Convincing senior leadership teams within public sector organisations that these programmes are worthwhile is one of the hardest parts of the transformation process. Heads of digital and technology who want to transform can take heart from the fact that tackling legacy technology is a priority for the Government, GDS and other public sector thought leaders. However, this top down direction is not necessarily enough to sway opinions within organisations. Leaders who want to bring about change must work hard to convince others. The risks and potential impacts of not tackling legacy technologies need to be highlighted at board-level. Cyber security and the significant cost savings over the long-term need to be referenced regularly as they are likely to catch the attention of someone with the authority to give you the green light. The ‘VC funding’ model As well as highlighting important risk factors, public sector leaders should also consider innovative funding models that might make big bills easier to swallow. One option is to fund a number of small transformation investments, in much the same way a Venture Capital (VC) firm would invest in a lot of different startups. The reason they do this is to see which ones gain traction and are worth following up with more money. Therefore your small transformation investments should be made with clear objectives and metrics attached, in order to assess whether any progress deserves further funding. This approach requires courage and usually relies on the assistance of a senior stakeholder who can see the long-term benefits and provide head cover for any short-term hiccups. When it pays off, this agile and incremental approach can have a truly transformative effect on major public sector organisations, as evidenced by the story of NHS Spine. The original version of Spine, which connects patients and clinical staff to prescription services, appointment bookings and care records, was built and maintained by BT. In 2014, the NHS decided it wanted to move away from a single vendor contract that was costing it over £100m per year. Instead of diving straight in, a proof of concept was developed to demonstrate the system could be replicated using open source technology. This was then used to build a business case for the Department of Health and the Government Digital Service (GDS) in order to open up further funding and development. By 2016, it had resulted in a new open source NHS Spine service that is run in-house. It has removed vendor lock in, allowed the team to work with small and medium-size service providers and, most significantly, saves the NHS £26 million per year and 750 working hours per day on an ongoing basis. The development of an open source proof of concept to replicate the existing BT-built service would have looked like a risky project when it began. There would have been concerns that it duplicated an existing system and that the vendor might cease to co-operate if it didn’t approve of the parallel work taking place. However, the significant cost reductions and increases in maintainability demonstrate that this courageous approach has paid off in a big way. Mixing courage and pragmatism Clearly, the issues around financing legacy transformation need to be tackled head on as early as possible. Failing to do so will either result in your transformation agenda never leaving the station or being derailed further down the tracks. This process isn’t simple though as some big issues need to be broached. There’s the double financial hit of maintaining legacy technology while also funding transformation. There’s also the fact that these early stages of transformation bring with them significant capital costs of their own. Leaders need courage and determination to push the transformation agenda forward but they can also employ innovative approaches to finance to help convince the hard to please. This can seem daunting at first but, when you consider the huge savings that can be made from transforming legacy applications, the rewards are clearly worth the effort. This blog post has been co-authored by our CEO Rory MacDonald and Content Consultant Joe Carstairs. It is part of a series that covers modernising legacy applications . Keep an eye on the blog for my next post looking into how legacy transformation is a team sport . If you are interested in learning more about this topic, our team has released our ‘Modernising Legacy Applications In The Public Sector’ e-book which will empower you to define and implement the right approach for your organisation to make legacy applications a thing of the past. Download a free copy here. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-04-30"},
{"website": "Made-Tech", "title": "A day in the remote life of a software engineer", "author": [" David Winter"], "link": "https://www.madetech.com/blog/a-day-in-the-remote-life-of-a-software-engineer/", "abstract": "The vast majority of people are now being forced into a remote working lifestyle that they may be unprepared for. It can be a big shock with the sudden drop in human contact that you are used to in an office setting. Especially with it being unclear as to how long this will go on for. I’ve been a mostly remote worker at Made Tech now for around two years and wanted to share some of my routine in the hope that some may find it helpful. Obviously, this is just my personal experience of what has worked for me. Your mileage may/will vary! The morning Wake up the same time you would if you were commuting into the office It may be tempting to work in your pyjamas… but you’ll feel much better for getting dressed as normal That includes putting your shoes on! Setting some clear work/life boundaries will be more important than ever If possible, work in a different room than you do for day-to-day living Commuting can (sometimes) be that mindful period where you’d normally read a book or listen to music, so why not start the day the same way Starting for the day It may be tempting to start work early, but try and stick to a daily routine Configure your working hours schedule in Google Calendar You want to still be avoiding burnout, which is an increased risk if you consistently work extra long days just because you’re not commuting There aren’t those water coolers or coffee machines to chat around, but be sure to still take those moments! Setup some remote-friendly non-work related chats Work is often intense and your mind needs mini-breaks during the day to subconsciously process ideas and thoughts Remote toolkit When it comes to remote tooling, Made Tech has come up with a bunch of favourite tools: Trello we use all the time planning but it’s also great for interactive meetings where you’d normally use post-its Google Hangouts for video conferencing, screen sharing or those remote water cooler chats Tuple for remote two-way pairing (for those who remember how great Screen Hero was!) Slack for all of your chat needs Spotify for collaborative remote office playlists Lunch Take your lunch break the same time each day Setup a recurring daily calendar event It’s a good opportunity to take the dog for a walk, or just yourself around the block for some fresh air Weather permitting, eat lunch outside and soak up the vitamin D if there’s any going Wrapping up for the day Finish work at the same time Use that regular commute time to pack away your work things for the day into a bag, or away in a drawer Cooking is a great way to unwind! Experiment! And use that time to unwind and shift from work to relaxing mode If you don’t feel like cooking tonight, as the restaurant sector will be one of the hardest hit by the pandemic, try to support local businesses by using delivery apps to order in Under normal circumstances, I’d try and go for a run or the gym at this point – though it’s not clear whether the gym is safe to do currently Stay strong, support each other Just like every organisation in the UK, we’re having to embrace remote working more than we ever have before during this pandemic. We feel our teams are remote-ready and remote-experienced but we’re monitoring the situation on a daily basis to ensure we can help them as much as possible. For our customers who are making the move to remote now, we are also offering any advice and support we can. One thing that hasn’t changed though is we’re still actively hiring. Interview routines have had to adapt, with video-calls replacing some stages where we would have invited candidates in for a face-to-face meeting. But we’re still looking for the best talent to join our high-growth company and have a number of open roles in London, Manchester and Bristol . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-03-18"},
{"website": "Made-Tech", "title": "Connecting loved ones: building the NHS Book a virtual visit service in 48 hours", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/connecting-loved-ones-building-the-nhs-book-a-virtual-visit-service-in-48-hours/", "abstract": "The world sure moves fast when it needs to! On Tuesday evening I received an introduction to Sonia Patel , Joint CIO of both London North West University Healthcare NHS Trust & The Hillingdon Hospitals NHS Foundation Trust, and future CIO of NHS nationally. Sonia had put out a tweet that evening requesting help from SME tech companies to build an app in 24-48 hours. The challenge to SMEs we need an alpha version in the next 24-48 hours = time is precious. — Sonia Patel (@chat2sonia) April 14, 2020 A reply from Sonia woke me at 05:30 on Wednesday morning and we spoke shortly after that. By 09:00 we had a full team allocated to a 48 hour hackathon to build a digital service that enables hospital staff to book and facility virtual visits from loved ones to patients via video call. Connecting loved ones with virtual visitations. Thanks @MadeTech for standing up an alpha app within 48 hours. Really looking forward to rolling out public beta across @LNWH_NHS and @HillingdonNHSFT next week. @LondonCIO @NHSX maybe interested in a secure and OS agnostic offer. https://t.co/8oVwWQHFrm — Sonia Patel (@chat2sonia) April 17, 2020 By Friday morning we had a working but certainly not polished digital service. Our team of seven, including delivery and technology staff as well as service design help from our partner Difrent had worked into the night to get this turned around so fast. The desired outcome of the service is to facilitate the booking and organising of video calls between patients and loved ones so that they may feel connected in a time where they are unable to be physically. This is especially relevant now with social distancing measures but we hope that this service can continue to provide value in the future too with personal device restrictions on wards. The simple digital service enables ward staff to book in calls which we hope will reduce the burden on staff having to do this manually and reduce the number of calls to wards from loved ones. From the loved ones perspective they receive an SMS stating when their virtual visit is booked for and receive a unique link to join the call when the ward is ready. The way we’ve built the service effectively means it can back onto any video service. For now we’ve integrated both with open source video call software Jitsi and also with a SaaS product called Whereby. In the future we could facilitate Zoom, Google Meet and Duo, along with others. We are releasing the service as open source and are looking into longer term plans on how the service can be run and maintained. Get in touch if you have any ideas! We are currently testing the service on wards in Sonia’s hospitals. We hope to begin scaling out the solution to more wards as soon as possible. We’ve also been contacted by many other trusts and we will be working with them as well as NHSX and NHS Digital to ensure this service can have as wide and as positive impact as possible. We’d like to thank Sonia and especially her staff who are enabling us to move so fast and deploy this much needed service. We love what we do and try to help the public sector build and improve software. If you are interested in other projects we have worked on, please see our GovWifi Case Study or our Hackney Council API Case Study . If you’d like to find out more about the service please contact us at nhs-virtual-visit-enquiries@madetech.com . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-04-23"},
{"website": "Made-Tech", "title": "Which factors drive legacy transformation in public sector organisations?", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/which-factors-drive-legacy-transformation-in-public-sector-organisations/", "abstract": "The risks of neglecting legacy transformation include everything from reputational damage to loss of life. However, trying to predict exactly what will happen to a particular public sector organisation if it ignores these risks is extremely difficult. A more pragmatic approach is to start by identifying the drivers of legacy transformation and deciding which are the most pressing for you. Changes to legacy systems across the public sector are almost inevitable, given the significance that bodies like the Government Digital Service (GDS) have placed on change. Therefore digital and technology leaders need to act now to identify these drivers as a critical first step towards transformation. One thing that should be front of mind from the beginning is cost. It is an outcome of almost all the other factors driving change and is an undeniably important consideration throughout any transformation process. Cyber security risks The Government regards certain areas of the public sector as critical national infrastructure, with protection of the IT networks, data and systems in these areas falling under the remit of the National Cyber Security Centre. Hackers are attracted to nearly all levels of government infrastructure as they regard them as treasure troves of sensitive information. Conscious of the major reputational damage that would be caused by a breach, digital and technology leaders have adopted the mantra of “reducing risk” wherever and whenever possible. As a result, these systems have become more and more locked down and more and more difficult to work with. There is also the issue of how cyber security audits take place. Over time, they have been systemised into a rubber stamping process rather than a practical, collaborative exercise that helps engineers to embed security into their processes from the start. Rather than developing software, have it audited six months later and potentially roll back with fixes, engineers need to be helped to ‘shift left with security’ and build secure software from the start. Maintainability issues One of the consequences of locking away legacy systems is that they become harder to maintain because the skills, documentation and knowledge required to keep them up-to-date disappear. But maintainability is a driver of transformation for legacy applications that aren’t considered critical too. The programming languages your apps are written in can become out-of-date simply through a lack of ongoing upkeep, to the point where it’s not unusual to see language stacks in public sector organisations that haven’t been updated for ten years. As this happens, more and more security issues occur that can’t be patched easily because the language or operating system is no longer supported. Where out-of-license software is still being used, organisations need to make custom (read ‘expensive’) arrangements with software providers to patch security issues or make changes. These ‘ransom contracts’, where public sector organisations are locked-in to paying whatever a single vendor demands, are still worryingly common. Furthermore, their existence is a clear warning sign that legacy technology is an issue within an organisation. Complacency can quickly become an issue for leaders who see maintainability decreasing but do nothing about it. If you don’t transform, you will start to see a snowball effect, with issues occurring across multiple layers, from network infrastructure, across operating systems and languages. Technical debt needs to be paid off regularly before it gets out of hand. Regular updates, at least every three months, are needed if an organisation wants to avoid getting into a situation where they have no choice but to sign expensive ransom contracts or hire engineers with niche, legacy skills to make changes. Meeting user needs One of the most important drivers of legacy transformation is government policy being changed and technology not being able to keep up. This results in organisations experiencing a tension between the desire to serve users and the inability of existing systems to meet these demands. New or evolved policy usually necessitates a change to existing systems. However, the desired change to a service journey can be blocked immediately if the underlying technology that powers it has been outsourced to a third party or left untouched for so long that no-one knows how to make changes. Forward thinking public sector leaders want to deliver services that citizens can benefit from but they encounter off-the-shelf systems that were designed for what was needed ten years ago. Unfortunately, it is common for departments to give up on making the changes they know are needed and instead settle for carrying on with the clunky, outdated processes just because it seems easier to kick the issue down the road. For those leaders who cannot compromise, the need to deliver new user-oriented services at all costs often comes with its own issues. They will go ahead with delivering a new digital service that works around the legacy system but this doesn’t solve the problem if the legacy system they’ve worked around still holds critical data. The result is an ever growing list of greenfield projects and legacy applications, which become more and more difficult to manage because modernisation has done nothing to transform boring but critical internal systems. Skills shortages Transforming legacy technology isn’t just about systems, languages and software. A lack of relevant skills can also be a big driver of change for public sector organisations. One side of this is upskilling technology teams in modern capabilities, so they are able to execute a transition to a cloud strategy and follow modern ways of working on an ongoing basis. However, it is also important to recognise the urgent skills gap that many digital and technology leaders face is a shortage of people who can work with legacy technology. Even when they look outside their organisations for assistance, they may find that such skills are dying out in the wider engineering community, leaving them with little option but to work with a legacy supplier who has managed to retain a niche skill set. A well known example of this, which is also an issue for banks and other established institutions too, is finding developers with COBOL skills, a language developed in 1959 that is still driving various core mainframe systems. If an organisation is having to rely on a handful of engineers to make changes to a core system, especially if they are contracted to a single supplier, this is a sure sign that urgent transformation is required. Leadership changes Another non-technical driver of change is the urgency many thought leading public sector organisations are placing on transforming legacy technology. GDS has identified legacy IT as one of its five pillars of focus for the next decade, while the House of Commons Science and Technology Select Committee has urged GDS to conduct an audit of all government legacy systems by the end of 2020. GDS has a clear message for all departments that the government should be cloud-first and plenty of people from within GDS have taken leadership roles at various departments over the last few years. Leadership on legacy transformation strategies isn’t just coming from public sector people either. Individuals who have joined central and local government departments, agencies and public bodies from outside the sector have brought fresh ideas about building rather than buying software. In these situations, the most important driver of legacy transformation might simply be a new person with new ideas and fresh ways of working being given the reins to drive change forward. Streamlining costs Financing is such a major part of legacy transformation that it needs to be tackled separately. However, the way technology costs have changed so drastically in recent years should definitely be thought of as an important driver of change. Cloud technologies have driven down the cost of running apps, both in terms of licensing costs and a reduction in people costs that comes from the ‘shared responsibility model’. When organisations switch to this model, there’s no longer a need to have staff who can physically kit out a data centre or, in the case of Platform-as-a-Service (PaaS), run an operating system. On the licensing side, open source databases such as PostgreSQL have completely eliminated the need to pay for Oracle or IBM Db2. Key components of an organisation’s infrastructure that used to be paid for upfront, like web servers and databases, can be consumed on a ‘pay as you consume’ basis. Therefore leaders of public sector organisations that still see money being allocated to these areas need to ask themselves why. Modern, virtual database technology is more cost effective and generates lower overheads than legacy, on-premise data centres could ever allow. This change has been a key driver of disruption in many industries, as startups have attacked incumbents with better user experiences powered by lower technology costs. Now is the time for it to drive down the costs of legacy technology across public sector organisations. These savings become particularly apparent when data centre costs need to be paid, including the replacement of on-premise hardware that has started to die. All of these recurring capital expenditure events should start to ring alarm bells for public sector organisations that a proper legacy transformation strategy is urgently required. Transformation in your organisation Not every public sector organisation will be experiencing all of these factors at once or to the same degree. Identifying which ones are the most pressing is something all digital and technology leaders can do to assess how risky any legacy applications might be. From there, it’s a matter of money and finding a way to convince board-level executives that long-term transformation programmes are the only way to ensure your organisation doesn’t become the latest victim of a legacy technology disaster. This blog post has been co-authored by our CEO Rory MacDonald and Content Consultant Joe Carstairs. It is part of a series that covers modernising legacy applications . In my next post, I’ll be looking at how financing affects legacy application transformation . If you are interested in learning more about this topic, our team has released our ‘Modernising Legacy Applications In The Public Sector’ e-book which will empower you to define and implement the right approach for your organisation to make legacy applications a thing of the past. Download a free copy here. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-04-16"},
{"website": "Made-Tech", "title": "Documentation should meet outcomes too", "author": [" Duncan Bell"], "link": "https://www.madetech.com/blog/documentation-should-meet-outcomes-too/", "abstract": "As a DevOps specialist, I often find myself defending the value of documentation. It can often be a delicate topic in development teams, and individuals from different technical backgrounds and experiences can have fairly extreme opinions about it. In this blog post I will take you through some history, the benefits of transparency and collaboration, how documentation must be easy to read, update and maintain, and debate whether it is actually required at all. Along with some guidelines for reducing the effort required, sizing, the importance of including manual steps, and how to keep the team motivated to maintain the documentation. Documentation is necessary to explain how something complicated works, and also defines a critical process to ensure a successful software release. We use outcomes, derived from discovery/research, stakeholder input and other business needs to define how and why we write code. I believe that our approach to writing documentation should learn from our approach to writing code. Traditionally writing documentation was seen as a burden, listed at the end of a ticket’s acceptance criteria. It can often be seen by developers as uninteresting or not as important as writing the “actual code”.  I have even heard it referred to as “not real work” by some programmers. How do we go beyond this and ensure that documentation is maintainable, reliable and valuable? A Dark History Documentation obviously has a dark history in Software Development, appearing in the first few lines of the Agile Manifesto introduction, “ Working software over comprehensive documentation.” Something in the rigid, linear, Waterfall development practices of the past have tainted this important part of development as the thing “ that held up the delivery of software ”.  I have visions of software delivery being delayed by months by businesses demanding comprehensive documentation before shipping. Traditionally the authoring of documentation was the final step before code can be released, but in modern agile practice, the inevitable delay to the release cycle isn’t tolerated. Ironically everyone who has used a computer to write code has relied on other people’s documentation. The very first Unix Programmer’s Manual man page was written in 1971, and where would a lot of us be without this fairly rigid and unchanging process being adhered to over the last fifty years of computing, to enable us to use this complex toolset that our modern applications still rely on? Transparent and Collaborative Clearly everything in life cannot be tarred with the man page brush, and there are many different types of documentation required to suit each scenario. In every business, documentation is required for almost every aspect of the day to day running. At Made Tech our Company Handbook is on Github, version controlled and Open Source.  This was something that drew me to apply for a role at Made Tech, and I have heard similar positive feedback about this from many of my colleagues. We have a business outcome to be open and transparent with our ways of working, job description and salary banding, and this Handbook fulfils that. Documentation should meet user requirements User-centred design should not just be focussed on delivering software aimed at users needs, but also ensuring documentation is aimed at the right level for the end-user or customer. Often the poor people left running the application long after the development team have moved on to something else are an afterthought.  Although, hopefully, with evolving DevOps practices, the team writing the application are also deploying and responsible for running it. Therefore identifying your user, or group of users that your different documentation streams are for is paramount.  Your user is likely to be yourself in a few months when you have completely forgotten everything about your own code.  Be kind to your future self. My code is the docs In extreme programming circles, some would argue that the code should be self-documenting.  This is a valid argument and at Made Tech we encourage the practice of TDD and Pair Programming, which both fall under the XP (Extreme Programming) label. However, Martin Fowler states that code not needing documentation is a common misconception of extreme programming and, “ Usually there is a need for further documentation to act as a supplement to the code.” Writing clear, readable, well-structured code, with well-chosen Class, Method, Function, Variable names, and with descriptively named Unit Tests all help describe the code without the need for extensive code comments. Many languages offer self-generated documentation from the code and features such as the Golang Test Framework Examples , which enables you to provide self-documenting implementation within your Unit Tests that can be published to the godoc website along with your package. Ron Jeffries , one of the founding fathers of extreme programming, also argues that within a team the code should be self-documenting, and the team close and communicative enough that documentation is not required. However, “When it is time to hand off the software, whether to maintenance people or to users (perhaps programmers who will use the software to build other things, or other kinds of end users), then of course you need to build appropriate documentation to go with it. In the case of users, I’d think this would be just like any other kind of user documentation. In the case of maintenance programmers, we typically suggest a short document describing the system’s architecture or metaphor (perhaps with some UML diagrams), the key concepts, the important classes, and so on. This should, we suggest, be thought of as an introduction to the system, a high-level index if you will.” Perhaps our outcomes for internal technical documentation should be to try and rely on the quality of the code.  Documentation as code will meet some user needs, but not all, and this needs to be reviewed each time a piece of work is refined.  Documentation should be treated as any other requirement and prioritised within the scope of the whole task. Who are your users? Establishing who our users are is paramount to ensuring that our documentation outcomes are met.  At Made Tech we work heavily with Public Sector Government and Local Authority Departments.  The GDS (Government Digital Service) Service Standards , are a set of guidelines around how to build digital services for the Government, built heavily around “ User Needs ”.  This includes a set of Documentation Guidelines which help standardise documentation to end-users. Documentation should be useful, valuable and have an outcome to ensure it fits our end-user needs.  Nothing should be written just for the sake of it. Learn from the community Most complex community projects such as Kubernetes have contribution guidelines with clearly defined rules of engagement.  It is clear that the users here can also be the maintainers themselves, and this self-managing cycle of information is version controlled in Github, peer-reviewed with pull requests with some fairly advanced automated pull request checks, allowing confidence to merge into master and deployed with minimal human interaction. Using Github Markdown, the Hugo static website generator and deploying seamlessly with the click of a button via pull request checks and approvals via the CI/CD hosting platform Netlify.  This process is impressive, and a lot of commercial products could take a page out of the Open Source community’s book here.  I have seen similarly smooth processes done with Github Actions/Pages, S3 buckets and other tools. If a documentation outcome could be to make it quick and easy to contribute, follow best practice in a version-controlled way, update and deploy a new copy, then this would be an excellent example. Eliminating toil Documentation can help eliminate toil.  Toil is defined in Google’s SRE Handbook as Manual, Repetitive, Automatable, Tactical (reactive), leaves no enduring value after the work is complete, and scales linearly with traffic/load/users etc, then it is something worth investing in eliminating. Handing over work with multiple manual steps in order to get something shipped is fine for an MVP or as a stepping stone towards full automation.  No company I have ever worked with has prioritised full automated blue/green, feature flagged deployments, over having a viable product they can sell. Clearly documenting the manual steps can feed back into your backlog of continuous improvement, and ensure all members of the team are aware of the more complex manual steps that may have manifested. We can’t always go from zero to full CI/CD hero in the first iteration, but ensuring we set an outcome that we attempt to script and automate toil where we can, and document anything left over in a quick README to help our future selves. Striving for Operational Excellence Runbooks , Playbooks and Gamedays all feature heavily in the AWS Well-Architected Framework’s “Operational Excellence” pillar as being crucial documents, and feedback loops within a successful organisation.  Those operating and managing the systems and specifically individuals “on-call”, will certainly see value in this area. As DevOps engineers, we are increasingly responsible for building and maintaining our own code and systems.  Being proactive in documenting manual processes in Runbooks allows us to improve and automate these processes, whilst Playbooks enable us to respond to failure swiftly.  I like to hope that the newest member of a team should be able to deal with any kind of failure or outage to a certain extent without having to escalate to a more senior engineer. Running regular Gamedays to simulate or preempt these events provides valuable insight and feedback into your Runbook and Playbook content through the process of a Root Cause Analysis (RCA) with the aim to stop the problem from happening again, whether it be through a code, process or cultural change by the team. Minimum Viable Documentation Documentation is essential for our end users to use or manage the software we deliver.  Approaching the documentation process in the same Lean way as a Minimum Viable Product will reduce the size and complexity from the task. Looking at process documentation outcomes, in the same way, breaks that cycle of it being a chore we do at the end of the project.  Breaking the task down into smaller iterations with a fast feedback loop allows us to get down just enough information to be valuable, small enough to review and iterate on with minimal resources. How much or little documentation can we get away with, how comprehensive does it need to be, and is it required at all?  All of these choices will depend on the type of task or industry.  Medical or aeronautical documentation where risk is high requires a high level of detail, audit and review.  Whereas, other industries with more room for failure and iterative improvement can probably get away with a more minimal approach.  This should be reviewed and tweaked for a balance between time taken to write, review and maintain, and the value it provides.  A one size fits all approach certainly doesn’t apply here, and the outcome of the documentation will vary depending on the requirements. Feedback Loop One of the outcomes of technical documentation is that it should be a pleasure to read.  Achieving this only comes from peer review and a feedback loop from your end-users.  Pair programming is great for this but using a public version control system for your documentation if customer-facing allows direct feedback and suggestions from your actual end-users. This kind of iterative feedback is invaluable.  The development team demonstrating transparency and honesty by Open Sourcing your docs is always valued by your customers and end-users.  Dogfooding your own documentation and manual steps after a period of time can be a good indication if they hold value for others. Documentation standards Why have an established standard?  A hybrid blend of documentation locations like Confluence, Google Drive, Dropbox, Office 365, with a few repo level README.md’s thrown in is a recipe for disaster.  We have all worked in places with multiple silos of documentation, or multiple copies of the same files in cloud storage, with names/timestamps/versions in the file name.  When different teams and departments use different tooling, quickly things spiral out of control and soon enough you don’t know where to look for information, or where to put information. Internally we have clearly defined protocols and tooling for certain tasks.  However, at the start of customer engagements Made Tech will run a ways of working session, to align on many elements of the project and how we work and collaborate together. The outcomes from this session will establish, amongst other things, tooling choices, where documents are located, an agreed format and review period. The ultimate outcome is to have a single point of truth for your documentation. Ensuring your outcomes have been met Everything in software development needs a definition of done; however, in ongoing operational, process-driven departments, reviewing and updating documentation is a necessary workstream. Documentation should only exist if valuable, well-written code with tests can often document itself at a technical level.  Documentation should be user-driven with clearly defined outcomes before any work begins on it.  In larger organisations and projects, some guidelines and frameworks can be useful to standardise and simplify a complicated but necessary process. Documentation should be version controlled, peer-reviewed, easy to find and updated frequently, or removed if no longer relevant.  Well written documentation can be a company asset, but done badly it can be “the thing” everyone hates and moans about. Make sure you establish clear outcomes for your documentation before you start writing it, to ensure that it is not just acceptable, but valuable to its target audience. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-07-29"},
{"website": "Made-Tech", "title": "Launching the Made Tech Talks webinar series", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/launching-the-made-tech-talks-webinar-series/", "abstract": "We are excited to announce the launch of our new webinar series, Made Tech Talks. During this series, we aim to provide helpful, accessible information to empower digital leaders to improve software delivery in the public sector. You will hear from members of our team, from our Lead and Senior Engineers to our Head of Product, as they share our experience working with clients across the UK public sector. We will talk about the challenges, success stories and lessons learnt, as well as share some of the best practices, ways of working and actionable insights you can adopt to deliver technology at pace. Digital service delivery, modernising legacy applications, building high performance agile teams, and government as a platform are just a few examples of the topics we will cover during these sessions.  To ensure everyone can access the information in our webinars, each session will include live verbatim subtitles. How to successfully spin up digital projects remotely How can you run successful workshops remotely? Lead Engineer Clare Sudbery and Head of Product Andreas England will be kicking off our Made Tech Talks webinar series by sharing their experience of starting up while locking down. How to spin up a digital project during lockdown: challenges, tips and tools Date: Tuesday, 14th July 2020 Time: 1pm – 2pm BST Register Here Made Tech are currently partnering with Difrent and Skills for Care to deliver a service that collects data from (and delivers value to) adult social care providers. We planned to spend a fortnight together to kick off our engagement, then lockdown happened and we were forced to go fully remote with only a few days’ notice. Clare and Andreas will share the highs and lows of bringing together 10 people who have never worked together before, all working from home and in the midst of a global pandemic. Key takeaways from this webinar: Understand the biggest challenges and how to mitigate them. Discover tools to help people interact remotely. Appreciate the importance of looking after people. Learn how to lay the groundwork for successful collaboration. What do you want in a webinar? As this series evolves, we would love to hear from you what topics you would find most useful to be covered in future webinars. Please let us know your preferences by submitting this form . You can also stay up to date with our upcoming webinars by subscribing to our events list or following us on Twitter or Linkedin . We hope to see you at one of our Made Tech Talks soon! Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-07-10"},
{"website": "Made-Tech", "title": "Modernising legacy applications in the public sector – new book launch!", "author": [" Luke Morton"], "link": "https://www.madetech.com/blog/modernising-legacy-applications-in-the-public-sector-new-book-launch/", "abstract": "I’m really pleased to announce the launch of our latest book, Modernising legacy applications in the public sector. Over the last few months, I’ve been sharing some of the content in this blog and now it is time to release the new book for download . We have written the book to share our experience of successful legacy application transformations with technology leaders working across the UK public sector. Legacy transformation is one of the biggest threats to the organisations these leaders work in. So much so that, in 2019, the House of Commons Select Committee for Science and Technology directed the Government Digital Service (GDS) to conduct an audit of all legacy systems across government before the end of 2020. Our latest book is a field guide for any technology leaders grappling with the risks of legacy technology but it also helps you to lay the foundations of sustainable technology in your organisation. From legacy applications to sustainable technology When you begin your transformation journey, you know your legacy applications pose a significant risk to your organisation. One of the most important lessons you learn as you modernise these applications is that you can never let that situation occur again. This is what it means to truly and definitely move away from legacy applications. It is not just about modernising your technology estate. It’s about transforming your entire operating model into one that is tuned to deliver sustainable technology. This does not happen overnight but, as the book explains, can be achieved by prioritising a legacy application for transformation and learning important lessons as you progress. In writing this book, we have drawn on our experience of helping a range of UK public sector organisations to transform legacy applications and increase technology skills . For example, our ongoing work with Hackney Council to modernise housing needs services has already included building an API platform and transforming the Council’s data infrastructure . The use of APIs and the ways you can deal with complex data are both covered in the book as key elements of modernising legacy applications. Recently, we also helped the Ministry of Housing, Communities and Local Government (MHCLG) to transform the EPC register into a modern digital service . This involved taking an application that was inflexible and expensive to maintain because it had been run by an outsourced supplier and turning it into a cloud-based service that could be run in-house along modern government standards. Every transformation is different but there are clear themes that run through all of them, including the planning you do, how this affects the decisions you make and what processes you should follow to ensure your project is a success. These themes, steps and processes are what we cover in the book. Helping technology leaders in the UK public sector The book has been written for Digital, Data and Technology leaders, including Chief Digital Officers, Chief Technology Officers, Chief Information Officers and Directors of IT. Whether you’ve started to think about transforming legacy applications already or you don’t know where to begin, the book is for forward thinking leaders who want to deliver sustainable technology by making legacy applications a thing of the past. You may have already identified a legacy technology risk in your organisation and be working on solving it. You may have had to design your processes around your legacy systems to the detriment of your staff and the citizens you serve. You may simply be aware that an application has been left untouched for so long that it is becoming riskier by the day. Most technology leaders in public sector organisations are aware of at least one area of legacy technology that poses a risk. However the reasons for it not being addressed are never the same for each organisation. It might be that funding hasn’t been directed towards modernising legacy applications. Or that an application has been left untouched for so long that no-one dares make a change in case it breaks a critical service for citizens. How to modernise legacy applications Whichever situation you find yourself in, the book has been written to guide you through the steps you should take to modernise your legacy applications, empowering you to define and implement the right approach for your organisation. For those of you at the very start of your journey, this means helping you to understand the risks your legacy applications pose and the drivers for transformation in your organisation. From here, the book describes the initial steps to ensure your transformation is a success, including accessing finance and building the right team for the job, so that the full range of internal and external users are considered and designed for. Then, the book helps you to prioritise your transformation by explaining the organisational and application mapping that needs to take place in order to choose which application should be prioritised. Once you’ve researched the problem, you need to know what can be done to solve it, which is why a large portion of the book is dedicated to explaining the different strategies for modernising legacy applications and how to implement them. Modernising legacy applications isn’t just about picking the right strategy to transform your existing estate though. It’s about changing the way you operate and empowering your team to embrace and adapt to change. That’s why the final part of the book looks to the future and how you can maintain the positive changes you’ve made in order to continue building sustainable technology from now on. Your first steps to transformation The book outlines the best practice approaches we have used in a range of public sector organisations. So, if you know legacy application transformation is an issue you want to tackle, download the book and start to make progress on that journey. When you do, let us know what you think. The book has been launched following a lean publishing process and we’re always keen to improve it. So, if you have some feedback to share, get in touch, as we’d love to chat about it over coffee. Using feedback to iterate and improve is an important part of delivering sustainable technology too. To help you achieve this change, you should surround yourself with like-minded organisations, suppliers and experts who can keep you on track and support the ongoing transformation of your entire operating model. If you do, you will not only be enabling your team to embrace and adapt to change but also to deliver sustainable technology long into the future. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-08-13"},
{"website": "Made-Tech", "title": "Why data-driven organisations: The problem", "author": [" John Nicholas"], "link": "https://www.madetech.com/blog/why-data-driven-organisations-the-problem/", "abstract": "An overview and brief introduction This is the first in a series of posts that get to the heart of what a data-driven organisation is for a given organisation. The problem for an author is that each organisation will have its own goals and needs. So a one size fits all notion of a data-driven organisation is either going to be far too vague to be applicable (i.e. measure all the things, then apply mathematical reasoning to come up with decision making!) or simply not relevant. To get around this, we are going to focus in on an area that is vital, common to all organisations and will bootstrap the organisation into getting itself into a position where being ‘data-driven’ is just the de facto mode of being. It will provide tools, methods and examples that can be applied elsewhere. This post is about why we should care about data-driven organisations, the specific problem that drives our inquiry into this area and forms the basis of the direction in how this large topic can be broken down. Before we can define a solution to a problem, we need to understand the problem it is attempting to solve. Every organisation needs to make decisions – if you are reading this, hopefully you want to base these decisions on measuring the external world – IE data. This is the broadest and least useful understanding of a data-driven organisation. Technique Decisions can be made in many ways – after all, it is better to be lucky and technically wrong than unlucky and technically correct! However, in any field, bad technique will eventually limit your ability to progress. In organisations the technique we care about is decision-making ability, Superficially it appears that an organisation that makes decisions at all levels based upon data is a data-driven organisation. What we are going to go into here are a set of key problems that need to be addressed as part of the core of developing such a strategy. The nub is that if companies knew what and how to measure, they would already be data-driven organisations. So a definition based around everyone measuring, analysing and having access to data to increase competitive advantage demonstrates a dangerous misunderstanding of the problem that is being addressed. If they follow this definition and start to measure things and analyse the data they are going to invest in a lot of extra work with very little understanding of how this will generate any return. It will be a return of the same practises that led to asking ‘how can we make better decisions’ in the first place. The Problem (or the Why) We know from looking at the history of whatever industry you are in that we are not very good at successful projects. What makes you think you are different? Do you have a way to answer that with confidence? How are you moving forward? Dunning-Kruger A man once robbed a bank without a mask, was caught and prosecuted. He proclaimed ‘But I wore the Juice’. Somehow he thought that if he wore lemon juice (because it is used in invisible ink) the camera wouldn’t be able to see him. This is quite a spectacular example of a set of assumptions that could be good in a different world, if only the individual had tested them. As a result, there is a paper on this cognitive bias by Dunning and Kruger called ‘Unskilled and Unaware of It: How Difficulties in Recognizing One’s Own Incompetence Lead to Inflated Self-Assessments’. This is especially interesting as it is an example of a local optima. The problem with moving from one peak to another is that it is necessary to descend into a valley of chaos. Why Should You Care? How do you know that you are not in the midst of a Dunning-Kruger scenario? Where does this confidence (or lack thereof) come from? Question Organisation-wise, we can restate the above questions: What happens if we collectively have found ourselves in a reinforcing system that keeps us all trapped in this state with regards to organisational skill and awareness? How can you tell? How Does Your Organisation Learn? The hypothesis is that we are trapped and therefore, are not learning because every industry has projects running late and over-budget – yet every industry has examples of organisations that buck this trend. Even when they are finished, things have been cut (and added). This is why we tragedies like the Grenfell Tower Disaster occur and we still have the same cladding on buildings a year later. Similarly, we get web projects that are not only a year late, but cost more than the system being replaced and provide fewer features. My challenge to the reader is to prove the hypothesis wrong; what follows in these posts are a set of ideas and strategies to get into the heart of the problem. That we are bad at software projects was true 20 years ago and is still true today – this suggests that we are bad at this and we have not gotten better. We are stuck in a Dunning Kruger scenario – despite some interest the status quo has not moved. But it also suggests that there is a lot of space from innovation here. Dunning-Kruger is where an entity is unable to evaluate its ideas and therefore unable to move in a directed fashion – because its parts move independently and in conflicting directions . The Danger of Fads There have been many approaches to this over the years, however, many of these approaches are based upon received wisdom of management consultants – usually based on fads. A great example of fad-based advice that is easier to relate to is dietary advice. With dietary advice, results of analysis have so much noise and variance it is very hard to know what is true and what is not. The result is that it is next to impossible to evaluate all the different and contradictory recommendations in an objective way – so most stay safe and restrict calories and take exercise. The problem with dietary advice is that some will be correct and some will be wrong (Assuming that some are true!). We have no way of being able to tell and so we find a compromise – i.e. we reject both the correct and false to take a new position that must be less than optimal – it is not correct and has some false in it. As a result, it is more than likely that very few people have that ‘healthy’ diet we all strive for – but what we have is okay. Yet according to NHS Statistics , not only is obesity going up, admission due to it is going up but 97% of adults are active. Other stats show overweight as being fairly constant – yet obesity is increasing. Maybe diet advice is contributing to obesity? (But we will see correlation vs causation later). Maybe this compromise is far more damaging than our primitive brains realize! Rather than compromise like this, we are going to look for situations where we can find false assumptions and try to break contradictions by finding a context in which we can meet both conditions ie find win-win scenarios. Here it is the desire to maintain a healthy body against 2 contradictory hypotheses – perhaps we can look at the context and determine that in some situations both are valid, yet in others one is false. Now we have testable hypotheses. Given the complexities and feedback loops in the largely invisible world of biochemistry this compromise is understandable. Fortunately, companies are far simpler and can become largely observable. Scientific Method Scientific method consists of making a hypothesis from logic and then testing this through repeatable experiments. Understanding the effect of changing something requires isolating that thing and changing it on its own to measure the effect and graph the result – from this we can build up complex relationships and test them. If we want to be data-driven, we need to not only react to data but also know what that data means. We do this by constantly building our understanding of the external and internal world of our organisation and running tests against it. Therefore, we need to not only have the tools to carry out this analysis, but we also need to know what we want to measure. More importantly, we want a powerful logic that enables us to identify and change only one part of an organisation at a time – so that we can measure its effects upon the organisation as a whole. Fortunately, someone else has already done a lot of this work, his name is Eliyahu Goldratt and he came up with the Theory of Constraints. Using this in conjunction with data-driven organisational goals and principles is a powerful focusing tool. Hint at Resolution The focus here will be the use of being data-driven to make an organisation far more capable of meeting its goals. This involves understanding how to define goals, how to align areas to them and break them down, how to help people represent their work in terms of the measures derived from organisational needs and so meet the needs of the whole. The goal is to avoid the idea of there being localised goals at all (For example a departmental goal that doesn’t directly connect back up). Technical Debt as epitome of the problems to overcome One example of this could be the concept of technical debt. It is a terrible phrase – it implies that there is something technically correct beyond that which serves the needs of the business. When people want to get technical debt out of a system there is push back. The organisation has goals it wants to reach and ‘technical debt’ isn’t in those terms. Moreover, the ‘debt’ is there because a decision was made to not do something because it didn’t align with a larger organisational goal. It is a very good example of a local measure – a group of developers don’t like a global decision. The term seems to have value but really just serves to work against the real goal. When it comes to ‘fix’ this there will be a very simple business case – this work needs to be done now because we want to do x, in order to make x faster and safer to do we need to do work y. It all just folds naturally into a piece of work without any hoo har. Technical debt exists in organisations where product and development teams are not talking the language of organisational needs. To have ‘technical debt sprints’ is the most organisational contradictory thing that I see and really it is hiding some other problem  – but that is a topic for another time! Optimize the Whole So, the solution will be to look at organisations as a whole and run experiments that benefit the organisation rather than its individual parts. By reorienting around the whole, we look to use logic and data to hypothesis and then careful measurement with statistical methods to evaluate and gain new direction. The resolution cannot be a control structure; we need an organisation to organise itself into being successful, otherwise we only have the expertise of the very few making those decisions. During the process of doing this, a reorientation of everyone’s decision and judgement techniques needs to be realigned towards a global direction rather than local. Nobody can be told to do this, it needs to be a behaviour that the system induces through its needs (Demming). Whether this happens organically if done well or everyone becomes aligned is moot, this is really governed by the structure of the system (we will see more of this later with Demmin and Sange whilst discussing learning organisations). Once we think we understand our system and start to focus on changing the system (ie communication patterns) to remove constraints in order to meet needs then – precisely because we meet people’s needs – the behaviour will emerge. What Next? Thanks for reading! Next time we will look at what a data-driven organisation will look like , from the perspective of what and how it does. This will get into data vs information, correlation vs causation, behaviour, learning organisations and systems thinking. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-08-05"},
{"website": "Made-Tech", "title": "Made Tech Team Interviews: Tom Taylor, Market Principal", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/made-tech-team-interviews-tom-taylor-market-principal/", "abstract": "This month we had our second interview in our Made Tech team interview series with our Market Principal Tom Taylor to better understand his role and to feature the great work he has been doing. Our Market Principal roles work to build and grow a sustainable business within their region. If you would like to watch the full interview, it is available here . Q: How did you become interested in working in tech? A: I was at university and had a few small businesses with a friend just to sort of pay our way through university. We had this idea of starting a company that sold tickets and drinks online, and it was back when those deal-a-day kind of Groupon things were getting big. So we started this website that sold tickets and drinks and we ended up getting some festivals on board and the some people gave us some money to keep building it and suddenly I was in the tech industry. I was studying law at the time, so it was quite a big change. But I guess it was at the time when some big tech companies were taking off and it was really obvious to me that if I wanted to be in business that technology was where I wanted to be. Q: Outside of tech, what other hobbies do you have? A: I’m from New Zealand and I grew up near the beach so I’m into water sports. So sailing, windsurfing, surfing, free diving. That’s kind of where I spend most of my time. Q: Before you joined Made Tech, had you ever worked in the public sector before? A: No. Actually, the first project I did when I joined Made Tech was at the Government Digital Service working on Gov Wifi . I realise now how lucky I was to work at GDS, who are doing some really world-leading things for public sector technology. Now, after a few years working for Made Tech, I actually can’t picture myself working in private sector stuff anymore. The public sector stuff we’re working on just has loads of meaning and really delivers value to so many people. Q: How long have you been at Made Tech? A: A little over two years. Q: How did you find out about the company? A: I actually found out about the company about six or seven years ago. I think it was through a London Ruby group because I was a Ruby engineer at the time. I got in touch with Chris Blackburn, who is one of the Directors, and I didn’t actually join the company then because I was in the process of moving to Amsterdam. Then when I was back in London I got in touch with them and ended up joining then. Q: What was the interview process like? A: It was two years ago, it was pretty similar to as now. It was easy. It didn’t really feel like an interview. I had a really good experience. I remember doing some pair programming and some TDD. It was overall a really good experience and fast. I think I remember doing it all within a week. Q: What does your role here involve? A: So I’m the Market Principal for Bristol, which basically means I’m in charge of the Bristol business. It’s a bit of a mix of technology, looking after clients, some delivery, some project management, and also some sales. I wear quite a few different hats. The core of it is making sure we’ve got a good sustainable business that’s growing in Bristol. Q: What do you like most about being a Market Principal? A: I guess the variety. Each day’s problems are quite different so it’s having to put on those different hats and solve lots of different problems. It’s quite enjoyable. Q: What are some of the most challenging things about this role? A: I guess focusing my time on the right things. There’s never enough time in the day so just prioritising things. Making sure I’m doing the best things for my team and for the company. Just trying to get the most out of the time that I do have. Q: What has been one of your favourite projects you have worked on? A: I’d say Gov Wifi . The first one I worked on. I was working at GDS a few days a week and at Made Tech a few days a week. So we were co-located and it really just taught me so much about the sort of new way of doing technology in the UK and I’ve taken everything I learned there and applied it to all the other projects I’ve worked on like Ministry of Justice and HMRC. It’s been really helpful. It’s cool too because a lot of the public sector organisations in the UK use Gov Wifi. I sort of say I worked on Gov Wifi and everyone’s a bit of a fan. Q: What has been one of the biggest challenges you’ve had to face since lockdown started in this position and how did you overcome it? A: We’ve managed to swap so many of our processes online, like hiring. We’ve hired so many people remotely and some team members I haven’t even met in person yet. Yeah, I know them and I’ve worked with them for months now. I think the hardest thing is supporting engineers and teammates who might have had a a bad day or bad week and you can’t go for a coffee with them or walk around the block or go for a beer with them. I think that’s the hard thing. Supporting people remotely and I guess I’m still figuring out how to overcome it. I’m doing my best. Q: What do you like most about working at Made Tech? A: I really like the culture of learning we’ve got. We take Friday afternoons off to do Learn Tech where we all get together as a company to learn and that kind of permeates into the rest of the week. It means that people are really eager to pick up new skills and to learn something. There’s this attitude of if you don’t know it you just go away and learn it and you’ve got the time to learn it. I think that makes everyone really good at what they do and really passionate about it and picking up new things. It kind of creates this really cool environment of learning and excellence. I’m trying to convey that feeling that I feel being at work here but I think that is probably it. It’s good for everyone. It’s this really nice win-win that we all do on a Friday afternoon. Q: Do you have any books or resources you would recommend for someone interested in this position? A: There’s so many. I started this role in January this year (2020) and I was on a holiday in New Zealand and before I started I read the book, The Hard Thing About Hard Things. It’s by Ben Horowitz I think. It’s quite a well-known business book but it really kind of dives into the detail of wearing many hats and the hard problems that you encounter when you’re filling a role like Market Principal. I’m actually planning on re-reading it as soon as I’ve got time. If you have any more questions for Tom about his role here, you can get in touch by reaching out on Twitter . Additionally, if you are interested in joining our team, you can view our current open positions here . Be sure to stay tuned for our next Made Tech Team Interview coming next month with our Delivery Manager Heather Venter. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-09-16"},
{"website": "Made-Tech", "title": "Made Tech Team Interviews: Charlene Hunter, Lead Engineer", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/made-tech-team-interviews-charlene-hunter-lead-engineer/", "abstract": "This month we had our first interview in our Made Tech team interview series with our Lead Engineer Charlene Hunter to better understand her role and to feature the great work she has been doing. If you would like to watch the full interview, it is available here . Q: How did you become interested in working in tech? A: I think I’ve always secretly been interested in it but I didn’t really realise that I was. My dad has a software company that I always used to hang out at when I was growing up, so that was one of the key reasons. At the time, when I was growing up I didn’t really want to work in tech. I thought I wanted to do something else because I saw it all the time so I didn’t really like it. But I’ve always just been interested in tech. I’ve always been doing things in that space. I think I started coding when I was about 10 or something and ever since then I’ve just been involved in it and I’ve enjoyed it. It’s a passion of mine so I really enjoy it. Q: Outside of tech, what other hobbies do you have? A: I don’t think I have any hobbies outside of tech anymore. I think everything I do is kind of tech related. There was a time I tried doing some sewing but that didn’t last very long and I’m back in tech again. Q: Had you worked at all in the public sector before you joined Made Tech? A: Yeah, so I’ve actually worked in the public sector for around 10 years. The first job I had was at BAE Systems and that was public sector work and then I became a contractor in the public sector. Now I’m back in the public sector! I quite like doing work that means something. You know you’re contributing to the greater good, I suppose. I tried getting out of the public sector and it just felt strange. It didn’t feel like I was making a difference so I had to go back into that space. So I’ve always worked in it for about 12 years. It’s different, but it’s nice. It’s worth it. Q: How long have you been at Made Tech? A: I think I’ve actually only been here about three months. It feels like a lifetime but it’s been about three months. I started at the beginning of May. Q: How did you find out about the company? A: It was actually through Luke and Yas. So about a year and a half ago I attended a meetup group called Black Devs London and Made Tech was sponsoring it. It was my first time ever speaking at an event and I remember seeing Yas and loads of Made Tech notepads on all the desks. I didn’t even know that companies sponsored meetups. So I found out about it that way and then over time I think I saw Luke on Linkedin and we connected. Then after that, he was like “come into the office. Let’s have a chat about the public sector.” So I came in and had a chat about the public sector and then it just seemed like you guys are actually a really good company. Made Tech’s just really nice. It’s just a really lovely company. So we stayed in touch for the last year I guess and then I applied and I’m here. Q: What was the interview process like for you? A: The whole process didn’t feel like an interview. There was an initial chat where we spoke about a bit of my background and just to understand me a little bit better and then the next stage there was essentially a three-stage interview. So the first part was another chat about my experiences in the roles I’ve done and the core values of Made Tech. Then there was a pairing interview and then we went on to do a technical scenario. So that was quite fun. That was it. Then you get a call within 24 hours. So it’s quite nice. Q: What does your role here involve? A: My current role is actually quite interesting. I’m at the moment looking after the Academy . So we’ve got the Academy starting in September and what I’m doing is trying to make sure that that runs smoothly. We’ve got the September Academy and then the March Academy and I’ve been massively involved in that. It’s been exciting meeting all the candidates and trying to make sure that we have the right programme in place for them when they get started. So that’s what my role is at the moment. I’ve done a bit of bid work as well but nothing on one of the actual projects yet. Q: What do you like most about your role? A: I think it’s Made Tech in general. So it’s knowing that you’re doing something that is actually benefiting people. I joined here because I wanted to work on projects that would benefit the public sector and instead, I work on a project that’s going to benefit people that wouldn’t otherwise have got into tech roles necessarily. It’s great to be working on something where you know that you’re giving back. You know that you’re giving people opportunities and it’s really fun to create a training programme and to work with a bunch of people that are really cool. For me, it’s that you know that you’re working towards something that’s bigger than just writing a bit of code. Q: What is one of the most challenging things about your role? A: At the moment, it’s probably the lockdown to be honest. I think if I was in an office it would all be going very differently. I would feel like I was meeting people and seeing people, rather than seeing people for 15 minutes and then ending a call. So at the moment, I think it’s very much the separation of not working with people. It’s the working away from people I find quite difficult at times. Q: What’s one of your favourite projects you have worked on so far? A: It’s the Academy. It’s been really fun just getting that going. Interviewing lots of people and running a really exciting day for people to try to get into the company has been really fun. So yeah, the Academy, definitely. Q: You were recently featured in BBC News for your work with Black Codeher and Coding Black Females . I was wondering if you could tell us a bit more about those projects? A: Outside of work I run a non-profit called Coding Black Females and that’s to address the under representation of black women in tech. So what we do for that is we have loads of events that we run. We’ve got mentorship programmes, we’ve got training that we provide to people and we do quite a lot to just make sure there’s a community around black women who are in tech roles and to make sure that it’s more representative. At the same time, I’m also working alongside another company called Niyo Enterprise to run a coding bootcamp called Black Codher . So we started the Black Codher bootcamp two or three weeks ago and it’s a 30-week coding bootcamp for 50 black women. So we’ve got sessions happening every single night for the next 30 weeks. It’s so exciting to see the energy of people learning tech and getting into tech and people that wouldn’t necessarily have thought of it as a career and now are trying to get into an industry that I absolutely love. So for me, it’s just exciting that I get to do loads of really fun stuff every single evening. In a nutshell, that’s Coding Black Females and Black Codher. If you have any more questions for Charlene about her role here, you can get in touch by reaching out on Twitter . Be sure to stay tuned for our next Made Tech Team Interview coming next month with our Market Principal, Tom Taylor. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-08-19"},
{"website": "Made-Tech", "title": "Stop buying specialists and start delivering outcomes", "author": [" Richard Race"], "link": "https://www.madetech.com/blog/stop-buying-specialists-and-start-delivering-outcomes/", "abstract": "Building a multidisciplinary team allows the members to wear multiple hats, and swap hats based on need, they can focus on what matters, delivering user value. Hiring or contracting specialists for specific team roles such as QA, Testers or DevOps Engineers and Developers who specialise on specific technologies, can lead to problems as your team focus becomes delivering outputs. They might deliver a lot more, and add value to your team if you don’t stick them in a box . To explore this topic we’ll look at the following example teams, Team A and Team B. Team A has: Backend Developer Frontend Developer Tester User Researcher Product Manager Delivery Manager Team B has: Three Developers User Researcher Product Manager Delivery Manager Team A consists of 3 specialists: Backend developer, Frontend developer and a tester. Whereas, in Team B we have 3 developers who are able to work on all aspects of the system. In our hypothetical scenario we have a backlog of issues and we’ve done a discovery of work and we know what we think we should be building to enable the citizens to achieve their goal. So, we start to get to work. In both team A and B everything is looking good. The teams are communicating and collaborating well, there’s a good split of work between the team members. We’re 2 sprints in and we’ve developed some functionality and are able to test the real application with users. We get some initial user feedback and it transpires that one of our assumptions is wrong and we need to pivot. We initially started our service with simple business logic and it needs to be expanded for the user to fulfil their goal. We identify that our backend application powering our service needs to be a lot more complex than once thought. Team B is able to respond to this with no issue as our developers are able to work on all aspects of our service. As Team B has no tester the team identify that they need to do some manual Quality Assurance to ensure they’ve understood the complex business rules before getting feedback from the users. As the developers don’t have a tester in their team, they ask the product manager and one of the developers who hasn’t worked on the feature to test it themselves. Everything works out as they’re able to split all the work and react to other important feedback from users. Team A is struggling. We’ve only got one person who’s working on the backend application. The frontend developer isn’t confident enough to work in an area they’re not familiar with, and has no interest in that area. On top of this, they’re also facing pressure that the team has to deliver. The backend developer now feels like they need to rush to deliver and start taking shortcuts. They stop writing comprehensive tests and throw that responsibility over the fence for the tester to pick up. This slows the backend developer’s feedback loop. Instead of knowing some code that they wrote isn’t correct at the time of writing, it’s now taking days to get that same feedback. The frontend developer is now waiting for the backend application to be complete. They’re looking for things to do, but they’re happy that it’s not their fault that the team’s velocity has decreased. Passing whole responsibilities to individuals within the team rather than owning the responsibilities as a team can lead to toxic behaviours like blame culture. Team A’s developers are now focusing on output they can control rather than the whole journey. An example of this can be seen when a team with specialists have tickets in their backlog that are very system based and technical focused. Compared with Team B’s tickets that are user story focused that focus on an outcome that requires changes to one or more systems. I’ve created my example to emphasise how easily focusing on output can negatively influence different parts of the delivery process. Like where the frontend developer and tester can use their specialisms as a way of avoiding to help solve the problem. It’s important to note that a team shape like Team B would not always succeed when things start going wrong. You’d still need to ensure your team is aligned around outcomes they need/want to deliver. Focusing on what outcomes we’re delivering and enabling the team to share the responsibility of all aspects of delivery, we build a shared drive to deliver. Having a team of generalists, they are more likely to self-organise around the work and look for support from each other. However, if you do have specialists, having a strong product manager/delivery manager to keep the focus on delivering the outcome as specifying that direction is key. Close attention is required for making sure the specialists are focusing on achieving outcomes. Sometimes we do need specialists to solve well understood specific problems with well defined outcomes. For example, Team B is looking at integrating their new system with a specific legacy application. However, none of the team are familiar with this technology, and they have a deadline to deliver a Minimal Viable Product. Here, it makes sense to bring in a specialist who can help upskill the team to enable them to have the knowledge to progress quickly. What we don’t want to happen here is the whole responsibility of integrating the systems together to lie solely with this specialist person, siloing a critical path of the service. Your team should not generally be built around specialisms, it’s all too easy to amplify toxic behaviours and encourage those behaviours within a team. Specialists are more likely to work within their area of expertise and start creating other issues like gaps of knowledge, silos, blockers and half finished solutions. Team B would work together to solve problems, allowing them to share knowledge as they go and ensure everyone has the same level of understanding. However, within Team A specific system knowledge would only exist in specific individuals. This opens up other problems, such as if a team member is not available this would impact the ability for the team to deliver, effectively blocking the whole team. Sometimes our users do not have a choice whether or not they want to use the service we build. We should be doing our best to ensure that we’re building software that delivers actual value by focusing on outcomes rather than output. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-08-27"},
{"website": "Made-Tech", "title": "Evolving the NHS Book a virtual visit service with Kettering General Hospital", "author": [" Laura Plaga"], "link": "https://www.madetech.com/blog/evolving-the-nhs-book-a-virtual-visit-service-with-kettering-general-hospital/", "abstract": "In April this year, we blogged about the NHS Book a virtual visit service that we rapidly developed with two NHS trusts in North West London at the height of the pandemic when patients could not be visited by their loved ones. The service has proved a huge success ever since, with Trusts from around the country wanting to use it as part of their COVID-19 responses and more generally across wards. One of these is Kettering General Hospital, which got in touch with us soon after the first alpha was developed so they could use it in their hospital. This has resulted not only in the widespread use of the service, with over 540 virtual visits having been completed, but also its further development to include new features designed around patient needs. We spoke to Ian Roddis , Deputy Chief Digital and Information Officer, and Anna Awoliyi , Chief Allied Health Information Officer, about how we collaboratively worked with ward staff and patients to iteratively improve the service based on user research. The Trust with digital ambitions Kettering General Hospital is part of Kettering General Hospital NHS Foundation Trust and provides healthcare services in North Northamptonshire and South Leicestershire. It employs around 4,000 staff, deals with 255 A&E patients per day and has 310,000 outpatients each year. Speaking to Ian, the Trust has big plans for making the most of digital and technology to serve patient needs and to help staff provide excellent care. “We have the ambition to be the most digitally enabled hospital in the country. We want to deliver solutions for patient experiences that are based on their needs. That means taking the best of the digital landscape as it exists today and applying it to the hospital,” he said. This strategy is being driven by Andy Callow , the Chief Digital and information Officer, looking to blend the best of national developments with local innovation, working with a range of ‘best in breed’ suppliers and developing the Trust’s own internal capability. This approach was very much front of mind when the NHS Book a virtual visit service came onto Ian and Andy’s radar, as a result of the exposure it received on Twitter from senior NHS digital and technology leaders at the height of the pandemic. A service for COVID and beyond COVID meant people in the hospital wouldn’t be able to receive visitors, and as the Trust ‘resets’ this is still the case. The risk of hospital acquired infection is a real one, and in all areas of hospital life we need to maintain social distancing. This meant that the ‘virtual visiting’ service seemed to fit the bill! “We knew we had to do virtual visits, we knew the solution we were using wasn’t ideal (it was a commercial freely available one, and had ‘information governance’ and privacy concerns). We also suspected that COVID wasn’t going to end soon and we would need something for when we reset too. This looked like a sweet little product that had been developed along the right lines, based on user needs and designed to be flexible”. For Anna, the most important thing was that the service could benefit patients and ward staff during such a difficult time. “It needed to be easy for our staff and the patients they were using it with. Imagine you are here, ill in a pandemic, with no phone, no loved ones and lots of fear around. At the same time, we had to think about how we could help staff, who are tired and overworked, to do even more.” Initially, the Trust had planned to rollout the service over two months but, due to the enthusiasm with which ward managers and staff embraced it, the rollout was completed in just two weeks. “What was phenomenal for me was how staff embraced the technology because they realised it could help them to work better in the difficult circumstances of the pandemic,” she said. Evolving the service to meet patient needs The original NHS ‘Book a virtual visit’ service had been delivered as an alpha in just 48 hours. It was released as open source code so that others could start to use it and evolve it for their own needs, which is exactly what occurred in Kettering. “When we first saw the app, it was ready to go and ready to use but we still felt we had to do some more work to understand how we could involve and engage our people and our patients with it,” said Anna. Therefore, we worked collaboratively to speak with patients and staff about their needs, and engaged with them as they used the service, in order to iteratively improve it. This led to the ‘rebook’ feature being delivered and then the ‘complete’ notification being added. “Just from going on the ward and sitting with people, it was like a lightbulb moment to see that people didn’t know when the call was completed. You don’t find this sort of thing out unless you have these conversations with users.” As the service was rolled out to different wards, comments from users would trigger new innovations, with agreed feature ideas rapidly delivered in one week sprints. According to Anna, “the swiftness with which features were delivered was amazing”, while Ian said this ability to rapidly make changes to software based on the hospital’s needs is exactly how the Trust wants to continue in future. “Our ability to influence big enterprise products is probably less than 1%, whereas with this service, our ability to change it is 100%. We’re using modern development tools and working off our own branch of the code, so we can develop different experiences for different wards and the different needs they have,” he said. Open development in healthcare As a result of the reception it has received from ward managers and the work they have put into the user centred design process, the NHS Book a virtual visit service is now used across 25 wards in the hospital and has facilitated over 540 virtual visits with an average duration of 50 minutes. This includes the Emergency Department, where it has been used for 8 virtual visits so far, even though it was originally never expected to be used there. The service is expected to become even more important in the coming months as the restrictions on visitors continue at the same time as the Trust aims to return to 80% of normal services. Furthermore, there are plans to explore using the service at Northampton General Hospital – part of the KGH/NGH group model. According to Ian, the technology and ways of working that have been introduced with the NHS Book a virtual visit service are exactly what he hopes the Trust can do more of as part of its ambitious future. “The NHS is building its own digital capability now and this is how we want the Trust to work. Commercial suppliers obviously play a role but we want to build our own capability using the best technology and methods available. But more than anything else, this is about putting patients at the heart of what we do and the services we design.” We’ve hosted a webinar about this service as a part of the Leeds Digital Festival 2020 . The webinar recording can be viewed on our YouTube channel: Delivering a user-centred NHS virtual visit service during COVID . A full case study about the NHS virtual visits project is also available on our website. If you’d like to find out more about the service please contact us at nhs-virtual-visit-enquiries@madetech.com . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-09-08"},
{"website": "Made-Tech", "title": "11 ways to see if your team is high performing: A Litmus test for your team", "author": [" Andreas England"], "link": "https://www.madetech.com/blog/11-ways-to-see-if-your-team-is-high-performing-a-litmus-test-for-your-team/", "abstract": "Introduction In this post I’m going to describe a series of indicators that you can use to see if a team that you manage, or are part of, are ‘high performing’. I’ll explain what the terms we use to describe the attributes of a ‘high performing’ team mean, the reasons we encourage particular team behaviours, and what we can do to move a team towards becoming ‘high performing’. Terminology What is a team, and how are you related to them? A quick refresher with help from Google Re:work . A team is a term that is used to describe a group of people who are brought together to get work done. Re:work use these definitions: A team is; “ Teams are highly interdependent – they plan work, solve problems, make decisions, and review progress in service of a specific project. Team members need one another to get work done.” As a counterpoint, a workgroup can look similar, however their behaviours and so the manner in which they are empowered is different; A workgroup is; “Work groups are characterized by the least amount of interdependence. They are based on organizational or managerial hierarchy. Work groups may meet periodically to hear and share information.” If you’re running a workgroup , the ‘signals’ described in this post may be less apparent. What do we mean by ‘high performing’? I’m paraphrasing Anita Williams re:work presentation & Amy Edmonson’s TED talk on Psychological safety . A high performing team is one that has a higher than average collective intelligence. Let’s assume that an individual has the requisite intelligence to fulfill their role within a business, it’s reasonable to assume (ok, I’m aware of the mass of environmental influences that can affect this) that they will be able do this. When groups of people assembled as teams introduce new dynamics come into play. If a team dynamics are moving in a positive direction, then we can see traits emerge, including; Group dynamics enable divergent opinions to be discussed Skills are shared so that roadblocks are quickly removed Team members are reliable Team has a high emotional intelligence Why would I want a team to be ‘high performing’ The benefits of a high performance team is that they are more likely to succeed in a project, they are more able to manage pivots, they will remain within the business for longer, they are able to solve problems more effectively/creatively, and they represent the company as a strong positive statement. High performing teams are a joy to work with, and be a part of. As the Google Re:work research discovered, they don’t have to be populated with exceptionally gifted individuals, nor do they have a failure rate that is lower than normal. However, they self manage, deliver with greater consistency, have significantly lower staff attrition rates, and have magnetic properties for recruitment. The indicators I’m assuming for the purpose of this post that you’re in a position where you’re able to have an external view of the team. I envisage the roles that can do this are: Delivery manager, Program manager or similar. These roles share an ability to objectively view the team, and are not subsumed by the minutiae of daily project delivery. Indicators, or ‘smells’ are behaviours that a team will show when interacting internally, and externally. Each indicator can be reduced to a root cause, which could then be discreetly managed. How is feedback managed within the team? Do retrospectives produce debate with actionable outcomes, does the team encourage feedback, is it delivered and received without fear? How does the team generate ideas (in fact, do they generate ideas?) Is there a method for generating ideas, are they developed, and does the team’s culture enable them to openly discuss ideas? How are ideas accepted and used within the team? Does the team have a culture of open question asking, and are domain experts happy to share their knowledge? Dependability of individuals within the team Is every member of the team able to describe where the project is (from a task perspective), and what the current priorities are? Do team members have clear responsibility for individual tasks within the project? Are the team members able to describe the tasks they own, and how they integrate into the project as a whole? Structure and Clarity of tasks Is it clear how a task’s ownership is received, is there a negotiation process? Does the project have a clear direction, and are all the team members working to move the project in that direction? Are the team members able to take ownership of a task without being micromanaged? Meaning of work Do team members treat each task as an opportunity to learn, mentor, share and transfer knowledge? Or is each task simply a job to be done by a team member with the appropriate skills and availability? Does the team celebrate individual and collaborative success? Impact of work Does the team consider itself to be continually firefighting? Is the team so overwhelmed with the project scope and goals that they feel that they’re making little headway? Are there symptoms of burnout? (physical and emotional exhaustion, cynicism and detachment, feelings of ineffectiveness and lack of accomplishment) Working with failure If the team feels safe enough to openly discuss failure, which is a key part of many design methods (E.g. Hypothesis driven design), then failure can be used as a learning method rather than a reason for punishment. Quality Is quality being maintained by the team? There are many dimensions in which quality can be compromised (ill defined features, poor documentation of the code written, inconsistent code styling, etc.) Any failure will result in the team taking an ‘uncomfortably’ long time to recover from a quality issue. Recruitment Growing a high performing team is (almost) an easy task. A team that radiates psychological safety, trust and respect will have a magnetic effect within the organisation. Recruitment is a lower priority when team retention is significantly higher due to the team being a great place to be. Deployment Are team members trusted to deploy their own code? A software team’s inherent agility, trust, quality and ability to rely on one another can be seen by how they ‘ship’ code. Though it could be argued that deployment methodology is an environmental factor, outside the influence of the team. How do we move towards becoming ‘high performing’ A team is affected by a myriad of factors, some of which cannot be managed by a team culture (Global pandemic anyone?). However the team’s ability to respond to external influences, and how its culture reacts to internal issues can be hugely influenced by a few factors. Once again my source here is Google Re:work . Psychological safety Psychological safety is the foundation that underpins a high performing team (and all the other dynamics). If this is absent, then the team cannot perform strongly. A team with a strong sense of psychological safety will confidently “take risks on the team without feeling insecure or embarrassed”. Dependability Dependability is the ability to rely on team members to not only be able to do work, but be able to estimate how long it will take, and can be confident that the work will be of adequate quality. Structure and clarity This could be argued to be an external factor when considering a team’s dynamics, however my expectations are that a team will negotiate and own the goals, roles and execution plans. If these are clear, and demonstrable by any team member, then they will be adding to their ability to be high performing. Have meaning for work Here’s a tricky dynamic, if a team member is not motivated by the work the team is tasked with, they will have problems being motivated and delivering on time and to an acceptable quality. A person who is primarily motivated by financial gain, career opportunity or fear will not make a good team member. Impact of work My area of expertise is protecting and promoting the end users’ interests within any project. The close contact with the users begins at the research stage, and often involves deeply understanding the motivations for their behaviour. I know this level of engagement gives me a strong motivation for all the projects I’m involved with. “ Do we fundamentally believe that the work we’re doing matters?” Anecdotal breakout: In the summer of 2016, I was running a 2 week Inception (rapid agile project startup) for the digital transformation of Virgin Money Giving. The existing service was 10 years old, definitely pre-smartphone and showing its age. I was facilitating a persona goals workshop, where we determine what it’s reasonable for various users to be able to achieve with the service. So far, a run of the mill engagement. Then I was introduced to the persona of a father whose child has recently died. This group of users make up a significant proportion of the Virgin money giving service, because aligned with the grieving cycle, the fathers motivation to ‘do something’ can be fulfilled through charity fundraising. My attitude to this project moved from a simple digital service rebuild, to building a service that could provide support and avoid crisis for grieving parents. What’s next This post’s aim is to introduce the concept of high performing teams, and hopefully cause you to consider whether you are enabling the teams you manage to perform as strongly as possible. An engagement with Made Tech has team building built in from the project Inception, through governance and the mechanics of deployment. Please let me know if you enjoyed this post, and whether you’re interested in the method we use to build high performance teams. If you are interested in learning more about this subject, you can download our “Building High Performance Agile Teams” ebook from our website or order a physical copy from Amazon . We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-09-02"},
{"website": "Made-Tech", "title": "Made Tech Team Interviews: Heather Venter, Delivery Manager", "author": [" Karsyn Robb"], "link": "https://www.madetech.com/blog/made-tech-team-interviews-heather-venter-delivery-manager/", "abstract": "This month we had our third interview in our Made Tech team interview series with our Delivery Manager Heather Venter to better understand her role and to feature the great work she has been doing. Our Delivery Manager roles are accountable for the effective delivery of complex, high-risk products and services. They have strong communication skills and are comfortable engaging with senior stakeholders. They typically manage one or two agile projects as part of a highly skilled, multidisciplinary team. If you would like to watch the full interview, it is available here . Q: Outside of Made Tech, what hobbies do you have? A: Really important for me is to move and get out. I live near Wimbledon and our favourite thing to do at the weekend is to walk around Wimbledon and walk around the common. I try to keep going to yoga classes, pilates classes and try to keep flexible. Mostly for mental health to try and keep moving. Q: Before you joined Made Tech, had you ever worked in the public sector before? A: Yes, a long time ago, probably over ten years ago now. I worked for a busy police media room in a Marketing Assistant role, which was kind of scary as the news rooms would phone. I wouldn’t pick up the line unless there was no one else in the room. But that was quite a short role. Since then I haven’t, no. It has all been in the private sector until Made Tech at the start of this year. Q: How long have you been at Made Tech? A: I started the first working day of January this year. You think to yourself ‘oh, it’s not been that long.’ But then it is already September. Where has the time gone? Q: How did you find out about the company? A: Just through Linkedin. On a particular day I did a quick job search to look at what was around and I think Made Tech was one of the first listings. I hadn’t come across them before, but as soon as you get to the website there’s lots of great info on there and the Handbook . It was really easy to feel like you’ve got the gist of what the company is about with just a few minutes of taking a look. It was great. Q: What was the interview process like? A: It was great. I had a fairly short telephone interview. Then I was able to come into the lovely Made Tech offices in London. Lovely lighting, lovely flooring, there’s a certain smell. I got to meet a couple of people. There was also a dog that day walking around. I remember being part way through an answer and being like ‘there’s a dog!’ and then ‘what was I talking about?’ Q: What does your role here involve? A: I sit at the account level at the moment alongside a Technical Principal . I sit on the Ministry of Justice account and we have a few streams of work there. So I’m offering delivery assurance across those streams and I’m also a point of contact for our customers. Providing hopefully high-level support for my teams. Q: What do you like most about being a Delivery Manager? A: I like the challenge of bringing together a group of people who have maybe never worked together before and then evolving our ways of working and going through a bonding process over time. But I also enjoy getting to know our customers and their unique challenges. How do we get the best from their Made Tech experience? Going on a journey with them and getting to know them. I really enjoy that as well. Q: What are some of the most challenging things about this role? A: Delivery is really subjective so everyone, engineers, delivery managers, tech leads, account leads, we’ve all had different experiences of what has worked and what hasn’t. So we’ve all probably got different opinions on delivery. As a Delivery Manager you are trying to pull all of that together and share your own experience and your own training. You are also trying to nurture the opinions and open up the opinions of everyone else. At the beginning of a project that can be quite challenging because you need to try something and then you need to adapt and iterate on that. It’s doable but I think in remote times when you are trying to take everyone’s opinions on a video call, there is certainly a challenge there. Q: What has been one of your favourite projects you have worked on? A: Not necessarily just a project, but we work on a programme for the Ministry of Justice as the moment. So we work on the Prison Technology Transformation Programme, which gets shortened to PTTP. Made Tech are a big part of that. We have multiple work-streams and we work really closely with our Ministry of Justice colleagues. They work on our teams and we collaborate together. I’m really enjoying plugging into that big programme with big outcomes they’ve got over a two year period and working closely with our colleagues. Q: Do you have any advice for anyone who wants to be a Delivery Manager? A: Yeah, so I often say that the role I have probably didn’t exist ten years ago. I sort of started in Marketing and evolved into a Product Owner and a Scrum Master and kind of gone from there. I would say if you are early in your career, maybe don’t panic too much about where your career is going. Just keep building on what you enjoy doing and a new job might open up and you can go that way. Maybe you’ve got a rough goal, but I would say be open minded. For me, the path I took through getting trained in agile and scrum, also being a Product Owner then a Scrum Master, that gave me a really great foundation to then move on to be a Delivery Manager. But I understand everyone’s got their own path. I’d say stick to what you are good at and be open minded. Q: What has been one of the biggest challenges you’ve had to face since lockdown started in this position and how did you overcome it? A: I guess firstly from a personal point, when we first went into lockdown, like many people, I got scared of going outside for a few weeks and then I ended up stopping exercise, eating more, drinking more. Just becoming really lethargic and didn’t really notice that was happening for a while. And you start feeling sad in yourself. Obviously there’s a pandemic going on, but for me as soon as I started getting out there and moving again, I instantly felt the rewards of that. Work-wise, I think the biggest challenge has been sensing the morale of our teams. I see people on calls all day long, great, but I’m really missing observing the body language of our team members when they are not on a work call. Actually, when they are going to grab a coffee. Not that I sit and stare at everyone, of course, but you do get a sense of how someone’s doing. A big part of the Delivery Manager role is to check in on the teams and make sure they are okay and that they are not just joining a call saying ‘I’m fine’ but actually they are not. So there are ways to get around that, and 1-2-1s I think are a big part of that. But it is certainly a challenge in lockdown. Q: What do you like most about working at Made Tech? A: What a lovely company Made Tech is! I don’t want to discredit where I’ve worked before, but this is possibly my favourite place I’ve ever worked. There’s just such a genuine focus on diversity, equality, learning, career development, it’s people-focused. Where do people want to go? A genuine interest in that. It’s the first role for a while where I’ve worked with females daily and with multiple females. Not that I have anything against men, but it’s nice to have a mixture. I’ve never really worked in such a safe-feeling, blameless environment with such a focus on people. I’ve just got wonderful things to say about Made Tech. Q: Do you have any books or resources you would recommend for someone interested in this position? A: Yeah, absolutely. If you are at the start of your agile journey, you want to learn about scrum. I always recommend scrum.org. They have some great mock exams on there if you want to start training and getting your accreditations. I would think about getting certified as a Scrum Master or Product Owner. Look into things like scaled agile, safe. I’m reading a book at the moment which was recommended by one of my colleagues. It’s called Accelerate: The Science of Lean Software and Devop. Particularly, if you want to get into tech, which is what Made Tech focuses on. As well as being a Delivery Manager and focusing on agile, you’re going to want to learn the more technical ways of working: continuous integration, continuous development. Since working at Made Tech, I’ve improved my knowledge of those areas through reading and online courses. So I recommend maybe doing the same. If you have any more questions for Heather about her role, you can get in touch by reaching out here . Additionally, if you are interested in joining our team, you can view our current open positions here . Be sure to stay tuned for our next Made Tech Team Interview coming next month. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-10-15"},
{"website": "Made-Tech", "title": "What is a data-driven organisation? Bringing order to chaos", "author": [" John Nicholas"], "link": "https://www.madetech.com/blog/what-is-a-data-driven-organisation-bringing-order-to-chaos/", "abstract": "This is the second post in a series about how to build a data-driven organisation. The first was about why anyone should be interested . This goes more into the problem of data and therefore the end purpose of a data-driven organisation. As tradition dictates, we will redefine the topic to better describe our gist. Here it is from ‘data-driven organisations’ into ‘informed learning organisations’ and to explain how this distinction will be important in these posts. Future posts will get into the plan of topics covering areas of an organisation, tools and techniques that can be applied to generate collaborative system-level understanding and then finally end with examples and applications of the ideas – or at least analysis of semi-fictional case studies through these lenses. What is a Data-Driven Organisation? Simply put, an organisation that uses data to inform its decision-making processes – this is very different from the concept of ‘open data’ which involves publishing data sets for others to consume in innovative ways. The general definition of a data-driven organisation will be something like: ‘making decisions offering competitive advantage via gathering data from all areas of an organisation and empowering employees to make them from data-related analysis’. This is absolute undirected nonsense that will only increase the amount of directionless work and hamper the organisations ability to deliver additional value because it does nothing to address the real problem. Organisations lack awareness of what their problems are, awareness of where those problems are and an awareness of how to build a strategy to tackle them. If this was not the case, they would already be measuring things and making informed decisions. The focus in these posts will be to outline a strategy that not only enables data-driven organisations to grow and run experiments on themselves but to also figure out what changes work and provide data to feed into a generation of new testable hypotheses. The Problem of Data The problem with ‘data’ is that there is an effectively infinite amount of it for any given moment in time – and a tiny infinitesimal amount of this can be captured. Even more frustratingly that tiny amount that you can capture is still effectively infinite relative to your analytic prowess. Then, when you have reduced your input to a vaguely useful dataset and trawl it for a pattern, you will fall victim to finding all the coincidences that just happened to create a strong signal rather than any real insight. I.e. you simply create fiction from noise. This is the basis of any correlation is not causation argument – you find what you look for. For instance http://www.tylervigen.com/spurious-correlations I find this concerning because I don’t recall seeing Nicholas Cage swimming in any films except maybe that one with Shaun Connery and Alcatraz. Maybe method acting causes drowning? This is the most counterintuitive thing about data. It is merely events – it does not capture causation unless very special and hard to control conditions are met. So we need something completely different. The problem is the result of both processes looks the same – and it is the same unless you happen to know a lot about how it was captured. Therefore saying an organisation is driven by data is pretty meaningless (or even damaging) unless we start to develop our ideas. Data is noise, it is what is streamed off sensors, something ‘magic’ then happens and the output of that is what informs the actuators of the organisation. Getting to that magic requires logic, hypothesising and experimentation. It also requires accepting that true (deductive) causation is impossible and so letting data reshape the system of beliefs is essential. This is achieved by constantly testing our judgements against the new reality – we need abduction. Which leads to the core problem here: intelligence and learning. We know we cannot deduce causality and we also know that we do not want to merey infer causality because correlation is not causation. We want to start to build systems of organisation that abduct causal relations by testing hypotheses based upon data – later we can then start to build heuristics to simplify decision making in the future. Intelligence + Learning: Neural Nets – the problem. This is a specific example of the kinds of logical problems we face – this might get briefly a bit technical, bear with! The generalism above is so abstract it actually forms the basis of neural nets – possibly the most abstract generalisation of a system that does ‘stuff’. Neural nets literally come from the hypothesis ‘What if we build a system that has sensors connected to actuators that can do things. What if it can feedback a score back into itself to adjust the weightings of bits of information in order to do something different – can that system do something predictable / stable?’ Yes, for instance, it is quite easy to build something that turns on when a sensor goes beyond a threshold and then train a system to do this – it would be in effect moving a lever. The taking of input applying a weighting and deciding whether to output a signal is the basis of the neuron. More complex neurons can have multiple inputs and produce an output based on some weighting across them – but hopefully it isn’t a big leap to see how that could be seen as a clump of many very simple ones working together. Then, given that we think applying logic to predictable behaviours for given inputs can produce more complex behaviours – what happens if we layer these things, connect them all up and let them feedback on themselves to adjust their own weightings? Now we can start trying to solve problems with many many inputs and many many outputs by scaling the number of nodes. Join them up large vertical lines with each vertical node connected to each node in the next vertical, ‘just’ train the system and magic happens. This gross simplification of the story has far more truth in it than many would like to admit. AI success is due to computation costs and feedback loop costs of running those neurons dropping rather than some huge conceptual breakthroughs. The Catch The catch is that all the work in getting these things to work goes into carefully assembling a training data set. Especially when complex – like image recognition. This data set will undergo iterations and refinements throughout the process of getting a network to behave. Also, it is required to get another set of data and categorise that in order to test the network against afterwards – this is a scoring algorithm that rates success so that it can be compared to other versions. The actual scorer is essentially people manually verifying because people do the categorisation. Why are ‘Algorithms’ Bias? There is a lot of very careful selection happening here – there is also a lot of human intervention in selecting the data. This is why neural nets are bias (Eg Highly racially bias) – they contain not only the bias of the people selecting the data but also the innate bias that exists in the data sets of the world . Best of all these networks are very good at finding the cracks in your rules. Anne Ogborn (expert in symbolic AI) told me several stories of how this goes wrong. For instance, when trying to evolve something that can walk from a set of hinged limbs (Eg a biped or a spider). If you allow it to change the length of hinged limbs then you will find that it will exploit the vagueness. It will probably generate a single long vertical limb that simply falls over and crosses the finish line. You are required to constrain the maximum length on the limbs to avoid this. Another cautionary tale (unsure of how true) was a military net designed to identify camouflaged tanks. It was 100% successful in test and 0% in wild. It turned out each picture with a tank had some marking that was hard for humans to spot. So they built a machine to spot that marking by mistake. We Need Something Dynamic – Continuous Judgements So even convolutional neural networks don’t do magic, there is a lot of very careful planning, analysis, categorisation, iteration and refinement that goes into building a working ‘Self Learning Artificial intelligence’ (which at the end is more like a stereo than a brain because its response is known given a set of known inputs – its behaviour is baked in and no longer changes). So state of the art AI is never going to solve – unless you already know the problem can be scored and answered through logic. However a great many are known not to be. We need something like continuous judgement (and refining judgement abilities) because our environment and its needs change at a very rapid pace. Hopefully, over the course of these posts, you will come to agree that the dominant cost of software isn’t its development cost, it is the failure to capitalise on opportunity, and this is often based on timings in the external world. What does the solution look like? The reader is, therefore, fairly audacious in wanting to build such a system of systems with neurons as complex as people in a changing environment! This is what this series of posts is going to get into: unpacking a lot of different fields and joining them together into a framework that is directed into the heart of the problem Really, these are Informed Learning Organisations . Starting from data will get nobody anywhere – unless it is very carefully controlled in how it is both generated and captured at which point it is really information. We want to refine this further into developing methods to know where to look to find useful information – otherwise we generate a horrendous amount of probably meaningless undirected work. We need to move as a graceful whole. Grace involves minimalism, getting to grace requires method, practise, continual improvement and self reflection but most of all a relaxed naturalness that can only come from a strong core. As such we will develop an approach that enables ignoring the vast sea of data to pick out the areas that will give the most return in a measurable way. It turns out that – contrary to the initial conception, the goal of a data-driven organisation is not to measure all the sources of data – it is to narrow down to measuring the few things that matter at the right time. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-09-10"},
{"website": "Made-Tech", "title": "Getting to know the September 2020 Academy cohort", "author": [" Bella Cockrell"], "link": "https://www.madetech.com/blog/getting-to-know-the-september-2020-academy-cohort/", "abstract": "At the beginning of September, a new cohort of 12 Academy Engineers joined the Made Tech team! As I was one of these new engineers, I wanted to speak with my Academy colleagues to learn more about their backgrounds and the experiences they have had during our first week in the programme. The Made Tech Academy is a 12-week programme that trains beginner coders to fully-fledged software engineers who go on to work on exciting Made Tech projects with real-life clients. What did you do before you joined the Academy? AZLINA YEO: I was a secondary school mathematics teacher before I joined the Academy. Before that, I was a full-time mum at home. I spent the bulk of my career in Apparel Merchandising. BELLA COCKRELL: I worked for six years as an editor at an illustrated, non-fiction publishing house, working with authors and designers to create gorgeous craft books. I’m a mean knitter, me! BEN DALTON: Prior to the Academy and post-uni, I was a revenue executive in a large hotel company. I had worked in the hospitality industry for about two years. CLAIRE GUEST: My degree was in Chemical Engineering. Before joining the Academy, I worked as a technical consultant, with a lot of my work involving data processing and analysis. CHLOE WONG: I just graduated this summer from Imperial College London where I studied Chemical Engineering. EMMA BLOTT: I studied Physics and then Geophysics at university. I worked and travelled for a couple of years between my undergraduate and master’s degrees and then I worked as a geophysicist doing seismic processing for the last four years. Software development is a complete career change for me! ZACK ADLINGTON: I managed supply chains in the Royal Navy. Getting engine parts flown out to us in exotic locations was an ongoing challenge, if a nice problem to have. My last job was procuring services to move fuel to the Armed Forces around the world. What made you want to get into software engineering? AZLINA YEO : I got into software engineering because I enjoy coding, debugging and optimising code. I find it quite addictive and I gain great satisfaction when the code works as expected. BELLA COCKRELL: I didn’t know the first thing about coding before I joined a web development course for fun, but then I caught the bug and spent a lot of my spare time coding websites and animations. It’s not so dissimilar to making books, but at least I know there’s a typo in my code because the program won’t work. BEN DALTON: Out of curiosity, and having a strong desire to learn new skills, I began learning to code in my spare time. I knew I could apply my problem solving skills to what I have come to realise is my area of passion and commitment professionally. I’m thrilled at the prospect of delivering value in one of the fastest growing industries in the world. CLAIRE GUEST : A small part of my previous job involved programming, which I really enjoyed, and inspired me to learn more in my own time. Because I enjoyed it so much, I was keen to make it the focus of my career. CHLOE WONG: During the first year of university, I had a very bad experience with coding (MATLAB, if you know you know) and after that I decided to NEVER code again. As time went on though, I started to hear the word ‘Python’ floating around a lot from my course mates, and how absolutely amazing it is, so I finally decided to see what all the fuss was about at the start of lockdown. It didn’t take long for me to realise that this was a ton more fun than anything I’ve learnt on my course and software engineering is what I want to do with my life! EMMA BLOTT: I dabbled in coding when I was at university and always found it really fun. In all my previous jobs I have found some way to introduce coding into my work, usually by automating repetitive tasks. I’ve always been really interested in technology and knew I wanted to make the switch into software engineering at some point, now seemed like the perfect time! ZACK ADLINGTON: I’ve been fascinated by technology since I was a child, but by my teens it wasn’t getting me invited to many parties, so I found other interests! Around 18 months ago I fell back in love with solving problems through software and decided to make it my career. What made the biggest impact on you during your first week at Made Tech? AZLINA YEO: The biggest impact was how surprised I was that writing good code was not our first priority in Made Tech. Yes, it is important to write good code. However, what took precedence was the importance of continuous self improvement as an individual, as a team and as a company. BELLA COCKRELL: I was really surprised one of the first workshops was about how to give feedback – I now realise that an open feedback culture is the only way we will be able to improve quickly, not only as engineers but also as kind human beings, inside and outside of work. Giving and receiving praise and constructive feedback make us more confident coders. BEN DALTON: Definitely the people! Everyone is so friendly and welcoming – I immediately felt part of a team. Made Tech embraces such a positive and supportive learning environment. CLAIRE GUEST: Definitely the people, both the other Academy members and the rest of Made Tech. With the current pandemic, the Academy is being run remotely, but everyone has been friendly and supportive despite the difficulties. CHLOE WONG: How kind and friendly everyone is! Also, after an extended summer of not doing much, it’s great to be back in a learning environment (especially one where everyone is so supportive). EMMA BLOTT: It was really great to dive into feedback culture from the get-go. On day one we had a presentation about how to give and receive feedback. Coming from a very different industry it was really great to see the emphasis on feedback being a positive thing that’s used to help us grow! ZACK ADLINGTON: We had a workshop on how to give and receive feedback on our first day, before we’d even been set up with our Made Tech email addresses. This really hammered home to me how much people and self-improvement are valued here. What are you most excited about going forward? AZLINA YEO: I am excited about starting my journey at Made Tech, most importantly growing and developing skills as a software engineer, to create meaningful programs and make a difference in the public sector. It will be interesting to see how much I will improve in one or two years’ time. BELLA COCKRELL: I’m really excited to learn everything that the Academy has to offer and become a better engineer! We’ve got workshops on user research, clean architecture, test-driven development and data engineering plus lots more coming up in the next few weeks – I can’t wait! BEN DALTON: I’m most excited about my growth, the fact that I will have a positive influence on the lives of others and on the success of the company. The learning never stops, the goals will be measurable and the opportunities really are endless! CLAIRE GUEST: I’m looking forward to the last part of the Academy, where we get to have a go at our first project. CHLOE WONG: In two short weeks I’ve learnt more than I could have imagined, so the thing I’m most excited about going forward is to see how much progress I will have made by the end of the Academy! EMMA BLOTT: I’m super excited to get going building some software and work with a team of developers. Being self taught, I’ve only really coded alone so it will be great to build something and learn from more experienced developers than me. I’m also excited about working in the public sector and building tech that helps people. ZACK ADLINGTON: Learning how agile projects deliver complex but well-designed systems. I love trying to make code clean, and I’m really excited to take it to the next level and learn good software design principles. By joining our Academy, you can build the foundations of a software development career – and be paid a full salary from day one. We will be opening applications for our 2021 Academy soon so please keep an eye on our website. In the meantime, you can learn more about the programme here . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-10-01"},
{"website": "Made-Tech", "title": "3 strategies for transforming legacy applications", "author": [" David Winter"], "link": "https://www.madetech.com/blog/3-strategies-for-transforming-legacy-applications/", "abstract": "In a previous blog, I explained how to decide which is the right legacy transformation strategy for your application by evaluating different strategies sequentially. Now, it’s time to look at each strategy in a bit more detail, including when to choose them, the steps to take, the decisions to make and the issues you may encounter. In this blog, I will briefly introduce some of the key points for each strategy to provide an overview of what’s involved. If you want to find out more about how to implement the steps mentioned or to avoid the issues raised, these can be found in our recently released ‘ Modernising Legacy Applications in the Public Sector ’ book. Strategy one – Iterative modernisation This strategy is about reviving an existing application so it’s maintainable, sustainable and able to evolve in future. To choose this strategy, you need to be able to see a future path that involves continuing the investment in and evolution of the technology beyond the initial modernisation stage. In the case of off-the-shelf software, you may simply want to get your application into a state from where you can upgrade the product. You will also need to consider whether you have the right capabilities at your disposal and the right access to the codebase to make iterative modernisation possible. Steps to take, decisions to make By proceeding with an iterative modernisation, your application should become easy to upgrade with security patches and provide a foundation from which you can plot a path for future investment in the technology. Set your goals – Your first aim is to improve the application so that it is at least maintainable and supportable but you may need to change or add new functionality that enables you to meet an organisational or user need. Start with an audit – You should start auditing the application to assess how you can get to this desired state, drawing on your earlier mapping to build your understanding of how the application is architected. Implement a test harness – A test harness will help you to describe and verify how the application works before you start making changes, such as refactoring the code or upgrading versions and dependencies. Replatform the application – If moving to the cloud is a key driver for modernising your application, you need to get it running on a new platform. While doing so, you should ensure that deployment and monitoring are automated. Refactor the application – You can now safely upgrade dependencies and run your test suite to assert if it still works as expected. It is advisable to make changes that are as small as possible, so you can quickly detect what is causing your test suite to fail. Add new value – At this point, your team should be able to add new functionality to the application if desired and needed, as well as begin implementing the plan for continued investment going forward. Issues you may encounter The issues you encounter when iteratively modernising will depend on the particular circumstances of the application you are attempting to modernise and the team responsible for doing the work. Complexity of business logic – if you’re dealing with many lines of code that are hard for anyone to understand and follow, it’ll be hard to untangle and get to a stage where it is easy enough to understand and extract the core business logic from. Bugs and workarounds – you may encounter bugs in the application that the people using it have developed workarounds for. When deciding to fix them, you need to consider that the bug’s behaviour is now expected behaviour. Discontinued software dependendencies – You may find that certain dependencies are not available or that they run on old technology or on software that runs on old technology, which doesn’t match your modernisation strategy. Strategy two – Iterative replacement If iterative modernisation isn’t possible because the existing codebase or technology stack becomes a hurdle your team cannot overcome, you may be able to pivot towards an iterative replacement strategy instead. You should consider this approach if the application you’re trying to modernise is a vital organisational cog that certain critical processes rely on. As your business processes are tied to it, you may be reluctant to buy or build new software to replace it, as this might mean having to change your processes too. Delivering value to your organisation and its users by meeting user needs is of paramount importance and if this can be best achieved by delivering bespoke software or an off-the-shelf replacement, it’s likely worth the cost and can be justified. Steps to take, decisions to make When iteratively replacing an application, you should stick to the Government Digital Service (GDS) principles of understanding user needs and delivering in an agile way that iteratively progresses through discovery, alpha, beta and live phases. Define components to be replaced – In order to iteratively replace the components of an application, you need to work out how to “decompose” the application. By trying to reduce the surface area of the application, you can slice up the functionality into components to be replaced. Assess off-the-shelf replacements – Look at the product roadmaps and decide what the future holds for a product. It may take time and effort to introduce a replacement into your organisation, so you need to ensure the investment is worthwhile. Reflect user needs in your architecture – Once you have defined components and assessed replacements, you need to come back to the architecture of your application and overlay your users’ needs, in order to decide what the future state of the application should look like. Slice and sequence the problem – As you replace components, the changes should be sliced into small chunks that balance disruption and cost, while also considering the sequencing from a user perspective. Retire legacy components – Once your replacements are live and you have moved users, you will gradually come to realise that legacy parts of the application are still live but with few or no users using them. You need to consider how you fully retire these components. Issues you may encounter By adopting an iterative approach, you are reducing risk by replacing functionality gradually and providing some level of functionality for users on an ongoing basis. However there are still issues you need to be aware of. Switchovers may go wrong – One of the big risks of iterative replacement comes when you switch out old components for new ones. If it goes badly, you may need to revert while preserving the integrity of any data. Overcoming complexity takes time – In attempting to mitigate these risks and take caution over data handling, your iterative replacement may take a lot longer than it would to replace everything all at once. Getting lost in the detail – As you iterate through replacing workflows, you might feel like the process is becoming a bit of a mess, with systems running in parallel, different users at different stages of onboarding and service, IT and operations teams straddling all of it. Strategy three – Big bang transformation If an iterative and incremental approach to transformation isn’t possible, you are left with the option of transforming your application in one big-bang release. This inherently risky approach, which involves replacing the entire application and all its components in one go, should only be attempted as a last resort. You would usually only use the big bang approach for replacing an application with an off-the-shelf product. If you were developing a new application, you should develop and release it iteratively as there are few reasons to big bang custom applications. One of the reasons you might resort to a big bang transformation is because you have concluded that both the iterative approaches would cost too much. Another is data complexity. As complexity increases, so does the likelihood of needing to resort to a big bang transformation. Finally, you may also find that organisational constraints, such as deadlines brought about by licenses expiring or urgent policy changes, play a role in choosing this strategy. Steps to take, decisions to make Tread carefully if you choose to move forward with this type of transformation. You should rely on the important insights you have gathered in mapping your organisation and applications, as well as incorporating modern software techniques associated with agile development. Find the right product for you – To find the sort of products that will not become legacy technology, you should refer back to the knowledge you have gathered about users and also consider the application mapping you did. Test it with users – You should decide whether it’s possible to do a demo of the product and to test with users by sitting with them to evaluate the product and ensure their needs are being met. Reduce risk with pre-production – Before you go live, you want to undertake further user research but with real data in a pre-production environment. This will be a valuable process to set up not just for your first release but all subsequent releases. Form an iterative release plan – Think of your releases from a user perspective first and attempt to mitigate risks and disruptions to their daily routines in doing so. Also, consider adding users incrementally by moving a small subset first. Consider creating a playbook – Although you should always aim for automated scripts for your code documentation, a playbook should also be considered. Anything that can’t be automated should be included, as well as instructions for using the automated scripts. Plan your exit – Remember that you haven’t found yourself implementing a big bang release as a result of perfect circumstances surrounding your legacy transformation. Therefore you should start to think about how you can iterate beyond it and plan your escape. Issues you may encounter A big bang transformation, which replaces incremental change with an all-at-once approach, is inherently risky and brings a number of issues with it. By identifying what they are, it will be easier to mitigate the risk they cause. Running parallel systems – To run the systems in parallel, you need to consider whether a) you have a team that is able to support them, b) you can keep the data in sync and c) your licensing agreements allow it. Building tomorrow’s legacy today – Your choice of a big bang strategy will have been your last resort, so you need to recognise this compromise, avoid the mistakes of the past and plan for an exit. All strategies require ongoing risk mitigation Hopefully this blog has given you some idea of what each strategy involves and why you might choose them. As mentioned, you can find out more about all of them in ‘ Modernising Legacy Applications in the Public Sector ’ book. Don’t forget that the iterative approaches allow you to reevaluate and pivot as you progress, which is why a big bang transformation should be reserved as an option of last resort. Every transformation strategy requires ongoing risk mitigation though and this is something you need to be constantly aware of. This blog post has been co-authored by our CTO Luke Morton, Technical Consultant Ben Pirt, and Content Consultant Joe Carstairs. It is part of a series that covers modernising legacy applications . We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . If you are interested in learning more about this topic, our team has released our ‘Modernising Legacy Applications In The Public Sector’ e-book which will empower you to define and implement the right approach for your organisation to make legacy applications a thing of the past. Download a free copy here. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-09-24"},
{"website": "Made-Tech", "title": "Delivery doesn’t finish in production", "author": [" Scott Edwards"], "link": "https://www.madetech.com/blog/delivery-doesnt-finish-in-production/", "abstract": "Our delivery teams at Made Tech fully embrace the devops culture and mindset, which means that we take on many more responsibilities across the software development lifecycle than just writing code. Apart from building well-engineered digital services, we also contribute to the provisioning and managing of the infrastructure that these services run on, the automation and running of deployments, and the handling of post-deployment monitoring and management. After all, if you built it you get to run it too! But software delivery is not only about providing technical outputs such as code, infrastructure, or deployment packages. As software engineers we are also responsible for focusing on the outcomes that we are contributing to in our day-to-day development work. Outcomes are a great way to measure the success of a delivery because they focus on the net effect that the software we release has on our citizens and users, rather than on pure technical deliverables such as code or provisioned infrastructure. The work doesn’t stop once a new release has been successfully pushed to production, as our teams also need to ensure that we have achieved these desired outcomes. We consider all of these post-deployment activities to be as important as the development work itself. In other words, delivery doesn’t finish in production. Measuring Outcomes Once a digital service has successfully gone live, we need to understand whether it is actually achieving the outcomes we aimed for. There are many ways to gather this information and we find it to be most effective when the whole team is involved in this process. There are, of course, several standard technical routes available to collect this information. Examples include anonymised web and data analytics, analysis of call center statistics, and real-time feedback capabilities built into the services themselves. But it can also come from more direct, personal sources. We find that user research with the citizens who use the digital services we build gives us access to a deep lake of useful feedback which we can use to measure the effectiveness of these services. There are several approaches that we find to be effective including (but are not limited to) surveys, feedback forms, testing directly with users, as well as partnerships on more formalised studies with user research organisations. We also gather feedback from the public sector service teams who interact directly with citizens in order to understand usage patterns. This information gives us a strong indication of whether our services are providing the value and outcomes that we expected them to. Apart from checking whether our services are meeting the needs of citizens, we also make sure to check that they are simple to navigate, easy to understand and accessible to as many people as possible. The guidelines in the UK government’s Service Standard are a good starting point for this sort of analysis, and we recommend that anyone with an interest in measuring their outcomes in the public sector reads through this standard. Evolution and Enhancement Software projects are akin to living entities in that they require constant nurturing throughout their lifecycle, including in the post-deployment phase. Once we have collated all of the data from the measurement phase, we have a set of high-quality data points which help us to understand whether the expected outcomes have been achieved. Following lean principles, we are able to use these learnings to feed back into the next delivery cycle. This helps us to ensure that we always measure before building. Additionally, it is sometimes necessary to consciously take on a bit of technical debt during the development lifecycle. We find that post-deployment is a good time to analyse if there are major debt items that need to be tackled. These items are especially important to consider before moving onto our next build phase, as letting technical debt pile up can lead to fragile systems that are hard to change and take longer than they should to enhance. We make this process easier for ourselves by ensuring that we keep a log of our decisions to take on technical debt during development. We then address this debt as soon and as often as possible. Doing so ensures that we consistently maintain the stability of the digital services that we build to serve our citizens. Technical debt, learnings from data, testing with users and strategic organisational aims all help to drive our future roadmap, both in terms of desired outcomes and technical deliverables. Monitoring – Performance and Proactivity The primary outcome we strive for is to build services which help public sector organisations to better serve citizens. While the delivery of features is important, a digital service that performs its task with slow lead times and low reliability is not a useful one. This means that our teams are responsible for ensuring that the services they build are both performant and reliable. While this is something that we focus on strongly during the development phase, it is also extremely important to make sure that these metrics are logged and monitored post-deployment, as this is the time when the system experiences real-world load levels and usage patterns. For example, we will alway ensure that we have automated logging, monitoring and alerting in place and that we consistently monitor these data streams post-deployment. Our preference is to build our monitoring with preemptive warnings in mind, meaning that we can often detect and resolve issues before they occur. This post-deployment focus on performance and reliability also extends back to the codebase. It is often easier to analyse your code with a fresh set of eyes once a production delivery is completed. This, paired with statistics gathered from monitoring mechanisms, gives us a strong set of data points to plan performance- and reliability-related codebase enhancements. You can find more detailed information about our approach to monitoring and production-readiness in our Productionisation Checklist . Ongoing Focus On Security And Privacy Public sector organisations have an extremely important responsibility to ensure that both the security and privacy of citizens are maintained throughout their I.T. infrastructure and services. Therefore, the developers who build these services must collaborate with security teams on an ongoing basis to make sure what they build is as attack-proof as possible. This responsibility can range from keeping support software and operating systems up to date with security advisories, to providing security breach monitoring, to implementing and deploying any required software-level security enhancements. Some of the methods we commonly use to maintain stringent security standards include: Building threat models to create a shared understanding of the risks we are trying to mitigate. Implementing automated dependency-vulnerability checking using tools such as Dependabot. Applying the principle of least privilege for cloud services and system access. Maintaining strong isolation wherever possible between networks, environments and containers. Bringing in external security teams to penetration test our services and infrastructure when necessary. Empowering In-House Development Teams From a customer’s point of view, there is always a danger when outsourcing development to external vendors that they will build your software with a “fire and forget” mentality. In these cases, your software becomes legacy software as soon as the vendor signs off delivery and moves on to their next engagement. This leaves you in a predicament where you are unable to maintain or enhance your digital services without either relying on the vendor who initially delivered them, or introducing risk to your organisation as you attempt to maintain them yourself. We believe that this (sadly common) approach is not only counter-productive but also extremely unscrupulous on the part of software vendors. The services we build affect many people’s lives every day and we have a responsibility to make sure that they are managed by teams who have a clear understanding of how to maintain and upgrade them, even if we are no longer directly involved. In an ideal scenario we prefer to work in collaboration with in-house developers during the development process, but this is not always possible. In either case, if we are moving onto other projects after we have managed a successful “final” production release, it is essential that we provide a full, comprehensive handover to the teams that will go on to maintain and upgrade the services we build. This is equally important both when handing over to in-house teams and to other external suppliers. Techniques for ensuring that this handover is managed smoothly and effectively can include the provision of sufficient (but not excessive) documentation, training sessions for the teams taking over the codebase, and formalised handover sessions. We also find that our focus on DevOps and automation simplifies the handover process. The fact that aspects such as automated regression testing and continuous delivery are implemented as part of our delivery typically lowers the volume of information required for handover, which lowers the cognitive load on the teams taking over the services we have built. Final Thoughts Our responsibilities as software delivery professionals extend well beyond the build and deployment phase of the software delivery lifecycle. When following the Build-Measure-Learn feedback loop, you should treat the deployment phase not as a final step, but rather as a stepping stone to the following phase of a successful delivery. This approach will allow you to go above and beyond as a delivery organisation and to consistently exceed your customers’ expectations. It enables you to deliver sustainable technology that achieves your desired outcomes, both now and in the future. We hope you enjoyed this post and found the content informative and engaging. We are always trying to improve our blog and we’d appreciate any feedback you could share in this short feedback survey . Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-05-06"},
{"website": "Made-Tech", "title": "Do users really need GOV.UK accounts?", "author": [" Kyle Chapman"], "link": "https://www.madetech.com/blog/do-users-really-need-gov-uk-accounts/", "abstract": "The GOV.UK team at the Government Digital Service (GDS) have recently announced they are looking at GOV.UK accounts . It’s in early stages, but their post discusses “centralised GOV.UK accounts” for “complex interactions” across many services. I’m an unashamed fan of GDS – I’ve been lucky enough to work with them in the past and I’ve made great use of their shared tools to build services ( Notify and the Design System to name a few), which have saved months (or years!) of re-work across the public sector. The idea of a shared account infrastructure, however, has me worried. Let’s look at the reasons why. I believe focusing on user needs has helped transform government digital over the last decade. It seems a small thing to say “our users are important”, but stating it as your first rule for good services helps keep it in focus when you’re making things in challenging environments. What’s the user need for a GOV.UK account? I believe that there are valid answers, but I also believe good services are verbs not nouns – and “Accounts” is definitely a noun. I would have loved to have seen this announced as “Helping repeat visitors to GOV.UK” or “Making it easier to complete groups of transactions”, because it would help broadcast a better way of working across government – putting user needs first, and recognising accounts might not always be the right way to meet those needs. Accounts considered harmful? There’s a lot of problems with accounts from a user perspective, especially when covering multiple services. For example, accounts generally burden users with a set of credentials to remember or store – more tech-savvy users may have password managers, but government services are for every level of digital literacy. Our friends at FutureGov have written about some of these frustrations – and there’s even a recommendation against accounts in GDS’ own guidance, backed up by user research. We recently worked with the Department for Education on a service for applying to teacher training , and users would sometimes need to complete their application over multiple visits. The service team decided to go with ‘magic links’ , saving returning users from the burden of signing up for an account. That team started with user needs, researched multiple possible solutions, and ended up with a better service because of it. I’m worried about a world where the standard is GOV.UK accounts – and teams like this are discouraged from considering the alternatives. Delivery under pressure It’s possible that this team has actually done all the discovery work and research to get them from user needs to accounts being the best solution – please share this work publicly if so! The prototype isn’t a public repo at the moment –  opening this up would be a great start. Sometimes accounts will be the right choice, sometimes they won’t – I’d love to see an expansion of the current guidance on accounts as part of this work, especially with reference back to research. I’m not on the ground with the GOV.UK Accounts team and I want to use the prime directive when thinking about others’ work – that is to say, I believe that the involved parties are doing the best given the various pressures they’re under. I hope this post can be read in the spirit it’s intended – as someone outside the team who has worries that others might have too. Do the hard work to make things better Finally – talking more to GDS in general rather than the GOV.UK team – I’m not convinced repeat visitors are the most pressing challenge facing government digital today. Almost every public sector body is facing the looming challenge of their legacy technology and building digital services in this context is really hard work. If you want to keep changing things for the better, get in the trenches. Build more exemplar services alongside government departments. Return to the strategy being delivery – putting users’ needs and a working first iteration of a service first. Take on the mountains of legacy technology – and then share back what you’ve learned to help guide the public sector as a whole. It worked the first time around off the back of building GOV.UK and it can work again. Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-10-08"},
{"website": "Made-Tech", "title": "Black History Month spotlight: Advice from Black Made Tech Engineers", "author": [" Ninamma Rai"], "link": "https://www.madetech.com/blog/black-history-month-spotlight-advice-from-black-made-tech-engineers/", "abstract": "Black History Month has been celebrated every October in the UK since 1987 – a month to celebrate the achievements and contributions of Black people throughout history. However, that doesn’t mean we should limit reflecting on Black history to one month a year – appreciating and understanding Black culture and experiences should be happening all year round! For this post, we were inspired by this wonderful blog by Webflow to highlight some of our Black colleagues in Made Tech. We spoke to 3 Black Engineers: Renny , Derek and Charlene , who have generously shared some advice based on their experiences. Do you have any advice for Black people in Tech, who keep finding that they’re the only Black person on the team? RENNY: This is a challenging situation to be in. I’ve been the only Black Software Engineer on a couple of projects before. It felt odd and lonely at first, but I hope these tips help. If you keep finding yourself being the only Black person on a team: Acknowledge it but don’t dwell on it, trail-blaze instead: If you can, raise it with whoever organises the team structure but don’t let it become a feeling that you ponder upon to the point of self-exclusivity from your team. If you’re the only Black person in the team because there aren’t a lot of Black people in the company or in the role you work, challenge yourself to think of yourself as a trailblazer. Learn from the experiences to inspire others too. Network outside of the team/company: Find networking events or meet-ups you can attend outside of work. I attended a few Black tech meet-ups and leadership conferences whenever I had the chance and these helped me a lot. It was nice to be able to meet other Black techies like me, learn from them, connect with them and hang out! The tech industry is still evolving when it comes to ethnic diversity, especially in technical roles like Software Engineers, DevOps, Tech Leads, etc. So I would like to encourage you: Try your best to have a positive mental attitude towards the situation and give yourself some consistent self-appreciation! Being the only black person in a team makes you more noticeable so do your best to SHINE and SHINE BRIGHT! DEREK: The advice I would give is if you can’t find a community at work then there are plenty of communities to join outside. There are amazing organisations built by Black people in tech to support and help your development. CHARLENE: In the last 10 years as a Software Developer, I’ve pretty much always been the only Black person on the team. Initially, I definitely felt as though I had to act differently to fit in at work. A piece of advice I would give is be yourself at work. There is no need to leave any part of yourself at home, bring all elements of who you are into your day job. If you feel as though you aren’t able to be yourself when you’re at work, then it probably isn’t an accepting and inclusive environment. Remember that you may have different experiences and cultural background, and that could be a huge benefit to the projects that you end up working on as that different perspective is a perspective of the people using applications. Do you have any advice for Black people who want to get into Tech? RENNY: • Do some self-reflection REGULARLY: The tech industry is pretty broad, and there are loads of opportunities in it. Some of these opportunities could even be created by yourself! Have regular conversations with yourself (yes, with yourself) to find out which areas of the tech interest you, which ones don’t and apply these to your decision making, e.g. Do you like to solve problems? Do you want to create visual interfaces? Do you like designing interfaces and interactions with the user? If you want to try out anything, for example, building a website, look for resources on how to get started and give it a go. After attempting it do some reflection, e.g. what did you like or not like? Was there anything that interested you that you want to look into a bit more? Should you try something more challenging or completely different, e.g. building a game. • Take ownership of your progress: You might not always have an idea of which technical role you want to get into, which is fine, but I strongly recommend having an idea of what you like to do or want to learn more about. Based on this, draw up action plans that suit you for things you want to learn/practice and for building up your creative portfolio. Take active responsibility for your learning and development. • Celebrate every step you’ve made, no matter the size: Getting into tech isn’t easy, so do your best to not exclude the tiniest achievements from your most significant breakthroughs. Every step counts. • Don’t be afraid to change direction: Try not to feel pressured into staying in a specific area of the tech industry just because you’re already in it, have experience in it or not sure you would do well somewhere else. Self-reflection should help to identify your interests and passions. So if these have changed for since you started your journey into tech, e.g. maybe you want to switch to user research from programming or perhaps you would like to start an enterprise with an idea you have. Take your time to decide but don’t be afraid to change direction! DEREK: Keep your skills up and never stop learning. Don’t get hung up on one particular path of entry there are many different ways in. Find strength in community whether it’s in your organisation or others in your field on Twitter, Facebook etc. I have had many days when I felt alone and that I didn’t belong. Try with all your might to ignore that. Your voice and skills are needed and belong. CHARLENE: If you want to get into tech, there are loads of platforms you can use to develop your skills and opportunities to get started. One thing that may make a difference and make you have a sense of belonging is joining Black tech communities. You’ll be able to build a network of Black people who are already in the tech industry who can provide mentorship and advice. Many tech communities also have partnerships organisations and bootcamps which may give you access to and information about companies and opportunities that you may not previously have considered. Some communities to take a look at: Coding Black Females UK Black Tech Black Devs UK BYP Network Coders of Colour YSYS Do you have any Black people in Tech who you admire and you think people should check out? RENNY: Here are few black people in tech, I think you should check out: Charlene Hunter : Founder of Coding Black Females . Callum Daniel: CEO of iCodeRobots . Jennifer Opal : DevOps Big Data Engineer. Ashleigh Ainsley FRSA : Co-Founder of Colorintech . Tiana S. Clark: Microsoft Leader DEREK: @lolaodelola on Twitter . She started an organisation called blackgirl.tech who helped to get Black women into tech via workshops and events. She is an incredible mentor, poet, coder and her timeline gives an honest insight into the tech world. CHARLENE: There are so many Black people in tech who are doing amazing things, the list is endless. As a starting point I would check out: Dionne Condor Farrell : Senior Developer and Founder of BAMEinTech Jennifer Opal : DevOps Big Data Engineer Siobhan Baker : Senior Software Engineer Mark Martin : Lecturer and Computer Science Lead Temi Olukoko : Software Developer Amina Aweis : Software Engineer Elle Hallal : Software Engineer This is definitely not an exhaustive list! If you have any more questions for Renny, Derek or Charlene, you can get in touch with them via Twitter or Linkedin below. Renny: Twitter Linkedin Derek: Twitter Linkedin Charlene: Twitter Linkedin Tweet 0 LinkedIn 0 Facebook 0", "date": "2020-11-11"}
]