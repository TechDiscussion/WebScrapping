[
{"website": "Arkency", "title": "Managing Rails Event Store Subscriptions ‚Äî How To", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/managing-rails-event-store-subscriptions-how-to/", "abstract": "Recently we got asked about patterns to manage subscriptions in Rails Event Store: @arkency Hiya, do you have any patterns for managing lots of subscriptions? i.e. adding to initializers file only scales for short time! It‚Äôs a very good question which made me realize how much knowledge there is yet to share from my everyday work. I took it as an opportunity to gather knowledge in one place ‚Äî chances are this question will be asked again in the future, thus a blog post in response. Subscription in Rails Event Store is a way to connect an event handler with the events it responds to. Whenever an event is published all its registered handlers are called. We require such handlers to respond to #call method, taking the instance of an event as an argument. By convention we recommend to start with a single file to hold these subscriptions. Usually this is an initializer: The idea for such glue file came straight from one on my favourite keynotes. Greg Young in his ‚Äú8 Lines of Code‚Äù talk presents a concept of a bootstrap method, which ties together various dependencies. It is a single place to look at to understand the relationships between collaborators. There is no ‚Äúmagic‚Äù in it, it is a boilerplate code. And such is best kept out of the code that really matters. At some point in project lifecycle the dependencies will differ in production as compared to development and test environments. In tests we prefer fake adapters to real ones for 3rd party services. So we substitute them in a bootstrap for appropriate environments. Having a different bootstrap method for test environment has an additional benefit of the possibility to disable particular handlers . Or quite the opposite ‚Äî very selectively enable them for the subset of integration tests when they‚Äôre most needed. Here we extracted a map of subscriptions to ApplicationSubscriptions bootstrap: Single file for subscriptions or a bootstrap method takes you this far. With sufficiently complex applications you will eventually discover many bounded contexts. Speaking of code, one way of representing bounded contexts in a monolithic application may be via modules . Below are some events from insuring context, defined in its own module. Now let‚Äôs take a sip of inspiration from Elm and scaling its architecture pattern . One can be found in an excellent elm-spa example. In short: Translating above example to Ruby and RES: Bootstrap method in a module to subscribe any Insurance handlers to events from external contexts and ApplicationSubscriptions collecting all module subscriptions it knows about That is the gist of it. I can imagine one could make module subscriptions to be discovered dynamically but the general idea is more or less the same. An ability to look on subscriptions not only as a handler-to-events but also as an event-to-handlers comes handy in some situations, most notably when debugging. We don‚Äôt yet have a tool yet in Rails Event Store ecosystem to help in such use cases. However my colleague made a following script to generate both mappings. Consider this to be a quick spike. With RubyMine code analysis and its Jump to Definition this actually becomes very handy when navigating the code. I would very much welcome this or similar tool in Rails Event Store for a broader audience. Something like rails routes but for subscriptions. Either in the console or in the browser . Who knows, maybe that could be your contribution ? üôÇ So far the code examples circulated around events. The very same ideas can be applied to commands and command handlers, with little help of command bus . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-24"},
{"website": "Arkency", "title": "Most controversial rules in Arkency", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/most-controversial-rules-in-arkency/", "abstract": "In random order: Of course most of these rules have their own: Do you find any of these rules particularly exciting, outraging or confusing? Let us know via email or twitter (you can reply to this tweet ). We appreciate your input. We‚Äôd like to keep elaborating on these points in coming blogposts. If you wanna stay in the loop subscribe to our newsletter . We believe these guidelines are very valuable and we‚Äôd like our prospective clients and teammates to be aware of the way we like to work. We already covered most of these topics in The Book or in our 26 other blogposts about async/remote . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-27"},
{"website": "Arkency", "title": "Rename stream in Rails Event Store with zero downtime", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/rename-stream-in-rails-event-store-with-zero-downtime/", "abstract": "A question has been posted on our Arkency‚Äôs slack today: How to rename stream in event store?\nIs link & delete_stream enough to do it?\nAsking for a friend ;) At first it looks quite easy. Sure, we can! Just link add events from source stream\nto target stream, then change publisher to write to new stream and use delete_stream method of Rails Event Store to remove old stream. Ahhh, and of course we must handle incoming domain events, with zero downtime. And now where the fun begins :) The concept is still the same: But having publisher constantly writing to source stream creates a few problems to handle. Source stream could not be just switched to target stream when all source stream‚Äôs events\nare linked to target stream. There could be a race condition and after we link last event\nand switch publisher to target stream new domain events could be published in source stream.\nThis of course will be bad as we could lose some domain events. So let‚Äôs use catchup subscription to ‚Ä¶ you know‚Ä¶ catch up with source stream and only then\nswitch to the target stream. This time concept is: But there is a catch. Race condition. Or I should say race conditions.\nThere are moments in code execution where we still miss some domain events. We need to have a lock on 2 critical operations here.\nFirst while fetching source stream events and making a switch\nof target stream on catchup process. And second while publisher fetches current stream from\ndata store and writes new events to it. I‚Äôve spent some time today to experiment how to implement this with the Rails Event Store.\nOr actually with Ruby Event Store. I do not need Rails for that, just pure Ruby. Let‚Äôs setup some basic objects This code will create a new instance of RubyEventStore::Client using in memory repository\nand NullMapper just to skip some friction. Then it defines a sample domain event class. So now our publisher (simulated): Is just a method that constantly publishes a new domain event (with index) to some stream.\nThe stream here will be the most interesting part here. Then the catchup process: As described it fetches some events from source stream and links them to target stream.\nIt stops when there is nothing more to read from source stream. Please notice the difference in: To give a catchup process a chance to finally catchup with source stream it must process\nevents a little faster than they are published by publisher.\nA TIME_UNIT is just a constant to define how fast you want this experiment to process events. To check if the me experimental code works I start 2 threads. First will be the publisher\n(executing the publish method). Second will be the catchup process (executing the catchup method). Before catchup process starts it will wait for some time - just\nto let publisher write some events to source stream. Then after catchup thread is finished I will\nwait again some time to let publisher publish a few more events - this time to target stream. And finally some assertion to check if target stream starts with all events from source stream. Here is the logic. The publish , catchup & link methods are called by\npublisher & catchup threads. The publish will always write domain event to\ncurrent stream. Notice that it uses the synchronized block to avoid race conditions.\nThe same lock is used by catchup method to avoid race condition where we read source stream\nand - if there is no more events to link - do the change of current stream. I‚Äôve used a Mutex class here to synchronize critical operations - but this is only experimental code, not production ready . In real life scenario the lock should depend on what kind of EventRepository you are using in your system. If you store your domain events in SQL database consider named locks to implement\na synchronization. There is still a race condition that some events may be skipped or added out of order\nwhen there are the events currently published in other transactions,\nwhich has not yet finished when catchup process has went through them. Here sample execution for TIME_UNIT = 10.0 : Looks very simple. But play a bit with it. It looks much more interesting when TIME_UNIT = 100000.0 . Now you could finally remove the source stream: BTW Neither link nor delete_stream does not affect any domain event in any way.\nStream is just a grouping mechanism for domain events. Once you write domain event to event store\nit could not be deleted (at least not without use of rails console :P). Code is fun! Go play with it! Here is the source of code spike for this blog post. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-27"},
{"website": "Arkency", "title": "Rails Event Store 1.0 ‚Äî The Past, Present and Future", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/rails-event-store-1-dot-0-the-past-present-and-future/", "abstract": "Rails Event Store is a Ruby library to persist, retrieve, publish and organize application architecture around domain events in an event-driven fashion. It‚Äôs not a database itself ‚Äî it is built on top of an existing (typically SQL) data store in the application. It makes a great foundation for CQRS, Event Sourcing and loosely coupled components in applications driven by domain events. We‚Äôve just reached 1.0 milestone ! It all is a part of ‚ÄúMy Ruby Story‚Äù. I‚Äôve joined Arkency in 2014 after a long time working as .NET developer.\nIt was a completely new environment for me. At that time I‚Äôve not known Ruby. I‚Äôve heard something about Ruby On Rails,\nbut I‚Äôve never have any experience with dynamic languages. You might ask how I was able to join experienced Ruby/Rails developers in Arkency?\nIt is the same way you could join us now. By bringing other valuable experiences,\nor knowledge that will make our work more efficient. My ‚Äúassets‚Äù was a knowledge of Domain-Driven Design and experience in developing web applications.\nI‚Äôve attended a Ruby User Group meetup in Wroc≈Çaw, when I‚Äôve explained to Ruby devs the concepts\nof DDD and in more details the Event Sourcing pattern. Two beers later I‚Äôve become a Ruby developer ;) The Rails Event Store prototype was ‚Äúborn‚Äù on one of our customer‚Äôs project. It was a way to solve the\nbusiness problem. It was very ‚Äúnaive‚Äù implementation, with a lot of problems. But the job has been done.\nIt has been working in production for a long time, gradually replaced by various Rails Event Store components. Piece by piece, until finally it has been replaced by Rails Event Store as a whole completely. The learnings from that migration have been\ndescribed by Pawe≈Ç in a blogpost . The first ‚Äúprototype‚Äù version of our event store has been only 248 lines of code.\nThe current version of Rails Event Store is much bigger, it has more features, it supports different\nuse cases and, what is really important, it is much better tested. The only thing that has not changed is the project philosophy. We build thing we need in our day to day work\non customer‚Äôs projects. Arkency client projects are to Rails Event Store what Basecamp is to Rails.\nWe learn, sometimes the hard way, we define new needs, we implement them in Rails Event Store and\nas always we share our experiences on Arkency‚Äôs blog . It‚Äôs just a milestone. In Rails Event Store we are using Semantic Versioning and\nwe follow the versioning guidelines defined by it. We have reached the point where the answer for question ‚ÄúHow do I know when to release 1.0.0‚Äù was ‚Äúoops, we should have done that some time ago‚Äù. Rails Event Store is already used in production. Not only by us\n‚Äî there is no trivial project in Arkency where Rails Event Store is not part of the solution. Also by other\ncompanies and software houses in their ventures. The API is stable and with each release we worry not to\nbreak other projects that use Rails Event Store as a dependency. This does not mean we now stop introducing changes. We will implement new things, also the ones\nthat will change the public API. We will be following the SemVer versioning guidelines and preparing comprehensive changelogs ‚Äî business as usual. The project philosophy does not change. Arkency is a consulting agency. We implement ourselves the features we need the most for successful client projects. And we‚Äôre hesitant when adding the ones not widely used in production systems. However Rails Event Store is bigger than us and is already used by dozens of companies . It is not uncommon to see other gems building on top of RES as well. We welcome any contribution that makes the project better for all and encourage experimenting . What we will focus on now? The key elements will be: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-27"},
{"website": "Arkency", "title": "Upgrading a trivial Rails app from Ruby 2.3.1 to 2.7 and from Rails 4.2.6 to 6.0.3", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/upgrading-a-trivial-rails-app-from-ruby-2-dot-3-dot-1-to-2-dot-7-and-from-rails-4-dot-2-dot-6-to-6-dot-0-dot-3/", "abstract": "This blogpost describes the upgrading process of a trivial Rails app. \nThe app is called PlusOne (it‚Äôs open-sourced ) and it‚Äôs a small but fun Slack bot (MS Teams support coming). This app doesn‚Äôt rely on any external gems, consists only of 3 database tables. It just stores who gave upvote to whom. \nAdditionally it shows some stats - who has how many points. Upgrading Ruby and Rails for such no-external-gems apps is mostly trivial, but there are some gotchas which I have documented\nalong the way. The actual initial trigger for those upgrades was the enforcement of Heroku to upgrade the stack. This required newer Ruby, so here we are. The app was still on Ruby 2.3.1 (and Rails 4), so I decided to make a small step first - upgrade to the latest Ruby from the 2.3 series - to the 2.3.8 version. This went smoothly. Bundle install went fine. All tests passed. Commit, push, deploy. Next step - upgrade to 2.4.10 - the most recent one in the 2.4 series. A typical problem while upgrading Ruby versions in Rails apps is the bundler version. Usually when you install recent Ruby versions, it comes with a new bundler gem. The fix is to uninstall the new one and install the required older one. In my case it was: gem uninstall bundler --version 2.1.4 gem install bundler --version 1.17.3 After a successful 2.4.10 upgrade, it was time to upgrade to the 2.5 series. At the moment of writing this post, the newest one is 2.5.8. This went smoothly, the same with going 2.6.3. At this stage some deprecation warnings appeared. It‚Äôs also here, where I had to make an upgrade from Rails 4 to Rails 5, before I upgrade to Ruby 2.7 easily. This showed up after upgrading to Rails 5.2.4.3 (newest one from Rails 5 series). UPDATE: As was noted on Reddit by matthewd , it might be better not to skip Rails 5.0 and 5.1, as they expose deprecations. In my app I could probably just disable Sprockets, but it‚Äôs not an obvious solution - I‚Äôd need to unpack railites/all to all the components manually. The alternative is a temporary hack: Which means we‚Äôre just creating an empty manifesto file. After doing this, my tests could finally be run. They failed with some errors, though: I‚Äôve had a number of such failures, all in Rails integration tests - the ones inheriting from ActionDispatch::IntegrationTest The issue is that now the API for simulating http requests has changed: Instead of: It needs to use named parameter params : After this fix, all my tests passed. During the Heroku deploy, though, a new problem appeared: In my case, it was a ‚Äúlegacy‚Äù piece of code in my Rakefile: It wasn‚Äôt really needed anymore, so I just dropped it. In more complex scenarios, heroku recommends a technique like this: During the production deployment (it‚Äôs a common pattern to discover problems only at production environment) I‚Äôve had another issue: As a hotfix I added this to Gemfile: and the deploy went fine. After this I was ready to try the newest Rails version - 6.0.3. It failed during tests with: The fix was fairly simple: in the Gemfile. That‚Äôs it. My app now works with Ruby 2.7 and Rails 6. Yay! You can see all commits here . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-05-12"},
{"website": "Arkency", "title": "Overcome 10k rows database limit on Heroku by upgrading the database plan", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/overcome-10k-rows-database-limit-on-heroku-by-upgrading-the-plan/", "abstract": "Over time I have collected quite a number of small Rails heroku apps. They usually start small, but overtime they hit the limit of 10k rows and it‚Äôs time to upgrade the database plan. Every time I do it, I hit the heroku documentation just to realize that their way of explaining doesn‚Äôt fit me well. This usually means that I then google a lot and only after 10 minutes I find what I look for. This blogpost is my attempt to make myself a quick summary of what needs to be done: And yes, Andrzej, if you read it in the future - you do need to provision a new database to overcome the 10k limit. There‚Äôs no way of just upgrading this limit on the UI with one button. Before I start I make sure that I don‚Äôt need to append the name of the app to each command, by: heroku git:remote -a myapp Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-05-05"},
{"website": "Arkency", "title": "Comparison of approaches to multitenancy in Rails apps", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/comparison-of-approaches-to-multitenancy-in-rails-apps/", "abstract": "Multitenancy means serving multiple independent customers from one app. Pretty typical for SaaS model.\nYou can implement it on several different levels: Here‚Äôs how they compare to each other: MySQL has no feature like PostgreSQL schemas, but MySQL databases can be used in a similar way. You don‚Äôt need to establish another connection to change the database in MySQL - you can switch via the use statement, similarly to what you‚Äôd do with PostgreSQL‚Äôs set search_path . You can also similarly mix data from different databases by prefixing the table names. The drawback is that in MySQL you need to make sure there‚Äôs no name collisions with other DBs. You also need to have create-database privileges to setup a new tenant. This can be a substantial difference if you don‚Äôt fully control the DB server. In case of PostgreSQL you only need the privilege to create new schemas inside your existing DB (and name collisions are constrained to it). This can work fine even on managed databases. These three options don‚Äôt constitute the whole spectrum of approaches. For example: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-05-26"},
{"website": "Arkency", "title": "Update Rails Event Store to v1.0.0 - walkthrough", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/update-rails-event-store-to-v1-dot-0-0-walkthrough/", "abstract": "Recently I‚Äôve posted a tweet: We are updating @RailsEventStore in our workshop reference app (and a base for exercises for Domain-Driven Rails book). Who knows what‚Äôs coming next :) You could purchase the book & get access here https://t.co/cKiPFfMio0 pic.twitter.com/o60o8CyoMC Since then we have published 2 more Rails Event Store versions. And we have finally reached a 1.0.0 milestone ! The process of the upgrade between versions is always described in release notes , but here I‚Äôve decided to summarise all changes required and to emphasize the most important ones. Here is nothing to update. No known (public) historical sources. No changelog. The origins of Rails Event Store are hidden in a private repository of one of our customers. It was born as a small tool to help to integrate with 3rd party systems. We‚Äôve started publishing domain events, have some subscribers that have been reacting to the published events. And most important we have started to store the published domain events. All in only 248 lines of code. This was a violent time, with separate repositories and frequent API changes. No prisoners have been kept, no deprecations warnings have been issued. Also licensing has not yet been clarified. Thankfully I do not need to update these versions. The strategy I have used to update Rails Event Store in our workshop application was simple: step by step, and prefer small steps. I‚Äôve always updated only to the next version, run bundler, and run all tests to check if all are green all the time. The update started here. The version of the rails_event_store in Gemfile was 0.14.3 . After cloning the application repository and make bundle install I‚Äôve needed to start with making test passing. The problem was the ClassyHash gem we are using in the application to define the schema of domain events. The version in Gemfile has not been specified. And in the meantime, the API of the ClassyHash.validate method has changed. It needs to be fixed. Having all test green I‚Äôve moved to update Rails Event Store. But this time it has been a piece of cake. Just update version in GemFile, bundle install & run the tests. Additionally, I‚Äôve added a rails_event_store-rspec gem and started using RSpec matchers provided by Rails Event Store in tests. Without issues and in ~2 hours I‚Äôve reached the 0.18.2 version. The biggest discovery in this Age Of Discovery was that there are no surprises here ;) The Modern Times has started with a big milestone - change of database schema (a.k.a V2 schema). The process of generating migration and running it is well described in v0.19.0 release notes but there are additional things to be beware of: After solving these issues the first version of the workshop application with the ‚Äúmodern‚Äù version of Rails Event Store was ready. The next noticeable difference (remember I update versions one by one) was 0.26.0 . With this version, I‚Äôve to change how subscriptions to events are defined because API has been changed. Also, I‚Äôve started using a new API that allows passing a proc/lambda as a subscriber. I‚Äôve replaced: with updated code: The 0.27.1 version allowed me to use Arkency‚Äôs command_bus gem, which it is from this version included in Rails Event Store. Also here you could no longer compare generated & stored domain event‚Äôs metadata because of change in metadata enrichment . With a 0.29.0 version, I was able to start correlating events using with_metadata method of RailsEventStore::Client . See more how to use it in the documentation . Also the RubyEventStore::Specification::Result has replaced previous reader API methods. All usages of: need to be replaced with new read API : This change could be done using provided migrator: Check the release notes for details. Another API change has been allowed by 0.31.1 . But this was just a rename, replacing append_to_stream with append and publish_event with publish . If you use link_to_stream it can be also changed here to link . The old deprecated here method names have been removed in 0.33.0 . With 0.34.0 a database migration was needed to add indexes for searching by event type & limit length of event_id field. And 0.35.0 comes with next data migration - to change data & metadata fields to binary The Rails Event Store 0.37.0 comes with redesigned aggregate_root gem. The aggregate objects should no longer have load and store methods but you should use AggregateRoot:Repository implement aggregate persistence. Instead of: you need to: or even better: All other versions up to 1.0.0 it‚Äôs just updating gem versions (remember to update also rails_event_store-rspec ) and checking if everything is ok by running tests. Version 0.40.0 : Version 0.31.0 : Version 0.28.0 : Version 0.27.0 : Version 0.19.0 : The issue with the additional index for SQLite goes away with the update to Rails 5.2. The support for this Rails version has been added in 0.28.0 . Currently Rails Event Store is tested with Ruby 2.4 , 2.5 & 2.6 (it works with 2.7 but there are issues with mutation testing) and with Rails 4.2 , 5.0 , 5.1 , 5.2 (it works with 6.0 but it is not yet included in test matrix). Read the ‚Ä¶ manual or call the developer‚Äôs police ü§£ Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-28"},
{"website": "Arkency", "title": "Practical use of Ruby PStore", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/practical-use-of-ruby-pstore/", "abstract": "Arkency blog has undergone several improvements over recent weeks. One of such changes was opening the source of blog articles . We‚Äôve have concluded that having posts in the open would shorten the feedback loop and allow our readers to collaborate and make the articles better for all. For years the blog has been driven by nanoc , which is a static-site generator. You put a bunch of markdown files in, drop a layout and on the other side out of it comes the HTML. Let‚Äôs call this magic ‚Äúcompilation‚Äù. One of nanoc prominent features is data sources . With it one could render content not only from a local filesystem. Given appropriate adapter posts, pages or other data items can be fetched from 3rd party API. Like SQL database. Or Github! Choosing Github as a backend for our posts was no-brainer. Developers are familiar with it. It has quite a nice integrated web editor with Markdown preview ‚Äî which gives in-place editing. Pull requests create the space for discussion. Last but not least there is octokit gem for API interaction, taking much of the implementation burden out of our shoulders. An initial data adapter looked like this to fetch articles looked like this: This code: Good enough for a quick spike and exploration of the problem. Becomes problematic as soon as you start using it for real . Can you spot the problems? For a repository with 100 markdown files we will have to make 100 + 1 HTTP requests in order to retrieve the content Making those requests parallel will only make the process of hitting request quota faster. Something has to be done to limit number of requests that are needed. Luckily enough octokit gem used faraday library for HTTP interaction and some kind souls documented how one could leverage faraday-http-cache middleware. Notice two main additions here: If only that cache worked‚Ä¶ Faraday ships with in-memory cache, which is useless for the flow of work one has with nanoc. We‚Äôd very much like to persist the cache across runs of the compile process. Documentation indeed shows how one could switch cache backend to one from Rails but that is not helpful advice in nanoc context either. You probably wouldn‚Äôt like to start Redis or Memcache instance just to compile a bunch of HTML! Time to roll-up sleeves again. Knowing what API is expected, we can build file-based cache backend. And there little-known standard library gem we could use to free ourselves of reimplementing the basics again. So much for standing on the shoulders of giants again. PStore is a file based persistence mechanism based on a Hash. We can store Ruby objects ‚Äî they‚Äôre serialized with Marshal before being dumped on disk. It supports transactional behaviour and can be made thread safe . Sounds perfect for the job! In the end that cache store turned out to be merely a wrapper on pstore. How convenient! Thread safety is achieved here by using Mutex internaly around transaction block. With persistent cache store plugged into Faraday we can now reap benefits of cached responses. Subsequent requests to Github API are skipped. Responses are being served directly from local files. That is, as long as the cache stays fresh.. Cache validity can be controlled by several HTTP headers . In case of Github API it is the Cache-Control: private, max-age=60, s-maxage=60 that matters. Together with Date header this roughly means that the content will be valid for 60 seconds since the response was received. Is it much? For frequently changed content ‚Äî probably. For blog articles I‚Äôd prefer something more long-lasting‚Ä¶ And that is how we arrive to the last piece of nanoc-github . A faraday middleware to allow extending cache time. It is a quite primitive piece of code that substitutes max-age value to the desired one. For my particular needs I set this value 3600 seconds. \nThe general idea is that we modify HTTP responses from API before they hit the cache. Then the cache middleware examines cache validity based on modified age, rather than original one. Simple and good enough. Just be careful to add this to middleware stack in correct order üòÖ And that‚Äôs it! I hope you found this article useful and learned a bit or two. Drop me a line on my twitter or leave a star on this project: Happy hacking! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-28"},
{"website": "Arkency", "title": "Avoid coupling between Bounded Contexts using Weak Schema", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/avoid-coupling-between-bounded-contexts-using-weak-schema/", "abstract": "The Rails Event Store comes with\na bounded_context gem\n(and a generator) that simplifies making your application modular.\nUsing the command: you can quickly generate folder structure, add load path and start working\non implementing your business logic without friction. You could see how we‚Äôve used this gem in our sample application showing how to use CQRS and Event Sourcing to implement a sample business process. The idea of bounded context is to have separate modules, with its own\nubiquitous language and with its concepts (you know, a Customer in\nordering context could be a DeliveryAddress in the shipping context, and a Customer in invoicing context may be a different concept than the one on ordering context). But we still need to communicate the business events between contexts. Because no\ncontext is an information silo. We build systems. We build things that cooperate.\nThat system cooperation is what makes the difference. This is where business\nprocesses are defined. Probably a lot of companies have similar ordering, invoicing\n& shipping contexts build in their system. Also, most of them are quite similar.\nAt least the non-core ones. Is there any e-commerce company that builds its\ninvoicing system? Yeah, I know Amazon might have one, but do you think it is\na reasonable thing to invest in building custom invoicing system by small e-commerce\nshop? Or it is better to buy access to an existing solution and invest in integrating it\ninto your business process? And that‚Äôs the place where the coupling is introduced. At least in our simple\n(sometimes too simple) sample application. So where the coupling is? Event sourced aggregates defined in modules (bounded contexts) are using\nmodule defined domain events for both - storing state changes (event sourcing) and\nfor communicating business events between system components (via Rails Event Store pub/sub).\nThe default configuration of RailsEventStore::Client uses a mapper with YAML serializer. Also the RailsEventStore::Event uses class name as event type.\nAnd here is the problem. Why there is a problem? This is coupling we have on several levels: There are tradeoffs, but we have deliberately made those\nchoices because of several reasons. The main of them was: You must not store as an internal state change of aggregate and publish outside of\nthe bounded context the same message (event). You could use the mailbox pattern,\nknown from Actor Model to handle\nincoming messages and a outbox pattern to communicate important business facts that have happened in the bounded context (module).\nThe events used to store aggregate state changes are now only internal\nimplementation of this module and must not be exposed outside of it.\nKeep them private in the scope of the module. This also means you could no longer use\na class name as an event type. This sample code is a definition of ‚Äúbusiness‚Äù events in separate modules: Overriding the event_type method will allow to identify the event and match\nit to different event‚Äôs classes in both modules. To do it you need to define\nevent class remapping in each module‚Äôs Rails Event Store configuration: But the domain event is not just a name (event type). Event if we decouple from event class\nwe still might have coupling on the event‚Äôs schema level. Here we have 2 events. In the beginning, they look different.\nThey have different class names, different schema - however they\nshare some attributes. As defined before these events share event\ntype. As a base class, I use here my implementation of base\nevent class, compatible with RailsEventStore::Event but\nallowing to define attributes using dry-schema and dry-types gems.\nYou could see the implementation of this base class here .\nThese events have a different schema. But the way they are defined allows\nusage of the Weak Schema technique. However to be albe to use the weak schema we need to change the serialization\nformat in Rails Event Store. YAML has been a really bad idea ;)\nFortunatelly for us it is very simple with Default mapper: BTW do you know that it is just a wrapper for a PipelineMapper and you could build your  mapper by composing any transformations you need?\nBut this is a story for a different post. There are some rules that you need to be aware of to use the weak schema.\nThe rules for mapping are simple. When reading the event from the event store,\nyou look at the serialized JSON and the event instance.\nAnd then the rules apply: You could read more about Weak Schema in Event Versioning book by Greg Young (available for free to read on LeanPub). With the use of dry-types attributes, we could also define coercion rules\n(i.e. replacing integers with strings) and define default values. The last coupling to avoid is the persistence & publishing of the domain events.\nI‚Äôve already mentioned the solution here.\nYou just don‚Äôt publish outside of your bounded context (module)\nthe internal events you use to persist state changes\nof the aggregates. This technique has several advantages: The separation could be done via physical separation of data in different data stores.\nIn this solution, each BC should have its private data store and specific\nRails Event Store configuration, and an additional Rails Event Store (or any other\npub/sub implementation that will support Weak Schema) as a communication interface\nbetween different bounded contexts. In a modular-monolith application, we could simplify this by using only a single instance\nof Rails Event Store and separate domain events on streams level. This will require\nmore reliance on conventions and discipline of the development team as there is no\nsuch restriction implemented in Rails Event Store. The mixed model is also possible. Separate instances of Rails Event Store with a wrapper\nfor EventRepository to force the convention by adding module prefix to stream names.\nThis way we still have single data store but each context (module) could only write\nto its streams and ‚Äúpublic‚Äù streams via the RES instance which is used to\ncommunicate between bounded contexts. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-05-06"},
{"website": "Arkency", "title": "Introducing EventStoreClient - a ruby implementation for EventStore DB", "author": ["Sebastian Wilgosz"], "link": "https://blog.arkency.com/introducing-eventstoreclient/", "abstract": "A guest post by Rails Architect Masterclass alumnus, Sebastian Wilgosz from Useo. Not so long ago, I‚Äôve been challenged by one of my clients to split a big, monolithic 10-year old rails application into a Domain-Driven Designed, microservice-based, event-sourced ecosystem of distributed applications. Not on my own of course, but still - it was quite a challenge. One of the key components was to design a communication channel for our services and after a lot of options checks, we‚Äôve decided to go with events as our Source Of Truth and eventual consistency for the whole ecosystem. To make a long story short, we‚Äôve created an EventStoreClient gem for Ruby . It‚Äôs a ruby client for HTTP communication with EventStore DB . And here is our why. When we checked options for implementing Event Sourcing with Ruby, we‚Äôve obviously met the Arkency‚Äôs RailsEventStore which is amazing and I use it a lot in my other projects. However, it‚Äôs designed for monolithic applications - not distributed systems with servers scattered around the world. There was an option to use distributed version of Rails Event Store , but it was in the very early stage at the moment and we weren‚Äôt sure how a rails-based event store will behave when we scale up too much. This forced us to look for other solutions out there on the web and surprisingly there were not too many of them. This is how we‚Äôve ended up using EventStore DB from Greg Young , a project that proved to be used in production by applications of all sizes and all kinds of traffic involved. There was a problem, though. There was no Ruby client for their API. There was - an Http Event Store from Arkency team . It was not maintained, however, as Arkency focused on supporting RailsEventStore. We could grab this project and continue from there, but under time pressure, we could not think too much about supporting backward compatibility or guides to upgrade for old projects - also, at the very beginning, my client was not sure if we want to have it open-sourced. That‚Äôs how we‚Äôve ended up with implementing the EventStoreClient - from scratch - to support 5.x version of EventStore DB. We‚Äôve been heavily inspired by the work Arkency did on both of their projects - http_event_store AND rails_event_store . If something is great, there is no need to invent the wheel again. As I‚Äôve already got used to the RailsEventStore gem, I wanted to keep the interface as similar as possible to make it easy to switch if needed (we‚Äôve already started to use RailsEventStore in the project for testing purposes). At the end of the day, the usage of this gem is quite similar: Defining an event Defining a handler Subscribing to the events If you‚Äôve got used to the RailsEventStore, this code will look very similar to you and that‚Äôs intentional. We use dry-rb to define events and we also have different mappers to support We mostly publish events via the transactional endpoints I‚Äôve described in the separate article not so far ago. For that, we inject the proper command_bus dependency into the transaction and then we call commands using an aggregate to control the business logic behind the scenes. It looks more or less like this: So again - this stuff is pretty much what you‚Äôd probably do when you‚Äôve ever worked with RailsEventStore. Under the hood, you connect with the EventStore database via the HTTP connection. I‚Äôve tried to keep the interface agnostic of which kind of client it uses, so there is a bit of code duplication, where you have similar sets of methods in the EventStoreClient::StoreAdapter::Api::Client class and the EventStoreClient::Client class. The most important class being an interface to everything inside is the base EventStoreClient::Client class. It implements all methods to communicate with the EventStore API to allow using subscriptions, publishing events, reading from a stream, and so on. Most of it is just a delegation to the given adapter, like here: However, there are some additional tricks, like implementing the poll method which sends a request to the Event Store to get new events for all subscriptions we have in the service. The EventStoreClient is easily configurable by using the EventStore::Configuration.instance - an instance of the configuration class defined using the singleton pattern. Easy stuff and simple in use. We‚Äôve tried to keep everything framework-agnostic, however, we use it in Rails applications only so far, so it‚Äôd not been proved yet that we‚Äôd succeeded in that field. As we‚Äôve been concerned about the security and all the GDPR requirements, we‚Äôve also developed a way to encrypt/decrypt events by injecting the encryption key repository. You can configure it easily by just replacing the default mapper: It also had been inspired by the EncryptedMapper implemented in RailsEventStore, but here we‚Äôve been forced to improve the performance of it - which I can proudly say that we‚Äôve succeeded in it. I‚Äôll write more about that soon, as It‚Äôs an extremely interesting topic. When we‚Äôve implemented this thing, we‚Äôre in the process of intense learning. We needed to learn how the EventStore works in details, but also understand all the Event Sourcing and Microservice weirdos - all stuff that is completely different than in monolithic applications. At the same time, the clock was ticking - as usual when we talk about applications that should generate income. At the end of the day, we‚Äôve prepared a Minimal Viable Product - a gem that allowed us to go out and deliver a feature to production. However, we‚Äôve made some mistakes that are already on our schedule to be improved and some of the functionalities were just not implemented due to the lack of urgent need. Here is a list of topics that can be improved to make this gem much more useful than it is right now. Microservice architecture is a really, really interesting topic and I‚Äôm very happy having a chance to work with it. It puts challenges in front of our team every day and I love it as well. However, to go into the microservices, you should really know your WHY. Do you know your WHY? Why do you work on microservices OR the monolith? Why not the other one? I‚Äôll leave it for you to think about. By the way, CONTRIBUTIONS WELCOME ! Have comments? Reply under this tweet or ping us on twitter - @sebwilgosz @arkency . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-06-16"},
{"website": "Arkency", "title": "Rails multiple databases support in Rails Event Store", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/rails-multiple-databases-support-in-rails-event-store/", "abstract": "Rails 6 released in August 2019 has brought us several new features .\nOne of the notable changes is support for multiple databases . To make the story short, to use multiple databases you need to: All details have been described in Rails guides and I‚Äôve already read several blog posts describing how to do it. But how to use this feature to allow Rails Event Store data to be stored in a separate database? I‚Äôve started with a new Rails 6 application. I‚Äôve generated this application using RES application template : The template generated all the files and setup needed to start using Rails Event Store in the Rails application. All I needed to do was to define the database configuration: And the base class for Rails Event Store Active Record models: That should be enough‚Ä¶ but‚Ä¶ The rails_event_store_active_record gem (a part of the whole package of gems installed when you require rails_event_store )\nhas defined models for its data model. In Rails Event Store 1.0 it is defined like this : The hardcoded ActiveRecord::Base class prevented me from the use of a new base class with a defined database setup. So I‚Äôve started experimenting. The goals were: The implementation is conceptually quite easy: And here is the code: Please notice that I‚Äôve used Object.const_set to build model classes.\nThat is required because active_record_import gem needs the model classes to\nbe constants. And finally with all these changes I‚Äôve made a small change in RES setup - pass the\nnew base class to event repository to allow the event store models to use the database setup\ndefined in it. All this allowed me to separate event store data (domain events) from the rest of the\napplication data - other models that still inherit from the ApplicationRecord model. Having 2 databases leads of course to another issue. Your data are now distributed.\nYou cannot have a database transaction that will span across these 2 databases.\nNo more transactional changes in application data and the event store. This forces you to use only asynchronous event handlers. Unfortunately RES 1.0\nuses ImmediateAsyncDispatcher (actually it is a ComposedDispatcher with 2 dispatchers, async & sync one).\nFortunately, this is easy to change - and with multiple databases, this should be your new default: You might have noticed the instance_uuid used to generate EventRepository‚Äôs model class names.\nEach instance of EventRepository will generate new constants with different names.\nThis allows for having several instances of EventRepository - each with a separate database.\nI could now start experimenting with separate Rails Event Stores - 1 for application, and 1 for each Bounded Context\ndefined in the domain. And all of them could be separated. Only commands & domain events could be used to communicate between them.\n‚Ä¶ but that‚Äôs a story for another blog post ;) Another way to use this feature could be having separate write & read databases in Rails Event Store.\nBut this requires more changes in EventRepository . Also with upcoming Rails release new features could be added, some like horizontal sharding could be interesting for my future experiments here :) This code has not been released yet. You can join me in these experiments - just post your comments to my code on Github on multiple databases repository sample app or talk to me on twitter . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-06-17"},
{"website": "Arkency", "title": "Don't blindly apply software patterns", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/dont-blindly-apply-software-patterns/", "abstract": "I went for a run today and I was catching up with some podcast episodes today\nand I would like to share my comments to the great episode about sagas & process\nmanagers published by Mariusz Gil in the Better Software Design . Mariusz has been talking with Kuba Pilimon .\nThis was the third episode when these devs have discussed how to design software using Domain Driven Design\ntechniques & design patterns. (The podcast is in Polish but some episodes - like the inverview with Alberto Brandolini are recorded in English). I‚Äôve listened to this podcast and the overall discussion is very interesting but\nI have some remarks: Mariusz & Kuba have dicsussed the saga pattern based on the example of cinema seats reservations.\nThe model is simple - each Seat is an aggregate and to book 4 seats you need to have a saga\nthat will ensure that all 4 reservations are processed or all of them will be revoked by compensating actions. The example was as follows (pardon the pseudocode): This looks so simple - we have 2 processes (sagas). Each of them tries to book some\nseats. The first wins. The other one runs its compensating action to release already\nbooked seats. But the reality might not be that simple. There are only two hard problems in distributed systems: 2. Exactly-once delivery 1. Guaranteed order of messages 2. Exactly-once delivery Having that in mind we could imagine situation like this: What will be the result? Process A could not complete the saga - because seat A-3 is already booked. Process B could not complete its saga because seat A-4 is already booked. Both are starting its\ncompensating actions and release all bookings. With some bad luck we could end up with seats that will\nnot be sold even when there was a huge demand - translating to business terms: diappointed customers\nand lost revenue. Another example I would like to comment is the most common example of saga pattern usage.\nBooking a plane, a hotel and a car and releasing the bookings when one of these has failed. Mariusz & Kuba have discussed several scenarios how this could be extended. But what I‚Äôve missed here,\nand what is always an issue for me when I read/hear this example is: When you book plane, hotel & car and the car is not available what are your expectations?\nI could be wrong - but I would expect that my plane & hotel is booked and then the system will\nask me what to do with missing car reservation. Definitelly I would not like the application\nto cancel my plane & hotel bookings when the car is not available! Just ask your business/domain expert. They probably handle this kind of situations\nevery day, business as usual. Don‚Äôt blindly apply software design patterns. Talk to people.\nSolve real world problems. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-06-23"},
{"website": "Arkency", "title": "Painless Rails upgrades", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/painless-rails-upgrades/", "abstract": "Sooner or later your Rails application will require an upgrade of the framework itself. There are many reasons behind that. Bugs, incompatibility with modern libraries, or the worst: the version you use will no longer receive security updates. Living on the edge might be tempting, but it can also end badly for the business which relies on the application. User data leak, frauds, this all can simply lead to serious legal and financial issues. At Arkency we mostly work with legacy applications. Customers that we tend to cooperate with are successful businesses. They often need assistance with improving existing codebase, implementing new business features, and rather technical tasks like upgrading the Ruby & Rails version. It‚Äôs not a rare case when they hit the wall and can‚Äôt progress with their app because of outdated libraries. You might think: it‚Äôs easy, just fasten your seatbelts and run bundle update üôà I must disappoint you, it usually doesn‚Äôt work that way. It‚Äôs rather: Even if you don‚Äôt run into could not find compatible versions and it will work somehow, you may run into: I must disappoint you, I don‚Äôt have such. However, as Arkency , we successfully helped numerous clients in this area. We have well-established practice for doing so. I will show you how to prepare for Rails upgrade, what to avoid while developing your application and how to write code which won‚Äôt bother you while upgrading the Rails itself. It‚Äôs simpler than you think. Your next upgrade will end up in bumping Rails version number in Gemfile and running bundle update rails. Just follow the rules below and everything will be fine. Keep your app test coverage on a high level. Static analysis tools won‚Äôt tell you whole truth, especially rake stats saying that you have 140% coverage if test/code ratio is high enough. They can be helpful, give you some basic information, but you can‚Äôt fully really on them. If you want to be sure about your coverage you should dive into mutation testing . It‚Äôs good to know about errors happening in the app. Especially this might be crucial if you pushed some significant changes to production. But let‚Äôs stay here for a moment. Are errors the only sign of something bad happening in the application? My experience tells me that the app might now work properly, eg. checkout flow is completely not working but no error will appear in bug tracker. Track your business metrics. Get notified if the payments level drops significantly. Maybe people can‚Äôt finalize checkout, but you‚Äôre not aware and no error happens because something is wrong on the frontend only? Push often, release often. Let each gem bump land on production as a separate release. You will instantaneously realize if something is not working as expected because you monitor how things are going. It‚Äôs easier to revert single commit than pull request with gazillion of changes. You‚Äôll probably spot the problem easier if single, atomic change is taken into account, rather than multiple ones. Always leave the campground cleaner than it was ‚Äì it says. You‚Äôve probably heard it before in s/campground/code version. If you work in given area of code, improve it. It uses outdated library, bump it upfront to make further Rails upgrade easier. You can bump your Ruby version even today, it‚Äôs usually easier than upgrading the Rails itself. Don‚Äôt try upgrading both Ruby and Rails in a single step since it may hit you hard. At least it can confuse you and waste your time. It covers most of the topics you have to be aware of when bumping your framework version. It can be good starting point for creating backlog tickets and planning the upgrade. Each Rails version has a dedicated page with Release Notes . It usually dives into details more than Upgrade guide mentioned in previous paragraph. It covers Removals , Deprecations and Notable Changes . Check their compatibility with your desired Rails version. Maybe a library is no longer maintained? Maybe someone forked it and supports modern Rails ? It‚Äôs good to know such things upfront. Digging into GitHub issues, especially the open ones is also a good idea. Gem maintainers not always put everything into CHANGELOG , sometimes the issues become the only source of knowledge. Don‚Äôt wait until the feature will be removed or completely changed. This can be a huge blocker for you, trust me. When you see a deprecation, fix it or at least put it into backlog to make rest of your team aware and not forget about it. Put all the knowledge you‚Äôve gained up to that point into backlog. Probably you won‚Äôt be working alone on that. Discuss it with the team and communicate with management about potential issues if you see any. Over-communication might be a key to success. Gems monkey patching ActiveRecord can be really harmful to your maintenance process. Everything goes fine unless it no longer does. Do you remember protected_attributes ? It was extracted from Rails and maintained by core team, then it was no longer maintained and someone renamed it to protected_attributes_continued . Now it‚Äôs payback time, it won‚Äôt work with Rails 6 . I‚Äôve seen many stories similar to this. All those state machine gems relying highly on callbacks, blocking applications for upgrades for months, or even years. Go for tactical DDD patterns for your core domain. Modularize your code, extract Bounded Contexts . Use Rails where they shine: ApplicationController , ActiveRecord used for writes and reads without the callback hell and STI. We‚Äôve shown you the alternative approach many times: commands , service objects , process managers , etc. Believe us, your next upgrade will be just a matter of Rails version bump in your Gemfile . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-07-03"},
{"website": "Arkency", "title": "People's experiences with approaches to multitenancy", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/peoples-experiences-with-approaches-to-multitenancy/", "abstract": "Recently I‚Äôve been researching the topic of multitenancy in Rails. You might have already seen the previous blogpost comparing approaches to multitenancy . In the meantime there was another discussion about multitenancy on HN which pretty much blew up. It means that a lot of people have experiences in the topic and their opinions were strong enough to go and post about them. I don‚Äôt typically read all the comments on HN, but this time I wanted to go through it to know people‚Äôs experiences on the topic I was researching. This was the original question: Ask HN: Has anybody shipped a web app at scale with 1 DB per account? A common way of deploying a web application database at scale is to setup a MySQL or Postgres server, create one table for all customers, and have an account_id or owner_if field and let the application code handle security. This makes it easier to run database migrations and upgrade code per customer all at once. I‚Äôm curious if anybody has taken the approach of provisioning one database per account? This means you‚Äôd have to run migrations per account and keep track of all the migration versions and statuses somewhere. Additionally, if an application has custom fields or columns, the differences would have to be tracked somehow and name space collisions managed. Has anybody done this? Particularly with Rails? What kinda of tools or processes did you learn when you did it? Would you do it again? What are some interesting trade offs between the two approaches? The original question asks about db-level multitenancy, but a lot of answers relate to schema-level as well (which people also often explicitly related to). Also schema-level in Postrgres has a lot in common with db-level in MySQL as you can read in my previous blogpost . Below are some quotes that drew my attention, extracted to save you some time. They seem to be pretty harsh on db/schema-level approach at the begginning, with more balanced takes coming in later parts of the discussion. All the credit goes of course to original comments‚Äô authors. You can easily find them by grepping the original discussion page . *** My startup currently does just this ‚Äòat scale‚Äô, which is for us ~150 b2b customers with a total database footprint of ~500 GB. We are using Rails and the Apartment gem to do mutli-tenancy via unique databases per account with a single master database holding some top-level tables. This architecture decisions is one of my biggest regrets, and we are currently in the process of rebuilding into a single database model. FWIW, this process has worked well for what it was originally intended to do. Data-security has a nice db level stopgap and we can keep customer data nicely isolated. It‚Äôs nice for extracting all data from a single customer if we have extended debugging work or unique data modeling work. It saves a lot of application layer logic and code. I‚Äôm sure for the most part it makes the system slightly faster. However as we have grown this has become a huge headache. It is blocking major feature refactors and improvements. It restricts our data flexibility a lot. Operationally there are some killers. Data migrations take a long time, and if they fail you are left with multiple databases in different states and no clear sense of where the break occurred. Lastly, if you use the Apartment gem, you are at the mercy of a poorly supported library that has deep ties into ActiveRecord. The company behind it abandoned this approach as described here: https://influitive.io/our-multi-tenancy-journey-with-postgres-schemas-and-apartment-6ecda151a21f *** Echoing this as well, I worked for Influitive and was one of the original authours of apartment (sorry!) There are a lot of headaches involved with the ‚Äútenant per schema‚Äù approach. Certainly it was nice to never have to worry about the ‚Äúcustomer is seeing data from another customer‚Äù bug (a death knell if you‚Äôre in enterprisish B2B software), but it added so many problems: Migrations become a very expensive and time-consuming process, and potentially fraught with errors. Doing continious-deployment style development that involves database schema changes is close to impossible without putting a LOT of effort into having super-safe migrations. You‚Äôll run into weird edge cases due to the fact that you have an absolutely massive schema (since every table you have is multiplied by your number of tenants). We had to patch Rails to get around some column caching it was doing. Cloud DB hosting often doesn‚Äôt play nice with this solution. We continually saw weird performance issues on Heroku Postgres, particularly with backup / restores (Heroku now has warnings against this approach in their docs) It doesn‚Äôt get you any closer to horizontal scalability, since connecting to a different server is significantly different than connecting to another schema. It will probably push the need for a dedicated BI / DW environment earlier than you would otherwise need it, due to the inability to analyze data cross-schema. I still think there‚Äôs maybe an interesting approach using partioning rather than schemas that eliminates a lot of these problems, but apartment probably isn‚Äôt the library to do it (for starters, migrations would be entirely different if partioning is used over schemas) *** I agree that migrations are painful at the best of times, but dealing with the complexity of migrating a single database is far simpler than dealing with migrating hundreds of schemas: Migrations will first of all just take longer - you‚Äôre multiplying the number of schema changes by the number of tenants you have. While in an ideal world migrations should be purely run within a transaction, occasionally performance considerations mandate that you run without DDL transactions - when some tenants fail and your migrations are in a partially completed state for some of your tenants and not others, it can be scary and painful. In my experience, almost no one approaches data migrations in a way that is purely backwards compatible 100% of the time without exception. You certainly can, but there‚Äôs a significant tax associated with this, and if you‚Äôre in a traditional scoped environment, you can often get away with the potential for errors in the minuscule time that a schema change is operating (of course, some schema changes aren‚Äôt run in minuscule times, but those are the ones you‚Äôre more likely to plan for) Going read only during migrations is an interesting approach, but there‚Äôs real business costs associated with that (particularly if your migration speed is multiplied by running it across tenants). I don‚Äôt want to say that you should never isolate data on a schema level, but I do think it‚Äôs something that shouldn‚Äôt be a standard tool to reach for. For the vast majority of companies, the costs outweigh the benefits in my mind. *** Can confirm, here be dragons. I did a DB per tenant for a local franchise retailer and it was the worst design mistake I ever made, which of course seemed justified at the time (different tax rules, what not), but we never managed to get off it and I spent a significant amount of time working around it, building ETL sync processes to suck everything into one big DB, and so on. Instead of a DB per tenant, or a table per tenant, just add a TenantId column on every table from day 1. *** I do both. Have a tenant_id column in every table. This gives me flexibility to either host each client separately or club them together. *** How does the architecture block major refactors or improvements? Are you running a single codebase for all your tenants, albeit with separate schemas for each? Here I‚Äôll give you one: If you want to change a property of the database that will give your specific use improved performance, you have no way to transactionally apply that change. Rolling back becomes a problem of operational scale, rolling out as well. What if you need to release some feature, but that feature requires a database feature enabled? Normally you enable it once, in a transaction hopefully, and then roll out your application. With this style you have to wait for N database servers to connect, enable, validate, then go live before you can even attempt the application being deployed, much less if you get it wrong. *** For me it falls into the category of decisions that are easy to make and difficult to un-make. If for whatever reason you decide this was the wrong choice for you, be in tech needs (e.g. rails) or business needs, merging your data and going back into a codebase to add this level of filtering is a massive undertaking. *** Indeed, but if you are making a B2B enterprise/SMB SaaS, I think you are most likely to regret the opposite choice [1][2]. A lot of companies run a single instance, multitenant application and have to develop custom sharding functionality down the road when they realize the inevitable: that most joins and caches are only needed on strict subsets of the data that are tenant specific. If you get successful enough in this type of application space, you reach a mature state in which you need to be able to: Run large dedicated instances for your largest customers, because either their performance or security requirements mandate it. Share resources among a large number of smaller customers, for efficiency and fault tolerance reasons. You can get there in two ways: You start with a massive multi tenant application, and you figure out a way to shard it and separate in pieces later. You start with multiple small applications, and you develop the ability to orchestrate the group, and scale the largest of them. I would argue the latter is more flexible and cost efficient, and requires less technical prowess. *** I‚Äôve managed a system with millions of users and tens of billions of rows, and I always dreamed of DB per user. Generally, ~1% of users were active at a given time, but a lot of resources were used for the 99% who were offline (eg, indexes in memory where 99% of the data wouldn‚Äôt be needed). Learned a few tricks. If this is the problem you‚Äôre trying to solve, some tips below. *** You can always take a multi-tenant system and convert it into a single-tenant system a lot more easily. First and foremost, you can simply run the full multi-tenant system with only a single tenant, which if nothing else enables progressive development (you can slowly remove those now-unnecessary WHERE clauses, etc). True, but: In my experience by the time you reach this point you have a lot of operational complexity because you and your team are used to your production cluster being a single behemoth, so chances are it‚Äôs not easy to stand up a new one or the overhead for doing so is massive (i.e. your production system grew very complex because there is rarely if ever a need to stand up a new one). Additionally, a multi tenant behemoth might be full of assumptions that it‚Äôs the only system in town therefore making it hard to run a separate instance (i.e. uniqueness constraints on names, IDs, etc). *** If an ‚Äúaccount‚Äù is an ‚Äúenterprise‚Äù customer (SMB or large, anything with multiple user accounts in it), then yes, I know at least a few successful companies, and I would argue in a lot of scenarios, it‚Äôs actually advantageous over conventional multitenancy. The biggest advantage is flexibility to handle customers requirements (e.g. change management might have restrictions on versioning updates) and reduced impact of any failures during upgrade processes. It‚Äôs easier to roll out upgrades progressively with proven conventional tools (git branches instead of shoddy feature flags). Increased isolation is also great from a security standpoint - you‚Äôre not a where clause away from leaking customer data to other customers. I would go as far as saying this should be the default architecture for enterprise applications. Cloud infrastructure has eliminated most of the advantages of conventional multitenancy. If an account is a single user then no. PS: I have a quite a lot of experience with this so if you would like more details just ask. *** Yes, for multi-tenancy. Database per tenant works alright if you have enterprise customers - i.e. in the hundreds, not millions - and it does help in security. With the right idioms in the codebase, it pretty much guarantees you don‚Äôt accidentally hand one tenant data belonging to a different tenant. MySQL connections can be reused with database per tenant. Rack middleware (apartment gem) helps with managing applying migrations across all databases, and with the mechanics of configuring connections to use a tenant based on Host header as requests come in. *** Not a problem with MySQL, ‚Äúuse tenant ‚Äù switches a connection‚Äôs schema. Rails migrations work reasonably well with apartment gem. Never had a problem with inconsistent database migrations. Sometimes a migration will fail for a tenant, but ActiveRecord migrations records that, you fix the migration, and reapply, a no-op where it‚Äôs already done. *** Using schemas gives you imperfect but still improved isolation. It‚Äôs still possible for a database connection to cross into another tenant, but if your schema search path only includes the tenant in question, it significantly reduces the chance that cross-customer data is accidentally shared. *** It‚Äôs a terrible idea in the same way that using PHP instead of Rust to build a production large scale application is a terrible idea (i.e. it‚Äôs actually a great idea but it‚Äôs not ‚Äúcool‚Äù). It‚Äôs not a cool factor issue. It‚Äôs an issue of bloating the system catalogs, inability to use the buffer pool, and having to run database migrations for each and every separate schema or maintaining concurrent versions of application code to deal with different schema versions. *** As you can see now that the thread has matured, there are a lot of proponents of this architecture that have production experience with it, so it‚Äôs likely not as dumb as you assume. *** As you can see now that the thread has matured, there are a lot of proponents of this architecture that have production experience with it, ‚Ä¶ Skimming through the updated comments I do not see many claiming it was a good idea or successful at scale. It may work fine for 10s or even 100s of customers, but it quickly grows out of control. Trying to maintain 100,000 customer schemas and running database migrations across all of them is a serious headache. ‚Ä¶so it‚Äôs likely not as dumb as you assume. I‚Äôm not just assuming, I‚Äôve tried out some of the ideas proposed in this thread and know first hand they do not work at scale. Index page caching in particular is a killer as you lose most benefits of a centralized BTREE structure when each customer has their own top level pages. Also, writing dynamic SQL to perform 100K ‚Äú‚Ä¶ UNION ALL SELECT * FROM customer_12345.widget‚Äù is both incredibly annoying and painfully slow. *** I don‚Äôt think we share the definition of ‚Äúscale‚Äù. Extremely few companies that sell B2B SaaS software for enterprises have 10K customers, let alone 100K (that‚Äôs the kind of customer base that pays for a Sauron-looking tower in downtown SF). Service Now, Workday, etc, are publicly traded and have less than 5000 customers each. All of them also (a) don‚Äôt run a single multitenant cluster for all their customers and (b) are a massive pain in the ass to run in every possible way (an assumption, but a safe one at that!). *** In the past I worked at a company that managed thousands of individual MSSQL databases for individual customers due to data security concerns. Effectively what happened is the schema became locked in place since running migrations across so many databases became hard to manage. I currently work at a company where customers have similar concerns around data privacy, but we‚Äôve been to continue using a single multitenant DB instance by using PostgreSQL‚Äôs row level security capabilities where rows in a table are only accessible by a given client‚Äôs database user: https://www.postgresql.org/docs/9.5/ddl-rowsecurity.html We customized both ActiveRecord and Hibernate to accommodate this requirement. *** I am aware of at least one company which does this from my consulting days, and would caution you that what you get in perceived security benefits from making sure that tenants can‚Äôt interact with each others‚Äô data you‚Äôll give back many times over with engineering complexity, operational issues, and substantial pain to resolve ~trivial questions. I also tend to think that the security benefit is more theatre than reality. If an adversary compromises an employee laptop or gets RCE on the web tier (etc, etc), they‚Äôll get all the databases regardless of whose account (if any) they started with. (The way I generally deal with this in a cross-tenant application is to ban, in Rails parlance, Model.find(‚Ä¶) unless the model is whitelisted (non-customer-specific). All access to customer-specific data is through @current_account.models.find(‚Ä¶) or Model.dangerously_find_across_accounts(‚Ä¶) for e.g. internal admin dashboards. One can audit new uses of dangerously_ methods, restrict them to particular parts of the codebase via testing or metaprogramming magic, etc. *** This is true if your application is running on a shared servers - however if you have fully isolated application and database deploys then you really do benefit from a security and scalability perspective- and by being able to run closer to your clients. I‚Äôd also say that it works better when you have 100s, rather than thousands of clients, most probably larger organisations at this point. *** For Postgress you can use and scale one schema per customer (B2B). Even then, depending on the instance size you will be able to accommodate 2000-5000 customers at max on a Postgres database instance. We have scaled one schema per customer model quite well so far ( https://axioms.io/product/multi-tenant/ ). That said, there are some interesting challenges with this model like schema migration and DB backups etc. some of which can be easily overcome by smartly using workers and queuing. We run migration per schema using a queue to track progress and handle failures. We also avoid migrations by using Postgres JSON fields as much as possible. For instance, creating two placeholder fields in every table like metadata and data. To validate data in JSON fields we use JSONSchema extensively and it works really well. Probably you also need to consider application caching scenarios. Even you managed to do one database per customer running Redis instance per customer will be a challenge. Probably you can run Redis as a docker container for each customer. *** I worked for a company that did this, we had hundreds of database instances, one per customer (which was then used by each of those customers‚Äô employees). It worked out pretty well. The only downside was that analytics/cross customer stats were kind of a pain. The customers all seemed to like that their data was separate from everyone else‚Äôs. This never happened, but if one database was compromised, everyone else‚Äôs would have been fine. If I were starting a B2B SaaS today where no customers shared data (each customer = a whole other company) I would use this approach. *** It has been an actual requirement from our customers that they don‚Äôt share an instance or database with other customers. It also seriously limits the scope of bugs in permissions checks. Sometimes I will find a bit of code that should be doing a permissions check but isnt which would be a much bigger problem if it was shared with other companies. *** As one example, New Relic had a table per (hour, customer) pair for a long time. From http://highscalability.com/blog/2011/7/18/new-relic-architecture-collecting-20-billion-metrics-a-day.html (2011): Within each server we have individual tables per customer to keep the customer data close together on disk and to keep the total number of rows per table down. *** I‚Äôve maintained an enterprise saas product for ~1500 customers that used this strategy. Cross account analytics were definitely a problem, but the gaping SQL injection vulnerabilities left by the contractors that built the initial product were less of a concern. Snapshotting / restoring entire accounts to a previous state was easy, and debugging data issues was also much easier when you could spin up an entire account‚Äôs DB from a certain point in time locally. We also could run multiple versions of the product on different schema versions. Useful when certain customers only wanted their ‚Äúsoftware‚Äù updated once every 6 months. *** We do that where I am. I think it‚Äôs been in place for about twenty years - certainly more than a decade. We‚Äôre on MySQL/PHP without persistent connections. There have been many questionable architectural decisions in the codebase, but this isn‚Äôt one of them. It seems quite natural that separate data should be separated and it regularly comes up as a question from potential clients. *** Schemas[0] are the scalable way to do this, not databases, at least in Postgres. If you‚Äôre going to go this route you might also want to consider creating a role-per-user and taking advantage of the role-based security features[1]. That said, this is not how people usually handle multi-tenancy, for good reason, the complexity often outweighs the security benefit, there are good articles on it, and here‚Äôs one by CitusData[2] (pre-acquisition). *** I‚Äôve done this. But the service was suited for it in a couple ways; Each tenant typically only has <10 users, never >20. And load is irregular, maybe only ever dealing with 2-3 connections simultaneously. Maybe <1000 queries per hour max. No concerns with connection bloat/inefficiency. Tenants creates and archives a large number of rows on some tables. Mutable but in practice generally doesn‚Äôt change much. But >100M row count not unusual after couple years on service. Not big data by any means, limited fields with smallish data types, but‚Ä¶ I didn‚Äôt want to deal with sharding a single database. Also given row count would be billions or trillions at a point the indexing and performance tuning was beyond what I wanted to manage. Also, this was at a time before most cloud services/CDNs and I could easily deploy close to my clients office if needed. It worked well and I didn‚Äôt really have to hire a DBM or try to become one. Should be noted, this was a >$1000/month service so I had some decent infrastructure budget to work with. *** I guess the question is, why do you want to? The only real reason you mention is security, but to me this sounds like the worst tool for the job. Badly written queries accidentally returning other users‚Äô data, that makes it into production, isn‚Äôt usually a common problem. If for some reason you have unique reasons that it might be, then traditional testing + production checks at a separate level (e.g. when data is sent to a view, double-check only permitted user ID‚Äôs) would probably be your answer. If you‚Äôre running any kind of ‚Äútraditional‚Äù webapp (millions of users, relatively comparable amounts of data per user) then separate databases per user sounds like crazytown. If you have massive individual users who you think will be using storage/CPU that is a significant percentage of a commodity database server‚Äôs capacity (e.g. 1 to 20 users per server), who need the performance of having all their data on the same server, but also whose storage/CPU requirements may vary widely and unpredictably (and possibly require performance guarantees), then yes this seems like it could be an option to ‚Äúshard‚Äù. Also, if there are very special configurations per-user that require this flexibility, e.g. stored on a server in a particular country, with an overall different encryption level, a different version of client software, etc. But unless you‚Äôre dealing with a very unique situation like that, it‚Äôs hard to imagine why you‚Äôd go with it instead of just traditional sharding techniques. *** I have used this architecture at 2 companies and it is by far the best for B2B scenarios where there could be large amounts of data for a single customer. It is great for data isolation, scaling data across servers, deleting customers when they leave easily. The only trick are schema migrations. Just make sure you apply migration scripts to databases in an automated way. We use a tool called DbUp. Do not try to use something like a schema compare tool for releases. I have managed more than 1500 databases and it is very simple. *** WordPress Multisite gives each blog a set of tables within a single database, with each set of tables getting the standard WordPress prefix (‚Äúwp_‚Äù) followed by the blog ID and another underscore before the table name. Then with the hyperdb plugin you can create rules that let you shard the tables into different databases based on your requirements. That seems like a good model that gives you the best of both worlds. *** I have a bit of experience with this. A SaaS company I used to work with did this while I worked there, primarily due to our legacy architecture (not originally being a SaaS company) We already had experience writing DB migrations that were reliable, and we had a pretty solid test suite of weird edge cases that caught most failures before we deployed them. Still, some problems would inevitably fall through the cracks. We had in-house tools that would take a DB snapshot before upgrading each customer, and our platform provided the functionality to leave a customer on an old version of our app while we investigated. We also had tools to do progressive rollouts if we suspected a change was risky. Even with the best tooling in the world I would strongly advise against this approach. Cost is one huge factor - the cheapest RDS instance is about $12/month, so you have to charge more than that to break even (if you‚Äôre using AWS- we weren‚Äôt at the time). But the biggest problems come from keeping track of scaling for hundreds or thousands of small databases, and paying performance overhead costs thousands of times. *** Virtual Private Databases. What a lot of enterprise SaaS vendors do is have one single database for all customer data (single tenant). They then use features like Virtual Private Database to hide customer A data from customer B. So that if customer A did a ‚Äúselect *‚Äù they only see their own data and not all of the other customers data. This creates faux multi-tenancy and all done using a single db account. This sounds very much like Row Level Security, but I‚Äôve never heard the term ‚ÄúVirtual Private Database‚Äù to describe it. *** What we do at my current job is server per multiple accounts each server holds 500-1000 ‚Äúnormal sized‚Äù customers and the huge or intensive customers get their own server with another 10-50 customers Currently moving from EC2 + mysql 5.7 to RDS, mainly for ease of managing. However, we dont use a tenent id in all tables to differentiate customers we use (confusingly named) DB named prefix + tenent id for programatically making the connection. Have a single server + db for shared data of tenents like product wide statistics, user/tenent data and mappings and such things. In the tenent table just have column for the name of the DB server for that tenent and that‚Äôs pretty much it. Migrations are handled by an internal tool that executes the migrations on each tenent DB and 99% of the time everything works just fine if you are careful on what kind of migration you do and how you write your code Some pitfalls concern column type changes + the read replicas going out of sync but that was a single incident that only hurt the replica. *** When using a single db I‚Äôd highly recommend adding account_id to every single table that contains data for multiple accounts. It‚Äôs much easier to check every query contains account_id , as opposed to checking multiple foreign keys etc. Depending on the db you can then also easily export all data for a specific account using filters on the dump tool *** Seems impractical and slow at scale to manage even a few hundred separate databases. You lose all the advantages of the relational model ‚Äî asking simple questions like ‚ÄúWhich customers ordered more than $100 last month‚Äù require more application code. You might as well store the customer info in separate files on disk, each with a different possible format and version. Those queries are definitely convenient early on but eventually you shouldn‚Äôt be making those against that system and instead aggregate the data into warehouse. *** In my case this worked out pretty well. Other than data separation and ease of scaling database per-customer (they might have different behavior of read/write operations), they other benefit was that we could place customer‚Äôs database in any jurisdiction, which for some enterprise customers appeared an important point, regulations wise‚Ä¶ *** The apartment gem enables multi-tenant Rails apps using the Postgres schemas approach described by others here. It‚Äôs slightly clunky in that the public, shared schema tables, say, the one that holds the list of tenants, exists in every schema ‚Äî they‚Äôre just empty. I rolled my own based on apartment that has one shared public schema, and a schema for each tenant. Works well. *** Seems pretty odd. The closest example I can think of would be maybe salesforce? Which basically, as far as I can tell, launches a whole new instance of the application (hosted by heroku?) for each client. I‚Äôm not a 100% sure about this, but i think this is how it works. Not at all how Salesforce works, they take a lot of pride in their multi-tenant setup (for better or worse). Every org on a given instance shares the same application servers and Oracle cluster. If I were to make a Salesforce competitor that‚Äôs one thing I would do differently, with tools like Kubernetes it‚Äôs a lot easier to just give every customer their own instances. Yes, it can take up more resources - but I cannot imagine the security nightmare involved with letting multiple customers execute code (even if it‚Äôs theoretically sandboxed) in the same process, plus the headache that is their database schema. *** As snuxoll writes, Salesforce does use a shared database with tenant_id (org_id) as a column on every table. You can read a lot about our multi-tenancy mechanisms in a whitepaper published a while back [ https://developer.salesforce.com/wiki/multi_tenant_architecture ]. *** There aren‚Äôt a lot of benefits to doing it. If you have frequent migrations, then it probably isn‚Äôt something you ever want to do. For a site I run, I have one large shared read-only database everyone can access, and then one database per user. The per-user DB isn‚Äôt the most performant way of doing things, but it made it easier to: Encrypt an entire user‚Äôs data at rest using a key I can‚Äôt reverse engineer. (The user‚Äôs DB can only be accessed by the user whilst they‚Äôre logged in.) Securely delete a user‚Äôs data once they delete their account. (A backup of their account is maintained for sixty days‚Ä¶ But I can‚Äôt decrypt it during that time. I can restore the account by request, but they still have to login to access it). There are other, better, ways of doing the above. *** How about dozens per account? :) I didn‚Äôt ship this, but I work for Automattic and WordPress.com is basically highly modified WordPress MU. This means every time you spin up a site (free or otherwise) a bunch of tables are generated just for that site. There‚Äôs at least hundreds of millions of tables. Migrating schema changes isn‚Äôt something I personally deal with, but it‚Äôs all meticulously maintained. It‚Äôs nothing special on the surface. You can look up how WordPress MU maintains schema versions and migrations and get an idea of how it works if you‚Äôre really curious. If you don‚Äôt have homogeneous migrations, it might get pretty dicey, so I‚Äôd recommend not doing that. *** Jira Cloud and Confluence use a DB per user architecture at reasonable, but not outrageous, scale. I can‚Äôt share numbers because I am an ex-employee, but their cloud figures are high enough. This architecture requires significant tooling an I don‚Äôt recommend it. It will cause you all kinds of headaches with regards to reporting and aggregating data. You will spend a small fortune on vendor tools to solve these problems. And worst of all despite your best efforts you WILL end up with ‚Äúsnowflake‚Äù tenants whose schemas have drifted just enough to cause you MAJOR headaches. *** I have similar. One PG database per tenant (640). Getting the current DSN is part of auth process (central auth DB), connect through PGBouncer. Schema migrations are kind of a pain, we roll out changes, so on auth there is this blue/green decision. Custom fields in EAV data-tables or jsonb data. Backups are great, small(er) and easier to work with/restore. Easier to move client data between PG nodes. Each DB is faster than one large one. EG: inventory table is only your 1M records, not everyone‚Äôs 600M records so even sequential scan queries are pretty fast. *** I worked for one of the biggest boarding school software companies. The only option was full-service, but clients could chose between hosted by us or hosted by them. We didn‚Äôt just do 1 database per school, we did entirely separate hardware/VMs per school. Some regions have very strict data regulations and the school‚Äôs compliance advisors tended to be overly cautious; they interpreted the regulations and translated them to even stricter requirements. These requirements were often impossible to satisfy. (How can the emergency roll call app both work offline AND comply with ‚Äúno student PII saved to non-approved storage devices‚Äù? Does swap memory count as saving to a storage device?? Is RAM a ‚Äústorage device‚Äù??? Can 7 red lines be parallel!?!?) Shared DB instances would have been completely off the table. Thankfully, most boarding schools have massive IT budgets, so cost minimization was not as important as adding additional features that justified more spend. Also the market was quite green when I was there. Strangely, the software seemed to market itself; the number of out-of-the-blue demo requests was very high, so first impressions and conversion to paying clients was the primary focus. *** I worked for a company that did this, and our scale was quite large. It took a lot of work to get AWS to give us more and more databases on RDS. We had some unique challenges with scaling databases to appropriately meet the needs of each account. Specifically, it was difficult to automatically right-size a DB instance to the amount of data and performance a given customer would need. On the other hand, we did have the flexibility to manually bump an account‚Äôs database to a much larger node size if we needed to help someone who was running into performance issues. I think the biggest problems had to do with migrations and backups. We maintained multiple distinct versions of the application, and each had a unique DB schema, so there was frequent drift in the actual schemas across accounts. This was painful both from a maintenance POV, and for doing things like change data capture or ETLs into the data warehouse for data science/analysis. Another big problem was dealing with backup/restore situations. I suspect this decision was made early in the company‚Äôs history because it was easier than figuring out how to scale an application originally designed to be an on-prem solution to become something that could be sold as a SaaS product. Anyway, I think choosing a solution that nets your business fewer, larger database nodes will probably avoid a lot of maintenance hurdles. If you can think ahead and design your application to support things like feature flags to allow customers to gradually opt in to new versions without breaking backwards compatibility in your codebase, I think this is probably the better choice, but consider the safety and security requirements in your product, because there may be reasons you still want to isolate each tenant‚Äôs data in its own logical database. *** Years ago I worked for a startup that provided CMS and ecommerce software for small business. Each of our 3000+ customers had their own MySQL database. We had a long tail of customers with negligible usage and would run several thousand MySQL databases on a single server. As customers scaled we could migrate the database to balance capacity. We could also optionally offer ‚Äúpremium‚Äù and ‚Äúenterprise‚Äù services that guaranteed isolation and higher durability. Scaling was never a real issue, but the nature of our clients was steady incremental growth. I don‚Äôt think we ever had a case of real ‚Äúovernight success‚Äù where a shared host customer suddenly melted the infrastructure for everyone. However, managing and migrating the databases could be a real issue. We had a few ways of handling it, but often would need to handle it in the code, if schemaVersion == 1 else . Over time this added up and required discipline to ensure migration, deprecation and cleanuop. As a startup, we mostly didn‚Äôt have that discipline and we did have a fair bit of drift in versions and old code lying around. *** B2B CRM space startup. We have somewhat of a middle-ground approach. Our level of isolation for customers is at a schema-level. What this means is each customer has her own schema. Now, large customers want to be single tenant, so they have a single schema on the entire DB. Smaller (SMB) customers are a bit more price conscious so they can choose to be multitenant i.e multiple schemas on same DB. Managing this is pushed out to a separate metadata manager component which is just a DB that maps customer to the DB/schema they reside on. Connection pooling is at the DB level (so if you are multitenant then you may have lower perf because some other customer in the DB is hogging the connections)‚Ä¶ But this has not happened to us yet. Large customers are more conscious in terms of data so want things like disc level encryption with their own keys etc, which we can provide since we are encrypting the whole DB for them (KMS is the fave here). We are not really large scale yet, so dunno what they major gotchas will be once we scale, but this approach has served us well so far. *** Stackoverflow‚Äôs DBA had just posted about this: https://twitter.com/tarynpivots/status/1260680179195629568 He has 564,295 tables in one SQL Server. Apparently this is for ‚ÄúStack Overflow For Teams‚Äù *** One model I have seen used successfully is a hybrid model in which the product is designed to be multi-tenant, but then it is deployed in a mix of single tenant and multi-tenant instances. If you have a big mix of customer sizes (small businesses through to large enterprises) ‚Äì single-tenant instances for the large enterprise customers gives them maximum flexibility, while multi-tenant for the small business customers (and even individual teams/departments within a large enterprise) keeps it cost-effective at the low end. (One complexity you can have is when a customer starts small but grows big ‚Äì sometimes you might start out with just a small team at a large enterprise and then grow the account to enterprise scale ‚Äì it can become necessary to design a mechanism to migrate a tenant from a multi-tenant instance into their own single-tenant instance.) *** There are definitely downsides to scaling out thousands of tenants - I‚Äôve been told Heroku supports this, and at a glance I found this doc that says it may cause issues, https://devcenter.heroku.com/articles/heroku-postgresql#multiple-schemas but it really doesn‚Äôt change whether you‚Äôre on Heroku or not. At the end of the day it‚Äôs just about your application structure, how much data you have, how many tables you have etc. Unfortunately the Apartment gem even has these problems, and even its creators have expressed some concern ( https://mtm.dev/multitenancy-without-subdomains-rails-5-acts-as-tenant/#why-acts_as_tenant ) about scalability with multiple schemas. The acts_as_tenant gem might be what you‚Äôre looking for: This gem was born out of our own need for a fail-safe and out-of-the-way manner to add multi-tenancy to our Rails app through a shared database strategy, that integrates (near) seamless with Rails. My recommended configuration to achieve this is to simply add a tenant_id column (or customer_id column, etc) on every object that belongs to a tenant, and backfilling your existing data to have this column set correctly. When a new account signs up, not a lot happens under-the-hood; you can create a row in the main table with the new account, do some initial provisioning for billing and such, and not much else. Being a multi-tenant platform you want to keep the cost really low of signing up new accounts. The easiest way to run a typical SQL query in a distributed system without restrictions is to always access data scoped by the tenant. You can specify both the tenant_id and an object‚Äôs own ID for queries in your controller, so the coordinator can locate your data quickly. The tenant_id should always be included, even when you can locate an object using its own object_id. *** Yes, we did it at Kenna Security. About 300 paying customers, but over 1000 with trials, and overall about 6B vulnerabilities being tracked (the largest table in aggregate). Some of the tables were business intelligence data accessible to all customers, so they were on a ‚Äúmaster‚Äù DB that all could access; and some of the tables were fully multi-tenant data, so each customer had their MySQL DB for it. The motivation was that we were on RDS‚Äôs highest instance and growing, with jobs mutating the data taking a less and less excusable amount of time. The initial setup was using just the Octopus gem and a bunch of Ruby magic. That got real complicated really fast (Ruby is not meant to do systems programming stuff, and Octopus turned out very poorly maintained), and the project turned into a crazy rabbit hole with tons of debt we never could quite fix later. Over time, we replaced as many Ruby bits as we could with lower-level stuff, leveraging proxySQL as we could; the architecture should have been as low-level as possible from the get-go‚Ä¶ I think Rails 6‚Äôs multi-DB mode was going to eventually help out too. One fun piece of debt: after we had migrated all our major clients to their own shards, we started to work in parallel on making sure new clients would get their own shard too. We meant to just create the new shard on signup, but that‚Äôs when we found out, when you modify Octopus‚Äôs in-memory config of DBs, it replaces that config with a bulldozer, and interrupts all DB connections in flight. So, if you were doing stuff right when someone else signs up, your stuff would fail. We solved this by pre-allocating shards manually every month or so, triggering a manual blue-green deploy at the end of the process to gracefully refresh the config. It was tedious but worked great. And of course, since it was a bunch of Active Record hacks, there‚Äôs a number of data-related features we couldn‚Äôt do because of the challenging architecture, and it was a constant effort to just keep it going through the constant bottlenecks we were meeting. Ha, scale. Did we regret doing it? No, we needed to solve that scale problem one way or another. But it was definitely not solved the best way. It‚Äôs not an easy problem to solve. *** I believe that FogBugz used this approach, back in the day (with a SQL Server backend). The reasoning was that customers data couldn‚Äôt ever leak into each other, and moving a customer to a different server was easier. I vaguely recall Joel Spolsky speaking or writing about it. *** This question reminds me of some legacy system which I‚Äôve seen in the past :D :D :D In summary it was working in the following way: There was table client(id, name). And then dozens of other tables. Don‚Äôt remember exactly the structure, so I will just use some sample names: - order_X - order_item_X - customer_X - newsletter_X ‚ÄúX‚Äù being ID from the client table mentioned earlier. Now imagine dozens of ‚Äútemplate‚Äù tables become hundreds, once you start adding new clients. And then in the code, that beautiful logic to fetch data for given client :D And to make things worse, sets of tables didn‚Äôt have same DB schema. So imagine those conditions building selects depending on the client ID :D *** We did this in a company long long time ago, each customer had their own Access database running an ASP website. Some larger migrations were a pain, but all upgrades were billed from the customers, so it didn‚Äôt affect anything. If you can bill the extra computing and devops work from your customers, I‚Äôd go with separate environments alltogether. You can do this easily with AWS. On the plus side you can roll out changes gradually, upgrade the environments one user at a time. Also if Customer X pays you to make a custom feature for them, you can sell the same to all other users if it‚Äôs generic enough. *** This is a common approach outside of the SaaS space. I‚Äôd worry less about Rails and tools, and more about the outcomes you need. If you have a smaller number of high value customers (big enterprises or regulated industries), or offer customers custom add-ons then it can be advantageous to give each customer their own database. Most of the HN audience will definitely not need this. In some industries you‚Äôll also have to fight with lawyers about being allowed to use a database shared between customers because their standard terms will start with this separation. This approach is helpful when you have to keep data inside the EU for customers based there. If you want to get creative, you can also use the approach to game SLAs by using it as the basis to split customers into ‚Äúpods‚Äù and even if some of these are down you may not have a 100% outage and have to pay customers back. This design imposes challenges with speed of development and maintenance. If you don‚Äôt know your requirements (think: almost any SaaS startup in the consumer or enterprise space) which is trying to find a niche, then following this approach is likely to add overhead which is inadvisable. The companies that can use this approach are going after an area they already know, and are prepared to go much more slowly than what most startup developers are used to. Using row-level security or schemas are recommended for most SaaS/startup scenarios since you don‚Äôt have N databases to update and keep in sync with every change. If you want to do any kind of split then you might consider a US/EU split, if your customers need to keep data in the EU, but it‚Äôs best to consider this at the app-level since caches and other data stores start to become as important as your database when you have customers that need this. Consideration should be given to URL design. When you put everything under yourapp.com/customername it can become hard to split it later. Using URLs like yourapp.com/invoice/kfsdj28jj42 where ‚Äúkfsdj28jj42‚Äù has an index for the database (or set of web servers, databases, and caches) encoded becomes easier to route. Using customer.yourapp.com is a more natural design since it uses DNS, but the former feels more popular, possibly because it can be handled more easily in frameworks and doesn‚Äôt need DNS setup in developer environments. *** We did this for 2 large projects I worked on. Works really well for env. where you can get a lot of data per customer. We had customers with up to 3-4 TB databases so any other option would either be crazy expensive to run and or to develop for. You need to invest a bit of time into nice tooling for this but in a grand scheme of things it‚Äôs pretty easy to do. *** Yes!! In hosted forum software this is the norm. If you want to create an account you create an entire database for this user. It isn‚Äôt that bad! Basically when a user creates an account you run a setup.sql that creates the db schema. Devops is pretty complex but is possible. EG! Adding a column - would be a script. Scaling is super easy since you can move a db to another host. *** This is pretty much how WordPress.com works - or used to work, I don‚Äôt know if they changed this. Each account gets its own set of database tables (with a per-account table prefix) which are located in the same database. Upgrades can then take place on an account-by-account basis. They run many, many separate MySQL databases. *** Already some great answers. Some color: A lot of B2B contracts require this sort of ‚Äúisolation‚Äù. So if you read 1 database per account and think that‚Äôs crazy, it‚Äôs not that rare. Now you know! I certainly didn‚Äôt 2 years ago. *** Nutshell does this! We have 5,000+ MySQL databases for customers and trials. Each is fully isolated into their own database, as well as their own Solr ‚Äúcore.‚Äù We‚Äôve done this from day one, so I can‚Äôt really speak to the downsides of not doing it. The piece of mind that comes from some very hard walls preventing customer data from leaking is worth a few headaches. A few takeaways: Older MySQL versions struggled to quickly create 100+ tables when a new trial was provisioned (on the order of a minute to create the DB + tables). We wanted this to happen in seconds, so we took to preprovisioning empty databases. This hasn‚Äôt been necessary in newer versions of MySQL. Thousands of DBs x 100s of tables x innodb_file_per_table does cause a bit of FS overhead and takes some tuning, especially around table_open_cache . It‚Äôs not insurmountable, but does require attention. We use discrete MySQL credentials per-customer to reduce the blast radius of a potential SQL injection. Others in this thread mentioned problems with connection pooling. We‚Äôve never experienced trouble here. We do 10-20k requests / minute. This setup doesn‚Äôt seem to play well with AWS RDS. We did some real-world testing on Aurora, and saw lousy performance when we got into the hundreds / thousands of DBs. We‚Äôd observe slow memory leaks and eventual restarts. We run our own MySQL servers on EC2. We don‚Äôt split ALBs / ASGs / application servers per customer. It‚Äôs only the MySQL / Solr layer which is multi-tenant. Memcache and worker queues are shared. We do a DB migration every few weeks. Like a single-tenant app would, we execute the migration under application code that can handle either version of the schema. Each database has a table like ActiveRecord‚Äôs migrations, to track all deltas. We have tooling to roll out a delta across all customer instances, monitor results. A fun bug to periodically track down is when one customer has an odd collection of data which changes cardinality in such a way that different indexes are used in a difficult query. In this case, we‚Äôre comparing EXPLAIN output from a known-good database against a poorly-performing database. This is managed by a pretty lightweight homegrown coordination application (‚ÄúDrops‚Äù), which tracks customers / usernames, and maps them to resources like database & Solr. All of this makes it really easy to backup, archive, or snapshot a single customer‚Äôs data for local development. *** Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-08-14"},
{"website": "Arkency", "title": "Anti-IF framework - if/else based on type", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/anti-if-framework---if-slash-else-based-on-type/", "abstract": "I have to admit that I‚Äôm a bit crazy when it comes to IF statements. I know that they‚Äôre the foundation of programming. I know that it‚Äôs impossible to completely eliminate them. Still, I know we can do better than if/else-driven development. IEDD - If/Else Driven Development To me IEDD is all about emotions. The fear of adding new classes.\nThe pain of changing the existing nested IFs code structure.\nThe shame of adding Yet Another IF.\nThe challenge of adding a new feature with a minimal number of keystrokes.\nThe hope of not getting caught on code review. YAI - Yet Another IF Yes, that probably means creating new method, extracting new object. It might be a bit OOP. If that‚Äôs not your taste and you‚Äôre fine with if/else then this may not be for you. Here is one ‚Äúframework‚Äù that I came up with: The second point might be the most challenging in the case of a big and ugly nested if/else. Let‚Äôs look at this example: Let‚Äôs just focus on what we see here. No emotions, no blaming, no asking - who did that and why. We can see a complex nested if/else statements structure. It seems to be about products and their quality and when should they be sold. Assuming you have a good testing coverage (I recommend mutation testing tools) you could feel safe to refactor this. But I don‚Äôt believe in refactoring without a bigger vision. This code is point A, where is your point B? What is your destination? In the ‚Äúframework‚Äù as expressed above, we‚Äôre targeting a design where we have an object per each behaviour, per each type. What can bring us one step closer to that? Refactoring the conditions so that the dominant if/else structure is all about type and only about type. Additionally, there should be no type check embedded inside. This can be the result: The code does the same. All tests pass and I have 100% mutation coverage. I‚Äôm claiming this as an improvement. Why? Because now it‚Äôs easier to get to our destination. I have extracted the classes responsible for each behaviour. They still contain if‚Äôs but they‚Äôre now better encapsulated. This code is still not perfect. The factory logic is screaming to us to make a factory method or a factory class. But those are optional. We‚Äôve managed to conquer the main issue here - the nested if statements. If you liked this example, you will enjoy the free training on Wednesday, September 15th, 7pm CEST: https://arkency.com/anti-ifs/ - see all the details Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-09-03"},
{"website": "Arkency", "title": "Filepicker and Aviary - Image uploading on steroids", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2012/09/filepicker-and-aviary-image-uploading-on-steroids/", "abstract": "We all have been using the same code for uploading images for years, but didn‚Äôt\nyou always feel that there is something wrong with it? For every other task\nlike writing texts, picking a date, selecting from lot of choices we have a\ngood tool that can help in implementing such feature and improve the user\nexperience, but file uploads almost always feel a little broken. There are some\nFlash tools that might help, but they are still not good enough. Well, welcome to the world invaded by Filepicker and Aviary . Speaking short, Filepicker is a tool that\nlet the user upload images not only from the computer itself but also from web\nservices such as Facebook or Dropbox. Aviary provides you with a powerful HTML5\neditor for manipulating photos. Both of them process the images on their\nservers and provide you an url for downloading a file. If you only need more\npowerful uploading you can stick with Filepicker widgets otherwise we need to get our hands dirty with their Javascript APIs\n(or CoffeeScript as you will see) but is not hard at all. Let‚Äôs start with the view: Nothing fancy here. Classic Rails link_to method, using _('') method for\ntranslating with FastGettext . We\ndon‚Äôt care about URL because we are going to handle clicks in Javascript so I\nused \"#\" as URL. Instead of using css classes or id for such link I prefer to use custom data-* attribute First, we need to display Filepicker popup for choosing image when our link is\nclicked. I use jQuery delegate because if it was a\nSingle Page Application ( SPA Todo app example ) or the link is dynamically added via AJAX, it can still be properly handled. After clicking the user needs to give permission for using data from a service\nor simply upload file from computer, or even take a photo using computer\nbuilt-in camera. It‚Äôs time now to run the photo editor when the file is picked instead of\njust using console.log . Create instance of the editor: Use it when file picked: When user finishes editing the photo and presses ‚ÄúSave‚Äù button, onSave callback is executed. You can save the url value in JS variable or use it to\nfill some hidden field in a form or send it to the server. However the\ndocumentation states that ‚Äúthis image may not yet be ready so you will have\nto poll this link, or alternatively handle the hi-res image server-side‚Äù .\nThis is my biggest disappointment when using those two products. For that\nreason we are going to use postUrl option so that Aviary will send us a\nrequest to this given URL when the image is ready. Obviously you will have\nto use different value of the setting for development, staging and production\nenvironment. In development you can either forward some port from your router\n(I assume it is publicly available) to your computer or alternatively, if\nhave a server you can use ssh to forward traffic from the server to your\nlocal machine. Forwarding ports with ssh : (Update) Alternatively you can use a solution that does it for you and does not\nrequire having custom server. Avdi did a really nice research of http forwarding\ntools Launching the editor with postUrl : Let‚Äôs see the controller that is used when Aviary notifies\nus of the ready image: Find user, set avatar url and save. As simple as that. Where does remote_avatar_url setter comes from ? It is a feature of carrierwave library that I use\nto store and resize avatars. It can download the remote avatar itself so I\ndo not need to bother myself with that. You can use it with RMagick , mini_magick or vips . The default_url is used when avatar is not set. That way User#avatar method can nicely behave as Null Object . Avdi blogpost about Null Objects is definitely worth reading. But we don‚Äôt want anyone to be capable to send requests to our application\nand change avatars, do we ? We need to add some protection. And we need to know\nwhich user avatar should be changed. Every user will have its own token for\nupdating the avatar. Again, we use custom data-* (exactly data-avatar-token ) attribute to store the token in HTML. We use postData to store additional metadata that should come with the request\nfrom Aviary to our App. Now we can use this data in our controller to verify the request: And this leaves us with the implementation of AvatarToken class. What do you compute digest of ? Well, that depends on your application. One more thing that I would like to do is to always get square images\nfrom Aviary. I could not find 100% reliable way of doing that. My trick\nis to allow users to use only one type of crop ratio and show the crop\ntool as initial one. However, the user can still press ‚ÄúCancel‚Äù unfortunately. So the whole JS part looks like this: Few more notes about good and bad parts of this solution: Pro: The concepts behind Filepicker and Aviary are amazing and I believe they will change the web. It‚Äôs like ‚ÄòEditor as a Service‚Äô, ‚ÄòPicker as a Service‚Äô. What else could be a service ? I would love to use Gliffy editor in my app the same way I use Aviary. Filepicker can store files directly in S3 so you do not have to keep them. I just prefer to have them on my machine. Javascripts are available via HTTPS links. Cons: When using filepicker the user accepts filepicker.io application when connecting to Facebook or Dropbox, not our own application. This might be also considered a good thing if you did not connected your App with Facebook, but I would prefer if the widgets asks for permissions for my app. However I am not sure if that would be possible at all. You cannot force Aviary to provide image in one ratio. You cannot download from Aviary the image in different resolutions. The workaround is to upload it again to Filepicker and download converted . Too much hassle for me. It was just easier to this step on our server. Both services ask you to link directly to their Javascript files instead of downloading them and using in your asset pipeline solution. So there are going to be additional HTTP requests when loading the page. But the good side is that if they fix some bug or improve the editor, the changes will be automatically available to your users with you deploying your app again. After save, the photo URL from Aviary is not available immediately. This presents a huge UI problem. What should I show to my user after setting new avatar when I might not yet have a new avatar image to display ? Even after refresh of the page the new avatar might not yet be ready if the server is still waiting for a request from Aviary. Aviary is not doing exponential backoff. It sends the request to your server only once. The game is over if you failed to handle it. (Sidenote: if you ever need to implement exponential backoff strategy in Ruby or Rails, check exponential-backoff gem . The full list of Aviary translations is not bad but it is still missing few important ones for me like Greek or Turkish (forgive me my Eurocentrism). You cannot change the language after initial Aviary configuration. Single Page Applications that are capable of changing language without reloading the page probably need to create a new instance every time they want to use Aviary editor instead of calling launch method multiple times on one object. Filepicker does not allow you to choose any translation. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2012-09-21"},
{"website": "Arkency", "title": "JavaScript objects philosophy", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2012/10/javascript-objects-philosophy/", "abstract": "As a web programmer you use JS from time to time or on daily basis. You probably know how to create and use objects, but don‚Äôt feel that approach is natural. It‚Äôs awful JS, right? No, it‚Äôs you and your lack of knowledge. Let‚Äôs see what object orientation in JavaScript mean. In terms of JS, object is collection of key-value pairs, where key have to be string. Value doesn‚Äôt have any type constraints, so it can store primitives, objects or functions (in that context we call them ‚Äúmethods‚Äù). Variables and object fields store only reference to object. Objects can be compared only in terms of reference identity. There are many ways to create objects in JavaScript, so let‚Äôs name some of them. It explicitly creates dog object with name field and woof method. Basic concept here is many times in your app there is no need for many objects with same data structure and behaviour, but rather one object with its data and behaviour. Object orientation reveals on instances, not classes. If you feel better with it you can call that it‚Äôs real object orientation. Object.create instantiate empty cat object ‚Äì without any field. If you‚Äôd pass object as create ‚Äòs parameter it‚Äôd become prototype of created object. More on prototypes later, so let‚Äôs focus on cat . We add its data and behaviour by assigning values and functions to its not-yet-existing fields. Of course in runtime you can change both field value to new values (or functions) and even remove field. First of all what is Owl? As you see it‚Äôs a function, but kind of special. It‚Äôs called constructor, because it constructs new objects with defined data and behaviour. You must be warned - in this example each object constructed with Owl will have different hoohoo method, because it‚Äôs defined as part of construction. Later we‚Äôll figure out how to share methods. That‚Äôs the first way to differentiate class of objects - if you created object with constructor you can say, that object is it‚Äôs instance But remember - owl is not instance of Owl class, but rather owl is instantiated by Owl constructor. In JS inheritance base on objects, so object a can inherit data and behaviour form object b and then object b is called prototype of object a . Of course b can also have prototype, so each object has chain of prototypes. Ok, let‚Äôs see example. As you see in this example protoCat‚Äôs fields are fallback for cat‚Äôs one - if cat doesn‚Äôt have field interpreter looks for it in prototype, and then recursively in prototype‚Äôs prototype‚Ä¶ If that field is function it also passes right object - on which method was invoked - as this. And if found method uses object‚Äôs field interpreter start searching from original object. So prototype defines default data and behaviour of objects that inherits from it and that‚Äôs the way to share and reuse common behaviours. The biggest difference here is that you don‚Äôt inherit from class of instances, but just instance, so if you‚Äôd change prototype field in runtime, all object‚Äôs that inherits from it will be affected unless they override that field. I showed you how to create object with prototype with Object.create . You can also assign common prototype for objects created by constructor: Owl.prototype = animal means, you want each of constructed object to have animal as prototype. Of course prototype can be also created with constructor: In that case owl object is both instance of Owl and Animal in terms of instanceof operator. Why owl instanceof Animal ? Suppose that we remove all Owl-specific fields - how would owl behave like? Animal, of course, so that‚Äôs the answer. Prototype-based inheritance is emanation of inability to create perfect taxonomy of objects in terms class-based inheritance. As software engineer you probably know that there‚Äôs no way to create perfect class inheritance tree, that won‚Äôt be affected by change of your knowledge about domain. Prototype-based object orientation is no better, but simplicize meaning of object - it doesn‚Äôt have to have type, class - it‚Äôs just container for data and behaviour, which have meaning in current context - for our knowledge of problem domain. Please remember: when you are thinking about prototype-based object orientation you should focus on easy-to-create objects, not prototype-inheritance, which is just consequence. I hope that now you feel JavaScript object orientation, but if you still feel uncertain about object-oriented programming in JS write a comment or ping me on Twitter . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2012-10-12"},
{"website": "Arkency", "title": "Sending async data from Rails into the world", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2012/10/sending-async-data-from-rails-into-the-world/", "abstract": "Exceptions and business metrics. These are two common use cases involving\ndelivery of data from our Rails application (or any other web application)\nto external services that are\nnot so crucial and probably we would like to send them asynchronously instead\nof waiting for the response, blocking the thread. We will try to balance\nspeed and certainty here, which is always a hard thing to achieve. This is a series of posts which describe what techniques can be used in such\nsituation. The first solution that I would like to describe (or discredit) is\nZMQ. What is ZMQ ? According to the lengthy and funny ZMQ guide : √òMQ (ZeroMQ, 0MQ, zmq) looks like an embeddable networking library but acts\nlike a concurrency framework. It gives you sockets that carry atomic messages\nacross various transports like in-process, inter-process, TCP, and multicast.\nYou can connect sockets N-to-N with patterns like fanout, pub-sub, task\ndistribution, and request-reply. It‚Äôs fast enough to be the fabric for\nclustered products. Its asynchronous I/O model gives you scalable multicore\napplications, built as asynchronous message-processing tasks. It has a score\nof language APIs and runs on most operating systems. √òMQ is from iMatix\nand is LGPLv3 open source. You can also watch an introduction to ZMQ delivered by one of the creators of\nthis library: Martin Sustrik: √òMQ - A way towards fully distributed architectures It seems like a perfect candidate at first sight, so let‚Äôs dive into this topic a little bit. I believe that we could use diagram here. Every rails thread (I assume multithreading app here, but it does not matter\na lot) would have a ZMQ socket for sending exceptions and business metrics. Sending\na message with such socket means only that it was delivered to ZMQ thread\nwhich will try to deliver it further. If you are building a solution and would like your customers to\nsend you some data from their applications, ZMQ is probably not the way to go. Another common way of solving this problem is to have separate process which\nyour application communicate to. That process receives events from your app\nand sends them further to the external service. Let‚Äôs see a diagram: Rails application can communicate with the process running in customer\ninfrastructure using any protocol it wants. That could be for example\nZMQ or UDP if we value simplicity. That process is then responsible for\ndelivery of events to the external service. This\nis a common pattern in business metrics solutions. Application can send\nhuge number of events to the process who is responsible for aggregation\nand periodically sends aggregated data further. There could be benefits of using such architecture for exception\ndelivery. The middle process is a very good candidate to put in the\nresponsibility of doing retries with exponential backoff strategy. Stay tunned for the next episodes. Follow us on twitter or subscribe to RSS feed so you do not miss it. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2012-10-17"},
{"website": "Arkency", "title": "Why we don't use ORM", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2012/12/why-we-dont-use-orm/", "abstract": "You‚Äôve probably already read that we don‚Äôt use Rails or any other framework to build chillout.io . Having that said I must add we neither use ORM. First of all - your business models shouldn‚Äôt know anything about storage. Why? Single Responsibility Principle could be one of valid answers. But I‚Äôd rather argue it‚Äôs because storage responsibility is to extend app state, in which models live - your models don‚Äôt know anything about memory management, so why should they be interested in persistent memory? There‚Äôs something more: forget about your storage default - database . Imagine, that data from your domain model could be persisted in many ways - to simple files, to key-value stores, to relational databases and so on. In application with high-quality architecture you can defer decision which one to choose - some models will need to be restored in very short time, and for some of them it won‚Äôt matter. Maybe one model will look like relational record, and another like document? To be accurate - that‚Äôs why we don‚Äôt use ORM on domain-level: we don‚Äôt want to mix storage with our business, we don‚Äôt want to depend on non-domain interface of our models. But to be honest - we also don‚Äôt use ORMs on app-level, because we didn‚Äôt find any tool that only map object by interface description to storage and vice versa. DataMapper 2 looks promising , but it‚Äôs not there yet. So how do we handle storage? We use repository objects that encapsulate information how to map models into storage entities and acts like domain collection. Each domain model should have own repository (or none, if we don‚Äôt have to store it) - that way the only reason to modify repository implementation is change in model interface. Each of repositories can have different interface, based on your domain needs. Each of them can use different storage, but all storage adapters should have same API to make them easy to change to other. Storage is not a part of your domain, even if most of your domain objects have to be persisted. It‚Äôs just one of the details that you should change easily. Do you think I‚Äôm wrong and that‚Äôs too complicated to handle data? Leave a comment and tell your story. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2012-12-03"},
{"website": "Arkency", "title": "Google Analytics for developers", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2012/12/google-analytics-for-developers/", "abstract": "Yes, we have to be honest - we‚Äôre doing content marketing for our product for Rails developers , but we want that content to be meaty and very useful. So today I‚Äôm gonna focus on basic functions of Google Analytics you could not know and how we use them on our blog and landing page. You probably use most of basic features of Google Analytics - you know how to get information how many visits was made each day, you know your users browser segmentation etc. But how can you measure effects of your blog post? You can check what happened when you posted link to Hacker News or Reddit in Real-Time - just go ‚ÄúHome‚Äù and click ‚ÄúReal-Time‚Äù. When you promote product with blog you should be able to measure how many readers did what you want - click link or sign up with some form. Here‚Äôs our little script that solves that problems. As you can see it requires jQuery, but of course can be easily rewritten to not use. To be honest we use additional attributes to make our events more readable - our links can define category, actions and labels of event. We track events, so where can we find them? In this section you can choose which category, action or label is interesting for you and show only occurrences of that type. You can also see flow of your users in terms of events - below ‚ÄúEvents overview‚Äù you can find ‚ÄúEvents Flow‚Äù. You now have quite good insight what‚Äôs happening on your site, but that‚Äôs not all. You can set goals based, for example, on events. Here‚Äôs where you can define them: We set two goals - for event with label http://chillout.io/ and for newsletter sign up event. How to use goals? You can find them in ‚ÄúStandard Reporting‚Äù in ‚ÄúConversions -> Goals‚Äù section. It‚Äôs useful, that you can see which page have biggest conversion to goals. Don‚Äôt waste your creative effort - start tracking what‚Äôs catchy and interesting. It could be useful if you‚Äôre trying to make product or if you‚Äôre looking for trend in your industry. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2012-12-05"},
{"website": "Arkency", "title": "Blogging for developers", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/01/blogging-for-developers/", "abstract": "There are many possible blogging platforms out there to be used, yet we decided not to use any of them.\nIt was a controversial decision even inside our own, small team. Before we started blogging there was\na heated discussion whether we should use something that can quickly get you running so that when you\nfeel in a mood for a blog post, there are no obstacles preventing you from writing. Or the alternative\nwas to build something custom and have more control. We ended up using existing tools\nbut put a little effort to combine them together into something that we like. We are developers, we use console daily, feel comfortable using it and prefer text over fancy GUI. We\nlike to be in control. But there is no need to reinvent the wheel so we started with nanoc . It even comes with built-in helpers for blogging. Built with Ruby\nso perfect fit for a company that started by creating Rails application. I won‚Äôt go into much details\nabout Nanoc itself as you can read more about it on its website and see basic nanoc blog example on github . So Nanoc\ngives us the bricks to build a house, a basic infrastructure or framework for blogging as some would call it\n(because it seems that these days everything can be called a framework). Pro: Cons: About 20% of our traffic comes from mobile devices. I love reading blog posts on my mobile when\nmoving around the city. Especially those that are comfortable, so no wonder that it was important\nfor us to keep readers using mobile devices happy. We achieved it simply by using bourbon . You can see how our blog looks when using\ndifferent devices using responsive.is (seriously, \nclick the link to see it). Overall we are very happy with bourbon and started using it in\nmany other projects. However we‚Äôve been hit by one bug when using it with nanoc. You can read more about how to properly use bourbon and nanoc on dutygeeks blog\nand see the issue on github if your are curious. I really like lightboxes for displaying images from posts. They work great on desktops. However I have\nyet to see a lightbox that works great on desktop and mobile device. I tested dozen of them and every\none had an issue on mobile. Is it with zooming, taking too much space, bad navigation, poor performance, lack of\nsupport for swipe, or too small button for closing the popup with image. So our solution is very simple.\nWe use lightbox on desktop and turn it off for mobiles. Clicking an image on mobile will simply display\nthe image itself using your mobile browser. And guess what‚Ä¶ It works great. It is super fast,\npinch-to-zoom works flawlessly, the image is displayed with proper zoom and when you want to close it you\njust press ‚ÄúBack‚Äù and browsers redirect you back to the previous page (the blog post) and position the\nscreen at the place where you finished reading. Works way better than any lightbox that I have tested. However if you think that I am wrong and there is lightbox of such high quality, please leave a comment.\nI would be more than happy to try it out. For every image we generate additional 2 versions. Thumbnail that can be used when displaying multiple\nimages in a gallery box: And ‚Äúfit‚Äù version that is as wide as it can be on a tablet in landscape position. The code for doing that is pretty straightforward and uses ImageMagick convert binary: Other nice optimization are yet to be added such as: Nothing fancy here, we use good, old pygments with\na ruby wrapper . Does it look good ? You bet :) Here is an example: If you ever read our blog before you might have noticed that we usually try to end our posts\nwith call to action. That is in most situations invitation to one of our newsletters or link to chillout.io landing page . Fortunately we don‚Äôt need to add them\nmanually. Instead we use a feature of nanoc which let‚Äôs you include\nmetadata to every post. At the end of post layout file we use custom #newsletter helper method to output proper code\nbased on the metadata. If we ever decide to change it, we can do so in one place. Here is the simplified version of the helper: Thanks to custom google analytics events we can easily track user actions on our blog.\nWe described this technique in our previous ‚ÄúGoogle Analytics for developers‚Äù post Before we publish a new post we first ask for opinion our coworkers. We show them new entries by\ndeploying to different internal host visible only for those connected via VPN. Nanoc supports\nmultiple deployments natively in config.yaml : We host our blog in our own infrastructure that we manage with chef for our customers. It is built\naround LXC containers and our blog requires only nginx for serving it. It works nicely even when\nyou hit front page of Hacker News or Reddit. And if we decide\nto serve our blog using SPDY there is\nnothing to stop us. You can expect more blog posts about the infrastructure and SPDY experiment soon. You might be tempted to say that this is lot of effort just to have a company blog. Probably you are right,\nbut on the other hand we have fun, knowledge and area for experiments that teach us valuable lessons\nand can be further used in other, commercial projects. Blogging and sharing knowledge is one thing, having fun and learning new things is another.\nBut nobody said we cannot mix those two. Blog is an important medium for us to communicate our ideas\nbut also is a little toy project. Do you have a toy project in your company\nto test your crazy ideas in the wild ? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-01-02"},
{"website": "Arkency", "title": "You don't inject for tests", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2013/01/you-dont-inject-for-tests/", "abstract": "What is unit testing for? Is it a way to make sure that your code is correct and bugless? Or rather OOD tool, that expose most of places where you break object orientation principles? You may have your own answer (please comment though) but I would vote for the second one - of course it may assure me that I haven‚Äôt introduced some totally stupid bug, but that‚Äôs less interesting part. We started this week drama in Ruby community with DHH‚Äôs ‚ÄúDependency injection is not a virtue‚Äù blog post. I agree that we shouldn‚Äôt use dependency injection to make software testable - testability is not a basic goal when creating software. It‚Äôs rather part of maintainability goal. Let‚Äôs walk through application parts and find out how testability helps in maintanance and by this why dependency injection is useful. On this level unit tests shows you how complex your business objects are. Each line of test code on this level triggers question ‚ÄúShould I split this class to few smaller?‚Äù, which is emanation of SRP . With constructor dependency injection you can expose how many other objects can get messages from current and decide if there is something that can be abstracted from what you found out. Where‚Äôs maintainability in this example? With smaller classes each change in one of them introduce less changes in dependencies. Communication of your objects have well defined flow. Let‚Äôs suppose you use some external services like database or REST API. First of all you use them for a reason, like ‚ÄúI want to get all tweets with #sillycat hashtag‚Äù. When you write classes that will communicate with those services you should expose that reason. And here‚Äôs the place for border tests - you write them to make sure, that service adapters (which expose our need) do what they should really do. For db it would be getting records from that db, for REST API it would be making real request and checking if the answer is what we assumed. With dependency injection in objects that use such adapters you get easiness of adding new similar services, single place to fix when you have problems with one of services etc. If you‚Äôd hardcode such adapter dependency (i.e. by instantiating) it could be hard to use different configurations too. As you can see dependency injection is a way to make you application more flexible, but what‚Äôs more important - more ordered. With dependency injection you have to ask yourself very often on how many objects this one depends and how to minimize it. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-01-09"},
{"website": "Arkency", "title": "Rails API - my simple approach", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/01/rails-api-my-approach/", "abstract": "I have seen people using very different techniques to build API around Rails applications.\nI wanted to show what I like to do in my projects. Mostly because it is a very simple\nsolution and it does not make your models concerned with another responsibility. First, I have a problem with the naming around API. I believe we use invalid nomenclature to describe our intentions.\nLet‚Äôs think about it for a moment. Imagine we have a Customer object\nand we need to keep it somewhere between the restarts of our application (not necessarily Rails application).\nSo what do we do ? We use serialization to store it in a file. May it be binary format, JSON, XML or YAML: What is the point of serialization ? To store the inner state of an object and use it to recreate it later. But this is not what we usually want to achieve when building APIs. In such case we want to deliver\nsome data to the consumer of our API. We don‚Äôt try to save the state of an object. Rather I would say, we present it. Therefore I prefer to use the name serialization when the object\nis stored and processed by the same application and its inner state is stored. And the name presenter sounds good to me in cases when you talk about an object with a separate application. When you display it\nto others. When you show its, what I would say, external state (if such thing might exist). You might wanna ask ‚Äúwell, what is the difference‚Äù? I shall answer you immediately. The inner state and external state might often not be the same thing. In our case we store first_name and last_name separately but our clients might only be interested in full_name . There is no reason to send them {\"first_name\":\"Robert\",\"last_name\":\"Pankowecki\"} when they actually need: {\"full_name\":\"Robert Pankowecki\"} . So‚Ä¶ What shall we do ? Bring up the presenters on stage. Presenter, for me, in API requests has a role similar to the View layer in classic requests to obtain HTML page.\nWe want a layer whose responsibility is to build the response data. And we want it to be separated from our\ndomain and most likely contain some presentation logic that should not be in model. You look at that as_json method and you know from the first look what is being sent to your API clients.\nHow do you use it in a controller ? As simple as that. Let‚Äôs say that the consumers of the API would like to display the avatar of Customer . We know the\nemail of a customer so we might compute Gravatar url and give it the consumer. We might be tempted\nto write such logic in our model (and it is not that bad idea) but because it is of no use to our app,\nI would prefer to have a method for that in the presenter itself. Do you like Hypermedia API ? I still don‚Äôt know but let‚Äôs give it a try here just to prove my point ‚ò∫ .\nThere is a feature that customer can be notified about promotions and other events. It is done by\nsending request to URL that we have available under customer_notification_url route method in our controller.\nWe would like to send it also to the API clients of our app. And the controller: You can simply have you presenter talk multiple dialects by including ActiveModel::Serializers : And embrace it in your controller by responding to multiple mime types: I am also a big fan of decent_exposure and love how the controllers look when using it: It might happen that different usecases demend different presentation. We might need a different value\nor additional field. I heard you like inheritance: Or maybe you prefer dynamic mixins ? Delegation ? It doesn‚Äôt matter which way you like most. All options are still available to you. You know\nhow they work and what are the implications of using the solution you have chosen. Because\nthey are part of the language that you use daily. Not yet another DSL which must implement its\nown syntax to let you share some parts of the code and mimic inheritance. Plain, old, simple\nRuby. Nothing of what I showed here will help in the case where you actually need to created objects\nbased on XML or JSON that you received. Roar might be really helpful in such situation. If you dislike my solution, feel free to use roar , rails-api or active model serializers .\nI think they all have their own advantages. There many libraries that try to help you deliver a well crafted API representations.\nBut maybe you do not need them and you can achieve your goals using just plain Ruby features ? If you like this article you might be interested in our product that we would like to publish\nin the future. It will be full of such analysis. You can sign up below. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-01-10"},
{"website": "Arkency", "title": "Black-box Ruby tests", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2013/02/black-box-ruby-tests/", "abstract": "Chillout.io architecture is distributed - currently just as few processes on same production server. As professionals we decided to prepare integration tests for each of application (maybe in terms of their business we could call them acceptance tests), but that wasn‚Äôt enough - we wanted to expose business scenarios that we‚Äôre actually aiming in . We achieved that with fully black-box tests. Curious how we did that? Read on. Chillout.io purpose is to help Rails developers maintain existing projects - currently react on exception events and be up-to-date with simple business metrics like model creations. To provide this features its backend is splitted into following apps: It has also gem dedicated to Rails applications, which communicates with our API. To test whole stack we have to run all apps + Rails server. Let‚Äôs have a look at our AcceptanceTestCase class: As you see we also have sample Rails app to make sure, that our gem really works. To make development easier configuration of that gem is hardcoded in this sample app, and this configuration is also hardcoded in our tests. Business scenarios - stories or use cases - are defined with high-level terms , i.e. ‚ÄúSomeone created model in Rails app, so at the beginning of next day (when scheduler trigger) owner should get email with report containing model‚Äôs counter increased by one‚Äù. To expose that business, our product‚Äôs real value, we use same terms in tests: Here‚Äôs an explanation what really happens in scenario: As you can see all steps of this scenario are described in terms of real usage. If we‚Äôd describe how our product subcomponents cooperate to handle this use case it would look like this: As a side note: approach of tests with actors was introduced in bbq gem. As you can see it is about what scenario actor‚Äôs will do in real life - one of them will get to Rails app and create some entity, scheduler (cron) will invoke daily report at given time, and then Rails app‚Äôs owner will go to web mail and see our notification. All details about our architecture are not available on scenario level. If we decide to split one of our apps to few smaller we will just have to add new commands to run in AcceptanceTestClass . We can also show it to you to tell you about our new feature - automatic simple business metrics . How do like this style of testing? Please leave a comment - maybe you have something better? Make sure to check our books, videos and bundles Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-02-01"},
{"website": "Arkency", "title": "Web is no longer request-reply", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/02/web-is-no-longer-request-reply/", "abstract": "In the old times everything was simple. You received a request, did some computation (or not in case of static page)\nand sent a reply. However what we do now is no longer that easy. With the rise of rich, usually social applications almost every action triggers immediate chain of notifications.\nYou click a ‚ÄúLike‚Äù button and all of your friends should know this fact. Same way with Twitter, Gmail, Hackpad, Trello\nand soon probably almost every app. For years our tools was built and improved around request-reply pattern. Proxies, load balancers, benchmarks.\nTons of software which basically think in this model. But it was not always like that. Earlier we used to embrace\nthe connection. Yep, you heard me right. Because under this powerful pattern that we took to the edge there is still\nplain, old TCP connection. A stream of bytes. Not messages but merely bytes. But web no longer is HTTP. Nor request-reply. We have now Websockets, SSE, and SPDY. The web is now realtime.\nStreaming data both ways. Servers usually stream music and video and clients usually stream their location\n(especially if you develop modern, urban city game). So the question is: Do our current tools help us to develop applications with such requirements ? What I dream about are\nframeworks and libraries which would make it simple to think about the new part of the web, about notifications.\nI do not want to just respond to a request anymore. I want to do something additional. Like: And I want it to be a first-class citizen of my new framework. Not something that I hacked around. All you need to have is a webserver and technology stack which does not limit you to processing the request but makes\nit possible for you to send any data at any time to any other connection. A webserver that supports more than HTTP.\nAnd there is an increasing number of solutions which let you do it, for example: Of course, all of that can still be achieved with current technologies. A lot of existing applications prove it to be\npossible. However, what I dream of is that it is not only possible, but simple. So I could have it working in\na few minutes. Rails was a revolution, I am still waiting for the next revolution. The first step is to have it working on one server. The next step would be a distributed environment which would let\nme deliver notifications to user friends even when they are connected to a different server. On one server I would probably have to stick to evented frameworks such as EventMachine or node.js. But I have so many\nbad experiences with EM that I do not even want to think about it. And none of those evented frameworks give you any\nadvantage when it comes to developing and maintenance of a distributed solution. What do you think about current state of the web and our tools? Is req-repl now becomming a burden in the development?\nWhich of our frameworks and tools are ready for the future of the web? Or perhaps I delude myself that all of that can be solved\nwith one tool. Maybe the usage of multiple tools for solving such problems is good because you can tweak them according\nto their needs and the needs of HTTP connection are different than those of long running, mostly inactive Websocket or\nSSE connections? I am very curious of your opinion ‚Ä¶ Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-02-15"},
{"website": "Arkency", "title": "Introducing hexagonal.js", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2013/02/introducing-hexagonal-dot-js/", "abstract": "There‚Äôs an idea we were working on for more than one year till now. As backend developers we were thrown into mysterious world of frontend (client-side) apps without any good pattern how to create Single Page App. So we ( GameBoxed + Arkency) invented one - hexagonal.js . Our main inspiration was Alistair Cockburn‚Äôs Hexagonal architecture . In short hexagonal.js is its JavaScript‚Äôs implementation, but with some unusual solutions and additional philosophy. Let‚Äôs focus on philosophy first, because it‚Äôll let you judge if you‚Äôre still interested in this idea. In this post I want to focus on client-side layer, because it is most interesting part. All snippets in this post are copied from hexagonal.js‚Äô hello-world project. Architecture is build with: You‚Äôre probably familiar with use case term. If you have some experience with DCI (or figured it out different way) you probably know, that use cases can be represented as objects - and this is core idea. Our story is quite simple: we want to greet user that uses app - ask for his name and greet him using name. As you can see it uses only plain objects and don‚Äôt care about booting, GUI or storage. This sample app has only one adapter, the most basic - GUI. Let‚Äôs have a look at code of GUI for just first step of UseCase - askForName. GUI#showAskForName shows simple form and binds to click event of its confirm button. It has no idea about domain objects and doesn‚Äôt contain any logic. You probably wonder how GUI know what to present and how can it interact with our business logic. hexagonal.js uses Glue objects to glue those two layers: Ok, so this part can be hard, because your don‚Äôt know what After means. It‚Äôs shortcut from YouAreDaBomb library, which can be described by following code: So basically - it adds to original function additional behaviour. There are also Before and Around functions that let you prepend or surround original function with additional behaviour. To make it all run we have to implement some booting code, that‚Äôll build all required objects: domain, glue, gui and other adapters and start use case. Here‚Äôs an example from hello-world app. I showed you some basics of hexagonal.js and now it‚Äôs time for your action. If you‚Äôre interested in this idea, please join our small community - we‚Äôre on Github , freenode , Google Groups and Twitter . In about two weeks our beautiful city will host wroc_love.rb conference. If you‚Äôre going to participate in this event I have small announcement for you - I‚Äôll provide QA session and hackathon / workshop on hexagonal.js on friday. It‚Äôs not an official part of conference and of course it will be free, to confront (production proved) idea with other developers. If you want to join please leave a comment, ping me on Twitter or just look for me on friday. There‚Äôll be also fight between ember.js and hexagonal.js on saturday. To be honest: whole agenda looks very promising. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-02-18"},
{"website": "Arkency", "title": "Naming in OOP", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2013/03/naming-in-oop/", "abstract": "There is a well known cite on naming in our industry: ‚ÄúThere are only two hard things in Computer Science: cache invalidation and naming things.‚Äù Phil Karlton To be honest it‚Äôs not Computer Science specific issue, but common problem for whole science. History of science is composed of discoveries and evolution of definitions. Every math theorem is based on some definitions - I would call it ‚Äútheorem dictionary‚Äù. There is also huge branch of philosophy aiming to figure out how our language influence on our thoughts. So I asked myself - how naming can influence my design. What can I learn from bad names? Let‚Äôs have a look at most important constraints of good name: Now it‚Äôs time to figure out what each constraint really mean and what‚Äôs more important - how can we validate it and what can we learn about our code from breaking the rule. Name has to inform about object‚Äôs or method‚Äôs reponsibility. It should be easy to use in sentence and easy to understand solution behind it. If name break this rule it may mean, that you have incorrect abstraction - not only on given entity‚Äôs level. Maybe this object / method has more responsibilities than one? It‚Äôs about being meaningful in given context - you should be able to use given name in one sentence with it‚Äôs parent name. If this rule is broken then you probably missed at least one abstraction. If you represent similar concepts you should use same name. It can help you extract common responsibility, but of course it will also make your code easier to understand. Breaking this rule should trigger following questions: Am I missing any abstraction? Isn‚Äôt that module or project too big? It‚Äôs easier to think about something when it‚Äôs short. It‚Äôs also easier to talk about it. But this rule is not only about name‚Äôs length, but also about using ‚Äúor‚Äù or ‚Äúand‚Äù in class or method‚Äôs name. If your name contains these conjunctions you may have problem with many responsibilities in given entity. It may be also just wrong name for given abstraction. Avoid using Hungarian Notation or any other that informs about type or other metadata. You should trust your coworkers, that they‚Äôll look for object‚Äôs type and they‚Äôll construct meaningful interfaces of their objects or methods. So breaking this rule is a sign of lack of trust in your team. I‚Äôm aware of writing incomplete set of naming smells and their impact on OOD, but I think it‚Äôs a great subject for further research. So if you feel that you have something to add don‚Äôt hesitate - write a comment. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-03-21"},
{"website": "Arkency", "title": "Is it cute or ugly?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/05/is-it-cute-or-ugly/", "abstract": "Yesterday day I presented a small code snippet to my dear coworkers\n(who I of course always respect) and (as usually) they were not impressed,\neven though I liked my solution. So I wanted to know your opinion about\nthis little piece of code. Let‚Äôs start with the beginning. The story is simple. You are writing code to communicate with external api. You are\nthoughtful programmer so you don‚Äôt store credentials in the code. You deploy to Heroku\nso obviously you keep things in ENV . First thought: My immediate concern: Why should the instance of my class know in its constructor about\nthe fact that we use ENV to store default login and password values. So perhaps we should\nuse some kind of factory that would create the object with provided values or the defaults ? But in Ruby every class is a factory, so why not use is to our advantage‚Ä¶ There is still something wrong here, I think. Why would anyone want to provide login,\nbut not password ? Or password without login ? Doesn‚Äôt make much sense to me.\nSo I decided to extract a new, little class. Does it make sense here to use Struct ?\nI think so, because Credentials.new('l', 'p').should == Credentials.new('l', 'p') .\nBut there are coworkers who disagree with me and I wonder what you think. Somehow this seems to be less readable to me Nice, but the knowledge about defaults was transffered from ApiProvider.new factory method\nto Credentials and I believe that Credentials should but just a dumb class responsible only for\nkeeping login and password always together. Because in terms of this api it never makes sense\nto operate separately on them. This leads to repeated code if there are multiple places that need to instantiate ApiProvider . Which way do you like the most ? Do you agree with me ? Or have I just\nearned uncountable amount of haters ? Do you think that using Struct is a bad practice sometimes ? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-05-09"},
{"website": "Arkency", "title": "The A Team", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2013/05/the-a-team/", "abstract": "I work in Arkency for almost 4 years and lately I‚Äôve started to think how it influences me. Working together is not only about delivering some good as a bunch of people - team has own identity, own culture. These days I decided to write down what I found out about my team for two reasons. First is just to see some day how it evolved over years - but that‚Äôs a good reason to write it in ‚ÄúMy sweet diary‚Äù, holden under a pillow. I‚Äôm also inspired by great blog series ‚ÄúWe are principled‚Äù by 8th Light. I find team culture as a mix of team members common goals and values, which are mainly focused on good they produce, but also how they see the future, their career etc. So here‚Äôs what I found out about my team: Produce good for the customer - nothing to say, it‚Äôs a common goal for all companies. Hacker-friendly job - we all are more or less nerds. We want to play with code, we want new challenges, we want to automate things. Educate - great hackers share their knowledge and discuss about ideas. And we want to be great hackers. Remoteness - work wherever you are and whenever you want. Collective code ownership - each line of code is owned by whole team, so when bug shows up it‚Äôs whole team‚Äôs problem, not author of latest commit or author of given line. Don‚Äôt repeat mistakes - learn from them, test against them. Reliability - customer can depend on you: your skills and your commitment. Anarchy - organizational loose coupling. Respect - you listen carefully to people you respect. Business orientation - always keep in mind what problem do you solve for your customer and your customer‚Äôs users - where the money come from or where can we save them. Know the large picture of project and it‚Äôs scale. What can I get from this set of goals and rules? It‚Äôs kind of model of this small world ‚Äì team‚Äôs physics. So like in all other models - it simplicize reality, but helps to understand what‚Äôs going on. Knowing current state you can also try to shape it into something different. It can also help you when you try to hire someone. So here‚Äôs question to you, kind reader: how is your team? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-05-14"},
{"website": "Arkency", "title": "How to track ActiveRecord model statistics", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2013/06/how-to-track-activerecord-model-statistics/", "abstract": "If you‚Äôre really serious about your application you have to collect and analyze its statistics. You can use Google Analytics or any other tool to track visits and basic events , or you can send specific events on demand. There‚Äôs also a way to automatically track ActiveRecord model creations and in this post I‚Äôll show you how easy it is. Let‚Äôs dig into the most important source code: I think you already know what it does - it binds to ActiveRecord::Base‚Äôs callback and puts appropriate message with time of creation and class name of created model. Then log messages are parsed with the following rake task: I just define how to look for and parse creation messages, which log file I want to check and for which date. Then both parsing and calculating result happens - if line matches to regexp and given date is one we are looking for it increments result for given model. So as a result you get the list of all model classes which instances were created on given day. You can check how it works using this sample project . In this example I assume, that the only method to persist information about created model is to use log messages. Of course it‚Äôs just a simplification. In real world you don‚Äôt want to gather all statistics in log: it can be time consuming to calculate the results, logs can be really big or rotated. For alternative persistence method you have to be aware of 2 things: If you dig into chillout gem you‚Äôll see how you can achieve that - you can use Thread.current to pass information about created models and middleware to get this information and send it to the storage - in our case to API endpoint. There are a few simple optimizations that will help you not to kill app‚Äôs performance when dealing with API, but that‚Äôs subject for another post. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-06-05"},
{"website": "Arkency", "title": "Are we abusing at_exit?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/06/are-we-abusing-at-exit/", "abstract": "If you are deeply interested in Ruby, you probably already know about Kernel#at_exit .\nYou might even use it daily, without knowing that it is there, in many gems, solving\nmany problems. Maybe even too many? Let me remind you some basic facts about at_exit . You can skip this section if\nyou are already familiar with it. The output of such little script is: Yeah. Obviously. You did not come to read what you can read in the documentation. So let‚Äôs\ngo further. In ruby you can terminate a script in multiple ways .\nBut what matters most at the for other programms is the exit status code. And at_exit block can change it. Let‚Äôs see it in action. But exit code might get changed in implicit way due to an exception: Output: But there is a catch. It will not change if the exit code was already set: See for yourself: But wait, there is even more: The documentation says: If multiple handlers are registered, they are executed\nin reverse order of registration . So, can you predict the result of the following code? Here is my output: So it is more like stack-based behaviour. There were even few bugs when this\nbehavior changed and things broke: Which brings us to minitest One of the best known example of using at_exit is minitest . Note: My little\nexamples are using minitest-5.0.5 installed from rubygems. Here is a simple minitest file: You can run it with ruby test.rb . As easy as that. But here is the question: How can minitest run our test if the test is defined after we require minitest ? You probably already know the answer: You can see that rspec is also using at_exit Minitest at_exit usage is a little complicated: But why does it need to use at_exit hook at all? Is it not some kind of hack?\nDon‚Äôt know about you, but it certainly feels a little hackish to me. Let‚Äôs see\nwhat we can do without at_exit ? It works: So we can imagine that if the mentioned issue was not a problem, we could trigger\nrunning specs at the end of file with one line and avoid using at_exit . But if we want to\nrun tests from multiple files situation gets more complicated. You can solve it\nwith a little helper: But then you need to keep Minitest.run out of your test files (to avoid running\nit multiple times), which make it impossible for us, to run tests from a single file, using\nthe old syntax that we are used to: ruby single_file_test.rb . We could dynamically require needed files in our script based on its\narguments like ruby helper.rb -- test.rb test2.rb . So with time we are getting\ncloser to building our own binary for running the tests. And I think that is what minitest is currently missing. Binary for running\ntests that would let you specify where they are. The only difference would\nbe that we would have to run our tests using minitest file_test.rb instead\nof ruby file_test.rb . Because the shipped binary would be starting and\nending point for our programs we would not have to use at_exit for\ntriggering our tests. After all it sounds way more logical to say program do something with file A by typing program a.rb instead of saying Ruby run file A and when you are finished do something completelly different\nthat is actually the main thing that I wanted to achieve . I hope you agree. We are starting our Rails apps with rails command or unicorn command or rackup command (or whatever webserver you use ;) ).\nWe do not start them by typing ruby config/environment.rb and running the web server in at_exit hook. So by analogy minitest file_test.rb sounds natural to me. But minitest is not the only one doing interesting things in at_exit hook.\nAnother very common example is capybara . Capybara is using at_exit hook\nto close a browser such as Firefox, when tests are finished. As you can see there is quite\ncomplicated logic around it: What could capybara do to avoid using at_exit directly? Perhaps a better way\nwould be to keep this kind of code dependent on test suite used underneath and\nspecify the hook via different gems such as capybara-minitest , capybara-rspec etc. It is now possible in some major frameworks: Of course at_exit is more universal, and capybara might be used outside of\ntesting environment. In such case I would simply leave the task of closing\nthe browser to the programmer. Sinatra is using at_exit hook to run itself (the application). I think it would be best if every long running and commonly used process such as\nweb servers or test frameworks provide there own binary and custom hooks for\nexecuting code at the end of a program. That way we could all forget about at_exit and live happily ever after. We were considering at_exit usage for\nour chillout gem to ensure that\nstatistics collected during last requests just before the webserver is stopped are also\nhappily delivered to our backend. Although we are still not sure if we want to go\nthat way. So much words said and I still gave you no reason for avoiding at_exit right?\nWell it seems that every project using this feature is sooner or later being hit by bugs\nrelated to its behavor and tries to find workarounds. Big kudos to Seattle Ruby Brigade (especially Ryan Davis) and Jonas Nicklas\nfor creating amazing software that we use daily. I hope you don‚Äôt mind a little\nrant about at_exit ;) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-06-22"},
{"website": "Arkency", "title": "Implementing worker threads in Rails", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/2013/06/implementing-worker-threads-in-rails/", "abstract": "If you care about your application performance you have to schedule extra tasks into background when handling requests. One of such tasks may be collecting performance or business metrics. In this post I‚Äôll show you how to avoid potential problems with threaded background workers. I was working on chillout client to collect metrics from ActiveRecord creations. Initially the code was sending collected metrics during the request. It was simpler but slowed down the application response to the customer. The response time was also fragile with regard to metrics endpoint availability. So I had the idea to start a worker thread in background responsible for that. Since everything worked like a charm in development, a deployment was inevitable. Then things started to get hairy. My production application was running on Unicorn and it was configured to preload application code. In that settings Unicorn master process will boot an application and next when code is loaded it will fork into several application workers. The problem with fork call is that only main thread survives it: Inside the child process, only one thread exists. It is made from a copy of the thread that called fork in the parent. This means that under any forking server (e.g Unicorn, Phusion Passenger) our background thread will die, provided it was started before process forked. You may think: I know, I‚Äôll use after_fork hook. And this might be solution for you and your specific web server. It definitely isn‚Äôt a solution when you don‚Äôt want to be tied to particular deployment option or explicitly support all webserver specific solutions. The other possibility is to start our worker thread lazily when it‚Äôs actually needed for the first time. A naive implementation may look like this: An attentive reader may notice that lazy starting solution applies to any kind of background worker threads - it will solve similar problems in girl_friday or sucker_punch . Now that we have lazy loading mechanism we‚Äôre good to deploy anywhere, right? Wrong! As soon as we deploy to threaded server (e.g. Puma) we‚Äôll encounter another problem. Since changing webserver model to threaded we will service several requests in one process concurrently. Each of these threads servicing request will be racing to start the worker in background but we want only one instance of the worker to be present. Thus we have to make worker starting code thread-safe: Now we‚Äôre good to go on any forking or threading web server. We‚Äôre covered even in such a rare case of webserver forking to threaded workers (does it actually exist?). Life is good. There‚Äôs one peculiar thing left. If you happen to use logger in your worker thread and it is BufferedLogger from Rails you‚Äôll be surprised to find out some of your messages don‚Äôt get logged. It‚Äôs a known and apparently solved issue. If you have to support apps which didn‚Äôt get the fix just remember to explicitly call flush on logger. You can see all the solutions from above applied in chillout gem. If you‚Äôre interested how we‚Äôre collecting metrics have look on How to track ActiveRecord model statistics . Happy hacking! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-06-17"},
{"website": "Arkency", "title": "Ruby and AOP: Decouple your code even more", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2013/07/ruby-and-aop-decouple-your-code-even-more/", "abstract": "We, programmers, care for our applications‚Äô design greatly. We can spend\nhours arguing about solutions that we dislike and refactoring our code to\nloose coupling and weaken dependencies between our objects. Unfortunately, there are Dark Parts in our apps - persistence,\nnetworking, logging, notifications‚Ä¶ these parts are scattered in our code - \nwe have to specify explicit dependencies between them and domain objects. Is there anything that can be done about it or is the real world a nightmare for purists?\nFortunately, a solution exists.\nLadies and gentlemen, we present aspect-oriented programming! Before we dive into the fascinating world of AOP, we need to grasp some concepts\nwhich are crucial to this paradigm. When we look at our app we can split it into two parts: aspects and components . Basically, components are parts we can easily\nencapsulate into some kind of code abstraction - a methods, objects or procedures.\nThe application‚Äôs logic is a great example of a component. \nAspects, on the other hand, can‚Äôt be simply isolated in code - they‚Äôre things\nlike our Dark Parts or even more abstract concepts - such as ‚Äòcoupling‚Äô or ‚Äòefficiency‚Äô. \nAspects cross-cut our application - when we use some kind of persistence (e.g. a database) or network communication (such as ZMQ sockets) \nour components need to know about it. Aspect-oriented programming aims to get rid of cross-cuts by separating aspect code from component code using injections of our aspects in certain join points in our component code. The idea comes from Java community and it may sound a bit scary at first\nbut before you start hating - \nread an example and everything should get clearer. Imagine: You build an application which stores code snippets. You can start one of the usecases this way: Here we have a simple usecase of inserting snippets to the application.\nTo perform some kind of SRP check, we can ask ourselves: What‚Äôs the responsibility of this object? The answer can be: It‚Äôs responsible for pushing snippets scenario. So it‚Äôs a good, SRP-conformant object. However, the context of this class is broad and we have dependencies - very weak, but still dependencies: Use case is a kind of a class which belongs to our logic. But it knows about aspects in our app - and we have to get rid of it to ease our pain! I have told you about join points. It‚Äôs a simple, yet abstract idea - and how can we turn it into something specific? What are the join points in Ruby?\nA good example of join point (used in the aquarium gem) is an invocation of method . We specify how we inject our aspect code using advice . What are advice? When we encounter a certain join point, we can connect it with an advice, which can be one of the following: While after and before advice are rather straightforward, around advice is cryptic - what does it mean to ‚Äúevaluate code around‚Äù something? In our case it means: Don‚Äôt run this method. Take it and push to my advice as an argument and evaluate this advice . In most cases after and before advice are sufficient. We‚Äôll refactor our code to embrace aspect-oriented programming techniques. You‚Äôll see how easy it is. Our first step is to remove dependencies from our usecase. So, we delete constructor arguments and our usecase code after the change looks like this: Notice the empty method user_pushed - it‚Äôs perfectly fine, we‚Äôre maintaining it only to provide a join point for our solution. You‚Äôll often see empty methods in code written in AOP paradigm. In my code, with a bit of metaprogramming, I turn it into a helper, so it becomes something like: Now we can test this unit class without any stubbing or mocking . Extremely convenient, isn‚Äôt it? Afterwards, we have to provide aspect code to link with our use case. So, we create SnippetsUseCaseGlue class: Inside the advice block, we have a lot of info - including very broad info about join point context ( jp ), called object and all arguments of the invoked method. After that, we can use it in an application like this: And that‚Äôs it. Now our use case is a pure domain object, without even knowing it‚Äôs connected with some kind of persistence and logging layer. We‚Äôve eliminated aspects knowledge from this object. Of course, it‚Äôs a very basic use case of aspect oriented programming. You can be interested in expanding your knowledge about it and these are my suggestions: Aspect-oriented programming is fixing the problem with polluting pure logic objects with technical context of our applications. Its usecases are far broader - one of the most fascinating usecase of AOP with a huge ‚Äòwow factor‚Äô is linked in the ‚ÄòFurther Read‚Äô section. Be sure to check it out! We‚Äôre using AOP to separate these aspects in chillout - and we‚Äôre very happy about it. What‚Äôs more, when developing single-page apps in Arkency we embrace AOP when designing in hexagonal architecture . It performing very nice - just try it and your application design will improve. Someone can argue: It‚Äôs not an improvement at all. You pushed the knowledge about logger and persistence to another object. I can achieve it without AOP! Sure you can. It‚Äôs a very simple usecase of AOP. But we treat our glues as a configuration part , not the logic part of our apps. The next refactor I would do in this code is to abstract persistence and logging objects in some kind of adapter thing - making our code a bit more ‚Äòhexagonal‚Äô ;). Glues should not contain any logic at all. I‚Äôm very interested in your thoughts on AOP. Have you done any projects embracing AOP? What were your use cases? Do you think it‚Äôs a good idea at all? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-07-13"},
{"website": "Arkency", "title": "CoffeeScript tests for Rails apps", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2013/07/coffeescript-tests-for-rails-apps/", "abstract": "You may know this pain too well - you‚Äôve created rich client-side in you Rails app and when your try to test CoffeeScript features it consumes much time to run all test scenarios with capybara and any of browser drivers (selenium, webkit, phantomjs). Let‚Äôs apply painkiller then - move responsibility of testing front-end to‚Ä¶ front-end. This is just a beginning of series about testing CoffeeScript in Rails stack, so if you‚Äôre familiar with basics - you know toolset and you know how to test your models - don‚Äôt waste your time. In next post I‚Äôll show how to extract existing views and write unit tests for them. Next I want to cover acceptance tests topic. If you‚Äôre interested just subscribe with RSS or mailing list . Let‚Äôs start with toolset, because it will influence a way we test - with frameworks‚Äô syntax and behaviours. I recommend you to use konacha gem - it‚Äôs dedicated for Rails apps, it uses mocha.js + chai.js as test framework and can be easily run in browser and command line. Each test suite is run in iframe, which prevents leaks on global state - both global variables and DOM. You can try jasmine or evergreen as well, but you‚Äôll eventually get back to konacha ;) I won‚Äôt run into details of konacha installation, but I recommend you to use :webkit or any other headless browser driver instead of default - selenium. You shouldn‚Äôt start with complicated tests of your views or any other hard piece of code. Start with testing small model or value object. Here‚Äôs how I would test Money value object: At first sight it should resemble RSpec with its newest ‚Äúexpectations‚Äù syntax. Let‚Äôs distinguish mocha.js and chai.js responsibility first. mocha.js provides test case syntax - so: #describe , #it , #beforeEach etc. chai.js is assertions library, so it defines #expect function and all matchers. I like expectation style, but you can use assertion or should as well - they all are wrappers on same concept of assertion. How test suite is built? It has root #describe which informs about object or feature under test - good practice is to use object‚Äôs constructor name . #describe (not only root one) function can call other #describe functions in it, but also test cases - #it and some setup and teardown code - #beforeEach and #afterEach accordingly. As I mentioned #it contains single test case - in perfect world it should always have one assertion. Test case without callback, so without function with test case‚Äôs body, will be marked as pending. Of course you have to remember to load object or function you want to test. Look at the first line - I use Rails‚Äô assets pipeline for this. Let‚Äôs get back to assertions. #expect function wraps result that we want to check - it can be result of function under test or function spy/mock . This wrapper provides chainable language to construct assertions - there are few special methods that are used just as chains, without any assertion: #to , #be , #been , #is , #that , #and , #have , #with , #at , #of and #same - they are just syntactic sugar. Let‚Äôs name few basic assertions: You‚Äôll find more chainable assertions in chai.js BDD API . Ok, you know how to write tests, but how can you run them? While developing feature it might be useful to run all tests in browser - it will be easier to debug by using console.log or browser‚Äôs debugger. You can serve all tests using following command: It will run server on http://localhost:3500/ with mocha.js HTML reporter. You can also run all tests with command line - you just have to use selenium or any headless browser. Konacha uses capybara as browser driver, so you can use any of provided capybara drivers like webkit, poltergeist etc. To run tests in command line just execute: You‚Äôve learned basics about testing CoffeeScript front-end in Rails stack. This is just a very beginning of blog series - in next posts I want to show how to extract and test already existing views, then how to write front-end-level acceptance tests. Of course if any other topic related to CS testing comes up I‚Äôll also write few lines about it, so don‚Äôt hesitate to comment. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-07-26"},
{"website": "Arkency", "title": "Single Table Inheritance - Problems and solutions", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/07/sti/", "abstract": "As a consulting agency we are often asked to help with projects which embrace\nlots of typical Rails conventions. One of the most common of them is the usage\nof STI (Single Table Inheritance). It is even considered best practice by some\npeople for some use cases [YMMV]. I would like to show some typical problems related to STI usage and\npropose different solutions and perhaps workarounds. It is very common for US-related projects to store customer billing and\nshipping address. In Poland, you might have multiple addresses also such as\nregistered address, home address, mailing address.\nSo I will use address as an example. Although I have\nmostly seen STI usage for different kind of notifications and for events\n(such as meeting, concert, movie, etc). Remember that this code is purely for demonstration of problems and solutions.\nUsing STI to implement billing and shipping address is just wrong.\nThere are many better solutions. Let‚Äôs say that your user can have multiple addresses. In the beginning everything always works. Things get complicated with time\nwhen you start adding new features. Obviously we are missing some validation.\nFor whatever reason, let‚Äôs assume that they need to differ between types. In\nour example ShippingAddress we would like to restrict number of countries. Of course, this is trivial example, and probably nobody would write it this\nway. But it will suit our needs and I have seen similar (in the technological sense\nof using STI and different validations per type) code in many reviewed projects. This code is possible in Rails 4 where building association with STI type was fixed . When using Rails 3 you will have to use workaround discussed in next paragraph also when creating\nnew record . STI is problematic when there is possibility of type change. And usually\nthere is. Frontend is displaying some kind of form and is responsible for\ntoggling visible fields depending on selected type of object and user can\nupdate object type. Very useful in case of user mistakes. Let‚Äôs see the problem in action: The problem is that we cannot change object class in runtime. This problem is not\nlimited to ruby, many object oriented programming languages suffer from it.\nAnd when you think about it, it makes a lot of sense. I think this tells us something about inheritance in general. It is very\npowerful mechanism but you should avoid it when there is possibility of type\nor behavior change. And favor other solutions such as delegation, strategy\nor roles. Whenever I want to use inheritance I ask\nmyself is it possible that such statement will no longer be truthful ?\nIf it is, avoid inheritance. Example: Admin < User . Is it possible that my User will no longer be an Admin . Yes! Ah, so being admin is more likely\na role that you have in organization. Inheritance won‚Äôt do. In fact I think there is very little place for inheritance when modeling real\nworld. Whenever your object changes properties at runtime and its behavior must\nalso change because of such fact, you will be better with delegation and\nstrategy (or creating new object). But, there are areas of code when I never had problem with inheritance\nsuch as GUI components .\nIt turns out that buttons rarely change into pop-ups :) . The workaround requires fixing Rails in two places. First the update_record method\nmust execute the query without restricting SQL update to the type of object\nbecause we want to change it. We also need a second method ( metamorphose ) that heavily relies on\nlittle known ActiveRecord#becomes method which deals with copying all the Active Record variables from one object to another . Let‚Äôs see it in action: There are two potential problems here: Would I recommend using such hack in production? Hell no! You can see in the\noutput that there is something wrong and check it easily: When going such route (but without hacking rails),\nI would probably create a new record with #metamorphose ,\nsave it, and destroy the old record if saving succeeded. All in transaction,\nobviously. But this might be even harder when there are lot of associations\nthat would also require fixing foreign key. Maybe destroying old record first, and\ncreating a new one with same id (instead of relaying on auto increment)\nis some kind of solution? What do you think? But finding workarounds for such Rails problems is a good exercise. Mostly through\nsuch debugging and looking at Rails internals I got better in understanding\nit and its limitations. I no longer believe that throwing more and more logic\ninto AR classes is a good solution. And the more you throw (STI,\nstate_machine, IdentityMap, attachments), the more likely you will experience\ncorner cases and troubles with migrations to new Rails version. OK, now that we know the solution that we don‚Äôt like, let‚Äôs look into something\nmore favorable. Instead of inheriting type, which prevents us from dynamically changing\nobject behavior, we are going to use delegation, which makes it trivial. Let‚Äôs see it in action: In our little example we are only delegating some validation aspects but\nin real life you would usually delegate much more. In some cases it might\nbe even worth to create the delegate with delegator as\na constructor argument, that will be used later in methods ( #to_s ). So our two objects change the role of delegate and delegator depending on the task they need to accomplish, playing\nlittle ping-pong with each other. That was fancy way of saying that\nwe created Circular dependency . Delegation is one of many techniques that we can apply in such case.\nPerhaps you would prefer DCI or Aspects instead.\nThe choice is always yours. If you feel the pain of having STI in your\ncode, switching to delegation might be simpler than you think. And if\nyou were to remember only one thing from this long post, remember that\nthere is #becomes and it might help you with creating different ActiveRecord object with the\nsame attributes. If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-07-27"},
{"website": "Arkency", "title": "Testing client-side views in Rails apps", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2013/09/testing-client-side-views-in-rails-apps/", "abstract": "In previous post I‚Äôve only showed you how to implement most basic tests for your front-end code. Now I want to show you how to unit test your views and, what‚Äôs more important, how to make your views testable. First let‚Äôs define what is the view in front-end app. View is an object responsible for presenting model to user as piece of HTML (DOM subtree) and giving ability to interact with system - by passing events based on click, key pressed etc. to controller or any other object. Depending on model‚Äôs complexity and quality of your code view object can be really big or small. It can just show one label or be a complex multi-step form - which could be container of smaller views, btw. ;) I will assume, that view also contains view-model - data object important in scope of view, but meaningless outside. Let‚Äôs start with something really simple - cyclic color change on button click. Let‚Äôs assume, that cycle contains only two colors: red and blue. You‚Äôve got following HTML: And following CoffeeScript: Looks pretty familiar, right? Before we can write test we have to do the first refactoring: separate definition from start-up. That‚Äôs really simple: Now we can test it. Let‚Äôs focus on what should be tested - what are our requirements for this piece of code. It should change Text‚Äôs color to red on odd clicks and to blue on even. We also want to start with blue color (you may notice there‚Äôs a bug in code - good catch!). Let‚Äôs start with ‚Äúodd clicks should mark Text‚Äôs color to red‚Äù requirement. Implementation of this first requirement will be also a foundation for all other tests. As you can see we need to deliver part of DOM that our colorChanger can bind to - we do it by copy&pasting our view‚Äôs HTML and appending to body node. Yes, this is a smell, but we‚Äôll get rid of this in next step of refactoring. Let‚Äôs focus on test case. We call colorChanger function which binds to existing DOM, then we click button - we use jQuery click event trigger. At last we check whether color of Text really changed to red. Now that we have test foundation we can implement missing test cases - Text should be blue by default, and after even number of clicks: You should have ‚Äúshould set color blue as a default‚Äù test case failing, because it‚Äôs not met with current code. I leave fixing colorChanger to pass tests as an exercise. Side note: If you‚Äôre going to use jQuery heavily you may want to install chai matchers for jQuery . The easiest way is to install konacha-chai-matchers gem - it contains many useful chai matchers easily embedable by asset pipeline. Let‚Äôs get back to smell introduced in view test foundation - HTML hardcoded in test suite. Of course the problem is that your app‚Äôs HTML may change, so you have to remember to update test‚Äôs HTML every time you touch similar subtree of DOM in real app. At first you may think of test‚Äôs HTML as a contract for your real app - if following HTML occured and function was called then declared behaviour should be applied. But that kind of thinking leads you to additional test for your Rails view - make sure that following HTML exists in given view. What‚Äôs worse - you still don‚Äôt have any relationship between back-end view test and front-end view test, so after 2 months you won‚Äôt remember why you test such thing. The other way is to move responsibility of rendering most of HTML from back-end to front-end. You may achieve it by using view objects with inlined HTML - good enough for a start. You may also use some templating language, especially one supported by asset pipeline, i.e. Handlebars.js . This leads us to new understanding of colorChanger . Previously it was just a function, that binds to already existing DOM subtree, and now we have to think about as an object, that can both render itself (or be rendered by something else) and bind to rendered DOM, to interact with user. Here‚Äôs how we can refactor our colorChanger to an object: There are things that ask for refactoring, but you see that main goal is achieved - our view object can be rendered inside of any container and then can receive click events from button. This makes it reusable and easier to maintain: If you want to test your already existing views follow these steps: In this post I‚Äôve tried to show you how to write tests for your front-end views and how to make them testable. Next time we‚Äôll try to write acceptance test for Single Page Application. If you want to follow this series just sign up to newsletter below. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-09-02"},
{"website": "Arkency", "title": "Developers oriented project management", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/09/arkency-survival-guide-to-effective-programming/", "abstract": "Working remotely is still a relatively new thing compared to going to an office,\nwhich has centuries long tradition. Despite its tremendous growth recently\n(especially in IT industry), there is not a lot of literature about best\npractices and working solutions. We still miss patterns for remote\ncollaboration. Many companies try remote working, but fail at doing it\neffectively. They go with the conclusion that it is a broken model. But the truth\nis, that working remotely is just different, and expecting it to be the same as\nstationary work, with just people in different places, is the biggest mistake\nthat one can made about it. To fully benefit from it, one must learn how to get\nmost out of it. You need to learn how to embrace remote work, instead of working around it. On the other hand,\nsome companies are already there, when it comes to remote working. Or at least\nthey think they are. But is there something more that we can strive for? More\nideas that we can try and benefit from? Many of them. As programmers and managers we are in constant search for techniques that can\nimprove our effectiveness. We want to deliver software faster and of better\nquality. And when projects succeed we need to scale developers team with the\ngrowth of the business. Imagine that you can add team members without much worrying\nof additional communication costs. Imagine your team reaching its full potential\ninstead of mediocre. That‚Äôs why we are writing a book, to help you. It will teach you how certain practices can empower\npeople working in your organization. How to avoid wasting time and resources on\nrepetitive ceremonies that bring little value. And how to communicate so that nobody\nneeds to hear you, yet everybody can listen you. However, it is not a book for project\nmanagers or business owners only. The advices here are also intended for\nprogrammers. Because as we all know, the change can be as well introduced bottom-up.\nAnd with this pack of knowledge, programmers can become great managers. So if\nyou ever felt undervalued as developer and wondered how a different environment,\nwhich gives you more responsibility, might look like, go and read it. The first chapter focuses on topic of story size. Specifically why using\nstories of size 1 helps you deliver and gives team members\na closure . And how it\nalso allows you to manage priorities on daily basis and avoid risk. The second chapter, that is currently being written by Andrzej Krzywda, is about\novercommunication. That is the essence of our communication. How a little too much\nis always better than a little not enough. What things are worth communicating\nand which new tools and techniques can help you deliver your message more\nclearly. Another topics that we hope to cover: The final release of Async Remote is already available. If you are not sure yet, but want to receive tips and excerpts from our upcoming\nbook, just subscribe to the newsletter below. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-09-16"},
{"website": "Arkency", "title": "Developers oriented project management: Story of size 1", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/09/story-of-size-1/", "abstract": "In one of our projects we decided to try a lot of new things in the area of\nproject management. One of the most beneficial change that I noticed was using very,\nvery small task as the primary tool to assign and track work. It‚Äôs the middle of the Wednesday. You thought it is gonna be such a good week.\nYou started coding your task on Monday, and you still keep working on it.\nYour boss keeps asking for status update. Customer would\nalso like to know how things are going. It‚Äôs the third day of working on\nit. It‚Äôs finally the time to deliver some code. You need to merge your branch\nwith master often to stay in the loop. And you can‚Äôt help much your friends\nworking on different part of the system. Lot of time put into the task,\nbut yet no visible effects to anyone except you so far. This whole situation\nfeels little stressful. Not only for you, but actually for everyone. This story might sound familiar to you. Maybe you don‚Äôt experience it every\nweek but surely every now and then. If not, consider yourself lucky! There\nare many factors that can lead to such situation but one of the problem is\nusually the size of the story (ticket). It‚Äôs just too big. The solution?\nMake it small. How small? Really small. About the size of one point. Story of size one has few constraints: We sticked with this rule because it turned out to be beneficial: This technique can be used for managing all kinds of projects but in our case\nit was battle tested on full team working remotely. Remote projects and teams\nhave their own nature and small tasks fits great in this environment. Did you like this article? You can find out more on this and similar topics\nin our book Async Remote .\nYou can also subscribe to the newsletter below if you want to receive tips and excerpts from our work on the book.\nThey will be similar in form to this blogpost. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-09-23"},
{"website": "Arkency", "title": "Throw away Sprockets, use UNIX!", "author": ["Mateusz Lenik"], "link": "https://blog.arkency.com/2013/09/throw-away-sprockets-use-unix/", "abstract": "The Sprockets gem is the standard way to combine asset files in Rails, but it\nwasn‚Äôt very straightforward to use in stand-alone projects, like Single Page\nApplications without backend, before the sprockets command was added. Few weeks ago I realized that Sprockets solve the problem that has been\nalready solved, but in a different language and in different era of computing. Later I wanted to check whether my idea would actually work and started\nhacking. You can see the results below. The designers of C language had to solve a similar problem, so they came up\nwith a preprocessor that understands directives that allow concatenating\nmultiple files into one. Additionally, it offers some macros and other stuff,\nbut it isn‚Äôt really important in this application. In most UNIX-like systems there exists a separate binary, called cpp , that\ncan be used to invoke the preprocessor. Its key feature here is that it can be used with any programming language, not\nnecessarily C, C++ or Objective-C. Say I have two files, one called deep_thought.coffee and the other one called answer.coffee . They‚Äôre listed below. answer.coffee : deep_thought.coffee : Now let‚Äôs run the preprocessor and see what happens. Looks like it‚Äôs what we need. The only thing that‚Äôs left to do is to compile\nthe file. As you can see from the above, there is no magic and even old UNIX tools can\nget this work done properly. The short answer is yes. To prove this I resurrected the hexagonal.js\nimplementation of TodoMVC and replaced coffee-toaster with a Makefile listed below. That‚Äôs it. There are three targets defined: debug , release and clean . The\ndefault one is debug . .PHONY just means that there are no dependencies for\nthese targets and they should be executed every time. You can see all the relevant changes in this\ncommit .\nTo compile it, just run make from the command line and given you have coffee and cpp command line utilities installed, it just works! To check it I modified the Makefile to run Sprockets and performed simple\nbenchmark. I ran both versions in the clean environment 50 times and took an\naverage. The run time for Sprockets doesn‚Äôt include the time of running bundle\nexec . You can see the modifications on a separate\nbranch . The cpp took 0.23 seconds to compile the assets, while for Sprockets it was\n1.57 seconds, which is almost seven times slower! Looks like it is doing a lot\nmore work than is needed to just compile few CoffeeScript files. You can easily perform similar benchmark using the time command if you don‚Äôt\nbelieve the results. You may have noticed some differences in the output file produced by the cpp solution. There is only one wrapping anonymous function on the top level. This\nis because it first concatenates all CoffeeScript files and then it compiles\none big file.  Sprockets work the other way around - the files are compiled and\nthen they are concatenated. That allows mixing JavaScript and CoffeeScript\nfiles. Comments in CoffeeScript files don‚Äôt work either, because they are treated as\ndirectives for the preprocessor and are reported as errors. At Arkency we\nrarely use comments in the code - we believe that the code should be always\nreadable without needing additional explanation in the comment. It isn‚Äôt an\nissue if you do the same. The performance may be also a problem, even though the benchmarks show that cpp is clearly faster. However, when a single file is modified in the large\nproject, Sprockets recompile only that file, whereas in this solution all\nimported files need to be recompiled. The problem with Sprockets is that they are responsible for doing lot of tasks.\nThey have to manage the dependencies, run the compiler and then concatenate all\nthe resulting files. It is clearly, against the UNIX way. There should be one\ncomponent for each task. The make command can be used to schedule the\ncompilation, compiler should only do the compilation, another tool should\ncreate the dependency map and yet another one should put the resulting files\ntogether using the compiled results and the dependency map. That‚Äôd be the UNIX\nway to solve this problem! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-09-23"},
{"website": "Arkency", "title": "Services - what are they and why we need them?", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2013/09/services-what-they-are-and-why-we-need-them/", "abstract": "Model-View-Controller is a design pattern which absolutely dominated web frameworks.\nOn the first look it provides a great and logical separation between our application components. When we apply some basic principles (like ‚Äòfat models, slim controllers‚Äô) to our application, we can live happily very long with this basic fragmentation. However, when our application grows, our skinny controllers become not so skinny over time. We can‚Äôt test in isolation, because we‚Äôre highly coupled with the framework. To fix this problem, we can use service objects as a new layer in our design. I bet many readers had some experience with languages like C++ or Java. This languages have a lot in common, yet are completely different. But one thing is similar in them - they have well defined entry point in every application. In C++ it‚Äôs a main() function. The example main function in C++ application looks like this: If you run your application (let it be ./foo), main function is called and all arguments after it ( ./foo a b c ) are passed in argv as strings. Simple. When C++ application grows, nobody sane puts logic within main . This function only initializes long-living objects and runs a method like start in above example. But why we should be concerned about C++ when we‚Äôre Rails developers? As title states, Rails has multiple entry points. Every controller action in Rails is the entry point! Additionaly, it handles a lot of responsibilities (parsing user input, routing logic [like redirects], logging, rendering‚Ä¶ ouch!). We can think about actions as separate application within our framework - each one with its private main . As I stated before, nobody sane puts logic in main . And how it applies to our controller, which in addition to it‚Äôs responsibilities takes part in computing response for a client? That‚Äôs where service objects comes to play. Service objects encapsulates single process of our business . They take all collaborators (database, logging, external adapters like Facebook, user parameters) and performs a given process. Services belongs to our domain - They shouldn‚Äôt know they‚Äôre within Rails or webapp! We get a lot of benefits when we introduce services, including: Ability to test controllers - controller becomes a really thin wrapper which provides collaborators to services - thus we can only check if certain methods within controller are called when certain action occurs, Ability to test business process in isolation - when we separate process from it‚Äôs environment, we can easily stub all collaborators and only check if certain steps are performed within our service. Lesser coupling between our application and a framework - in an ideal world, with service objects we can achieve an absolutely technology-independent domain world with very small Rails part which only supplies entry points, routing and all ‚Äòmiddleware‚Äô. In this case we can even copy our application code without Rails and put it into, for example, desktop application. They make controllers slim - even in bigger applications actions using service objects usually don‚Äôt take more than 10 LoC. It‚Äôs a solid border between domain and the framework - without services our framework works directly on domain objects to produce desired result to clients. When we introduce this new layer we obtain a very solid border between Rails and domain - controllers see only services and should only interact with domain using them. Let‚Äôs see a basic example of refactoring controller without service to one which uses it. Imagine we‚Äôre working on app where users can order trips to interesting places. Every user can book a trip, but of course number of tickets is limited and some travel agencies have it‚Äôs special conditions. Consider this action, which can be part of our system: Although we packed our logic into models (like agency, trip), we still have a lot of corner cases - and our have explicit knowledge about them. This action is big - we can split it to separate methods, but still we share too much domain knowledge with this controller. We can fix it by introducing a new service: As you can see, there is a pure business process extracted from a controller - without routing logic. Our controller now looks like this: It‚Äôs much more concise. Also, all the knowledge about process are gone from it - now it‚Äôs only aware which situations can occur, but not when it may occur. You can easily test your service using a simple unit testing, mocking your PaymentAdapter and Logger. Also, when testing controller you can stub trip_reservation_service method to easily test it. That‚Äôs a huge improvement - in a previous version you would‚Äôve been used a tool like Capybara or Selenium - both are very slow and makes tests very implicit - it‚Äôs a 1:1 user experience after all! Services in Rails can greatly improve our overall design as our application grow. We used this pattern  combined with service-based architecture and repository objects in Chillout.io to improve maintainability even more. Our payment controllers heavy uses services to handle each situation - like payment renewal, initial payments etc. Results are excellent and we can be (and we are!) proud of Chillout‚Äôs codebase. Also, we use Dependor and AOP to simplify and decouple our services even more. But that‚Äôs a topic for another post. What are your patterns to increase maintainability of your Rails applications? Do you stick with your framework, or try to escape from it? I wait for your comments! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-09-26"},
{"website": "Arkency", "title": "Developers oriented project management: Take the first task", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/10/take-the-first-task/", "abstract": "We recently talked how small and unassigned tasks can help you manage the risk in the project . In short: they help to\nguarantee that despite the obstacles people keep working on the most important\nthings. But to take most from such\napproach we need one more rule. Let‚Äôs talk a little about it. When project managers assign tasks to developers they will usually try to do it\nbased on who they think are most qualified for them. Unfortunately when\ndevelopers are in charge of assigning stories the effect is not much better. They\nwill usually try to take tasks, that they feel most comfortable with. Frontend\ndevelopers will take frontend tasks. Developers with more backend background\nwill take backend tasks. The long term the effect is that people specialize in\nparticular areas of code and Collective Ownership declines . And this is exactly\nwhat we would like to avoid. This happens because sometimes the developers have different goals to your goals.\nThey want to make their job easy or fun, where the goal of the project owners is\nto have the most important task get done. And the most important task is not\nalways the most compelling, neither related to the most fancy, recent technology\nthat the developers would like to learn and use The solution is to have very strict and simple rule for the developers to follow. And the\nrule says Take the first task . Where First means unstarted tasks with highest\nbusiness value. When I join the project as a developer for the first time, or\non Monday after weekend, or after a two-week-long vacation, I am interested in\none thing only: What can I do for you? . The answer should be immediately\nvisible for me. I do not care if you use Pivotal , Redmine or Trello . I just\nwant to look at the top of the list and finish your most important task today.\nGet it done, deliver on production and forget about it. You can find out more in our book: Async Remote or if you are not sure yet, by subscribing to the newsletter with tips and excerpts from the book similar in form to what you just read. The next blog post will be about Project Managers . Whether you actually need them\nand what are the alternatives to hiring them. Subscribe to be notified when it is out. Is it immediately clear to you in your current project what you should be\nworking on as a developer? How long does it take to figure it out? Do you need to\ntalk to someone to get that information? Leave your feedback in comments or\non twitter . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-10-09"},
{"website": "Arkency", "title": "Developers oriented project management: Leave tasks unassigned", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/10/refactor-to-remote-leave-tasks-unassigned/", "abstract": "As with software, improving your company is an ongoing process consisting of\nmultiple small steps . If you want your company to be more remote-friendly it\nis also not all-or-nothing decision. You can gradually start using techniques that\nwill bring you closer to your goal. This series of blogposts is not about why you should go remote, but rather how . Also, we believe that those techniques\nwill help you even if you are not planning to work remotely. The first part was about having very small stories and how it helps everyone track progress and provides closure.\nToday we are going to talk about leaving tasks unassigned instead of delegating\nthem to particular developers at the beginning of an iteration. Let‚Äôs start with an assumption. You want your developers to be doing the most\nimportant task for the project that can be currently done . Usually this is one\nof the most important criteria taken into consideration when assigning a task. So\nlet‚Äôs say you have three developers and ten tasks to be done during the iteration.\nHere is how you decided to split the work. Looks very promising. But here comes the reality. The most important task (no. 1)\ntakes longer than anticipated. And suprisingly one of the tasks (no. 3) turned out to\nbe easier and was finished earlier. What effect does it have for your project? Multiple times during this iteration\npeople are working on tasks which are not most important . The first developer is\nworking on task 6, when task 4 and 5 are not even started. Task 7 won‚Äôt be\nfinished by the end of the sprint even if it could be done instead of task 8.\nAnd so on‚Ä¶ You can simply avoid these kind of problems by leaving tasks unassigned and\nprioritized. Let your developer start the most important task when they are\nfree working on anything else. If one of your developers gets sick or needs a little break your most\nimportant tasks might be damaged. If you look at the original estimation\nimage the third developer is crucial to the success of this iteration. If (s)he\nleaves the team due to health or personal reasons for some time there will\nbe some damage. The effects are similar to the ones mentioned in the previous\nparagraph. People are not working on the most important tasks, the ones bringing the most business value . If you want most possible freedom for your developers and coworkers they cannot\nbe tight to the project they are currently working on. If your prioritiy is\nfor them to balance their personal and work life you cannot relay that they will\nbe available for given number of hours for the work on the project. Not to\nmention the fact that the performance of developers vary greatly on daily\nbasis depending on their mood, health, and many external factors. There is only\none way to manage it. Surrender ;). Accept the fact that our power over reality\nis limited and adapt often and quickly. Leave unassigned tasks to be done and let people\ndo them. If someone is not working today, someone else can simply do the task\ninstead. Small, unassigned tasks are your tools for managing risk on daily basis.\nThe problems are constantly attacking our projects from all sides. People\neffectiveness vary, illness happen, personal life interferes, tasks take longer,\nexternal events occur. To finish things as planned is almost impossible. So\nwe need guidance that will help us deliver as much business value much as possible\nand being constantly focused on most important aspects of our project. We need\nflexibility in managing the project. The more the better. But good things also\nhappen. Sometimes people suprise us with their solutions and performance.\nOpportunities arrive and we would like to take them. So we need elasticity to\nmanage all that complexity that a software project is. And small, unassigned\ntasks bring you that . Also it lowers the amount of micrmanagement and attention that the project\nneeds. Instead of daily assigning tasks to people you just need to set their\nprorities which you do for everything anyway. There is more beyond what we wrote in this article. Find out in our Async Remote ebook\nor by subscribing to the newsletter with tips and excerpts from the book similar in form to what you just read.\nThe newsletter will also containt exclusive content from the book that is\nnot published on the blog. The next blog post will be about Collective Ownership and what it has in\ncommon with the techniques that we already described. Subscribe to make sure you do not miss it. Does your company practice these techniques? How is that working for you? Share your opinion in comments or on twitter . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-10-04"},
{"website": "Arkency", "title": "Developers oriented project management: What our new book is all about?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/10/what-our-new-book-is-all-about/", "abstract": "At first, we thought that our new book is all about remote\nwork. But then after writing few chapters we saw that it is not. That those\ntechniques can be applied to any team. So what is it all about? And happiness. Exactly this kind of happiness related to working in a nicely organized project. Because\neven if we develop applications in our favorite programming language (Ruby\nanyone?) we can still be in a lot of stress due to the way the project is\nmanaged, right? Are developers feeling unhappy in your current project? Is it draining energy\nfrom them? Maybe the whole team is trapped in micro-management and does not\nknow a way of escape from it. Perhaps peoples skills stuck on certain level and\neverybody is working on the same, fenced parts of the code over and over,\nleading to an isolation. Possible you are all working from the office, when\nsometimes you just want need to be alone to focus fully. And even when you\ndeliver new features you still feel like nothing is actually finished-finished.\nBut does it have to be like that? Is your project easy to manage for the managers,\nto work with for the programmers and to follow by the customer? What if programmers working on a project could be more happy? Feel trusted and enjoy tremendous freedom . What if they could experience progress in their\nskills, learn and master new technologies without having to do it only in\ntheir free time? Could the team be like a small community, supportive and\ninterested in each other problems? What can help making such team? What if you could improve many areas of your current project management\nthat has a direct effect on you and your team happiness.\nLike immedietaly knowing what you should be working on when you start your day.\nDelivering something valuable even twice a day and being proud of that.\nBeing able to organize your time according to your liking and priorities ?\nTake your time in the middle of the day to create a meal compatible with\nyour diet, exercise, walk  or take a lovely nap to refresh your mind.\nWhat if tasks in unfamiliar technology are learning opportunities and\nsmall challenges and instead of being scary monsters? Can you imagine\nyour team collaborating together on the whole code? Our ebook is for you if you value Independence, Clarity, Focus, Freedom and Collaboration .\nIt contains a list of guidelines that we established at Arkency throughout years\nof working remotely . You can apply them to your project slowly and every\none of them will help you improve some of the aspects. Together they make\na tremendous difference and let you enjoy lot of the benefits that a\nprogrammer job can offer. They create a programmer friendly environment in\nwhich you can feel comfortable and productive. After all, most of your team\nconsist of programmers, so the project should be optimized for their\nefficiency and happiness. But it also creates a nice set of rules that make the customers\nand product owners communication with developers easier. As programmers we are privileged. In our lives we can enjoy many things that\npeople doing other kinds of job cannot. But we need a work-flow that will let\nus actually seize the opportunities such as eg. the ability to work remotely. As team leaders and company owners we always want to hire and work with\nthe best people. But they also have very high expectations. Is your company\nready to provide them a nicely working experience to benefit most from their\ntalent and experience? We are also always overloaded with tons of other duties\nand activities so we would like the dev team to take care of itself as much as possible .\nIt would be best if they could self-organize the work, find priorities and assign\ntasks. But do we let them? Do we know how? Don‚Äôt worry, we will show you\nhow. When we started writing the book, we promised that we will help your team\ntransition into remote working and\nwe will still do that. Because the crucial ingredient for successful remote\nworking is project organization and excellent communication . And we intend to\nwrite about that. But you can still benefit greatly by following the advices from\nthe book even if remote working is not your highest priority right now. You can find out more in our book: Async Remote ebook or if you are not sure yet, by subscribing to the newsletter with tips and excerpts from the book similar in form to what you can read in our previous blog posts: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-10-14"},
{"website": "Arkency", "title": "3 ways to do eager loading (preloading) in Rails¬†3¬†&¬†4¬†&¬†5", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2013/12/rails4-preloading/", "abstract": "You are probably already familiar with the method #includes for eager loading\ndata from database if you are using Rails and ActiveRecord.\nBut do you know why you someties get few small and nice SQL queries and sometimes\none giant query with every table and column renamed? And do you know\nabout #preload and #eager_load which can help you achieve the same goal?\nAre you aware of what changed in Rails 4 in that matter? If not, sit down and\nlisten. This lesson won‚Äôt take long and will help you clarify some aspects\nof eager loading that you might not be yet familiar with. Let‚Äôs start with our Active Record class and associations definitions that we\nare going to use throughout the whole post: And here is the seed data that will help us check the results of our queries: Typically, when you want to use the eager loading feature you would use the #includes method, which Rails encouraged you to use since Rails2 or maybe even\nRails1 ;). And that works like a charm doing 2 queries: So what are those two other methods for? First let‚Äôs see them in action. Apparently #preload behave just like #includes . Or is it the other way around?\nKeep reading to find out. And as for the #eager_load : It is a completely different story, isn‚Äôt it? The whole mystery is that Rails\nhas 2 ways of preloading data. One is using separate db queries to obtain the additional\ndata. And one is using one query (with left join ) to get them all. If you use #preload , it means you always want separate queries. If you use #eager_load you are doing one query. So what is #includes for? It decides\nfor you which way it is going to be. You let Rails handle that decision.\nWhat is the decision based on, you might ask. It is based on query conditions.\nLet‚Äôs see an example where #includes delegates to #eager_load so that there\nis one big query only. In the last example Rails detected that the condition in where clause\nis using columns from preloaded (included) table names. So #includes delegates\nthe job to #eager_load . You can always achieve the same result by using the #eager_load method directly. What happens if you instead try to use #preload explicitly? We get an exception because we haven‚Äôt joined users table with addresses table in any way. If you look at our example again you might wonder, what is the original intention of this code.\nWhat did the author mean by that? What are we trying to achieve here\nwith our simple Rails code: Do you know which goal we achieved? The first one. Let‚Äôs see if we can\nachieve the second and the third ones. Our current goal: Give me users with polish addresses but preload all of their addresses. I need to know all addreeses of people whose at least one address is in Poland. We know that we need only users with polish addresses. That itself is easy: User.joins(:addresses).where(\"addresses.country = ?\", \"Poland\") and we know\nthat we want to eager load the addresses so we also need includes(:addresses) part right? Well, that didn‚Äôt work exactly like we wanted.\nWe are missing the user‚Äôs second address that expected to have this time.\nRails still detected that we are using included table in where statement\nand used #eager_load implementation under the hood. The only difference compared to\nprevious example is that is that Rails used INNER JOIN instead of LEFT JOIN ,\nbut for that query it doesn‚Äôt even make any difference. This is that kind of situation where you can outsmart Rails and be explicit\nabout what you want to achieve by directly calling #preload instead of #includes . This is exactly what we wanted to achieve.\nThanks to using #preload we are no longer mixing which users we want to fetch\nwith what data we would like to preload for them. And the queries are plain\nand simple again. The goal of the next exercise is: Give me all users and their polish addresses . To be honest, I never like preloading only a subset of association because some\nparts of your application probably assume that it is fully loaded. It might only\nmake sense if you are getting the data to display it. I prefer to add the condition to the association itself: And just preload it explicitely using one way: or another: What should we do when we only know at runtime about the association conditions\nthat we would like to apply? I honestly don‚Äôt know. Please tell me in the\ncomments if you found it out. You might ask: What is this stuff so hard? I am not sure but I think most ORMs\nare build to help you construct single query and load data from one table. With\neager loading the situation gest more complicated and we want load multiple data\nfrom multiple tables with multiple conditions. In Rails we are using chainable API\nto build 2 or more queries (in case of using #preload ). What kind of API would I love? I am thinking about something like: I hope you get the idea :) But this is just a dream. Let‚Äôs get back to reality‚Ä¶ ‚Ä¶ and talk about what changed in Rails 4. Rails now encourages you to use the new lambda syntax for defining association\nconditions. This is very good because I have seen many times errors in that\narea where the condition were interpreted only once when the class was loaded. It is the same way you are encouraged to use lambda syntax or method syntax to\nexpress scope conditions. In our case the condition where(country: \"Poland\") is always the same, no matter wheter interpreted\ndynamically or once at the beginning. But it is good that rails is trying to\nmake the syntax coherent in both cases (association and scope conditions)\nand protect us from the such kind of bugs. Now that we have the syntax changes in place, we can check for any differences\nin the behavior. Well, this looks pretty much the same. No surprise here.\nLet‚Äôs add the condition that caused us so much trouble before: Wow, now that is quite a verbose deprection :) I recommend that you read\nit all because it explains the situation quite accuratelly. In other words, because Rails does not want to be super smart anymore and\nspy on our where conditions to detect which algorithm to use, it expects\nour help. We must tell it that there is condition for one of the tables.\nLike that: I was wondering what would happen if we try to preload more tables but\nreference only one of them: I imagined that addresses would be loaded using the #eager_load algorithm (by doing LEFT JOIN ) and places would be loaded using\nthe #preload algorithm (by doing separate query to get them) but\nas you can see that‚Äôs not the case. Maybe they will change the\nbehavior in the future. Rails 4 does not warn you to use the #references method if you\nexplicitely use #eager_load to get the data and the executed\nquery is identical: In other words, these two are the same: And if you try to use #preload , you still get the same exception: If you try to use the other queries that I showed you, they still work\nthe same way in Rails 4: Finally in Rails 4 there is at least some documentation for the methods,\nwhich Rails 3 has been missing for years: Check out how you can test eager-loading the associations and make sure the code actually makes as many queries as you expected. There are 3 ways to do eager loading in Rails: #includes delegates the job to #preload or #eager_load depending on the\npresence or absence of condition related to one of the preloaded table. #preload is using separate DB queries to get the data. #eager_load is using one big query with LEFT JOIN for each eager loaded\ntable. In Rails 4 & 5 you should use #references combined with #includes if you\nhave the additional condition for one of the eager loaded table. If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-12-08"},
{"website": "Arkency", "title": "CoffeeScript acceptance tests", "author": ["Jan Filipowski"], "link": "https://blog.arkency.com/2013/12/coffeescript-acceptance-tests/", "abstract": "You‚Äôve already learned how to implement simple model and view tests, so as I promised now I‚Äôll show you how you can introduce acceptance tests on client-side. We‚Äôre not too far from this goal - you know how to write test cases, make assertions and ‚Äúclick through DOM‚Äù. Let‚Äôs apply some abstraction then - to look at our app like the end user. First let‚Äôs think what our application really is - it could be single page app that control whole DOM or just a widget (subtree of DOM). The real question is where would you put the border between widget and rest of HTML. Here are some aspects of widget that should help you finding the border - from most to least important: If you go with the approach presented in Testing client-side views in Rails app you should be able to extract widget you‚Äôve found - for a moment you can just assume it‚Äôs a huge view with a state and access to external services (called ‚Äúbig ball of mud‚Äù). You know how to unit test the view, even if the unit is so big. The job is to handle external services interactions with mocks and write tests as scenarios using higher-level language. Your application may use backend via AJAX, WebSockets or external library with backend - like Facebook‚Äôs JS SDK. You may try to use real data sources, but it will be hard - if you use konacha gem you won‚Äôt have easy access to your backend, it won‚Äôt be easy to clear state on backend or in external service. So it will be easier to just mock them - it violates end-to-end testing principles, but I didn‚Äôt find better way yet. Please remember that there might be external services with easy access from DOM - like LocalStorage, Web Audio API - some of them could be used directly with no need to mock them, but you still might need to mock other - i. e. if you don‚Äôt want to hear the sound when testing application which uses Web Audio API. You probably know what Cucumber is, but just to remind you - testing framework that uses Gherkin DSL to describe context and expectations. You write your test suite almost in natural language and it translates instructions to real actions and assertions. I‚Äôm not a big fan of this approach, however it really influenced me on how should acceptance scenario look like. Such test should focus on end-user perspective - how one interacts with GUI and sees results. Let‚Äôs have the first attempt on writing test for sample TODO application using such perspective. Yeah, you‚Äôre right - it‚Äôs not end user perspective. It‚Äôs jQuery perspective. Let‚Äôs fix this with our own capybara-like wrapper for DOM, based on jQuery: It‚Äôs still focused on GUI details, but for small scenarios it may be good enough. For longer scenarios you can extract chunks of interaction, like logging in, adding product to cart etc. In this blog series I didn‚Äôt cover few interesting, but quite heavy, topics: object orientation (like bbq ), implementing own, meaningful assertions in chai.js and technics to work with 3rd party services. If you‚Äôre interested please leave your email in the form below. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-12-23"},
{"website": "Arkency", "title": "37 signals was not lying, you win by being remote", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/2014/01/you-win-by-being-remote/", "abstract": "There is such moment in developer‚Äôs life when you start looking for a new job, sooner or later. You can observe that even in Poland, there are plenty of Ruby on Rails job offers, often in very perspective companies. I probably could find interesting job in Pozna≈Ñ, where I live, but there were some presumptions which pushed me to apply to Arkency. Due to my life situation becoming a bit complicated and me feeling totally whacked out, I needed to find workplace where I can feel really comfortable. I wanted it to be a place where I could develop my skills and face new challenges. Few months ago I have found ‚Äúwebworktravel‚Äù facebook page and started thinking that it would be great to have a possibility to work this way. I developed my first commercial project as a remote team member, so I was close to this nominally. Unfortunatelly I was studying simultaenously and it was really hard for me to achieve this. Then I run for three and a half year of ‚Äúclassic‚Äù office work. During this, I had some bad experiences in cooperating with some remote workers and I tried to figure why some things gone wrong and whether it is possible to make things right in the future. I started reading articles about remote work and trying to figure out how to handle it properly. Then Arkency released preview of their Async Remote book. I bought it without a falter and read within single breath. Suddenly, 37signals released ‚ÄúRemote‚Äù. I was joking that maybe it would be as good as the Arkency one. I started to believe again that remote work could be possible if company has well organised processes to handle this way of working. When I was looking for a job I received some propositions, mainly with relocation to some other cities in Poland or to Berlin, Germany. Some of the companies looked promising to me, some of them even offered remote work. I was able to relocate, but working remotely was an interesting alternative for me. Even so I felt that I could become some kind of a dropout because of not being in the office and having less contact with the rest of the team. Working remote, but still from 9am till 5pm makes not really big sense for me. In this case, the only value is no need of relocation. Then I realised that there‚Äôs a team that works remotely and asynchronously. Asynchrony is the key factor which makes remote working really attractive. It‚Äôs really great feature when you can work in your comfort zone, you can go to the doc or plan your working hours to spend more time with your family, especially when your spouse doesn‚Äôt have such flexible work. I have known some of Arkency guys and I have known their skills, care about proper design and engagement in Ruby community. Their articles are often published in Ruby Weekly. That really impressed me, but thing which tipped the scale was their approach to remote work, described in the Async Remote book. During first day of my new job I was coworking, pair programming and when I was travelling back home from Wroc≈Çaw, I started writing this blog post. Cool, huh? At last my new company is developer oriented. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-01-09"},
{"website": "Arkency", "title": "Rails and SOA: Do I really need to have this big app?", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2013/12/rails-and-soa-do-i-really-need-to-have-this-big-app/", "abstract": "Developing complex web applications (not particularly Rails apps) is a tricky\ntask. You write your models, controllers, services and optimize application\narchitecture iteratively. But even with the great architecture when your application starts to be huge, bad things happen. Every little feature you‚Äôll add will cost you precious time - tests must pass on your CI and workers needs to be reloaded every time you deploy. In addition, you begin to realize that mental overhead with keeping modules separated is huge - you have to remember a lot of things to keep it in right fashion. Ultimately, working on a project is hard, TDD technique is inefficient to use and you‚Äôre not happy at all with your code. Have something gone wrong? No. You just didn‚Äôt notice that you need more\napplications than just one. Complex applications tend to create whole ecosystems inside of them - we can\nthink of such applications as of galaxies. Inside these galaxies we have stars - our services and adapters. \nThey should be separated, but they‚Äôre still in the one galaxy which \nis our big application. What glues these stars together is\ntheir purpose - they‚Äôre created by us to solve certain problems. Service Oriented Architecture takes different approach. We can put the same\nstars into different galaxies and add an explicit communication between them. This way we create a solid boundaries between our services and make our solution simpler to maintain - working on a small Rails application is easy, right? The most attractive thing about this kind of architecture is an ease of working with smaller applications - you have a small subset of tests and external libraries. Your mini-application has a narrow scope, business domain is singular (as opposed to your previous approach, where you had payments and business logic inside a single app - maybe separated, but still inside one app) and you don‚Äôt need to have such sophisticated architecture inside - when you choose Rails to implement it, you can even be fine with your ActiveRecord and tight coupling with a framework. What is more, when it comes to replication of your app, you have much more control about which part and how much you want to replicate it. You can distribute your API within some kind of CDN or cloud, keeping your data processing app centralized. Small application is not as heavy as your previous monolithic application - you can spawn more workers with the same resources you had before. As a developer, you would certainly appreciate that with this approach you‚Äôre absolutely technology agnostic. With this approach you can create your API app in Rails with a traditional RDBMS like PostgreSQL and payments processing application in Haskell with MongoDB as a persistence layer. It‚Äôs your choice and service oriented architecture allows you to have such flexibility. When you change something in one part of your system, you don‚Äôt have to reload all subcomponents - deploys are separated from each other and you have zero downtime with API when you only update a data processing app. That makes your system more reliable and clients happier. When something goes wrong and one subsystem hangs, you can defer message passing to it and go on - before you had single point of failure, now your system is much more durable. You can define or choose protocols you choose, not just HTTP - you can tailor your message passing to suit your needs. You can provide reliable or unreliable communication, use different data formats - it‚Äôs your choice. When it comes to optimalisation, it‚Äôs a huge improvement compared to monolithic Rails app, which is adjusted to work with a HTTP protocol and simple, stateless request-response cycle. In chillout, our application which gathers metrics about creation of a certain models within your Rails app, we use ZMQ sockets for internal communication thorough our system and only use HTTP to get requests from our clients. That allowed us to be flexible about reliability of our transmission. We certainly want to be sure when someone pays us, but we don‚Äôt need to be exactly sure that 1 of 100 gathered metric won‚Äôt be delivered. When it comes to coupling, there is less possibilities to couple independent components of your system - in fact, you can‚Äôt be more explicit about saying ‚ÄúHey, now we‚Äôre dealing with a completely different domain‚Äù! Of course, there is no perfect solution - and SOA have its flaws. Unfortunately, with this approach you have to provide code for internal communication between subsystems. Often it would imply that your total codebase (sum of codebase of all subcomponents of your system) will be bigger. To overcome this issue I recommend to put this communication code as an external library, shared between your components. Another issue is that every application which wants to communicate with a certain subsystem needs to know a message format - thus, knows something about a domain of the receiving app. This can be fixed, but with a cost - you can provide a ‚Äúmediator‚Äù app, which is reponsible for translating a message from a sender domain to (possibly multiple) recievers domain format. It‚Äôs nothing new, though - you made it before when you introduced an adapter to your application. This issue induced a nice discussion inside Arkency team, and it‚Äôs the only solution we‚Äôve found so far. It‚Äôs good, but not as good - we have to provide more code to do so. I would recommend creating a simple adapter first - when you feel it‚Äôs not enough, you can easily extract it as a new application. If you‚Äôre ready to pay this price, SOA is a viable architecture to work with your system. And the best part is that‚Ä¶ Very good thing about SOA is that it‚Äôs not all-or-nothing - we can iteratively\ntransform the code from a certain point to the stand-alone mini-application. \nIn fact, when we transform our business logic into services it‚Äôs quite simple.\nLet‚Äôs define steps of this extraction, and I‚Äôll provide a simple example. Let‚Äôs introduce our example. We have a simple service object which processes\npayment creation requests - it communicates with an external service to delegate\nthe payment processing and handles response from this service. It‚Äôs important to see it‚Äôs a boundary context of our application - it‚Äôs not tightly related with what our application does and it‚Äôs business rules - it‚Äôs only providing an additional (but needed) feature. These kind services are the most viable choice for extraction, because we can easily build an abstraction around it (more about this later). The callback object here is usually a controller in a Rails application.\nIf you‚Äôre unfamiliar with this kind of handling outcoming messages from service within Rails controller, here‚Äôs an example how the code may look inside the controller: Here‚Äôs how our code might look like: We put this file (with it‚Äôs dependencies, but without a callback object - it‚Äôs not a dependency!) to the separate directory. Here we create the code which processes our requests. It‚Äôs a lie it‚Äôs only for a request\nprocessing - in our example it‚Äôs also setting up a HTTP server - but it‚Äôs all about a protocol. We use HTTP, so we need a HTTP server for this. We can use many\ntechnologies - like raw sockets, ZMQ and such. You can really skip this code if you don‚Äôt want to learn about webmachine internals. \nIn short it creates HTTP server which processes /payments POST calls and binds \nmethods read and render appropiate JSON to it. Since webmachine is really small\ncompared to Rails, we have to create resources (we can think about it as \ncontrollers) by ourselves. We did it already. The #render_resource , #payment_data_invalid , #payment_unknown_error and #payment_successful methods is the response creation code. All it does is providing an interface for a service and creating a JSON response \nbased on callback service calls. When it gets bigger, I recommend putting \nresponse creation code into a separate object. Now we have to change our old controllers code. It can now look like this, using Faraday library: This ends our tricky parts of extraction. Step five is dependent on your application and it‚Äôs fairly easy. \nNow we have the tiny bit of our complex application as a separate application instead. We can run our new application and check out if everything works fine. Remember that I mentioned this is a great candidate for extraction due it‚Äôs a \nboundary context? You always have this kind of context in your application - for\nexample, controllers are managing boundary context of Rails applications, like logging\nor rendering responses. Coming back to our cosmic metaphor - after our step we have a brand-new created galaxy which contains exactly one star. It‚Äôs not quite efficient to leave a one-star galaxy - we just added some code and separated one particular action away from our old, \nbig galaxy. But added code/profit ratio is poor for now. Your next step should \nbe finding stars which share similar dependencies and nature of actions, like PaymentNotificationService and transfer this kind of stars to your new galaxy. We have used an service-oriented approach in our product called Chillout . It‚Äôs a relatively simple app which is gathering metrics about model creations within clients‚Äô Rails applications. We are sending a mail report each day/week/month, containing changes and charts which shows trends of model creations. During development, 6 applications were created: These application have narrow scopes and totally separate domains. With this kind of logical separation, we can distribute our system whatever we want. In future we can, for example, provide more brains without a hassle. I would greatly recommend a video by Fred George about micro-services . It‚Äôs a great example how SOA can improve thinking and development of your systems. Also, Clean Architecture is a great start to organize monolithic app to create a SOA from it in the future. You can read more about it here . Service oriented architecture can be a great alternative for maintaining a big,\ncomplex singular application. When we were developing chillout we had lots of fun\nwith this kind of architecture - features adding were simple, tests were quick\nand thinking about an application was simpler. What do you think about this kind of architecture? Have you tried it before?\nI really looking forward for your opinions. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2013-12-26"},
{"website": "Arkency", "title": "Stop including Enumerable, return Enumerator instead", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/01/ruby-to-enum-for-enumerator/", "abstract": "Many times I have seen people including Enumerable module into their\nclasses. But I cannot stop thinking that in many cases having methods\nsuch as each_with_index or take_while or minmax and many others\nthat are available in Enumerable are not\ncore responsibility of the class that is including them itself. In such case I prefer to go Java-way and provide external Enumerator for\nthose who need to call one of the many useful Enumerable methods on the\ncollection. I think that we need to ask ourselves a question: Is that class a\ncollection? . If it really is then it absolutely makes sense to include Enumerable . If however it is not a collection, but rather a class\nwhich happens contain something else, or providing a collection,\nwell then maybe external Enumerator is your solution. If you call the most famous Array#each method without a block, you will see that\nyou get an enumerator in the response. You can manually fetch new elemens: Or use one of the Enumerable method that Enumerator gladly provides for you There are 3 ways to create your own Enumerator : But if you look into MRI implemention you will notice that both #to_enum and #enum_for are implemented in the same way: You can check it out here: And if you look into rubyspec you will also notice that they are supposed to\nhave identicial behavior, so I guess currently there is really no difference\nbetween them Therfore whenever you see an example using one of them, you can just substitue\nit with the other. What can #to_enum & #enum_for do for you? Well, they can create the Enumerator based on any method which yield s arguments. Usually\nthe convention is to create the Enumerator based on method #each (no surprise here). We will see it in action later in the post. This way (contrary to the previous) has a nice documentation in Ruby doc which I am just gonna paste here: Iteration is defined by the given block, in which a ‚Äúyielder‚Äù object, given as block parameter, can be used to yield a value : The optional parameter can be used to specify how to calculate the size in a lazy fashion. It can either be a value or a callable object. Here is my example: Of course returning Enumerator makes most sense when returning collection (such as Array )\nwould be inconvinient or impossible due to performance reasons, like IO#each_byte or IO#each_char . Not much actually. Whenever your method yield s values, just use #to_enum (or #enum_for as you already know there are identical) to create Enumerator based on the method itself, if block code is not provided.\nSounds complicated? It is not. Have a look at the example. We are working in super startup having milions of users. And thousands of them can\nhave gravatar. We would prefer not to return them all in an array right? No problem.\nThanks to our magic oneliner return enum_for(:each) unless block_given? we can\nshare the collection without computing all the data. This might be really usefull, especially when the caller does not need to have it all: Or when the caller wants to be a bit #lazy : Did i just say lazy ? I think I should stop here now, because that is a completely\ndifferent story . To be consistent with Ruby Standard Library behavior, please return Enumerator for your yield ing methods when block is not provided. Use this code to just do that. Your class does not always need to be Enumerable . It is ok if it just\nreturns Enumerator . If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your everyday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-01-08"},
{"website": "Arkency", "title": "Rails Refactoring: the aha! moments", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2014/02/rails-refactoring-the-aha-moments/", "abstract": "Have you ever been stuck with some code? Looking at it for minutes, hours, feeling the code smells, but not being able to fix it? I‚Äôve been pair programming with many people in my career. Some of them were very skilled with refactoring. They did it very quickly, confident with their skills, confident with their tools (editors, IDEs), confident with their safety net (tests). Sometimes I watched a block of code being put in 4 different classes within 10 minutes, while being green with the tests all the time. It wasn‚Äôt just for fun. There was a deep focus involved. The aesthetics mattered. During this time we were heavily discussing how the code fits in this place. Thanks to the quick refactoring skills, we were able to experiment with many different ideas. We were not experts with this particular module, that was also a learning procedure for us. Very often, in such situations, we had those ‚Äòaha‚Äô moments. After some code transformations, after some lessons, we were able to find the best design for the current needs. What started as an unknown blob of code, ended as a nice structure with clear responsibilities. Would we achieve that without the quick refactoring skills? I don‚Äôt know. There are many programmers and each has its own approach to the design. It definitely worked for us, though. How often were you involved in heated coding discussions? Was is it easy to discuss the ideas without looking at the code for each of them? What if your skills allowed you to transform the code as quickly as the ideas appear? Would that make the discussion easier? Sometimes the ‚Äòaha‚Äô moments are very small, but they help you with a specific module. Suddenly, you know the way it should be implemented. I was just working with this code. It‚Äôs not mine (it‚Äôs Redmine). As part of the research for my ‚ÄòRails Refactoring‚Äô book , I was transforming this code into many possible structures. When I started, I didn‚Äôt know much about it. Before I played with it, I started some manual mutation testing to see if the tests cover most of the cases (they did). I learnt a lot, by moving some pieces of this code into different forms. I knew this code has code smells. It does a lot of things and some better structure is possible. I knew it‚Äôs responsible for creating time entries, but not much more than that. First, I applied some simple transformations to look at the problem from different perspectives. After every change the tests were run. Inline controller filters Explicitly render views with locals Extract Service Object with the help of SimpleDelegator Extract the ‚Äòif‚Äô conditional This turned the code into this: As you see the 40-lines block turned into 80 lines, temporarily. It‚Äôs uglier. I basically inlined all the dependencies. It‚Äôs more explicit now. It‚Äôs a look at the code from a different perspective. A perspective, without separation of concerns. What was previously hidden in different places is now in front of me in one piece. The ‚Äòaha‚Äô moment is coming. It‚Äôs only now, that I realised that the controller action was in fact responsible for two different user actions: CreateProjectTimeEntry CreateIssueTimeEntry The difference may not be huge, but this explains the number of if‚Äôs in this code. What may seem to be a clever code reuse (‚ÄúI‚Äôll just add this if here and there and we can now create time entries for a project as well‚Äù, may also be a problem for people to understand in the future. Where do I go with this lesson now? After some more typical transformations, I ended with: extract render/redirect method extract exception objects from a service object change CRUD name to a domain one (CreateTimeEntry -> LogTime) return entity from service object And the service object That‚Äôs an important question to ask. I wasted some time, right? Since I‚Äôm practicing those refactoring techniques recently my skills are quite good, I didn‚Äôt waste much time here. It would be much more, if I didn‚Äôt have good skills, good tool support (thank you, RubyMine) and good test coverage (thanks to the Redmine team!). What if this lesson took me 1 day of work? Sounds like a waste of time and money. Ruby/Rails is a difficult environment to be perfect in refactoring. It requires practicing, failures, lessons, trials, patience. However, once you become more confident with your refactoring skills, you‚Äôll save a lot of time in the future. You will not only deliver more features, but also the code quality will be much better. I think it‚Äôs worth it. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-02-10"},
{"website": "Arkency", "title": "Pretty, short urls for every route in your Rails app", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/01/short-urls-for-every-route-in-your-rails-app/", "abstract": "Photo remix available thanks to the courtesy of Moyan Brenn . CC BY 2.0 One of our recent project had the requirement so that admins are able to generate\nshort top level urls (like /cool ) for every page in our system. Basically a\nurl shortening service inside our app. This might be especially usefull in your app\nif those urls are meant to appear in printed materials (like /productName or /awesomePromotion ). Let‚Äôs see what choices we have in our Rails routing. If your requirements are less strict, you might be in a better position to use a\nsimpler solution. Let‚Äôs say that your current routing rules are like: We assume that :id might be either resource id or its slug and you\nhandle that in your controller (using friendly_id gem or whatever other\nsolution you use). And you would like to add route like: that would either route to AuthorsController or PostController depending on\nwhat the slug points to. Our client wants Pretty Urls: Well, you can solve this problem with constraints. This will work fine but there are few downsides to such solution and you\nneed to remember about couple of things. First, you must make sure that slugs are unique across all your resources\nthat you use this for. In our project this is the responsibility of services which first try to reserve\nthe slug across the whole application,\nand assign it to the resource if it succeeded. But you can also implement\nit with a hook in your ActiveRecord class. It‚Äôs up to you whether you choose\nmore coupled or decoupled solution. The second problem is that adding more resources leads to more DB queries.\nIn your example the second resource (posts) triggers a query for authors first\n(because the first constraint is checked first) and only if it does not match,\nwe try to find the post. N-th resource will trigger N db queries before we\nmatch it. That is obviously not good. One of the thing that you are going to decide is whether visiting such short url\nshould lead to rendering the page or redirection. What we saw in previous chapter\ngives us rendering. So the browser is going to display the visited url such as /MartinFowler . In such case there might be multiple URLs pointing to the same\nresource in your application and for best SEO you probably should standarize\nwhich url is the canonical : /authors/MartinFowler or /MartinFowler/ ? Eventually you might also consider\ndropping the longer URL entirely in your app to have a consistent routing. You won‚Äôt have such dillemmas if you go with redirecting so that /MartinFowler simply redirects to /authors/MartinFowler . It is not hard with Rails routing.\nJust change into But we started with the requirement that every page can have its short\nversion if admins generate it. In such case we store the slug and the\npath that it was generated based on in Short::Url class. It has the slug and target attributes. Now our routing can use that information. You can simplify this code greatly (and throw away most of it)\nif you go with either render or redirect and don‚Äôt mix those two\napproaches. I just wanted to show that you can use any of them. Let‚Äôs focus on the Render strategy for this moment. What happens here.\nAssuming some visited /fowler in the browser, we found the right Short::Url in the dispatcher, now in our Render#call we need to do some work that\nusually Rails does for us. First we need to recognize what the long,\ntarget url ( /authors/MartinFowler ) points to. Based on that knowledge we can obtain the controller class. And we know what controller action should be processed. No we can trick rails into thinking that the actual parameters coming from recognized url were different If we generated the slug url based on nested resources path, we would have here two hash keys\nwith ids, instead of just one. And at the and we create new instance of rack compatible application based on the #show() method of our controller . And we put everything in motion with #call() and pass it env (the Hash with Rack environment ). That‚Äôs it. You delegated the job back to the rails controller that you\nalready have had implemented. Great job! Now our admins can generate\nthose short urls like crazy for the printed materials. Interestingly, after prooving that this is possible, I am not sure whether\nwe should be actually doing it üòâ . What‚Äôs your opinion? Would you rather\nrender or redirect? Should we be solving this on application level (render)\nor HTTP level (redirect) ? Subscribe to our newsletter below so that you are always\nthe first one to get the knowledge that you might find useful in your\neveryday programmer job. Content is mostly focused on (but not limited to)\nRails, Webdevelopment and Agile. 2200 readers are already enjoying great content\nand we are regularly included in Ruby Weekly issues. You can also follow us on Twitter Facebook , or Google Plus Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-01-18"},
{"website": "Arkency", "title": "2 ways to deal with big and complicated features", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/02/from-feature-vision-to-stories/", "abstract": "Photo remix available thanks to the courtesy of EladeManu . CC BY 2.0 One of the challenges of managing software projects is delivering big features.\nWe already told you that we prefer to work with small , prioritized and unassigned stories that anyone can take and finish . But how\ndo any up there? Especially, if what the customer comes to you with, is a big document,\ndescribing the features that you must implement. You can create all the tickets upfront based on the document\nyou received or vision of the customer. This is sometimes possible. When the change or feature \nrequest by the customer is in the scope of week\nwork and unlikely to change, it might be worthy\nto spend a little time, think about how to reach the\ngoal with small steps, and extract all the tickets . The\nbenefit is that from now on, other people can just\ntake the first task and start working. Perhaps some of\nthe tasks are independent so you can prioritize them\nwith your customer and have a very clear path to\nyour goal. When all the tasks are visible, it is easy\nfor everyone to see the progress. But this is often impossible and impractical for a really\nbig tasks that are going to take longer than a week. But before\nwe dive into another strategy, let‚Äôs talk for a moment what\nwe want to actually achieve. We don‚Äôt want to assign one programmer for 3 weeks to work on a\nseparate branch in a complete isolation from the\nrest of the team . Instead we believe that having multiple people\nworking on the feature is beneficial because they provide fresh\nview and feedback about the code and feature. It also improves Collective Ownership which we care deeply about. In the spirit of Agile we don‚Äôt want to deploy this feature after\nlong time of working on it. We want to build it iteratively and\ndeploy often . We will seek the feedback from the customer and from\nthe users of our software. We want to continuously deliver value.\nAnd if the customer decides to change the priorities and focus after\n10 days of working on the feature to something new, that can possibly\nbring greater value, then it shall remain her/his right. In such case\nwe would like everything that has been done and deployed so far\nto be usable and beneficial to the users. When the programmers implement small parts of the feature we want\nthem to have a good overview of it . Although when they work on a\nsmall ticket, their responsibility is to implement the small part\nenough to mark the story as done, it is also their responsibility\nto make the solution friendly to next programmers implementing\nfurther stories related to the feature. In other words, to implement\nsomething small, but have in mind the big picture. And that brings us to the second strategy. We call it Documentation as\nfloating ticket . You keep the big feature (specification) as a ticket in your backlog. But\nwhenever you reach it as part of take the first task rule, instead of\nstarting to work on it as a developer, you put your project manager hat on. You look into the specification and compare it to the current\nstate of project. The specification is also a document that\ncan be changed by everyone so you can see what parts\nof it have already been extracted into tickets in the past\nand what still needs to be done. Based on the priorities\nmentioned in the ticket or based on your conversations\nwith the client it is now time for you to extract tickets.\nHow many of them? That‚Äôs up to many factors. Maybe\nyou can see clear path that can lead to having nice feature\nfrom the documentation so want to extract few small tasks.\nMaybe you know that the client want to keep working\nslowly on these features so it is ok to only extract one story\nfrom it. Whatever the reasons are, make sure the strategy\nis discussed with the customer and your team and the rules\nare clear for everyone. So you decided to extract two task. You create them on top\nof your backlog so that you or your team mates can start\nworking on them. Now based on similar factors described\nin previous paragraph you need to decide how to reschedule\nextracting next tasks from the spec. If the spec is very\nimportant and everyone should be working on it, you can\nleave it in place so that whenever currently extracted tasks\nare finished, you will reach the document again and mine\nnew tasks from it, until the spec is fully implemented. If\nhowever you want to work on multiple parts of the system\nat the same time and customer expects progress also in\ndifferent parts of the app, you should move the spec down\nin the backlog. The least important spec, the lower you\ncan move it down. You can have strict rule how much it\nshould be moved or you can do it based on your judgment\nand knowledge about project priorities. If everything from\nspec is done, mark it as done as you do with the rest of the\ntickets. If you get the knowledge in the meantime that there are\nmore important issues and your team should stop working\non features listed in the spec, then it might be a good idea\nto put it out of backlog, until the customer decides to bring\nit back in the game. You end up with extracted tickets, spec knowledge, updated\ndocument about which parts are already moved into tickets\nand which parts are still only covered in the spec. You can\ngo back to using Take the first task strategy until you are\nout of tickets, at which point you need to find your project\nmanager hat and wear it for at least a moment again. I hope reading this article was beneficial for you. If you want to\nfind out more about techniques that will please sign up now to the newsletter below. We will email you from time\nto time about other techniques that work for us. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-02-28"},
{"website": "Arkency", "title": "Zero uptime deploy", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/04/unicorn-successful-deploy-verification-procedure/", "abstract": "Photo remix available thanks to the courtesy of Rob Boudon . CC BY 2.0 Unicorn has a nice feature that bought it a lot of popularity and set standards\nfor other Ruby web servers: The ability to do Zero Downtime Deploy , also known\nby the name rolling deploy or rolling restart aka hot restart . You start it by issuing USR2 signal. But here is something that most websites won‚Äôt tell you. It can fail\nand you won‚Äôt even notice. You will be thinking that everything went ok, living in Wonderland,\nwhereas in reality your deploy achieved uptime of exactly 0 seconds. So what you need is a small verification procedure that everything worked as\nexpected. This article will demonstrate simple solution for achieving it\nin case you are using capistrano for deploying the app. However you can use very similar\nprocedure if you deploy your app with other tools. Here is what we assume that you already have Nothing fancy here. As the documentation states: USR2 signal for master process - reexecute the running binary. A separate QUIT should be sent to the original process once the child is verified to be up and running. Whenever we spawn new child process we decrement the number of worker\nprocesses by one with sending TTOU signal to master process. At the end we send QUIT so the new master worker can take it place. Let‚Äôs add the verification step after deployment. We want to trigger our verification procedure for deploy no matter whether we\nexecuted it with or without migrations. Also we don‚Äôt want to implement the entire verification procedure algorithm in\nthis file. So we extract it into './config/deploy/verify' and require\ninside the task. The whole idea is that we do the request to our just deployed/restarted webapp\nand check whether it returns randomly generated token that we set before\nrestart. If it does, everything went smoothly and new workers started, they\nread the new token and are serving it. If however the new Unicorn workers could not properly start after deploy,\nthe old workers will be still working and serving requests, including the\nrequest to /about/deploy that will give us the old token generated during\nprevious deploy. It takes some time to start new Rails app, create new workers, kill old workers\nand for the master unicorn worker to switch to the new process. So we wait max 60s\nfor the entire procedure to finish. In this time we are hitting our application\nwith request every now and then to check whether new workers are serving requests\nor the old ones. Here is the controller doing basic auth and serving the token. It does\nnot try to dynamically read the TOKEN file because that would\nalways return the new value written to that file during last deploy. Instead it returns the token that is instantiated only once during Rails\nstartup process. Here you can see that we are storing the token when rails is starting. Now that you know how, you are still probably wondering why. Not everything can be caught by your tests, especially not errors made in\nproduction environment configuration. That can be even something as simple as\ntypo in config/environments/production.rb . We also experienced gems behaving differently and preventing app from being\nstarted due to tiny difference in environment variables ( ENV ). So now,\nwhenever we manage application that is not hosted in cloud because of customer\npreferences, we just add this little script to make sure that the deployed code\nwas actually deployed and workers restarted properly. Because sending signal\nis sometimes just not good enough :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-04-01"},
{"website": "Arkency", "title": "Sitemaps with a bit of Metal", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/2014/02/sitemaps-with-a-bit-of-metal/", "abstract": "Photo remix available thanks to the courtesy of tinkerbrad . CC BY 2.0 Sooner or later, you will probably start taking care about your application‚Äôs SEO,\nespecially if it provides a lot of content and you want to be discovered by users\nin search engines results. There are several ways to do this in your Ruby app.\nYou can create sitemap.xml file manually if there aren‚Äôt very much urls,\nbut it will become pretty ineffective when you have more than a dozen or so.\nThere are some very neat tools which will do this for you out of the box,\neven if you need a lot of customization. Tool which I would like to mention is Sitemap Generator by kjvarga . It‚Äôs pretty cool,\nit keeps the standards so you don‚Äôt have to care too much.\nIt also have custom rake tasks, which will generate Sitemap under given criteria\nand ping selected search engines about availability of new one one for your site. Magic. Installation is very easy. You only need to add one line to your Gemfile : Then you should run bundle and rake sitemap:install .\nNow you should have config/sitemap.rb in your directory structure,\nwhich you need to tweak for your needs. And that‚Äôs it! All you need to do is to run rake sitemap:refresh .\nNow you have new sitemap.xml.gz file in your /public directory. If your app handle multiple domains, there‚Äôs no problem,\nbecause you can render multiple sitemap files for different domains,\nsubdomains or specific locales. You might also heard that a single Sitemap must have no more than 50,000 URLs and can‚Äôt be larger\nthan 10MB. And it‚Äôs true. There was a risk that our app will hit that limit in close future.\nFortunatelly Sitemap protocol provides\na possibility to handle such situation through index files. As I mentioned earlier, sitemap_generator keeps the standards pretty good, so it creates index file if such one is needed by default.\nYou can also force it to always create index file. Let‚Äôs use some real life example. Mentioned application is presenting a huge amount of events and allows users to buy tickets to them.\nWe will fetch all events from database through find_each method to get objects in batches.\nWe do this in case that large amount of objets could not fit into memory.\nOn each event we would use event_path helper to add proper URL to our sitemap. After running rake sitemaps:refresh we now have at least two files: sitemap.xml.gz and sitemap1.xml.gz .\nAt least because for each n multiple of 50,000, sitemap{n}.xml.gz would get created. Let‚Äôs take a close look at sitemap.xml.gz content: It no longer contains a Sitemap , but index which specifies where the Sitemaps are. Content of sitemap1.xml.gz : Content of sitemap2.xml.gz : Pretty easy, isn‚Äôt it? If you use CDN for static files and don‚Äôt want to keep Sitemap in your /public directory,\nyou can use specific adapter and just customize this in config file: or even I really appreciate how the author of the gem solved different storage mechanisms.\nIf you need more customization in this area, you can just write your compatible adapter and save Sitemap whenever you want: database, key-value storage or whatever.\nIf we want to use ActiveRecord for this purpose, we can just write: One more thing to do to keep this running is to create such db migration: We must also update our config/sitemap.rb file and tell that we want to use custom\nadapter: Ok, now we have up and running creation of Sitemap. But how to render it if it‚Äôs no longer\navailable in /public directory?\nWe need to find away to get the file from db and render to user, in this case search engine crawler. To render our file we need to create proper controller.\nIn typical Rails application we would probably do something like this: Our controller responds to xml_gz format which is not supported in Rails by default.\nWe need to register this format, so our controller could render proper response when *.xml.gz format is requested by client. We can to do this by putting line below\nin config/initializers/mime_types.rb file. One more necessary thing is adding these few lines to config/routes.rb : We use constraints on format because we need to handle non standard, double resource extension xml.gz .\nWithout this, our Rails app would lookup for resource with .gz extension and sitemap.xml would be treated as filename. Let‚Äôs take a look what exactly our controller has inside: But do we really need to carry whole this stuff which is usually inherited by ApplicationController?\nHow about no. We don‚Äôt really need skipping before filters, we don‚Äôt need url helpers,\nturbolinks, devise and any other useless in this case stuff. So, let‚Äôs slim this down a bit. Let‚Äôs take a look what we have achieved: Our controller is much lighter and contains mostly necessary things to serve our sitemap.xml.gz file to Google,\nBing, Yandex or whoever wants our Sitemap . Jos√© Valim inspired me\nto use ActionController::Metal in his Crafting Rails Applications book. Picking only those modules which are indispensable for our\ncontrollers is a pretty cool approach, but in my humble opinion, not often seen in Rails applications.\nMounting Sinatra application with requested functionality in routes.rb or config.ru could be alternative,\nbut still lightweight solution. As you can see, such chore like rendering Sitemap can be done in a smart way,\nin most of the cases with just few lines of code. It‚Äôs very useful, especially for the applications\nwith a lot of content. Easy customization is another advantage of presented solution. And now go and make\nyour sales and marketing teams happy providing better search engine results. I will try to write another blogpost focused on usage of ActionController::Metal in different, but maybe more\nsurprising use case, on condition that this topic is interesting for you and Robert won‚Äôt forestall me. :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-02-13"},
{"website": "Arkency", "title": "Don't call controller from background job, please. Do it differently!", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/03/pdf-in-rails-without-controllers/", "abstract": "Photo remix available thanks to the courtesy of francescomucio . CC BY 2.0 Some time ago while doing Code Review for one of our clients I stumbled\non a very unexpected piece of code. Background job calling a controller. I was a bit speechless. Calling controller out of HTTP request/response\nloop sounds like an odd thing to do. I had to investigate and refactor. As you can probably imagine, it turned out that the PDF was generated based\non HTML, and the HTML was generated by the controller. After all, controllers\nin Rails can render views, that‚Äôs one of their responsibilities. But they\nalso take care of HTTP request parsing, managing cookies, session,\nauthentication, authorization, content negotiation building respone, \nand all the stuff necessary for your webapp to work. But when you call a controller from background job, in a situation like this,\nnone of this things happens. What you actually want is not a Controller but\njust a Renderer . If we can achieve that, we won‚Äôt need to have this ugly skip_before_filter in the code, and our intentions are going to be way more\nclear for the rest of the team reading the code. After a few moments of struggling with rails, reading the doc, and trying\nthings in irb , it turned out that all we need is this. The view for the rendered pdf is in app/views/order_pdf_renderer/order.html.erb file. The layout in app/views/layouts/pdf.html.erb . Nothing surprising here. And\nif you prefer presenters/decorators over helpers you can just remove include ActionController::Helpers and helper :orders and have even less code to\nmaintain. During the refactoring this part of code I also extracted a separate layer\nresponsible for getting all the data required to generate the PDF. As a result the\nrenderer is called with order and not just order_id . That makes testing everything\neasier. We could probably try to go even further and use less and less of what we don‚Äôt need from Rails in such situation. Perhaps there is \na clean way of using ActionView part for such purpose without the coupling to\ncontrollers and HTTP-context at all? I hope this technique can be a useful tool in your refactoring\ncontrollers toolbox . Whenever you stumble upon a\ncontroller methods that are not used to deal with HTTP requests-response loop,\nthink about extracting them into a separate object and giving a proper name. Even if\nthey relay on Rails controllers features, taking from Rails what you just need,\nmight be easier than you think. The list of modules to include is a little different: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-03-20"},
{"website": "Arkency", "title": "The biggest obstacle to start with Continuous Deployment - database migrations", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/04/the-biggest-obstacle-to-start-with-continuous-deployment/", "abstract": "Photo remix available thanks to the courtesy of Mark Engelbrecht . CC BY 2.0 There are still companies which disable the website during deploys with database\nmigrations and believe that db migrations prevent them from going fully with CD.\nThey even have time window for triggering such activities (usually early in the\nmorning or late at night). If you are working in a place like that, and you would deeply want in your heart to\nadopt Continuous Deployment but you can‚Äôt find a way to convince your boss, this\nis a step by step guide for you. Instead of going from zero to warp nine , you can go towards your goal\nwith small steps that can be part of your current, normal tasks that you are doing for\nthe client. When you go with CD and do many deploys a day, most of them are not running db\nmigrations at all. You can save your developers time at least on these kinds of\nactivities. Your CD solution can easily detect (at least in case of Rails app)\nthat deploy would trigger new migration and notify your developer that in such\ncase the deploy should be done manually. Still better than nothing.\nStill saving lots of people time. Even if your deploy is containing migrations, in many cases they are\nnon-destructive. They are rather creating new tables, new columns and new indexes.\nIn such case you can for example follow a naming convention so that CI can easily know\nthat the migrations are indeed non-destructive and might be executed at any time because\nit will not cause troubles to already running application. Still, you need to be a bit\ncautious. If you add new column with NOT NULL constraint and without a default, it will\nmost likely cause troubles to you app if executed when new request are coming. Why? Because\nthe previous version of the app (the on running when migrations are executed) won‚Äôt be\nfilling the column with proper data during new records creation. So whenever marking migration\nas non-destructive, you should ask yourself a question Will executing this migration when\nsystem is online and serving requests cause any trouble when record from table X is added,\nupdated or deleted? If not, it means that your migration is non-interrupting for the system\nand can be deployed automatically at any time. In the worst case, when the deploy is containing destructive migrations you can fallback to the\nold way of disabling the website during the deploy. But still your deployment script can be\ntriggered automatically and it can verify whether the migration procedure was triggered in the\nallowed time window for turning off the application. So if a developer commits or merges\ndestructive migration to master branch during the time window, the app would be still deployed\nautomatically, saving your developers time. In the last step you teach your team how to write code adapted for zero-downtime deploys triggered by\nContinuous Deployment. So let‚Äôs say you want to add new not null column. And you achieve it step by\nstep. Firstly you add new column with nulls allowed. Then you deploy code which is actually starting\nto fill the column with some data from the app. Then you deploy code/migration which computes the data\nfor all old records containing nulls. Then you deploy code which marks that column as not null. All\nstep by step, without trying to achieve it in one giant commit or giant deploy. Notice how very well\nthis approach can work with small stories . There is an amazing presentation about how to do it: As you can see you can achieve Continuous Deployment step by step. It does not have to be all or nothing. It can be: This post is a small extract from a chapter of our Async Remote ebook. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-04-13"},
{"website": "Arkency", "title": "Mastering Rails Validations: Contexts", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/04/mastering-rails-validations-contexts/", "abstract": "Many times Rails programmers ask How can I skip one (or more) validations in Rails.\nThe common usecase for it is that users with higher permissions are granted less strict\nvalidation rules. After all, what‚Äôs the point of being admin if admin cannot do more\nthan normal user, right? With great power comes great responsibility and all of that\nyada yada yada . But back to the topic. Let‚Äôs start with something simple and\nrefactor it a little bit to show Rails feature that I rerly see in use in the\nreal world. This is our starting point Our users can change the slug ( /u/slug ) under which their profiles will appear. However the most valuable\nshort slugs are not available for them. Our business model dictates that we are going to\nsell them to earn a lot of money [disclaimer: polish joke, I could not resist] So, we need to add conditional validation that will be different for admins and\ndifferent for users. Nothing simpler, right? Now this would work, however it is not code I would be proud about. But wait, you already know a way to mark validations to trigger only sometimes.\nDo you remember it? We‚Äôve got on: :create option which makes a validation run only when saving new\nrecord ( #new_record? ). I wonder whether we could use it‚Ä¶ Wow, now look at that. Isn‚Äôt it cute? And if you want to only check validation without saving the object you can use: This feature is actually even documented ActiveModel::Validations#valid?(context=nil) Now it is a good moment to remind ourselves of a nice API that can make it less\nredundant in case of multiple rules: Object#with_options The problem with this approach is that you cannot supply multiple contexts. If you would like to have some validations on: :admin and some on: :create then it is probably not gonna work the way you would want. When you run user.valid?(:admin) or user.save(context: admin) for new record,\nit‚Äôs not gonna trigger the last validation because we substituted the default :create context with our own :admin context. You can see it for yourself in rails code : The trick with on: :create and on: :update works because Rails by default\ndoes the job of providing the most suitable context. But that does not mean\nyou are only limited in your code to those two cases which work out of box. We could go with manual check for both contexts in our controllers but we would\nhave to take database transaction into consideration, if our validations are\ndoing SQL queries. I doubt that the end result is 100% awesome. I once used this technique to introduce new context on: :destroy which\nwas doing something similar to: The idea was, that it should not be possible to delete user who already took\npart of some important business activity. Nowdays we have has_many(dependent: :restrict_with_exception ) but you might still find this technique beneficial in other cases where you would\nlike to run some validations before destroying an object. That was quick introduction to custom validation contexts in Rails. In the\nnext episode we are going to talk about other, perhaps better, ways to solve our initial\ndilemma that started with validations being context dependent.\nSubscribe to our newsletter below if you don‚Äôt want to miss it. You might also want to read some of our other popular blogposts ActiveRecord-related: The next part is out: If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-04-30"},
{"website": "Arkency", "title": "Would you love to work remotely (and asynchronously)?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/06/would-you-love-to-work-remotely/", "abstract": "We are Arkency and we have been working remotely since the very beginning of our company in 2007. Remoteness is a core value for us. If you opened the email, I hope you are at least curios as to why and how to work remotely. I think it all starts with dream and vision. For me, the biggest reason for a switch, was commuting. I couldn‚Äôt help but feel that it was a huge time loss. Yes, you can read a book when commuting, but try it out in winter, when it is freezing and you are waiting for a bus. Yes, you can listen to the music or podcasts, but good luck enjoying it when the metro is so loud. The only solutions seemed to be getting a home closer to the city-center, which in my book mean more expensive. Or getting a car, good bye my environment-friendly lifestyle. At least riding a bicycle to the office can have some potential health benefits, if that‚Äôs an option for you (I envy you, Amsterdam) . And when I was finally there, at the office, it was filled with people talking over phones or discussing their issues and I couldn‚Äôt event focus enough to do my job properly. Even though I have a medium hearing deficiency I still consider noise to be very problematic. I can‚Äôt even imagine how it is like for people who don‚Äôt have such kind of condition. For them the reality is even louder than for me. Distracted programmer is not an effective programmer, I am sure you can agree. I know there are offices other there which are quiet but after visiting some offices I find it to be an exception rather than a norm. So I thought to myself what if I could work somewhere where I wouldn‚Äôt spend 90 minutes every day commuting to and from? What if I could work in a distraction-free environment? What if I could work‚Ä¶ from home At first it looked impossible but first I negotiated with my boss working 2 or 3 days from home, just to give it a try. And I enjoyed it very much. Later I moved to Arkency where everyone was working remotely for full time and my dream became my everyday reality. Here is a winter picture from my current in-home office window. Don‚Äôt know about you, but sometimes in the middle of the day I feel a little sleepy (thank you insulin resistance). In such case either I take a short power nap or I go for a 40 minute-long refreshing walk in a lovely forest in my neighborhood. Both options possible thanks to the fact that I can work from home. I always thought that offices should have a nap room, but somehow most office designers don‚Äôt agree. If you can take a nap in your office, share your story with me. I would love to hear more about it. Of course you might be into remote work for your own reason like travelling. When home is no longer just quiet but rather boring, we often wish to be able change our location to something new and feel fresh again. Not necessarily take holidays just for that. But to change the environment, work, and enjoy the city or countryside after working. Is that an option for you? And then there is the most important reason for some of us. Family. If you want to spend more time with your growing kids and lovely partner, home working might be a solution for you. But no matter what reasons for going remote you have, you also need to know how. So in the next posts we are going to tell you how we did it. What tools we use, and how we handle communication. What‚Äôs our process like and what we value. How we work remotely and asynchronously and what interesting techniques you might wanna try yourself. This was just a teaser. In next 2 weeks you will get 4 articles covering a lot of content. See you in two days. Next part: Async & remote - toolbox Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-06-09"},
{"website": "Arkency", "title": "Mastering Rails Validations: Objectify", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/05/mastering-rails-validations-objectify/", "abstract": "In my previous blogpost I showed you how Rails validations might become context dependent and a few ways how to handle such situation. However none of them were perfect\nbecause our object had to become context-aware. The alternative solution that\nI would like to show you now is to extract the validations rules outside, making\nour validated object lighter. For start we are gonna use the trick with SimpleDelegator (we use it sometimes in\nour Fearless Refactoring: Rails Controllers book as an intermediary step). So now you have external validator that you can use in one context and\nyou can easily create another validator that would validate different\nbusiness rules when used in another context. The context in your system can be almost everything. Sometimes the\ndifference is just create vs update . Sometimes it is\nin save as draft vs publish as ready . And sometimes it based on the\nuser role like admin vs moderator . But let‚Äôs go one step further and drop the nice DSL-alike\nmethods such as validates_length_of that Rails used to bought us and that we all love, to see what‚Äôs beneath them . The DSL-methods from ActiveModel::Validations::HelperMethods are just tiny wrappers for a slightly more object oriented validators.\nAnd they just convert first argument to Array value of attributes key in a Hash . When you dig deeper you can see that one of validates_with responsibilities is to actually finally create an instance of validation\nrule . Let‚Äôs create an instance of such rule ourselves and give it a name . We are going to do it by simply assigning it to a constant.\nThat is one, really global name, I guess :) Now you can share some of those rules in different validators\nfor different contexts. The rules: Validators that are using them: I could not find an easy way to register multiple instances of validation rules. So below is a bit hacky (although valid) way\nto work around the problem. It gives us a nice ability to group common rules in Array and add\nor subtract other rules. Rules definitions: Validators using the rules: The previous examples won‚Äôt cooperate nicely with Rails features expecting\nlist of errors validations on the validated object, because as I showed in\nfirst example, the #errors that are filled are defined on the\nvalidator object. But you can easily overwrite the #errors that come from including ActiveModel::Validations ,\nby delegating them to the validated object, which in our case\nis #user . That was a brief introduction to the more object oriented aspects of rails\nvalidations. Subscribe to our newsletter below if you don‚Äôt want to miss our next\nblogpost that are going to be about problems with refactoring in rails,\nactive record aggregates, another part on validations problems and service\nobjects. We have plenty of ideas for our next posts. You might also want to read some of our other popular blogposts ActiveRecord-related: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-05-08"},
{"website": "Arkency", "title": "Async & remote - toolbox", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2014/06/async-remote-toolbox/", "abstract": "I‚Äôve been speaking recently at a local IT entrepreneur meetup about remote work and about the ways a company can transition towards being more remote. The part of the talk that seemed to be most interesting was the toolset that we‚Äôre using at the moment. I thought I‚Äôd share this list here, as well. Remember, that tools should just support your process and not replace it. If your goals are clear to you and your team, then the tools are just implementation details. For 7 years we‚Äôve been using IRC. It‚Äôs a kind of a old-school tool but it served us well.  We‚Äôve been experimenting with alternatives, like Slack and I can‚Äôt decide for the entire team, because we haven‚Äôt made a decision yet, but I am pretty sure it‚Äôs either Slack or Flowdock for us. The tool itself is not that important, it‚Äôs the goal it serves that is interesting. We have one channel per project and several generic channels, like #lol, #arkency, #links, #coffee, #products, #blog, #social etc. The project channels are often integrated with other tools and receive notifications from Trello, Github, Jenkins. This is like the headquarter. The integration with 3rd party tools works very well in both of them (slack and flowdock). The main difference is that on Slack you receive notification in the same window as your normal chat. Whereas on flowdock you receive the notifications in left window and the chat happens on the right window. Flowdock has a very nice feature that every new chat message can start a new conversiation. If you click an icon near your message, then people can reply to in the the left window. In this mode the left window is used for conversion of one thread (flow) and the right window shows messages from all threads (like in a chat) for currently opened channel. The only problem is, that I almost never close my threads in the left window so I almost never see any notifications. They are hidden when you use left window for discussing one flow. If you know if it can be configured in some way to behave differently, please let me know. Another nice thing in flowdock is that every notification is just a message and as I said every message can start a new thread. Because of that when you receive for example an exception notification from honeybadger you can ping your team members in thread and start chatting about potential fix or root source of the exception. So in flowdock, every discussion has a URL. And that‚Äôs pretty important for me because, if you happen to have a conversation about scope of the ticket with product owner, you can later add it to trello or pivotal ticket for a fuller picture and put it in commit description so that anyone reviewing your commits can read it as well and have common mindset with the author of commit. You can create a code snippet on Slack or upload a file and discuss it in comments, but that‚Äôs it. If you have a 100 message-long discussion about something, you can‚Äôt easily link to it. If you would like to discuss exception or failed build in a thread without the interference of other discussions happening in the same time on channel, well not on Slack. But, on the other hand you don‚Äôt miss any notifications just because you happened to be in the middle of flow conversation So if it was for me to design a chatting solution for developers I would go with 3 panes. One for notifications, one for current thread and one for chatting. We already have monitors > FullHD so place on screen is not a big issue ihmo. As you can see on the screenshots Flowdock keeps the list of the channels on top which became problematic for some of us when there are a lot of channels you are subscribed to. Slack is displaying them in left panel and it handles them better visually ihmo. One of the things that almost every tool is missing, is the ability to give +1 (or ‚Äúlike‚Äù or whatever you call it) without cluttering the interface. Facebook got it right and I belive it should be possible to just express your approval easily. Why are we having this feature in social networks only? We should have it in chats, story trackers and even emails. TLDR: Every tools has its pros and cons. Give them a try in your entire team and make decision based on your real experience. Not based on screenshots or reviews like this one ;) I am pretty sure you are agile and you can switch to a different chatting solution for one week to evaluate it properly. Slack in normal mode Slack in compact mode Notifications on slack This is our default project management tool. It works as a backlog. It contains many tickets/stories, prioritized and described. It helps detailing the requirements and seeing what‚Äôs the status. The tickets are also refactored - we extract new tickets, rename them, group them - whatever is needed. One problem of trello is very limited number of tags that you can assign to ticket. Pivotal tags and epics works way better I think. However I find Trello visually more appealing than Pivotal, because ihmo it is easier to focus on current task and just see few next ones. Pivotal is great for showing a lot of information on one screen, but form it is just overwhelming. And then there is the problem of every tool for managing backlog that I know. The don‚Äôt show you changes in priorities. They don‚Äôt notify you about them. Everything you do for one ticket, inside the ticket, every status change, every comment, assignment, checkbox crossed, they tell you nicely about. But if someone moves task from bottom of the backlog to the top, thus changing its priority to being the most important one, silence. I really wish we could easily see the history of changes to priorities to the boards (backlogs). Something like ‚ÄúTask ‚ÄòX‚Äô was moved 6 positions up. It is now more important than task ‚ÄòA‚Äô and less important than task ‚ÄòB‚Äô‚Äù. If one of my colleagues makes such decision after talking to client about priorities, I would really like to know. For me that is way my important than anything else that is happening :) In some projects we use Pivotal, Redmine or Asana for the same goal. Living on the edge‚Ä¶ Trello with one column only :) One ticket on trello Email notifications from trello Hackpad - this is my favorite one. If you‚Äôre not familiar with it already, it works similarly to Google Docs. In my opinion, it‚Äôs a bit more interactive. It‚Äôs basically a wiki on steroids. It has support for collections, it notifies about changes via email. You clearly see who wrote what. You can comment sections of code and checkboxes. Whatever interesting happens in our company, it gets an URL on Hackpad. Do I have an idea for a bigger refactoring in one project? I create a hackpad, paste some code and describe the plan. Others can join whenever they want (async!) and add their comments. The email notifications are very powerful tool to keep being updated in an asynchronous discussion about topics that you subscribed to. Here is a screenshot with notifications from our hackpad dedicated to our annual Arkency Camp event. As you can see the notifications are provide with a a bit of context for the changes, so sometimes you don‚Äôt even have to open the pad, to know what‚Äôs been changed or what is someone opinion about the topic. Mumble is probably unknown to you. It‚Äôs a tool very popular among gamers. They use it to communicate in a team, during a game. We started using it, as it was much more reliable than Skype. It‚Äôs a very minimalistic tool. It has the concept of channels, so we designed it similarly to our communication tool (IRC). It also allows recording the conversations, so that people who were not able to attend (async!) can later listen to the most important fragments. You can configure it to work in ‚ÄúPress button for talking mode‚Äù which is very convenient to use. I wish more software adopted this approach. Most of them transmit voice all the time which if you happen to be somewhere loud means that you need to click somewhere on the interface to constantly mute/unmute yourself. Other tools activate automatically when there is sound which also doesn‚Äôt work great sometimes. But mumble configured in this mode works great because you just keep the button pressed as long as you are talking and this coupling feels very natural and it is easy to get such habit. Our mumble server is self-hosted by us. Maybe that‚Äôs one of the reasons that the sound quality is better than any other tool like skype or google hangout. Or maybe it‚Äôs just mumble. I don‚Äôt know, but it is a pleasure to use this tool for voice communication. Works on mac, linux, windows, ios and android. We use Github for hosting code and for making the micro-code reviews. I call it micro, as they only let us comment the deltas - commits or pull request. They don‚Äôt let us comment the existing code base, which is a limitation many similar tools share. If there‚Äôs one place, I‚Äôd like to see improvement for remote teams it would be a proper code review tool. We host the Jenkins instances to build our projects. I‚Äôm very far from liking it - it has many quirks, but overall we didn‚Äôt find a good alternative to switch, yet. We had some problems with CircleCI and TravisCI running our biggest projects. But for smaller projects CircleCI worked great in our case. Maybe it‚Äôs time for us to review these tools again. Maybe they matured and handle things better now. I blogged about remote pair programming 6 years ago. At that time, I was using screen + vim and I still think it‚Äôs a good combo (together with Skype or Mumble). Nowadays, we don‚Äôt pair program too often, but when people in my team do that, they often use tmux to connect to each other (terminal-based). Another tool is tmate.io, which is also tmux-based. On a good connection you can also use Google Hangouts and their Desktop Sharing feature. We use video calls very rarely and mostly, when external teams are involved who prefer it. In that case, we use Google Hangouts. We have our small self hosted app which just lists uploaded .webm files that every new browser can play without any plugin. And automatically create and embed hackpad below the video to have a place for discussion. But recently we started to experiment with non-public videos on youtube and that also works great. It‚Äôs important to understand that those are only tools. They can change. I‚Äôm pretty sure, next year, we‚Äôll use a different toolset. What‚Äôs important is the process around it - how you collaborate on projects, how you split stories, how you discuss, how you collaborate on the code, how do you spread the knowledge and achieve consensus. What tools are you using? What would you recommend? What‚Äôs your process? Next part: Effective async communication Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-06-11"},
{"website": "Arkency", "title": "Take most out of async textual standups", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2014/06/async-standups/", "abstract": "When you work remotely, you want to have some kind of a standup meeting regularly. In our team, after experimenting with many different approaches, we settled with text-based, asynchronous standups every day . Additionally, every project has a weekly ‚Äòsync‚Äô meeting. Whatever tool we currently use for remote communication (irc in the past, now Slack), we create a channel that is dedicated to #standup. We don‚Äôt have a specific time to post there, usually we do it, when we begin our work session - so, in the spirit of async - different people at different times. I consider #standup to be a very good opportunity to communicate cross-project, to educate, to learn, to help . Short standup messages are not bad, but they miss this opportunity. When writing the standup message, think more about the others, than about yourself - what can they get from it by reading your status? Yesterday I finished the ‚Äúfix the price calculator‚Äù feature, which was mostly about removing the Coffee code and rely on the value retrieved from the backend, via ajax. The nice thing was that the backend code was already covered with tests, while the Coffee one wasn‚Äôt. After that I helped Jack with the ‚Äúallow logging in with email‚Äù feature (we need it now because we have a batch import of users from system X soon). After that I did a small ticket, where I block buying licences for special periods of time. This was nicely TDD'ed, thanks to the concept of aggregate, introduced by Robert recently - all the tests pass < 1s. Here is a commit worth looking at. Today I‚Äôm going to start with foreman'ing the recent commits and after that I want to work the XYZ system to allow a better editing of entries. I‚Äôm not sure how to start it, so all help is welcome. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-06-16"},
{"website": "Arkency", "title": "Effective async communication", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/06/effective-async-communication/", "abstract": "What we mean by working asynchronously is not to always work async, to avoid meetings and video calls at all cost. But to rather prefer async way of communicating when it is favorable , when you are sure that sync discussion wouldn‚Äôt bring you much more. At first you might think that it is almost never but if you keep pushing yourself a little bit you will soon start to realize that you are getting better at it. In fact now we have much more async discussions than sync discussions. Because that means less disruptive interruptions for you and your team. In sync mode developers try to keep writing code and reply to everything that happens in the surrounding environment (just like your CPU does). It‚Äôs hard to keep focused that way and write a good code. That‚Äôs why you see so many developers sitting with headphones in the office . They are trying to communicate to the world that they are busy and now it‚Äôs not a good time to be asked of any favor. In teams which embraced async communication and discussion (especially remote teams) it looks a little different. Developers don‚Äôt receive notifications that needs to be answered immediately. Instead they go seek them when they are ok processing them. So I might schedule 60-120 minutes of time for myself purely for coding . I set myself away on chat and turn of notifications fur that time. In that time I am fully in developers mindset. Coding, TDD, Architecture is what I breath at that time. When I am done with some part of the problem that I wanted to implement I get out of zone. I check my email, chat, answer peoples questions, send my own questions to people. I might take a break to grab a lunch or stretch a neck, send something nice to my wife. When I am done, I can schedule next uninterruptible part of time for myself to code. This is great for a daily work when you know what to work on, tasks are already discussed and properly prioritized or assigned. It‚Äôs a casual day of asynchronous programmer. It might look similar to you to Pomodoro technique except that Pomodoro default is usually very small (25min) and I found that for serious programming you usually need more time than that to get all the necessary concepts into my head. So I usually try to organize my day around 4 or 5 sessions that are about 90 minutes long . I don‚Äôt focus that much on time, but rather on my task and moving it forward. When I am happy about the results and feel good about it, when my attention starts to diminish, that‚Äôs about a good time to take a break and figure out what happened in the project in the meantime . If I finished something meaningful in 60 minutes, good for me, I can take a break earlier. If it took me loner, like 105 minutes, that‚Äôs still ok. I usually keep around myself nuts, seeds and almonds to be able to keep coding without feeling hungry. The time for bigger meals is between those scheduled sessions of work. However we don‚Äôt always have a nice, prioritized backlog of well understood tasks. In that moment it might be a good time to schedule longer, sync communication with your boss, PM or product owner. Remember, 5 hours of coding can save you 1 hour of research ;). Sync communication is good to discuss poorly defined, or not well understood requirements for next tasks if we are going to start working on them soon like today or tomorrow. But sometimes we have more time, the deadline is not even on horizon, so we can still discuss the details asynchronously and without rush. In such case we usually go with document (hackpad or google doc). We start by defining problem and proposing initial solution (sometimes even a dummy one in the spirit of McDonalds theory ). Sometimes we trigger the beginning of discussion on our weekly meeting and continue in the document. Sometimes the sparkling comes from screencast describing the problem. It doesn‚Äôt matter. The idea is that all discussion happen async because the problem to solve is not threatening us immediately . You can even add a task to your backlog to have people express their opinion on the subject. But usually you don‚Äôt have to. People will just jump on board and start discussing because they will receive mentions or notifications that someone else expressed their opinion in the document. That‚Äôs how it works. The nice side effect is that at the end you have everything nicely documented . All your options, all pros and cons, yays and nays in one place to make your final decision. Of course you can record voice and video communication (and we sometimes do) but the problem with them is that it takes way longer get something out them when you come back later. Recordings are great for sharing what happened with someone who were absent, but they are not great as documentation. Sharing knowledge in remote, async teams might look challenging at first . In the office, you usually have someone walk to your desk and either talk a bit about bunch of code or even practice pair programming for a moment. In Arkency we went different way. There are few tools and solutions that we use to share knowledge about code, good solutions, practices etc. This if my favorite one. Did you do something cool in your company? Did you just understand nice concept in your domain? Were you enlighten about new way to use an old tool? Is there a problem that you are struggling with? Record a screencast. It scales very nicely because you can use it to communicate with multiple people . It‚Äôs awesome for people who joined your company or team later. It can be useful for other teams in your company working on different projects. If you get accustomed to this way of sharing knowledge you can even start recording them professionally and sell online (something still on my todo list). It‚Äôs one of the most single effective technique that we adopted in our company. At first it was just an experiment. Instead of sharing screen and having conversation with someone who could potentially help you, we started to record screencast. Usually because someone was absent or working different hours or simply because we wanted to know the opinion from everyone involved in our team. We used to put them in Drobox and send each other link on our chat. Then it became so popular that we decided to standardize on .webm format and created a small app for serving them over our VPN. Now we are in the phase of experimenting of uploading them to Youtube (but not publicly). So we don‚Äôt need VPN, hosting, and if you link it in discussion you will get nice thumbnail of the video. And that works nicely as well. When we feel strongly that the video is good and does not contain any secret information, we can just publish it on YT and involve the community in our discussion. This helps our branding and shows to potential candidates how we work internally. It is also a nice training before presenting on Ruby User Groups meetups or on conferences. When recording screencast we usually just open browser with github commits or text editor to show the code we would like to discuss. We try to keep the videos short (5m) because it turns out more people are willing to watch them in such case but even the longer ones (some take half an hour) are welcomed nicely. In many cases it is good to break whatever larget topic you have in mind into few smaller things. People will more likely watch three 5-minute-long videos (in their small break or over multiple days) than one that takes 15 minutes. The pressure to have short videos also makes people think more deeply about the structure and cut the crap out of it. Just dive into the video topic and avoid unnecessary digressions. Leave them to the comments of the video. Example screencast: We discuss Hackpad more deeply in blogpost about tools that we use so I won‚Äôt go into much details here. I just wanted to mention that since it is a very nice mix of wiki&google-docs with very minimal friction , we use it for almost everything. But more for documenting decisions, requirements, discussing ideas, having checklists than for discussing code. You can paste code to hackpad and format it nicely but it just doesn‚Äôt feel right most of the time to discuss code outside of its context and surroundings. In some projects our customers collaborate with us on hackpad to properly distill the domain knowledge and establish ubiquitous language . When it happens that you and your coworker in need work at the same time (despite being async and remote, most people have similar schedule and start somewhere in the morning, usually between 7am and 10am) you are not in the zone, but in communication mode, switching to voice communication might be very efficient. I only wish that we recorded some of those spontaneous conversations more often. It happened many times that what started as typical conversation turned out be really interesting evaluation of potential options to solve a problem and very interesting discussion between two individuals. But you usually realize this after fact and wish you started recording 20 minutes ago so you could have share this talk with the rest of the team. So it might be a good habit to record every conversation and just not put it online if it happened to be a casual one with no shareable output. Or we should switch to a tool that would record everything for us just like chatting tools keep to remember everything in the history ;) In case when we want to share screen (usually because of the code) we jump to Google Hangout. But Mumble with better voice quality and lower broadband requirements seems to be less disruptive so for now this is our default ad-hoc communication channel. Whatever tool you use for recording your desktop (screencasts) it will probably let you record Google Hangout discussion as well. Our Mac users tend to use Screenflow app for this purpose. A small (usually last) part of our company weekly meeting is dedicated to sharing knowledge. Everyone has a chance to express something interesting about what they learnt, did or failed at this week. You can hear people success stories and their small, everyday failures . It is our time to share the knowledge in cross-project manner. To have it flow even between people who don‚Äôt cooperate together right now. We find it to be very valuable. Every company tries to balance the number of meetings (the lower the better) with knowledge flowing through the company (the more the better). And I won‚Äôt lie to you that this is hard and I am pretty sure we don‚Äôt have a silver bullet for that. But one of the things we found interesting after applying all those tools and techniques mentioned before is that we don‚Äôt have that much need for daily, sync standup in our projects . Audio/Video Standups are great at the beginning of the project when teams are forming and requirements might change of often. But over the time standup starts to become more boring, repetitious, less interactive and generally bringing less value. From a discussing tool they tend to change into status updates. I believe this is mostly due to a pressure to keep them short. So after every of your team member speaks their voice there is not much time for a discussion on bigger issues. No time seems to be a good one for standups in big teams . You make them in the morning, someone is not gonna be there because they wake up later. You make them before lunch, everybody is there but people are hungry and want to finish early. You put them after lunch, again might be problem to have everyone on board because some people eat at different hours and it takes them different amount of time to finish. If you have a scheduled meeting every day at the same hour, people will organize their schedule around it. They know that standup is going to interrupt their flow and they must finish coding something before the standup starts . Otherwise they will have to get into the zone again to finish the task and that takes time. And if you finish something 15-5 minutes before standup, you know there is no point in starting new task right now because you are going to be interrupted in a moment anyway. So we dropped the daily audio standup routine in favor of other techniques: As you can see we try to keep number of enforced discussion very low. Too many mandatory meetings are considered harmful. Instead we prefer smaller talks including only interested participants that mostly happen in text before they are evaluated on a synchronous audio/video meeting. In other words: a lot of async before we sync . In next blogpost we are going to show you how to take most out of async, textual standups so that they are truly a masterpiece :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-06-13"},
{"website": "Arkency", "title": "Real-time Web Apps", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/06/real-time-web-apps/", "abstract": "From the very beginning web pages were just static. Internet browsers received some HTML code to render and displayed it to user. When someone wanted to check if there are any news on our page, he had to refresh it and then some new data might appeared. Time to change it. Nowadays, more and more often we‚Äôd like to have a real-time experience in our web applications. We are lazy and we want to be notified about changes without continuously clicking on refresh button in our browsers. Unfortunately HTTP protocol is based on request-response paradigm, so rather than being notified by a sever or listening on an open connection, we make some GETs or POSTs and receive data as an answer. So far, to simulate this real-time behavior, we had to do some work-arounds such as a interval-timeout JS pattern or more sophisticated, but confusing long pooling design concept .\nLuckily, things have changed. In the age of modern browsers, HTML5 API, smarter users and skilled developers we have (and should use) great tools to build real real-time web applications. WebSockets establish persistent connection between user‚Äôs browser and a server. Both of them can use it any time to send messages to each other. Every side of such connection listens on it to immediately receive incoming data. These messages can be primitives or even binary data. WebSockets allow to cross-domain communication so developers should pay attention on security issues on their own, because we aren‚Äôt bound to same-origin policy anymore and can communicate across domains. There‚Äôs no special WebSocket opening method. After created, browser immediately tries to open a new connection. One of WebSocket property is readyState which is initialized with WebSocket.CONNECTING . Once connected, state changes to WebSocket.OPEN . Client side: Server side: Source: dsheiko.com Because establishing a WebSocket connection might be a little bit tricky, it is worth to describe here some more details about that. A client connects with a server using so called handshake process. The initial request should look like this: And a server‚Äôs response: We can see here that client sends Sec-WebSocket-Key header with some base64 value. Next, server append to it 258EAFA5-E914-47DA-95CA-C5AB0DC85B11 string and return base64 of SHA-1 from this concatenation as a Sec-WebSocket-Accept header.\nThis handshake is supposed to replace initial HTTP protocol with WebSocket using the same TCP/IP connection under the hood. Provisioning process allows to get known both sides and be recognized in future messages. Here is some nice demo. If you are reading us, there is a high chance that you are a Ruby developer. So naturally you might be tempted to use\nruby solution for you server side code such as EventMachine like that: That might be great learning exercise to get\nmore familiar with async code and feel more confident. However we would advise against using EM on production ( speaking\nfrom experience because we once did ). Long story short. EM is quite OK, but the ecosystem around is unfortunately pretty small. And sometimes buggy in hard to\nreproduce or notice way. There are a bunch of Ruby libraries that can help integrate your software with many\n3rd party solutions and speed up the development process. However you can‚Äôt use most of them in your EM-based\napplication because their behavior is blocking and it would block your event loop (and take away all the benefits from\nusing EM and its non-blocking approach). If you wanted to integrate such a blocking library into your EM application anyway\nyou would have to schedule calling the library by using EventMachine.defer .\nThat would lead to mixing non-blocking event based approach represented by EM and blocking behavior from your library.\nThe mental overhead of it is not small. At first sight it might seem that EM-compatible libraries are existing and working. There are certainly some HTTP libraries and database drivers. But once you start looking for those that will work properly\nwith more advanced options such as replication you might find your options limited. It‚Äôs a shaky ground. Another problem is that you won‚Äôt probably find much commercial options available for deployment of EM-based\napplications. So you will have to figure out the problem of deployment, rolling deploys, clean exit yourself. So if you need non-blocking, server side solution for your application our recommendation would be to rather go with node.js which is much more popular and battle tested solution compared to EM. We really enjoyed using EM and a few \ngem which were nicely designed and tested. But the more you move away from get N-bytes from here and send them out there usecase and more towards application logic, flow and business processes, the more painful experience it becomes to keep\nusing EM. TLDR: EM for fun? Yes. EM in production? You better consider twice your options. Server-Sent Events intended for streaming text-based event data from server directly to client . There are two requirements to use this mechanism: browser interface for EventSource and server 'text/event-stream' content type. SSE are used to push notifications into your web application, which makes it more interactive with user and provides dynamic content at the time it appears. Client side: Server side: Source: dsheiko.com Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-06-19"},
{"website": "Arkency", "title": "My journey to Arkency world", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2014/06/my-journey-to-arkency-world/", "abstract": "Do you feel the need for a change? My developer‚Äôs career started in one of Wroc≈Çaw corporations as a Java Developer. For most of my studies I had been developing my knowledge about that technology so It was effortless for me to work in that language.  I had been working for that company for almost a year, but I didn‚Äôt really feel it.  Something was wrong. Days were similar to each other. I was working in mode ‚ÄúDaily meeting -> coffee -> fix old problem -> coffee etc.‚Äù.  I started to think about it and I came to conclusion that I have to change something. A few months ago I went to local IT meeting.  Robert Pankowecki had there his lightning talk about Async Remote and he was speaking about the style of work in Arkency. Thanks to his speech, I learned that you can work in a different way and I thought that ‚ÄúThis is it!‚Äù. After the meeting I started to look for information about Arkency (then I found this blog) and despite the lack of information about the application process I sent my application anyway. This is how things changed for me. Before I came to Arkency I talked a lot with my former work mates about being remote. In my case I heard many times that I was less experienced and It would be hard to work asynchronously. I would need to get a lot knowledge transferred on me and this would require spending many hours on talking, which would be difficult to do remotely. But we use technologies which helped me in that case. We use technologies such as hackpad, screencasts or simply slack which help us to share knowledge. There I can find all the information I need if not there is always fast mumble session. I think those means of communication are even better than standard contact between people (please don‚Äôt understand me wrong;)) because I can go back easily to what we talked about earlier and I don‚Äôt waste anyone‚Äôs time. I started my work almost from scratch. New language, technologies, type of work (remote) and project. I wasn‚Äôt a very experienced as a developer and it would seem that it would be hard for me to bring value in the project fast. But I‚Äôve noticed It isn‚Äôt true. The way we manage work made it easier for me to enter the company. The way we manage tasks - creating small tickets, getting first top task and not assigning story to anybody, all this has meant that I got to know business logic and used technology. In the previous company I often had to work on some modules that I was familar with before and that was supposed to speed up my performance. But this resulted in me exploring just a part of the system and it took long to get to know the logic behind the domain. After a month in Arkency, I gained virtually the entire knowledge about the system. I‚Äôm very happy that I tried both the standard office work and my current ‚ÄúArkency-way‚Äù work. I can see all pros and cons of both types of work and now I know that the current work type suits me better. It was one of the many life changes and there is an important lesson that I learned. If you see that you can‚Äôt fit somewhere or you can‚Äôt find yourself in some situation simple listen to yourself and change it. You can win a lot. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-06-23"},
{"website": "Arkency", "title": "Create, run and manage your Ruby background processes with upstart", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/06/create-run-and-manage-your-background-processes-with-upstart/", "abstract": "Have you ever wanted to run some of your ruby program as a background service ? There are a couple ways to do that starting from simple & , through runit to complex services. This tutorial is intended to give you a quick overview of upstart , which is one of the possible solutions to run and manage your background processes. According to upstart homepage : Upstart is an event-based replacement for the /sbin/init daemon which handles starting of tasks and services during boot, stopping them during shutdown and supervising them while the system is running. It was originally developed for the Ubuntu distribution, but is intended to be suitable for deployment in all Linux distributions as a replacement for the venerable System-V init. In properly set up production environment there is no ruby version manager . Using one ruby per user makes your script extremely easy to run and there‚Äôs no need to provide any special configuration here. Unfortunately, there are still some environments, which use rvm or rbenv so, to make this article comprehensive, we need to show how to configure at least one of them. Let‚Äôs choose rvm for managing rubies. You‚Äôll need to create wrapper for ruby executable to be accessible from my_program.conf . According to rvm website it can be done via simple command: Now your ruby bin can be found under: Then, create configuration file that contains script to be executed in a background job: a) You have to tell your script when to run and start on command is used for that. The syntax looks like: <your_command> can be for example: b) In the same way you can define stop on command. c) The very next command is respawn which tells your process to run after being killed. d) You can execute your program right now, and there are two way of doing so. execute is one-liner keyword to run simple script, block is for multiline code. e) our conf file might look like: As you might have guessed, there a couple ways to do that. a) when we run our script on custom event you can run it like b) if you want use old service syntax you can do: c) you can do also: You can execute the basic commands like start , stop , restart and status directly without preceding them with initctl . I recommend you to take a look at initctl , which offers quite nice helpers like e.g. list , which lists all registered services and their state. If you want to see execution logs (to check whether everything goes correctly) there‚Äôs a handy way to display them: On the very end I‚Äôd like to recommend you a bunch of resources containing many examples and explanations of what we‚Äôve done here. This article only scratches the surface of powerful upstart tool. Please dive into following links to get more information about it. Although upstart has been sunset in favour of systemd as a default init system, it is still used for other use cases. What is more, systemd adoption is wider than upstart across Linux distributions. However, there are some fallacies about the former, which is thought to be more complex than intended. Moreover there are still not many resources and tutorials that well explains systemd and provide step-by-step solutions to write and run background scripts. We encourage you to dive into both of them and choose by yourself which one is more suitable and less complex for your own needs. If you want to know even more about upstart , there‚Äôs an another blogpost with additional features involving upstart user jobs. See you then! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-06-29"},
{"website": "Arkency", "title": "From Rails to RubyMotion: basic toolkit", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/07/one-ruby-to-rule-them-all/", "abstract": "Have you noticed the tendency that development is moving strongly towards mobile ? There are more and more projects with mobile companion, there are a lot of just-mobile software houses, and there are even only-mobile projects.\nWhat if a company has only web developers specialized in Ruby, but want to be really full-stack? Doesn‚Äôt matter, with Ruby, it can still goes mobile ! Yet some time ago, if Ruby developer wanted to experiment with Android or iOS, he have to dive into Java or Objective-C. That might have been cumbersome if he only want to play with it, because he had to learn a new language. For these more assiduous it worked - they learned it, used it, then went back do daily work and‚Ä¶ forgot it. You know how is that - not used is being forgotten. We‚Äôd like to introduce here and encourage to use RubyMotion - toolchain that lets you quickly develop native iOS and Android applications using the Ruby programming language. Our adventure with RM started recently, so we don‚Äôt feel skilled enough to present here some advanced code yet. That will happen, for sure, in the next blog posts, so you won‚Äôt miss anything. For now we‚Äôd like to focus on our research done so far in that area. As we are RoR developers , we usually depend on gems. They help us not to reinvent the wheel and speed up our development process. Sometimes they are deceptive, but it‚Äôs not the topic of this blogpost so let‚Äôs say that there are generally helpful.\nBefore we started RM we made some research how does the 3rd party support looks like. What struck us the most is that RubyMotion has tremendous bunch of libraries, pods and gems to improve our productivity. So let‚Äôs talk about them. I‚Äôm not sure if framework is the best word word for describing what I want do present now, but in RM world there are some tools that makes mobile platforms completely separate from their native languages end extremely easy to implement. They are essentially gems but this is analogy to Rails among other Ruby gems. Here‚Äôs the list: Vanilla RubyMotion - this is RubyMotion itself, that allows to write the same methods taken form native platforms in ruby with just a little tweaks. It‚Äôs the closest implementation to Java or Objective-C. ProMotion - it makes verbose Objective-C syntax more ruby-like by hiding native methods behind ruby-convention ones. PM also offers a bunch of ready classes to manipulate views without struggling with sometime complex implementation. RMQ - this is the jQuery for Objective-C. It makes extremely easy to manipleulate views, traverse between components, animate and style whatever we want and handle events and user gestures. From the many videocasts , sources and examples we‚Äôve extracted basic configurations used among the projects and here they are: Must-have-one Styling (with emphasize on SweetKit) Models Frameworks (with preference to simpler for the beginning RMQ) We encourage every ruby developer to try RubyMotion . It‚Äôs a great way to go into mobile even if you don‚Äôt know (and like) Java or Objective-C. We are impressed the RubyMotion great support , tools and community despite it‚Äôs a standard as befits the Ruby world. For now, stay tuned for more mobile blogposts! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-01"},
{"website": "Arkency", "title": "Service objects as a way of testing Rails apps (without factory_girl)", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/06/setup-your-tests-with-services/", "abstract": "There‚Äôs been recently an interesting discussion about setting up the initial state of your tests. Some are in favor of\nusing built-in Rails fixtures (because of speed and simplicity). Others are in favor of using factory_girl or similar\ngems. I can‚Äôt provide definite numbers but judging  based on the apps that we review, in terms of adoption, factory_girl seems to have won . I would like to present you a third alternative ‚Äú Setting up tests with services ‚Äù (the same ones you use in your\nproduction code, not ones crafted specifically for tests) and compare it to factory_girl to show where it might be beneficial to go with such approach. Let‚Äôs start with a little background from an imaginary application for teaching languages in schools. There is a school in our system which decided to use our software and buy a license. Teacher can create classes to\nteach a language (or use existing one created by someone else). During the procedure multiple pupils can be imported\nfrom file or added manually on the webui. The teacher will be teaching a class. The school is\nhaving a native language and the class is learning a foreign language. Based on that we provide them with access\nto school dictionaries suited to kids‚Äô needs. Let‚Äôs think about our tests for a moment. So far so good. Few months pass by, we have more tests we setup like that or in a similar way and then we start\nto stumble upon more of the less common usecases during the conversations with our client. And as it always is\nwith such features, they force use to rethink the underlying architecture of our app. One of our new requirements is that when teacher is no longer assigned to a class this doesn‚Äôt mean that a class is not\nlearning the language anymore. In other words in our domain once pupils are assigned to a class that is learning\nFrench it is very unlikely that at some point they stopped learning French (at least in that educational system which\ndomain we are trying to reflect in our code). It might be that the class no longer has a french teacher for a moment\n(ex. before the school finds replacement for her/him) but that doesn‚Äôt mean they no longer learn French. Because we try to not delete data (soft delete all the way) we could have keep getting this knowledge about dictionaries\nfrom Assignments . But since we determined very useful piece of knowledge domain ( the fact of learning a language is\nnot directly connected to the fact of having teacher assigned ) we decided to be explicit about it on our code. So we\nadded new KlassLanguage class which is created when a class is assigned a new language for the first time. We changed the implementation so it creates KlassLanguage whenever necessary. And we changed #dictionaries_for method to obtain the dictionaries from KlassLanguage instead of Assignment . We migrated old data. We can click\nthrough our webapp and see that everything works correctly. But guess what. Our tests fail . Why is that so? Our tests fail because we must add one more piece of data to them. The KlassLanguage that we introduced. Imagine adding that to dozens or hundred tests that you already wrote. No fun. It would be as if almost all those tests\nthat you wrote discouraged you from refactorings instead of begin there for you so you can feel safely improving your\ncode . Consider that after introducing our change to code, some tests are not even properly testing what they used\nto test . Like imagine you had a test like this: This test doesn‚Äôt even make sense anymore because we no longer look for the dictionaries that are available for a pupil\nin Assignments but rather in KlassLanguages in our implementation. When you have hundreds of factory_girl-based test like that they are (imho) preventing you from bigger changes to your\napp. From making changes to your db structure, from moving the logic around. It‚Äôs almost as if every step you wanna\nmake in a different direction was not permitted. Before we tackle our problem let‚Äôs for a moment talk about basics of TDD and testing. Usually when they try to teach you\ntesting you start with simple data structure such as Stack and you try to implement it using existing language structure\nand verify its correctness. So you put something on the stack, you take it back and you verify that it is in fact the same thing. Why am I talking about this? Because I think that what many rails projects started doing with factory_girl is no longer similar to our\nbasic TDD technique . I cannot but think we started to turn our test more into something like: So instead of interacting with our SUT ( System under Test ) through set of exposed methods we violate its boundaries\nand directly set the state . In this example this is visible at first sight because we use instance_variable_set and no one would do such thing in real life. Right? But the situation with factories is not much different in fact from what we just saw. Instead of building the state\nthrough set of interactions that happened to system we tried to build the state directly . With factories we build the state as we know/imagine it to be at the very moment of writing the test. And we rarely tend\nto revisit them later with the intent to verify the setup and fix it . Given enough time  it might be even hard to imagine\nwhat sequence of events in system the original test author imagined leading to a state described in a test. This means that we are not protected in any way against changes to the internal implementation that happen in the future.\nSame way you can‚Äôt just rename @collection in the stack example because the test is fragile. In other words, we introduced a third element into Command/Query separation model for our tests. Instead of issuing Commands and testing the result with Queries we issue commands and test what‚Äôs in db. And for Queries we set state\nin db and then we run Queries . But we usually have no way to ensure synchronization of those test. We are not sure\nthat what Commands created is the same for what we test in Queries . What can we do to mitigate this unfortunate situation? Go back to the basic and setup our tests by directly interacting\nwith the system instead of building its state. In case of our original school example it might look like. This setup is way longer because in some places we decided to go with longer syntax and set some attribute by hand\n(although) we didn‚Äôt have to. This example mixes two approaches so you can see how you can do things longer-way and shorter-way (by using attributes). We didn‚Äôt take a single step to refactor it into shorter expression and to be more reusable in\nmultiple tests because I wanted you to see a full picture of it. But extracting it into smaller test helpers, so that the\ntest setup would be as short and revealing in our factory girl example would be trivial . For now let‚Äôs keep focus on our case. What can we see from this test setup? We can see the interactions that led to the state of the system . There were 3 of\nthem and are similar to how I described the entire story for you. First teacher registered (first teacher creates the school\nas well and can invite the rest of the teachers). Teacher created a class with pupils (well, one pupil to be exact).\nTeacher assigned the class to himself/herself as a French teacher. It‚Äôs the last step implementation that we had to change to for our new feature. It had to store KlassLanguage additionally and required our tests to change, which we didn‚Äôt want to. Let‚Äôs recall our test: I didn‚Äôt tell you what teaching was in our first version of the code. It doesn‚Äôt matter much for our discussion or\nto see the point of our changes but let‚Äôs think about it for a moment. It had to be some kind of Repository object implementing #dictionaries_for method.\nOr a Query object. Something definitely related and coupled to\nDB because we set the state with factories deep down creating AR objects. It can be the same in our last example. But it doesn‚Äôt have to! All those services can build and store AR objects and\ncommunicate with them and teaching would be just a repository object querying the db for dictionaries of class that\nthe pupil is in. And that would be fine. But teaching could be a submodule of our application that the services are communicating with. Maybe the key Commands/Services\nin our system communicate with multiple modules such as Teaching , Accounting , Licensing and in this test we are\nonly interested in what happened in one of them. So we could stub other dependencies except for teaching if they were\nexplicitly passed in constructor. So with this kind of test setup you are way more flexible and less constrained. Having data in db is no longer your only\noption. In some cases you might wanna consider setting up the state of your system using Services/Commands instead of directly\non DB using factory_girl . The benefit will be that it will allow you to more freely change the internal implementation\nof your system without much hassle for changing your tests . For me one of the main selling points for having services is the knowledge of what can happen in my app . Maybe\nthere are 10 things that the user can do in my app, maybe 100 or maybe 1000. But I know all of them and I can mix and\nmatch them in whatever order I wish to create the setup of situation that I wish to test. It‚Äôs hard to set incorrect\nstate that way that would not have happened in your real app, because you are just using your production code. This is an excerpt from Fearless Refactoring: Rails Controllers . For our blog post and newsletter we end up here but in the book there will be a\nfollowing discussion about shortening the setup. We will also talk about the value of setting UUIDs and generating them\non frontend. As well why it is worth to have an Input class that keeps the incoming data for your service (usually user input). Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-01"},
{"website": "Arkency", "title": "RubyMine basic navigation features (that make you move around code fast)", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/07/rubymine-basic-navigation-features/", "abstract": "Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-09"},
{"website": "Arkency", "title": "Developer matters", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/07/developer-matters/", "abstract": "Have you ever wondered what does your regular workday look like ? Do you have the same daily habits ? So how is it going? Breakfast, coffee, daily standup, reading mails, some work, off-topic talk with coworkers, dinner, some work and the day is over? If you follow these chain (not necessary in the same order) it is important signal that things must change . Not tomorrow, but right now . Since I‚Äôm working in Arkency, a lot of things changed for me. What is more, recent blogpost by Tomek gave me a fresh overview on my work, being productive and self organized. Moreover, I recently read Robert ‚Äôs book Async Remote , which gave me a better understanding of being a good developer so I‚Äôd like to share with you my impressions. Have you ever worked in Agile methodology? You probably have. It is also possible that you were working with Scrum or some kind of its variation depending on your needs. And that is good, because it means you are good . You can deliver, work in dynamic environment, follow iterations and create robust software. So what next? Why change anything? This book is intended to show you a different approach to development . A better one?  An approach, where developer matters . Not software, neither estimations, nor deadlines. The final book‚Äôs appraisal is up to you. You may wonder right now ‚Äúwhy do I need yet another development methodology?‚Äù .  I am not going to convince you, because that‚Äôs not my goal. This book is to give you an opportunity to see how development process can be agile and alternatively to change yours. No pressure, no requirements, just a free talk. We are talking about agile , right? But what agile actually is ? Agile means transparency . What‚Äôs the most important in dealing with clients, working with teammates, writing a software and managing a project is flexibility . So we have two more buzzwords beyond agile, they are transparency and flexibility and after lecture, you‚Äôll get familiar with concepts behind them, what they truly mean and how to achieve them. It‚Äôs good when you can define from the very beginning why do you want to read this book at all. There may be a bunch of reasons so let‚Äôs discuss some of them. Firstly, you might be curious like developers are. It‚Äôs good, because you are open-minded , ready to confront with different approach and willing to get to know new solutions. This book is perfect for you then, because it covers all topics, which describes not so standard way to software development and project management. The another reason is that you want to change something . It‚Äôs good too, because it means that you have some expectations . If you expect to find a silver bullet, this book won‚Äôt you one. Although it won‚Äôt offer you complete and comprehensive solution for all kind of problems, what is written here will definitely open your mind and directs you towards agile way. I recommend you to read a couple of sample chapters on our blog before final decision. After that you can visit book‚Äôs website and download a free sample and then click magical blue button , which will change your life forever. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-13"},
{"website": "Arkency", "title": "Microservices - 72 resources", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/2014/07/microservices-72-resources/", "abstract": "Microservices - everyone talks about them nowadays. There‚Äôs no common understanding however what they are. I‚Äôve been researching this topic for a while, gathering and evaluating articles, presentations and conference videos. http://martinfowler.com/articles/microservices.html Good broad introduction to the topic and further references, mentions ‚Äúyou build it, you run it‚Äù from Werner Vogels. Make sure to check related comments: https://news.ycombinator.com/item?id=7382390 http://yobriefca.se/blog/2013/04/28/micro-service-architecture/ Another broad description, how to go from legacy to microservices, also mentions importance of ops skills in this architecture. http://abdullin.com/journal/2014/1/20/how-micro-services-approach-worked-out-in-production.html Case study with some practical advice, mentions performance. https://github.com/tobyclemson/testing-micro-service-architecture-presentation/raw/master/presentation/testing-strategies-in-a-micro-service-architecture.pdf Shows anatomy of microservice, relation to DDD, various testing aproaches explained. http://bovon.org/index.php/archives/350 ‚ÄúEach business capability [microservice], should be no bigger than my head when you chunk up to this level. And these capabilities should be business-meaningful.‚Äù http://www.udidahan.com/2014/03/31/on-that-microservices-thing/ Remarks on microservices coupling on the example of product price. http://arnon.me/2014/03/services-microservices-nanoservices/ Microservices is essentialy SOA without vendor bullshit. http://service-architecture.blogspot.co.uk/2014/03/microservices-is-soa-for-those-who-know.html Apparently microservices is just SOA described with langauge that can be understood by ordinary people. I‚Äôve also learned that ‚Äúmicroservice‚Äù is insulting for SOA service (too small). http://blog.wordnik.com/with-software-small-is-the-new-big The concept of microservice owner - more like product owner - that answers pager alarms, more less You build it, you own it . http://www.paperplanes.de/2013/10/18/the-smallest-distributed-system.html Not sure if microservices but still relevant on importance of monitoring in distributed system. http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html and http://contino.co.uk/microservices-not-a-free-lunch/ Good list of potential problems and fantastic response in comments: Being one of the tech leads on transforming a monolithic Java application to a SOA implementation, I‚Äôve come across everyone of the issues you raise but instead of seeing those as problems I see them as opportunities to build software better. http://peopleandcode.blogspot.in/2014/03/microservices-and-agility.html Mentions most common reasons to partition the system and organization, also confirms that bounded context makes great microservice. http://davidmorgantini.blogspot.com/2013/08/micro-services-introduction.html http://davidmorgantini.blogspot.co.uk/2013/08/micro-services-what-are-micro-services.html http://davidmorgantini.blogspot.com/2013/08/micro-services-when-should-you-use.html http://davidmorgantini.blogspot.com/2013/08/micro-services-why-shouldnt-you-use.html and http://davidmorgantini.blogspot.com/2014/03/microservices-effective-testing.html Not bad but mostly obvious when you‚Äôve read Fowler, more why than how. http://klangism.tumblr.com/post/80087171446/microservices Good definition and expectations for microservice. http://dejanglozic.com/2014/04/07/micro-services-fad-fud-fo-fum/ Funny story comparing microservices to agile. TL;DR let‚Äôs kill microservices movement hoping that it will continue to practised but it does not become enterprise agile after X years. http://plainoldobjects.com/2014/03/25/thoughts-about-microservices-less-micro-more-service/ Services should be made as small as possible, but no smaller. http://www.slideshare.net/jeppec/soa-and-event-driven-architecture-soa-20 Truly a masterpiece. A lot about copuling of SOA and what architectures failed in past. Events, asynchronicity, autonomy. Composite UI (also in form of public API).\nRelated and recommended video http://www.tigerteam.dk/talks/IDDD-What-SOA-do-you-have-talk/ Most importantly this presentation shows difference between layered architecture in SOA form and proper SOA (microservices). Also related: http://www.tigerteam.dk/2014/micro-services-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-2/ Contains answer to important question How do we split our data / services and identify them? http://www.tigerteam.dk/2014/microservices-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-3/ On services communication. http://www.tigerteam.dk/2014/microservices-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-4/ Technique to decouple from monolith. Data duplication for events. Boundaries. Saga (workflow). Remarks on eventual consistency. Good exemplary system to be implemented using microservives. http://www.tigerteam.dk/2014/soa-synchronous-communication-data-ownership-and-coupling/ On SOA design principles, interesting. http://gawainhammond.blogspot.co.uk/2014/03/microservices-and-soa.html Microservices as a way to experiment and innovate, pre/lean-SOA,  guerrilla marketing tactic, micro meaning also micro effort to be up and running. http://rrees.me/2014/03/24/the-state-of-microservices/ Mostly general opinions on microservices http://byterot.blogspot.com/2014/04/reactive-cloud-actors-no-nonsense-microservice-beehive-restful-evolvable-web-events-orleans-framework.html Focused on actors but not far away from microservices, remarks on reactive vs. imperative. Also on coupling and presents good example of it. Contains Reactive Cloud Actors proposal. I think it violates many microservices principles however presents good arguments on events (less coupling) and provides good code samples to reason about. http://www.slideshare.net/michaelneale/microservices-and-functional-programming If you remove FP nothing really about microservices left. So-so coupling definition (RMI). http://qconlondon.com/dl/qcon-london-2014/slides/AdrianCockcroft_MigratingToMicroservices.pdf Immutable code with instant rollback. De-normalized single function NoSQL data stores. Inverse Conway‚Äôs Law ‚Äì teams own service groups. One ‚Äúverb‚Äù per single function micro-service. Size doesn‚Äôt matter. One developer independently produces a micro-service. Each micro-service is it‚Äôs own build, avoids trunk conflicts. Stateless business logic, stateful cached data access layer. Reactive model RxJava using Observable to hide threading. Even if you start with a protocol, a client side driver is the end-state. Best strategy is to own your own client libraries from the start. Leave multiple old microservice versions running. Fast introduction vs. slow retirement asymmetry. Zookeeper or eureka for service discovery. RPC/Rest as API patterns. Microservice lifecycle - mature slow changing, new fast changing, number increase over time, services increase in size then split and\nadd a new microservice, no impact, route test traffic to it version aware routing, eventual retirement. http://qconlondon.com/dl/qcon-london-2014/slides/BrianDegenhardt_RealTimeSystemsAtTwitter.pdf Bashing on monorail, how they split it [no details on process]\ndetails on tools used in twitter: twitter-server, finagle, zipkin. http://www.slideshare.net/mobile/pcalcado/from-a-monolithic-ruby-on-rails-app-to-the-jvm Microservices as a way to reduce the risk of trying thins\nchoose jvm [jruby, scala, clojure]. Composite UI (api) and services with own storage. Apparently they have custom tool to bring services up/down. http://blog.josephwilk.net/clojure/building-clojure-services-at-scale.html They use netflix/twitter tools on jvm, also rxjava. Use circut breakers. Apparently REST/RPC. Very clojure and tools-used specific. http://tx.pignata.com/2013/07/goruco-service-oriented-design.html Briefly on events, Kafka. Prevent cascading failure with circuit breaker, plan for failure. Background worker is usually the most obvious service to extract. http://pjagielski.pl/2014/02/24/microservices-jvm-clojure/ A bit of clojure, REST and misconceptions about must-haves. http://nerds.airbnb.com/smartstack-service-discovery-cloud/ They wrote theit own service discovery and friends, again REST/RPC and autogenerating Haproxy configs to speak with other services. http://techblog.netflix.com/2013/01/announcing-ribbon-tying-netflix-mid.html Communication through REST, Eureka for service discovery. This is complicated. http://monkey.org/~marius/funsrv.pdf Futrues (for asynchronous operations). Services (boundaries). Filters (authentication, timeouts, retries). http://wayfinder.co/pathways/53536427f7040a11002ae407 Just a link aggregator like this blog post, mostly duplicates but patient reader may find article not mentioned here. http://literateprogrammer.blogspot.co.uk/2014/03/the-microservice-declaration-of.html Trying to describe what SOA is and on this ambiguity, mostly missing the point. http://redmonk.com/sogrady/2014/03/27/micro-services Giveaway from Amazon‚Äôs story: http://www.slideshare.net/chris.e.richardson/microservices-decomposing-applications-for-deployability-and-scalability-jax Wise words sad earlier in slightly more complicated language. http://www.infoq.com/articles/russ-miles-antifragility-microservices Didn‚Äôt provide much value having consummed previous articles. http://www.infoq.com/articles/microservices-intro Didn‚Äôt provide much value having consummed previous articles. http://www.infoq.com/news/2014/05/microservices Didn‚Äôt provide much value having consummed previous articles. http://developers.soundcloud.com/blog/building-products-at-soundcloud-part-1-dealing-with-the-monolith Bounded context as boundary. Wow microservices communicate with monolith [however they still ask monolith for data, which is strange].\nPresumably Event Sourcing. http://blog.carbonfive.com/2014/05/29/an-incremental-migration-from-rails-monolithic-to-microservices API contracts, IDL, RPC. Not my microservices world. Interesting only if you‚Äôre into RPC and contracts. http://gotocon.com/video#47 Related to http://www.slideshare.net/kikibobo/fast-but-not-loose-typesafe-clientserver-architecture-at-gilt-groupe . E-commerce, used Rails + PostgreSQL previously, migrated to JVM. Unfortunately lots of irrelevant sidenotes in 52min talk. Futures, futures everywhere (apparently solves reliability, at least Akka Futures). Apparently latency is not a problem, bandwidth as well, though they made calling services so easy that people overused and saturated services/network. Type system apparently helps to detect failures. Over 300 services, event sourcing, lambda architecture, CQRS. http://yow.eventer.com/yow-2013-1080/practical-considerations-for-microservice-architectures-by-sam-newman-1389 Related to http://www.slideshare.net/spnewman/practical-microservices-yow-2013 . Practical - starts good\nwhy microservices or ‚Äúfine-grained‚Äù architectures. In reality many more options to scale in ‚Äúfine-grained‚Äù. Need to standardise for own sanity. Standardise in the gaps between services - be flexible about what happens inside the boxes. Avoid RPC-mechanisms or shared serialisation protocols to avoid coupling (REST is OK). Have one, two or maybe three ways of integrating, not 20. Pick some sensible conventions, and stick with them (i.e. how to return errors from API). Avoid distributed transactions if at all possible. You have to get much better at monitoring. Capture metrics, and logs, for each node, and aggregate them to get a rolled up picture. Use synthetic transactions to test production systems (something like continous live test - undestructive operation). Abstract out underlying platform differences to provide a uniform deployment mechanism. Have a single way of deploying services in any given environment (also applies to development machines). Consumer Driven Tests to catch breaking changes. Don‚Äôt let changes build up - release as soon as you can, and preferably one at a time. Use timeouts, circuit breakers and bulk-heads to avoid cascading failure. Consider Service Templates to make it easy to do the right thing. Good stuff, overall. https://www.youtube.com/watch?v=2rKEveL55TY Apache Kafka, ZeroMQ. Publish to rapids, subscribe to rivers. Asynchronous services. Channel interfaces - email, counter, website, all of the can pump events into bus. Services have own persistance.\nEvents, not entities. Push to client, so that it does not wait. Circuit-breaker with defaults. Service as a class in OOP. http://new.livestream.com/websummit/DeveloperStage2013/videos/33533032 Basically Fred George + Immutable Servers + no async Pub/Sub by default. Intelligent routing as pattern matching [so that service can make synchronous call without explicit receiver] - somehow I dislike this idea [and it wasn‚Äôt implemented]. Contacts in json schema, used by monitoring to match flowing requests. so that service can make synchronous call without explicit receiver . Code small enough so that you can remove it. https://www.youtube.com/watch?v=6mesJxUVZyI They use REST, services run on JVM, no big details. Fake service dependencies in development. Mostly uninteresting, trivial stuff. Mentioned http://square.github.io/cubism/ . ACLs for service API endpoints, enforced by certificates wiht certain OUs. https://www.youtube.com/watch?v=A9rwSDMp-ls Components scale individually. Break down knowledge of the huge monolithic system (scale organization), Rails as composite UI, however sometimes UI talks straight to service. Memcache becomes database. Troubles with moving from monolith‚Äôs AR to service. Migrating to services with ‚Äúdouble dispatching‚Äù - service is redundant at the beginning to monolith before completely migrating to it, allows profiling, leaves backup plan. Custom tools for deployment. Standardized tools, no snowflakes - they use dropwizard. 10-week teams, then they rotate. Good, general presentation. https://skillsmatter.com/skillscasts/5233-separating-fact-from-fiction-what-do-microservices-really-look-like Optimize for change. SOA can evolve to microservices. Small, the more chance you can accept change. Single purpose. May be stateful. Polyglot (and the weirdness of recruiting for X tech developer). Flat services. Pipelines interaction. Scaling - lots of options. Antifragility. Favors HTTP as protocol. No free lunch yet . Docus on data flow rather than strcuture (pipelines). Microservices don‚Äôt need manifesto, they‚Äôre just enough to build systems that change and adapt\nallow yourself to experiment and try. http://www.infoq.com/presentations/Micro-Services http://vimeo.com/74589816 http://vimeo.com/74452550 https://www.youtube.com/watch?v=2ofzdPXeQ6o#t=2739 https://www.youtube.com/watch?v=PTZAmViaXKQ http://vimeo.com/43690647 https://www.youtube.com/watch?v=wyWI3gLpB8o http://plainoldobjects.com/presentations/decomposing-applications-for-deployability-and-scalability/ http://microsvcs.io/ https://www.youtube.com/watch?v=ZFBvvUlqQ6w That‚Äôs a lot of reading and watching but not all for sure. I‚Äôll be glad to receive recommendations on publications you find influential. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-05"},
{"website": "Arkency", "title": "4 ways to early return from a rails controller", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/07/4-ways-to-early-return-from-a-rails-controller/", "abstract": "When refactoring rails controllers you can stumble upon one gottcha. It‚Äôs hard to\neasily extract code into methods when it escapes flow from the controller method (usually\nafter redirecting and sometimes after rendering). Here is an example: So that was our first classic redirect_to and return way. Let‚Äôs not think for a moment what we are going to do later with this code, whether some of it should landed\nin models or services. Let‚Äôs just tackle the problem of extracting it into a controller method. The problem with this technique is that after extracting the code into method you\nalso need to fix all the returns so that they end with return true (instead of just return ).\nIf you forget about it you are going to introduce a new bug. The other thing is that verify_order and return does not feel natural. When this method\nreturns true I would rather expect the order to be positively verified so escaping early\nfrom controller action does not seem to make sense here. So here is the alternative variant of it Now it sounds better verify_order or return . Either the order is verified or we return early.\nIf you decide to go with this type of refactoring you must remember to add return true at the\nend of the extracted method. However the good side is that all your redirect_to and return lines\ncan remain unchanged. If we wanna return early from the top level method, why not be explicit about what we\ntry to achieve. You can do that in Ruby if your callback block contains return . That\nway inner function can call the block and actually escape the outer function. But when you look at verify_order method in isolation you won‚Äôt know that this yield is\nactually stopping the flow in verify_order as well. Next lines are not reached. I don‚Äôt\nlike when you need to look at outer function to understand the behavior of inner\nfunction. That‚Äôs completely contrary to what we usually try to achieve in programming\nby splliting code into methods that can be understood on their own and provide us with\nless cognitive burden. With ActionController::Metal#performed? you can test whether render or redirect already happended. This seems to be a good solution for cases when you extract code into method\nsolely responsible for breaking the flow after render or redirect. I like it because in such case as shown, I don‚Äôt need to tweak the\nextracted method at all. The code can remain as it was and we don‚Äôt care about returned values from the subroutine. In sinatra you could use throw :halt for that purpose ( don‚Äôt confuse throw (flow-control) with raise (exceptions) ). There was a discussion about having such construction in Rails a few years ago happening automagically for rendering and redirecting but the discussion is inconclusive and looks like it was not implemented\nin the end in rails. It might be interesting for you to know that expecting render and redirect to break the flow of the method and exit it immediately\nis one of the most common mistake experienced by some Rails developers at the beginning of their career. As Avdi wrote and his blogpost Rack is also internally using throw :halt . However I am not sure if using this directly from Rails, deep, deep in\nyour own controller code is approved and tesed. Write me a comment if you ever used it and it works correctly. Because in the end you probably want to put this code into service anyway and separate checking\npre-conditions from http concerns. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-15"},
{"website": "Arkency", "title": "Code style matters", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/07/code-style-matters/", "abstract": "Have you ever wondered how your code looks? Exactly - no what it does or even how it is organized , but actually how it looks . One may think here ‚Äúoh screw it‚Äù , but I want to present that it can matter and show you my various thoughts about that topic. Recently Andrzej Krzywda raised a sensitive issue about code refactoring in Rails Refactoring Book . It provides a brief understanding about existing pains, possible solutions and gives us an opportunity to discuss it more detailed here. This article is intended to be a supplement for that book to show that not only architecture is important. In the beginning I‚Äôll start with some examples in Ruby, that may lead to inconsistency in one project. That‚Äôs not a comprehensive example but an overview to introduce this problem. If you decide to use new ruby (>= 1.9) , use also a new hash syntax, or if you are so used to the old one, keep it everywhere in your project, independent of your current mood. Why to use double quotes when not interpolating anything? Here ternary operator is less verbose than if , then , else , end , which are used rather in complex (multiline) cases. They can be really tricky so it is not recommended . Much nicer and no ' everywhere. I chose these, because I see them the most often. I hope they show that problem exists. Why I‚Äôm pointing on that? Because I‚Äôm working in many projects with many devs at one time. I‚Äôm using up to 6 different languages every day so sometimes it‚Äôs kinda overwhelming. To easily dive into one project from another, switch contexts, jump between environments and work with other people, some guidelines must be respected. Moreover I‚Äôm developing in JetBrains tools (RubyMine, IntelliJ, AppCode, AndroidStudio), which have really nice syntax check inspired by official guidelines like e.g. ‚Äú Ruby Style Guide ‚Äù and they help me to keep my code clean and coherent across files. In my opinion, the most important thing about our code is readability . Why? Because we don‚Äôt usually have time to read and parse others code. We need to use it. If we are using the same style, we can just take a look and understand everything. It‚Äôs much tougher to understand rather than Isn‚Äôt it? So the in the first example something will be executed if condition is not true so if it false , yep? Even if parsing takes only a couple milliseconds, when we have a lot of places like that, it may cause wasting more time to refactor code inside our minds. The other important thing is communication between developers which is done mostly through our code. When we don‚Äôt understand what they did and have to reinterpret their code, it means that communication fails. When everyone writes code that have the same look, it‚Äôs super-easy to make our work faster.\nHow may times did you have to rewrite a part of code that someone wrote and now you have to implement a new feature? How many times did you complain on others work, because you would do it better? The main problem is that the taste is sooo subjective . There may be pedants, some that like ‚Äúartistic mess‚Äù or people that don‚Äôt care at all. It might be hard to have each of them in one project working on the same code. Tastes differ . That‚Äôs why some writes key: value and some key : value . Some leaves empty lines (one or more) between methods, some don‚Äôt do that at all. Some take care of architecture and code separation, but don‚Äôt take care of their syntax. Small things, but can be annoying for those that pay attention to such issues. Of course there are developers which deal with legacy code very well. They easily understand the most tenebrous code, they have huge experience and great skills to interpret any part of existing software. If you are one of them, you may see this blogpost useless, but beware ‚Äî not everyone is like you. Some learn slower and bad code may discouraged them permanently from the very beginning. We cannot have a silver bullet here. Unfortunately code style is really personal and usually hard to change . It‚Äôs like old behavior or routine repeated all the time so do not expect immediate switch just like that. So where to begin? A very good start can be guidelines or best practices defined by community or language authors. That may be easy to begin with, learn and improve your code style . There are tons of them for nearly every language. Sometimes even companies define their own guidelines to make it easier and keep concise code across many projects. How to use them? It might be hard just to remember and use this new code style or guidelines so let‚Äôs configure your favorite IDE , find suitable package for Sublime, bundle for TextMate or plugin for VIM that will let you auto-reformat your code. They are usually called YOUR_LANGUAGE-[prettifier | beautifier | formatter] and are available for probably every tool you use to write the code. Some examples of these guidelines: If you think that it‚Äôs important topic in your daily work and you are willing to improve your code style I‚Äôd recommend you to start from some useful resources guiding you by small steps that will make your code better. Start with small steps , not with everything together. Make a little changes continuously introducing more and more new elements. You‚Äôre probably using a few languages at one time so pick one you want to improve and focus on it to avoid too much changes together and decrease new things to remember. Finally, if you want to know our opinion, take the most from Rails Refactoring book. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-14"},
{"website": "Arkency", "title": "Why we use React.js in our Rails projects", "author": ["Wiktor Mociun"], "link": "https://blog.arkency.com/2014/07/why-we-use-react-js-in-our-rails-projects/", "abstract": "Source: Subramanya Prasad Programming interactive user interfaces using JavaScript might be a tough task. User calls some action and we need to update his view - manipulating DOM using tool like jQuery. It just doesn‚Äôt feel good enough to modify document‚Äôs elements by hand. It is also really hard to re-use already written parts of UI and compose them with each other. Imagine if there was one tool, that would solve all these problems and help you deliver your front-end much faster . What if you didn‚Äôt have to worry about this whole DOM stuff when you just want to update the view? How would you feel being as efficient as when developing your Rails backend, when it comes to user interface? Here comes React.js! Recently in Arkency, we were experimenting with Facebook‚Äôs library called React.js. Its only responsibility is to build composable and reactive user interfaces. The power of this tool comes from its simplicity. Learning curve of React is very low, so any new developer comming to project based on React, had no problems with jumping in. In projects, where we adopted React, we noticed good things happening.\nThe first and the most important, We shipped our front-end significantly faster . We could write really complex UI parts and easily compose with each other.\nSecond, as our apps grew, we improved our code maintainability. Spending less time on maintaining code means more time spent on delivering business value for our customers. React objects are called components . Each of them may contain data and renders view in a declarative way - based only on current data state. Each React component has 2 inputs: After changing the state, React will automatically re-render the component to answer a new input. In addition, all React components must implement render method, which must return another React object or null (from version 0.11). Assume that we got to create a list of books with a dynamic search. First, we should create a simple book component that represent single book on a list. To build HTML structure I used built-in React.DOM components, which corresponds to standard HTML elements. In first argument, I pass the empty props object , the second one is just the content of my <li> tag. Moving on to the full list of Book items Ok, we are able to display list of books. Now it is high time to implement search. Let‚Äôs modify our BooksList component. We need to add form input and handle its changes. Summing it up, you can see the result in a frame below That‚Äôs all you need. After you type something into search input, React will automatically re-render the book list to contain only filtered items. Compared to another solutions, you won‚Äôt spend much time learning React. You should really give it a shot in your project. If you look for more information on React, check out official docs and sign-up for our newsletter below. We are going to write more about React.js Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-17"},
{"website": "Arkency", "title": "6 front-end techniques for Rails developers. Part I: From big ball of mud to separated concerns", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2014/07/6-front-end-techniques-for-rails-developers-part-i-from-big-ball-of-mud-to-separated-concerns/", "abstract": "Photo remix available thanks to the courtesy of Clever Cupcakes . CC BY 2.0 Current trends in web development forces us to write more and more front-end code in our applications. Single Page Applications, content changes without refreshing, rich user experience using custom controls - all those requirements needs you to write code. And this code, as any other code, can quickly turn into a big ball of mud . It can happen when you lack proper tools in your toolbox to design it correctly. In this course I want to share with you techniques and tools we‚Äôre using in our day-to-day work. Some of those allow you to create easier, more testable code . Some of those allow you to develop it faster, without losing quality. I believe it‚Äôs really worth to try these techniques and choose which suits you most. In the part one, I want to present you a simple case of refactoring of a badly written front-end code to a stand-alone micro-app with proper encapsulation. It‚Äôs quite common to see CoffeeScript code which is an imperative chain of DOM transformations and event handlers mixed with AJAX calls . It is a complete disaster when it comes to maintaining and adding new features to it. In addition, all responsibilities in such spaghetti code are entangled. Luckily it‚Äôs quite easy to segregate dependencies and create an Application class, which responsibility is to create and configure all objects you‚Äôve separated during refactoring. Let‚Äôs see an example code that you may find in (bad written) front-end codebase. It‚Äôs responsible for loading photos data (via an AJAX call) and displaying it on the screen. After clicking on a photo it should be grayed out: There are several problems with this code: Fortunately, you can easily refactor this code. As I‚Äôve mentioned before, this code has several responsibilities : Your first step should be to create classes with its responsibilities . In our projects it‚Äôs quite usual that we have Gui class (often it is composed of a few smaller classes), Backend class (which fetches data from Rails backend and pre-processes them) and UseCase class (which contains business logic within, operating on domain objects). Since this example does not contain much business logic at all, you can stick with only Backend and Gui classes. Since there is a business rule that is worth to be contained in an intention revealing interface, it‚Äôs a good decision to create a Photo domain object. When I work in a Sprockets-based stack I usually create a module definition within application.js to make my new classes accessible globally and namespaced. It‚Äôs quite simple - you can put Photos = {} in the body of your application.js file. Then you can require your new classes. They‚Äôll be available in a web inspector and in a code in a Photos namespace. There is a rule of thumb to always start with domain (or use case) . In our case it‚Äôs a tiny part of code that encapsulates grayscale photo URL transformation logic: You can easily transform existing code to accommodate this change. That means you take this as a series of small steps - feel free to stop this refactoring now and jump into next task. Let‚Äôs proceed with further decomposition of this code. Right now your can create our Backend class to accommodate AJAX fetching behavior. I mostly extracted existing implementation here to a method . Here is how I could create such a class: I‚Äôve removed onSuccess and onFailure callbacks here and replaced it with a Promise object . That allows me to expose ‚Äòstatus‚Äô of AJAX call to anyone interested in a result - exactly what I want if I want to pass control to another object. I‚Äôve also used a neat trick with #then - data for a caller of this method will come encapsulated in your new Photos.Photo object, not raw JSON data. You can argue that responsibility of backend is not to encapsulate JSON in a domain object. For me Backend is for ‚Äòseparating‚Äô world from the heart of your application - which should operate only on a domain objects. In a puristic implementation of a backend, you should create an object which is reponsible for mapping from JSON to domain object - and transform raw JSON data returned by a backend using this object as an intermediate step. The last step is to create a Gui class, which is responsible for rendering and binding events to the DOM objects. There are different approaches here - in Arkency we‚Äôre using Handlebars for templating or React.js for creating the whole Gui part. You can use whatever technology you want - but be careful to not extend responsibilities . The rules of thumb are: There is an example implementation that I‚Äôve written: That‚Äôs it. These components contain all the logic we‚Äôve implemented in the previous code. Now we need to coordinate those classes to make a real work. Classes that you‚Äôve created cannot do their work alone - they need some kind of coordination between each other. On backend, coordination like that is contained within a service object. If you don‚Äôt have service objects, you usually put this responsibility in a controller, which can be done better . That‚Äôs why you should create an Application class to initialize and coordinate all newly created objects . It‚Äôs a really simple code. When you perform this refactoring step-by-step, you‚Äôll notice that the end effect of your changes is quite similar to this code. The biggest difference is that you generally want to separate definition of your classes from a real work. This is what such application object could look like: This makes the complete, stand-alone app. You‚Äôll notice that you do not run this code yet. That‚Äôs because it‚Äôs advisable to separate initialization of our app from its definition . Creating such initializer is easy: You can see the end result here . Creating a stand-alone application is a first step to create robust and rich front-end code . Testing it is way easier since responsibilities are segregated and maintainability of this code is increased - when you want to make changes in backend fetching rules you need to focus only on a backend class. It‚Äôs only a starting point of course. But it‚Äôs a good start for further improvements . This post is a part of the 6-day course about front-end techniques for Rails developers. It‚Äôs absolutely free - just register to our newsletter (using a box below) and we‚Äôll teach you 6 techniques we‚Äôre using in a day-to-day work, including: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-18"},
{"website": "Arkency", "title": "Hidden features of Ruby you may not know about", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/07/hidden-features-of-ruby-you-may-dont-know-about/", "abstract": "How well do you know the language you‚Äôre using? What if I tell you that even you use it the whole day and every day it still can hide some tricks that you might not be aware of? I‚Äôd like to reveal some less or more known, but certainly interesting parts of Ruby to make you sure that you don‚Äôt miss anything. Disclaimer Every example was run with: and nonzero? returns self unless number is zero, nil otherwise. Thanks for Thiago A. Note that: In the other case: Of course you can use other non-alpha-numeric character delimiters: Source and examples however Here is nice trick to avoid tedious { |x| do_something_with(x) } . This is a different case from symbol-to-proc, because we don‚Äôt invoke method on x but call a method that takes x . And many more . Did you know that placing a semicolon at the very end of line hides its output? It may be helpful when some expression produces a lot of data, which we want to just assign to variable instead of directly printing in output. Inspired by 1jgjgjg In how many ways can we call a proc? Actually in a few: Thanks for Jordan Running Impressed? If no, that‚Äôs great! It means you are a trouper. Otherwise, it‚Äôs good too, because you learned something new today and I hope you find this useful. If you have your favourite tricks, you can share them in the comments below. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-24"},
{"website": "Arkency", "title": "Ruby background processes with upstart user jobs", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/07/ruby-background-processes-with-upstart-user-jobs/", "abstract": "Recently, my colleague at Arkency Pawe≈Ç Pacana wanted to manage application process with upstart. He started with the previous article about upstart and finished with robust deployment configuration with reliable setup using‚Ä¶ runit. He summarised upstart briefly: ‚Äúso sudo‚Äù so I decided to extend my latest blogpost with some information about upstart user jobs . Although I am glad that my article was inspiring, it turned out not to be comprehensive enough. I decided to extend it, so that anyone can use upstart in every environment. Last time we managed to run our job in a way that the deployer required sudo privileges to manage the application. However the user should be able to do all that without the root permissions. The whole reason for having the deployer user is to manage his own application without any additional requirements. In regular way upstart keeps all of the .conf files in /etc/init/ . We need to change it now to user own (home) directory, We have to modify upstart configuration to be able to run user jobs. Open it with your favourite text editor: /etc/dbus-1/system.d/Upstart.conf . To support fully functionality it should look like: Once you‚Äôve modified your upstart job you need to restart dbus the last time using sudo privileges: When we move my_program.conf into ~/.init , upstart will no longer log its output, so you won‚Äôt be able to see any errors, we need to modify my_program.conf now. So there are a few changes we need to add to get my_program.conf working right: Remember to update your $PATH from my_program.conf , forward output to .log file and set user name before process run. Note If you have user belonging to some group, you‚Äôll have to define this group in my_program.conf too as setgid GROUP_NAME . See more about that: Now you will be able to start my_program without appending sudo anymore. Reference Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-07-31"},
{"website": "Arkency", "title": "Using ruby Range with custom classes", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/08/using-ruby-range-with-custom-classes/", "abstract": "I am a huge fan of Ruby classes, their API and overall design . It‚Äôs still sometimes that\nsomething surprises me a little bit. I raise my eyebrow and need to find answers.\nWhat surprised me this time was Range class. But let‚Äôs start from the beginning (even though it is\na long digression from the main topic). Every time I implement any kind of reporting functionality for our clients I wonder why\nis there no Month class. I mean, there is such concept as month. Why not make it a\nclass? I wondered how other languages deal with it and it turns out Java recently added Month class to its API.\nI looked at its implementation, its methods and no‚Ä¶ That‚Äôs not what I want. To add more to the confusion I realized that there are two concepts here So to avoid confusion I decided to think about my little object that I have in mind ( January 2014 ) as YearMonth . If\nyou come up with a better name for it, leave me a comment . I honestly couldn‚Äôt come up with anything different and more\nsophisticated. Maybe because English as second language ‚Ä¶ Anyway‚Ä¶ I the domain of Reporting we often think in terms of Time periods. Our customers often would like to have reporting per days, weeks, months, quarters etc. When someone tells me to create a report from January 2014 to May 2014 with the accuracy of month, well‚Ä¶ I would like to say in my code YearMonth.new(2014, 1)..YearMonth.new(2014, 5) . That‚Äôs how my OOP part of the brain thinks about the problem. What are the clues telling us that despite having the variety of classes for operating on time\n(like Date , DateTime , Time and even ActiveSupport::TimeWithZone ) we still need more classes? I don‚Äôt know this\nwill convince you but here are my thoughts: vs Same goes for other: vs vs Here is the pattern that I see. Whenever we want to do something related to a period of time such as Year , Quarter , Month , Week we create an instance of moment ( Time , Date ) in time that\nhappens to belong to this period (such as first day or first second of year). Then we use this object to query it\nabout the attributes of the time period it belongs with methods such as #beginning_of_year , #beginning_of_quarter , #beginning_of_month , #beginning_of_week . So I think we are often missing the abstraction of time periods that we think about and that we work with. I understand\nthat these methods are very useful when what we are doing depends on current time or current day or selected moment provided\nby the user. However in my case, when the users gives me an integer representing Year ( 2014 ) I would really like to\ncreate an instance of Year and operate on it. Operating on bunch of static methods or creating\na Date ( January 1st, 2014 ) to deal with Years does not taste me . What does my boss say? üòâ He says that knowing about things such as next and previous month is not the responsibility\nof YearMonth class but rather something above (conceptually higher) like a Calendar . It‚Äôs not that May 2014 knows\nthat the next month in a year is June 2014 but rather the calendar knows about it. I find it an interesting point\nof view. What do you think? Ok, enough with the digressions. The main topic was using custom class with Range . Let‚Äôs have an exemplary class. This was used as a Value Object attribute in my AR class: And it was all supposed to work but‚Ä¶ That certainly wasn‚Äôt something that I was expecting. Let‚Äôs think a moment about it. What do we actually use the Range class for? There are at least two usecases: For both of the usecases we need to add different methods to our custom ( YearMonth ) class for it to be\ncompatible with Range . Iterating requires you to implement #succ method. That‚Äôs how our Range knows how to yield next element from the range collection. But how does it know when to stop yielding next elements? When it creates the instance of YearMonth.new(2014, 3) as a third element that is yielded how does it know that it is the last one? Well that‚Äôs when the next usecase comes handy. Checking the inclusion of values in Range require you to implement the <=> operator . In other\nwords your class should be Comparable . And that‚Äôs the thing I forgot about. And it actually makes sense because how\nelse would the Range know when to stop without the ability to compare last generated element with the upper bound of\nyour Range? If you are not familiar with <=> operator here is a little reminder for you. It should return -1 , 0 or 1 depending on whether the compared objects is greater, equal to, or lower: If you have <=> operator implemented and include Comparable module into your class you get the behavior\nof classic operators < , <= , == , >= and > for free: The Range documentation explains it nicely: Ranges can be constructed using any objects that can be compared using the <=> operator. Methods that treat the\nrange as a sequence ( #each and methods inherited from Enumerable ) expect the begin object to implement a succ method\nto return the next object in sequence. The step and include? methods require the begin object to implement succ or to be numeric. Somehow I expected that is the #succ methods that is most important for the Range to exist and work correctly.\nProbably because I was so focused on the fact that ranges can iterate over elements. It is however that the <=> method in your own class is the most important factor. That‚Äôs because you can check whether\nelement is part of range without the ability to iterate over subsequent elements. But you can‚Äôt generate subsequent\nelements without knowing which one is the last one (or whether you should start iterating at all). All this can be summarized in a few examples: So Range will give always you the ability to check if something is in the range, but it only might give you the\nability to iterate. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-08-03"},
{"website": "Arkency", "title": "RubyMotion app with Facebook SDK", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/08/rubymotion-app-with-facebook-sdk/", "abstract": "This will be short, simple, but painless and useful. We‚Äôll show you how to integrate Facebook SDK with RubyMotion application. Recently  we encouraged you to start using RubyMotion and we presented some useful gems to start developing with. Now, we‚Äôd like to show you how to integrate Facebook iOS SDK with RubyMotion and create sample application from scratch. Firstly, we have to generate RubyMotion application. We will use awesome RMQ gem for building initial skeleton. Our application is up and running. Now it‚Äôs time to include FB pod in our project. Pods are dependencies, something like gems, for iOS and are compatible with RubyMotion too. In our Gemfile we need to uncomment or add the following line: Then, in Rakefile inside Motion::Project::App.setup block we should add: After all that let‚Äôs install all dependencies: That installs Facebook SDK for iOS in our RubyMotion project. We can now build all logic as we want. Let‚Äôs build some kind of login feature. The use case may be as follows: In order to use FB application, we should create it on Facebook developers portal first. However, if you don‚Äôt want to follow simple tutorial how to do that, you still can use sample FB app ID provided by Facebook itself 211631258997995 . To be able to be redirected back to our application from Safari, we should register appropriate URL Scheme for URL types in Info.plist , which stores meta information in each iOS app. Just below app.pods in Rakefile add: What is more, we have to register our Facebook app ID too: Now is the time to build login screen with big blue button. In app/controllers/main_controller.rb in vievDidLoad method add the following line: It tells RMQ to add Facebook login button instance as a subview and apply fb_login_button style to it. What is more, it registers itself as a delegate to handle all login methods. We have to create our style yet. For that open app/stylesheets/main_stylesheet.rb and add the following code: That will center FB button. AppDelegate class is entry point to every iOS application. It should manage login state so we need to configure it as follows: Now, run application with rake . You should be able to see login or logout button accordingly to your current state. We have to handle login state now. On the very beginning we can just set navbar title for our application to be changed when user logs in and out. Let‚Äôs do it in MainController class: Let‚Äôs rake and play with that. We can display user info too. Here‚Äôs how it works: And some styling for that: And that‚Äôs it. I‚Äôm happy that you went through this article. In case you need ready code, I created repository with example application . Enjoy! For now, stay tuned for more mobile blogposts! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-08-09"},
{"website": "Arkency", "title": "SSH authentication in 4 flavors", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/08/4-ssh-authentication-in-4-flavors/", "abstract": "We are connecting with remote servers every day. Sometimes we explicitly provide passwords, sometimes it just happens without it. A lot of developers don‚Äôt care how it works internally, they just get access, so why to bother at all. There are a couple ways of authentication, which are worth to know and I‚Äôd like to present you them briefly. Each authentication method requires some setup on the very beginning. Once it‚Äôs done, we can forget about it and connect without any further configuration. However there are different ways to configure authentication on your server with different secure level and initial setup process. Let‚Äôs review the most common. The SSH authentication protocol is a general-purpose user authentication protocol.  It is intended to be run over the SSH transport layer protocol. This protocol assumes that the underlying protocols provide integrity and confidentiality protection. From: http://tools.ietf.org/html/rfc4252 Pros: Cons: Prerequisites are that user creates a pair of public and private keys . Private keys are often stored in an encrypted form at the client host, and the user must supply a passphrase before the signature can be generated. Even if they are not, the signing operation involves some expensive computation. From: http://tools.ietf.org/html/rfc4252#page-9 Then, public key is added to $HOME/.ssh/authorized_keys on a server. That may be done via ssh-copy-id . You can read nice tutorial describing it quite well. Connection itself: Pros: Cons: Both of previous methods was equally cumbersome because of necessity to enter password or passphrase each time we want co connect. This may be tedious when we communicate often with our remote servers. Key agent provided by SSH suite comes with help, because it can hold private keys for us , and responds to request from remote systems. Once unlocked, it allows to connect without prompting for credentials anymore. Pros: Cons: This last way is the most perfect of all above, because it gets rid of the second disadvantage in almost ideal previous method. Instead of requiring passwords or passphrases on intermediate servers, it forwards request , through chained connections, back to initial key agent . For better understanding and real-life example, let‚Äôs imagine that this second connection may be some kind of scp or sftp transfer. Pros: Cons: In order to connect with SSH server and authenticate using your public/private keypair, you have to first share your public key with the server. As we described before, that can be done using ssh-copy-id or some script Once it‚Äôs done, server can construct some challenge based on your public key. Because RSA algorithm is asymmetric, message encrypted using public key can be decrypted using private key and opposite. Key negotiaton may be as follows: client receives a message encrypted by your public key and can decrypt it using your private key. Next, it encrypts this message with server public key and sends back to server, which uses its own private key to decrypt and validates if message matches this sent one initially. Of course the above flow is only the example of how challenges may works. They are often more complicated and contain some MD5 hashing operations, session IDs and randomization, but the general rule is really similar. RFC offers far more comprehensive explanation of this whole process. What is worth to know, there are to versions (v1 and v2) of SSH standard. According to OpenSSH‚Äôs ssh-agent protocol : Protocol 1 and protocol 2 keys are separated because of the differing cryptographic usage: protocol 1 private RSA keys are used to decrypt challenges that were encrypted with the corresponding public key, whereas protocol 2 RSA private keys are used to sign challenges with a private key for verification with the corresponding public key. It is considered unsound practice to use the same key for signing and encryption. Note that private key belongs only to you and is never shared anywhere. As I described before, the basic benefit of using SSH agents is to protect your private key without need to expose it anywhere. The weakest link is SSH agent itself. Any kind of implementation must provide some way that allows to make request from client, some kind of interface to interact with. It‚Äôs usually done with UNIX socket accessible via file API. Although this socket is heavily protected by the system, nothing can really prevent from accessing it by root . Any key agent set by root has immediately granted necessary permissions so there‚Äôs no method preventing root user from hijacking SSH agent socket. It may not be the best solution to connect with Bar server when Foo cannot be entirely trusted. Now you see how authentication works and what are the ways to set it up. You may choose any configuration based on your needs, it‚Äôs advantages and drawbacks. Let‚Äôs secure your server without any fear now. Hope you find this useful. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-08-10"},
{"website": "Arkency", "title": "Truncating UTF8 Input For Apple Push Notifications (APNS) in Ruby", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/08/apns-and-utf8/", "abstract": "When sending push notifications ( APNS )\nto apple devices such iPhone or iPad there is a constraint that makes implementing it a bit challenging: The maximum size allowed for a notification payload is 256 bytes; Apple Push Notification Service refuses any notification that exceeds this limit This wouldn‚Äôt be a problem itself unless you want to put user input into the notification.\nThis also wouldn‚Äôt be that hard unless the input can be international and contain non-ascii character. Which still would\nnot be so hard, but the payload is in JSON and things get a little more complicated sometimes. Who said that formatting\npush notification is easy? This is simplified version of our payload. The notifications is about someone who started following you on our fancy\nsocial platform that we are writing. The path allows the app to open it on a view related to the user who started\nfollowing. The things that are going to vary are user name ( User X in our example) and user id ( 123 ). So let‚Äôs extract the template of the payload into a method. This will come handy later: Remember when I said that we have 256 bytes? We do, but number of useful bytes for our case is even smaller. Even when we don‚Äôt substitute data into our payload we are out of 73 bytes. That means we have only‚Ä¶ ‚Ä¶ 183 bytes for user input If your payload (required for the app to properly behave when the notification is clicked) is bigger or your\nmessage is longer you are left with even fewer bytes of user input. But wait‚Ä¶ We can‚Äôt truncate user id. If we did we could be misleading about who actually started following\nthe recipient of the notification. So even though its length vary, we can‚Äôt truncate it. We can see that the logic for this is slowly getting more and more complicated. That‚Äôs why for every push notification\nwe have a class that encapsulates the logic of formatting it properly according to APNS rules. Ok, we know how many bytes we have so let‚Äôs truncate our international string. But remember that we are not truncating\nup to N chars, we are truncating up to N bytes! We can\nuse String#byteslice for that. It‚Äôs all nice and handy if we happen to truncate exactly between characters. But sometimes we won‚Äôt: We are left we one proper character and one byte which is ugly. I‚Äôve been looking long time to figure out how to properly fix it and it seems\nthat the right answer is String#scrub . For those of you\nwho are stuck with older ruby version, there is backport of it in form of string-scrub gem . So if you ever need to truncate user provided utf-8 string and support international characters byteslice + scrub will do the job for you: Yay! We used our payload to full extent! I added this line size <= MAX_APS_BYTES or raise InvalidPayloadGenerated.new(\"Payload size was: #{size}\") at the end\njust to make sure that everything is ok with my approach and catch errors early (and implemented tests as well). Lucky me! In my case it turned out my json encoder was using numeric escape characters ,\nso they way I calculated the size of my truncated size was wrong because in JSON it turned out to be bigger: vs So I extracted the code responsible to truncating one string into a class This algorithm basically iterates over every char, checks how many bytes it is going to take in our json payload\nand stops when we don‚Äôt have more space for our text. I am not proud of this code. Do you know a better way of how to\ndo it? What‚Äôs they right way to check how many bytes a char will take if encoded as numeric escape character? I am\nsure there must be an easier way to do it. Warning : It has a bug when maxbytes is not enough for even one character to be left. The logic gets even more complicated if you want to embed in your payload multiple strings.\nGood example can be a notification like ‚ÄòUserX‚Äô & ‚ÄòUserY‚Äô invite you to game ‚ÄòGame‚Äô . We could use ‚Öì of bytes\nfor each substituted string in naive implementation. But I wanted the algorithm to be smart and work well even in\ncase when some names are long and some are short. My algorithm for truncating multiple strings so that they\nall use no more than N bytes looks like this: Be aware that it doesn‚Äôt favor any of the String. If they are all very long, then all of them will\nbe allowed to use same amount of bytes. If any of the strings is short, then the unused bytes are split equally amongst\nthe other strings. Here is an example of class that could be using it Remember that if you are using Urban Airship you should be in total using even less than 256 bytes so they can provide\nyou with tracking ability. Quote from their documentation The maximum message size is 256 bytes. This includes the alert, badge, sound, and any extra key/value pairs in the notification section of the payload. We also recommend leaving as much extra space as possible if you are using our reporting tools, as a portion will be used to help with response tracking if it is available. Unfortunately I couldn‚Äôt find out exactly how many bytes they need for this functionality to work properly. If any of you\nhave the knowledge, please let me know. If your messages are particularly long (at least in some locales) you can spare some bytes by\nstoring the template in the app and sending only the data. Quote from APNS documentation You can display localized alert messages in two ways. The server originating the notification can localize the text; to do this, it must discover the current language preference selected for the device (see ‚ÄúPassing the Provider the Current Language Preference (Remote Notifications)‚Äù). Or the client application can store in its bundle the alert-message strings translated for each localization it supports. The provider specifies the loc-key and loc-args properties in the aps dictionary of the notification payload. When the device receives the notification (assuming the application isn‚Äôt running), it uses these aps-dictionary properties to find and format the string localized for the current language, which it then displays to the user. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-08-13"},
{"website": "Arkency", "title": "How we structure our front-end Rails apps with React.js", "author": ["Wiktor Mociun"], "link": "https://blog.arkency.com/2014/08/how-we-structure-our-front-end-rails-apps-with-react-dot-js/", "abstract": "We‚Äôve tried almost everything for our Rails frontends - typical Rails views, Backbone, Angular and others. What we settled with is React.js. In this post we‚Äôre showing you, how we structure a typical React.js app when it comes to the files structure. Our file structure per a single mini-application: app_init - we got one per each application. We always keep it simple: app - starting point of application. Here we initialize and start every component of application backend - here we fetch and send data to backend. It is also a place, where we create domain objects components - our React.js components we use to render an application. domain - definitions of domain objects used in view. Example: immutable list of single entries (which are domain objects too). glue - hexagonal.js glue Hexagonal.js - implementation of clean hexagonal architecture - https://hexagonaljs.github.io RxJS - we use reactive data streams to communicate between apps - https://github.com/Reactive-Extensions/RxJS Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-08-19"},
{"website": "Arkency", "title": "React.js and Google Charts", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/09/react-dot-js-and-google-charts/", "abstract": "So today I was integrating Google Charts into a frontend app created with react.js .\nAs it always is when you want to integrate a 3rd party solution with react\ncomponents you need a little bit of manual work. But fortunatelly react gives us\nan easy way to combine those two things together. As you can see all you need to do is to hook code responsibile for drawing charts\n(which comes from another library and is not done in react-way) into the proper\nlifecycle methods of the react componenet. In our case it is: One more thing. Make sure you start rendering components only after the javascript for\ngoogle charts have been fully loaded. You can see the effect here: These are the things that I learnt today while integrating our code with Google Charts.\nIn my next blogpost I would like to share how we dealt with a similar problem when using\nTwitter Bloodhound library for autocomplete. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-09-29"},
{"website": "Arkency", "title": "Adapters 101", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/08/ruby-rails-adapters/", "abstract": "Sometimes people get confused as to what is the roles of adapters, how to use them,\nhow to test them and how to configure them. Misunderstanging often comes from lack\nof examples so let‚Äôs see some of them. Our example will be about sending apple push notifications (APNS). Let‚Äôs say in our\nsystem we are sending push notifications with text (alert) only\n(no sound, no badge, etc). Very simple and basic usecase. One more thing that we\nobviously need as well is device token. Let‚Äôs have a simple interface for sending\npush notifications . That‚Äôs the interface that every one of our adapters will have to follow. So let‚Äôs\nwrite our first implementation using the apns gem. Wow, that was simple, wasn‚Äôt it? Ok, what did we achieve? As you can imagine looking at the images, the situation is always the same. We‚Äôve got to parts with\nincompatible interfaces and adapter mediating between them. Part of your app (probably a service) that we call client is relying on some kind of interface for its proper behavior.\nOf course ruby does not have explicit interfaces so what I mean is a\ncompatibility in a duck-typing way . Implicit interface defined by how we\ncall our methods (what parameters they take and what they return). There is\na component, an already existing one ( adaptee ) that can do the job our client wants but does not expose the interface that we would like to use . The mediator between\nthese two is our adapter . The interface can be fulfilled by possibily many adapters. They might be wrapping\nanother API or gem which we don‚Äôt want our app to interact directly with . Let‚Äôs move further with our task. We don‚Äôt wanna be sending any push notifications from our development environment and\nfrom our test environment. What are our options? I don‚Äôt like putting code such as if Rails.env.test? || Rails.env.production? into my codebase. It makes testing\nas well as playing with the application in development mode harder. For such usecases new\nadapter is handy . Now whenever your service objects are taking apns_adapter as dependency you can use this one\ninstead of the real one. I like this more then using doubles and expectations because of its simplicity .\nBut using mocking techniques here would be apropriate as well. In that case\nhowever I would recommend using Verifying doubles from Rspec or to go with bogus . I recommend watching great video about\npossible problems that mocks and doubles introduce from the author of bogus and\nsolutions for them. Integration tests are bogus . Ok, so we have two adapters, how do we provide them to those who need these adapters to work? Well, I‚Äôm gonna show you an example and not talk much about it because it‚Äôs going to be a topic\nof another blogpos. Sending push notification takes some time (just like sending email or communicating with\nany remote service) so quickly we decided to do it asynchronously. And the ApnsJob is going to use our sync adapter. Did you notice that HoneyBadger is not hidden behind adapter? Bad code, bad code‚Ä¶ ;) What do we have now? We separated our interface from the implementations. Of course our interface is\nnot defined (again, Ruby) but we can describe it later using tests. App with the\ninterface it dependend is one component. Every implementation can be a separate\ncomponent. Our goal here was to get closer to Clean Architecture . Use Cases ( Interactors, Service Objects ) are no longer bothered with implementation details. Instead they relay\non the interface and accept any implementation that is consistent with it. The part of application which responsibility is to put everything in motion is called Main by Uncle Bob. We put all the puzzles together by using Injectors and\nRails configuration . They define how to construct the working objects. In reality I no longer use apns gem because of its global configuration. I\nprefer grocer because I can more easily and safely use it to send push\nnotifications to 2 separate mobile apps or even same iOS app but built with\neither production or development APNS certificate. So let‚Äôs say that our project evolved and now we need to be able to send push\nnotifications to 2 separate mobile apps . First we can refactor the interface of\nour adapter to: Then we can change the implementation of our Sync adapter to use grocer gem\ninstead (we need some tweeks to the other implementations as well).\nIn simplest version it can be: However every new grocer instance is using new conncetion to Apple push\nnotifications service. But, the recommended way is to reuse the connection.\nThis can be especially usefull if you are using sidekiq. In such case every\nthread can have its own connection to apple for every app that you need\nto support. This makes sending the notifications very fast. In this implementation we kill the grocer instance when exception happens (might happen\nbecause of problems with delivery, connection that was unused for a long time, etc).\nWe also reraise the exception so that higher layer (probably sidekiq or resque) know\nthat the task failed (and can schedule it again). And our adapter: The process of sharing instances of grocer between threads could be\nprobably simplified with some kind of threadpool library. I already showed you one way of configuring the adapter by using Rails.config . The downside of that is that the instance of adapter is global . Which means you might\nneed to take care of it being thread-safe (if you use threads). And you must\ntake great care of its state. So calling it multiple times between requests is\nok. The alternative is to use proc as factory for creating instances of your adapter. If your adapter itself needs some dependencies consider using factories or injectors\nfor fully building it. From my experience adapters usually can be constructed quite\nsimply. And they are building blocks for other, more complicated structures like\nservice objects. I like to verify the interface of my adapters using shared examples in rspec. Of course this will only give you very basic protection. Another way of testing is to consider one implementation as leading and\ncorrect (in terms of interface, not in terms of behavior) and another\nimplementation as something that must stay identical. This gives you some very basic protection as well. For the rest of the test you must write something specific to the adapter implementation .\nAdapters doing http request can either stub http communication\nwith webmock or vcr . Alternatively, you can just use mocks and expectations to check,\nwhether the gem that you use for communication is being use correctly. However,\nif the logic is not complicated the test are quickly becoming typo test ,\nso they might even not be worth writing. Test specific for one adapter: In many cases I don‚Äôt think you should test Fake adapter because this is what we use for\ntesting. And testing the code intended for testing might be too much. Because we don‚Äôt want our app to be bothered with adapter implementation\n(our clients don‚Äôt care about anything except for the interface) our\nadapters need to throw the same exceptions . Because what exceptions are raised\nis part of the interface. This example does not suite us well to discuss it\nhere because we use our adapters in fire and forget mode. So we will have\nto switch for a moment to something else. Imagine that we are using some kind of geolocation service which based on\nuser provided address (not a specific format, just String from one text input)\ncan tell us the longitude and latitude coordinates of the location. We are in\nthe middle of switching to another provided which seems to provide better data\nfor the places that our customers talk about. Or is simply cheaper. So we have\ntwo adapters. Both of them communicate via HTTP with APIs exposed by our\nproviders. But both of them use separate gems for that. As you can easily imagine\nwhen anything goes wrong, gems are throwing their own custom exceptions. We need\nto catch them and throw exceptions which our clients/services except to catch . This is something people often overlook which in many cases leads to\nleaky abstraction . Your services should only be concerned with exceptions\ndefined by the interface. Although some developers experiment with exposing exceptions that should be caught\nas part of the interface (via methods), I don‚Äôt like this approach: And the service But as I said I don‚Äôt like this approach. The problem is that if you want to\ncommunicate something domain specific via the exception you can‚Äôt relay on 3rd\nparty exceptions . If it was adapter responsibility to provide in exception\ninformation whether service should retry later or give up, then you need custom\nexception to communicate it. There are few problems with adapters. Their interface tends to be\nlowest common denominator between features supported by implementations .\nThat was the reason which sparkled big discussion about queue interface for\nRails which at that time was removed from it. If one technology limits you so\nyou schedule background job only with JSON compatibile attributes you are\nlimited to just that. If another technology let‚Äôs you use Hashes with every\nRuby primitive and yet another would even allow you to pass whatever ruby object\nyou wish then the interface is still whatever JSON allows you to do. No only\nyou won‚Äôt be able to easily pass instance of your custom class as paramter for\nscheduled job. You won‚Äôt even be able to use Date class because there is no\nsuch type in JSON. Lowest Common Denominator‚Ä¶ You won‚Äôt easily extract Async adapter if you care about the result . I think that‚Äôs\nobvious. You can‚Äôt easily substitute adapter which can return result with such\nthat cannot. Async is architectural decision here . And rest of the code must be\nwritten in a way that reflects it. Thus expecting to get the result somehow later. Getting the right level of abstraction for adapter might not be easy . When you cover\napi or a gem, it‚Äôs not that hard. But once you start doing things like NotificationAdapter which will let you send notification to user without bothering\nthe client whether it is a push for iOS, Android, Email or SMS, you might find yourself in\ntrouble. The closer the adapter is to the domain of adaptee, the easier it is to\nwrite it. The closer it is to the domain of the client, of your app, the harder it\nis, the more it will know about your usecases. And the more complicated and\nunique for the app, such adapter will be. You will often stop for a moment to reflect\nwhether given functionality is the responsibility of the client, adapter or maybe\nyet another object . Adapters are puzzles that we put between our domain and existing solutions such\nas gems, libraries, APIs. Use them wisely to decouple core of your app from 3rd party code\nfor whatever reason you have. Speed, Readability, Testability, Isolation,\nInterchangeability . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-08-24"},
{"website": "Arkency", "title": "Behind the scenes", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/10/behind-the-scenes/", "abstract": "We work from wherever we want and we ship it. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-10-03"},
{"website": "Arkency", "title": "How to start using UUID in ActiveRecord with PostgreSQL", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/10/how-to-start-using-uuid-in-activerecord-with-postgresql/", "abstract": "Author of photo: Christian Haugen . Although it may be obvious for many developers, there are still some that are not aware of great features that PostgreSQL allows to use with ActiveRecord . This tutorial is intended to reveal UUID type, which we can use in our Rails applications, especially as models‚Äô attributes . Special Postgre‚Äôs data types are available in our databases by enabling so called extensions . According to the documentation : So not only we get a possibility to create our own extensions, but we get a bunch of useful features out of the box as well. Let‚Äôs see one of them in action right now. You can follow all of presented steps with your brand new Rails application. To create one, for the purpose of this tutorial, you can run: We skipped some tests, javascripts, views and sprockets and set our database to PostgreSQL. Personally I think that UUID is extremely interesting topic to discuss and Andrzej has already written an excellent article about using this feature. This is 16-octet / 128 bit type compatible with most common GUID and UUID generators, supporting distributed application design, defined by RFC 4122, ISO/IEC 9834-8:2005 . It is represented by 32 lowercase hexadecimal digits, displayed in five groups separated by hyphens, in the form 8-4-4-4-12 for a total of 36 characters (32 alphanumeric characters and four hyphens). Although UUID might appear in different versions (MAC address, DCE security, MD5 or SHA-1 hash), the most generators relies on random numbers and produces UUID version 4. Version 4 UUIDs have the form xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx where x is any hexadecimal digit and y is one of 8, 9, A, or B. So the validation regexp may be as follows: How can we produce it in Ruby? It‚Äôs available since Ruby 1.9 and provides UUID version 4 described above, which is derived entirely from random numbers. How to use it in SQL code? The pgcrypto module provides functions to generate universally unique identifiers (UUIDs) using one of several standard algorithms. There are also functions to produce certain special UUID constants. Let‚Äôs see how can we use it in our applications. To enable this extension in our database through rails, we can use convenient helper method for PostgreSQLAdapter : We can create our model now: And then we can play with them a little bit: So we created our book with auto-generated id as a UUID, great! But what if I need just another field  of uuid type? We can do it too. After migration we have: And we‚Äôre done. Note that UUID is accessible only after retrieving a record from the database (or reloading it in place), not immediately in brand new created object, because we get UUID generated from Postgres and not Rails itself. To be honest I should mention about some drawbacks right now. Maybe they are not crucial, but it‚Äôs worth to be aware that they exists. Some inconvenience may be referencing with a foreign key to associated model. We can‚Äôt just add reference column like: because it produces: Which may seems OK at the first sight, but it‚Äôs actually a little bit tricky. We have the following models: and we are trying to create an association: See what happened? If not, take a look: What is wrong? add_reference associates model by default integer ID, which is not present in our database. Here‚Äôs created schema: Instead we should have a string field for referencing UUID, so any time we connect two tables, we can make a proper association. Fortunately it‚Äôs a small change: which creates: and that is what we‚Äôre talking about. While the UUIDs are not guaranteed to be unique , the probability of a duplicate is extremely low . The UUID is generated using a cryptographically strong pseudo random number generator. There‚Äôs very slight chance to get the same result twice. Wikipedia provides some nice explanation of possible duplicates. PostgreSQL offers many more extensions and types out of the box, that are compatible with Rails in easy way. What might be worth to check out are: The rest will be covered in further blogposts very soon. If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-10-02"},
{"website": "Arkency", "title": "React.js and Dynamic Children - Why the Keys are Important", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/10/react-dot-js-and-dynamic-children-why-the-keys-are-important/", "abstract": "Recently I‚Äôve been building a pretty dynamic interface based on\ngoogle analytics data for one of our customers. There was a bug that\nI couldn‚Äôt quite figure out because of my wrong understanding of\nhow react works. In the end it turned out to be a very easy fix\nand very important lesson on understanding react.js . It is even pretty accurately described in react documentation however\nwithout showing what will go wrong and what kind of bugs you can\nexpect if you don‚Äôt adhere to it. In other words, the documentation\nexplains the right way but without going much into details about why . Let‚Äôs see the simplified version of my problem in action. I have\ntwo components. One is a list of countries and one is a list of cities\nin the country. When you click the country it becomes active and\nand the list of cities is refreshed as well to show only cities from this\ncountry. In real life there is much more data presented based on selected\ncity and country but that‚Äôs enough for our demo. The problem is however that when you select a second city and switch\nbetween the countries, the second city remain selected (on another list).\nWe would like to always have first city on the list selected when country\nwas switched . Here is a little demo: Here is my code: Here is what I (unexperienced react padawan) was thinking: Properties are immutable, state is mutable therefore if I change\nthe properties of a component react will figure out it is a new\ncomponent, create it and call getInitialState . That was about this part of code: I imagined that because I was changing properties (the elements key), then after changing\nthe countries react was rendering a new cities list and a new city list\nshould have first item selected because of my getInitialState : But I was wrong . Wrong. Wrong‚Ä¶ And when the key is not provided? Well‚Ä¶ React will automatically use an\nincreasing integer number; I suspect based on the data-reactid attribute in DOM. So I was thinking that I am rendering conceptually a new component and\nreact was actually rendering the old one. The fact that I changed props doesn‚Äôt matter. Immutability of props is just a convention, not a\nrequirement. And react doesn‚Äôt care. Because it was the same component getInitialState was not called and the component remembered its old state. The fix is easy . Change the component key and react will know that it is a\ndifferent component and not the same one. You can try it below. When you\nselect a new country, the city is always the first one. And the code: The important part is the key for second list: You can see in DOM a different data-reactid attribute\nvalue now based on the provided key. Now the I‚Äôve actually been hit by this and I understood the\nproblem few more things makes sense‚Ä¶ React documentation on dynamic children The situation gets more complicated when the children are shuffled around (as in search results) or if new components are added onto the front of the list (as in streams). In these cases where the identity and state of each child must be maintained across render passes, you can uniquely identify each child by assigning it a key Props in getInitialState Is an Anti-Pattern OK, guilty as charged‚Ä¶ Using props, passed down from parent, to generate state in getInitialState often leads to duplication of ‚Äúsource of truth‚Äù, i.e. where the real data is. Whenever possible, compute values on-the-fly to ensure that they don‚Äôt get out of sync later on and cause maintenance trouble. In this example I remember in top-level CountriesComponent currently\nselected country because it is used to provide list of cities for the second list.\nBut I also keep this as active state in the first list. You wouldn‚Äôt have this problem if you didn‚Äôt keep state in this component This is what my coworkers said to me when I described my problem to them. It appears that it is often easier and wiser to move the state higher in component hierarchy .\nMaybe the list itself does not need to keep the state itself. Why? Because the\nparent component is interested that the list element was clicked. Something new was\nselected and not only the list must change and rerender but also another parts\nof parent component must change. If parent component knows about list status, then it\ncan render the list with new status, new currently select item. Notice that in my example I only need to change the key of cities list when country\nis changed because I keep state in that list. If I didn‚Äôt I wouldn‚Äôt have to and I\nwouldn‚Äôt care. If you liked this blogpost you might enjoy our books about React.js . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-10-03"},
{"website": "Arkency", "title": "Instantiating Service Objects", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/10/instantiating-service-objects/", "abstract": "In my last blogpost about adapters I promised a more detailed insight\ninto instantiating Adapters & Service Objects. So here we go. This is the simplest way, nothing new under the sun. When your needs are small,\ndependencies simple or non-existing (or created inside service, or you use\nglobals, in other words: not passed explicitely ) you might not need anything more. Ideally we want to test our controllers in simplest possible way. In Rails codebase,\nunlike in desktop application, every controller action is an entry point into the\nsystem. Its our main() method. So we want our controllers to be very thin,\ninstantiating the right kind of objects, giving them access to the input, and putting\nthe whole world in motion. The simplest, the better, because controllers are the\nhardest beasts when it come to testing. It‚Äôs up to you whether you want to mock the service or not. Remember that the\npurpose of this test is not to determine whether the service is doing its job,\nbut whether controller is. And the controller concers are These are the things you should be testing, nothing less, nothing more. However mocking adapters might be necessary because we don‚Äôt want to be sending\nor collecting our data from test environment. When testing the service you need to instantiate it and its dependencies\nmanually as well. When instantiating becomes more complicated I extract the process of creating\nthe full object into an injector . The purpose is to make it easy to create\nnew instance everywhere and to make it trivial for people to overwrite the\ndependencies by overwriting methods. The nice thing is you can test the instantiating process itself easily with injector\n(or skip it completely if you consider it to be typo-testing that provides very little\nvalue) and don‚Äôt bother much with it anymore. Here we only test that we can inject the objects and change the dependencies. Is it worth it? Well, it depends how complicated setting your object is. Some of my\ncolleagues just test that the object can be constructed (hopefully this has no\nside effects in your codebase): Our controller is only interested in cooperating with create_product_service .\nIt doesn‚Äôt care what needs to be done to fully set it up. It‚Äôs the job of Injector .\nWe can throw away the code for creating the service. You can use the injector in your tests as well. Just include it.\nRspec is a DSL that is just creating classes and method for you.\nYou can overwrite the metrics_adapter dependency using Rspec DSL\nwith let or just by defining metrics_adapter method yourself. Just remember that let is adding memoization for you automatically.\nIf you use your own method definition make sure to memoize as well\n(in some cases it is not necessary, but when you start stubbing/mocking\nit is). There is nothing preventing you from mixing classic ruby OOP\nwith Rspec DSL. You can use it to your advantage. The downside that I see is that you can‚Äôt easily say from reading\nthe code that metrics_adapter is a dependency of our\nclass under test ( CreateProductService ). As I said in simplest\ncase it might not be worthy, in more complicated ones it might be however. Here is a more complicated example from one of our project. You might also consider using dependor gem for\nthis. The nice thing about dependor is that it provides a lot of small APIs\nand doesn‚Äôt force you to use any of them. Some of them do more magic\n(I am looking at you Dependor::AutoInject )\nand some of medium level ( Dependor::Injectable )\nand some almost none magic whatsoever( Dependor::Shorty ).\nYou can use only the parts that you like and are comfortable with. The simple way that just checks if things don‚Äôt crash and nothing more. For testing the service you go whatever way you want.\nCreate new instance manually or use Dependor::Isolate . Thanks for reading. If you liked it and you wanna find out more subscribe\nto course below. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-10-12"},
{"website": "Arkency", "title": "Unit tests vs class tests", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2014/09/unit-tests-vs-class-tests/", "abstract": "There‚Äôs a popular way of thinking that unit tests are basically tests for classes. I‚Äôd like to challenge this understanding. When I work on a codebase that is heavily class-tested, I find it harder to do refactoring. If all I want is to move some methods from one class to another, while preserving how the whole thing works, then I need to change at least two tests, for both classes. Class tests are good if you don‚Äôt do refactoring or if most of your refactorings are within 1 class. This may mean, that once you come up with a new class, you know the shape of it. I like a more light-weight approach. Feel free to create new classes, feel free to move the code between them as easily as possible. It doesn‚Äôt mean I‚Äôm a fan of breaking the functionalities. Totally the opposite. I feel paralysed, when I have to work on untested code. My first step in an unknown codebase is to check how good is the test coverage. How to combine such light-weight refactoring style with testing? I was in the ‚Äúlet‚Äôs have a test file per a class‚Äù camp for a long time. If I created a OrderItem class, it would probably have an equivalent OrderItemTest class. If I had a FriendPresenter, it would have a FriendPresenterTest. With this approach, changing any line of code, would result in failing tests. Is that really a safety net? It sounds more like cementing the existing design . It‚Äôs like building a wall in front of the code. If you want to change the code, you need to rebuild the wall. In a team, where collective ownership is an accepted technique, this may mean that whoever first works on the module, is also the one who decides on the structure of it. It‚Äôs not really a conscious decision. It‚Äôs just a result of following the class-tests approach. Those modules are hard to change. They often stay in the same shape, even when the requirement change. Why? Because it‚Äôs so hard to change the tests (often full of mocks). Sounds familiar? What‚Äôs the alternative? The alternative is to think in units, more than in classes. What‚Äôs a unit? I already touched on this subject in TDD and Rails - what makes a good unit? . Let me quote the most important example: You‚Äôve got an Order, which can have many OrderLines, a ShippingAddress and a Customer. Do we have 4 units here, representing each class? It depends, but most likely it may be easier to treat the whole thing as a Unit. You can write a test which test the whole thing through the Order object. The test is never aware of the existence of the ShippingAddress. It‚Äôs an internal implementation detail of the Order unit. A class doesn‚Äôt usually make a good Unit, it‚Äôs usually a collection of classes that is interesting. In one of our projects, which is a SaaS service, we need to handle billing, paying, licenses. We‚Äôve put it in one module. (BTW, the ‚Äòmodule‚Äô term is quite vague nowadays, as well). It has the following classes: It‚Äôs not a perfect piece code (is there any in the world?), but it‚Äôs a good example for this topic. We‚Äôve got about 10 classes. How many of them have their own test? Just the Billing (the facade).\nWhat‚Äôs more, in the tests we don‚Äôt reference and use any of those remaining classes. We test the whole module through the Billing class. The only other class, that we directly reference is a class, that doesn‚Äôt belong to this module, which is more of a dependency (shared kernel). Obviously, we also use some stdlib classes, like Time. BTW, did you notice, how nicely isolated is this module? It uses the payment/billing domain language and you can‚Äôt really tell for what kind of application it‚Äôs designed for. In fact, it‚Äôs not unlikely that it could be reused in another SaaS project. To be honest, I‚Äôve never been closer to reusing certain modules between Rails apps, than with this approach . The reusability wasn‚Äôt the goal here, it‚Äôs a result of following good modularisation. Some requirements here include: It‚Äôs nothing really complicated - just an example. What do I gain, by having the tests for the whole unit, instead of per-class? I have the freedom of refactoring - I can move some methods around and as long as it‚Äôs correct, the tests pass. I tend to separate my coding activities - when I‚Äôm adding a new feature, I‚Äôm test-driven. I try to pass the test in the simplest way. Then I‚Äôm switching to refactoring-mode. I‚Äôm no longer adding anything new, I‚Äôm just trying to find the best structure, for the current needs. It‚Äôs about seconds/minutes, not hours. When I have a good shape of the code, I can go to implement the next requirement. I can think about the whole module as a black-box. When we talk about Billing in this project, we all know what we mean. We don‚Äôt need to go deeper into the details, like licenses or purchases. Those are implementation details. When I add a new requirement to this module, I can add it as a test at the higher-level scope. When specifying the new test, I don‚Äôt need to know how it‚Äôs going to be implemented. It‚Äôs a huge win, as I‚Äôm not blocked with the implementation structure yet. Writing the test is decoupled from the implementation details. Other people can enter the module and clearly see the requirements at the higher level. Now, would I see value in having a test for the Pricing class directly? Having more tests is good, right?\nWell, no - tests are code. The more code you have the more you need to maintain. It makes a bigger cost. It also builds a more complex mental model.\nLow-level tests are often causing more troubles than profit. Let me repeat and rephrase - by writing low-level tests, you may be damaging the project . As Damian Hickey puts it in an excellent way : Like writing lots and lots of fine-grained ‚Äúunit‚Äù tests, mocking out every teeny-weeny interaction between every single object?\nThis is your application: Now try to change something. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-10-12"},
{"website": "Arkency", "title": "How to persist hashes in Rails applications with PostgreSQL", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/10/how-to-persist-hashes-in-rails-applications-with-postgresql/", "abstract": "Our recent blogpost about using UUID in Rails projects with PostgreSQL was appreciated so we decided to publish a tutorial about storing hashes in our favorite database . Let‚Äôs see how it can be achieved. Sometimes we may need to not only store plain attributes like string , integer or boolean but also more complex objects. For now let‚Äôs talk about hashes . HStore is a key value store within Postgres database. You can use it similar to how you would use a hash in Ruby application, though it‚Äôs specific to a table column in the database. Sometimes we might need to combine relational databases‚Äô features with flexibility of No-SQL ones at the same time without having two separate data stores. If you need to think about possible use cases, please note that hashes in other languages are called dictionaries , so it‚Äôs already a hint how to and why use them. Firstly, let‚Äôs see how it looks like in plain SQL , everything according to official PostgreSQL documentation . Pretty simple. We created example DB, enabled extension and selected some key-value structure as a hstore . In previous blogpost about UUIDs we already showed how to enable particular extension. That‚Äôs what we gonna do again: Let‚Äôs reuse an existing example: And now we can play with that: The hstore datatype can be indexed with one of two types: GiST or GIN . In choosing which index type to use, GiST or GIN, consider these performance differences: As a rule of thumb, GIN indexes are best for static data because lookups are faster. For dynamic data, GiST indexes are faster to update. Specifically, GiST indexes are very good for dynamic data and fast if the number of unique words (lexemes) is under 100,000, while GIN indexes will handle 100,000+ lexemes better but are slower to update. So let‚Äôs create an another migration: I don‚Äôt have to explain why we should use indexes and why they are important when it comes to querying performance. If you want to use hstore selectors, you should definitely create these indexes. Here they are: Return the value from column description for key en : Does the specified column description contain a key en : Does the specified column description contain a value of Eccentric duck for key en : How to use them: Create example book: Find a book with particular polish description: Find a book containing ‚Äòduck‚Äô in english description: Find all books with english descriptions provided: Note the update statement stringifies our hash: In that case we may expect: Although it may not be a common case to store other types than string in a dictionary , it is still worth to have in mind that hstore supports only string data. The more possible case may be storing nested dictionaries: Ough! We have a problem, again. So is there any solution for more complex cases? In addition to hstore , json is a full document datatype. That means it supports nested objects and more datatypes. It also has more operators that are very well described in documentation . If you are using JSON somewhere in your application already and want to store it directly in database, then the JSON datatype is perfect choice. Then do whatever you want: However you should know that when searching for some records, you should stringify search parameters: I believe you see the power of using PostgreSQL with ActiveRecord in your Rails projects . If you still wonder whether MongoDB will be better choice for you needs, you should definitely check JSONB support introduced in 9.4 version of postgres , which is real data type with binary storage and indexing, a structured format for storing json. With the new JSONB data type for PostgreSQL, users no longer have to choose between relational and non-relational data stores: they can have both at the same time. JSONB supports fast lookups and simple expression search queries using Generalized Inverted Indexes (GIN). Multiple new support functions enables users to extract and manipulate JSON data, with a performance which matches or surpasses the most popular document databases. With JSONB, table data can be easily integrated with document data for a fully integrated database environment. You can also read PostgreSQL Outperforms MongoDB in New Round of Tests If you‚Äôre heroku user sometimes you may encounter the following error during rake db:migrate task: pg_dump command is not present on Heroku‚Äôs environment, which is not needed in production, but it‚Äôs nice to have this dumped SQL structure in development. You can get around it by turning of this feature when not needed: I believe after that article you see the benefits of using store and json types in Rails projects. A lot of flexibility combined with relational database give us almost unlimited room for improving our storage patterns. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-10-13"},
{"website": "Arkency", "title": "Frontend performance tips for web developers", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/10/frontend-performance-tips-for-web-developers/", "abstract": "Optimization has always been a tough topic. Donald Knuth said that premature optimization is the root of all evil . Performance is something that always causes emotions in web community. When we need it, we tune up our applications, try different and new servers , use load balancers, speed up our applications by playing with threads and processes, but sometimes we forget about frontend at all. Server side is important too, how if we need to provide fast-responding websites, we have to optimize them in our browsers as well. In this article I‚Äôd like to focus on client side and how to efficiently deliver content to end user. Before I start, I‚Äôd like to precise what won‚Äôt be mentioned in this article to give you a general overview about the overall content. A lot of useful links covering those topics are included in resources under this blogpost. Not a lot left, huh? When I was preparing to this article I was thinking about something unique, what is not mostly obvious , especially for backend developers. We all know the server side quite well, but sometimes we don‚Äôt have an opportunity to take care of client side - maybe because we have frontend developers in our team, maybe because we just don‚Äôt want or like to do frontend at all, maybe because we only maintain server stuff and don‚Äôt have any use cases on client side or for any other reason when we actually have some fear for touching a code that we are not experts in. A lot of us know how to configure workers, set proper HTTP headers, gzip and cache content and distribute it by CDN, but very few know about improving load processes for client side content, especially styles and scripts. In this blogpost I‚Äôll focus on the following things: JS: CSS: I‚Äôd like to focus on how we‚Äôre loading scripts in our html pages. It‚Äôs usually something like: It‚Äôs so called normal execution , which is the default behavior that pauses HTML parsing until script is executed. It means that when we have heavy-logic scripts, displaying page content will be significantly delayed. Hence it would be so obvious to mention that all scripts should be included in the bottom of <body> , just above the closing tag. Originally, this was nothing more than a hint to the browser that the script does not modify the DOM. Therefore the browser doesn‚Äôt need to wait for the script to be evaluated, it can immediately go on parsing the HTML. On older systems this might save some parsing time. However, IE has slightly changed the meaning of defer . Any code inside deferred script tags is only executed when the page has been parsed entirely. defer downloads the file during HTML parsing and will only execute it after the parser has completed. defer scripts are also guaranteed to be executed in the order they are declared in the document. A positive effect of this attribute is that the DOM will be available for your script . async downloads the file during HTML parsing and will pause the HTML parser to execute the script when it has finished downloading. Don‚Äôt care when the script will be available? Asynchronous is the best of both worlds: HTML parsing may continue and the script will be executed as soon as it‚Äôs ready. I‚Äôd recommend this for scripts such as Google Analytics. Typically you want to use async where possible , then defer, then no attribute. Here are some general rules to follow: Both async and defer scripts begin to download immediately without pausing the parser and both support an optional onload handler to indicate an initialization method contained in that script. The difference between async and defer centers around when the script is executed. Each async script executes at the first opportunity after it is finished downloading and before the window‚Äôs load event. This means it‚Äôs possible (and likely) that async scripts are not executed in the order in which they occur in the page. The defer scripts, on the other hand, are guaranteed to be executed in the order they occur in the page. That execution starts after parsing is completely finished, but before the document‚Äôs DOMContentLoaded event. The truth is that if you write your JavaScript effectively, you‚Äôll use the async attribute to 90% of your script elements. In older browsers that don‚Äôt support async attribute, parser-inserted scripts block the parser; script-inserted scripts execute asynchronously in IE and WebKit, but synchronously in Opera and pre-4.0 Firefox. In Firefox 4.0, the async DOM property defaults to true for script-created scripts, so the default behavior matches the behavior of IE and WebKit. To request script-inserted external scripts be executed in the insertion order in browsers where the document.createElement(‚Äúscript‚Äù).async evaluates to true (such as Firefox 4.0), set .async=false on the scripts you want to maintain order. Never call document.write() from an async script. In Gecko 1.9.2, calling document.write() has an unpredictable effect. In Gecko 2.0, calling document.write() from an async script has no effect (other than printing a warning to the error console). Documentation: While loading JavaScripts in different ways is pretty natural , loading CSS asynchronously and improving load behavior, time and order only in HTML code may be surprisingly. I‚Äôd like to present some of attributes that are worth to be included in <link> tags to improve performance of you website. dns-prefetch technique is very useful when your website links to related host names. Browsers do not execute DNS lookups on those domains before visiting or starting fetching the content, so adding rel=\"dns-prefetch\" indicates that particular hostnames should be resolved to their IP addresses. This is a feature by which browser proactively performs domain name resolution on both links that the user may choose to follow as well as URLs for items referenced by the document including images, CSS, JavaScript, and so forth. Prefetching is performed in the background, so that the DNS is likely to already have been resolved by the time the referenced items are actually needed.  This reduces latency when, for example, the user actually clicks a link. preconnect is used to indicate origins from which resources will be fetched. You can use preconnect to pre-open a new TCP connection so the browser can do this time intensive task while it waits for the rest of the HTML-page. By opening that TCP connection earlier we could save some valuable time. Initiating an early connection, which includes the DNS lookup, TCP handshake, and optional TLS negotiation, allows the user agent to mask the high costs of connection establishment latency. According to w3.org , rel=prefetch : indicates that preemptively fetching and caching the specified resource is likely to be beneficial, as it is highly likely that the user will require this resource.\nThere is no default type for resources given by the prefetch keyword. Link prefetching is a browser mechanism, which utilizes browser idle time to download documents that the user might visit in the near future. prefetch hints have the lowest priority. The browser observes all of these hints and queues up each unique request to be prefetched when the browser is idle. There can be multiple hints per page, as it might make sense to prefetch multiple documents. Keep in mind that prefetching does work across domains, including pulling cookies from those sites. According to the documentation : Prerendering extends the concept of prefetching. Instead of just downloading the top-level resource, it does all of the work necessary to show the page to the user without actually showing it until the user clicks. Prerendering behaves similarly to if a user middle-clicked on a link on a page (opening it in a background tab) and then later switched to that tab. However, in prerendering, that background tab is totally hidden from the user, and when the user clicks, its contents are seamlessly swapped into the same tab the user was viewing. From the user‚Äôs perspective, the page simply loads much faster than before. subresource hint identifies critical resources required for current page load. These resources should be downloaded as early as possible, they are necessary for further page load so they must be fetched immediately. Documentation: GoogleAnalytics provides asynchronous syntax for loading tracking script. The snippet below represents the minimum configuration needed to track a page asynchronously. It uses _setAccount to set the page‚Äôs web property ID and then calls _trackPageview to send the tracking data back to the Google Analytics servers. You can read about it in the official documentation . There are some tools as extensions to our browsers for measuring load times and suggest some optimization improvements for them. Chrome offers PageSpeed , which I find a great tool and in Firefox you can use ySlow , which is quite nice web page performance analyzer. If you need just some short description you may check awesome slides http://bit.ly/preconnect-prefetch-prerender prepared by Ilya Grigorik , web performance guru at Google. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-10-30"},
{"website": "Arkency", "title": "How to start using Arrays in Rails with PostgreSQL", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/10/how-to-start-using-arrays-in-rails-with-postgresql/", "abstract": "So far we covered a lot of PostgreSQL goodness. We‚Äôve already talked about using uuid or storing hashes in our Rails applications with PostgreSQL database. Now is the time to do something in the middle of these topics (more complex than uuid , but easier than hstore ) - we want to store list of simple values under one attribute . How can we do that? You may think ‚Äúarrays‚Äù right now and you are correct. Let‚Äôs see how we can achieve that. Arrays are ordered, integer-indexed collections of any object. They are great for storing collection of elements even with different types. First normal form ( 1NF ) is a relation‚Äôs property in a relational database. It requires every attribute to have domain that can consist of only atomic values so value of each attribute is always single value from that domain. For some wise reasons we pursue to normalize our databases so that no duplicated data is stored there. So should we consider combining relational databases with arrays as something that breaks 1NF ? It seems so . However there are cases when we might want to have redundant data, that‚Äôs the place where arrays suit the best. We‚Äôll start with empty database and create there example table: We can put some data now: Official Postgres documentation provides a lot of useful examples to start working on SQL level with database. In the last blogpost about hstore we showed how to enable particular extension. This time is different (maybe easier), because array is Postgres‚Äô data type , not an extension so there‚Äôs no need to enable that, because it‚Äôs accessible out of the box! In two previous articles (mentioned in the introduction of this article) we created Book model and appropriate SQL schema. Let‚Äôs stick to that and extend it a little bit: And the migration file: We can check it now: Now is the time to add some subjects for books and then query them. Please keep in mind that all of the following examples are executed Rails 4.2.0.beta1 environment. In previous versions of Rails we may encounter some weird behavior: What happened here? Why subjects array wasn‚Äôt updated? ActiveModel::Dirty module provides a way to track changes in your objects. Sometimes our record does not know that underlying object properties have been changed and that‚Äôs why we have to point this explicitly. And everything went as we wanted. So if you have any problem with updating properties of some enclosed object you can indicate that this particular object will be changed by your operations and then safely save it with new properties that will be updated: PostgreSQL have a bunch of useful array methods that you can leverage in your Rails applications. After reading all of these three articles you should be PostgreSQL trouper. You can now have flexible and relational database at the same time. There are a lot of topics worth to read, but not covered in any of these blogposts. I hope you find our tutorials useful. While I was researching arrays in Postgres I found an interesting thing, that I wasn‚Äôt aware of before: Tip: There is no performance difference among these three types, apart from increased storage space when using the blank-padded type, and a few extra CPU cycles to check the length when storing into a length-constrained column. While character(n) has performance advantages in some other database systems, there is no such advantage in PostgreSQL; in fact character(n) is usually the slowest of the three because of its additional storage costs. In most situations text or character varying should be used instead. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-10-21"},
{"website": "Arkency", "title": "You can move React.js root component around", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/10/you-can-move-react-root-component-around/", "abstract": "My recent challenge with react was to integrate it\nwith Magnific Popup library. I knew there was going to be a problem because the library\nis moving DOM elements around. But I had two interesting insights\nwhile solving this problem that I think are worth sharing. Here is completely oversimplified version of my problem.\nYou can press ‚ÄúButton inside popup‚Äù (playground below) to change state.\nWhen you press ‚ÄúShow popup‚Äù the Magnificent Popup\nlibrary however will move the popup content into different\nDOM element. When you try change the state one more time\nby pressing ‚ÄúButton inside popup‚Äù (this time actually\ninside popup) it will break with Invariant Violation:\nfindComponentRoot(..., .0.1.0): Unable to find element. This probably\nmeans the DOM was unexpectedly mutated . Go ahead. Try\nfor yourself. Open Developer Console to see the error. Here is the code for this example: Here is how the component looks like rendered in DOM before\nbecoming popup. And after magnificPopup moves it to a different place. So I did some research and found this interesting React JS - What if the dom changes thread that included some really nice hint: I think React won‚Äôt get confused if\njQuery moves the root around . So I decoupled in my little application the Popup component from my top-level Component and started\nrendering them separately. If you show popup and click inside it, you will change\nstate of both components. But even though the component\nrendered insided popup (handled by magnificPopup library)\nwas moved around in DOM, we no longer expierience our\nproblem. Because moving top-level react component around\nDOM works fine (here I am actually moving the element\nabove the react root node but the concept stays the same). Here is the code for this example‚Ä¶ The forum thread that I mentioned show a really nice demo for integrating\nreact with jQuery UI Sortable . You can\nmove the elements around thanks to sortable. But their content is\nrendered with react. It‚Äôs all possible because every element is rendered\nas separate react root. So this is a useful trick to know. While working with this code I had one more ‚ÄúAha moment‚Äù . I was looking at\nmy code and thinking Why am I calling show()/hide() on popup library in my\nhandlers? I didn‚Äôt come to React to keep doing that. The idea was to have\nprops and state and transform it into HTML view . Not to call show() or hide() manually. I should be setting state and the component should know\nwhether to use 3rd party library to show or hide itself. After all, if I\never want change the popup library (most likely) then such change should be\nlocalized to the popup component. I should not change my handlers because\nI changed my popup library . So‚Ä¶ Move the component behavior of popup inside Popup . And let it decide\nwhen to show and hide . That‚Äôs what it would be doing if it were pure React\ncomponent. That‚Äôs what is should be doing when it is not so pure, but\ncoupled with external library. Here is the code for this example‚Ä¶ Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-10-23"},
{"website": "Arkency", "title": "Beyond the Rails Way", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2014/12/beyond-the-rails-way/", "abstract": "Source: Justin Wolfe What is Rails for you? Is it just a technology? Is it about the community? How did it feel? Were you proud of achieving so much in so little time? Did you impress anyone by using Rails? Rails is no longer the youngest technology around. Did it change anything? Did you ever think how Rails ideas helped to shape the world? Did you notice how many startups choose Rails as the technology? It‚Äôs amazing that sometimes those people don‚Äôt fully know their business idea yet, but they know it will be implemented in Rails! It was no longer so expensive to come up with a prototype or with an MVP. More ideas could be validated. Some of the ideas were totally wrong. They didn‚Äôt have the market fit. Still, their authors lost less money than they would by choosing other (at that time) technology. Many ideas (and their authors!) were validated positively. The MVP has proved to be interesting for the users. People wanted to pay for using it. Those ideas turned into businesses. Many of those still exist today. As developers, we sometimes forget how much impact our work has on the world around us. All of the above wouldn‚Äôt have happened without us and Rails. What about those less technical people? Did they have their chance in the Rails revolution? Yes! It was late 2007 when I was contacted by a potential client. He said he was a fashion designer and he needed help with Rails deployment. What? A fashion designer needing help with Rails deployment! ‚ÄúWhat do you want to deploy?‚Äù, I asked, assuming he got some technical terminology wrong. ‚ÄúOh, it‚Äôs a prototype of a web application which helps men choose good clothes for them.‚Äù I looked at it and I was speechless. It was a fully working app, with a non-trivial algorithm implemented in Ruby. It was actually ready for deployment. That‚Äôs all I was needed for here. This scared me. One year before, I decided to rely my programming career on Rails. Is this what I signed up for? Non-technical people being able to implement an application needing me just for the deployment? I wanted to go back to my former Java world. To the world, where my job wasn‚Äôt threatened by fashion designers! I was lucky enough to be part of it. Rails enabled more people to be involved in creating web applications. I was very curious where it‚Äôs all going to. That was the time when new gems (they were called plugins at that time) started to pop up every week - acts_as_taggable, acts_as_anything, acts_as_hasselhoff (yes, there‚Äôs such plugin ). The fashion projects ended very well. When the client understood that I‚Äôm faster than him in developing the features, he took care of marketing and other stuff. I wasn‚Äôt just the deployment person anymore. Creating new Rails projects in 2008 was like combining little pieces together. At the beginning it was fun. However, the whole new wave of Rails developers started creating new versions of their gems every week. Each version had different dependencies. The authentication libraries kept changing every month at that time. At some point, it wasn‚Äôt just connecting the pieces, but also hard work on untangling the dependencies to make it all work together. This concept was never clearly defined. It was a term to describe the Rails approach. It‚Äôs worth noting that at that time, everyone in Rails was coming from somewhere. I was from the Java world. Some people came from the PHP world. There were even some ex-Microsoft people. At that time there were no developers who ‚Äúwere born with Rails‚Äù. When The Rails Way concept was appearing it was a way of distinguishing it from ‚Äúthe architecture astronauts Java way‚Äù or the ‚ÄúPHP spaghetti way‚Äù. We needed to be unique and have something to unite us. Most of our community DNA was very good, but there was also something negative. A big part of the Rails community was united with the anti-Java slogans. Everything Java-like was rejected. XML? No, thank you, we‚Äôve got yaml. Patterns? No, thanks. As a community, we entirely skipped the DDD movement, which took over the Java and .NET worlds. ‚ÄúWe don‚Äôt need this‚Äù ‚ÄúWe‚Äôve got ActiveRecord. We take the object from the database row and use it in all the three layers. Fat models or fat controllers? Whatever, let‚Äôs just not create new layers.‚Äù This way of thinking became more popular. A new generation of developers started to appear. They were the ones who were born with Rails. Ruby was their first language. Rails was their first framework. The didn‚Äôt bring the baggage of Java or PHP past life. They joined the Rails community and embraced what was presented to them. That was The Rails Way. It‚Äôs hard to define it easily. I tried to do it recently and I found a few features that make it so uniq: It‚Äôs really good for developers who start their career. I keep teaching The Rails Way to the students - at the beginning. That‚Äôs the most efficient way to get a result. It‚Äôs the best way to stay motivated while learning more. Within a project, The Rails Way is great at the beginning, when you‚Äôre still not sure, where you go with the features and you need to experiment. In different project, the meaning of the beginning may be different. In some projects, I see the need to get out of the Rails Way as soon as the second month of development starts. In other projects it may be a year. When you start wondering - does that code belong to the model or to the controller - it‚Äôs a sign that you may be looking for something more than the Rails Way. When it‚Äôs not clear how a feature works, because it‚Äôs MAGIC - it‚Äôs a sign the code controls you, not the other way round. You need something more to turn the magic into an explicit code. When you start creating model classes which don‚Äôt inherit from ActiveRecord::Base and you have problems explaining to the team, why you needed that. When you try to test, but it either takes ages, because you need full integration tests, or you die by over-mocking. When you try to switch to a hosted CI, but they are unable to run your test suite. When you can only migrate data at nights, because the migrations lock the tables. I‚Äôve had the ‚Äúluck‚Äù to review hundreds of Rails projects over the last 10 years. The same patterns were visible over and over again. An app was in quick development for the first months and then it started stagnating to the point where no one was happy with the speed. I‚Äôve started collecting those patterns. I grouped them into code smells, anti-patterns, magic tricks. Meanwhile, over the years, I was studying many non-Rails-Way architectures like DCI, DDD, CQRS, Hexagonal. Then I started to draw lines between those two. Ruby and Rails are very unique and specific. Some things fit well into it, while others seem foreign to the way we write code. I picked some of the building blocks of the architectures and tried to apply them in the Rails projects. The ones that didn‚Äôt fit, I rejected. At the end, I only kept the ones which looked helpful for the typical problems. This was just the beginning. Even if you know the starting problem (point A) and you know the end result (point Z), there‚Äôs many steps in between that need to be made very carefully. I assumed the code transformations will be done on production applications. No place for any risk here. Some of the changes may even be applied to untested code. Your application needs to be safe, even when you apply the changes. Your boss and your clients will never allow introducing any bug ‚Äúbecause I was improving the architecture‚Äù. It‚Äôs just not acceptable. It took me over a year to put together the refactoring recipes. Your code contains lots of small issues which make it harder to introduce a better design. You won‚Äôt introduce service objects if your controllers are filters-heavy. The dependencies will break your code. You won‚Äôt introduce service objects, if your views rely on @ivars magic. You need to be explicit with what you‚Äôre passing to the views. You won‚Äôt make the build faster if it the tests still hit the database. You won‚Äôt get rid of the real database as long as your ActiveRecord classes contain any logic. You need to introduce repository objects. You won‚Äôt introduce service objects easily, if your controller action can do different things, depending on the params (params[:action] anyone?). You need to use the routing constraints. You won‚Äôt find any shortcut, unless you know the SimpleDelegator trick which helps you move a lot of code into a temporary service object at once. Those are some of the things I was working on. Those recipes are tested in big Rails projects by many different teams. Those recipes work.\nThey will make your architecture improvement easier. This all led to me to writing the ‚ÄúFearless Refactoring: Rails Controllers‚Äù book. The core of the book are recipes. However, the recipes alone may leave you with just the mechanics, so we‚Äôve added many chapters which explain the techniques in details. We‚Äôve also added the ‚ÄúBig examples‚Äù chapters. They take you through some (ugly) code and apply the recipes, one by one. Thanks to all of you who bought the book when it was still beta since February I‚Äôm very confident about its quality. You sent me a great feedback. You sent me the before/after pieces of code. This book wouldn‚Äôt happen without the people who trusted us so early. Thank you! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-12-05"},
{"website": "Arkency", "title": "Does everyone in your team write good code?", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2014/12/does-everyone-in-your-team-write-good-code/", "abstract": "How does it feel when someone pushes new code to the repo and the code doesn‚Äôt fit your standards? Any emotions? Is there anything you can do about it? You could start an argument, but is it really the best way? Negative emotions won‚Äôt help here. You could fix the code, but are you sure that this time your code will be acceptable by the other person? If the team has totally the same opinions on the code, fixing is OK - no possible conflict here. Otherwise, you need to step back and improve the same understanding. How can you get the team to have a similar understanding of code quality? Each of us has a different background. Different projects, different languages, different paradigms. We read different books. We value different gurus/blogs. It‚Äôs close to impossible to get the same understanding, easily. Luckily, we can get there, step by step. We can help each other educate. This all sounds very abstract. Luckily, the Rails community is a bit more unified. Well, OK, we‚Äôre not really unified. That was a lie. Simplifying things a bit, there are two Rails camps - the OOP camp and the Rails Way camp . Both camps seem to be unified on their own. If you have a mix of those people in the team, then you may have a hard time to agree on some principles. The OOP camp (I don‚Äôt like the name, but don‚Äôt know any better one) seems to share some common fundamentals. They read Martin Fowler, Uncle Bob, Kent Beck, Michael Feathers, Greg Young and very often agree with their words. As a side note - there‚Äôs a more fundamental difference between OOP and The Rails Way than there is between OOP and FP . Let me focus on the OOP camp here. In this camp, it‚Äôs mostly OK to extract new methods, extract new classes, create new layers (like services or repos). If your team is mostly OOP, then the differences in coding standards won‚Äôt be big. Some of the differences may be a result of only slight differences in understanding. Some of them may be  just because someone wasn‚Äôt really familiar with the exact variation of the concept. I know at least 4 ways of implementing a form object. I know at least 6 ways of implementing a service object. See the problem? Some coding inconsistencies are OK to have within one codebase. Just recently, in our team we‚Äôve discussed where does authentication belong in a typical Rails app. Even in the OOP camp, there‚Äôs not much discussion about it - authentication is handled at the controller level. There‚Äôs a problem with authentication at the controller level, though. It makes the controllers deal with non-http concerns. Moving the authentication to the service objects also doesn‚Äôt sound ideal. In fact, we will not easily find a good place for authentication. It is a cross-cutting concern, so it doesn‚Äôt fit nicely into the OOP paradigm. I could talk about how we can use Aspect Oriented Programming for that, but that‚Äôs a topic for another occasion. In a way, we‚Äôve covered that in our blog post - http://blog.arkency.com/2013/07/ruby-and-aop-decouple-your-code-even-more/ What I‚Äôm trying to tell here is that, sometimes different standards are OK. They may show us in the code which approach is better. I‚Äôd accept all approaches to authentication in a code review, assuming that we all understand the pros/cons. Some code changes are harder to accept Adding new controller filters is one example. I‚Äôm very sceptical about it. Most of the logic in filters belong to the service layer. If I see a commit that introduces such change, I try to explain why it may not be the best idea. Another example is when I see that we pass some data to the Rails view and we do it with an instance variable. There are some reasons, why it‚Äôs not always the best idea. In the code review comments I try to explain that. Explaining such cases takes time. I usually try to explain the bigger picture - why certain things fit better in the overall architecture. The refactoring recipes Over time, I‚Äôve collected all of such arguments and released a whole book on this topic. The ‚ÄúFearless Refactoring: Rails Controllers‚Äù book is exactly this - a way of encapsulating those arguments into one place. It‚Äôs not only that, though. I‚Äôm focusing on the explanation how to apply the refactoring in a fast way. I call it - recipes. Thanks to the recipes, I can expect that people can take the instructions and apply the code change within 30 minutes of their time. Being time efficient is one of the reasons why the recipes exist. I‚Äôve seen refactorings taking DAYS and ending with bugs. This is not a refactoring. Recipes are about quick 20-30 minutes (pomodoro anyone?) sessions of super-safe code improvements. The refactoring recipes represent a consistent way of thinking about the code in a Rails app. I call it The Next Way. We often work with legacy The Rails Way apps, so we need to get the code from such state and gradually improve it. ‚ÄúThis codebase looks like a collection of random blog posts‚Äù That‚Äôs one of the problem of applying random advices from different places, how to change the Rails code. It‚Äôs a problem, indeed. The Next Way is not The Best Way by any means. What I tried to do is to create a consistent approach. Since the book started it was much easier to work in a team and just point to the recipes and chapters whenever we tried to explain some concepts. Everyone in the team has the book. I‚Äôve also noticed other teams doing the same - this is what I received from one of the readers: ‚ÄúIn our company each developer who achieved basic knowledge about rails way and all of it benefits and disadvantages gets a really important ticket - read Fearless Refactoring asap. This book helps us refactor old legacy code that is a lot more complicated that it should be for given feature. All solutions included there give us knowledge how to resolve complex problems by writing clear and well-organized code using the best OOP rules.‚Äù As much as I‚Äôd love everyone to own a copy of the book, I know it‚Äôs not possible. So, the teams where only one member knows about the recipes won‚Äôt really get much value from it - it‚Äôs not that easy to transfer the knowledge. The Rails Refactoring Recipes website That‚Äôs why the Rails Refactoring Recipes website was born. It‚Äôs impossible to move the whole book to the website, but we‚Äôve moved what‚Äôs most useful - the recipes algorithms and before/after code examples. If you own the book, you know there‚Äôs much more to it, in the book we list warning and edge cases on each recipe. We also show every step with the code, while here just have the final result. Still, what the website gives to all of us is the fact that each recipe now has an URL. Each recipe has an URL so you can always link to it in the code review comments. This may be a huge time-saver for you and your team. Now, instead of explaining the suggested refactoring you can just paste the URL. http://rails-refactoring.com/recipes/ What can you do with this website? First, have a look at the existing recipes. Find the ones, that may be most relevant to your project. If there were recent commits in this area, jump to github, find the Pull Request or commit and paste the URL of the recipe in the commit comment. This way, you will communicate the idea to the team and to the author of the code. Second, if you know that your team would benefit from knowing of a recipe that is not on our website yet, then please write us an email and let me know about it. We want the recipes collection to grow fast! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-12-19"},
{"website": "Arkency", "title": "The categories of validations", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/01/fearless-refactoring-1-dot-1-validations/", "abstract": "There are many reasons why rails changed the landscape of web development when it was released.\nI think one of the aspects was how easy it was to communicate mistakes (errors) to the user.\nValidations is a very nice convention for CRUD applications. Over time however, many of our active\nrecord objects tend to grow in terms of validations. I think they fit at least into a few\ncategories (probably more). So it‚Äôs not uncommon to have half a dozen unrelated validations in an active record class. And they sometimes lack cohesion when you look at them together. To achieve better clarity and modularity in the application it might be good to start moving them to separate places. Maybe the trivial validations are better suited in form objects in your case? Maybe domain checks should be moved into Service objects because our Order class shouldn‚Äôt know and access the Inventory related data? Maybe security validations don‚Äôt need to be validations at all ? If security constraint is violated raising an exception could be better. Polite users won‚Äôt see this validation ever anyway. Why be nice to hackers and display them a nice validation message? Use-case based verifications? Maybe they belong to that one usecase (service object) only and the rest of the app doesn‚Äôt need to know about it. The point being‚Ä¶ Once you find yourself in a situation where your object is coupled with many validations, especially such that are crossing multiple boundaries and verifying things from completely other parts of the system, you might wanna decouple it a little bit. Make it lighter. Move the validations into other places where they fit better. Validations are often big bag of things that we need to check before we let the user proceed further. It‚Äôs worth to think from time to time how to organize this bag. Maybe into a nice rucksack? That‚Äôs why Fearless Refactoring: Rails Controllers 1.1 release includes 2 practical and 2 theoretical chapters that will help you get started. Happy refactoring Robert Pankowecki & Andrzej Krzywda Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-01-03"},
{"website": "Arkency", "title": "It's easy to miss a higher level concept in an app", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/01/did-you-miss-a-higher-concept/", "abstract": "Just yesterday, I finished reading Understanding the Four Rules of Simple Design written by Corey Haines . I definitely\nenjoyed reading it. The examples are small, understandable and a good starting point\nfor the small refactorings that follow. It‚Äôs a short, inexpensive book, but dense with\ncompressed knowledge, and I can only recommend buying it. You can read it in a few hours, and contemplate it\nfor much longer. One of the examples inspired me to write this blogpost about higher level\nconcepts . What does that even mean? Especially in terms of programming? First, let‚Äôs have a look at this example: Now you know a bit of the story related to the example from the book. I looked at it and I was like: hey, how would that even work? ‚Ä¶\nThis is not the right place for this method. It should be in ... I won‚Äôt tell\nyou yet, because I quickly realized that it could actually work‚Ä¶ This Location is basically just a Value Object and it could return all neighbour locations as value objects as well. Let‚Äôs try to implement that. And you could use it like: This makes sense if you assume an infinite, 2-dimensional Map of square cells, which is the default for Conway game of life . But whenever you say Game to me, first thing I think\nabout is Civilization 5 and its hexagonal Map. When you say Game to me, especially board game , I think players,\nmaps, movements, rules . Not all of those things make sense for Conway game of life because\nit is a zero-player game but I think the intuition of Game still applies. When discussing a different code example in the book, Corey reverses\nthe dependency between two objects. Instead of Cell knowing its location Now the location can know what cell is on it, thus becoming a Coordinate : So when I think about location neighbors I start to wonder: What is the better dependency direction here? Did we miss the concept of Map perhaps? Could we gain something by adding\nit? Would it be more or less intention-revealing? These are good questions\nto ask. I think games are particularly hard to implement right because there are many rules\nand behaviors that often require knowledge about pretty much anything else that‚Äôs\nhappening in the Game. Corey explains it nicely at the beginning of the book when he\ntalks about the better design concept. A few months ago I wrote a blogpost that shows how to implement a custom YearMonth class in Ruby that would work with ruby Range . Basically YearMonth knows how to compute its successor, the next YearMonth. It also works nicely with iterating\nand comparison. It was pointed out, however, that I was missing a higher level concept: a Calendar .\nDays, Weeks, Months, and Years don‚Äôt exist in a vacuum, but are parts of something\nbigger: passing time, which we follow by using a Calendar. And I agree. There are even different kind of calendars in use. I am not sure yet\nif I have an intuition how to design a good Calendar class with a useful API.\nAnd how would I do that so that all chunks of knowledge don‚Äôt land in the Calendar class,\nbut only in one proper place? I was once writing an application for managing employees‚Äô holidays. In Europe you are allowed\na certain number of free days depending on how long have you been working, both in your life as a \nwhole and for that particular employer (and other factors as well). The application that I was working on was meant to be deployed separately to every company. So some of the queries\nthat I wrote were executed across entire sets in database, without scoping per company because\nthe data was meant to be for that one company. That made the code easier in some places, because I could\nquery things globally when verifying the correctness of some business rules. One pivot later, the product was an SaaS intended to be deployed in one place but to support\nmultiple companies. The concept that I was reluctant to introduce immediately became\nnecessary. Everything had to be scoped per company for each employee currently using the\nsystem. It was a multi-tenant application. Employees don‚Äôt live in a vacuum either. There are parts of a company hiring them. I was writing\nsoftware for helping companies manage holidays for their employees and there was no concept\nof the Company anywhere in the code. That was the higher-level concept I was missing. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-01-04"},
{"website": "Arkency", "title": "Ruby Exceptions Equality", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/01/ruby-exceptions-equality/", "abstract": "Few days ago my colleague was stuck for a moment when testing one service object.\nThe service object was responsible for a batch operation. Basically operating on\nmultiple objects and collecting the status of the action. Either it was successful\nor failed with an exception. We couldn‚Äôt get our equality assertion to work even\nthough at first glance everything looked ok. We had to dig deeper. The problem boils down to exceptions equality in Ruby. And few tests in console\nshowed precisely how it works, later to be confirmed by the documentation. Let‚Äôs see a comparison case by case. But first, exception definition: Yes. That was our first test and it behaved according to our intution. So why did our\ntest fail if everything told us that we are comparing identical exceptions. Ok, so apparently the message must be identical as well. But in our case the\nmessage was equal and our exceptions were still non-equal. Bummer. Let‚Äôs think\nabout one more aspect of exceptions: backtrace. The backtrace of unthrown exception is‚Ä¶ Ok, I didn‚Äôt excepted that. I imagined that the backtrace is assigned at the\nmoment of exception instantiation. But when you think deeper about it, you\nmight realize that it wouldn‚Äôt make sense. Would you like to know that the exception was raised at line 1 or rather\nline 2 in that example? So obviously backtraces are assigned when\nexception is actually raised, not merly instantiated. But do they play any role in exception equality? Let‚Äôs see. Apparently for two exceptions to be equal they must have identical\nbacktrace. Even 1 line of difference makes them, well‚Ä¶ , different. ruby Exception#== documentation says: If obj is not an Exception, returns false. Otherwise, returns true if exc and obj share same\nclass, messages, and backtrace. In my original problem it lead me to realization that we were comparing\nraised-and-catched exception (thus with stacktrace) with a newly\ninstantiated exception. That‚Äôs why we couldn‚Äôt get to make them equal. When you use assert_raises(RefundNotAllowed) or expect{}.to raise_error(RefundNotAllowed) these matchers take care of\nthe details for you: But when you check something like expect(result.first.error).to eq(RefundNotAllowed.new) because your batch\nprocess collected them for you, then you are on your own and this might not\nbe good enough and it won‚Äôt work. You might wanna just compare manually\nexception class and message. Because they inherit from Exception (through StandardError ) they share identical\nlogic as described in documentation. If you want something better you need to overwrite == operator yourself. I am personally not convinced about the usability of including backtrace in\nexception equality logic because in reality one would almost never create\ntwo exceptions with the exact same backtrace to compare them. Although maybe\nfor some usecases it is a nice way to determine if repeated attempt failed in\nexactly same way and for the same reason. But you can always overwrite == in a way that would not call super and\nwould completely ignore the backtrace, instead comparing only exception class, data,\nand perhaps message. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-01-11"},
{"website": "Arkency", "title": "Upgrade capybara-webkit to 1.4 and save your time", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/02/upgrade-capybara-webkit-to-1-dot-4-and-save-your-time/", "abstract": "I spent quite some time on Monday debugging an interesting issue. Our full\nstack acceptance tests stopped working on CI. Just CI. Everything was passing\nlocally just fine for every developer. So I had to dig deeper. After initial investigation it turned out that tests which were timing out on CI with\n3 minutes limit of inactivity were passing given enough time (around 15 minutes).\nI used SSH to log into Circle CI instance and tried executing them myself to see that.\nSo‚Ä¶ Suddenly, one day, subset of our tests become really slow. How would that happen? I was stuck trying to figure out the reason when my coworker suggested that\nit might be related to problems with javascript files. At the same time I was\nin contact with Marc O'Morain from Circle who\nsuggested it might be related to Mixpanel because other customers who used\nMixpanel experienced problems as well. So I disabled Mixpanel Javascript, tested it out and everything was working\ncorrectly. We were already using capybara-webkit version 1.3.1 with blacklisting\nfeature to prevent exactly such kind of problems: However mixpanel tracking was added later compared to this code.\nSo it was never put on the blacklist because we simply forgot .\nWhat a shame. But this is where new version of capybara-webkit comes into the story. It has a\nreally nice feature which allows you to disable any external JS by calling That way you don‚Äôt need to remember in the future to blacklist any\nexternal dependencies in your project. They make your test much slower\nand unreliable because of possible networking issues. So blacklisting\nas much as possible will save you time on executing tests and on debugging\nsuch issues as mine. It turned out that we couldn‚Äôt reproduce the problem\nlocally because our developers work from Europe and the mixpanel networking\nissue occured in US only. Guess where Circle CI node is located :) You can put the blocking snippet of code in before/setup part of your\nacceptance test, or in spec_helper or in a constructor of class that\nis using capybara api. Because we use bbq gem in our project,\nfor me it was: I added the respond_to? check because rack-test driver don‚Äôt\nhave (and don‚Äôt need) this feature available. If you follow standard way described in Using Capybara with RSpec you can write: or in Capybara DSL: Of course it doesn‚Äôt need to be in before/background/setup . It can be\nused directly in every scenario/it/specify but that way you will\nhave to repeat it multiple times. You can also configure it globally in spec_helper with: The nice thing about capybara 1.4 is that it is very verbose for the external resources that\nyou haven‚Äôt specify allow/disallow policy about. So next time you add new external URL you will notice that you need to do\nsomething. Unless of course you went with page.driver.block_unknown_urls which I recommend if your project can work with it. For all other cases\nthere is allow_url . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-02-18"},
{"website": "Arkency", "title": "Adding videos embedded in a page to a sitemap", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/02/adding-info-about-videos-embedded-in-a-page-to-a-sitemap/", "abstract": "One of our customer has a solution that allows them to quickly\ncreate landing pages that are then used\nfor SEM . Of course\nsuch pages are listed in the sitemap of the application domain. The lates addition to that combo was to\nlist the videos embedded on the landing pages in the sitemap. It sounded hard,\nbut turned out to be quite easy. Our landing pages contain html that is saved with content editor. It is just html.\nThe videos are embeded in a normal way recommended by the providers such as: Nokogiri for the rescue :) More about: VideoInfo to the rescue :) If an iframe is not for a recognizable video then VideoInfo will raise an exception\nthat we catch. If there is networking problem we gracefuly handle\nit as well. SitemapGenerator to the rescue. These three snippets are the essence of it. There are of course tests, and there is adapter for obtaining video\ndata so that tests don‚Äôt connect to the internet. But it turned out to be way simpler than I expected. Which is always a nice surprise\nin our industry. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-02-21"},
{"website": "Arkency", "title": "Burnout - do you need to change your job?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2014/08/burnout-programmer-do-you-need-to-change-your-job/", "abstract": "I‚Äôve been reading recently a story on Hacker News about a programmer who\n(depending on who you ask for a diagnose in the thread) was suffering from\nburnout . Some commenters even suggested depression. There were many\nadvices recommended (unfortunatelly I can‚Äôt find a link to the discussion\nright now) but one certainly spot my attention. The advice was to completely change the technology and start again with something new . If you are Rails backend developer, switch to frontend or even go with gaming . People\nsaid the money doesn‚Äôt matter, it‚Äôs your mental health that is the most important and\nearning 2x or even 4x less is not the thing to focus and not the most crucial factor. Well, I don‚Äôt know if that‚Äôs going to help, if that‚Äôs a good advice. I‚Äôm not a\npsychologist nor psychiatrist. Although I am guilty of dreaming occasionaly about\nswitching to gaming and releasing my own 2D platform game based on Unity probably.\nHowever, that is not the most important here. What got me thinking is Do we really\nneed to change a job to try out new things? If we do need to change the job, how did it happen? How is that despite being well paid,\nhaving a sophisticated job, that many would like to have, we still suffer from burnout? Well, we might start as let‚Äôs say C++ programmers, but do we wanna die as C++\nprogrammers? I don‚Äôt think so. So ask yourself, do you sometimes have a feeling that you\nare doing the same thing over and over? That you were categorized ( internally by yourself or externally by your agency, boss, coworkers, head hunters‚Ä¶ ) as X technology-developer and you can‚Äôt escape this? My guess is, that you are probably\nnot alone, feeling like that. If you want to switch from Ruby or Java or .NET to gaming (which i guess is prefering\nC++ and C#) then yeah, you probably need to switch company. Even using the same language\nmight not be enough because of the customer that your company has, the nature of the business\nand the tribal knowledge that you need to finish project. I guess web companies don‚Äôt take\nmuch gaming gigs. But when you are already a web developer (probably strongly oriented towards either backend\nor frontend) then why the hell would you need to change a job to try out something else? Can‚Äôt backend developers help with frontend, learn Angular or React, have fun and help\nwith the project? Can‚Äôt frontend developers learn node.js and finish backend features as well?\nI don‚Äôt get it. And maybe we all can do mobile just fine as well, especially when we have\nbackground in desktop apps? Could it be that way? I don‚Äôt think there is a silver bullett for burnouts but excuse me I think we can\nas industry do way more to minimize the scale of the problem . Here are few ideas: Let me elaborate a bit about each one of them. You know one reason why people get stressed and tired? Because bosses give them huge stories,\nhuge features to work on alone. People got something to do for a week or a month or even longer\n(i know, speaking from experience and from hearing from others) and they have no reason to talk\nand discuss and cooperate on it inside the team . Technically, you are part of a team. In\npractice, you are on your own doing the feature. And don‚Äôt think someone is going to help you. Everyone is busy . And you know why your backend developers never asked for a frontend story. Because they know it\nwould too big for them and they are scared. And they don‚Äôt want to overpromise. They are not yet\nconfident. What could help? Small stories . Split everything into small stories. Get people to track bigger\ntopics/features (but not implement them alone) and let everyone do frontend and backend stories.\nOf course we will be afraid and a bit slower at first. But then, we will get more confident. We\nwill better understand what our coworkers do and how much time it takes. We will have plenty of\nreasons to talk about code and how to write it so that everyone understands each others\nintentions. We will have better collective ownership . Ever joined a company and got stuck in a project for like‚Ä¶ how about‚Ä¶ forever? Yeah‚Ä¶ That\nsucks . If you are a member of a company which has more than 10 people, chances are, you could\ntheoretically switch to another project. Of course your boss would have to let you do it. And it would\nhave to be approved by the client. But switching the project and getting to know new domain,\nnew people, new client, new problems and new challenges is refreshing. Problem is (as almost\nalways) the inertia . Sometimes customers even fall in love with their developers (not\nliterally, but you probably know what I mean) and don‚Äôt want to let them go. They fear that\nthe replacment won‚Äôt be as good. It‚Äôs understandable . But that shouldn‚Äôt be the major\nfactor for the decision. Team rotations are easier if your company is having fewer projects but of\nbigger size. If there are 20 of you, then it is easier to convince customer to let developer go\nwhen you are working on 3 projects with about 7 ppl each one. Or 4 projects with 5 people. If you\nhave 6-7 projects with 2-3 people working on them, you customer might not be willing to let\none of the developers go. After all, that one developer is 50 or 33% of the entire team. So they\ntend to worry a lot about consequences. If one developer is 14% of a team, then there is high\nchance that domain knowledge will still remain in the team and can be passed completely until\nnext person leaves a team. Consulting can be exhausting . As everyone who ever did knows. One thing that can help is letting people work on their own projects . They don‚Äôt necesarly need to be open source ones (although\nthat is nice as well). But that can be products that your consulting company intends to sell.\nAs Amy Hoy said When you get paid to do a thing, you‚Äôve already got three built-in markets to tap : Why not let developers target those people as well? That can be challenging and as\nrefreshing as getting another project or another technology. Except that instead of learning\nnew tech, you need to learn research, marketing, prioritizing and much more . With your own products\nyou always want to do so much but your time is so limited. And sometimes our ideas fail. Just\nlike our clients. Getting better with skills in those areas can help us be better in consulting\nand prevent our customers from making mistakes. When you launch at least one of your project\nsuddenly you are well more aware of many limitations. And you can question and challenge the\ntasks way better. You are inclined to ask customer for reasons and goals behind doing the tasks.\nYou are not just building feature X , you are improving retention . You get the sense of all\nof it. There is so much hype recently for microservices. A lot of people mention that with microservices\nyou can write components more easily in the languages better suited for the task. But have you\never considered that with microservices you can give people some playground for their ideas\nwithout much risk. It‚Äôs not that you need to rewrite entire app in Haskell. But one, well isolated\ncomponent with clear responsibility. If they want to? Why not? Uncle Bob says we should learn\nat least one new programming language every year to expand our horizons. And if we do? And if\nwe expanded our horizons, where are we to apply that knowledge? In a new job? Let your people work and learn at the same time. You might not know it\nbut you probably hired geeks who would like to know everything there is in the world . They are\nnever going to stop learning, whether you let them or not. If they need to, they will change a job for\nit. But it doesn‚Äôt mean they want to do it. It‚Äôs just, you might not leave them much choice. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-08-17"},
{"website": "Arkency", "title": "The Beginners Guide to jQuery.Deferred and Promises for Ruby programmers", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2015/02/the-beginners-guide-to-jquery-deferred-and-promises-for-ruby-programmers/", "abstract": "Working with asynchronous code is the bread and butter for frontend developers. You can find it hard when working with it at first - as a Rails developer, you live with code which is usually synchronous. Luckily, promises were something which allowed us to get a huge step forward to deal with callbacks in a sane way. While widely adopted by backend JavaScript developers, Promises and their tooling is something often missed by Rails devs. It can be different - and you have all important libraries bundled by default in our Rails app. Let‚Äôs see what jQuery can offer you here - and I‚Äôll discuss how I managed to solve typical async problem with features that jQuery provides. I‚Äôm working on fairly big project which have frontend composed of microapps. Such microapp is providing services to build our frontend - it‚Äôs often a package of React components to build a view, commands which you invoke to perform an action on backend, and the storage - a class which encapsulates our data and reacts if this data is changed (it is similar to Store in Flux terminology). The whole thing is ‚Äòglued‚Äô together in a dispatcher - you may think of it as a telephone central when we ‚Äòroute‚Äô events from our pieces to another piece. In our app we are making some kind of surveys (called assessments) to rate ‚Äúassets‚Äù in a certain way. There are two roles in the process - an user which is the owner of a given asset (it‚Äôs often a piece of hardware). Owner can go to a view and perform as many surveys as he want to provide rating of his owned assets. An analyst defines what assets are and who owns them - he also defines criteria of surveys. Everything was working fine without much coordination before a request from my client came. Some surveys are way more important than the other - just because some assets are more ‚Äòcritical‚Äô now than the others. He asked me to introduce feature of assigning things - it‚Äôs basically just a reminder via an e-mail to do this particular survey, chosen by an analyst. To implement such thing I‚Äôd need a direct link to a survey which I can include in an e-mail. After selecting this link a modal with a survey should pop and the asset owner can start working on this survey without hassle. After closing this survey he also needs to have an overview of his owned assets, opened at exact place where rating change occurs. This feature is dependent on three apps - Assets which provides an overview of assets available for the owner, Surveys which is responsible for surveys modal and Rating which is an app for storing rating we surveyed before. We can complete a survey exactly once - once a rating is set, it cannot be changed. That‚Äôs why we need to ask whether we need to complete a survey or not - clicking on a link second time should present an user with a message that this rating is already set. These apps works completely asynchronously - each starts, fetches its data, sets dispatcher and waits. When an asset owner roams through components of Assets app to find a survey he like to make, timing issues does not occur. But in this particular case we want to present survey modal as soon as possible - timing is critical to make user‚Äôs experience fine. I decided to put the code of this new feature in the Surveys app - in our case it must wait for data from Rating (to ensure the survey is still valid), Assets (we need to open tabs in the view on the certain asset, so we need to have this data loaded first) and its own data (fetched from backend), and THEN open our modal. How it can be achieved? Apparently, jQuery comes with an elegant and concise solution. This is an easy one. Our storage objects have a sync method, which returns a Promise . What is a promise? Promise is an object which is returned when there is a process waiting for its completion . For example, $.ajax method returns a Promise. Promises can be rejected or resolved - and you can register callbacks to each case using success and fail methods of each Promise object. Here‚Äôs an example: You can register as many callbacks to Promises as you want. What‚Äôs more, even if you register a success callback after the Promise is resolved it‚Äôll be fired immediately. The same goes for rejecting and fail callbacks. We can‚Äôt resolve and reject Promises by ourselves. We can only register callbacks to it. It makes sense for a process like an AJAX request - jQuery handles this stuff for us and we‚Äôre only interested in getting our hands on data (or not, if error occurs). There must be a way to control this process - but I‚Äôll write about it later. So, our final solution looks like this: So far so good. Now we need to take care of the more complex thing - waiting for dependent microapps to be ready. In architecture I have in my project applications communicate only through events - there is an @eventBus object which has @publish(eventName, data...) and @on(eventName, callback) methods to publish and listen to events. Since I started in Surveys app I had an direct access to storage object which is synced - so I had a nice Promise to register on - in this case I can only listen to an event. So I‚Äôve introduced two new events - assetsStarted and ratingStarted which are published when those applications are ready for interactions. That is not helpful, though. I could‚Äôve made something like this: There is a lot of imperativeness here. And I repeat myself three times here - I consider this solution a hack. But what can I do to improve this code? What‚Äôs less known, jQuery provides us a way to turn any process into a Promise. There are also tools which allows us to work with many promises. I‚Äôll use this approach to implement this code in a cleaner way. We can turn our waiting for start events to a promise, using jQuery.Deferred : jQuery.Deferred is the side that jQuery have when managing our $.ajax calls - it‚Äôs a Promise with resolve and reject methods available. This way we can manually reject or resolve our Assets and Rating promises. promise method returns a real Promise from this object - without an ability to resolve and reject Promise. This is what we pass to our listeners. You can pass a function to jQuery.Deferred constructor - it will be applied to the deferred object itself. The same code could be written as: Any arguments passed to resolve and reject methods of Deferred object will be passed to all success or fail callbacks, respectively. Deferred objects can be rejected or resolved only once - there are methods for making more ‚Äògrained‚Äô notifications from Promises (registering callbacks using progress and triggering those callbacks using Deferred ‚Äòs notify ) but it‚Äôs beyond scope of this post. Ok, so we got promises for our apps already. What we can do now? There is set of tools from jQuery available to work with Promises - pipe , then , when ‚Ä¶ they are all returning another Promises, transformed in a way. The idea is like with Enumerable collections in Ruby - you transform collections using Enumerable methods to achieve different Enumerable s or the result. In our case we‚Äôll use jQuery.when method. It takes a set of Promises and returns a Promise which is resolved if and only if all passed Promises are resolved. If any of promises passed rejects, the whole when rejects. It resolves with data collected from contained promises. Apparently it is exactly what we‚Äôre looking for! That‚Äôs it! Promises are one of the most effective and elegant ways to deal with asynchronous code in JavaScript. They were introduced to deal with anti-pattern called ‚ÄúCallback hell‚Äù - 3-4+ levels of nested callbacks. jQuery already provides us quite powerful implementation of Promises out of the box. You can use it to greatly improve your frontend code! In Node.js promises is widely adopted tool to deal with 'async everything‚Äô approach in their backend code. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-02-24"},
{"website": "Arkency", "title": "The reasons why programmers don't blog", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/02/the-reasons-why-programmers-dont-blog/", "abstract": "I was wondering why only few programmers actively blog, so I asked for reasons on Twitter. Here are the responses. If you‚Äôre a programmer, could you reply to me - why are you not blogging?\n\nIf you are , then please reply, why are you not blogging more? Click here to see all the responses as they appeared in original. Here‚Äôs a short summary of the reasons: Is there any other reason you‚Äôd like to add? It‚Äôs worth noting that this survey may not be a good sample - those are from programmers who don‚Äôt blog, but they do tweet (which on its own is quite interesting). Do you think the reasons can be addressed in any way? Does it make sense for programmers to blog more? I‚Äôll leave you with those questions here :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-02-27"},
{"website": "Arkency", "title": "Concurrency patterns in RubyMotion", "author": ["Kamil Lelonek"], "link": "https://blog.arkency.com/2014/08/concurrent-patterns-in-rubymotion/", "abstract": "The more we dive into RubyMotion, the more advanced topics we face with. Currently, in one of our RubyMotion applications we are implementing QR code scanning feature. Although it may seem already as a good topic for blogpost, this time we will focus on concurrency patterns in RubyMotion , because they are a good start for any advanced features in iOS like this 2D code recognition. From the very beginning, it‚Äôs worth to quote RM documentation : Unlike the mainstream Ruby implementation, race conditions are possible in RubyMotion, since there is no Global Interpreter Lock (GIL) to prohibit threads from running concurrently. You must be careful to secure concurrent access to shared resources. Although it‚Äôs a quotation from official documentation, we experienced that despite of using GIL, we still can fall into race condition. So before any work with concurrency in RubyMotion, beware of accessing shared resources without preventing them from race condition. RubyMotion wraps the Grand Central Dispatch (GCD) concurrency library under the Dispatch module. It is possible to execute both synchronously and asynchronously blocks of code under concurrent or serial queues .\nAlthough it is more complicated than implementing regular threads, sometimes GCD offers a more elegant way to run code concurrently . Here are some facts about GDC: A Dispatch::Queue is the fundamental mechanism for scheduling blocks for execution, either synchronously or asychronously . Here is the basic matrix of Dispatch::Queue methods . Rows represent whether to run in blocking or non-blocking mode, columns represent where to execute the code - in UI or background thread. .main.sync - it‚Äôs actually equivalent to regular execution. May be helpful to run from inside of background queue. .main.async - schedule block to run as soon as possible in UI thread and go on immediately to the next lines. When can this be helpful? All view changes have to be done in the main thread. In the other case you may receive something like: To update UI from background thread: .new('arkency_queue').async - operations in background thread ideal for processing lots of data or handling HTTP requests. .new('arkency_queue').sync - may be use for synchronization critical sections when the result of the block is not needed locally. In addition to providing a more concise expression of synchronization, this approach is less error prone as the critical section cannot be accidentally left without restoring the queue to a reentrant state. Conceptually, dispatch_sync() is a convenient wrapper around dispatch_async() with the addition of a semaphore to wait for completion of the block, and a wrapper around the block to signal its completion. These functions support efficient temporal synchronization, background concurrency and data-level concurrency. These same functions can also be used for efficient notification of the completion of asynchronous blocks (a.k.a. callbacks). This time, some facts about queues: Queues are not bound to any specific thread of execution and blocks submitted to independent queues may execute concurrently. Singleton? Dispatch module has only one module method which is once . It executes a block object once and only once for the lifetime of an application. We can be sure that whatever we placed inside passed block, will be run exactly one time in the whole lifecycle. Sounds like singleton now? This technique is recommended by Apple itself to create shared instance of some class. In native iOS it may look like: which is actually the same thing as: As you can see, the dispatch_once function takes care of all the necessary locking and synchronization. Moreover it is not only cleaner, but also faster (especially in future calls), which may be an issue in many cases. In RubyMotion implementation may be as follows: { @instance ||= new } block is guaranteed to be yielded exactly once in a thread-safe manner to crate singleton object. Concurrency in native iOS, or rather C, is far more advanced than in RubyMotion. From the other side, Dispatch module offers a lot of features too, more complicated than we described here. It‚Äôs worth to get familiar with these methods so that we can better manage code execution. It‚Äôs also worth to take a look at BubbleWrap Deferable module , which wraps some Dispatch::Queue operations in even more elegant way. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2014-08-19"},
{"website": "Arkency", "title": "My favorite ActiveSupport features", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/02/my-favorite-activesupport-features/", "abstract": "This is short and not so comprehensie list of my favorite\nActiveSupport features. Useful in ad-hoc scripts when you get primitive data from file or API. Together\nwith map they give you some_array.map(&:second) to get what you want. But with ruby 2.0 keyword arguments aka kwargs already present\nand ruby 1.9.3 support ending in February 2015 you should probably migrate to it: Remember that you can add false as a second argument to avoid nil s\nin the arrays. Wraps its argument in an array unless it is already an array. Nicely explained in the documentation why it is better then usual ruby idioms ActiveSupport overwrites to_s on my types to use\nits to_formatted_s version instead (especially when\narguments provided) The class behind the little trick: I hate time zones, but I love ActiveSupport::TimeWithZone . It is so easy to use. And I love that it can properly compare times from different timezones\nbased on what moment of time they point to. Returns a hash that includes everything but the given keys. Except that I always think that this method is called #without . Slice a hash to include only the given keys. If only I could remember that this method is not named #only :) is equivalent to This is particularly useful for default values. Reading this is way easier for me, compared\nto Forwardable#def_delegator . With prefix and allow_nil options that you can use with it,\nit probably solves 95% of my delegation cases. Never check for nil or empty string again. Too long for our short blogpost but check out instrumentation API You can use it to generate and verify signed messages That‚Äôs it. You can browse entire ActiveSupport codebase quickly and easily at github If you liked it, you may also enjoy Hidden features of Ruby you may not know about Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-02-25"},
{"website": "Arkency", "title": "How to split routes.rb into smaller parts?", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2015/02/how-to-split-routes-dot-rb-into-smaller-parts/", "abstract": "Each application created using Ruby on Rails framework has a routing engine and config/routes.rb file where we define routes paths. \nThat file very often becomes very large in the proces of development. Each additional line makes the routing file harder to maintain. \nAlso, searching for specific paths during the development phase becomes increasingly difficult. \nCurrently, I work on an application in which the routing file contains about 500LOC. \nQuite a lot, isn‚Äôt it? The solution is very simple. All you need to do is split the file into a couple of smaller ones. When a request comes, routes.rb file is processed in ‚Äútop to bottom‚Äù order. When the first suitable entry is found, the request is forwarded to the appropriate controller. \nIn case of not finding a matching path in the file, Rails will serve 404. Because of the possibility of sorting the order of loading files, we can define the priorities for our namespaces. The following example is a short part of routes.rb: There are some some default namespace (with /home, /about, /login routes) and four other namespaces. \nThese namespaces define nicely existing contexts in our application. So, they are great canditates for division to other files. \nSo we have created api.rb, admin.rb, messages.rb and orders.rb. Usually, I put the separated files in config/routes/ directory which is created for this purpose.\nNext step is to load above files. We can do this in several ways. In applications based on Rails 3, loading route files from application config is a very popular method . \nFinally, we have to add to our application.rb following line: If you want to have control over the order of loading files you can do this this way: However, since version 4 of Ruby on Rails if you attempt to add the above line application will throw an exception. \nThe Rails 4 does not provide [‚Äòconfig/routes‚Äô] key in Rails::Engine. There is another option that works in both versions of the framework. \nHere we have another solution: It allows us to add a new method to ActionDispatch::Routing module which helps as to load paths. \nRails 4 initially had a similar solution but it has been removed. \nYou can check out the git commit here: https://github.com/rails/rails/commit/5e7d6bba79393de0279917f93b82f3b7b176f4b5 Splitting files is a very simple solution for improving the condition of our routes files and developers lives. In this regard, this solution makes life easier in this. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-02-27"},
{"website": "Arkency", "title": "How to get anything done? - 4 tips", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/03/how-to-get-anything-done/", "abstract": "If you have low self-esteem and constantly worry (like I do), your own thoughts might be the\nbiggest obstacle preventing you from getting great things done.\nHere are 4 mental tips that I use to overcome my objections. Don‚Äôt think about finishing a project. About the end of it. You will get there. Just start working . Because this is not how things get done most of the time: This is: If you start enough times, and continue making progress every time, you will\neventually finish the task. So notice how you perceive time .\nand just start working on the next task. Without the promise or expectation that you will finish\nanything in current session. Awful lot of people get stressed about their work before it even begins. Often in\nthe mornings, during commuting, before you even open the door of your office.\nIf you are like me, you might even get stressed the evening of one day before (especially on\nSundays). Do i have the skills to do this task? Will I have to use the library of framework\nI am not yet fully comfortable with? How long will it take? Will I end up writing\ncode that I am not fond of? Maybe even ashamed? Will my coworkers like it? Will it\nwork? Will it deploy without any problems? When we worry we tend to avoid the job and procrastinate . We don‚Äôt want to be\nconfrontend with the fact that it might not work, that are we not as good as we think\nwe are. We don‚Äôt want our ego to get hurt. Don‚Äôt run away from those feelings. They are real. Try to accept them. Yes, I worry. Yes, I am not sure if I can do it. Even less sure if the end effect will\nbe pretty and likeable . Accept it, confirm it, don‚Äôt deny it. But, start working .\nWith the negative feelings, alongside with them. The only thing that makes them fade\naway is progress. aka stack the bricks . Your success or\nfailure don‚Äôt depend on one big thing. Your success or failure is compound of thousands\n(if not millions) of micro-failures and micro-successes. It doesn‚Äôt matter what your job is. Even if you are working at your dream company\n(what is it nowdays? Tesla, Apple, Google, Airbnb ‚Ä¶?) your job will still consist of\nsome amount of repetitive tasks . Can you learn to love it? Could it be enjoyable like\nin games ? Or could you do them anyway, despite your feelings? Because you care . You feed\nyour animals even when you are tired and exhausted. You make sure that your kids are safe\nand loved even at the end of a very long day. Because you care. If you are working on things that matter to you, that you care about, you can learn\nto love the grind . Why are you doing this blogpost, this side-project? Do you remember?\nIs that important to you? More than one year ago I wrote a rails related blogpost that is\nstill visited by more than 4K developers every month. That‚Äôs a lot in my world. It set the bar\nvery high for me. I am still trying to beat it. But when I worry about pageviews, likes and upvotes on reddit and hackernews, it‚Äôs\nvery hard to ship anything. So I lowered my expectations. I write a blog post to help\na single person . If I helped one person, then I am good. It was worth writing, worth\nspending time, worth the effort. Sure, from time to time you write a post or create a product that helps hundreds or thousands. But you might also get 4 upvotes, dozen of visits and\nthat‚Äôs it. Detach from results doesn‚Äôt mean to ignore the data. You can use it to get\nbetter, to improve (especially product). But your code\nis not you, and your upvotes are not you . That might sound obvious . But I still need to \nremind myself about it. You might even start to believe that you are not helping anyone according to some random\ninternet comments.\nBut then one year after a not-much-noticed blogpost, someone writes you an email to say thank you and ask a very good question regarding the topic. Remember, most of posts that were helpful to your\nreaders won‚Äôt get a retweet, comment or upvote . I don‚Äôt jump quickly myself into saying thank you for every blogger that helped me today finish my task. Perfectly normal.\nBut the appreciation is there. Few weeks ago in a shopping-center I got a wristband from anti-cancer group for filling out a survey.\nIt says best time for action is now . I don‚Äôt want to wait till cancer to get myself\ninto doing something meaningful. This wristband actually changed something in me for the\nbetter. I do more. Now. That‚Äôs it, going back to writing a book . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-01"},
{"website": "Arkency", "title": "Blogging - start from the middle", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/03/blogging-start-from-the-middle/", "abstract": "Time constraints seems to be the main reason for not blogging more often. Today, I‚Äôd like to teach you a technique which reverses the situation - the blog post is created in your available time. Start from the middle I call this technique - ‚Äú start from the middle ‚Äù. The point is to start your post with the main message. Find the MVP of your blog post. What‚Äôs the minimal content to write and still get the message through? What‚Äôs the minimal thing you need to write to help your readers in the chosen topic? Sometimes, it may be just 3-4 sentences. Sometimes it‚Äôs a picture with a short explanation. It can also be a piece of code. It can be just a list of points. Shippable The point is to make it consistent and shippable . If I run out of time, I want to ship (publish) the post and still deliver value. It doesn‚Äôt need to be perfect. It probably won‚Äôt be. This approach is not about being perfect. If you‚Äôre a perfectionists, you may have hard time applying it in practice. Does it sound similar to what you‚Äôve done before? The agile approach to coding is very similar. Release early, release often. Start with something minimal which works and then iteratively improve it. Timeboxing The start from the middle technique works well in combination with timeboxing . Timeboxing is reversing the situation with lacking time for blogging. You start with the constraint - you only have N minutes to ship this blog post. You no longer have the problem of lacking time. You now have a different challenge - choosing the scope of the blog post. That‚Äôs often what product owners, PMs and clients need to do when it comes to giving work to developers. They need to scope the feature so that it‚Äôs not too expensive cost-wise and time-wise. As a developer you‚Äôre usually presented with the scope and your job is to make it under the available time. In case of blogging this means that you‚Äôre playing both roles. You are the product owner. You are the developer. Deal with it :) What‚Äôs a good time constraint ? Depends on your availability. I experiment with 30 minutes, but I may have it easier, after writing > 100 blog posts. 60 minutes may be a good start. As for perfectionism - this may be hard to some of you. Your blog post won‚Äôt be perfect. It‚Äôs similar to the features you sometimes ship. Product owners need to make such decisions - what to omit. It‚Äôs your role now - decide what is the thing that you‚Äôre going to explain in the blog post as first. By this decision, you‚Äôre also deciding what‚Äôs not included - thanks to the time constraints. Keeping it shippable is the key here. Even if you timebox the writing to 120 minutes, try to keep it shippable from the beginning. If you‚Äôre using a tool like nanoc (as I‚Äôm doing it now), then the post is in the repository. Commit often. TODO Keep your TODO list for the blog post. Prioritise the TODO items. Start from the top. If you have more time, just keep going with the tasks. What can be a blogging task? For this blog post, I wasn‚Äôt sure if I cover all the points, so I split them into points: As you see, I‚Äôve managed to do almost all of them. I didn‚Äôt finish the last ones - dealing with screenshots and pictures takes more minutes and I put it at the end - no time available for that. BTW, It‚Äôs an interesting situation. Now, when I present you the list of points, you see that this post is unfinished. What if I didn‚Äôt present the list to you? The thing is, as authors we know more than our readers. It‚Äôs only our mind that makes us think it‚Äôs unfinished - because we started with some bigger vision. Our readers usually don‚Äôt know the vision. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-01"},
{"website": "Arkency", "title": "You don't need to wait for your backend: Decisions and Consequences", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2015/03/you-dont-need-to-wait-for-backend-decisions-and-consequences/", "abstract": "Photo remix available thanks to the courtesy of mripp . CC BY 2.0 As front-end developer your part is often to provide the best possible experience for your application‚Äôs end-users . In standard Rails application everything is rather easy - user clicks on the submit button and waits for an update. User then sees fully updated data. Due to async nature of dynamic front-ends it is often missed what happens in the ‚Äòmid-time‚Äô of your user‚Äôs transaction - button is clicked and user waits for some kind of notification that his task is completed. What should be displayed? What if a failure occurs? There are at least two decisions you can take to answer those questions. The most common solution is to update your front-end if and only if backend notifies us that particular user action is successful . It is often the only choice to solve consistency problem - there are actions that have effects we unable to compute on front-end due to lack of required information. Consider sign in form - we can‚Äôt be sure user signed in or not before the backend finishes its logic. Implementation often is rather straightforward - we just make some AJAX call, wait until a promise is resolved (you can read about it in more detail here ) and then perform an update to your views. Imagine you have a simple to-do list application - one of its functions is that users can add a task to it. There is an event bus where you can subscribe to events published by your view. Your data is stored within the ReadModel object - you can ask it to return current list of tasks and update it via addTask method. Such updates automatically updates the view. Your Dispatcher ( Glue ) class can look like this: Here you wait for your addTask command to finish - it basically makes a POST request to your Rails backend and the task data is returned via JSON. You definitely saw this pattern many times - it is the most ‚Äòcommon‚Äô way to handle updates. It aligns well with Rails conventions - let‚Äôs take a small part of the code introduced above: As you may see, ID of the given task is returned inside JSON response. Basically such pattern is provided by convention in a typical Rails app - primary keys are given from your database and such knowledge must be propagated from a backend to a frontend. Handling such use cases in ‚ÄúWait for backend, then update‚Äù method requires no change in Rails conventions at all. All front-end data is persisted - there is no problem with ‚Äòbogus‚Äô data that may be introduced only on front-end. That means you can only have fewer data than on backend at any time. Developers are forced to provide and maintain different kind of visual feedback - waiting without a visual feedback is not enough. If completing an action needs a considerate amount of time, providing no visual feedback would force an user to repeat his requests (usually by hitting button twice or more) because such time would be misinterpreted as ‚Äúapp doesn‚Äôt work‚Äù. That means we need to implement yet another solution - the most common ‚Äúhacks‚Äù here is disabling inputs, changing value of the button to something like ‚ÄúSubmitting‚Ä¶‚Äù, providing some kind of ‚ÄúLoading‚Äù visual indicator etc. Such ‚Äòtemporal‚Äô solution must be cleaned up after failure or success. Errors with not cleaning up such ‚Äòtemporal‚Äô visual feedbacks is something that users directly see and it is often very annoying for them - they just see that something ‚Äúis broken‚Äù here! It is hard to go with ‚Äòeventual consistency‚Äô with this approach -  and with today requirements it‚Äôs a big chance you‚Äôd want to do so. If you implement your code with ‚Äúwait for backend, then update‚Äù it can be hard to make architecture ready for ‚Äúoffline mode‚Äù, or to defer synchronisation (like with auto-save feature). You can listen for ajaxSend ‚Äúevents‚Äù to provide the simplest visual feedback that something is being processed on backend. This is a simple snippet of code you may use to your needs (using jQuery): We bind to ajaxSend and ajaxComplete ‚Äúevents‚Äù to keep track of number of active AJAX transactions. You can then query this variable to provide some kind of visual feedback. One of the simplest is to provide an alert when the user wants to leave a page: You can take the another approach to provide as fast feedback for an end-user as possible. You can update your front-end and then wait for backend to see whether an action succeeds or not . This way your users get the most immediate feedback as possible - at the cost of more complex implementation. This approach allows you to totally decouple the concern of doing an action from preserving its effects. It allows you a set of very interesting ways your front-end can operate - you can defer the backend synchronisation as long as you like or make your application ‚Äòoffline friendly‚Äô, where an user can take actions even if there is no internet connection. That‚Äôs the way many mobile applications work - for example I can add my task in Wunderlist app and it‚Äôll be synced if there will be an internet connection available - but I have my task stored and can review it any time I‚Äôd like. There is also a hidden effect of this decision - if you want to be consistent with this approach you‚Äôre going to put more and more emphasis on front-end, making it richer. There is a lot of things you can do without even consulting backend - and most Rails programmers forget about it. With this approach moving your logic from backend to front-end comes naturally. In this simple example there is little you have to do to make implementation with this approach: As you can see, there are little changes with this approach: The most interesting thing is that you can take @commands call and move it to completely different layer. You can add it to a queue of ‚Äòto sync‚Äô commands or do something more sophisticated - but since there is immediate feedback for an user you can make it whenever you like. To avoid a code smell with creating methods just to compensate failures, you can refactor your commands to a Command pattern . You can instantiate it with data it needs and provide an undo method you call to compensate an effect of such command. Here is a little example of this approach: That way you ‚Äòenhance‚Äô a command with knowledge about ‚Äòundoing‚Äô itself. It can be beneficial if logic you need to implement is valid only to compensate an event - this way your other code can expose interface usable only for doing real business actions, not reversing them. In sophisticated frontends it is a good step to build your domain object state from domain events. This technique is called ‚Äúevent sourcing‚Äù and it aligns well with idea of ‚Äòreactive programming‚Äô. I just want to signal it is possible - RxJS is a library which can help you with it. Decisions you can make to handle effects of user actions can have major consequences with your overall code design . Knowing those consequences is a first step to make your front-end maintainable and more usable for your users. Unfortunately, there is no silver bullet. If you are planning to make your front-end richer and want to decouple it from backend as much as possible it is great to try to go with ‚Äúupdate first‚Äù approach - it has many consequences which ‚Äúpushes‚Äù us towards this goal. But it all depends on your domain and features. I hope this post will help you with making those decisions in a more conscious way. Do you have some interesting experience on this field? Or you have a question? Don‚Äôt forget to leave a comment - I‚Äôll be more than happy to discuss with you! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-02"},
{"website": "Arkency", "title": "Extract a service object in any framework", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/03/extract-a-service-object-in-any-framework/", "abstract": "Extracting a service object is a natural step in any kind of framework-dependent application. In this blog post, I‚Äôm showing you an example from Nanoc, a blogging framework. The difference between a library and a framework is that you call the library, while the framework calls you. This slight difference may cause problems in applications being too dependent on the framework. Another potential problem is when your app lives inside the framework code. The ideal situation seems to be when your code is separated from the framework code. The ‚ÄúExtract a service object‚Äù refactoring is a way of dealing with the situation. In short, you want to separate your code from the framework code . A typical example is a Rails controller action. An action is a typical framework building block. It‚Äôs responsible for several things, including all the HTTP-related features like rendering html/json or redirecting.\nEverything else is probably your application code and there are gains in extracting it into a new class. We‚Äôre using the nanoc tool for blogging in our Arkency blog. It serves us very well, so far. One place, where we extended it was a custom Nanoc command . The command is called ‚Äúcreate-post‚Äù and it‚Äôs just a convenience function to automate the file creation with a proper URL generation. Here is the code: It was serving us well for over 3 years without any change. I‚Äôm extracting it to a service object, mostly as an example to show how it would work. I‚Äôve created a new class and passed the arguments into it. While doing it, I‚Äôve also extracted some small methods to hide implementation details. Thanks to that the main algorith is a bit more clear. There‚Äôs more we could do at some point, like isolating from the file system. However, for this refactoring exercise, this effect is enough. It took me about 10 minutes to do this refactoring. I don‚Äôt need to further changes now, it‚Äôs OK to do it in small steps. It‚Äôs worth to consider this techniqe whenever you use any framework, be it Rails, Sinatra, nanoc or anything else that calls you. Isolate early . If you‚Äôre interested in such refactorings, you may consider looking at the book I wrote: Fearless Refactoring: Rails Controllers . This book consists of 3 parts: Thanks to that you not only learn how to apply a refactoring but also know what are the future building blocks. The building blocks include service objects, repositories, form objects and adapters. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-02"},
{"website": "Arkency", "title": "Use your gettext translations in your React components", "author": ["Jakub Kosi≈Ñski"], "link": "https://blog.arkency.com/2015/03/use-your-gettext-translations-in-your-react-components/", "abstract": "Photo remix available thanks to the courtesy of miguelb . CC BY 2.0 In one of our projects, we are using gettext for i18n. We were putting Handlebars x-handlebars-template templates directly in Haml templates to provide translated views for frontend part - all translations were made on backend. Recently we have rewritten our frontend to React and decided not to use ruby for translations anymore. During rewrite, we created an simple API endpoint on backend that was returning translation for given key and locale and mixed it with React component that was asynchronously getting translations. The code was pretty simple and was using jQuery promises : This approach was good for quick start but did not scale - it required multiple ajax calls to backend on each page render, so we decided to find something better. After some research we have chosen i18next - full-featured i18n JS library that have pretty good compatibility with gettext (including pluralization rules). With i18next you can easily return translations using almost the same API as in gettext: This library also supports variables inside translation keys: It has also sprintf support: And supports plurar forms (even for languages with multiple plural forms): There are much more configuration options and features in i18next library so you‚Äôd better look at their docs . To convert our gettext .po files to json format readable by i18next, we‚Äôre using i18next-conv tool and store generated json in public/locale directory of our Rails app. Here‚Äôs a simple script we‚Äôre using during deploy to compile JS translations ( script/compile_js_i18n ): To use it, just run script/compile_js_i18n in your app‚Äôs root directory (but make sure you have node & npm installed and \"i18next-conv\": \"~> 0.1.4\" line in your package.json file before). What‚Äôs great about i18next-conv, it has built-in plural forms for many languages. i18next has also a bunch of initialization options . Here‚Äôs our setup that works in our app: Some of those initialization options need more explanation. First, we‚Äôre using variable interpolation in our gettext translations. They have format different than i18next defaults ( %{variable_name} instead of __variable_name__ ) so we had to set interpolationPrefix and interpolationSuffix . Second, since we‚Äôre using english translations as gettext msgids (usually full sentences), we need to change key and namespace separator ( keyseparator and nsseparator options). The default key separator in i18next is a dot ( . ) and namespace separator is a colon ( : ) and that was making most of our translations useless, since they were not translated at all when translation key contained . or : . We also had to change resGetPath since we decided to store our json in public/locale (e.g. public/locale/en/acme.json for acme namespace). \nIn our app, we wrapped initialization code in i18n CommonJS module for easier use: With this helper, you don‚Äôt need to initialize library each time you use it, it would be initialized only once, on first use. By default, i18next retrieves translations asynchronously, using ajax get requests to the endpoint set in resGetPath when you set locale using i18n.setLng method. setLng method accepts locale as first parameter and optional callback that would be fired after loading translations. You can make use of it in your‚Äôs app bootstrap code: Having this setup we can just use regular i18next API in our React components: i18next has much more features and integrations, including localStorage caching, jQuery integration and ruby gem that can automatically rebuild your javascript translations from YAML files. Have a look at their docs for further information. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-01"},
{"website": "Arkency", "title": "You get feature toggle for free in event-driven systems", "author": ["Jakub Rozmiarek"], "link": "https://blog.arkency.com/2015/03/you-get-feature-toggle-for-free-in-event-driven-systems/", "abstract": "Event-driven programming has many advantages. One of my favourite ones is a fact that by design it provides feature toggle functionality.\nIn one of projects we‚Äôve been working on we introduced an event store. This allows us to publish and handle domain events. Below you can see an example of OrderEvents::OrderCompleted event that is published after an order has been completed: After this fact take place, we want to deliver an email to the customer. We utilize an event handler to do it. To make the handler work we need to subscribe it to the event. We subscribe handlers to events in a config file like this: When the event is published it is stored in a stream and for each of subscribed handlers ‚Äúperform‚Äù class method is called with the event passed as an argument: Happy customer has just received a confirmation email about their order. Now if we want to turn email delivery off for some reason, we can do it easily by unsubscring the handler - in this case by removal of the handler line from the config file. \nAs you can see it doesn‚Äôt require any additional work to implement feature toggle - it‚Äôs available out of the box when using event store. It can be very handy, for example when business requirements change or when we develop a new feature - we can safely push the code and don‚Äôt worry if it isn‚Äôt fully functional yet. As long as the handler is not subscribed to the event it won‚Äôt be fired. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-06"},
{"website": "Arkency", "title": "Configurable dependencies in Angular.js", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2015/03/configurable-dependencies-in-angular-dot-js/", "abstract": "Photo available thanks to the courtesy of streetmatt . CC BY 2.0 Angular.js is often a technology of choice when it comes to creating dynamic frontends for Rails applications. As every framework, Angular has it‚Äôs flaws - but one of the most interesting features it has is built-in powerful dependency injection mechanism. Compared to Rails, it is a great advantage - to achieve similar results you would need to use external gems like dependor . Here you have this mechanism out of the box. In my recent work I needed to learn Angular from scratch. After learning about providers mechanism, my first question was: Can I have a dependency and configure which implementation I can choose? Apparently, with a little knowledge about JavaScript and Angular it was possible to come with a very elegant solution of this problem. Why would I need this feature? - you may ask. The most important advantage you‚Äôd have from this feature is that you don‚Äôt need to touch the code of your application if you want to substitute your dependency - all you need to do to change implementation is to modify one config variable and you‚Äôre done. With switchable implementations you can achieve: First of all, create your Angular module: Let‚Äôs say you want to show dummy data on frontend just for quick prototyping, and then switch to a real AJAX requests to fetch it. Let‚Äôs create our implementations: $q is used here to create a consistent interface of a Promise to work with both implementations in the same way. Next step is to create a configuration variable to switch implementations as needed. This is the simplest approach - you may have more sophisticated rules to switch implementations (like user-based): You are nearly done. Now, to the heart of this solution - a factory (you canread about it more here ) will be used to encapsulate logic of implementation switch. Notice you need to pass all implementations as separate dependencies - you can easily omit this step if you implement your dependency implementations as plain JavaScript prototypes (use of class notation in CoffeeScript is something I‚Äôd recommend) and make this code reachable within a closure where the factory is defined - you can even inline those implementations inside the factory‚Äôs body. I like approach with plain objects a lot - if I can decouple from a framework, I‚Äôd happily do so every time I have an occasion for it. The full code looks like this: Dependency injection is a powerful technique to make working with your code much easier. I‚Äôm really happy that Angular supports this way of doing things out of the box - I can‚Äôt wait to see more opportunities of wise usage of this framework features. With such small amount of code you can achieve great gains now. I‚Äôm really curious if you tried similar techniques before. How your implementations look like? Is this implementation is a case of the NIH principle? If you‚Äôd like to discuss about it, leave a comment! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-09"},
{"website": "Arkency", "title": "Blogging: your English is good enough, but", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/03/blogging-your-english-is-good-enough-but/", "abstract": "English may be one of the reasons you‚Äôre not blogging. Here are some tips to change this thinking. TLDR: You English is good enough, don‚Äôt worry. Your English doesn‚Äôt need to be perfect to blog. Look at this blog and our blog posts. We all here don‚Äôt speak English natively. We make mistakes. We make typos. We make silly grammar errors. Luckily, you don‚Äôt need to hear our English too often as it‚Äôs the typical Polish accent (if you really want to hear us, go to our: Rails Refactoring podcast ). As for writing - it‚Äôs all about sending your message . It‚Äôs only a tiny amount of people who will be bothered about your English. Often they are helpful and even fix your mistakes. Let me present some simple techniques to improve your English for the purpose of blogging. For code-less blogging, I use iaWriter. It has a built-in dictionary checker which is good enough. This usually means I don‚Äôt need to do a special round of typo checks. When you are about to publish the blog post, make a special round of a/the check. During this round focus on all of the potential a/the mistakes. If you‚Äôre like me - you miss some of those ofen. This is probably the easiest thing to do to make your English look more like it‚Äôs ‚Äúproper‚Äù. When in doubt, write short sentences . I know this temptation to write a very long sentence which shows how great my English is, so that I‚Äôm almost like my London friends, however this is very difficult and often results in unparsable blobs of text to anyone else apart from you - thus your message doesn‚Äôt get through and that‚Äôs one of the main goals for blog posts, would you agree? See what I did here? ^^ Let me now try the above with the ‚Äú Refactor to short sentences ‚Äù technique. I know this temptation to write a very long sentence. A very long sentence often shows how great my English is. Such sentence make me look like one of my London friends. Looking like your London friend is very difficult. It often results in unparsable blobs of text to anyone else apart from you. Unparsable blobs of text don‚Äôt get trough. That‚Äôs one of the main goals for blog posts right. Would you agree? This technique is simple - grab the subject of the sentence. Finish the first part with a dot. Put the subject at the beginning of the next sentence. It‚Äôs a duplication. We, as programmers don‚Äôt like duplications. This kind of duplication can make miracles, though. It‚Äôs actually more than denormalisation than duplication. There‚Äôs many techniques for writing a better English. I‚Äôve presented just some of them here. I chose the ones which may make the biggest impact at the lowest cost. Let me finish by saying - I know how my English sucks. Separating my English from the message I want to send was a huge unblocking point for me. I‚Äôm very sorry to all the people who think it‚Äôs terrible. I was involved in writing 2 books. I like to believe that they bring good value to the buyers, despite my English :) When writing books, though, I do several other rounds of checks. The chapters are reviewed by many people before they‚Äôre ‚Äúreleased‚Äù. Fearless Refactoring: Rails Controllers Developers oriented project management Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-09"},
{"website": "Arkency", "title": "Fast introduction to Event Sourcing for Ruby programmers", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2015/03/fast-introduction-to-event-sourcing-for-ruby-programmers/", "abstract": "Many applications store a current state in these days. Although there are situations where we want to see something more than a current information about our domain model. \nIf you feel that need Event Sourcing will help you here. The Event Sourcing is an architectural pattern which allows us to keep information about object‚Äôs state as a collection of events. \nThese events represent modifications of our model. If we want to recreate current state we have to apply events on our ‚Äûclean‚Äù object. Domain Events are the essence of whole ES concept. We use them to capture changes on model‚Äôs state. Events are something that has had already happened. Each event represent one step of our model‚Äôs life. \nThe most important feature is that every Domain Event is immutable. This is because they represent domain actions that took place in the past. We should not modify persisted event.\nEvery change has to be reflected in model‚Äôs state. Events should be named as verb in past tense. The name should represent Ubiquitous Language used in project. For example CustomerCreated , OrderAccepted and so on. \nImplementation of event it is very simple. Here I have an example created by one of my team-mates in Ruby: As we can see It is only a data structure with all needed attributes. (Example solution has been taken from here ) Event Sourcing approach events are our storage mechanism. The place where we keep events is called Event Store. \nIt can be everything like a relational DB or NoSQL. We save events as streams. Each stream describe state of one model (Aggregate). \nTypically, event store is capable of storing events from multiple types of aggregates. We save events as they happened in time. This way we have complete a log of every state change ever. \nAfter all we can simply load all of the events for an Aggregate and replay them on new object instance. This is it. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-09"},
{"website": "Arkency", "title": "Explaining Greg's Event Store", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2015/03/explaining-gregs-event-store/", "abstract": "Event Store is a domain specific database for people who use the Event Sourcing pattern in their apps. It is a functional database which based on a publish-subscribe messages pattern. Why functional? \nIt uses a functional language as its query language. In ES It is the Javascript. I will say something more about this later. The lead architect and designer of Event Store is Greg Young who provides commercial support for the database.\nI decided to create this two-part tutorial to bring the idea closer to you. I will describe issues related to Event Store in the first part and I will present some simple examples of the ES usage in the second one. All you have to do is download the latest release from here and run one command. That is all. The Event Store runs as a server and you can connect to it over HTTP or using one of the client APIs. If It run you can access to the dashboard on http://127.0.0.1:2113 (default credentials login: admin, pass: changeit). You will find a lot of useful information there but it is material for another post ;). You can connect to an Event Store over TCP or HTTP. Which one is better? Of course it depends on your needs. TCP is strongly recommended for a high-performance environment. There is also a latency increase when using HTTP. We will push events to the subscribers in TCP variant. \nUsing HTTP subscribers will pool to check events availability what is less effective. Additionally the number of supported writes is higher in case of TCP. In Event Store documentation we can find following comparison: ‚ÄûAt the time of writing, standard Event Store appliances can service around 2000 writes/second over HTTP compared to 15,000-20,000/second over TCP!‚Äù The Event Store provides a native interface of AtomPub over HTTP. The AtomPub is more scalable for many subscribers and it becomes easy to use the Event Store in heterogeneous environments. It is easier to use if we have to integrate with different teams from different platforms. It may seem like HTTP is less efficient at the outset. \nHowever It offer intermediary caching of Atom feeds. It will be useful for replaying streams. Live-only - This kind of subscription allows you to get every event from the point of subscribing until the subscription is dropped. If you start subscribing from event number 200 you will get every event starting from 201 to the end of subscription. Catch-up - A catch-up subscription works in a very similar way to a live-only subscription. There is one difference. You can specify the starting point of your subscribing. For example if your stream has 200 events you can specify starting point at 50. You will get every event starting from 51 to the end of subscription. Projection is very interesting feature. It allows as to query over our streams using Javascript‚Äôs functions. This is why we call the Event Store functional database. I am interested in using projections as a method of building View Models, for example collecting repartitioned data for some reports. I will show you some example of usage in next part but if you look for some more sophisticated examples you can check Rob Ashton‚Äôs series. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-12"},
{"website": "Arkency", "title": "Why use Event Sourcing", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/2015/03/why-use-event-sourcing/", "abstract": "Event Sourcing relies on not storing current state. All of application state is first level derivative out of facts. That opens completely new ways of architecting our applications. There is a lot of reasons to use Event Sourcing. When you browse through Greg Young‚Äôs and other articles & talks you will find most of them. Usually they mention: Every database on a planet sucks. And they all suck it their own unique original ways. Greg Young, Polyglot Data talk For me the biggest advantage is that I could have different data models generated based on domain events stored in Event Store. Having an event log allows us to define new models, appropriate for the new business requirements. That could be not only tables in relational database. That could be anything. That could be a graph data model to store relations between contractors in your system with easy way to find how the are connected to each other. That could be a document database. That could\nbe a static HTML page if you are building newest and fastest (or of course most popular) blogging platform :) As the events represent every action the system has undertaken any possible model describing the system can be built from the events. Event Sourcing Basics at Event Store documentation You might not know future requirements for your application but having an event log you could build a new model that hopefully will satisfy emerging business requirements. And one more thing‚Ä¶ that won‚Äôt be that hard, no long migrations, no trying to guess when something has changed. Just replay all your events and build new model based on the data stored in them. If you are interested in pros and cons of Event Sourcing and another point of view on why to use it read Greg‚Äôs post from 2010 (I‚Äôve said Event Sourcing is not a new thing): http://codebetter.com/gregyoung/2010/02/20/why-use-event-sourcing/ Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-10"},
{"website": "Arkency", "title": "Your solid tool for event sourcing - EventStore examples", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2015/03/your-solid-tool-for-event-sourcing-eventstore-examples/", "abstract": "In this part I will show you basic operations on the Event Store. I sent simple event to a new stream called helloworld . You don‚Äôt have to create a new stream separately. The Event Store creates it automatically during creation of the first event. Using application.json Content Type you have to add the ES-EventType header. \nIf you forget to include the header you will be given an error. It is also recommended to include the ES-EventId . If you leave off that header Event Store reply with a 301 redirect. Than you can post events without the ES-EventId to returned URI.\nIf you don‚Äôt want to add information about event‚Äôs id and type into header you can use application/vnd.eventstore.events Content Type. It allows you to specify event‚Äôs id and type in your request body. To get information about your stream you have to call at http://domain:port/stream/#{stream_name} . I will do simple GET to this resource: You can notice here couple interesting things. You get here all basic information about the stream like id, author, update date and unique uri. The stream is also pageable. You get links to pages. You also don‚Äôt get information about events, only links to each event. \nIf you want to get event‚Äôs details you have to go over each entry and follow link. In my case It will be: Projections allow us to run functions over streams. It is interesting method to collect data from different streams to build data models for our app. There is Web UI to manage projection available at 127.0.0.1:2113/projections . You can create there projection with specific name and source code. After all you can call it using unique URL. Lets check following examples.\nAt the beginning we have to prepare some sample events. I‚Äôve added following events to stream: I also created simple projection to count every type of event in my stream. I called it $counter . It is important to start name of projection from $. If you don‚Äôt do that projection won‚Äôt start. Now you can call above projection using HTTP request: We can do the same with multiple streams. I modified the previous projection to iterate over two separate streams and I added a listener on one more event type. I‚Äôve added new event to orderlines stream: The result of the modification: It was great experience to work with Greg‚Äôs Event Store. Although using cURL isn‚Äôt the best method to experience the ES. We have to create own ruby tool to work with Greg‚Äôs Event Store. After all we are rubyists, right? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-12"},
{"website": "Arkency", "title": "Tracking down unused templates", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/2015/03/tracking-down-unused-templates/", "abstract": "Few days ago, my colleague raised #sci-fi idea. Maybe we could somehow track templates rendered in application to track down which ones aren‚Äôt used? Maybe we could have metrics how often they‚Äôre used? Metrics? That sounds like gathering data using Chillout.io. We have already installed Chillout gem in other project I work at. To know which templates aren‚Äôt used we firstly would like to know which ones are used.\nWe would like somehow to hook into rails internals and increment specific counter (named like counter_app/views/posts/new.html.erb ) when rails renders a template. Well, that sounds hacky . However it‚Äôs good to work with people more experienced in rails - they know about parts of rails, you haven‚Äôt idea about. There exists a module called Active Support Instrumentation . Let‚Äôs read what‚Äôs its purpose: ‚ÄúActive Support is a part of core Rails that provides Ruby language extensions, utilities and other things. One of the things it includes is an instrumentation API that can be used inside an application to measure certain actions that occur within Ruby code , such as that inside a Rails application or the framework itself.‚Äù These are the methods we are looking for! After quick look on table of contents, we can see two hooks which would suit us: render_partial.action_view and render_template.action_view . Both of them return identifier of the template which is full path to the template. Great, now we have to learn how to subscribe to these hooks. Example from the same rails guide : Now let‚Äôs write the code which will track using of our partials. We put it into config/initializers/template_monitoring.rb because we want it to execute only once. As you can probably guess, Chillout::Metric.track(name) is incrementing a counter named template_name .\nThus now every time rails renders a template it notifies Chillout which handles the rest. However, again from previously referenced rails guide, event.payload[:identifier] is an absolute path to the template. That‚Äôs not good - what will happen when we deploy with capistrano new version of our application? In absolute path we have number of release which changes on each deployment. Let‚Äôs change that. Obviously now in our previous code we‚Äôve to change to Great, now we are tracking usage of used templates! We got chillout report and we can read how many each one partial was rendered. And it‚Äôs total opposite of what we wanted to achieve because partials which weren‚Äôt rendered at least once are not present on the list. That‚Äôs going to be pretty chillout-specific. Firstly we need to create container which keeps templates‚Äô counters. We‚Äôre assigning it to Thread.current[:creations] because that‚Äôs place where chillout seeks for container (or creates it, if it‚Äôs uninitialized). Then we need to initialize counters for all templates to 0. We can do that by asking chillout ‚ÄúWhat is counter of template_name now?‚Äù. We do that by fetching container[template_name] . From that moment Chillout will be aware that there exists such counter named template_name . Thus it will show it in reports. In the end whole config/initializers/template_monitoring.rb looks like this: That‚Äôs how we are tracking unused templates in our app. Obviously we can‚Äôt be 100% sure that templates which have counter equal to 0 aren‚Äôt used anywhere. Maybe this template is just very rarely used? But it‚Äôs also very useful information. Now we can discuss that with client. Maybe maintenance of the feature using that template is not worth it? Maybe we could drop it? Note that you could make this not only by using chillout. One of my colleagues did this using plain redis hash. Take a look on Active Support Instrumentation and use it creativly. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-18"},
{"website": "Arkency", "title": "Stream pagination in Greg's Event Store", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2015/03/stream-pagination-in-gregs-event-store/", "abstract": "Every stream in Event Store is represented as a paged feed. This is because reading from streams based on an AtomPub protocol. A paged feed is a set of feed documents where each document contains some part of a whole data. This is very useful solution when the number of information is very large. So basically in the ES reading a stream is a process of collecting events in small portions.\nYou can find some information about this feature in main Event Store‚Äôs documentation but in my opinion It is described very briefly. So this is why I decided to write this blog post. When you get stream data you receive information about links. Each link leads to different a feed page which contains specified number of events. For the purposes of this post I created paginationtest stream with 43 events inside.\n(Creation of events I described in this post ). Lets make a fast request to get some data: Ok what we have here? I called here the stream‚Äôs head . Head page contains the latest stream‚Äôs events. As you can see events (aka entries) are sorted descending . It is very important information that entries are always sorted desc on an every page.\nThere are twenty entries on each page by default. You can modify the number of events per page changing specified link. We do have also the above-mentioned links. I will try to describe them al little bit: I think about feed paging as a pagination on a website. It is more intuitive for me and it allows to understand the whole concept easier.\nIf we would like to get all entries from newest we have to iterate backward over the stream. In the case of my example iterating will looks following: First step: Second step: Third step: To get all events starting from the begin we have to walk forward over whole stream: First step: Second step: Third step: This is only a simple example of iteration over whole stream. But using ES‚Äôs streams is more flexible. You can easily modify url parameters to get more entries per page or you can start from different place in your stream.\nIf you modify the number of events on a page that Event Store will calculate for you links in response. This solution is very useful in case of parallel pagination. For example\nif various users start paginate in different places on stream that the structure of pages is different. Despite the move through the same stream. This is very useful to easier cache user‚Äôs events. Important things to remember: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-24"},
{"website": "Arkency", "title": "Why saving data using MongoHQ takes so long?", "author": ["Robert Krzysztoforski"], "link": "https://blog.arkency.com/2015/03/why-saving-data-using-mongohq-takes-so-long/", "abstract": "Recently in one of the projects we‚Äôve taken over, which uses MongoDB and is hosted on Heroku, we have been asked to speed up an importing file process.\nIt seemed to be a simple thing but without a few tricks we wouldn‚Äôt have made it. I need to say a few things first, we‚Äôre using a standard Heroku app configuration and MongoHQ addon. \nIt‚Äôs been enough for our needs so far. The question is why import of 10MB file takes so long? It took around 1h. You would say that the problem is in the code, but it‚Äôs a half-truth. Below you can see an example of a service to import files with BIM (Building Information Model) objects.\nEvery BIM object has properties. The properties may be duplicated. The important thing is that we have 2 loops here.\n10MB file may include 10k objects and 5k properties, so the service has to save 15k records in DB. We can imagine where the problem is. 15k requests to DB isn‚Äôt a small number, especially when we‚Äôre using MongoHQ and Heroku. Usually DB server is in a different location than webserver, so the latency isn‚Äôt so small like it‚Äôs on local environment. In our case the difference between Heroku and local environment was quite big. On development we were able to import the file in 7 min, on Heroku in 1h. We can use MongoDB insert method, however insert doesn‚Äôt run validations and it‚Äôs on our hands to make sure that our model is correct. We can compare insert with storing raw data in DB. There is the last thing to remember, before we store data, we have to add fields like updated _ at and created _ at to attributes. Thanks to solution presented above we were able to reduce the number of requests from 15k into 5k, but we can make it even better. Be aware that part of the code responsible for saving properties isn‚Äôt optimal. We could reduce find_or_initialize_by calls. To do that, we can use some kind of cache which stores only unique properties. Thanks to BimPropertyUniqCache class we were able to avoid unnecessary requests. Remember that access time to MongoDB locally is faster than on Heroku. You can easly bypass it by using mass insert. Unluckily by insert we‚Äôre skipping validations and we need to validate records before. We‚Äôre forced to write more code, but processing time is significantly decreased. Eventually importing a 10MB file takes around 1 min. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-23"},
{"website": "Arkency", "title": "How to use Greg's Event Store from ruby", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2015/03/how-to-use-gregs-event-store-from-ruby/", "abstract": "I one of a previous blog‚Äôs post I mentioned that we have to create a some tool to communicate with Greg‚Äôs ES. We did it. We created HttpEventstore gem which is a HTTP connector to the Greg‚Äôs Event Store. The reason of creating such tool was that in our projects we already use Event Sourcing and we experiment with Greg‚Äôs tool. To communicate with ES you have to create instance of HttpEventstore::Connection class. After configuring a client, you can do the following things. OR You can pass event‚Äôs data as a Hash or Struct . As you can see in above example event_id is optional parameter. If you don‚Äôt set it we will generate it for you. The expected version is a number representing the version of the stream. It is a next expected identifier of event. So, if your last event‚Äôs position id is equal 40 that expected_version will be 41. The soft delete cause that you will be allowed to recreate the stream by creating new event. If you recreate soft deleted stream all events are lost. After an hard delete any try to load the stream or create event will result in a 410 response. Long Pooling in ES works as when you want to load head of stream and no data is available the server will wait specified amount of time. So, if you want to fetch the a newest entries you can specify pool_time attribute to wait before returning with no result. The pool_time is time in seconds. This method allows us to load all stream‚Äôs events ascending. This method allows us to load all stream‚Äôs events descending. One of my teammates has created sample application which uses Greg‚Äôs Event Store and our gem. You can find out it here . This project represents example of rails app based on CQRS/ES architecture. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-27"},
{"website": "Arkency", "title": "Implementing Game Dashboard With React.js", "author": ["Wiktor Mociun"], "link": "https://blog.arkency.com/2015/03/implementing-game-dashboard-using-react-js/", "abstract": "Source: 412 digital Many developers starting their adventure with React.js ask me about one thing. How to mount many independent React.js components on a single page? I‚Äôll show you my approach to handle this problem. We have some JavaScript applications showing user information using React.js components. We are good developers, so each one of them handles a separate responsibility. We have applications, but now we need to put them all together on a screen. Some time ago I was working on a simple game inspired by CivClicker . I wanted to have one big screen with primary information about my virtual city . I developed applications responsible for game control and city management: resources, society and infrastructure. Each rectangle represents separate app We can use Rails views to solve our problem. Only thing we need is to expose empty HTML elements. And now in each of our application would need code like this: We got basic HTML structure covered and empty divs for React to plug-in. It works great, but what if we would want to add some logic here? Example: we want to show the City Infrastructure widget after the player reached the level 2. We can change our code. Our view isn‚Äôt dead simple anymore. The controller responsible for rendering this view needs to pass the player object to the template engine. Our code just got more complex. In future development it may get worse as it grows. It‚Äôs a good solution for a start. Let‚Äôs move on. We will use React.js and some event bus to help us with this issue. Let‚Äôs make a simple JavaScript application that will render empty HTML elements for other applications. Here‚Äôs the main idea. When all elements get rendered, the global event bus tells all applications about this fact. We will use the componentDidMount method from React component‚Äôs API to achieve this. Only thing left to do are event handlers for all applications. This simple solution gives us flexibility for further changes. We can use all benefits of a dynamic front-end without introducing new libraries. Moreover, using this approach we gained new feature for free. We can render any app anytime during application life cycle. We just need to publish an event. Often, the first solution is good enough. Yet, it gets problematic when we need to add some logic to the application. We can move this logic to the front-end and simplify our Rails backend. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-30"},
{"website": "Arkency", "title": "Gulp - a modern approach to asset pipeline for Rails developers", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2015/03/gulp-modern-approach-to-asset-pipeline-for-rails-developers/", "abstract": "Rails asset pipeline is one of things that makes Rails great for creating simple applications fast . It solves a big problem of serving our assets efficiently in both development and production environment. While being very useful, Sprockets has its age . There is a rapid growth of technologies around JavaScript. A growth which often cannot be used in an easy way by Rails developers. There are good technologies for code modularization, languages transpiled to JS, CSS preprocessors and  much, much more. Those technologies are easy to use in Node.js-based stacks, but not in Rails. Rails asset pipeline has a big advantage of being painless to use . We do not need to configure anything to have our assets served or precompiled. There is a built-in file require system to help us split our code into files. In bigger frontend codebases we‚Äôd live happier with more sophisticated solutions - and we cannot throw away a legacy that Sprockets have. How to live with both CommonJS and Sprockets require system? How to optimize our compilation steps? Sprockets is implicitly doing its job - and that‚Äôs great until you want to have something more . Fortunately, asset serving is a low-hanging fruit when it comes to decoupling parts from Rails . You can easily remove all asset serving responsibilities from Rails and use a modern build system like Gulp to compile your assets. In this blogpost I‚Äôd like to show you how to replicate major part of Sprockets responsibilities in 82 lines of JavaScript code , with ability to use CommonJS and modern technologies straight from npm . As a bonus this 82 lines will also generate source maps for your CoffeeScript and Sass. There are many Node.js-based build systems we can use to replace Sprockets. But after long consideration Gulp seems to be the best candidate: CoffeeScript and Sass are two gems which ship by default with all modern versions of Rails. As a starting point it is wise to provide features of compiling CoffeeScript and Sass via Gulp. Since you won‚Äôt have Sprockets require system, a proper replacement is needed. CommonJS is a standard in Node.js world and it is easier to use than AMD. It has some nice features: it‚Äôs easy to grasp, provides a proper modularization of your code via require ‚Äôs and looks quite similar to what we had with #= require syntax before. We‚Äôve used CommonJS recently to make a painless upgrade from React 0.11 to 0.13 in our projects - this process is covered in details in our book . To provide CommonJS, Browserify will be used. You can use this command to create a brand-new rails project without Sprockets and ‚Äòdefault‚Äô JavaScript assets: Here -J option removes JavaScript and -S option removes Sprockets. Turbolinks option is self-explanatory ;). You need to have Node.js installed to use npm . Follow this link for more info. You need to create a package: After providing info, a package.json will be created. This is an equivalent of Gemfile for Node.js apps. To use Gulp you need to have it installed. It is advised to install it globally (to be able to run gulp by convenient gulp command) and locally: --save-dev option will add gulp as a dev dependency to your package.json file. There is an equivalent of Rakefile for Gulp, called Gulpfile . You need to create it manually: Gulp operates on tasks. Task can be a function or an array of tasks to be performed sequentially. You can run gulp without arguments and it will perform a default task, and gulp <taskName> to run a particular task. Let‚Äôs create a first task which does nothing: You can now run it with gulp task : You can now add it to be ran by default by running just gulp : Then: It would not be so interesting if you can‚Äôt do something with it. Let‚Äôs make Gulp compile our Sass assets! There are lots of ready-to-use gulp transformations that you can use by just installing them. We‚Äôll use gulp-sass to compile Sass assets. Let‚Äôs install it: It can be done by creating two tasks - compile-sass and compile-scss to compile both sass and scss files: Let‚Äôs stop and understand what happened here. The whole build system of Gulp is based on a streams concept . Gulp provides a ‚Äòstarting stream‚Äô, which you create via gulp.src . It takes a path as an input and returns a stream of (virtual) files as an output. All you do then is passing this output as an input of a transformation, which takes a stream of virtual files as an input and returns transformed virtual files as an output. On the ‚Äòend‚Äô side of this transformation is gulp.dest - it takes a stream of virtual files as an input and writes a real ones as the output in a directory specified as parameter of this transformation. That means each task should consist of a sequence of stream transformations ended with a final gulp.dest transformation . In this example all files matching app/assets/stylesheets/**/*.sass are passed to a sass transformation. This transformation compiles those files using node-sass implementation of Sass. There is a gulp-sourcemaps transformation which is responsible for creating source maps from transformations compatible with it. Fortunately, gulp-sass is one of these transformations . What you should do is you should pass sourcemaps.init() transformation before sass transformation and sourcemaps.write() transformation after you finish your sequence of transformations you‚Äôd like to be source mapped. In this case there is only one transformation - sass . First, install gulp-sourcemaps : Then modify your tasks: Now change your app/assets/stylesheets/application.css to app/assets/stylesheets/application.sass . For this example: I received this output after running gulp : That means this task works and generates source maps as intended! Gulp is not the first application which uses streams to work with inputs. Browserify which will be used also have a stream API that will be used in this example. This is a preferred way to work with Browserify and Gulp . gulp-browserify package is no longer maintained and is blacklisted - you can‚Äôt use it. The rationale of this decision is that creating a separate gulp packages for Browserify is wrong because it is already using streams and have its own ecosystem of ‚Äòtransforms‚Äô. We‚Äôll use one of these transformations - one called ‚Äòcoffeeify‚Äô which compiles our CoffeeScript files if needed. There is one problem to solve, though. Browserify by default takes a file path or an array of file paths and outputs a text stream with compiled bundle. This is not the way gulp works - each transformation needs a virtual file as input, not a text. But there is a simple solution: a vinyl-source-stream package is a stream transformation which takes text stream as an input and returns a virtual file with name passed as a transformation parameter as an output. Ideal solution for this problem! Let‚Äôs install browserify, coffeeify and vinyl-source-stream: Then, create a compile-coffee task and add it to default list: This will take your app/assets/javascripts/application.coffee , compile it and all of its dependencies if needed and create a source maps for the whole bundle, saved in public/assets/bundle.js . If you want to use an external library, like jQuery, you can install it via npm: And then in code: That‚Äôs how Browserify manages your dependencies. You can read more about it in a Browserify site . Ok, so basic compilation of assets is done. But we also need to watch for changes - a feature which sprockets provides us in development and is super useful. Fortunately, it‚Äôs easy in Gulp! Gulp provides a gulp.watch method, which takes a glob path (with * ) and a task to perform if files change. So here‚Äôs how watching for scss/sass change can be made: Watching for Browserify changes can be made in a similar way, but it is not efficient. There is a special package called watchify which will recompile only when changes to dependencies or a module itself are made. It is advisable to install gulp-util package for easy logging. In this example also lodash will be used for a assign utility function. You can read more about working with watchify on its home page. Here, a full Gulpfile after changes will be shown. First, install required dependencies: Then, modify your Gulpfile : Now you can run gulp watch and watch for changes of your Sass and CoffeeScript files. Each time you change it, they‚Äôll compile automatically. In this 82 line JS code snippet we have actually rewritten a major part of default Rails Sprockets configuration. But of course you can provide more features: There is a lot to do beyond standard workflow of Rails. Now, when you have full control you can do a lot things you cannot with Sprockets. Using Gulp as a replacement of Sprockets seems to be a natural way to easily grasp all modern JavaScript techniques and technologies. Explicit asset pipeline is a thing that can be beneficial both in terms of debugging and easily adding new parts to your frontend. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-03-31"},
{"website": "Arkency", "title": "How to store large files on MongoDB?", "author": ["Robert Krzysztoforski"], "link": "https://blog.arkency.com/2015/03/how-to-store-large-files-on-mongodb/", "abstract": "The common problem we deal with is importing files containing a large amount of records. In my previous article\nI‚Äôve presented how to speed up saving data in MongoDB . In this article i will focus on how we can store these files. Sometimes we want to store file first and parse it later. This is the case when you use async workers like Sidekiq.\nTo workaround this problem you need to store the file somewhere. MongoDB allows us to store files smaller than 16MB as a string in DB. We can simply do it by putting all the data in file_data attribute. The code above may work well if you upload files smaller than 16MB, but sometimes users want to import (or store) files even larger.\nThe bad thing in presented code is that we are losing information about the original file . That thing may be very helpful when you need to open the file in a different encoding. It‚Äôs always good to have the original file. In this case we‚Äôll use a concept called GridFS . This is MongoDB module for storing files. To enable this feature in Rails we need to import a library called mongoid-grid_fs . The lib gives us access to methods such as: In the second solution we are storing the original file. We can do anything what we want with it. GridFS is useful not only for storing files that exceed 16MB but also for storing any files for which you want access without having to load the entire file into memory. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-04-02"},
{"website": "Arkency", "title": "Rails meets React.js is out: What you can find inside?", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2015/04/rails-meets-react-dot-js-is-out-what-you-can-find-inside/", "abstract": "Dealing with frontends can be a tedious task for Rails developers. In Arkency, we blogged the about frontend development a lot, searching for the best solutions and practices. React.js is one of the most successful findings for us. It brought solid solution to a problem of building modern, dynamic interfaces to web applications we were working on. React.js is one of jewels in our toolbox now. There is a common practice in our company to propagate knowledge within and outside of the team with blogposts and books. After a few months of work, we‚Äôre happy to present you a book with our knowledge about React.js. We had a great fun writing it. We filled it with our best practices, theory behind React and tons of examples. You can buy it right now - with all future updates for free. Interested? Let me tell you more about what you can find inside. Let‚Äôs start our journey with smallest, meaningful thing we can do with React. Every technology comes with a set of terms and conventions you need to understand to write a code using it. React.js is a technology with a well-defined scope and a single responsibility - thus there are fewer things to grasp than, for example, Rails or Angular.js which have a lot of terminology behind. First chapter is all about slowly introducing building blocks of React.js . We start with a minimal example and extend it through the chapter, step by step. With each step there is an explanation what we actually do and how is is done in React. At the end of this chapter you should have a knowledge sufficient to build a simple component with event handlers. From our own experience React is most commonly used to enhance user interaction with non trivial forms. That‚Äôs why our first bigger example is going to be about submitting data.\nOur goal is to code a form for creating new meetup. It will have to interact with various user actions and grow to accumulate more features. After grasping basic concepts of React, there is a bigger hands-on example constructed in the same manner - in a step by step manner. You are about to build a big, dynamic form with frontend validations. You can see a real world example of the React dynamic UI - and polish your skills obtained in previous chapters. You get a code repository with all examples from the book, absolutely for free. We tried to split our work into commits which shows our workflow with React. You can also read a book and make steps from it by yourself. The code is in Coffeescript and Rails. Passing state is still subject of our interest since we are unsatisfied with solutions we‚Äôve tried. You can also find a more proper way to pass state - classical approaches can be hard to operate with when user interface becomes huge. We are working with React.js for a more than a year. With this year we managed to solve common problems you may have when working with React by yourself. With this book you can learn patterns for passing state, integrating with external libraries and structuring your components - with pros and cons and meaningful tips from experienced developers. The art of programming in React is creating components from smaller ones to achieve your user interface goals. You may find it easier to think about it as a partial in your typical Rails ERB view. React.js comes with a solid declarative approach to workflow. There is an interesting theory worth to know when you want to master your skills as a front-end developer. In Rails meets React.js there are chapters about each building block described in a more ‚Äòtheoretical‚Äô way. You can learn about philosophy behind React.js and use it for your own good. Inside React.addons.TestUtils we have special module called Simulate . We can simulate all kind of events that can originate from the browser. Testability is very important for us in Arkency. Testing is a part of our culture and a technology without a way to proper testing has a little value for us. Fortunately React components are easily testable. In our book common testing approaches are shown - through mutating state, using simulated events and through references. React.js is a technology which does have really smooth learning curve. You can easily transform your simpler views to React components even without knowing what‚Äôs going on inside the components. I want to show you an example. Apart from the ‚Äòmain‚Äô content of a book, there are a lot materials we wanted to share with you. We included around 50 pages of content from our blogs and dropped chapter of the book (transforming a Rails view into a React component) as bonus chapters. It is a great source of inspiration if you want to improve your front-end skills further. Recently React.js got an update which is a step forward towards making your components less ‚Äúmagic‚Äù. There are some breaking changes, especially if you are not using JSX. Fortunately, upgrade process is rather straightforward. If you had an occasion to work with React before, there is a chapter about upgrading old React code to be ready for a 0.13 version of this library. This version introduced major changes, both in code and terminology, decoupling some concepts even more. For now there are 10 chapters - 146 pages of the main content . There is also 46 pages of bonus content we‚Äôve prepared. It is a first release of our book - if you buy it now you get all further updates for free. Roadmap for v2 version is already prepared. Our book is already available for purchase. You can buy it from our DPD for a great price of $49 - with a repository access and bonus materials included. As one of authors (we wrote this book with Robert Pankowecki ) I hope you will have as much fun of reading this book as I had writing it. React.js brings joy to a complicated task of building frontend UIs. I hope this book will help you to be a great frontend developer. Which is a role which more and more often demanded from our clients nowadays. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-04-15"},
{"website": "Arkency", "title": "The Event Store for Rails developers", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2015/04/the-event-store-for-rails-developers/", "abstract": "We have experimented for some time with an Event Sourcing in our projects.\nThis is why we released a free HTTP connector to the Greg‚Äôs Event Store written in Ruby.\nOn the basis of earlier experiences from the one of our projects we decided to create own implementation of an Event Store.\nI would like to announce the first release of the Rails Event Store gem. If you already have the rails_event_store gem in your Gemfile then you have to create a table in your database. To do this you have to run the provided task. This will generate an activerecord migration. To use our gem‚Äôs functionality you have to create an instance of RailsEventStore::Client class. Creating events is very simple. At the beginning you have to define your own event model extending RailsEventStore::Event class. Now you are prepared to create event‚Äôs instance and save it to a database. We use the concept of streams like in Greg‚Äôs Event Store but (as you can see in the above example) you are able to create a global event. The event_id is also an optional value. If you leave it blank then the application generate UUID for you.\nThe rails_event_store provide also optimistic concurrency control. You can define an expected version of stream during creating event. In this case the last event identifier. You can fetch events from database in a several ways. In any case, loaded events are sorted ascending. You can permanently delete all events from a specific stream. Use this wisely. Using our library you can synchronously listen on specific events. The only requirement is that the subscriber class has to implement the ‚Äòhandle_event(event)‚Äô method. Check out the following example. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-04-21"},
{"website": "Arkency", "title": "Bring CommonJS to your asset pipeline", "author": ["Jakub Kosi≈Ñski"], "link": "https://blog.arkency.com/2015/04/bring-commonjs-to-your-asset-pipeline/", "abstract": "A few weeks ago, Marcin recommended Gulp as the Rails asset pipeline replacement . Today I am going to tell you how you can benefit from CommonJS modules in your Rails application with asset pipeline enabled. You don‚Äôt have to disable asset pipeline in your Rails application in order to use CommonJS module loading in your JavaScripts. One of tools that allows you to do so is Browserify and a nice gem called browserify-rails . It lets you mix and match //= require directives from Sprockets with require() calls in your JavaScript (CoffeeScript) assets. You can manage your JS modules with npm , so you can use a wide variety of existing node modules, including React directly in your Rails assets. To get started, you need to have node and npm installed on your development machine and include browserify-rails gem in your Gemfile . Then you should initialize your package.json file using in your‚Äôs application root directory and add the following packages to your dependencies: That‚Äôs all! You can now start writing your CommonJS modules and use them in your Rails application. You can also install more node modules and require them in your assets: BTW, in our Rails meets React book, we‚Äôre also using asset pipeline to use React components in Rails. Using browserify-rails does not force you to use only CommonJS-compatible libraries. You can still use //= require directive to load arbitrary JavaScript assets. After installing browserify-rails you are starting with default configuration that makes some assumptions: You may tune up your config if you need. I would recommend enabling source maps generation in development environment for easier debugging. To do so you need to add the following line to your config/application.rb file: If you are using CoffeeScript, you should follow .js.coffee naming convention, install coffeeify plugin: and enable this plugin by adding the following line to your config/application.rb : Now you should just restart your server and write your assets in CoffeeScript flavour. browserify-rails supports the same features as node-browserify , e.g. you can have multiple bundles . You can read about all possible configuration options on browserify-rails github page . In order to deploy your assets to production server, you don‚Äôt need anything but running rake assets:precompile task. If you‚Äôre running that task on your production server during deployment (e.g. when using capistrano ), you also need to make sure you have node & npm installed on your production. You should also install all npm dependencies before compiling your assets. You may use rake tasks provided by browserify-rails gem to do so: Using browserify-rails can significantly increase modularity of your JavaScript assets, but have also some disadvantages. Pros: Cons: Using browserify-rails may be a good option when you want to use asset pipeline and improve modularity of your JS assets. I think you may definitely give it a try! Heroku is a very popular platform for deploying Rails applications. By default, it automatically determines how to build your app during deploy by using some heuristics (e.g. it assumes you have Ruby application if your root directory contain Gemfile ). In order to use browserify-rails and run bundle along with npm install on target machine, you need to use a custom buildpack - heroku-buildpack-multi . To use it, you will first need to set it as your custom buildpack by running the following command: Then you should create special file .buildpacks in your application‚Äôs root directory that contains the list of all (ordered) buildpacks you would like to run when you deploy. In our case it would contain the following buildpacks: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-04-23"},
{"website": "Arkency", "title": "Beautiful confirm window with React", "author": ["Jakub Kosi≈Ñski"], "link": "https://blog.arkency.com/2015/04/beautiful-confirm-window-with-react/", "abstract": "When designing your web application, you would like the user to confirm some actions sometimes. For example, you may want the user to confirm deletion of his data. There is window.confirm JavaScript method that might be useful in this case but it could not be styled and just displays native browser‚Äôs dialog window. In this article I would like to show you how to create React component as a replacement for window.confirm that can have similar behaviour and your application‚Äôs look & feel. It has similar API to window.confirm so migration should be really easy. In this article I am using the latest React (0.13.2) and Bootstrap v3.3.4 for styling modal window. I am also using jQuery promises to handle confirm and abort actions. Let‚Äôs start with creating React component for Bootstrap-styled modal window. We will use that component later in confirm window implementation. We will create modal with backdrop and lock the whole UI under backdrop until the user clicks on confirm action button. The division with modal-backdrop will be used to cover and lock everything on the page. We would not close modal on backdrop click in this case. Now it‚Äôs time to implement the confirm dialog component. It will use Modal component created in previous step. We will add title, two buttons (confirm and abort) and optional descriptive text. We are using promises in confirm and abort methods. If you are not familiar with the concept of promises, I recommend you read our beginners guide to jQuery Deferred and Promises . In short, using promises would allow us to asynchronously decide what code should be called after clicking confirm or abort button in our dialog window. You can also notice we are using componentDidMount lifecycle method. This method is called right after the component was mounted (its representation was added to the DOM tree). We are creating a promise object in that method - you may not be familiar with using instance variables instead of state in react components. Since that promise has no effect on the rendering of our component, it should not be placed in state, because adding it to state would cause unnecessary calls of render method.\nThere is also one more line in componentDidMount - React.findDOMNode(@refs.confirm).focus() . We are using it for better UX, similar to the native window.confirm behaviour, so you can just press Enter when confirm dialog appears. You can also easily extend this component to enable aborting dialog when pressing Escape. If you would like to know more about using React especially in your Rails application, take a look at React meets Rails book we have written. We have created modal and confirm dialog components. Now it‚Äôs time to make it work. We will create a method that will render our confirm dialog and return a promise. Once the promise is resolved or rejected, the dialog will be unmounted from DOM. When resolving or rejecting a promise, we are unmounting the whole Confirm component to cleanup the DOM (I prefer removing nodes from DOM than just hiding them via CSS). We are also removing the wrapper node since it‚Äôs not needed anymore after the dialog is closed - each time we call the confirm method, new wrapper node would be created and added to DOM (you may also create a confirm target node upfront - then you would only need to mount and unmount the component in confirm ). OK, we have now all parts of our window.confirm replacement. How to use it in your code? Very simple, you should only change your conditional: that would produce something like this: to promise version: that looks like this: But since we have a React component, you can add more descriptive information to your confirm dialog or change button labels (with window.confirm you can only set description, the dialog title is generated by your browser): This is the final look: Replacing native window.confirm with custom solution gives you ability to have the same behaviour but without restrictions - you can have beautifully styled dialog with custom button labels or dialog title. You can grab the demo on jsfiddle . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-04-24"},
{"website": "Arkency", "title": "Why I want to introduce mutation testing to the railseventstore gem", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/04/why-i-want-to-introduce-mutation-testing-to-the-rails-event-store-gem/", "abstract": "We have recently released the RailsEventStore project. Its goal is to make it easier for Rails developers to introduce events into their applications. During the development we try to do TDD and have a good test coverage. The traditional test coverage tools have some limitations, though. Mutation testing is a different approach. In this post I‚Äôd like to highlight why using mutation testing may be a good choice. Let me start with one example. In this example, mutant discovers  uncovered code. Other tools think this code is well-covered. In the RailsEventStore (RES) implementation, we use the concept of a Broker. The broker allows subscribing to certain kinds of events. As part of the subscription we pass the subscriber object. In the current implementation, we expect that such a subscriber has a handle_event method. When the raise MethodNotDefined    unless subscriber.methods.include? :handle_event line was introduced it didn‚Äôt come with any test. Despite this fact, the coverage tools assume it does have a coverage. That‚Äôs because the previous tests do go through this line and consider it covered. Let‚Äôs turn this line into this: With this code, the simple coverage tools are able to detect that the if block is never executed. As you see, the coverage metric is now depending on the fact how you format the code. That‚Äôs not good. If we run the first piece of code with mutant, it does detect that there is a lacking coverage. Mutant output When I run mutant on RES, I use the following: which results in the following summary: It‚Äôs worth noting that mutation testing is very time-consuming. In our case, the time spent was the following: My goal is to setup Travis to run the mutation tests on every push. Also, I‚Äôd like to set up a 100% expected mutation coverage in the future. This is an example output of the mutant run on a Travis machine. It‚Äôs worth looking at, as you can see the full output. Mutant shows us every alive mutation - the ones that don‚Äôt break tests. One example: This output means, that our coverage here was not perfect. Simply replacing the expected_version with nil is still passing all tests. That‚Äôs not good. We can‚Äôt really rely on our tests if we want to refactor this code. What‚Äôs wrong with the lack of coverage? The problem with lacking test/mutation coverage is that it‚Äôs easy to break things when you do any code transformations. The RailsEventStore is at the moment changing very often. We try to apply it to different Rails applications. This way we see, what can be improved. The code changes to reflect that. If we have features without some tests, then we may break them without knowing it. My goal here is to have every important feature to be well-covered. This way, the users of RES can rely on our code. Feel free to start using RailsEventStore in your Rails apps. We already plugged it into several of our applications and it works correctly. In case of any question, please jump to our gitter channel , we‚Äôll be happy to help. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-04-28"},
{"website": "Arkency", "title": "On my radar: RethinkDB + React.js + Rails", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/04/on-my-radar-rethinkdb-plus-react-dot-js-plus-rails/", "abstract": "Sometimes you hear positive things about a technology and\nyou put it on your radar. To explore it when you have some time.\nTo get a feeling if it is fun at all. To mix and match things\ntogether to see what comes out of it. For me recently it was RethinkDB .\nIts slogan says The open-source database for the realtime web .\nInteresting enough to get me curious. RethinkDB\npushes JSON to your apps in realtime also sounded good. I was sick one week ago so I had a moment to give it a try. If RethinkDB pushes changes and React can automatically re-render\na page effectively it would seem that merging these two technologies\ntogether could give us nice real time page updates. We just need a\nglue technology that would connect the DB to the browser and\nauthorize the request. Let‚Äôs go with Rails. If you want to learn the basics of RethinkDB I highly recommend their Ten-minute guide with RethinkDB and Ruby .\nThe installation process with provided OS X and Ubuntu packages was\nvery simple for me as well. We start by adding puma , nobrainer and react-rails to our Gemfile. We are going to use Puma because going with multi-threaded servers is fastest\nway for us to start enjoying the results. NoBrainer is a Ruby ORM for RethinkDB. Similar in taste\nto ActiveRecord. And finally react-rails is fastest\nway to start using react in rails environment. If we go with rails g scaffold Article title:string text:string we will have a basic\nstructure generated. But it will use NoBrainer instead of ActiveRecord . Our\ndocument looks like that: You can find out more about integrating RethinkDB with Rails in their own documentation. We are gonna leave the entire write side untouched. But we will\nfiddle with the read part. How we display the document. Instead of using html & erb to display an article, we will play with a React component.\nWhen written with CoffeeScript it looks similar to HAML. Of course you could go with JSX if that‚Äôs your flavor. @props are the properties passed to the component when rendered. We are going to use react_component helper that comes from the react-rails gem.\nIt makes easier to start react components that will run when a browser fetches\nthe page. And with prerender: true they even render the component server-side\nfirst and then react.js in a browser handles the lifecycle of that component. And\ninteractions with it. And all that stuff that your UX is responsible for. data: {reactive: start_show_path} is a path for URL that will be used for streaming\nchanges. Let‚Äôs dive into it. So we‚Äôve got the first render covered. But we need to make this component auto updates\nwhen the data changes. We are going to use Server Sent Events for that. It‚Äôs a browser API for one way, server to browser communication over\nHTTP connection. It even has automatic re-connections built-in. We look for data-reactive elements and make a connection\nto the URL. On event we ask react to re-render a new version of the\ncomponent in the same place. This is just a tiny wrapper for formatting data according to SSE spec. For SSE streaming in Rails I used ActionController::Live . You can read\na great blog-post by Aaron Patterson where he introduced Live Streaming in 2012 to get familiar with it.\nYep, it was that long time ago. And Rails documentation for ActionController::Live Here we use the feature of changefeeds from RethinkDB.\nYou can subscribe to changes from a table, a single document or even a query and be notified every time\nsomething changed. In our example we subscribe to changes from one document, the last Article: This syntax mixes higher-level API (Nobrainer ORM) with low-level API (RethinkDB official driver)\nbut that‚Äôs how I managed to get it work. You can do much more with changefeeds but that‚Äôs what I needed for our basic use-case. Surprisingly (or not) it works. You can watch the 20s demo. You can see the whole code on github arkency/rethinkdb-reactjs . I had to use config.cache_classes = true and config.eager_load = true to get SSE working in development mode . I set the config.per_thread_connection = true config option for NoBrainer as we\nuse puma, a multi-threaded web server. I have a feeling that react_component() with prerender: true is not very\nperformant but I haven‚Äôt benchmarked yet. It might highly depend on the JS engine\nand the ruby version that you build your app with. But that‚Äôs my gut feeling for now.\nI want to truly benchmark one day. The RethinkDB query from our Live Controller is blocking and taking one thread\nout of the puma‚Äôs pool. This can lead to thread pool exhaustion if too many people are\nconnected and the pool size is too small. But one of the next things I want to investigate\nis sending the stream of changes with EventMachine and/or Thin instead of Puma. This should be possible as\nthe official driver comes with the em_run method which can be used in EventMachine single-threaded non-blocking environment\nand should scale much better. Also the thread used by SSE is not stopped if the user navigates away and the browser disconnects.\nThat is because as I said we are waiting (and blocking) for changes from RethinkDB.\nIf those changes occur the attempt to write to the disconnected browser will fail, and\na proper exception will be raised (that we catch) so we can end this thread. People using redis pub-sub experienced similar problem and as a workaround they publish ping every now and then. You could\nachieve similar thing with RethinkDB: Subscribe to both notifications. Send pings. Despite many dragons I have a feeling that there is a big potential in RethinkDB.\nI will keep it on my radar and explore more deeply. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-04-30"},
{"website": "Arkency", "title": "Mutation testing and continuous integration", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/05/mutation-testing-and-continuous-integration/", "abstract": "Mutation testing is another form of checking the test coverage. As such it makes sense to put it as part of our Continuous Delivery process. In this blog post I‚Äôll show you how we started using mutant together with TravisCI in the RailsEventStore project. In the last blogpost I explained why I want to introduce mutant to the RailsEventStore project . It comes down to the fact, that RailsEventStore, despite being a simple tool, may become a very important part of a Rails application. RailsEventStore is meant to store events, publish them and help building the state from events (Event Sourcing). As such, it must be super-reliable. We can‚Äôt afford introducing breaking changes. Regressions are out of question here. It‚Äôs a difficult challenge and there‚Äôs no silver bullet to achieve this. We‚Äôre experimenting with mutant to help us with ensuring the test coverage. There are other tools, like simplecov or rcov but they work on much worse level of precision. Another way of ensuring that we don‚Äôt do mistakes is to rely on automation. Whatever can be done automatically, should be done automatically. A continuous integration server is a part of the automation process. We use TravisCI here. Previously, TravisCI just run bundle exec rspec . The goal was to extend it with running the coverage tool as well. When I was experimenting with mutant and run it for the first time, I saw about 70% of coverage. That was far from perfect. However, it was a good beginning. My idea was to introduce mutant as part of the CI immediately - with the first goal being that we don‚Äôt get worse over time. Mutant supports the ‚Äîscore option: I didn‚Äôt read this output carefully, at first. I assumed that the score option is there to check if my coverage is equal or higher than the expected coverage. When I run the tests and checked the exit code ( echo $? ) afterwards I saw the result being 1 (a failure). I assumed that something was broken and went to the mutant sources to find this here : BTW, if you want see a pretty Ruby codebase - you should look at the mutant code . Raising the coverage bar This must have been a mistake, I thought. Why would you ever want to assume that the coverage is equal to expected coverage. Being higher than the expected coverage is a good thing, right? Actually, it‚Äôs not a good thing. As I learnt from Markus (the author of mutant), this setting is intentional. The reason for that is that you want to fail in both cases - when the current coverage is lower than expected - that‚Äôs clear. You also want the build to fail, when it‚Äôs higher. Why? Because otherwise you may miss the point of time when you improved the coverage. Later on, you may have reduced again. You never noticed that the expected coverage should be raised. If I got it correctly, this technique is called ‚Äúraising the bar‚Äù. After this explanation it made a perfect sense to me. Unfortunately, at the moment, there‚Äôs a small problem with using this technique. Due to the rounding precision problems , we can‚Äôt pass the ‚Äúright‚Äù number to mutant. Very often your coverage is like 74.333333333% and you can‚Äôt pass such precision easily. This is not a big problem, though. There‚Äôs a better way of using mutant - whitelisting/blacklisting. Whitelisting/blacklisting uncovered classes Another technique that I learned from Markus was to whitelist or blacklist certain classes which don‚Äôt pass the 100% coverage. The idea is to never break the coverage of the perfectly covered units. \nThis motivated us to get all the ‚Äúalmost‚Äù covered units to the 100% mark, which we did. BTW, it‚Äôs worth mentioning that we only test our code through the public API. In our case, it‚Äôs the Client facade and the Event class. We avoid grabbing an internal class and testing it directly. I wrote more about this topic in the past . We were only left with 2 (private) classes that were left uncovered: MigrateGenerator and the EventRepository. Both of them are Rails-related. The MigrateGenerator looks like this: This is just a helper for the Rails developers who use our tool to generate a migration for them. It creates a table, where events will be stored. At the moment this class is blacklisted from the mutant coverage. How would you put it under a test coverage? The repository pattern and the ways of testing it The second case is EventRepository : We use the repository pattern to encapsulate and hide the storage part of our tool. If you want to read more about the repository pattern, I wrote a book which explains why it‚Äôs worth using and how to introduce this pattern to the existing Rails application. In tests, we use an InMemoryRepository: Replacing the repository with an in memory equivalent (with the same API) is a nice technique. It lets us run the tests super fast. There‚Äôs one drawback, though - we don‚Äôt test the real repository. This was rightly pointed out by mutant. I don‚Äôt want any place in the code to remain untested. I want us to have the confidence that whenever tests pass we can ship a new release of the gem (ideally automatically). So, how to test the ActiveRecord-related code? How to make it fast (mutant runs those tests several times)? If you have any idea, please share it with us here. I‚Äôve got some ideas but that‚Äôs a topic for another blogpost probably :) Mutant and TravisCI Now - the main point. To summarise - we now have 100% mutant coverage in all but 2 classes. We can make it part of the CI process just by putting the following into our .travis.yml file: Thanks to that, the CI will check the coverage every time the code is pushed. It may influence the way we work in some interesting ways - we need to ensure that the coverage is always the same. That‚Äôs an interesting challenge! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-01"},
{"website": "Arkency", "title": "Extract a service object using SimpleDelegator", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/05/extract-a-service-object-using-simpledelegator/", "abstract": "It‚Äôs now more than 1 year since I released the first beta release (the final release was ready in December 2014) of the ‚ÄúFearless Refactoring: Rails controllers‚Äù book . Many of the readers were pointing to the one technique which was especially useful to them - extracting a service object using SimpleDelegator. This technique has also been very popular in our Arkency team. It gives you a nice way of extracting a service object immediately, within minutes. It is based on a bit of a hack, though. The idea is to treat this hack as a temporary solution to make the transition to the service object more easy. Before going into the chapter, let me quickly explain the structure of such a Rails Refactoring recipe . We start with a short introduction to the problem. Then we list the prerequisites - things that are needed to be done before this refactoring. After that we present a short algorithm and we jump into the examples . At the end we explain the benefits and we list the resources . OK, now it‚Äôs time for the killer technique: New projects have a tendency to keep adding things into controllers. There are things\nwhich don‚Äôt quite fit any model and developers still haven‚Äôt figured out the domain exactly. So these features land in controllers. In later phases of the project we usually have better insight into the domain. We would like to restructure domain logic and business objects. But the unclean state of controllers, burdened with too many responsibilities is stopping us from doing it. To start working on our models we need to first untangle them from the surrounding mess. This technique helps you extract objects decoupled from HTTP aspect of your application. Let controllers handle that part. And let service objects do the rest. This will move us one step closer to better separation of responsibilities and will make other refactorings easier later. As of Ruby 2.0, Delegator does not delegate protected methods any more. You might need to temporarly change access levels of some your controller methods for this technique to work. Once you finish all steps, you should be able to bring the acess level back to old value. Such change can be done in two ways. Change into Change into I would recommend using the second way. It is simpler to add and simpler to remove later. The second way is possible because #public is not a language syntax feature but just a normal method call executed on current class. Although not strictly necessary for this technique to work, it is however recommended to inline filters. It might be that those filters contain logic that should be actually moved into the service objects. It will be easier for you to spot it after doing so. This example will be a much simplified version of a controller responsible for receiving payment gateway callbacks. Such HTTP callback request is received by our app\nfrom gateway‚Äôs backend and its result is presented to the user‚Äôs browser. I‚Äôve seen many\ncontrollers out there responsible for doing something more or less similar. Because it is such an important action (from business point of view) it usually quickly starts to accumulate more and more responsibilities. Let‚Äôs say our customer would like to see even more features added here, but before proceeding we decided to refactor first. I can see that Active Record models would deserve some touch here as well, let‚Äôs only focus on controller right now. In this example I decided not to move the verification done by the whitelist_ip before filter into the service object. This IP address check of issuer‚Äôs request actually fits into controller responsibilities quite well. For start you can even keep the class inside the controller. We created new class ServiceObject which inherits from SimpleDelegator . That means that every method which is not defined will delegate to an object. When creating an instance of SimpleDelegator the first argument is the object that methods will be delegated to. We provide self as this first method argument, which is the controller instance that is currently processing the request. That way all the methods which are not defined in ServiceObject class such as redirect_to , respond , failed_order_path , params , etc are called on controller instance. Which is good because our controller has these methods defined. First, we are going to extract the redirect_to that is part of last rescue clause. To do that we could re-raise the exception and catch it in controller. But in our case it is not that easy because we need access to order.id to do proper redirect. There are few ways we can workaround such obstacle: Here, we are going to use the first, simplest way. The third way will be shown as well later in this chapter. Next, we are going to do very similar thing with the redirect_to from ActiveRecord::RecordNotFound exception. We are left with two redirect_to statements. To eliminte them we need to return the status of the operation to the controller. For now, we will just use Boolean for that. We will also need to again use params[:order_id] instead of order.id . Now we need to take care of params method. Starting with params[:order_id] . This change is really small. The rest of params is going to be be provided as second method argument. When you no longer use any of the controller methods in the Service you can remove the inheritance from SimpleDelegator . You just no longer need it. It is a temporary hack that makes the transition to service object easier. This would be a good time to also give a meaningful name (such as PaymentGatewayCallbackService ) to the service object and extract it to a separate file (such as app/services/payment_gateway_callback_service.rb ). Remember, you don‚Äôt need to add app/services/ to Rails autoloading configuration for it to work ( explanation ). You can see that code must deal with exceptions in a nice way (as this is critical path in the system). But for communicating the state of transaction it is using Boolean values. We can simplify it by always using exceptions for any unhappy path. ‚ÄúWhat about performance?‚Äù you might ask. After all, whenever someone mentions exceptions on the Internet, people seem to start raising the performance argument for not using them. Let me answer that way: Hard data for those statements. Feel free to reproduce on your Ruby implementation and Rails version. In other words, exceptions may hurt performance when used inside a ‚Äúhot loop‚Äù in your program and in such case should be avoided. Service Objects usually don‚Äôt have such performance implications. If using exceptions helps you clean the code of services and controller, performance shouldn‚Äôt stop you. There are probably plenty of other opportunities to speed up your app compared to removing exceptions. So please, let‚Äôs not use such argument in situations like that. This is a great way to decouple flow and business logic from HTTP concerns. It makes the code cleaner and easier to reason about. If you want to keep refactoring the code you can easily focus on controller-service communication or service-model. You just introduced a nice boundary. From now on you can also use Service Objects for setting a proper state in your tests. What I showed you here is one chapter of the ‚ÄúFearless Refactoring: Rails Controllers‚Äù book. The whole book is divided into 3 parts: The chapter above was one of the recipes. The book is much more than that. The third part of the book is explaining in a very detailed way what are: If you think that your Rails application can benefit from those patterns, consider buying the book . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-04"},
{"website": "Arkency", "title": "Building an Event Sourced application using railseventstore", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/2015/05/building-an-event-sourced-application-using-rails-event-store/", "abstract": "Still not sure what is Event Sourcing and CQRS? Looking for a practical example? You‚Äôve just read my previous post Why use EventSourcing and decided to give it a try? No matter why let‚Äôs dive into building an Event Sourced application in Rails using Arkency‚Äôs Rails Event store . But first you need to learn what CQRS is. CQRS stands for Command and Query Responsibility Segregation. It‚Äôs a term coined by Greg Young and later polished in discussion with others (naming only a few Udi Dahan , Martin Fowler ). It is based on CQS (Command Query Separation) devised by Bertrand Meyer : Every method should either be a command that performs an action, or a query that returns data to the caller, but not both. In other words, asking a question should not change the answer. Bertrand Meyer CQRS is a way of building your application. It is not a pattern, not an architecture, not a framework. The best description here will be ‚Äúarchitectural style‚Äù . It‚Äôs just about separating reads and writes and building a separate stack (layers) for each of them: You may say ‚ÄúOk, but I wanted to learn about Event Sourcing and you‚Äôre writing here about some CQRS architecture style.‚Äù There is a good reason for that: You can use CQRS without Event Sourcing, but with Event Sourcing you must use CQRS. Greg Young Let‚Äôs start building our Event Sourced application with defining a domain model. When using Event Sourcing your domain model (your aggregates) are build based on domain events. Its underlying data model is not storing current state but series of domain events that have been applied to that aggregate since the beginning of its life. This allows you to build your aggregate that will not expose its state and will be able to protects its invariants. Also an aggregate is a source of new domain events. Every call of a method for an aggregate might result in publishing new domain events. Every change in internal state of an aggregate must be done by publishing and applying a new domain event. No state change in other way! Only then we could ensure that our aggregate will be rebuild to the same state from events. I usually create a helper method to access event‚Äôs attributes witch are stored the in @data hash attribute of RailsEventStore::Event . And I like also to add a class method create to build a new event with explicitly given parameters, but a RailsEventStore::Event is also a good way of creating a domain event. Ok, now when we have domain events defined let‚Äôs apply them to our domain object. Let‚Äôs see what is going on in our Domain::Order class.\nFirst the initialize method. It builds the initial state of an aggregate, a state where all begins :)\nThen we have the create method. It should be used by other objects to invoke aggregate features. Here we should protect our invariants, check business rules do validations etc. From CQRS we know it should not return a value - it should just execute or fail (by raising error). In the create method we never change the state of an aggregate. Instead we build and apply a new domain event. The apply method is defined in AggregateRoot module: By calling apply Events::OrderCreated.create(@id, order_number, customer_id) we are creating a new OrderCreated domain event, applying it to our aggregate - what results in a change of state, and storing it in the changes collection. It stores all domain events created by the aggregate during its method execution. The last thing to take a look here is the apply_order_created method. This method updates the aggregate state based on domain event. It does not matter if the event was published by the aggregate itself or it was read from Event Store and applied when rebuilding the aggregate state. There must not be any business rules check or validations here. What has happened has happened. It is already done. We just reflect those changes in aggregate state. And one more thing: notice that all state of an aggregate object is private. Not available outside aggregate itself - even for reads. A command is a simple object that encapsulates parameters for an action to be executed. Should be named in a business language (Ubiquitous Language) and express the user‚Äôs intention. Before a command is executed it should be validated. Those validations should be simple, out of full context, just based on command data and read model data. Should not check business rules (that will be handled later by domain object). They are also known as Form Object (in Ruby world). Base class Command uses some ActiveModel features to allow simple validation and creation of command objects. A command handler is a entry point to your domain. It should handle all ‚Äúplumbing‚Äù, orchestrate domain objects and domain services and execute domain object‚Äôs methods with parameters given in a handled command. There could be several sources of commands send to our application: users, external systems or sagas (process managers). Here is how I‚Äôve defined ‚Äúplumbing‚Äù for my sample Event Sourced application: https://github.com/mpraglowski/cqrs-es-sample-with-res/blob/master/lib/command_handler.rb This is the only module where I use core features of Rails Event Store. It loads events from RES in load_events using aggregate id as a stream name. It publishes events in the publish method. The publish in RES will store the published event in a given stream (again aggregate id) and then send it to all subscribers. This is an important assumption! What is not stored is never published . So with the use of the CommandHandler module the CreateOrder command is handled by: The with_aggregate(command.aggregate_id) method will return a Domain::Order object recreated from events read from RES. On an Order command handler executes a create method using parameters from given command. The command is send from any controller using method from ApplicationController (notice it is validated before execution): Till now we have implemented: But when domain objects are not exposing their state we need another way to build the data for views - the read model. The model is a model as your application needs. It is denormalized - no costly joins, no lazy load, just select data and present them as is. It is tailor-made!\nAnd one more thing: it could be anything - relational DB, NoSQL store, graph database (i.e. RethinkDB ). Here we go with build in Rails Event Store pub/sub feature. If you took a look at source of CommandHandler module you might have noticed The es.subscribe(Denormalizers::Router.new) will create a subscription for all events to event handler Denormalizers::Router where events will be routed to appropriate denormalisers that will create/update read model defined as a simple ActiveRecord classes. Currently all this works synchronously - stay tuned for next post when some async features will be introduced. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-05"},
{"website": "Arkency", "title": "One year of React.js in Arkency", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2015/05/one-year-of-react-dot-js-in-arkency/", "abstract": "What always makes me happy with new technologies are hidden positive side-effects I‚Äôve never expected to happen. When I first introduced React in Arkency, which was on 4th of March 2014 I never expected it‚Äôll become so popular in our team. But should I be surprised? Let me tell you a story of how React allowed us to re-think our frontend architecture and make us a lot more productive . I worked on a big application and one of client requirements was a fully dynamic UI . Our client sells that SaaS to big companies - all employees are accustomed to working with desktop apps, like Excel. It was a very important goal to provide a familiar experience to them. We prepared a list of tickets and started working with our standard practices. And that was a hell lot of work! Demos for end-users were very often . With those demos priorities changed - and it was a matter of hours , not days. ‚ÄúHey, I got a VERY IMPORTANT CLIENT and demo for him will be the next monday - can you provide me a fully-working UI of <insert your big feature name here>?‚Äù - client asked such questions very often . On Thursday‚Ä¶ Clients are usually happy with our productiveness but such tight deadlines were exhausting , even for us. But we worked. Worked hard. Cutting the niceties out, leaving only the most necessary elements for demo purposes, even shipping the frontend-only prototypes which were good enough since our client was in charge of making a proper presentation - we can consult him and tell what he should avoid to show. But code started to slow us down . We designed our frontend code as a set of specialized ‚Äòmicro-apps‚Äô, each in a similar fashion - there was an use case object with domain logic and adapters which provided the so-called external world - backend communication, GUI and more. Then we‚Äôve used ‚ÄòGlue‚Äô objects which wired things together using using advice mechanism to wire an use case and adapters together (that is called aspect-oriented programming - look at this library if you are interested in this topic). This architecture was fine in a situation where such apps are not designed to communicate between themselves. But the more we dived into a domain the more we understood that some apps will communicate between themselves. A lot. The next problem was a GUI adapter. That was the part of every app then - we just needed a UI for performing our business. And it was the most fragile and the hardest part to get right. We‚Äôve used Handlebars + jQuery stack to deal with UI then. And this part took us like 80% time of shipping a feature . Now imagine: You‚Äôre working hard to build features for your client with a tight deadline. You are crunching your data, trying to understand a hard domain this project has . You carefully design your use case object to reflect a domain language of the project and wire in adapters. Then you write a set of tests to make sure everything works. After 8 hours of work you managed to finish tickets needed for an upcoming demo. Hooray! You contact your client that everything is done and close the lid of your laptop. Enjoy your weekend! Monday comes. Your client is super-angry since his demo went wrong. Ouch. What happened? You enter Airbrake and investigate. That click handler you set on jQuery was not properly instantiated after a mutation of the DOM. And confirmation works, yeah. But it has an undefined variable inside and you did not check it in your tests since it was such a small thing‚Ä¶ since testing is such a PITA in the jQuery-Handlebars stack . And your business logic code was fine. Your Rails code was awesome. But fragility of your GUI adapter punched you (and your embarrassed client) in the face. Atmosphere was dense. And we still had big architectural changes to be done‚Ä¶ HOW CAN WE FIND TIME FOR THAT? Then I decided something had to be done about it. I went on a camp with some fellow developers and a friend of mine had a presentation about React. I had a laptop opened and was looking at UI code of this project. The React presentation was good. I imagined how declarativeness of React will help me with avoiding such embarrassments we had before. I needed to talk with my co-workers about it. After I got back from a camp, this was my first commit: I rewrote this nasty part that destroyed the demo of my client in React. It took me 4 hours with a deep dive to React docs since I had no experience with React before. Previous version took me 6 hours of writing and debugging a code. In a technology I understood well and had experience with. And it worked. It worked without a debug‚Ä¶ I then talked with my co-workers and showed them the code. They decided to give React a try. First two weeks were tough . Unfamiliarity of React was slowing us down. But in this additional time we were thinking about answers to questions like ‚Äúhow to split this UI into components?‚Äù or ‚Äúhow to pass data to this component in a sane way?‚Äù . There was less time to make all this write code-refresh browser-fix error cycles we had before. Declarativeness of React allowed us to think about code easier and took all nasty corner-cases of handling user interactions and changing page away. And ultimately we spent less and less time of writing our UI code . Next demos went fine. React gave us more time to think about more important problems. We finally found time to change our architecture - we replaced advice approach with event buses as a first step. As the project grew, we needed to overcome performance problems - we loaded the same data many times from different API endpoints. We fixed this problem with introducing stores, highly influenced by a similar idea from Flux architecture which is also a part of the React ecosystem. But I‚Äôll be honest here: it was not React that fixed our problems. Not directly. What helped us is that writing UI code became easy - and fun! Fun is a big thing here. What unlocked our full potential is that we stopped thinking about writing UI code as an unpleasant task. We started to experiment freely. We had more time to think about more important problems - writing UI was faster with React. We spent less time in ‚Äòfailure state‚Äô. We had a more organised way to think about UI elements - components abstraction helped us to produce tiny pieces fast and without failures. Our frontend tests were much easier to write, so we improved our code coverage a lot. All those tiny side-effects React gave to us made us successful. Now we got React in many projects. In many states - some apps have the UI fully managed by React (like the project I am writing about here), some got both Rails views and React-managed parts. Some got parts in other technologies like Angular. We write blogposts about React and other front-end technologies we started to love. More and more people in Arkency that used to dislike frontend tasks became happy with them. You can be too! Since React was so successful for us we decided to write a book about it. You can buy the beta version now for $49 . We took an effort to make it friendly for Rails developers. It consists of: We had fun writing it. We put our best practices to this book - and we experimented a lot to examine those practices. Me and my co-workers worked to improve quality of its content. The side effects of React helped us with our projects. You have an occasion to bring fun to your front-end code too! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-10"},
{"website": "Arkency", "title": "How to store emoji in a Rails app with a MySQL database", "author": ["Jakub Kosi≈Ñski"], "link": "https://blog.arkency.com/2015/05/how-to-store-emoji-in-a-rails-app-with-a-mysql-database/", "abstract": "Suppose you have a Rails app and you are storing your data in a MySQL database. You were requested to add emoji support to your application. Probably you are thinking: Oh, that‚Äôs simple, it‚Äôs just unicode, isn‚Äôt it? The answer is: no . Unfortunately, MySQL‚Äôs utf8 character set allows to store only a subset of Unicode characters - only those characters that consist of one to three bytes. Inserting characters that require 4 bytes would result in corrupted data in your database. Look at this example: As you can see, using utf8 character set is not enough. You are getting a warning and your data is truncated at the first 4-bytes unicode character. MySQL 5.5.3 introduced new character set - utf8mb4 that maps to real UTF-8 and fully support all Unicode characters, including 4-bytes emoji. It is fully backward compatible, so there should be no data loss during migrating your database. You just need to convert your tables to the new character set and change your connection‚Äôs settings. You can do it in migration: Please notice the VARCHAR(191) fragment. There is one important thing you should know - when switching to utf8mb4 charset, the maximum length of a column or index key is the same as in utf8 charset in terms of bytes. This means it is smaller in terms of characters, since the maximum length of a character in utf8mb4 is four bytes, instead of three in utf8 . The maximum index length of InnoDB storage engine is 767 bytes, so if you are indexing your VARCHAR columns, you would need to change their length to 191 instead of 255. You should also change your database.yml and add encoding and (optionally) collation keys: Now you are ready to handle emoji üëç After changing character set, you may experience the Mysql2::Error: Specified key was too long; max key length is 767 bytes: CREATE UNIQUE INDEX error when performing rake db:migrate task. It is related to the InnoDB maximum index length described in previous section. There is a fix for schema_migrations table in Rails 4+, however you still can experience this error on tables created by yourself. As far as I am concerned this is still not fixed in Rails 4.2. You can resolve this issue in two ways: You wouldn‚Äôt experience this issues when using PostgreSQL, but sometimes you just have to support legacy application that uses MySQL and migrating data to other RDBMS may not be an option. If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-06"},
{"website": "Arkency", "title": "Building a React.js event log in a Rails admin panel", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2015/05/building-a-react-dot-js-event-log-in-a-a-rails-admin-panel/", "abstract": "Recently I talked with some awesome Rails developers about the Event Sourcing.\nWe talked about introducing ES concept in a legacy Rails applications.\nThat conversation inspired me to write a post about our experiences with the Event Sourcing.\nThe most important thing to remember is that we don‚Äôt have to implement all blocks related to ES at the beginning (Aggregates, Read models, Denormalizers and so on).\nYou can implement only one pattern and improve it slowly to full an Event Sourcing implementation.\nThis strategy will involve small steps down a long road. This is how we work in the Arkency. We have experimented with the Event Sourcing in couple client‚Äôs projects.\nSome time ago we launched our vision of an Event Store (we call it RES ) which we use in customer‚Äôs applications.\nIt help as a lot to start Event-think during implementation. This example will show you how to simply introduce an ES in a Rails app.\nWe will create a simple events browser. We will collect events describing user‚Äôs registration.\nEvents will be saved to streams, each stream per user. This way we will create a simple log. The idea is to display events to the admin of the Rails app. We treat it as a ‚Äúmonitoring‚Äù tool and it is also first step to use events in a Rails application. We start by adding a rails_event_store gem to our Gemfile ( installation instructions ).\nNext thing is that we need some events to collect. We have to create an event class representing a user creation.\nTo do this we will use the class provided by our gem. Now we need to find place to track this event. I thing that UsersController will be the best place. In the create method we build new User‚Äôs model.\nAs event_data we save information about user and some additional data like controller name or IP address. The last thing is to implement a simple API to get information about streams and events. Instead of using Rails views we will use React‚Äôs components. I created four components. The view structure you can see on following schema. I use coffeescript to build components. As you can see on following example I use requirejs to manage them.\nRecently we launched a great book about React where you can read more about our experiences with React and coffeescript.\nOf course you could go with JSX as well. Last thing is to render above components on the view. I created an additional class to build the ShowStreams component and render it on the page.\nI implemented it this way because we use the react-rails gem in version 0.12 . In newer version you can use react_component helper to render component on server side.\nThis makes using easier to start with React with Rails views. The last piece of the puzzle is the Storage class. This simple class is responsible for calling the API using Ajax. The above example shows how simple is to introduce events in you app. For now it is only simple events log. We started to collect events related to User model.\nWe don‚Äôt build state from this events. Although you can use them in some Read models .\nIn next step you can collect all events related to User. Then you will be able to treat User as a Aggregate and build state from events. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-13"},
{"website": "Arkency", "title": "Introducing Read Models in your legacy application", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/2015/05/introducing-read-models-in-your-legacy-application/", "abstract": "Recently on our blog you could read many posts about Event Sourcing. There‚Äôre a lot of new concepts around it - event, event handlers, read models‚Ä¶ In his recent blogpost Tomek said that you can introduct these concepts into your app gradually. Now I‚Äôll show you how to start using Read Models in your application. In our case, the application is very legacy. However, we already started publishing events there because adding a line of code which publishes an event really cost you nothing. Our app is a website for board games‚Äô lovers. On the games‚Äô pages users have ‚ÄúLike it‚Äù button. There‚Äôs a ranking of games and one of the columns in games‚Äô ranking is ‚ÄúLiked count‚Äù. We want to introduce a read model into whole ranking, but we prefer to refactor slowly. Thus, we‚Äôll start with introducing our read model into only this one column - expanding it will be simple. We‚Äôll use just a database‚Äôs table to make our read model. The events which will be interesting for us (and are already being published in application) are AdminAddedGame , UserLikedGame and UserUnlikedGame . I think that all of them are pretty self-explanatory. But why would you like to use read models in your application anyway? First of all, because it‚Äôll make reasoning about your application easier. Your event handlers are handling writes: they update the read models. After that, reading data from database is simple, because you just need to fetch the data and display them. The first thing we should do is introducing GameRanking class inherited from ActiveRecord::Base which will represent a read model. It should have at least columns game_id and liked_count . Now, we are ready to write an event handler, which will update a read model each time when an interesting event occurs. Firstly, we will start from having records for each game, so we want to handle AdminAddedGame event. In our GamesController or wherever we‚Äôre creating our games, we subscribe this event handler to an event: Remember, that this is legacy application. So we have many games and many likes, which doesn‚Äôt have corresponding AdminAddedGame event, because it was before we started gathering events in our app. Some of you may think - ‚ÄúLet‚Äôs just create the GameRanking records for all of your games!‚Äù. And we‚Äôll! But we‚Äôll use events for this : ). However, there‚Äôs also another road - publishing all of the events ‚Äúback in time‚Äù. We could fetch all likes already present in the application and for each of them create UserLikedGame event. So, as I said, we are going to create a snapshot event. Such event have a lot of data inside, because basically it contains all of the data we need for our read model. Firstly, I created RankingHadState event. Now we should create a class, which we could use for publishing this snapshot event (for example, using rails console). It should fetch all games and its‚Äô likes count and then publish it as one big event. Now we only need to add handling method for this event to our event handler. After this deployment, we can log into our rails console and type: Now we have our GameRanking read model with records for all of the games. And all new ones are appearing in GameRanking, because of handling AdminAddedGame event. We can finally move on to ensuring that liked_count field is always up to date.\nAs I previously said, I‚Äôm assuming that these events are already being published in production, so let‚Äôs finish this! Obviously, we need handling of like/unlike events in the event handler: After that you should subscribe this event handler to UserLikedGame and UserUnlikedGame events, in the same way we did it with AdminAddedGame in the beginning of this blogpost. Now we‚Äôre almost done, truly! Notice that it took some time to write & deploy code above it. Thus, between running CopyCurrentRankingToReadModel on production and deploying this code there could be some UserLikedGame events which weren‚Äôt handled. And if they weren‚Äôt handled, they didn‚Äôt update liked_count field in our read model. But the fix for this is very simple - we just need to run our CopyCurrentRankingToTheReadModel in the production again, in the same way we did it before. Our data will be now consistent and we can just write code which will display data on the frontend - but I believe you can handle this by yourself. Note that in this blog post I didn‚Äôt take care about race conditions. They may occur for example between fetching data for HadRankingState event and handling this event. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-14"},
{"website": "Arkency", "title": "Using domain events as success/failure messages", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/2015/05/using-domain-events-as-success-slash-failure-messages/", "abstract": "We had an issue recently with one of our internal gems used to handle all communication with a external payment gateway. We are using gems to abstract a bounded context (payments here) and to have an abstract anti-corruption layer on top of external system‚Äôs API. When our code is triggered (no matter how in scope of this blog post) we are using our gem‚Äôs methods to handle payments. There are different payment gateways - some of them respond synchronously some of them prefer asynchronous communication. To avoid coupling we publish an event when the payment gateway responds. (a very simplified version - payments are much more complex) You might have noticed that when our API call fails we rescue an error and raise our one. It is a way to avoid errors from the 3rd party client leak to our application code. Usually that‚Äôs enough and our domain code will cope well with failures. But recently we got a problem. The business requirements are: When refunding a batch of transactions gather all the errors and send them by email to support team to handle them manually . That we have succeeded to implement correctly. One day we have received a request to explain why there were no refunds for  a few transactions. The first thing we did was to check history of events for the aggregate performing the action (Order in this case). We have found an entry that refund of order was requested (it is done asynchronously) but there were no records of any transaction refunds. It could not be any. Because we did not publish them :( This is how this code should look like: The raise of an error here was replaced by the use of domain event. What is raising an error? It is a domain event ‚Ä¶ when domain is the code. By publishing our own domain event we give it a business meaning. Check the Andrzej‚Äôs blog post Custom exceptions or domain events? for more details. Of course we could do it without the use of domain events that are persisted in Rails Event Store but the possibility of going back in the history of the aggregate is priceless. Just realise that a stream of the domain events that are responsible for changing the state of an aggregate are the full audit log that is easy to present to the user. And one more thing: you want to have a monthly report of failed refunds of transactions? Just implement a handler for TransactionRefundFailed the event and do your grouping, summing & counting and store the results. And by replaying all past TransactionRefundFailed events with use of your report building handler you will get a report for the past months too! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-15"},
{"website": "Arkency", "title": "Start using ES6 with Rails today", "author": ["Wiktor Mociun"], "link": "https://blog.arkency.com/2015/05/start-using-es6-with-rails-today/", "abstract": "Source: Asif Salman The thing that made me fond of writing front-end code was CoffeeScript. It didn‚Äôt drastically change syntax. Coffee introduced many features that made my life as a web developer much easier (e.g. destruction and existential operators). That was a real game changer for Rails developers. We can write our front-end in language that is similar to Ruby and defends us from quirks of JavaScript. Fortunately the TC39 committee is working hard on sixth version of ECMAScript. You can think about it as an improved JavaScript. It added many features, many of which you may have already seen on CoffeeScript. You can read about some goodies added to ES6 in this blogpost . The best part of ES6 is that you can use it, despite the fact it hasn‚Äôt been finished yet . New features of ES6 can be emulated in JavaScript (used in our web browsers) using Babel . It provides full compatibility. However one of the features may require some extra work. One of most exciting features of ES6 are built-in modules. Before ES6 we used solutions like CommonJS or RequireJS. By default Babel uses CommonJS modules as a fallback. If you didn‚Äôt use any type of packaging and want to use one, you will need to setup one. Sprockets 4.x promise to bring ES6 transpiling out of the box. This release doesn‚Äôt seem to come up soon, so we need to find some way around . On babel website we can find link to sprockets-es6 gem, which enables ES6 transpiling for sprockets. Unfortunately it does not come without problems - the gem requires sprockets in version ~3.0.0.beta . By default babel converts ES6 modules to CommonJS modules. Two gems providing CommonJS ( browserify-rails and sprockets-commonjs ) requires sprockets to be in version lower than 3.0.0. You can try using other gem to get JavaScript packaging like requirejs-rails gem . Remember to register ES6 module transformer with valid option in Sprockets. See this test file for example usage. If you decide to go with this method, you just need to put these two files in Gemfile. And now run bundle install . After installation you can write your ES6 code in files with .es6 extension. Marcin wrote some time ago about unusual approach for asset serving in Rails applications. We can completely remove sprockets and do it on our own with simple Node.js application. We want to remove any dependencies on Sprockets or any other Ruby gem, when it comes to asset serving. Moreover, using this method, we get faster overall asset compiling than with Sprockets. With Gulp, we can use babelify and browserify node packages in our asset processing process. It let us to use all ES6 features without any inconvenience. You can see example Gulpfile.js with ES6 transpiling and SASS compiling on gist: Gulpfile.js There are many more workarounds to get ES6 in Rails environment that doesn‚Äôt require discarding Sprockets. Unfortunately none of them are good enough to mention as production-ready. I strongly recommend going with Gulp. It‚Äôs simple, powerful and provides native environment to work with assets. \nIf you don‚Äôt want to switch from Sprockets, you can try-out sprockets-es6 gem. If you want to receive more articles about Rails and front-end, sign-up for our newsletter below. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-16"},
{"website": "Arkency", "title": "You can use CoffeeScript classes with React - pros and cons", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2015/05/you-can-use-coffeescript-classes-with-react-dot-js-pros-and-cons/", "abstract": "One of the big advantages of React components is that they are easy to write. You create an object literal and provide functions as fields of your object. They you pass this object to a React.createClass function. In the past React.createClass was a smart piece of code. It was responsible for creating a component‚Äôs constructor and instantiating all fields necessary to make your plain object renderable using React.renderComponent . It was not an idiomatic JavaScript at all. Not to mention it broke the basic SRP principles. It changed with a 0.12 version of React. React developers took a lot of effort to improve this situation. A new terminology was introduced. React.createClass now does a lot less. One of the most important change for me is that now you can use CoffeeScript classes to create React components. Apart from the nicer syntax, it makes your code more idiomatic. It emphasizes the fact that your components are not a ‚Äòmagic‚Äô React thing, but just CoffeeScript objects. I want to show you how you can use the new syntax - and what are pros and cons of this new approach. Starting from React 0.12 the new terminology is introduced. There are now elements - they are an intermediary step between component classes and components . Since before 0.12 children type was not formally specified, we have a new term for that - it is a node now. There is also a fragment concept introduced, but it is beyond the scope of this blogpost - you can read more about it here . As I said before, previously React.createClass made a lot of things. It made your object renderable by adding private fields to an object passed. It made a constructor to allow passing props and children to create a component. Now all this functionality is gone. React.createClass now just adds some utility functions to your class, autobinds your functions and checks invariants - like whether you defined a render function or not. That means your component classes are not renderable as they are. Now you must turn them into ‚Äòrenderable‚Äô form by creating an element . Previously you passed props and children to a component class itself and it created an element behind the scenes. This constructor created by React.createClass now needs to be called by you explicitly. You can do it calling React.createElement function. React elements can be passed to render a component. Component classes can‚Äôt be rendered. You create elements from your component classes. This is a signature of the React.createElement function: Where type can be a string for basic HTML tags ( \"div\" , \"span\" ) or a component (like in the example above). props is a plain object, and children is a node . A node can be: Node is just a new fancy name for arguments for children you know from previous versions of React. This is a bit verbose way to create elements from your component classes. It also prevents you from an easy upgrade to 0.13 if you are not using JSX (we got this process covered in our book ). Fortunately, with a little trick you can use your old Component(props, children) style of creating elements. React provides us React.createFactory function which returns a factory for creating elements from a given component class. It basically allows you to use the ‚Äòold‚Äô syntax of passing props and children to your component classes: Notice that you can still use React.DOM like you‚Äôve used before in React. It is because all React.DOM component classes are wrapped within a factory. Now it makes sense, isn‚Äôt it? Also, JSX does the all hard work for you. It creates elements under the hood so you don‚Äôt need to bother. There is a trend in the React development team to put backwards compatibility into the JSX layer. All those changes made possible to have your component classes defined using a simple CoffeeScript class. Moving the responsibility of ‚Äúrenderable‚Äù parts to createElement function allowed React devs to make it happen. If you want to use class syntax for your React component classes in ES6, it is simple. Your old component: Becomes: Notice that getInitialState and getDefaultProps functions are gone. Now you set initial state directly in a constructor and pass default props as the class method of the component class. There are more subtle differences like that in class approach: As you can see, this approach is more ‚ÄòCoffeeScript‚Äô-y than React.createClass . First of all, there is an explicit constructor you write by yourself. This is a real plain CoffeeScript class. You can bind your methods by yourself. Syntax aligns well with a style of typical CoffeeScript codebase. Notice that you are not constructing these objects by yourself - you always pass a component class to createElement function and React.render creates component objects from elements. Pure classes approach brings React closer to the world of idiomatic Coffee and JavaScript. It is an indication that React developers does not want to do ‚Äòmagic‚Äô with React component classes. I‚Äôm a big fan of this approach - I favor this kind of explicitness in my tools. The best part is that you can try it out without changing your current code - and see whether you like it or not. It opens a way for new idioms being introduced - idioms that can benefit your React codebase. We are going to release a ‚ÄúRails meets React.js‚Äù update with all code in the book updated to React 0.13 this Friday . All people who bought the book already will get this update (and all further updates) for free. It is aimed for Rails developers wanting to learn React.js by example. For the price of $49 you get: Interested? Grab a free chapter or watch a quick, 3-minute overview of it now. You can buy the book here. Use V13UPDATE code to get a 25% discount ! Join the group of 350+ happy customers who learned how to build dynamic user interfaces with React and Rails! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-05-31"},
{"website": "Arkency", "title": "How good are your Ruby tests? Testing your tests with mutant", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/06/how-good-are-your-ruby-tests-testing-your-tests-with-mutant/", "abstract": "There are many kinds of bugs. For the sake of simplicity let me divide them into new-feature-bugs and regression-bugs. New-feature-bugs are the ones that show up when you just introduce a totally new feature. Let‚Äôs say you‚Äôre working on yet another social network app. You‚Äôre adding the ‚Äúfriendship‚Äù feature. For some reason, your implementation allows inviting a person, even though the invitee was already banned. You‚Äôre showing this to the customer and they catch the bug on the testing server. They‚Äôre not happy that you missed this case. However, it‚Äôs something that can be forgiven, as it was caught quickly and wasn‚Äôt causing any damage. Now imagine that the feature was implemented correctly in the first place. It was all good, deployed to production. After 6 months, the programmers are asked to tweak some of the minor details in the friendship area. They‚Äôre following the scout rule (always leave the code cleaner than it was) so they do some refactoring - some methods extractions, maybe a service object. Apparently, they don‚Äôt follow the safe, step-by-step refactoring technique to extract a service object . One small feature is now broken - banned users can now keep inviting other users endlessly. Some of the bad users notice this and they keep annoying other people. The users are frustrated and submit the bug to the support. The support team notices the customer and the programmers team. Can you imagine, what happens to the trust to the programming team? ‚ÄúWhy on earth, did it stop working?‚Äù the customer asks. ‚ÄúWhy are you changing code that was already working?‚Äù It‚Äôs so close to the famous ‚ÄúIf it works, don‚Äôt touch it‚Äù. From my experience, the second scenario is much harder to deal with. It breaks trust. Please note, that I used a not-so important feature, over all. It could be part of the cart feature in the ecommerce system and people not being able to buy things for several hours could be thousands dollars loss for the company. How can we avoid such situations? How can we avoid regression bugs? Is ‚Äúnot touching the code‚Äù the only solution? First of all - there‚Äôs no silver bullet. However, there are techniques that helps reducing the problem, a lot. You already know it - write tests. Is that enough? It depends. Do you measure your test coverage? There are tools like rcov and simplecov and you may be already using them. Why is measuring the test coverage important? It‚Äôs useful when you‚Äôre about to refactor something and you may check how safe you are in this area of code. You may have it automated or you may run it manually just before the refactoring. In RubyMine, my favourite Ruby IDE, they have a nice feature of highlighting the test-covered code with green colour - you‚Äôre safe here. Unfortunately, rcov and simplecov have important limitations. They only check line coverage. What does it mean in practice? In practice, those tools can give you the false feeling of confidence. You see 100% coverage, you refactor, the tests are passing. However, some feature is now broken. Why is that?\nThose tools only check if the line was executed during the tests, they don‚Äôt check if the semantics of this line is important. They don‚Äôt check if replacing this line with another one changes anything in the tests result. This is where mutation testing comes in. Mutation testing takes your code and your tests. It parses the code to the Abstract Syntax Tree. It changes the nodes of the tree (mutates). It does it in memory. As a result we now have a mutant - a mutated version of your code. The change could be for example removing a method call, changing true to false, etc. There‚Äôs a big number of such mutations. For each such change, we now run the tests for this class/unit. The idea here is that the tests should kill the mutant. Killing the mutant happens when tests fail for a mutated code. Killing all mutants means that you have a 100% test coverage. It means that you have tests for all of your code details. This means you can safely refactor and your tests are really covering you. Again, mutant is not a silver bullet. However, it greatly increases the chance of catching the bugs introduced in the refactoring phase. It‚Äôs a totally different level of measuring test coverage than rcov or simplecov. It‚Äôs even hard to compare. Suggested actions for you: If you‚Äôre not using any kind of test coverage tools, try simplecov or rcov . That‚Äôs a good first step. Just check the coverage of the class you have recently changed. Watch this short video that I recorded to show you the mutant effect in a Rails controller and this video which shows visually how mutant changes the code runtime: Why I want to introduce mutation testing to the rails_event_store gem Mutation testing and continuous integration Listen to the third episode of the Rails Refactoring Podcast , where I talked to Markus, the author of mutant. Markus is a super-smart Ruby developer, so that‚Äôs especially recommended. Subscribe to the Arkency YouTube channel - we are now regularly publishing new, short videos. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-06-10"},
{"website": "Arkency", "title": "Thanks to repositories...", "author": ["Piotr Macuk"], "link": "https://blog.arkency.com/2015/06/thanks-to-repositories/", "abstract": "Source: Wikimedia Commons I am working in Arkency for 2+ months now and building a tender documentation system for our client. The app is interesting because it has a dynamic data structure constructed by its users. I would like to tell you about my approaches to the system implementation and why the repository pattern allows me to be more safe while data structure changes. The app has users with its tender projects. Each project has many named lists with posts. The post structure is defined dynamically by the user in project properties. The project property contains its own name and type. When the new project is created it has default properties. For example: ProductId(integer), ElementName(string), Quantity(float) Unit(string), PricePerUnit(price). User can change and remove default properties or add custom ones (i.e. Color(string)). Thus all project posts on the lists have dynamic structure defined by the user. I was wondering the post structure implementation. In my first attempt I had two tables. One for posts and one for its values (fields) associated with properties. The database schema looked as follows: That implementation was not the best one. Getting data required many SQL queries to the database. There were problems with performance while importing posts from large CSV files. Also large posts lists were displayed quite slow. I have removed the values table and I have changed the posts table definition as follows: Values are now hashes serialized in JSON into the values column in the posts table. In the typical Rails application with ActiveRecord models placed all around that kind of change involve many other changes in the application code. When the app has some code that solution is scary :( But I was lucky :) At that time I was reading the Fearless Refactoring Book by Andrzej Krzywda and that book inspired me to prepare data access layer as a set of repositories. I have tried to cover all ActiveRecord objects with repositories and entity objects. Thanks to that approach I could change database structure without pain. The changes was only needed in database schema and in PostRepo class. All application logic code stays untouched. Placed in app/models . Used only by repositories to access the database. Placed in app/entities . Entities are simple PORO objects with Virtus included. These objects are the smallest system building blocks. The repositories use these objects as return values and as input parameters to persist them in the database. Placed in app/repos/post_repo.rb . PostRepo is always for single list only. The API is quite small: The properties array is given in initialize parameters. Please also take a note that ActiveRecord don‚Äôt leak outside the repo. Even ActiveRecord exceptions are covered by the repo exceptions. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-06-11"},
{"website": "Arkency", "title": "Subscribing for events in railseventstore", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/2015/06/subscribing-for-events-in-rails-event-store/", "abstract": "In my post Building an Event Sourced application I‚Äôve included sample code to setup denormalizers (event handlers) that will build a read model: Because that is only a sample application showing how easy is to build an Event Sourced application using Ruby/Rails and Rails Event Store there were some shortcuts. Shortcuts that should have never been there. Shortcuts that have made some doubts for others who try to build their own solution. The router was defined as: And denormalisers were implemented as: But we could remove it completely and we do not need that case at all! All this code could be rewritten using rails_event_store subscriptions as follows: You see? No Router at all! It‚Äôs event store who ‚Äúknows‚Äù where to send messages (events) based on subscriptions defined. Sometimes when you have a simple application like this it is tempting to define ‚Äúconvention‚Äù and avoid the tedious need to setup all subscriptions. It seems to be easy to implement and (at least at the beginning of the project) it seems to be elegant and simple solution that would do ‚Äúthe magic‚Äù for us. I wonder what would happen if we called it ‚ÄúImplicit Assumptions‚Äù instead of ‚ÄúConvention over Configuration‚Äù. Naming is important! If we do not use convention but instead implicit assumption we will realise that it is not that simple and elegant at it looks like. Even worse, project tent to grow. When you will start using domain events you will want more and more of them. You could even want to have several handles for a single event ;) And maybe your handlers will need some dependencies? ‚Ä¶ Here is the moment when your simple convention breaks! By coding the subscriptions one by one, maybe grouping them in some functional areas (bounded context) and clearly defining dependencies you could have more clear code, less ‚Äúmagic‚Äù and it should be easier to reason how things work. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-06-12"},
{"website": "Arkency", "title": "Arkency goes React", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/07/arkency-goes-react/", "abstract": "From its beginnings in 2007, Arkency was connected to Ruby and Rails. We‚Äôre still most active in those technologies.\nHowever, over time there‚Äôs another R-named technology that quickly won our hearts - React.js. Our journey with JavaScript was quite long already and sometimes even painful. We‚Äôve started with pure JavaScript, then went all CoffeeScript. Nowadays we introduce ES 6. We‚Äôve experimented with Backbone (some parts are OK), we‚Äôve had painful experiences with Angular (don‚Äôt ask‚Ä¶). We‚Äôve been proudly following the no-framework JS way, while being very hexagonal-influenced ( https://hexagonaljs.github.io is alive and fine). From the hexagonal point of view, the views and the DOM are just adapters. They‚Äôre not especially interesting architecturally. In practice, rendering the views is one of the hardest tasks, thanks browsers. At the beginning of our hexagonal journey we went with just jQuery. We were careful not to use outside of the adapters. This was just an implementation detail. It wasn‚Äôt bad. It wasn‚Äôt really declarative, though. For richer UIs, this was visibly problematic. When we learnt about React.js it felt like the missing piece in our toolbox. Thanks to our architectures, it was easy to introduce React.js gradually. Suddenly, all Arkency projects were switching to React based views. React is a small library. It doesn‚Äôt offer that many features. That‚Äôs why it‚Äôs so great. It does one thing and it does it well - handles the DOM rendering. What‚Äôs also important, it does it very fast. If you worked on big JS frontends, you know how difficult it was. We started sharing our React.js knowledge with our Ruby community, with which we feel strongly connected. We wrote many blog posts. At some point, we also started writing a React.js book for Rails developers . That‚Äôs where we felt the best - switching to React views from the Rails perspective. If you want to read the whole story (and more reasons) why we switched to React.js, then go here: 1 year of React.js in Arkency Over time, we started to do more. More blog posts, more chapters in the book. We added a Rails repo which goes with the book. At the same time, we were contacted by more and more clients who were mostly interested in our React.js experience and needed help with rebuilding their frontends. Then we came up with React.js koans . The idea was simple - let people learn React.js. Despite our Rails roots, we didn‚Äôt see any sense to couple this idea with Rails. The koans use ES6 and they run on node-based tooling. With koans, there was nothing Ruby-related, so it wasn‚Äôt targeted only to our beloved Ruby community. The popularity of the React.js koans was bigger than we ever expected. Within one day we went to over 1000 GitHub stars. The repo was trending and Arkency was the second trending developer on GitHub for a moment (ahead of Facebook and Google). When we worked on Koans, before the launching day - we were often discussing internally whether we need to extend ‚Äúour audience‚Äù to more than Ruby developers. It felt out of our comfort zone. It‚Äôs nice to feel that we are surrounded with like-minded Ruby devs. We have some recognition in the Ruby market. Outside of that, we‚Äôre not really known. At that time, we called the potential new audience, ‚Äúthe JavaScript developers‚Äù. Long story short - we‚Äôre opening a new chapter in the Arkency history. We‚Äôre announcing the React.js Kung Fu. We‚Äôre going to teach more and educate even more, about React.js. We‚Äôre no longer limiting ourselves to the Ruby audience with this message. We‚Äôll be releasing a new book about React.js very soon. This time, the book doesn‚Äôt require any Rails background. We‚Äôll be releasing more screencasts and blogposts. We‚Äôre also opening a new mailing list, that will be mostly about React.js and JS frontends. We‚Äôre still in the Ruby community, though. We are working on a new update to the Rails Refactoring book . BTW, this book is at the moment part of the Ruby Book Bundle .  The bundle contains 6 advanced Ruby books for a great price. I just presented a webinar about Rails and RubyMine . More stuff is coming here. We‚Äôre not leaving the Ruby community, we‚Äôre just broadening the React.js communication channel to more developers. Let me repeat - Arkency is still a mostly Ruby company. We love Ruby. However, we have a great team of developers and this allows us to do more things. One of those new things is React.js. Keep in mind, that Ruby and React.js are just technologies. They change, over the years. What is not changing is the set of practices. We‚Äôre doing TDD, despite the technology choice. We believe in small, decoupled modules. We understand the importance of higher-level architecture. We keep improving at understanding the domains of our clients. We translate the domain to code using the DDD techniques. We create bounded contexts. We let the bounded contexts communicate via events and we often consider CQRS and Event Sourcing. We measure the production applications. We believe in the importance of async and remote cooperation . We split features into smaller tasks.\nThe practices define us - not the specific technologies or syntaxes. React.js deserves to be listed as one of the R-technologies in our toolbox. Open this new chapter with us - subscribe to the new mailing list and stay up to date with what we‚Äôre cooking. Thanks for being with us! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-07-02"},
{"website": "Arkency", "title": "Am I ignored in my async team?", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/07/am-i-ignored-in-my-async-team/", "abstract": "When we work in async teams, we have many benefits from it - people not blocked on each other, freedom of place, freedom of time.\nWe also have some drawbacks. You can‚Äôt expect an instant reply to whatever you ask for. In the nature of async, you can‚Äôt even be sure when you get the reply. What‚Äôs interesting, you can‚Äôt even be sure that all (any?) people actually received your message. In distributed systems we have the same problems (is the other server down?). We‚Äôre not machines, but some techniques may be worth applying. Which ones come to your mind? In some cases, you may think that you are being ignored. That‚Äôs a bad feeling (been there). What I do in such case is that I remind myself that I also didn‚Äôt reply to every conversation that is happening. Sometimes I‚Äôm the one who ignores. It‚Äôs never intended. Often, it‚Äôs because I‚Äôm super busy and I don‚Äôt want to switch the context immediately. I try to be organized and when I see an important conversation to participate I add a task to my GTD (recently it‚Äôs Apple Reminders). It‚Äôs not always the case, though. Sometimes I fuck up and don‚Äôt remember to come back to that conversation. If there‚Äôs more people like me who don‚Äôt answer  - the asking person may feel terrible and that‚Äôs sad. I know that this may be expecting too much, but I expect the asking person to ‚Äúinternally sell‚Äù the conversation/question/problem in better ways. First of all, friendly-ping the topic again after some time. Consider using another medium - slack/hackpad/discourse/mumble/trello/github/email/standup. Async teams are great but require some special skills - overcommunication, proactiveness, trust (always assume the best intentions), selling skills, good writing. BTW, As Arkency, we add anarchy to the async environment. What does it mean in practice? You don‚Äôt need to ask for agreement on something. It‚Äôs good to overcommunicate what you‚Äôre doing and make your idea happen. In the worst case, me or someone else will jump and say - ‚Äúdon‚Äôt do that please‚Äù and this can start a discussion. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-07-06"},
{"website": "Arkency", "title": "Testing an Event Sourced application", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/2015/07/testing-event-sourced-application/", "abstract": "Some time ago I‚Äôve published a sample application showing how to build a simple event sourced application using Rails & RES. But there was a big part missing there - the tests. My sample uses CQRS approach to handle all operations. That means the control flow is as follow: This is a basic pattern how good test should be created. There are 3 parts: Arrange - when you setup initial state for a test, Act - where you perform actual operation you want to test and Assert - when you check results. And the AAA pattern should be preserved for Event Sources application. How to build an initial state when you don‚Äôt have a state? This should be quite easy. Any state is a derivative of domain events. You could build any state by applying domain events. To build a state you just need some events: Then we have our test state arranged. Notice that I‚Äôve used fake event store & domain services to avoid dependencies and have really fast tests. In Event Sourced application act (operation we want to test) is usually handling of a command. To do it you just need a command, you need the command handler and then just dispatch the command to the command handler. The same Command::Execute module is used in ApplicationController to dispatch real commands to the system. You should not assert on the current state, actually you should not rely on a state at all. All you need to verify is if the correct domain events have been produced. And because all state is a result of events checking what have been produced has a nice side effect. You test if all expected domain events have been produced and if only the ones expected. In that case, you test if any unexpected change have not been introduced. Remember that any command may end up with an error. There could be various reasons, technical ones (oh no! regression again?), or error could be just a result of some business rules validations. Complete code sample for blog post could be found here . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-07-07"},
{"website": "Arkency", "title": "Three most telling files in legacy Rails apps", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2015/08/three-most-telling-files-in-legacy-rails-apps/", "abstract": "Photo available thanks to the courtesy of wackybadger . License: CC-BY-SA 2.0 When working in a consultancy agency the ability to work with legacy applications is necessary. Your clients very often come with their codebases. If the app is already in use a rewrite doesn‚Äôt sound like a good idea - it‚Äôs best to approach to make improvements incrementally. But the truth is that this skill is very important for every Rails developer. Even if you‚Äôre working on your own projects or creating the Rails-based startup from scratch after some time your code will grow. Tests are very likely to become slower. There will be more and more implicitness within your code. You accept that because you are within the context for a long time. You know tradeoffs, assumptions and decisions - often you made them by yourself. It‚Äôs good to have skills to measure the situation and have some kind of guidelines to protect yourself from mistakes while refactoring. In this blog post, I want to show you three simple techniques we use to get ourselves a solid starting point for working with legacy Rails apps. You will be able to apply it to projects you‚Äôre working on and improve your planning and refactoring thanks to it. There are some steps you can take while working with legacy Rails apps to get an idea what problem you‚Äôd likely face when working with them. They are great because they give you some kind of overview with a very little commitment - you don‚Äôt need to read the application code at all. They may seem obvious to you. But having a structured approach (even in a form of a checklist) to those steps can be really beneficial for you. This is the first step you can do when investigating a legacy app. Gemfile tends to be a great source of knowledge when it comes to Rails apps. Since Rails ecosystem is so rich, people are accustomed to not reinventing the wheel but using gems made by the community. It‚Äôs especially true when the codebase was left by a Rails software shop. With codebases written by people with a smaller expertise you can still find code like external services integration reimplemented by previous codebase‚Äôs developers. Gemfile can also provide you heuristic about how the code was made. With projects with many (like 80-90+ gems) gems within the Gemfile it‚Äôs likely you‚Äôll see smells like dead code, gems included but unused (left after refactoring) and different approaches to structuring the same pieces of architecture (which is not that bad). While reviewing the Gemfile , you should take a great focus about those things: Your applications usually are all about the data you store and process. Since by default Rails apps use relational databases, they maintain a schema . It is a great help when restoring the database structure on a new workplace. But it can also have benefits when it comes to analyzing legacy applications. With schema.rb it is easy to see common smells with typical Rails apps: Routes file is another great source of knowledge when it comes to legacy Rails apps. If you treat a controller action as a C main method equivalent, taking a quick glimpse on routes can show you how complex in terms of starting points . This is an important measure. Apart from investigating a number of routes, you should take attention to following things: As you can see there is a lot that can be read without even touching the logic of a legacy app. Examining those three files helped us in a lot of situations - we got first points to discuss with clients and an overview what we can expect from an app itself. Applying such analysis to your app can bring you many ideas and knowledge about what can be done to improve maintainability of it. I greatly encourage you to try it out - it can save you a lot of work later. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-08-17"},
{"website": "Arkency", "title": "How can Rails developers benefit from reading the Arkency books?", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/09/how-can-rails-developers-benefit-from-reading-the-arkency-books/", "abstract": "As a Ruby/Rails developer, you have so many books to learn from. The choice is sometimes too big which can lead to choice paralysis. In this blogpost I‚Äôd like to focus on the different aspects of your developer life. There are so many skills to improve. The Rails skills are the obvious ones. But there‚Äôs much more than that. Working in a team, safe refactoring, well-structured frontend applications, communicating with other people, even blogging. I‚Äôd like to explain how we‚Äôre helping with improving those skills with our Arkency books. Big chances are that you‚Äôre working on Rails applications. Some of them are probably the ones, which are not so pretty. You‚Äôd like to improve them (Scout Rule anyone?) but it‚Äôs not clear how to do it. Also, it‚Äôs a risk - what if you break anything?\nAnother typical problem is the tests taking ages to run (and often fail for random reasons). This is exactly why we came up with the book called: ‚ÄúFearless Refactoring: Rails Controllers‚Äù. Thanks to this book, you‚Äôll be able to improve your Rails codebase with more confidence. You will become fluent with new abstractions, like service objects, repositories, adapters.\nYour tests will run faster, saving you and your team a lot of time (and money!). Thanks to this book, you will know where all kinds of Rails code belong to. You will get more conventions to use, even in bigger Rails apps. The book was released as 1.0 in December 2014. The techniques which are included are not related to any specific Rails version - they can be applied to all Rails apps. They will work with Rails 2, Rails 3, Rails 4 and Rails 5 versions just fine. If you ever heart about Domain Driven Design (DDD), then this book is a great introduction to DDD in the context of Rails applications. It presents almost all of the DDD building blocks. It prepares you to think in more domain terms. Buy Fearless Refactoring: Rails Controllers As Rails developers, we were happy writing just Ruby code, for many years. Nowadays, just Ruby is not enough. We need to build rich user interfaces, which are possible only in JavaScript. A big part of the Ruby community went with CoffeeScript, while some stayed with JavaScript. The community was also split between many different frameworks. Some of us went with Angular, some went with jQuery, some went with Ember.\nReact.js is the new approach to building JavaScript components. Backed by Facebook, it‚Äôs trending now, for good reasons. After many (sometimes unsuccessful) journeys with other frameworks, we felt in love with React.js. As Arkency, we were probably one of the first companies to adopt React.js fully (thanks Marcin!). Very quickly, all the Arkency developers noticed the clean structure which React provides and it was adopted in almost all of our projects. We learnt a lot about how to combine React.js with a typical Rails application. As a result, we shared the experiences in the ‚ÄúRails meets React.js‚Äù book. The book will teach you: Animations with React.js is such a great topic that we decided to record and include 2 videos on this topic. Buy Rails meets React.js We love working with React.js so much, that we also published another book on this topic - ‚ÄúReact.js by Example‚Äù. This book doesn‚Äôt focus on the Rails integration. We wanted to share with you how to setup a separated (from Rails) React.js-based frontend. This book is directed to people who are total React.js newbies. There are 12 typical examples implemented in React.js. Each of the examples is another chapter. Each example contains a narration on how one of us (from Arkency) approaches the development of a React.js component. What‚Äôs more - you receive all the repositories for this book! You can browse the code, you can run the examples, you can tweak them. After reading this book, you will be fully ready to approach a typical React.js component. The examples are written in EcmaScript2015 and use webpack. Buy React.js by Example Those books are very different. They teach you React.js in different ways. While the ‚ÄúRails meets React.js‚Äù book is more Rails focused, it focuses on developing one component from a small one to a very complex widget. The ‚ÄúReact.js by Example‚Äù is focused on the starting point. It shows 12 examples, which are typical things you will start with . Many of our readers bought both of the books and are happy with that. If you need to choose just one - if you want to use React.js within an existing Rails ecosystem, then go for ‚ÄúRails meets React.js‚Äù. If you are allowed to develop the frontend separately to Rails, then choose ‚ÄúReact.js by Example‚Äù. Many of us want to be happy in their Rails projects. We usually want less meetings, less bureaucracy, less stress. We want more freedom and we want to be working on features that are important. Many of us want to work asynchronously - we know the times of the day when we are most effective.\nMany of us want to work remotely - not to waste time in commuting, have the freedom of traveling and have more time to spend with our families. Many of us are scared to talk directly to the customers. That‚Äôs why we wrote the ‚ÄúDevelopers Oriented Project Management‚Äù book. We want to share what we learnt and how we‚Äôre able to work remotely and asynchronously. This book explains all the above topics. We teach what are the requirements for the team to work remotely and asynchronously. We teach how to communicate with the customers in ways which are win-win. We teach how to deliver value to the project even in situation when there‚Äôs not enough time to finish everything. We also explain the tools, which are helpful in async collaboration. Buy Developers Oriented Project Management If you follow our Arkency blog, you may have noticed that we‚Äôre quite active in blogging. Several people from our team share knowledge using this medium. It wasn‚Äôt always like that. Before 2012 we didn‚Äôt even have a company blog and almost no people blogged frequently. How did we change that? How can you start blogging? Over time, I developed a blogging therapy. I used it internally to encourage people to blog more. I have identified what are the biggest blocking points for developers to stop them from blogging - no time, perfectionism, the impostor syndrome, not having anything interesting to write about. I took those blocking points and addressed them in a way that helped many developers unblock themselves and share more of their knowledge. That‚Äôs what the ‚ÄúBlogging for busy programmers‚Äù is about. Thanks to blogging you‚Äôll learn more (learning by teaching is the best way!), you will build your (or your company) brand. What we found out was that blogging is a great tool for recruitment, for attracting new clients. The book is still in the beta phase (35 pages), but it already addresses all the above blocking points. Buying it now gives you all future updates for free. Buy Blogging for busy programmers We strongly believe in the value of all our books. We have over 1200 unique, happy customers of our books. We have a 100% refund policy - if you feel that the book didn‚Äôt deliver you the value you expected, send us an email to dev@arkency.com and we issue a refund without asking any question. If you‚Äôre interested in learning something specific from us - let us know! We plan to write more and such feedback will help us decide which topics are most interesting to you. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-09-03"},
{"website": "Arkency", "title": "Null Object pattern in practice", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/08/null-pattern-in-practice-ruby/", "abstract": "Wikipedia describes Null Object as an object with defined neutral behavior . Martin Fowler says Instead of returning null, or some odd value,\nreturn a Special Case that has the same interface as what the caller expects. But null object pattern can also be seen as\na way to simplify some parts of your code by reducing if -statements\nand introducing interface that is identical in both situations,\nwhen something is missing and when something is present. Also it gives you a chance to properly name the situation\nin which you want to do nothing ( is it always nothing? ).\nBy just adding Null prefix instead of using a more precise word,\nyou are missing an opportunity. There is a great Sandi Metz talk ‚ÄúNothing is something‚Äù when she talks\nabout it. I have a view that is displaying an event data, as well as\na collection of tickets available. It‚Äôs\nquite complicated view. It is reused in one different place . When the\nevent organizer is editing event and tickets properties we\ndisplay preview of the same page . So naturally we use the same\nview template that is used for rendering an event page for a preview page. There are slight differences however so one additional variable\n( is_preview ) is passed down through number of partials that\nthe page is consisted of . We have a class called EventPool which is responsible\nfor gathering and keeping data to quickly answer the one question - whether\na given ticket type is available to buy or not. In other words, it checks\nits inventory status. Naturally when organizers are in the process\nof adding tickets and haven‚Äôt saved them yet, they are still\ninterested in seeing preview of how they would be displayed on\nevent page. There is no point of checking inventory status of\nunexisting tickets (remember they are not saved yet). Also even\nfor existing tickets we want to pretend they are not sold out\nwhen displaying the preview . The code responsible for the situation (greatly oversimplified)\nlooked like this: But for the entire tree of partials to work correctly\nwe still need to pass down event_pool which needs to have #pool_for_ticket_id method, even if at the end we decide not to check\nthe availability status of returned ticket_pool . For me it looked like a great case for applying Null Object Pattern. If all the places which care whether we are in a real mode\nor preview mode adopted this approach, bunch of if-statements\ncould be removed in favor of using properly named classes\nwith identical interfaces. Some would be used only when a real\nevent is displayed, some only when the preview is shown. The code\nusing dedicated Null-* classes (in our case the view) wouldn‚Äôt care and wouldn‚Äôt know\nif this is preview or not. The logic for preview behavior would\nbe localized in those classes. We could also completely eliminate the is_preview variable in the end. In this case\nwe only eliminated it for the related part of code. Did you like the blogpost? Join our newsletter to receive more goodies. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-08-16"},
{"website": "Arkency", "title": "Testing race conditions in your Rails app", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/09/testing-race-conditions/", "abstract": "From time to time, there comes a requirement in your application\nwhere you have to guarantee that something can be done at most X\nnumber of times. For example, only 15 people can subscribe to a\ncourse, only limited number of people can buy this physical or\nvirtual product , only 200 people can go to a concert, etc. How\ndo you test that this limitation actually work? And not only work\nbut that it works under heavy load , when there are multiple customers\ntrying to buy the last item because it is a hot event or the\nproduct is offered with a big discount ? You might think that testing it is hard, and maybe not worthy? But I found\nout that sometimes it is not so hard at all , and it might be simpler than\nyou imagine. Sometimes, all you need is 4 threads to reproduce and fix\nthe problem . Let‚Äôs go step by step through this code. By default Active Record connection pool can keep up to 5 DB connections.\nThis can be changed in database.yml . This just checks if the\npreconditions for the test are what I imagined. That no developer, and no\nCI server has these values different. One DB connection is used by the main thread (the one running the test itself).\nThis leaves us with 4 threads that can use the 4 remaining DB connections\nto simulate customers buying in our shop. We instantiate a new TestMerchant actor which creates a new product with a limited\nquantity available. There are only 3 items in the inventory so when 4 customers\ntry to buy at the same time, it should fail for one of them. Actors are\nimplemented with plain Ruby classes. They just call the same Service Objects that our Controllers do. This code is specific to your application. We create 4 customers in the main thread. Depending on what being a customer\nmeans in your system, and how many Service Objects it involves, you might\nwant to do this in your main thread, instead of in the per-customer threads.\nBecause, we strive to achieve the highest contention possible around buying\nthe product. If you create your customer in the per-customer threads it might\nmean that one customer is already buying while another is still registering. In one of the threads, we want to catch an exception that placing an Order is\nimpossible because it is no longer in stock and remember that it occurred. We need to define the\nvariable outside of the Thread.new do end block, otherwise it would not be\naccessible in the main scope . I couldn‚Äôt find a way in Ruby to create a thread with a block, without starting it\nimmediately. So I am using this boolean flag instead . All the threads are\nexecuting a nop-loop while waiting for this variable to be switched to false, which\nhappens after initializing all threads. We create 4 threads in which 4 customers try to buy the product which can\nbe purchased max 3 times . One of the customers should fail in which case\nwe will set fail_occured to true. The first thread is created faster than the rest\nof them and remember that we want high contention. So we use true while wait_for_it to wait until all threads are created. Then main thread sets wait_for_it to false .\nThat starts the buying process for those customers. We don‚Äôt know how long it will take for the threads to finish so we gladly wait\nfor all of them . Then we can check all our expectations. One failure, three\nsuccesses, and product not over-sold . These, of course, are very specific\nto your domain as well. When building such test case it is very important to start with red phase when you\ngo through red, green, refactor cycle. Without the race condition preventing method\nthat you choose to implement the test should fail. If it doesn‚Äôt fail it means the\ncontention you created is not big enough . One more thing. This test has self.use_transactional_fixtures = false which means it will\nnot run in a transaction (otherwise other DB connections from other threads would not see\nthe records created by setup in the main testing thread). Your test is responsible for\ncleaning the database after running itself. I use database_cleaner with deletion strategy\nfor that. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-09-04"},
{"website": "Arkency", "title": "CQRS example in the Rails app", "author": ["Tomasz Rybczy≈Ñski"], "link": "https://blog.arkency.com/2015/09/cqrs-example-in-the-rails-app/", "abstract": "Recently I have worked in a new project for some time. We implement an application where one functionality is to show an organization‚Äôs (tree-like) structure.\nThe structure contains information about relations between employees. I have been thinking how to build this feature.\nI was wondering if I should go typical way implementing all CRUD actions. In this case I would have to generate the structure for each call.\nI thought It will be faster. It is a startup. So until the project is young performance is not a problem.\nAlthough after few syncs we decided to go an another way. I am fascinated by the DDD and CQRS approaches. In Arkency we are used to saying that every time we return to the ‚ÄúImplementing DDD‚Äù book we realize that it has answers to all the questions and doubts.\nSo going back to the feature we decided to implement the structure as a Read Model. I love examples so I will not leave you hanging. Our app is split into two parts. We have a frontend implemented in React and a Rails backend.\nI will focus only on the backend part. If you are interested in how we deal with React you can read some of ours books .\nIn next steps I will show you how we implemented simple CQRS. I will focus on building a Read Model using events. Starting from the top. The following example shows the controller with basic actions. As you can see we simply call Application services where each one has a separate responsibility.\nIn clean CQRS we should use Commands . We will refactor it in a next step. The following example shows one app service. We use this service to create new a Team. I have chosen a simple service to focus on most important parts. As you can see we call a domain service to create Team model and save it into the Database.\nTeam is an aggregate root in relation Team <-> Members. After creating a Team we publish event to the Event Store. We use our own Event Store called RailsEventStore .\nYou can check out the github repository . Publishing event should be placed in aggregate but It was a first step to put in an app service.\nWe don‚Äôt use the Event Sourcing. We decided to save a ‚Äûcurrent‚Äù state for now. Event Sourcing is a completely orthogonal concept to CQRS. Doing CQRS does not require event sourcing.\nBut we save all events so It will be very ease to build an aggregate‚Äôs state using events. We inject the EventStore instance using a custom injector. The whole setup you can see bellow. So the Write part is almost done. In the SetupEventStore class we define event handler called OrganizationBc::ReadModels::Structure for our Read Model.\nWe subscribe it to handle set of events. The organization‚Äôs structure is a tree structure. Each Team has relation to a parent member and collection of child notes.\nThese nodes are team‚Äôs members. We modify the structure‚Äôs model and save in Postgres Database for each handled event. We save model in JSON representation.\nWe save a new record in each update to keep whole change history. The following example shows how the repository looks like. When we have build Read Model the last step is to create query for fetching model. We have separate module called AppQueries where we keep all queries.\nSo the Read part is only one class. That‚Äôs all. I can only say that was great decision to start this way. Now I now that we save a lot of time on investigation performance problems in the future. Off course\nthe most important thing is to choose if CQRS is a good starting point. If you have simple CRUD feature it will be unnecessary. I didn‚Äôt focus on test part. I think It is a great subject for a separate post. If you are interested in testing event sourced app you can check this post . All code used in this post you can find here . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-09-18"},
{"website": "Arkency", "title": "Testing Event Sourced application - the read side", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/2015/09/testing-event-sourced-application-the-read-side/", "abstract": "In my last post, I‚Äôve presented a way how to test an Event Sourced application. But again (yes, again and again) some part was missing there. It is clearly visible when you look at test coverage: The missing part, of course, is the read side. This application uses read model that are build by a set of event denormalisers based on an events published by write side (an Aggregate to be specific). Current flow is as follows: The read model (a.k.a projection because it is a projection of domain events) should be build based on the set of domain events.\nThe subset of domain events needed to handle depends on the needs, depends on which domain events have the impact on a projection state.\nThe source of domain events could be a single aggregate‚Äôs stream, a class of streams (i.e. all streams of Order‚Äôs aggregates) or all domain events cherry-picked by a projection. How to build an initial state when you don‚Äôt have a state?\nThis should be quite easy. Any state is a derivative of domain events. You could build any state by applying domain events. To build a state you just need some events: There is always a problem how initial state for a test should be build. With the use of event handlers it should be easy to build it - all you need is to define a set of domain events and pass them through the event handler. Each event handler is a function: f(state, event) -> state In our case, the acting part of the test will be sending a domain event to an event handler and by knowing the initial state and payload of the domain event we could define our expected state. There could be various types of event handlers. There is no one way of asserting the output. In this case, where event handlers (denormalisers) produce relational denormalised model the thing we check is if the model is build as expected. No errors here - what has happened it has happened - you could not change the past. If you could not handle event you should fail? ‚Ä¶ or ‚Ä¶ Some might ask: But what if we fail to execute our event handler? No exceptions? Then what?\nThe answer is: more domain events ;)\nThe domain event is just a message. If you use queues, you might know how to deal with messages that could not be processed. There are several patterns: retry it, skip it, ‚Ä¶ and finally if you really could not do anything you will send that message to dead letter queue.\nSimilar actions could be applied here, retry later if the previous message has not been processed yet, skip it if it has been already processed or publish a compensation message if your domain model should take some actions. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-09-22"},
{"website": "Arkency", "title": "Sanitizing html input: youtube iframes, css inline styles and customization", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/09/sanitizing-html-input-youtube-iframes/", "abstract": "Sometimes we give users tremendous power over the content generated\non the web platforms that we write. The power to add content using\nHTML/WYSIWYG editors . There is only one gotcha. We need to make sure\nthat this power is not abused by malicious users . After all,\nyou are a responsible developer , right? One of the libraries that you can use for that is the Sanitize ruby gem . Most importantly to avoid XSS attacks . You are probably already aware of them and know of the danger. And to avoid CSS Injection attacks. They can be used to lead the customer to an unsafe page outside of your website without\nthe customer being aware of that . Imagine that your shopping platform allows people to buy\nproducts. When people click a ‚Äúbuy‚Äù button somewhere outside a product description they might\nthink they are still on your page. But malicious attacker can use CSS and HTML to place identically looking button at the same location on the page that your original button is\nplaced on. User can click such button and be redirected to their own domain with layout\nidentical to your shopping platform. They might think they are logging in or buying on your platform but\ninstead they are providing their login and password or credit card credentials on the attacker page . But sometimes you want to include or exclude some parts of the HTML conditionally. For example\nyou might not want the user to be able to include all <iframe> -s but you might want them\nto be able to include youtube videos of cats or ted.com talks. Be very careful when defining the regexp for the URL and make sure to write some tests.\nIf you forget to escape one character (for example a dot) the attacker can embed\nan iframe from similarly looking domain. And be careful to write tests for the attributes : For allowing certain styles you might want to use HTML::WhiteListSanitizer that comes\nfrom your Rails 4.1 or Rails::Html::WhiteListSanitizer from rails-html-sanitizer gem in later versions (which under the hood uses loofah gem ). Allowing and sanitizing inline styles might be required for your editor to work properly. Make sure to test it as well. I usually test that all allowed attributes/styles\nare left unchanged and some of the disallowed (after all the list is infinite‚Ä¶)\nare removed: Even though the list of HTML tags and attributes allowed by Sanitizer is quite long, you\nmight still want to customize it a bit depending on your needs and the\nway the editor of your choice works. The examples are using sanitize in version 2.1 . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-09-20"},
{"website": "Arkency", "title": "Do you have a framework for dealing with emergencies?", "author": ["Bartosz Krajka"], "link": "https://blog.arkency.com/2015/09/do-you-have-a-framework-for-dealing-with-emergencies/", "abstract": "In my pre-Arkency life, I worked for a corp. Keep in mind that it was my very beginning of the commercial-IT world. I sometimes felt lost inside this huge DevOps machine. Lost and alone - as usually, when something breaks, there was nobody to ask for help. It had to happen. One day, by accident, I ran some kind of a DB-clean-up script, with the wrong date, on production . The script‚Äôs job was to delete records that are old enough . While fiddling with the script, I changed the definition of records old enough to records visible today (not closed) . And accidentally ran the script. My reaction - a complete paralysis. I wasn‚Äôt even sure what the script exactly did. Unfortunately, I had never found time before to analyse the script line-by-line. Also, I have never seen unit tests, so most likely they didn‚Äôt exist. So I had no idea how much damage I caused. Maybe the script had a protection from so accidental usage as mine? It should have, right? I took a look at the frontend to see if everything is ok - it wasn‚Äôt. Literally no data for today. It was present a minute ago but disappeared. Cool. They will fire me. One of the managers visited me quickly and brought to book: Where are my data? I need it NOW for my work. When will you restore it? I already realized that I had to call an ex-coworker of mine (he was currently on vacation, but I had no choice‚Ä¶) to get all needed information. So my answer was: I don‚Äôt know. In the worst case scenario, when he is back, so next week [4 days]. The end of the story was pretty lucky, though. I immediately got an accurate instruction from my coworker-on-leave - there was a daily backup of the data, so probably all we need is a casual restore from /a_directory . A half-hour of preparation, with an extra pair of eyes and triple-checking if this time everything is ok - and voil√†! The records are back! Only a few of them were lost forever (those inserted after the backup), but come on! You can insert them again! We saved the day! Today, I‚Äôm not very proud of this situation. There were two good reactions, though: What should I have done better? Of course, easiest to say: You should simply don‚Äôt make such stupid things , but I‚Äôm afarid it‚Äôs impossible. We, in Arkency, are agreed - everyone makes mistakes. Smaller, larger, more or less foolish ones - but everyone does them. The professionalism doesn‚Äôt mean no mistakes , but rather as little mistakes as possible, asymptotically to zero; zero is unreachable . Speaking of the art of professionalism - the true value of the developer is not how many mistakes they do, but how they recover. What is a shame today? It was my duty to tell the managers what happened. They shouldn‚Äôt have needed to visit me and inquire about the accident. I should have told them immediately: An apology is nice here (it was clearly my fault), but useless without all above. If necessary, I should keep them informed about the progress. Luckily, the recovery was so quick that the next message was, at the same time, the last one: Hey, it‚Äôs ok now, we only lost today‚Äôs records . But staying quiet is never a good option. I don‚Äôt know. In the worst case scenario, when he is back, so next week [4 days]. I didn‚Äôt lie. The problem is - such an information doesn‚Äôt tell very much. At the end of the day, the incident was solved after less than an hour! Why were the numbers so different? Much better approach is to estimate with 3 values: So I should have told: All I did was telling only the third value. This was not the end of the world, if treated as an obligation (what the management often does). But the other two values were much less frightening, what should have been spoken. Surprisingly, this time the most optimistic estimation was true. This happens very rarely , I consider this as a luck. Surely, next time my coworker won‚Äôt break his sunbathe. That would be a dream. You have a list of things, step-by-step, what to do (and what not to do) in hard times. The list is very specific, but at the time it gives you some level of freedom in designated areas. Handling emergencies is an individual matter. In depends very much on you as a programmer and as a developer, your habits, your team and your project. There are some hard rules here, like be verbose or give 3 values in estimations . However, it‚Äôs not exhaustive. There‚Äôs always a whole lot of unknowns. You should have your own framework for such cases. Otherwise you are exposed to cases like this one above. We described ours in Responsible Rails book. Tired of weekend works? Missing a framework for handling emergencies? Looking for advice how to mitigate the number of production accidents? Try Responsible Rails! The book contains: Sometimes you can observe that you change the habits during emergency - maybe it‚Äôs time to reconsider your beliefs? If you usually TDD, but drop it in emergency times - you don‚Äôt really trust TDD. If you need a partner to fix an issue - well, maybe pair programming is a good idea also in the quiet times? If you turn the music off to gain the maximum mana - why don‚Äôt you do it all the time? If an acident shows that you can delay all the needless things (casual talks, facebook checkings, etc.) - take a look at Pomodoro Technique . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-09-30"},
{"website": "Arkency", "title": "Advantages of working on a legacy application", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/10/advantages-of-working-on-a-legacy-rails-application/", "abstract": "Are there any advantages of working on such an application?\nWould you rather prefer working on a greenfield?\nHere is my stance on this topic and my personal story. When I finished my Master‚Äôs thesis about Ruby on Rails, I had one dream.\nTo work with Ruby on Rails on daily basis . I have already quit my first\njob in a corporation a few months earlier to focus on studies. I was free\nto start any job, anywhere. I couldn‚Äôt find Rails job at my current city,\nat that times Rails was not really that much popular in Poland. But I\nfound a new job in Warsaw and moved there. My first Rails job. How\nexciting. On my first day I sit down, pulled the repository, installed dependencies and\nrun the tests. Almost everything failing. I looked at the code and it was\nterrible. So much business logic in controllers and views. I wanted to cry .\nMy contract obligated me to work there for 12 months and I wanted to run\naway on day one. My dreams were crashed . I read so many blogposts, articles and books about\nRails, about testing in Rails, doing TDD. I wanted to work in Rails because\nI wanted to work with codebase covered by tests . Just like Ruby is, just like\nRails is. But the reality was radically different. Big codebase (>100 models\nand controllers). Complicated business domain. And useless, failing, obsolete tests which didn‚Äôt give you much confidence in refactoring . You might think that after such experience my answer to the question, I asked\nat the beginning would be very negative. But on the contrary. I‚Äôve learnt\na lot since that time. Legacy codebase teaches you humility. We, the programmers are writing this code.\nWe, our colleages, our friends. Not some mythical creatures from Wonderland. We,\nourselves. Doing our best, one commit at a time. And when we look later at our\ncodebase we are often not so happy about the final result. And we need to live\nwith it . Understand our limitations. Keep reading, keep getting inspired, keep\ngetting better. 2 years later I was working in a different place and a developer called me,\ntrying to convince me to move to a new job. It was a marketing agency using Ruby\non Rails for websites for their customers. One of his argument when he was trying\nto convince me was that their projects have an average lifespan of 3 months .\nSo if you make a bad decision regarding architecture or gems or you wrote a bad\ncode, you don‚Äôt have to live very long with it. Soon, you will have a new chance\nto start a fresh project. A new greenfield. I rejected that offer . Legacy\napplications that you keep working on for years and maintain them give you\nthe ability to understand the long term effects of your decision. You have to\nlive with the consequences . You see which decisions were good and which were bad.\nWhat helps you one year later and what you regret. Short them project, new\nprojects usually don‚Äôt give you that kind of feedback. From a few years old, big, inherited Rails app you will learn a lot. What it means to\ncouple every part of the application with every other part of it without any kind of\nseparation. On top of that, coupling with every gem that hook directly into Active\nRecord. After week (months, or even a year - depending on the state of code you\ninherited) that you spent fighting with small and medium bugs happening everywhere (because\nof the coupling) you will probably have a much better understanding of the domain of\nthe business . I am working in a project where I can easily name 20 different modules that\nthis application should consist of. Now, whenever a new feature is introduced I am careful\nto think how it affects all the already existing features. In other words, you learn how\nthe decision you make and others made before you play in the long term . As my client once said to me: The only reward that we get for being successful (as a business) is more complexity .\nThe bigger the organization, the more processes, features and use cases, the more complexity\nneeds to be handled by the code. That is our curse (because we try to fight the complexity\neverywhere and there is always accidental complexity added as well) and our reward . Because the\nfeeling that you can handle all that complexity for your client and keep things\nworking and profitable is nice. Grown businesses and applications give you a tremendous opportunity to recognize bounded\ncontexts in the app. What modules it is built from. Even if you don‚Äôt see it on the code\nlevel, you start to see it on the organizational level . On the conceptual level.\nThis starts to give you clarity in terms of which way the application should be\nrefactored into . It‚Äôs true that there can be a lot of bad emotions, negativity, frustration, even rage when you work with legacy codebase. It‚Äôs interesting that codebase (non living thing,\nnot even a material object) can have such an impact on our feelings. But you\ncan treat it as a lesson. How to convert those emotions into something positive?\nBut to do that you need to have refactoring and communication skills . And desire\nto keep improving .\nIt‚Äôs hard, it‚Äôs tiresome and exhausting . But what other options do we have? To\ngive up? It‚Äôs not that you are alone with those problems and challenges. Small companies\nwrite shitty code and big companies write spaghetti code as well.\nIn the world where almost everything, from factories, power plants, planes and cars are controlled by software this problem is only going to get bigger. And we need to\ntake responsibility for our actions . Working on a legacy application makes it easier to operate on value when\nnegotiating with business. Recently I‚Äôve been implementing a cross-selling option\nfor one of our clients. From the frontend perspective it was one checkbox here\nand a couple of text inputs later. Backend required more work, much more changes,\nAPI integration, etc etc. 10 minutes after enabling it with a feature toggle the first customer\nalready purchased this option. We knew it is working, valuable for people and they\nwill keep using. One day later we more-or-less knew the conversion rate. Multiply\nit by their number of daily transactions, take average value of order into consideration\nand you have a pretty good guess as to how much money this brings to the table in the next\nyear . Next time when you negotiate your salary or your rate you can use those numbers .\nYes, we cost you $X but the features we do for you give you $Z. When you work on new projects you usually don‚Äôt have such a comfort. There is not much\ntraffic, not so many customers and the startup might be still burning money. In such\ncase it is much harder to use the value that you bring to the customer as a starting\npoint for a salary negotiation. Bigger, older, legacy business are already verified in some way. Either they are at least\nvery good at convincing investors or they are onto something and they are doing something\ngood for the customers. Otherwise, why would they keep coming? Also, we found that it is much easier to handle billing with such mature organizations. They don‚Äôt whine about every dollar they need to pay you. They just pay. In fact, they\noften want even more of our developers later. They can afford us . And a final thing that I love most about it. Super fast customer feedback loop.\nThe cross-selling feature for the platform that I told you about, it took 2 weeks and dozens of\npull requests and deploys. It was used by an user within 10 minutes. Then I was working on\nanother feature. I was sceptical that people will buy it, but the business product owner wanted\nto go for it. We did our best, shipped it, enabled on production. Same story. 10 days later\nand we already knew that it was a success. The fact that you can just ship something and get an\nimmediate feedback is a tremendous benefit . I wish you good luck in your legacy project and lot of learning opportunities . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-10-20"},
{"website": "Arkency", "title": "Run it in a background job, after a commit, from a service object", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/10/run-it-in-background-job-after-commit/", "abstract": "There is this problem that when you schedule a background job from inside of a running\ndatabase transaction, the background job can kick in before the transaction is committed. Then the job can‚Äôt find the data that it expects to already be there, in database .\nHow do we solve this issue? Let‚Äôs schedule a job after db commit. The easiest way to do it, would be to move all the code responsible for scheduling\nafter the transaction is finished. But in big codebase, you might not have the ability\nto do it easily. And it is not that trivial with nested dependencies. You might know that your ActiveRecord class have after_commit callback that can be\ntriggered when the transaction is commited. However, I didn‚Äôt want to couple enqueuing\nwith an existing ActiveRecord class. I think that integrations with such 3rd party systems as for example background queues are more the responsibility\nof Service Objects rather than ActiveRecord models.\nAnd I didn‚Äôt want to introduce a new AR class just for the sake\nof using after_commit callback. I wanted the callback without ActiveRecord class . Here is how it can be achieved and how I figured it out. Let‚Äôs see after_commit implementation in Rails. Well, this doesn‚Äôt tell me much on what and how is calling this callback. So I looked into set_callback and there I found in a documentation that such callbacks should be executed with run_callbacks :commit do . The next step was to investigate what part of ActiveRecord calls :commit hook. A simple grep told me the truth.\nOnly one place in code calling it Ok, so what calls the method commited! ? It is used in ActiveRecord::ConnectionAdapters::OpenTransaction : It is called on every record from records collection. But how are they added there? So I turns out, all we need to do, is add an object which quacks like an ActiveRecord one, to the collection\nof records tracked by currently open transaction (if there is one). Here is a class which mimics the small API necessary for things to work correctly: And here is a piece of code which checks if we are in the middle of an open transaction.\nIf so, we add our AsyncRecord to the collection of tracked records . When the transaction\nis commited, the new job will be queued in Resque. One more thing is important. You might be running some (all?) of your tests inside a database transaction\nthat is rolledback at the end of each test. I excluded such tests from this behavior: This is dependent on your testing infrastructure so it might differ in your project. If enjoyed this article and would like to keep getting free Rails tips in the future, subscribe\nto our mailing list below: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-10-24"},
{"website": "Arkency", "title": "Rolling back complex apps", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/10/rolling-back-complex-apps/", "abstract": "One of the customers of Responsible Rails ebook has\nasked us about the topic of Rolling back complex apps so I decided to share a\nfew small protips on this topic. If you are afraid that your code might break something and you will have to\ngo back to the previous version, then it is best to make your data structures\ncompatible with the previous version of the code. You need to ask yourself what will happen if you: This is relevant to database structure, state columns, method arity and many aspects\nof the system. Sometimes avoiding pitfalls is easy, sometimes it is harder. Most of the time the answer how to do things more safely is to divide them into more steps .\nLet‚Äôs see some specifics. Say you want to add a not null column. Lovely. I hate nulls in db. But if you add not null column without a default in revision B, and then you must quickly go back\nto revision A, then you are in trouble. Old code has no knowledge of the new column.\nIt doesn‚Äôt know what to enter there. You can, of course, reverse the migration but that means\nadditional work in stressful circumstances. And chances are, you are doing it so rarely that you won‚Äôt be able to just run the proper command without hesitation . Hosting providers\nusually don‚Äôt come with good UI for rarely executed tasks as well. So nothing is on your side. Solution? Break it into more steps: Wait enough time to make sure you won‚Äôt roll back this revision. If you do roll back, then\nno problem. Old code can still insert new records and they will have null in the newly added\ncolumn. You don‚Äôt have to revert the migration. The new column can be kept. Once you fix your\ncode and deploy it again, it will start using the new column. Of course, you will have to fill\nthe value of the column for the records created when revision A was deployed for the second time.\nBut that‚Äôs manageable. add not null constraint Adding a database default is another way to circumvent the problem. But that‚Äôs only posible if the\nvalue is simple and don‚Äôt vary per record. You don‚Äôt always have that comfort. Say you have a background job that expects one argument in revision A. And you want to add one more argument locale in your revision B. To keep the code in revision B comptabile with jobs scheduled in revision A you\nneed to use a default: Because when you deploy new version of background worker code, old jobs might still\nbe unprocessed. Ok, but what happens if you roll back to revision A? Jobs scheduled in revision B (and unprocessed) will fail on revision A. Too many arguments. Solution? Break into more steps: First, just add a default but don‚Äôt change the method code and the code enqueuing.\nJust the signature. This should be safe. And then in a second step change the method so it depends on the additional argument\nand the code that enqueues to pass that argument: If you need to revert back, already enqueued jobs will continue to work on\nprevious code revision. Some hosting providers offer the ability to roll back. However, we‚Äôve seen\nit been badly implemented. Maybe because it is not used too often. The way\nI approach it, is to deploy again revision number A. I look into the history\nof deploys, check out previously deployed revision number and keept it nearby. Deployment is done so often that we know the procedure to be reliable and\nverified. The only difference is that I am deploying older revision instead\nof newer one . Teoretically rolling back should be the same as deploying previous version\nof the code. But for some providers it is not. So I mention it here\nexplicitly. Have you ever deployed a new feature that was living on a separate branch after\ntwo or three weeks of developing? Despite all the tests I never feel\ncomfortable doing that. So instead I try to deploy once per day or two . If you are adding new features\nyou can usually continue to add classes and methods and deploy them safely.\nOften this path in code won‚Äôt be reachable unless you display it on the UI.\nAnd you can make it only available in development environment, staging or for admins. Which\nbrings us to our next technique. Usually when we land new big features for our biggest client they are protected\nwith feature toggles .\nOften those toggles are not for entire system but rather they work per tenant or\nper country. That means when the time comes and the feature is ready you can\nenable it in the biggest (if you feel brave) or the smallest (if not so brave)\nmarket . Or just in the market that is the targeted recipient of given feature.\nThe bigger the project the more often you need to adjust it\nfor local regulations, customers‚Äô habits and API providers. When we add new payment gateway integration we usually try it first on certain\nproducts, then on certain merchant accounts and then in certain countries. Gradually\nexposing it to more and more customers . Here is an example of such configuration. Settings for products take\nprecedence over settings for merchants which take precedence over\nsettings for countries. Feature toggles make the easiest rollbacks . Something is not right after enabling a feature?\nNo problem, just disable it, investigate, fix and re-enable. You can read more about programmer friendly workflow environment in our\nDevelopers Oriented Project Management ebook .\nWe describe there for example how to work on master branch without Pull Requests\nand quote Google Chrome team which works the same way. Of course these are just tools. No need to use them all the time. Apply when in\nneed . When you need to feel more safe and comfortable. There are core features\nof the platform that must just work, for example, the checkout process in a shop.\nAnd there are a lot of secondary features which are not as critical. Going safer way means deploying smaller chunks, deploying more often\nand hiding features which are not ready yet . So it is obvious that the cost\nof shipping new features\nis a little higher because of overhead. But if you already have Continuous Deployment\nthen it is not much bigger. It‚Äôs mostly your habits that need to change. Easy rollbacks is a similar problem to zero-downtime-deploy but a bit more complicated.\nInstead of having zero downtime going from A ‚Üí B , you need to also\nhave zero downtime going (eventually) from B ‚Üí A , which usually requires B to just be\nsmaller in size. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-10-24"},
{"website": "Arkency", "title": "Creating custom Heroku buildpack for Webpack and Ruby on Rails integration", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2015/10/creating-custom-heroku-buildpack-for-webpack-and-ruby-on-rails-integration/", "abstract": "Heroku and Rails loves each other from long time - and this combo is still widely used both by beginners and experts to build and host their web applications. It‚Äôs easy and it‚Äôs fast to host an app - and those two factors are very important in early stages of the project‚Äôs lifecycle. In modern web applications backends and frontends are often equally sophisticated - and unfortunately solutions that Sprockets offers by default are suboptimal choice. Using ECMAScript 2015 features, modern modularization tools and keeping track of dependencies are hard to achieve in typical Rails asset pipeline . That‚Äôs why modern JavaScript tooling is used more and more often to deliver those features. In Arkency we use Webpack and Babel.js to manage and compile code written in modern dialects of JavaScript . Apart from configuration and Rails integration problems there is also a problem of deployment and configuring the deploy machinery to wire everything together. In this article I‚Äôd like to show you how you can deploy Rails + Webpack combo to Heroku . This is the thing that is expected from us by our clients from time to time. Of course to deploy Rails together with Heroku we need to have both tools configured and working. In this article the goal is to make Webpack compile the bundle that can be used by Rails to serve this bundle . In the described configuration Webpack only provides modularization and ES2015 compilation using babel-loader . It doesn‚Äôt minify files - the assumption is that Rails‚Äô asset pipeline can do this just fine during precompilation phase. The assumption is that the stack is configured as follows: So from the Rails point of view there is only another JavaScript file to include - and this JavaScript file is a bundle emitted by Webpack. The rest (serving pages and serving the compiled JavaScript) is done on the Rails side. That configuration implies that the whole process of bundle compilation can be done in a total separation from Rails - and that will be important later. To understand how to deploy such configuration, you need to understand how Heroku deployment process works - and fortunately it is relatively simple. Heroku is using so-called slug compiler to create a version of an app that is suitable for being served in the Heroku dyno infrastructure. This tool is using so-called buildpacks to perform necessary steps to deploy your application. There are many buildpacks for many technologies - like Ruby buildpack for deploying Ruby (and Rails) apps or Node.js buildpack to deploy and serve Node.js-based apps. By default Heroku is guessing which buildpack to use by doing heuristics built into the buildpacks. In their essence buildpacks are simple bash scripts that are splitted into three parts: You can see it by yourself - these three bash scripts are always included in buildpacks under bin/ directory. In the case of described configuration there are two applications in Heroku terms - one is the Ruby app and it is seamlessly prepared for being run on Heroku (no steps needed) and the other is a Node.js app residing under app/assets directory. It would be ideal to use those two buildpacks - Ruby one and Node.js one and call it a day. Unfortnately, it‚Äôs not that simple. Node.js buildpack has almost everything that you need - it installs all necessary dependencies, caches them, downloads Node.js and so on. But unfortunately there are things that it can‚Äôt do out-of-the-box: To address those three issues you‚Äôd need to fork and modify heroku Node.js buildpack. First of all, you need to have Node.js buildpack forked or cloned and published in a repo. It is important because while configuring buildpacks you‚Äôd need to provide a Git repository from which buildpack is going to be fetched. After having source files, it is needed to modify four files: Let‚Äôs start with bin/detect script. It looks like this: Since package.json resides under app/assets of the build directory, you need to change this test: To: After this change everything will work as planned. The next step would be to change build directory in bin/compile . You need to look after: And change it to: This way the build directory of your builpack will change to X/app/assets where X is the argument passed to this script. BTW. You can test this script by calling bin/compile <your-app-path> /tmp to see what happens and troubleshoot if any problems arise with using this buildpack. The same with rest of the scripts inside bin/ . At the top of the file there is a comment describing the usage of these scripts. So far, so good. Build directory is set correctly, now you need to run your npm script. After: You need to have: So the compilation step is reconfigured correctly. Unfortunately it still doesn‚Äôt work. It is because devDependencies are not installed. By default they are not installed if NODE_ENV variable is set to production . It is a default in this buildpack and it needs to be changed. That‚Äôs why lib/environment.sh needs to be modified. You‚Äôre interested in the create_default_env() procedure: It needs to be changed to: You can also omit this step and set corresponding environment variables using Heroku Toolbelt . Since this Node.js buildpack is used for development tasks I find it sane to change defaults - it is not default way to use Node.js buildpack after all. The last part is about disabling any ‚Äòreleasing‚Äô behaviour in this buildpack. Modify bin/release to be: That‚Äôs it. Commit and push your changes - the buildpack is ready to use! Since you want to use multiple buildpacks you need to explicitly tell heroku which buildpacks will be used. By default Heroku guesses it and sets only one buildpack. Here two are used. To configure buildpacks used by an app you need to use Heroku Toolbelt . There is handy command called heroku buildpacks:add that will be used here. First of all, you need to add Ruby buildpack. heroku buildpacks:add accepts URL of the repository as an argument. So the command you need to issue in your app directory is: Then, you need to have your repository URL. You need to add it as the first buildpack that is executed. There is a --index 1 option which does exactly this - setting the buildpack as first to be executed. So the next command you need to issue is: Your repo must be in HTTP format, not the SSH one (so not git@github.com:heroku/heroku-buildpack-ruby.git for example, but https://github.com/heroku/heroku-buildpack-ruby ). It is also need to be accessible by Heroku. If you make a mistake during typing those values there is a heroku buildpacks:remove command that accepts URL or index to be removed. This is a lot of knowledge here. So to make a quick recap let‚Äôs enumerate how Heroku will behave now after you issue git push : So your app should be deployed with all compiled Webpack scripts, just as planned. As you can see, integration of Rails and Webpack on Heroku can still be done in a relatively easy way. Unfortunately it is not as straightforward as the typical Rails-only process, but it‚Äôs still manageable. I think being able to work with modern JavaScript tooling is worth the effort. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-10-27"},
{"website": "Arkency", "title": "Slack-driven blogposts", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/10/slack-driven-blogposts/", "abstract": "Slack (or any other team communication tool) may be a good source for your blogposts. You may have explained something to your team on the Slack channel. You already verbalised your thoughts in a written manner. That‚Äôs the biggest part of writing a blog post! Use it to your benefit now. Turn it into a blogpost. As developers we often share this feeling that what we just wrote or explained is not really that useful to anybody. Here are some techniques which may help you in turning the Slack conversations into a blog post: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-10-28"},
{"website": "Arkency", "title": "Stable Circle CI builds with PhantomJS for larger Rails-backed frontend apps", "author": ["Marcin Doma≈Ñski"], "link": "https://blog.arkency.com/2015/12/stable-circle-ci-builds-with-phantomjs-for-rails-backed-larger-frontend-apps/", "abstract": "The original photo is available on stocksnap . Author: Stephen Radford. One of the projects we work on is a rather large frontend app, built with React and backed by Rails. The app has quite a few tests and obviously we want them to run as fast as possible. We have tried a few drivers along the way but eventually we have chosen PhantomJS. So far we are pretty much happy about the choice, but it wasn‚Äôt always like that. Especially when it comes to our CI server where the tests would quite often fail randomly and prevent the app from being deployed. The random failures have been the biggest pain so far and so here are a few tricks that have helped us keep the build green. Our app is a typical frontend application, which means there are AJAX requests sent all over the place. Even the simplest edit and save operation sends one and then shows a flash message when it‚Äôs done. Now to have a test that checks if the proper flash message is visible, we need to wait for AJAX, it‚Äôs not enough to simply do: even though ‚ÄúCapybara is ridiculously good at waiting for content‚Äù . In our case this fails from time to time and so we have resorted to a custom wait_for_ajax helper method that checks if there are any AJAX requests still running: Then in our tests we call it after clicking a Save button: In our case, this one seems to be the main cause of our random failures. Switching it off has brought the build back to its green color and random failures are a very rare thing now. The downside is that the tests take much longer to run but it‚Äôs pretty much guaranteed that the app will be built and deployed right away without the need of rebuilding the whole thing again and again. In our worst cases, we had to do it quite a few times and already started to hate the rebuild option, knowing that it might not help and that we still have a problem somewhere else. Initially, we were using PhantomJS 1.9.8, but it didn‚Äôt have the bind method needed to support React (we had to add it ourselves). It also had some other issues, like clicking other elements than buttons or inputs. Eventually, we decided to upgrade to version 2.0 where most of the issues were eliminated. So far it has been the most stable version. Oh, and it‚Äôs slightly faster, too! Now, to actually use PhantomJS 2.0 on CircleCI, you need to have this in your circleci.yml: Here is a trick that may also make your build more stable. We have noticed that WEBrick, which is the default server, hangs from time to time and gives us weird timeouts during the test runs. So we searched for alternatives and ended up using Puma instead. It seems to be much more stable and here is how you can plug it in: Our frontend uses different animations, like fading out and in. This all looks nice but obviously also makes some functions slower, and as it turns out, causes some tests to fail randomly. For tests, however, the animations are totally unnecessary, so why not turn them off? Here is how we do it. First, we add a custom CSS class to our <body> tag, for the test environment only: Then we use the following styles: It‚Äôs one of the things that won‚Äôt hurt but may help eliminate the random test failures. Those few tricks have helped us eliminate most the random failures and are saving us long minutes, if not hours, of rebuiling the app over and over again. We hope they can also work for you. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-12-14"},
{"website": "Arkency", "title": "In-Memory Fake Adapters", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2015/12/in-memory-fake-adapters/", "abstract": "There are two common techniques for specifying in a test the behavior of a 3rd party system: I would like to present you a third option ‚Äî In-Memory Fake Adapters and show an example of one. I find In-Memory Fake Adapters to be well suited into telling a full story. You can use them to describe actions\nthat might only be available on a 3rd party system via UI. But such actions often configure the system that\nwe cooperate with to be in a certain state. State that we depend on. State that we would like to be present in\na test case ‚Äî showing how our System Under Test interacts with the 3rd party external system. Let‚Äôs take as an example an integration with seats.io that I am working with recently. They provide us with\nmany features: So as a service provider they do a lot for us that we don‚Äôt need to do ourselves. On the other hand, a lot of those things are UI/Networking related. And it does not affect the core business logic\nwhich is pretty simple: In other words: Don‚Äôt oversell . That‚Äôs their job. To help us not oversell. Which is pretty important. To have that feature working we need to communicate with them via API and they need to do their job. Let‚Äôs see a simple exemplary test. When seats.io returns with HTTP 400, the adapter raises SeatsIo::Error . The tested service knows that\nthe customer can‚Äôt book those seats. It‚Äôs OK code for a single class test. But I don‚Äôt find this approach useful when\nwriting more story-driven acceptance tests. Because this test does not say a story why the booking could not\nbe finished. Is that because seats.io was configured via UI so that Sector 1 has only 2 places? Was it because\nit has 20 standing places, but more than 17 were already sold so there is not enough left for 3 people? Now, this tells a bigger story. We know what was configured in seats.io using their GUI. When season\npasses are imported by the organizer, they took all the standing places in Sector 1 . If a customer\ntries to buy a ticket there, it won‚Äôt be possible, because there is no more space available. When using In-Memory Fake Adapters you don‚Äôt need to stub every call to the adapter (on method or HTTP level)\nseparately. This is especially useful if the Unit that you tests is bigger than one class .\nAnd when it communicates with the adapter in multiple places. To properly test a scenario that invokes multiple API\ncalls it might be easier for you to plug in a fake adapter. Let the tests interact with it. Here is an example of a fake adapter for our seats.io integration. There are 3 categories of methods: Seats.io has a lot of useful features for us. Despite it, the in-memory implementation of their\ncore booking logic is pretty simple. For seats we mark them as booked: @seats[seat] = :booked . For general admission\nareas we lower their capacity: @places[place_name] -= quantity . That‚Äôs it. In-memory adapters are often used as a step of building a walking skeleton .\nWhere your system does not integrate yet with a real 3rd party dependency. It integrates with something that pretends\nto be the dependency. Use the same test scenarios. Stub HTTP API responses (based on what you observed while playing with the API)\nfor the sake of real adapter. The fake one doesn‚Äôt care. An oversimplified example below. You know how to stub the HTTP queries because you played the sequence\nand watched the results. So hopefully, you are stubbing with the truth. What if the external service changes their API in a breaking way? Well,\nthat‚Äôs more of a case for monitoring than testing in my opinion. The effect is that you can stub the responses only on real adapter tests.\nIn all other places rely on the fact that\nfake client has the same behavior. Interact with it directly in services\nor acceptance tests. The more your API calls and business logic depend on previous API calls and the state of the external system.\nSo we don‚Äôt want to just check that we called a 3rd party API. But that a whole sequence of calls made sense\ntogether and led to the desired state and result in both systems. There are many cases in which implementing Fake adapter would not be valuable\nand beneficial in your project. Stubbing/Mocking (on whatever level) might be\nthe right way to go. But this is a useful technique to remember when your needs\nare different and you can benefit from it. If your state and business logic don‚Äôt depend at all on those API calls then you\ncan go with Dummy. What‚Äôs Dummy? You can find out more about different kinds of\nthese objects by watching Episode 23 of Clean Code by Uncle Bob . This blog-post was inspired by Fearless Refactoring . A book in which we\nshow you useful techniques for handling larger Rails codebases. If you liked reading this you can subscribe to our newsletter below and keep getting more\nuseful tips. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-12-04"},
{"website": "Arkency", "title": "A single Rails API endpoint to accept all changes to the app state", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2015/12/a-single-rails-api-endpoint-to-accept-all-changes-to-the-app-state/", "abstract": "This idea is heavily influenced by CQRS and its way of applying changes to the app via commands objects. In this blogpost we‚Äôre showing how it could work with Rails. Commands are data structures which represent the intention of the user. In the Rails community, we sometimes use the name of ‚Äúa form object‚Äù to represent the same meaning. In some of our projects, we started moving to a command-driven approach. A command is handled by a command handler (often it‚Äôs a service object). As a result of handling the command we publish domain events. When you switch to commands, you‚Äôll notice that many controllers look alike and they‚Äôre becoming a boiler-plate code which you repeat over and over. This is what led to a conversation between me and Pawe≈Ç . We discussed whether it makes sense to have just one controller, being represented by just one API endpoint. Pawe≈Ç decided to experiment with this idea and wrote the code below. This code is also a nice example of how concise can be a one-file-Rails application. I really like this concept. I think it has the potential of removing a lot of controller code. If this can work in some cases, this idea would become the most radical one in my book on dealing with Rails controllers . In the current version of the book, we talk a lot about the concept of form objects as a data structure which is initialized in the controller and passed to the service object. The approach with a generic controller handling commands removes a big part of the controller layer. Knowing Pawe≈Ç, there will be updates and improvements to this approach, so stay tuned :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2015-12-06"},
{"website": "Arkency", "title": "From legacy to DDD: Start with publishing events", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/01/from-legacy-to-ddd-start-with-publishing-events/", "abstract": "When you start your journey with DDD, it‚Äôs quite easy to apply DDD in a fresh app. It‚Äôs a bit more complicated for existing, legacy apps.\nThis blog post shows how you can start applying DDD by publishing events. In an existing app, the biggest worry is to not break the existing functionality. This makes applying DDD even harder, as full DDD will require some refactorings. I suggest to start with publishing events . Just publishing, no handling, no subscriptions. By just publishing events, you don‚Äôt change the main behaviour of your system. What you‚Äôre doing is adding a new no-op (no operation). An optional step is to also store the events . I have an easy tool for both those things at once, so I publish/store at the same time. Publishing events is like a compilable/interpretable code comment. You register a fact. This is what happened at this state of code. I recently work on an app called Fuckups. Its role is to allow teams to track fuckups in their projects and it allows learning from those situations. I started with a typical framework approach (The Rails Way) and only after some time, I gradually escape from the framework and start applying DDD/CQRS/ES. It‚Äôs best to focus on the events which are clearly statements of some state changes. If you escape from a CRUD app (as I did) - they will be all those CRUD operations. What I did, was I also tried not to use the CRUD verbs. Instead of FuckupCreated I called it FuckupReported. That‚Äôs more true, as I‚Äôm not really creating a fuckup by filling the form. It‚Äôs more that I report that fuckup to the system. This is what I ended up with, in terms of events: Using the Rails Event Store gem, this is how I publish those events: It‚Äôs still a bit too verbose as for my taste, but it‚Äôs quite explicit what it‚Äôs doing. Publishing events (and storing them) is just the first step. On its own it doesn‚Äôt really change your architecture that much. So what‚Äôs the value? The value is in the fact that you need to come up with non-CRUD names, that‚Äôs first. You start using more domain vocabulary in your code.\nThe main value, though, is that those events are quickly showing you potential next steps. The events tend to group in two ways .\nThey show you the aggregates . If you look at the event prefixes, it‚Äôs quite clear that User and Fuckup are aggregates.\nThe second grouping is by a bounded context . In my case, it‚Äôs quite clear that I have a Identity&Access bounded context (authentication, authorization, sharing, access). The other one is just the Core - Fuckups. You may notice that the aggregates split when you think in aggregates. The Fuckup can be shared. This is an Identity&Access concern, not the core Fuckups bounded context. In a way, the fuckup exists in both bounded contexts. This kind of thinking and analysing is very useful in the later phases. Stay tuned for the next steps! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-01-20"},
{"website": "Arkency", "title": "Loading all data when starting Flux SPA", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/2015/10/loading-all-data-when-starting-flux-spa/", "abstract": "Recently we‚Äôve been working on an application which is a typical SPA which uses the Flux approach. After some time we had a problem that besides our frontend being SPA, each time we clicked on link leading to some ‚Äúpage‚Äù, we‚Äôre loading data again, even if this data was loaded before . We‚Äôve decided that the simplest solution would be to load all data in the beginning and display an animation when it‚Äôs loading. In the future when necessity to hit refresh each time we want fresh data would become troublesome we could simply add Pusher-like solution to update data in real-time. In this post I want to present you solution how our implementation looked like. In our application we are using ES6 (using Babel) and alt.js as a library delivering Flux features. We can assume that this is the application for managing blog, so we have only two resources: posts and comments. Firstly we will define a store, which will keep information which data we‚Äôve already loaded. Code above is a sketch of our alt.js Store. Here we only say, that when an action PostsActions.allMyPostsFetchedSuccessfully will be triggered, our Store has to call postsLoaded method. This action will be triggered already when response with posts‚Äô data arrive from the server. Moreover a reset method is defined. It‚Äôs called in init callback, so just after our Store will initialize. We want only to set default state here. loadedData Map will keep information which resources are already loaded. Our flow will look like this: Here‚Äôs the code where we set corresponding keys after receiving responses from the server: Now the last one, the checkIfDataLoaded() function. As I‚Äôve said before when all data are loaded we want to trigger something - in this case it will be an action which will in result hide loading animation . Note our action is called finishedLoading - we‚Äôll define it in a while. Here we‚Äôve implementation of our actions. As you can see declaring actions in alt.js is pretty easy, it‚Äôs just ES6 class. Actions are methods but only methods containing this.dispatch() will be dispatched. Now you can ask where to call InitialStateActions.startLoading() action?\nI‚Äôll not cover it with code here, as it belongs more to the topic about authentication. In our application, there are two cases which trigger this action: Other missing piece is a loading spinner - thanks to Flux we can also pretty much decouple it from the initial state loader. In our case, we‚Äôve a LoaderStore which solely purpose is managing loading spinner. It‚Äôs so short and simple I can even include it whole below: Now, it‚Äôs pretty much it. As I said above, only things you need to customize is when you call the InitialStateActions.startLoading() action in your application and how you display the loading spinner. As you can see, this solution is pretty generic. It doesn‚Äôt interfere with other stores, which makes our application easier to reason about. It follows flux-way of doing things, introducing InitialStateStore allowed us to remove all fetchFoo methods scattered around our React components. This in the end lead to the simpler design of the overall app. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-01-22"},
{"website": "Arkency", "title": "One event to rule them all", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/2016/01/one-event-to-rule-them-all/", "abstract": "Today I was asked a question: How to ensure that domain event published by one of the aggregates is available in another aggregate? The story is simple. The user, who is a member of some organization, is registering a new fuckup (maybe by using Slack command).\nThe fuckup is reported by just entering its title. It is reported in the context of the organization where the user belongs.\nThe rest is not important here but what we want to achieve is: modify a state of an organization aggregate and,\nat the same time, create new fuckup aggregate. This is quite simple to implement using Rails Event Store gem. First let‚Äôs start with the definition of a command that is executed when fuckup is registered and a domain event that is published\nwhen fuckup is reported. Now we need a handler for our command. It should load an Organization aggregate from event store, execute domain logic responsible for reporting\nnew fuckup and store all published domain events in the event store. Our command handler needs an Organization aggregate. It should have all logic needed by the organization, does not matter now what it could be.\nOne thing to notice is that Organization does not create new Fuckup aggregate. Instead it ‚Äújust‚Äù publishes a FuckupReported domain event. So, how is Fuckup created? The answer is: by handling a domain event. The event handler should create new Fuckup aggregate (because we don‚Äôt have any to load it from event store) and just store it. With that implementation, our action responsible for reporting a fuckup should only execute our command handler.\nBoth aggregates have the domain events stored in its own stream, however as you may notice by comparing event_id\nthis is still the same domain event. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-01-26"},
{"website": "Arkency", "title": "Drop this before validation and just use a setter method", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/01/drop-this-before-validation-and-use-method/", "abstract": "In many projects you can see code such as: However there is different way to write this requirement. ‚Ä¶or‚Ä¶ ‚Ä¶or‚Ä¶ ‚Ä¶depending on the way you keep the data inside the class. Various gems use various ways. Here is why I like it that way: I especially like to impose such cleaning rules on objects used for crossing boundaries such as Command or Form objects . That‚Äôs it. That‚Äôs the entire, small lesson. If you want more, subscribe to our mailing list below or buy Fearless Refactoring . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-01-29"},
{"website": "Arkency", "title": "Where and why I'm still using Rails", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/02/where-and-why-im-still-using-rails/", "abstract": "I‚Äôve had many interesting conversations with experienced Ruby/Rails programmers recently. One of the topics that often pops up is when it makes sense (for us) to still use Rails, given the variety of other choices. This blogpost aims to summarize some of the views here. Some of the conversations started when we talked about Ruby conferences and when I was explaining the mission behind the wroc_love.rb conference. The mission is to serve the experienced Ruby programmers. According to Uncle Bob estimates , we double the number of programmers in the world every 5 years. It means that at any point of time, we have half of the programmers having less than 5 years of experience. That‚Äôs a lot of people. Many conferences, rightfully, aim to educate those new people. They do it either by making the whole conference newbie-friendly or they try to find the balance mixing the ‚Äúeasy‚Äù and ‚Äúhard‚Äù talks. It‚Äôs also a visible trend at Ruby conferences to present more and more non-Ruby talks. Some of them are the typical ‚Äúsoft‚Äù talks, while others are focusing on some other technologies (Elixir, React.js, Clojure, Rust, Go). With wroc_love.rb we try to stay with experienced-only content. Even when we introduce some non-Ruby topics (this year it‚Äôs React/Redux and the R language) we try to make it ‚Äúdeep‚Äù so that it‚Äôs intellectually interesting to experienced programmers. It‚Äôs not easy to ‚Äúsurprise‚Äù a Ruby dev, if just look at the language. But language is well, just a language. It‚Äôs the syntax. We‚Äôre with one of the most elegant and happiness-oriented languages in the world. I still find excitement while doing Ruby.\nSome examples: What mutant does while testing your test coverage is pretty amazing. Parsing the codebase, mutating the abstract syntax tree, unparsing and then running the tests again, how cool is that? BTW, the mutant tool has a nice ecosystem of tooling around it. In one of the larger projects we were working, we decided to get rid of FactoryGirl (mostly because the tests using it were creating a state that was not the right one ). My friend used the parser to find most of the calls to FactoryGirl and replaced (again, using Abstract Syntax Tree) them with the other ways of preparing the test state (test actors). Have you looked at what‚Äôs possible with Opal.rb - the Ruby to JavaScript transpiler? Many people use it in production to write Ruby code for the browser. There‚Äôs a whole framework ( Volt ) behind it, which allows you to reuse backend/frontend code. BTW, Elia , one of the Opal.rb creators will be presenting at wroc_love.rb this year. I‚Äôm deep into DDD/CQRS/ES recently (Domain-Driven Design, Command Query Responsibility Segregation, Event Sourcing). Those techniques are not ruby-related. But, they changed the way I write Ruby/Rails code. I‚Äôm giving this example, as it‚Äôs good to know that we can bring ideas from other programming worlds and bring them to Ruby. I know there are some cool things around the dry-* /ROM ecosystem. This is something still on my TODO list to discover, so I keep being excited about some new things. I recently blogged about the idea of having a single Rails API endpoint to accept all changes to the app state . I didn‚Äôt have time to go further with this crazy idea. Look at what we‚Äôre (Arkency) doing around the RailsEventStore project . Many of those things do appear at Ruby Weekly or /r/ruby or /r/rails . However they get lost among other more newbie-oriented content. If you don‚Äôt find new exciting things it‚Äôs because it‚Äôs now hidden in smaller Ruby communities. Those ideas and discussions take place on their related Gitter channels, Slack communities or Github-issues related to the relevant languages. I didn‚Äôt even mention what‚Äôs happening with JRuby and that‚Äôs like a whole world of innovations which are possible because we can use the whole JVM platform! Did I mention the Hanami project (old name: Lotus)? Did you look at apotonick‚Äôs Trailblazer? Have you read his book? It‚Äôs now more popular to criticize Rails than it was before. Once you get past the things that are possible with Rails you see the problems with some of its patterns - the active record pattern being the main one. Suddenly, we realize that Rails teaches new developers bad habits. We‚Äôre worried that Rails makes other developers stupid . We try to show that there‚Äôs a world beyond The Rails Way . This is the clue of the blogpost. Once you know the Rails limitations you try to find alternatives. You go with Hanami or you go with smaller tools like roda . You may go with Sinatra . Or you go with rich Single Page Applications and you find yourself writing more JS code than Ruby. Here‚Äôs my approach, based on my skills and on my experience: I still use Rails for the first version of the application. Me (and other people who were doing Rails for years) have the Rails-skills in our blood. Our muscle memory is based on Rails tricks. Most of the things you need for a typical web app is already there in Rails. You can build a full app within hours/days/weeks. This is what Rails is optimized for. I‚Äôm yet to find an alternative to Rails which is so productivity-oriented. There‚Äôs the whole issue with the rails-dependent gems. There are some which are more easy to remove/replace when needed - like Devise . While, there are some crazy gems which introduce huge coupling and are harder to replace (in my experience). This is the border for me. If I‚Äôm tempted to introduce some heavily-coupled gems then it‚Äôs time to slow down and do it the non-rails way. Yes, the Rails patterns make the code difficult to maintain in the long run. But in the shorter perspective they‚Äôre just hard to replicate in other environments. I can‚Äôt be faster with other tooling. There‚Äôs obviously the skills bias here - I‚Äôm fast with Rails, so I‚Äôm going to stick with what I know. But that‚Äôs the point of this post - to show you that if you‚Äôre fast with Rails then you can enjoy staying with Rails for longer. My approach of starting with Rails is based on the understanding that code is not set in stone . You can change it later. You can decouple your app from the framework step by step and I wrote a whole book about it . You can gradually separate the frontend and go into React.js (recommended!) and/or Redux. We wrote a book how to use React.js with Rails , but we also wrote a book full of examples (and +10 repos!) how to start with React.js . We have a huge list of Arkency React.js resources . I see the React/Redux movement as my frontend future. Heck, you can even move to DDD from a typical Rails codebase, if that‚Äôs your kind of thing (it is for me). OK, so that was the first reason when to use Rails - when you want to start quickly and you know how to gradually improve the code later on. If you want to go with Roda or Sinatra, but later you actually follow the active record pattern of just using the same object from the db layer to the view, then I don‚Äôt understand how this is different from just using Rails. I‚Äôd go with Rails. If I‚Äôm about to start a Rails app, the time-to-market is not a major factor, then I‚Äôd consider things like roda or sinatra. But in that case, I‚Äôd go with architecture like DDD, where I take care of the object design on my own. Rails is actually very good for the http layer - I don‚Äôt see the need to replace it with other libs.\nActiveRecord as a persistence library is also good enough. As long as the AR object don‚Äôt leak to your domain, then it‚Äôs cool. It is overcomplicated, but if you just use in limited ways, creating your own private API for it - then you have the persistence problem solved. I see a bright future for Rails. The Rails 5 release is controversial to many, mainly due to ActionCable . I‚Äôm not criticizing it. Without going into details of the ActionCable infrastructure (there are parts worth some critique), the whole idea is making Rails even more attractive for the typical backend+frontend setups. You will be able to do cooler things faster. Rails is a cool marketing product for many programmers. Yes, we live in times, where frameworks are products and require marketing. We have a charismatic and smart leader - DHH . I often disagree with him on some details, but he‚Äôs probably the best salesman in the whole programming community. That‚Äôs one of the reasons why Rails will thrive. Rails is a product where out of the box you have everything and everything is just working. It‚Äôs like the Apple products. Happy Rails coding! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-09"},
{"website": "Arkency", "title": "From legacy to DDD: What are those events anyway?", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/02/from-legacy-to-ddd-what-are-those-events-anyway/", "abstract": "In one of my previous posts , I‚Äôve suggested to start with publishing events. It sounds easy in theory, but in practice it‚Äôs not always clear what is an event.\nThe problem is even bigger, as the term event is used in different places with different meaning. In here, I‚Äôm focusing on explaining events and commands, with their DDD-related meaning. Events are facts . They happened. There‚Äôs no arguing about it. That‚Äôs why we name them in past tense: If those are only facts, then what is the thing which is the request to make the fact happen? Enter commands. Commands are the objects which represent the intention of the outside world (usually users). A command is like a request: It‚Äôs like someone saying ‚ÄúPlease do it‚Äù to our system. Usually handling commands in the system, causes some new events to be published. Commands are the input . Events are the output . Both commands and events are almost like only data structures. They contain some ‚Äúparams‚Äù. It‚Äôs important to note, they‚Äôre not responsible for ‚Äúhandling‚Äù any action. For now, just remember: commands are requests events are facts Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-09"},
{"website": "Arkency", "title": "How RSpec helped me with resolving random spec failures", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/2016/02/how-rspec-helped-me-with-resolving-random-spec-failures/", "abstract": "Photo available thanks to the courtesy of Robert Kash . CC BY 2.0 Recently we started experiencing random spec failures in one of our customer‚Äôs project. When the test was run in an isolation, everything was fine. The problem appeared only when some of the specs were run before the failing spec. We use CI with four workers in the affected environment. The all of our specs are divided into the four groups which are run with the same seed. In the past, we searched for the cause of such problem doing manual bisection. It was time-consuming and a bit frustrating for us. You probably already know RSpec‚Äôs --seed and --order flags. They are really helpful when trying surface flickering examples like the one mentioned in the previous paragraphs.\nRSpec 3.4 comes with a nifty flag which is able to do that on behalf of a programmer. It‚Äôs called --bisect . According to the docs , RSpec will repeatedly run subsets of your suite in order to isolate the minimal set of examples that reproduce the same failures. I simply copied the rspec command from the CI output with all the specs run on given worker with the --seed option and just added --bisect at the end. What happened next? See the snippet below: It took almost 20 minutes to find the spec which interfered with other ones. Usually, I had to spend 1-2 hours to find the issue. During this 20 minutes run of an automated task, I was simply working on a feature. The --bisect flag is a pure gold. It was simply before(:all) {} used to set up the test. You shouldn‚Äôt use that unless you really know what you‚Äôre doing. You can read more about the differences between before(:each) and before(:all) in this 3.years.old , but still valid blog post . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-12"},
{"website": "Arkency", "title": "Rails: MVP vs prototype", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/02/rails-mvp-vs-prototype/", "abstract": "In the last blog post I explained my approach to using Rails  to quickly start a new app. In short - I‚Äôm starting with Rails but I also learnt how to gradually escape from the framework and separate the frontend. Janko Marohnic commented on that and I thought it would be nice to reply here, as the reply covers the distinction between an MVP and a prototype. Here are his words: The problem that I have with always starting to Rails is that it‚Äôs later difficult to change to another framework or design. That‚Äôs why I think if you want to later start with frontend and backend separate, you should do so from the beginning. If we practice that like we‚Äôve practiced with Rails, we would become equally familiar . I see some people have arguments that you can quickly prototype with Rails. I think in a frontend framework like React.js it‚Äôs much easier to prototype, since you don‚Äôt need to write any backend. You just have to be familiar with it, of course . If you want the ActiveRecord pattern in a non-Rails framework, I think that‚Äôs a great opportunity to switch to Sequel, since it‚Äôs better than ActiveRecord in every possible way. So there is no need to switch to Rails here, but see for yourself how non-Rails libraries can be so much better than Rails . I don‚Äôt find things in Rails to be just working. Sprockets so often didn‚Äôt work properly in the past. Spring started working properly only like 1 year after it was released. ActiveRecord is still missing some crucial features, like LEFT JOINs, PostgreSQL-specific support (so that you can actually use your database instead of Ruby) and a low-level interface where you don‚Äôt have to instantiate AR objects that is actually usable (ARel is terrible). Turbolinks also didn‚Äôt work properly, was getting authenticity token errors (without any AJAX). I definitely didn‚Äôt find it just working . Janko touched some important topics here. Let me reply here. Janko‚Äôs words are made bold. The problem that I have with always starting to Rails is that it‚Äôs later difficult to change to another framework or design . I don‚Äôt think it‚Äôs always true. This is actually where most of my focus goes - how to gradually change your app so that it doesn‚Äôt rely on Rails . It‚Äôs not easy, but in many projects we proved it‚Äôs possible and worth the effort. The sooner you start the separation, the easier it goes. This doesn‚Äôt mean going there from the beginning. That‚Äôs why I think if you want to later start with frontend and backend separate, you should do so from the beginning. If we practice that like we‚Äôve practiced with Rails, we would become equally familiar . This is a problematic advice. If you‚Äôre so skilled to be able to build a nicely separated application from the scratch - then yes, that‚Äôs the way to go. What I‚Äôm seeing, though, is that even experienced React developers (as in 2 years of React experience) who happen to also have Rails skills, are not equally fast with the React frontend vs Rails-based frontend. So, when time to market is important, I think going with Rails (with the intention of refactoring it later) is faster overall. I do agree with the notion that if we‚Äôre practicing working with frontends/backends separately then we‚Äôll come to the position where it‚Äôs easier to separate from the beginning. It does take time, though. This is also the same with DDD. I think it‚Äôs easier (time-to-market-wise) to start with The Rails Way than starting with DDD. However, once you‚Äôre so good with DDD that you can make it faster (I‚Äôm not there yet), they don‚Äôt rely on Rails. It‚Äôs all based on the time-to-market metric here. If you have the luxury of doing ‚Äúthe right thing‚Äù from the beginning and shipping quickly is not the main prio, then let‚Äôs go with the right thing. I‚Äôm involved in such DDD projects and they have the maintainable architecture/design/code from the beginning. I see some people have arguments that you can quickly prototype with Rails. I think in a frontend framework like React.js it‚Äôs much easier to prototype, since you don‚Äôt need to write any backend. You just have to be familiar with it, of course . It‚Äôs this part of the comment that made me think the most here. In many places I‚Äôm advocating the idea of going frontend-first . This technique allows focus on the frontend (as the more important) part first. We can get it right as the first task and then we know how to build the backend because we know what data we need. I‚Äôve worked on many such projects and it worked very well. There‚Äôs one important distinction here. It‚Äôs the prototype vs MVP distinction. My definition of a prototype is of something that I can click, feel, experience. However, it‚Äôs usually not production-ready. If you start with the frontend, you don‚Äôt have an easy way to make it production ready, if there‚Äôs no backend. What Rails allows us to do is MVPs - the Minimum Viable Products. It‚Äôs more than a prototype. It‚Äôs a prototype + the fact that it‚Äôs production ready. Rails gives all the basic security rules - CSRF, SQL Injection protection which makes building the whole thing faster and actually release it. Both approaches are worth considering - if you feel that your project benefits more from just the prototype and your frontend/JS skills are good enough to make you deliver it quickly - then perfect. Do it. Then build the backend. Enjoy the separation. If it‚Äôs important to ship to the actual market as quickly as possible (I‚Äôm thinking days/weeks here, not months), then I believe Rails can make it happen faster. BTW, it‚Äôs a similar discussion to whether to go microservices first or not. If you want the ActiveRecord pattern in a non-Rails framework, I think that‚Äôs a great opportunity to switch to Sequel, since it‚Äôs better than ActiveRecord in every possible way. So there is no need to switch to Rails here, but see for yourself how non-Rails libraries can be so much better than Rails . It never happened to me to want to have the active record pattern in a non-Rails framework. If I  want to go with active record, then Rails makes it perfect for me with the ActiveRecord library. \nIt‚Äôs not that I‚Äôm against Sequel. We used it in our projects and it felt to me like just a slightly different API as compared to ActiveRecord. It‚Äôs definitely lighter. I think the distinction here is whether I want to go with The Rails Way or not. To me, The Rails Way means using the active record object in all layers of the application. If we want to do it, then AR makes more sense to me. If we seperate our persistence nicely, then Sequel may be a good alternative. However, it‚Äôs definitely possible to hide the ActiveRecord behind a repository layer and have the same gains, but with AR. I don‚Äôt find things in Rails to be just working. Sprockets so often didn‚Äôt work properly in the past. Spring started working properly only like 1 year after it was released. ActiveRecord is still missing some crucial features, like LEFT JOINs, PostgreSQL-specific support (so that you can actually use your database instead of Ruby) and a low-level interface where you don‚Äôt have to instantiate AR objects that is actually usable (ARel is terrible). Turbolinks also didn‚Äôt work properly, was getting authenticity token errors (without any AJAX). I definitely didn‚Äôt find it just working . This is a perfect summary of what is the danger of using some Rails features. Very well put. I did generalize and simplify in my last email, that Rails just works. This was a simplification. Rails just works, unless you start using the new and shiny things too quickly. I‚Äôm vey conservative in my approach, when it comes to new features. I‚Äôm excited about the ActionCable addition, but I‚Äôm not going to use it very soon (I love Pusher for that). Sprockets - they are a pain, especially in bigger projects. In smaller projects they don‚Äôt hurt as much. If you start with them, but then switch to more modern JS approaches like Webpack, you shouldn‚Äôt be affected by the Sprockets problems. Spring - I don‚Äôt use it all. ActiveRecord is missing some crucial features, but you can always go down and just use your own SQL, in those places. If your data layer is separated it shouldn‚Äôt hurt as much. I‚Äôm not advocating for using SQL everywhere, it‚Äôs just in those missing places. Turbolinks - I use it only when actually forget to disable it in a new app - and thanks for the reminder, in my current project I forgot to disable it. So, what is worth remembering here? The notion of the time-to-market metric is important. If time-to-market is crucial, Rails may be fastest. It‚Äôs worth to know the distinction between a prototype and an MVP.\nA prototype is something you can click on, while MVP is a prototype that is production-ready and can be exposed to the real world. PS. Janko, thanks for your valuable comment! PS2. If you‚Äôd like to improve your React/JavaScript skills, then our free React.js koans are a perfect place to start! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-12"},
{"website": "Arkency", "title": "How to teach React.js properly? A quick preview of wroc_love.rb workshop agenda", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2016/02/how-to-teach-react-dot-js-properly-a-quick-preview-of-wroc-love-dot-rb-workshop-agenda/", "abstract": "Hey there! My name is Marcin and I‚Äôm a co-author of two Arkency books you propably already know - Rails meets React.js and React.js by Example . In this short blogpost I‚Äôd like to invite you to learn React.js with me - and this is quite a journey! Long story short, Arkency fell in love in React.js. And when we fell in love in something, we‚Äôre always sharing this love with others. We made a lot resources about the topic. All because we want to teach you how to write React.js applications. Workshop is a natural step further to achieve this goal. But how to teach React.js in a better way? How to take an advantage a stationary workshop gives to learn you in an optimal way? What can you expect from React.js workshops from Arkency? There are many reasons. First of all, React.js helps you write even the most complicated dynamic interfaces. Facebook uses it, your clients will demand it soon (if not demanding it today). React.js makes easy things easy and hard things achievable. The programming model of React.js scales very well with the growth of your application. From small projects to big ones it is always applicable. The second thing is that React.js can be introduced gradually in your codebase. This is extremely important when you work on an existing code. You can take a tiny piece of your interface and transform it into the React.js component. It works well with frameworks. And speaking of frameworks - it is often way harder to introduce a JavaScript framework in such workflow-friendly way. Ryan Florence has a great talk about why React.js is well suited for legacy codebases . For us it is also very important - inside the team we‚Äôre working with legacy codebases all the time. It is inherent to the work of a consulting agency. React.js is also a great gateway drug to an interesting world of modern JavaScript. You may hate JS - but it is one of the most developing communities nowadays. The new standard of JavaScript polishes a lot flaws the old JS had. Node.js tools can be great drop-in replacements even in your Rails apps. It is really worth giving it a try - and in my opinion there is no better way to enter this world than learning React.js. Last, but not least - the learning curve of React.js is very smooth. You need to learn only a few concepts to start working. It only makes one job - managing your views. This is the biggest advantage and the biggest flaw the same time - especially for Rails people, who get accustomed to benefits the framework provides. But as always there are things which are harder than other. Let me talk a bit about those ‚Äúhairy‚Äù parts. Basically, there are three things that are needed to understand in order to master React.js: The third part is usually the hardest to grasp for React.js beginners. What you put into state? What you put into props? Aren‚Äôt they interchangeable? If you don‚Äôt get it right, you can get into some nasty trouble . Not to mention you can nullify all benefits React provides to you. There is also a problem of React.js being just a library. People can learn creating even the most complicated component, but they can still struggle in a certain field frameworks give you for free - data management. Building the user interface is very important but it is nothing if you can‚Äôt manage the data coming out from using it. What if you could get rid of both problems at the same time? That would certainly help you with getting into a right direction with your React.js learning. And you know what is the best part? In fact, you can. Initially React.js was published by Facebook and there was no opinionated way to solve problems of data management, nor cumbersome state management. After a short while Facebook proposed its own solution - a so-called Flux architecture . The community went crazy. There was a massive EXPLOSION of libraries that were foundations to implement your app in a Fluxy way. Those libraries was often focused on different goals - there were type-checked-flux libaries, isomorphic flux libraries, immutable-flux libraries and so on. It was a headache to choose wisely among all of those! Not to mention the hype over Flux caused some damage - this is not a silver bullet and people followed the idea blindly. Today this situation is more or less settled. Many libraries from this time just died, replaced by better solutions. It can be observed that this ‚Äúflux libraries war‚Äù has a one clear winner - Redux library. Redux won because many things. One is the most important - it is extremely simple. The second one - it needs a minimal amount of boilerplate. Third - it does only one job - and makes it right. The dreaded problem of most React.js and frontend beginners in general - data management. Let‚Äôs make a thought experiment. Let‚Äôs take three main parts of React.js: This is how React component works (in a great simplification): So state is something persisting within your component - hidden, yet important. This is a problem because to know exactly what is rendered on the screen you need to dive into the React component. And what if there‚Äôd be no state? Let‚Äôs rephrase it a little: So, basically, without state React.js is just a pure function (that is: a function which return value is determined only by their arguments). This makes things even simpler than they are with standard way of doing React. It also takes away the last learning obstacle - state management. Combo React + Redux is extremely efficient in working with components in a stateless fashion. That‚Äôs why it is my preferred way to learn people React.js on the upcoming workshop. I‚Äôm honored to make a workshop as a part of the wonderful wroc_love.rb conference in Wroc≈Çaw, Poland. This is my little thank you to the community, as well as an another occasion to share my knowledge about React.js. I wanted to make this workshop as Arkency-like as possible. You may know that we‚Äôre working remotely and we‚Äôre following async principles. You can learn more about it in Async Remote: The Guide to Building a Self-Organizing Team book which is our ‚Äòmanifest‚Äô of workflow, culture and techniques. While a workshop form is not remote at all, I wanted to make it as async as possible. In the workshop we‚Äôll be developing an app. A real one - it‚Äôll be an application to manage Call-for-Papers process which takes place before a conference. You‚Äôll be presented with a working API, static mockups and working environment where you can just start writing React.js-Redux code. Your goal will be to develop an user interface for this application. You can enter or leave anytime. During the workshop the questions & answers will be accumulated and available for you all time. Everything you‚Äôd need to jump in and code will be written on a blackboard. You can take only a first task and do it. You can just watch. I‚Äôll be here to help you, answer your questions and make a quick introduction to React.js and Redux basics. That‚Äôs all. You don‚Äôt need any prior React.js knowledge. It‚Äôd be great if you saw JavaScript code before - but not necessary. Do you think it is a crazy idea? Or maybe it‚Äôs impossible to make a working app this way in such short time? This is why because you haven‚Äôt seen React.js+Redux combo in action ;). You can enter the workshop free of charge (although the conference is a paid event). The event takes place in a lovely city of Wroc≈Çaw, Poland - 11th of March at 11:00. Mark your calendars - I‚Äôll be happy to see you there! Oh, and don‚Äôt hesitate to reach me through an e-mail , Twitter if you have any further questions. Or maybe you have your story to share - for example what is the hardest part of learning React.js? I‚Äôll be happy to hear them! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-15"},
{"website": "Arkency", "title": "Testing aggregates with commands and events", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/02/testing-aggregates-with-commands-and-events/", "abstract": "Once you start switching to using aggregates in your system (as opposed to say, ActiveRecord objects), you will need to find good ways of testing those aggregate objects. This blogpost is an attempt to explore one of the possible ways. The code I‚Äôm going to show is part of a project that I was recently working on. The app is called Fuckups (yes, I consider changing that name) and it helps us track and learn from all kinds of mistakes we make. Yes, we make mistakes. The important part is to really learn from those mistakes. This is our company habit that we have for years now. During the week we collect all the fuckups that we see. It doesn‚Äôt matter who did them, the story and the lesson matters. We used to track them in a Hackpad called ‚ÄúFakapy jak startupy‚Äù which means ‚ÄúFuckups as Startups‚Äù (don‚Äôt ask). That‚Äôs why this name persisted until today. Our hackpad has all the archives now.\nEvery Friday we have a weekly sync. As a remote/async company we avoid all kinds of ‚Äúsync‚Äù meetings. Fridays are the exception, when we disuss all kinds of interesting things as the whole team. We call it ‚Äúweeklys‚Äù. One part is usually the most interesting is the Fuckups part. We iterate through them, one person says what happened and we try to discuss and find the root problems. Once a fuckup is discussed we mark it as ‚Äúdiscussed‚Äù. The app is a replacement for the hackpad. In its core, it‚Äôs a simple list, where we append new things. I tried to follow the ‚ÄúStart from the middle‚Äù approach here and it mostly worked. It‚Äôs far from perfect, but we‚Äôre able to use it now. One nice thing is that we can add a new fuckup to the list by a simple Slack command. No need to leave Slack anymore. Although the app is already ‚Äúin production‚Äù, new organizations can‚Äôt start using it yet. The main reason was that I started from the middle with authentication by implementing the Github OAuth. This implementation requires Github permissions to read people organizations (because not all memberships are public). Before releasing it to public, I wanted to implement the concept of a typical authentication - you know - logins/passwords, etc. UPDATE: The Fuckups app is now open to the public (and free). It‚Äôs still rough on the edges, but feel free to test it at http://fuckups.arkency.com/fuckups This is where I got sidetracked a bit. It‚Äôs our internal project and not a client project, so there‚Äôs a bit more freedom to experiment. As you may know, we talk a lot about going from legacy to DDD . That‚Äôs what we usually do. It‚Äôs not that often that we do DDD from scratch. So, the fuckups app core is a legacy Rails Way approach. But, authentication is another bounded context. I can have the excitement of starting a new ‚Äúsubproject‚Äù here. Long story, short, I started implementing what I call access library/gem. A separated codebase responsible for authentication, not coupled to fuckups in any way. There will be a concept of organizations, but for now I just have the concept of Host (a container for organizations). We can think of it as the host for other tenants (organizations). I implemented the host object as the aggregate. At the moment it should know how to: Looking at different kinds of aggregates implementations, I decided to go with the way where the aggregate accepts a command as the input. It makes the aggregate closer to an actor. It‚Äôs not an actor in the meaning of concurrent computation, but an actor in the conceptual meaning. This means, the host takes 4 kinds of messages/commands as the input. The expected output for each command is an event or a set of events. For example, if we have a RegisterUser command, then if it‚Äôs successfully handled, we expect an UserRegistered event. In this case, I also went with Event Sourcing the aggregate. It means that an aggregate can be composed from events. BTW, here we get a bit closer to the Functional Programming way of thinking. I didn‚Äôt go with full FP yet, but I‚Äôm considering it. With ‚Äúfull‚Äù FP the objects here wouldn‚Äôt mutate state, but they would return new objects every time a new event is applied. If you‚Äôre interested what‚Äôs the AggregateRoot part, here is the current implementation (it‚Äôs part of our aggregate_root gem): What‚Äôs worth noticing is that the output of each aggregate command handling is an event (or a set of events). We collect them in the @unpublished_events and expose publicly. Exposing such thing publicly is not the perfect thing, but it works and solves the problem of a potential dependency on some kind of event store. How can we test it? In the beginning, I started testing the aggregate by preparing state with events. Then I applied a command and asserted the unpublished_events .\nIt works, but the downside is similar to using FactoryGirl for ActiveRecord testing . There‚Äôs the risk of using events for the state, which are not possible to happen in the real world usage. If you like this approach, we show it also as a way to test the read models and separately for the write side . Another approach that I‚Äôm aware of is by treating the aggregate as a whole and test with whole scenarios, by applying a list of commands. This is the command-driven testing in practice: I like this approach. The only downside is that I need to assert the whole list of events here. This is no longer just testing handling one command, though. It‚Äôs testing the whole unit (aggregate with commands, events and value objects) with scenarios. In this case, testing all events kind of makes sense. What‚Äôs your opinion here? If you‚Äôre stuck with a more Rails Way code but you like the command-driven approach, then form objects may be a good step for you. Form objects are like the Command for the whole app, not just the aggregate, but their overall idea is similar. We wrote more about form objects in our ‚ÄúFearless Refactoring: Rails Controllers‚Äù book . ‚Ä¶ and just to finish the Fuckups app story - once I‚Äôm ready with implementing this authentication gem, I‚Äôm going to plug it into the application. Then the next step is to extend the authentication with tenants feature and I can invite you to testing the app :) We‚Äôre talking about aggregates and the ways of testing them with more details at our Rails DDD workshops. The next one is in Lviv, Ukraine, 25-26th May, 2017. It‚Äôs worth mentioning that Lviv now got quite a number of new flight connections from many European cities. It‚Äôs a beatiful city, see you there! http://blog.arkency.com/ddd-training/ Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-19"},
{"website": "Arkency", "title": "Private classes in Ruby", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/02/private-classes-in-ruby/", "abstract": "One of the most common way to make some part of your code more understandable and explicit is to extract a class.\nHowever, many times this class is not intended for public usage . It‚Äôs an implementation detail of a bigger\nunit. It should not be used be anyone else but the module in which it is defined. So how do we hide such class\nso that others are not tempted to use it? So that it is clear that it is an implementation detail ? I recently noticed that many people don‚Äôt know that since Ruby 1.9.3 you can make a constant private. And that‚Äôs\nyour answer to how . The Person class can use Secret freely: But others cannot access it. So Person is the public API that you expose to other parts of the system and Person::Secret is just an\nimplementation detail. You should probably not test Person::Secret directly as well but rather through the public Person API\nthat your clients are going to use. That way your tests won‚Äôt be brittle and depended on implementation . That‚Äôs it. That‚Äôs the entire, small lesson. If you want more, subscribe to our mailing list below or buy Fearless Refactoring . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-22"},
{"website": "Arkency", "title": "How and why should you use JSON API in your Rails API?", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2016/02/how-and-why-should-you-use-json-api-in-your-rails-api/", "abstract": "Crafting a well-behaving API is a virtue. It is not easy to come up with good standards of serializing resources, handling errors and providing HATEOAS utilities to your design. There are a lot application-level concerns you need to make - whether you want to send back responses in mutation requests (like PUT/PATCH/POST requests) or just use HTTP headers. And it is hard - and by hard I mean you need to spend some time to get it right. There are other things you need to be focused on which are far more important than your API. Good understanding of your domain, choosing a right architecture of your whole app or implementing business rules in a testable and correct way - those are real challenges you need to solve in the first place. JSON API is a great solution to not waste hours on reinventing the wheel in terms of your API responses design. It is a great, extensible response standard which can save your time - both on the backend side and the client side. Your clients can leverage you‚Äôre using an established standard to implement an integration with your API in a cleaner and faster way. There is an easy way to use JSON API with using a great Active Model Serializers gem. In this article I‚Äôd like to show you how (and why!). JSON API is a standard for formatting your responses. It handles concerns like: How to present your resources to allow clients to recognize it just by the response contents? It is often the case that if you want to deserialize custom JSON responses you need to know both response contents and an endpoint details you just hit. JSON API solves this problem by exposing data type as a first class data in your responses. How to read errors in an automatic way? In JSON API there is a specified format for errors. This allows your client to implement their own representations of errors in an easy way. How to expose data relationships in an unobtrustive? In JSON API attributes and relationships of a given resource are separate . That means that clients which are not interested in relationships can use the same code to parse response having them or not. Also it allows to implement backends which can include or exclude given relationships on demand, for example by passing an include GET option to a request in a very easy way. This can make performance tuning much easier. There is a great trend of creating ‚Äúself-descriptive APIs‚Äù for which a client can configure all endpoints by itself by following links included in the API responses. JSON API supports links like these and allows you to take a full advantage of the HATEOAS approach. There is a clear distinction between resource-related data and an auxillary data you send in your responses. This way it is easier to not make wrong assumptions about responses and scope of their data. Summarizing, JSON API solves many problems you‚Äôd like to solve by yourself. In reality you won‚Äôt use all features of JSON API together - but it is liberating that all paths you can propably take in your API development are propably covered within this standard. Thanks to being standard there is a variety of client libraries that can consume JSON API-based responses in a seamless way. In Ruby there are also alternatives , but we‚Äôll stick with the most promising one - Active Model Serializers. JSON API support for AMS comes with the newest unrealeased versions, currently in the RC stage. To install it, you need to include it within your Gemfile : That‚Äôs it. Because it is the RC version it is unfortunately not supporting the whole JSON API spec (for example it‚Äôs hard to embed links inside relationships), but the codebase is still growing. With 0.10.x versions of Active Model Serializers uses the idea of adapters to support multiple response types. By default it ships with a pretty bare response format, but it can be changed by a configuration. You‚Äôre interested in JSON API, so the adapter should get changed to JSON API adapter. To configure it, enter this line of code in config/environments/development.rb , config/environments/test.rb and config/environments/production.rb : This way the response format will be transformed into format conforming JSON API specification. The idea of using AMS is pretty simple: Let‚Äôs take the simplest example: This is a piece of code taken from the backend application written for the React.js workshops . The Conference consists of a name and an id . There is also a relationship between a Conference and ConferenceDay in a one-to-many fashion. Let‚Äôs see the test for an expected response out of such resource. We assume there are no conference days defined (yet!). Also jsonize is transforming symbol keys into string keys deeply and json is just calling MultiJson.load(response.body) : As you can see, there is a clear distinction between three parts: The whole response is wrapped with a data field. There are two different ‚Äúroot‚Äù fields like this: links if you‚Äôd like to implement HATEOAS pagination/other links for a given resource and meta where you put an information independent of the given resource, but still important for a client. Data field is necessary, other ones are optional. So far, so good. But you need the controller code to make asking endpoint possible: conferences_repository is an example of the Repository pattern you may also know from our Rails Refactoring book . As you can it is quite normal controller - if you install AMS rendering through json: option of render is getting handled by your serializer by default. While I find such implicitness bad I can live with it for now. And, last but not least - a ConferenceSerializer : As you can see a syntax is very similar to what you have inside your model (especially for relationships). Attributes specify which fields from a model you will expose. For example here both created_at and updated_at can be added if there‚Äôs a need. This piece of code makes the whole test pass. And this is the most basic usage of AMS. You can do much more with it. Unfortunately for now AMS do not support links on a relationships level, making it a bit hard to implement HATEOAS on the relationship level. But you can implement links on a top level by passing an appropriate options. By default JSON API specifies only an information needed to retrieve a related object using a separate HTTP call - id and type . So for having one day inside a conference the JSON response will look like this: As you can see even after we defined our relationship serializer to include attributes like from , to or label , they are not serialized at all! This is because JSON API makes even another separation: included resources are in the separate root field . To render the response with days included, we need to pass an additional option: As you can see the whole object is contained within included root field. This way if you are not interested in included resources you can just read data and omit included completely. It is very neat and desirable if client wants to configure itself. JSON API is a great tool to have in your toolbox. It reduces bikeshedding and allows you to focus on delivering features and good code. Active Model Serializers make it easy to work with this well-established standard. Your client code will benefit to thanks to tailored libraries available for reading JSON API-based responses. If you‚Äôd like to learn more how we recommend to use JSON API within Rails apps, then look at our new book ‚ÄúFrontend-friendly Rails . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-23"},
{"website": "Arkency", "title": "The smart way to check health of a Rails app", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/2016/02/the-smart-way-to-check-health-of-a-rails-app/", "abstract": "Recently we added monitoring to one of our customer‚Äôs application. The app was tiny, but with a huge responsibility. We simply wanted to know if it‚Äôs alive. We went with Sensu HTTP check since it was a no-brainer. And it just worked, however, we got warning from monitoring tool. Authentication is required to access any of given app resources. It simply does redirect to login page. 302 code is returned instead of expected one from 2xx family. That‚Äôs not what satisfies us. We‚Äôve found out that the best solution would be having a dedicated endpoint in the app. This endpoint should be cheap for app server to respond. It shouldn‚Äôt require any authentication nor unexpected redirection . It should only return 204 No Content . Monitoring checks will be green and everyone will be happy. We decided to implement /health in our app. Nonetheless, we agreed that it‚Äôs a really good practice to do such checks in all of our apps and we released a tiny gem for that. Just to easily reuse this approach. The gem is named wet-healt_endpoint . Btw. We had to prefix health_endpoint with something since all simple names are already taken in the Rubygems world. The gem consists of Middleware which is being attached close to the response in the app‚Äôs request-response cycle. It checks if application responds to such route, it not it responds to the client with 204 No Content .\nWe used such approach not to override already existing endpoints in an app. Just in case, someone is developing app related to health . That‚Äôs how it‚Äôs attached to the app: To use it, you simply need to add to your Gemfile and run bundle install . You can simply run a curl command or even better, write a test: Reverse proxies like Haproxy or Elastic Load Balancer understand if app instance is down and don‚Äôt route traffic to such ones. Please see the sample Haproxy configuration: Ok, so we order Haproxy to make a GET request to /health endpoint. We consider everything is ok if 204 code is returned. The action is performed every 3 seconds. After 3 sequential failures, an instance is marked as failed and no traffic is being sent there. After 2 successful checks instance is considered healthy. Last two lines specify which instances should be checked. It‚Äôs better to know that the app is down from your monitoring tool than from angry customer‚Äôs call. ;) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-26"},
{"website": "Arkency", "title": "Creating new content types in Rails 4.2", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2016/03/creating-new-content-types-in-rails-4-dot-2/", "abstract": "While working on the application for React.js+Redux workshop I‚Äôve decided to follow the JSON API specification of responses for my API endpoints. Apart from a fact that following the spec allowed me to avoid bikeshedding, there was also an interesting issue I needed to solve with Rails. In JSON API specification there is a requirement about the Content-Type being set to an appropriate value . It‚Äôs great, because it allows generic clients to distinguish JSONAPI-compliant endpoints. Not to mention you can serve your old API while hitting the endpoint with an application/json Content-Type and have your new API responses crafted in an iterative way for the same endpoints. While being a very good thing, there was a small problem I‚Äôve needed to solve. First of all - how to inform Rails that you‚Äôll be using the new Content-Type and make it possible to use respond_to in my controllers? And secondly - how to tell Rails that JSON API requests are very similar to JSON requests, thus request params must be a JSON parsed from the request‚Äôs body? I‚Äôve managed to solve both problems and I‚Äôm happy with this solution. In this article I‚Äôd like to show you how it can be done with Rails. First problem I needed to solve is usage of a new content type with Rails and registering it so Rails would be aware that this new content type exists. This allows you to use this content type while working with respond_to or respond_with inside your controllers - a thing that is very useful if you happen to serve many responses dependent on the content type. Fortunately this is very simple and Rails creators somehow expected this use case. If you create your new Rails project there will be an initializer created which is perfect for this goal - config/initializers/mime_types.rb . All I needed to do here was to register a new content type and name it: This way I managed to use it with my controllers - jsonapi is available as a method of format given by the respond_to block: That‚Äôs great! - I thought and I forgot about the issue. Then during preparations I‚Äôve created a simple JS client for my API to be used by workshop attendants: Then I‚Äôve decided to test it‚Ä¶ Since I wanted to be sure that everything works correctly I gave a try to the APIClient I‚Äôve just created. I opened the browser‚Äôs console and issued the following call: Bam! I got the HTTP 400 status code. Confused, I‚Äôve checked the Rails logs: Oh well. I passed my params correctly, but somehow Rails cannot figure how to handle these parameters. And if you think about it - why it should do it? For Rails this is a completely new content type. Rails doesn‚Äôt know that this is a little more structured JSON request. Apparently there is a Rack middleware that is responsible for parsing params depending on the content type. It is called ActionDispatch::ParamsParser and its initialize method accepts a Rack app (which every middleware does, honestly) and an optional argument called parsers . In fact the constructor is very simple I can copy it here: As you can see there is a list of DEFAULT parsers and by populating this optional argument you can provide your own parsers. Rails loads this middleware by default without optional parameter set. What you need to do is to unregister the ‚Äúdefault‚Äù version Rails uses and register it again - this way with your custom code responsible for parsing request parameters. I did it in config/initializers/mime_types.rb again: Let‚Äôs take a look at this code in a step by step manner: As you can see this is quite powerful. This is a very primitive use case. But this approach is flexible enough to extract parameters from any content type. This can be used to pass *.Plist files used by Apple technologies as requests (I saw such use cases) and, in fact, anything. Waiting for someone crazy enough to pass *.docx documents and extracting params out of it! :) While new content types are often useful, there is a certain work needed to make it work correctly with Rails. Fortunately there is a very simple way to register new document types - and as long as you don‚Äôt need to parse parameters out of it is easy. As it turns out there is a nice way of defining your own parsers inside Rails. I was quite surprised that I had this issue (well, Rails is magic after all! :)), but thanks to ActionDispatch::ParamsParser being written in a way adhering to OCP I managed to do it without monkey patching or other cumbersome solutions. If you know a better way to achieve the same thing, or a gem that makes it easier - let us know. You can write a comment or catch us on Twitter or write an e-mail to us. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-03-13"},
{"website": "Arkency", "title": "Using anonymous modules and prepend to work with generated code", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/02/using-anonymous-modules-and-prepend-to-work-with-generated-code/", "abstract": "In my previous blog-post about using setters one of the commenter mentioned a case in which the setter methods are created by a gem. How can we\noverwrite the setters in such situation? Imagine a gem awesome which gives you Awesome module that you could use in your class\nto get awesome getter and awesome=(val) setter with an interesting logic.\nYou would use it like that: and here is a silly Awesome implementation which uses meta programming to\ngenerate the methods like some gems do. Be aware that it is a bit contrived example. Nothing new here. But here is something that the authors of Awesome forgot. They forgot to strip the val and remove the leading and trailing whitespaces. For example. Or any other thing that the authors of gems forget about\nbecause they don‚Äôt know about your usecases. Ideally we would like to do what we normally do: But this time we can‚Äôt. Because the gem relies on meta-programming and adds setter method directly to our class.\nWe would simply overwrite it. If the gem did not rely on meta programming and followed a simple convention: you would be able to achieve it simply. But gems which need the field names to be provided\nby the programmers don‚Äôt have such comfort. Here is what you can do if the gem authors add methods directly to your class: Use prepend with anonymous module. That way awesome= setter defined in the module is higher in the hierarchy. You can make the life of users of your gem easier. Instead of directly defining methods in the class, you can\ninclude an anonymous module with those methods. With such solution the programmer will be able to use super `. That way the module, with methods generated using meta-programming techniques, is lower\nin the hierarchy than the class itself. Which makes it possible for the users of your gem to just use old school super ‚Ä¶ ‚Ä¶without resort to using the prepend trick that I showed. That‚Äôs it. That‚Äôs the entire lesson. If you want more, subscribe to our mailing list below or buy Fearless Refactoring . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-02-29"},
{"website": "Arkency", "title": "Custom type-casting with ActiveRecord, Virtus and dry-types", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/03/custom-typecasting-with-activerecord-virtus-and-dry-types/", "abstract": "In Drop this before validation and just use a setter method I showed you how to avoid a common pattern of using before_validation to\nfix the data. Instead I proposed you just overwrite the setter, call your custom logic there\nand use super . I also showed you what you can do if you can‚Äôt easily call super . But sometimes to properly transform the incoming data or attributes you just need\nto improve the type-casting logic . And that‚Äôs it. So let‚Äôs see how you can add your\ncustom typecasting rules to a project. And let‚Äôs continue with the simple example of stripped string. If you want to improve type casting for you Active Record class or if you need it for a different layer (e.g.\na Form Object or Command Object )\nin both cases you are covered. Historically, we have been using Virtus for that non-persistable layers. But\nwith the recent release of dry-types (part of dry-rb) we started also investigating this angle as it looks very promising. I am very happy with the improvements\nadded between 0.5 and 0.6 release. Definitelly a step in a right direction. That‚Äôs it. That‚Äôs the entire lesson. If you want more free lessons\nhow to improve your Rails codebase, subscribe to our mailing list below.\nWe will regularly send you valuable tips and tricks. 3200 developers already\ntrusted us. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-03-20"},
{"website": "Arkency", "title": "How I hunted the most odd ruby bug", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/04/how-i-hunted-the-most-odd-ruby-bug/", "abstract": "Every now and then there comes a bug in programmer‚Äôs life that is different than anything\nelse you‚Äôve encountered so far. Last time for me it was 3 days of debugging to find out that MySQL\nwas returning incorrect results. We didn‚Äôt have to do much to fix it. We removed an index and created it again\nfrom scratch. There, problem gone. But that was a few years ago. Last week I was hunting an entirely different beast . But before we dive into details, let me tell\nyou a bit about the business story behind it. We are working on a ticketing platform which sells tickets for big events, festivals but also for smaller gigs. The nature of this industry is that from time to time there are spikes of sales when an organizer opens\nsales for hot tickets. Early birds discounts and other promotions. You‚Äôve probably attended some conferences\nand concerts. You know how it works. In our application there are tons of things that happen in the background . Especially after the sale. We want the\nsale to be extremely quick so that we can handle the spikes nicely without hiccups. So when the purchase is finalized\nwe have 15 background jobs or maybe even more. Some are responsible for generating the PDFs of those tickets,\nsome are responsible for delivering emails with receipts. Other communicate with 3rd party APIs responsible for\ndelivering additional services for the buyers. Let me show you how the sales spikes look like. And their effect on the number of queued jobs that we had: As you can notice we were not even processing our jobs fast enough to handle next spikes (other organizers, other events\nare still selling at the same time on our platform). During normal hours we were digging ourselves out from\nthe 4K of jobs (sometimes we even had spikes to 40K jobs) but very slowly. So obviously we were not happy with our\nperformance. But I know our codebase and I know that many of background jobs do almost nothing. They do 1 or 2 SQL query and\ndecide that given sale is not related to them. When the sales is related they need to do something which does take long times\nbut often they quit very quickly. Also, we had 6 machines with 4 resque workers running on them so technically we\nhad 24 processes for handling the load of jobs. And yet everything appeared to be‚Ä¶ slow. Very slow. I had this gut feeling that there is a hidden problem and we are missing something. We started investigating. We logged in to our utility machines and simply checked with htop what are the processes doing, what‚Äôs the load etc. The load was often between 0.2 - 0.4 . For 2 CPU machines with 4 resque jobs running on each of them. In the moment\nwhen they should be processing, like crazy, thousands of queued jobs. So that sounded ridiculously low . We started watching resque logs which show when a job is taken and when it‚Äôs done. I was able to see jobs being processed\nnormally one by one and then a moment later things stopped. A job was done but next job was not taken and started.\nBut if you looked at the tree of processes on the server you could clearly see that a master resque process was\nstill waiting for it‚Äôs child. Even though the child already reported the job as done. 31480 is a child processing a job, 30383 is the master process waiting for the child to finish. 31481 is a ruby\nthread in the master ruby process just trolling you in htop output. Use Shift+h to disable viewing threads\nin htop in the future. It took me long time to understand what‚Äôs going on because both parent and child processes had\n2 additional threads for honeybadger and new relic. So let‚Äôs reiterate the confirmed problem. You must know that resque is forking child processes for every job and waiting for them to die before starting\nwith next job in the queue. Here is related part of code from resque This was very hard. I started by adding more log statements on production to the resque gem to see what‚Äôs going on.\nI found nothing really interesting. All I was able to see was that the child process did the job,\nstarted firing at_exit hooks but was still hanging there. I used strace to see what‚Äôs going on when then process was doing nothing and later and it was not very helpful. I also used gdb to attach to the process and with thread apply all backtrace I was able to see: Based on main in Thread 1 I assumed this was the main thread. And apparently there were 2 more threads. rb_thread_terminate_all looked interesting to me but I could not google anything useful about it this internal\nRuby method. I wasn‚Äôt sure what those two threads were for, which gems would use them. I was able to figure it\nout later with one trick: which gave me: So I had two potential candidates to investigate. New Relic and Honeybadger. I grepped their codebase for Thread , sleep , at_exit , Thread.join and investigated the code around it.\nI was trying to find out if there could be a race condition, a situation in which the main ruby thread\nfrom at_exit callback would call Thread.join to wait for a honeybadger or new relic thread responsible for either\ncollecting or sending data to them. But I could not find anything like it. I configured New Relic and Honeybadger gems to work in DEBUG mode on one machine. I was watching the logs and\ntrying to figure out something odd at the moment when I saw resque doing nothing after the job was done. It was not\neasy task because the occurences of those do-nothing ` periods were quite random. Sometimes I was able to spot it\nafter 3 jobs were done. Other times it had to process 80 jobs for the problem to appear. Also it was not easy\nto spot any kind of similarity. But at some point I noticed one pattern. The previous log listing that I show you did not contain honeybadger when the problem occured. It looked like. So I decided to focus more of my efforts on looking into honeybadger logs and codebase. I still could not find\nout why would anything threading related in Honeybadger cause an issue. So I paid more attention to the logs. I even edited the gem on production to produce much more logs . And then I noticed something: This is how the logs looked like when everything went fine (line numbers might not reflect those in your\ncodebase as I added more debugging statements). One line highlighted for your convinience. And compare it to: The second time stopping agent appears in the log exactly after 10 seconds. And guess what I‚Äôve remembered from reading Honeybadger codebase . And here is where that delay is being used inside work method : And the work methods is being called from inside of run method which is what Honeybadger is scheduling in a\nseparate thread. Ok, so we have a thread running which sometimes sleeps. But how would that affect us? Why would it matter.\nAfter all when Ruby main thread finishes it does not care about other threads that are running. Also\nHoneybadger is not doing Thread.join at any point to wait for that thread. If anything, it‚Äôs doing the opposite.\nCheck it out .\nIt‚Äôs killing the Thread. At some point I realized a fact that was somehow missing my attention.\nThere are two cases what can happen inside work method. Imagine that there is an exception when we sleep What would happen is that the we would quit the ensure block and the exception would\nbubble up. That‚Äôs an easy and harmless scenario. But what happens when the exception happens inside one of the flush methods? We land inside the ensure block and we sleep . It was an interesting revelation for me.\nIt seemed important and similar to what might randomly happen in real life. A race condition in\naction. And thread being killed is like an exception in your ruby code. But I still could not find any Ruby code that would actually wait for that Thread. But everything pointed\nout that something does indeed wait for it. So I decided to create a very small piece of Ruby code similar\nto what Honeybadger does. Just to convince myself that Ruby is not guilty here. And that I am clearly\nmissing something which causes this whole situation to occur. Here is what I wrote. This 10_000_000.times { SecureRandom.hex } is just there so that it takes more than 10 seconds which we wait\nin main thread before trying to exit the whole program. And here is the output: Well, nothing interesting here. I was obviously right. Ruby does not wait for other threads to finish.\nSo the program stopped after 10 seconds. What else would you expect? But you never know, so let‚Äôs execute it again. And here we are with our problem reproduced . This time Ruby waited for the thread t to finish. So it waited\nfor the 15s sleep delay inside the ensure block. I did not see that coming. I was able to randomly cause this behavior in Ruby 2.1 , 2.2 and 2.3 as well. The behavior of this program\nwas non-deterministic for me. Sometimes it waits and sometimes it does not. Now that I was sure that I have not missed any code in Honeybadger and for, so far unknown, reason Ruby just waits for its\nthread to finish; it was time to hot-fix the problem. I knew I could just use send_data_at_exit config to omit the problem. But I want to have my exceptions from\nbackground jobs logged. And since if there is an exception the jobs immediately finishes and quits I was afraid\nwe would miss them. So I wanted to dig into the root problem of it and find a fix or workaround that would let us\nstill send the last data when the program quits. Here is my very simple hotfix. It skips the sleep phase if the thread is aborting which is the state it has\nafter being killed with Thread.kill . I notified Honeybadger , Resque and Ruby team about this problem and I hope they will come up with a good coldfix. The spectacular effect was visible immediately in all metrics that we had. The remaining jobs were processed much much faster after the hotfix deployment. The CPU usage became much higher on the utility machines for the time they were processing the spike. I suspect that with that improvement we will be even able to decommission some of them. Did you like this article? You will find our Rails books interesting as well . We especially recommend Responsible Rails which contains more stories like that. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-04-18"},
{"website": "Arkency", "title": "Packaging ruby programs in NixOS", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/2016/04/packaging-ruby-programs-in-nixos/", "abstract": "Recently at Arkency we‚Äôre exploring how NixOS could fit our infrastructure.\nFrom a few weeks we‚Äôve switched most of our projects CI systems from CircleCI to Buildkite .\nBuildkite offers unique offer in which it is us who provide infrastructure and containers to test our applications so we‚Äôve decided to setup these containers with NixOS and we are by far happy with this solution. However in this post I would like to describe how to package a simple ruby program, based on simple password manager CLI utility - pws . I‚Äôve chose such a simple program to reduce the scope of this post - I hope that in future I‚Äôll be able to describe more complicated cases. Firstly, we should get to know how (without details) packaging process in NixOS looks like.\nTypical package in NixOS is a directory with a default.nix file (and maybe others). *.nix files are files written in Nix , which is functional programming language and package manager.\nA default.nix file is a function which takes configuration options and dependencies as an input and creates package as an output. Just to recap the syntax of nix language, you should know a few things: Our program is distributed as a gem, so we can take an advantage of the fact that NixOS has a built-in bundlerEnv function for creating Bundler environments as packages .\nAfter checking out how other ruby programs‚Äô packages look like, we can write following function: As you can see, it‚Äôs a simple function. It‚Äôs argument, a record, has two fields: bundlerEnv and ruby . Body of this function does only one thing: it calls bundlerEnv function, with record as an argument. This record has fields: name , ruby , gemfile , lockfile and gemset`. About fields from the arguments‚Äô record: bundlerEnv is aforementioned function creating Bundler environment and ruby is‚Ä¶ Ruby, a program. You‚Äôve to get use to the fact that the programs are just values passed in as arguments to other functions. There are few important things right here: bundlerEnv function requires a few things to work: a Gemfile and Gemfile.lock files of the bundler environment it needs to build and a gemset.nix file. The easy part is to create Gemfile and Gemfile.lock .\nLet‚Äôs create simplest possible Gemfile containing pws gem: Let‚Äôs generate Gemfile.lock file by running bundle install command. Now, if we have both Gemfile and Gemfile.lock in one directory, you can generate gemset.nix using Bundix tool. gemset.nix is basically a Gemfile.lock but written in nix language. We need it because we want Nix to know what are dependencies of our package. Note that bundix is not yet finished, thus it is not able to translate less used Gemfile features like path: attribute. Examplary gemset.nix looks like this: Currently our function is generating a bundler environment and if we would release it this way, pws program would be able to run. However current default.nix has two major disadvantages: Thus, let‚Äôs create a wrapper package which will just use generated bundler environment and provide only pws as a binary . Our wrapper package can be achieved by the following code: mkDerivation from stdenv is a function which is usually used to create packages (the simple ones) written for example in C or Bash. As you can see env field is pretty much our bundler environment which we‚Äôve used before but now it‚Äôs only part of our package. phases field is an array which keeps list of phases needed to build this package. It‚Äôs mostly convention-driven and in the end we can use arbitrary names there. We declare that the process of building our package consists of only one phase (named installPhase ). installPhase itself is just a simple 2-line bash script. It uses a makeWrapper function provided by NixOS just for situation like this - it generates a simple script which does only one thing - call an exec given as an argument. If we check the source of such generated file, it looks like this: There‚Äôs only one issue left with our current package which I‚Äôve mentioned before. pws to run properly needs xsel (a clipboard utility). However it‚Äôs only run-time dependency . Run-time dependency (as opposed to build-time dependency ) means that we can successfully build a package without this dependency, but our program will misbehave when this run-time dependency is not present. That‚Äôs why we want to modify our package a little bit: We‚Äôve added xsel as dependency. We‚Äôve also modified the installPhase script ( makeWrapper call, to be specific) to prepend location of xsel (which is something like /nix/store/...-xsel/bin ) to our PATH environment variable. Now our package is done. You could follow Chapter 10. Submitting changes of nixpkgs manual to release your package to the public. Based on my experience, it‚Äôs very simple process . If you‚Äôre a ruby developer I hope this guide got you closer to the nix ecosystem and let me know if there are topics you would like to get coveraged. If you are interested in deploying your ruby applications in a declarative way, sign up to our newsletter to get info on that topic. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-04-29"},
{"website": "Arkency", "title": "The anatomy of Domain Event", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/2016/05/the-anatomy-of-domain-event/", "abstract": "Almost 2 years and over 16 million domain events ago I‚Äôve started a process of ‚Äúswitching the mindset‚Äù. I had no production experience with Event Sourcing (BTW it still is used only in some parts of the application, but that‚Äôs a topic for another post), I had only a limited experience with Domain Driven Design (mainly knowing the tactical patterns). During that time, a lot has changed. I‚Äôve started introducing new concepts in our project‚Äôs code base from the middle. Not with Domain Driven Design, not with Event Sourcing or CQRS (Command Query Responsibility Segregation). It all has started by just publishing Domain Events. The more Domain Events we have ‚Äúpublished‚Äù the better understanding of our domain I‚Äôve got. Also, I‚Äôve started better understand the core concepts of Domain Driven Design, the terms like bounded context , context map and other from strategic patterns of DDD started to have more sense and be more and more important.\nOf course, I‚Äôve made a few mistakes, some of them still bite us because of decisions made almost 2 years ago ;) Our first ever published domain event is: Here is a set of rules / things to consider when you will build your domain events. Each of them is based on a mistake I‚Äôve made ;) In my old talk I‚Äôve presented at dev‚Äôs user group meetup I‚Äôve defined domain event as: The name of a domain event is extremely important. It is the ‚Äúdefinition‚Äù of an event for others. It brings a lot of value when defined right, but it might be misleading when it won‚Äôt capture the exact business change. In the example event above the name of the domain event is ProductItemSold . And this name is not the best one. The application domain is not selling some products but selling tickets for events (actually that‚Äôs huge simplification but it does not matter here). We do not sell products. We sell tickets. This domain event should be named TicketSold . Yeah, sure we could also sell some other products but then it should be a different domain event. There are very few domains where something is really created. Every time I see a UserCreated domain event I feel that this is not the case. The user might be registered, the user might be imported, I don‚Äôt know the case when we really create a user (he or she exists already ;P). Don‚Äôt stop when your domain expert tells you that something is created (updated or deleted). It is usually something more, something that has real business meaning. And one more thing: don‚Äôt talk CRUD to your domain expert / customer. When he will start to talk CRUD, you are in serious trouble. You know what‚Äôs the biggest tragedy in software engineering? The customers gave up and learnt to speak CRUD to developers. Check the blog post of Udi Dahan where he explains this in more details. You might have spotted the attributes in the data argument of our first domain event. This is something I dislike most in that domain event. Why? Because it creates a coupling between our domain event & our database schema. Every kind of coupling is bad. Especially when you try to build a loosely coupled, event driven architecture for your application. The attributes of a domain event are their contract. It is not something you could and should easily change. There could be parts of the\nsystem that rely on that contract. Changing it is always a trouble. Avoid that by applying Rule #4. The serialized_type ? It this a business language? Really? Or does the business care about the scanned_at when a ticket has been just sold? I don‚Äôt. And all event handlers for this event do not care. That‚Äôs just pure garbage. It holds no meaningful information here. It just messing with your domain event contract making it less usable, more complicated.\nExplicit definition of your domain event‚Äôs attributes will not only let you avoid those unintentional things in the domain event schema but will force you to think what really should be included in the event‚Äôs data. The modified version of my first domain event. Much cleaner. All important data is explicit. Clearly defined contract what to expect. Maybe some more refactoring could be applied here (TicketType value object & OrderSummary value object that will encapsulate the ids of other aggregates).\nAlso, important attribute here was revealed. The ticket‚Äôs barcode. With the explicit definition of domain event schema, it is easier to notice that we do not need to rely on database‚Äôs id of a ticket ( product_item_id ) because we already have a natural key to use - the barcode . Why is the natural better?\nNatural keys are part of the ubiquitous language, are the identifications of the objects you & your domain expert will understand and will use when you will talk about it. It also will be used in most cases on your application UI (if not you should rethink your user experience). When you want to print the ticket you use barcode as identification. When you validate the ticket on the venue entrance you scan the barcode. When some guest has troubles with his ticket your support team asks for the\nbarcode (‚Ä¶or order number, or guest name if you doing it right ;) ). The barcode is the identification of the ticket. The database record‚Äôs id is not. Don‚Äôt let you database leak through your domain. ‚ÄúModelling events forces temporal focus‚Äù Greg Young, DDD Europe 2016 talk How things correlate over time, what happens when this happens before this becomes a real domain problem. Very often our understanding of domain is very naive. If you don‚Äôt include time as a modelling factor your model might not be reflecting what is happening in the real world. Struggling with finding a direction of refactorings to keep your app maintainable? Legacy codebase driving you nuts and not revealing intention? Join our Rails + Domain Driven Design Workshop .\nThe next edition will be held on 12-13th January 2017 (Thursday & Friday), in Wroc≈Çaw, Poland. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-05-09"},
{"website": "Arkency", "title": "Domain Events over Active Record Callbacks", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/05/domain-events-over-active-record-callbacks/", "abstract": "Recently Marcin wrote an article about ActiveRecord callbacks being the biggest code smell in Rails apps , that can easily get out of control. It was posted on Reddit and a very interesting comment appeared there : Imagine an important model, containing business vital data that changes very rarely, but does so due to automated business logic in a number of separate places in your controllers. Now, you want to send some kind of alert/notification when this data changes (email, text message, entry in a different table, etc.), because it is a big, rare, change that several people should know about. Do you: A. Opt to allow the Model to send the email every time it is changed, thus encapsulating the core functionality of ‚Äúnotify when changes‚Äù to the actual place where the change occurs? Or B. Insert a separate call in every spot where you see a change of that specific Model file in your controllers? I would opt for A, as it is a more robust solution, future-proof, and most to-the-point. It also reduces the risk of future programmer error, at the small cost of giving the model file one additional responsibility, which is notifying an external service when it changes. The author brings very interesting and very good points to the table. I, myself, used a few months ago a callback just like that: To schedule indexing in ElasticSearch database. It was the fastest solution to our problem. But I did it knowing that it does not bring us any further in terms of improving our codebase. But I knew that we were doing at the same time other things which would help us get rid of that code later. So despite undeniable usefulness of those callbacks, let‚Äôs talk about a couple of problems with them. Imagine very similar code such as: At first sight everything looks all right. However if the transaction gets rolled-back ( saving Order can be part of a bigger transaction that you open manually)** **you would have indexed incorrect state in the second database. You can either live with that or switch to after_commit . Also, what happens if we get an exception from Elastic. It would bubble up and rollback our DB transaction as well. You can think of it as a good thing (we won‚Äôt have inconsistent DBs, there is nothing in Elastic and there is nothing in SQL db) or a bad thing (error in the less important DB preventend someone from placing an order and us from earning money). So let‚Äôs switch to after_commit which might be better suited to this particular needs. After all the documentation says: These callbacks are useful for interacting with other systems since you will be guaranteed that the callback is only executed when the database is in a permanent state. For example after_commit is a good spot to put in a hook to clearing a cache since clearing it from within a transaction could trigger the cache to be regenerated before the database is updated So in other words. after_commit is a safer choice if use those hook to integrate with 3rd party systems/APIs/DBs . after_save and after_update are good enough if the sideeffects are stored in SQL db as well. So we know to use after_commit . Now, probably most of our tests are transactional, meaning they are executed in a DB transaction because that is the fastest way to run them. Because of that those hooks won‚Äôt be fired in your tests. This can also be a good thing because you we bothered with a feature that might be only of interest to a very few test. Or a bad thing, if there are a lot of usecases in which you need those data stored in Elastic for testing. You will either have to switch to non-transactional way of running tests or use test_after_commit gem or upgrade to Rails 5 . Historically (read in legacy rails apps) exceptions from after_commit callbacks were swallowed and only logged in the logger, because what can you do when everything is already commited? But it‚Äôs been fixed since Rails 4.2 , however your stacktrace might not be as good as you are used to. So we know that most of the technical problems can be dealt with one way or the other and you need to be aware of them. The exceptions are what‚Äôs most problematic and you need to handle them somehow. Here is my gut feeling when it comes to Rails and most of its problems. There are not enough technical layers in it by default. We have views (not interesting at all in this discussion), controllers and models. So by default the only choice you have when you want to trigger a side-effect of our action is between controller and model. That‚Äôs where we can put our code into. Both have some problems. If you put your sideffects (API calls, caching, 2nd DB integration, mailing) in controllers you might have problem with testing it properly. For two reasons. Controllers are tightly coupled with HTTP interface. So to trigger them you need to use the HTTP layer in tests to communicate with them. Instantiating your controllers and calling their methods is not easy directly in tests. They are managed by the framework. If you put the sideeffects into your models, you end up with a different problem. It‚Äôs hard to test the domain models without those other integrations (obviously) because they are hardcoded there. So you must either live with slower tests or mock/stub them all the time in tests. That‚Äôs why there are plenty of blog posts about Service Objects in Rails community. When the complexity of an app rises, people want a place to put after save effects like sending an email or notifying a 3rd party API about something interesting. In other communities and architectures those parts of code would be called Transaction Script or Appplication/Domain/Infrastructure Service . But by default we are missing them in Rails. That‚Äôs why everyone (who needs them) is re-inventing services based on blog posts or using gems (there are at least a few) or new frameworks ( hanami , trailblazer ) which don‚Äôt forget about this layer.\nYou can read our Fearless Refactoring book to get knowledge how to start introducing them in your code without migrating to a new framework. It‚Äôs a great step before you start introducing more advanced concepts to your system. When your callback is called you know that the data changed but you don‚Äôt know why. Was the Order placed by the user. Was it placed by an POS operator which is a different process. Was it paid, refunded, cancelled? We don‚Äôt know. Or we do based on state attribute which in many cases is an antipattern as well. Sometimes it is not a problem that you don‚Äôt know this because you just send some data in your callback. Other times it can be problem. Imagine that when User is registered via API call from from mobile or by using a different endpoint in a web browser we want to send a welcome email to them. Also when they join from Facebook. But not when they are imported to our system because a new merchant decided to move their business with their customers to our platform. In 3 situations out of 4 we want a given side effect (sending an email) and in one case we don‚Äôt want. It would be nice to know the intention of what happened to handle that. after_create is just not good enough. What I recommend, instead of using Active Record callbacks, is publishing domain events such as UserRegisteredViaEmail , UserJoinedFromFacebook , UserImported , OrderPaid and so on‚Ä¶ and having handlers subscribed to them which can react to what happened. You can use one the many PubSub gems for that (ie. whisper ) or rails_event_store gem if you additionally want to have them saved on database and available for future inspection, debugging or logging. If you want to know more about this approach you can now watch my talk: 2 years after the first domain event - the Saga pattern . I describe how we started publishing domain events and using them to trigger sideeffects. You can use that approach instead of AR callbacks. After some time whenever something changes in your application you have event published and you don‚Äôt need to look for places changing given model, because you know all of them. It only gets worse in Rails 5 Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-05-13"},
{"website": "Arkency", "title": "70% off the Rails / TDD / DDD / mutant video class until 11pm 19.05.2016 CEST", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/05/70-percent-off-the-rails-slash-tdd-slash-ddd-slash-mutant-video-class-until-11pm-19-dot-05-dot-2016-cest/", "abstract": "Hi, Just 24 hours left for the 70% discount of the Rails/TDD/DDD/mutant video class. You can buy it here: https://vimeo.com/r/1I82/STcxRjFuel Thanks to everyone who already bought the video class! The number of sales is so overwhelming that Vimeo wrote this to me today: Believe me, it‚Äôs really cool to receive such messages - thank you so much! To celebrate the results, I‚Äôve decided to make some of the videos FREE so that those of you who haven‚Äôt decided yet can have a look: Many people asked me if the code from the videos will be also available. Yes! It‚Äôs definitely going to be available. The minor blocker for now is that I‚Äôm not sure how to get emails of the customers (or any other way of contacting) so that I can collect the github logins and give you access to the Github repository. I‚Äôm talking to the Vimeo crew about it! The first ~20 videos are not touching Rails yet. It‚Äôs by design ;) The story will reveal itself to the point where we‚Äôll switch to Rails in an interesting way ;) The goal of this class is to show you how to do TDD with a Rails app! I guess the people asking this question may miss our React.js love ;) \nThis time, we‚Äôll save you the React.js or any other JavaScript things. No React.js this time ;)\n(those of you who are disappointed, stay tuned, we‚Äôll cooking something React/Rails related for you as well in another form). Here‚Äôs some of the things people wrote to me after watching some of the videos: Just click this link: https://vimeo.com/r/1I82/STcxRjFuel BTW, I‚Äôve just checked with Vimeo. It all looks like we can keep our refund policy with this class as well.\nJust to remind you - all Arkency products (ebooks/videos) are on the policy of refunding at any time, without asking any question. If you buy this class and then decide it‚Äôs not for you, just send as an email and we‚Äôll issue a refund, no questions asked! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-05-19"},
{"website": "Arkency", "title": "The story behind our Rails TDD & DDD video class", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/05/the-story-behind-our-rails-tdd-ddd-video-class/", "abstract": "Get the Rails TDD video class now It‚Äôs hard to say, but I suppose I‚Äôm somewhere in the 33% of the planned scope. Which may mean, the end result will be about 3 hours. Actually, I have a lot of fun doing this, so there might be more :) Don‚Äôt want to overpromise at the moment, though. As it is right now, the videos start with requirements to implement. The requirements are given by the mysterious Jane. Who is she? What are we doing here as a programmer? The story wasn‚Äôt really told in the videos yet. I postponed to later time, assuming that the technical content is more important. However, now I see that the story is also intruiging to many of you :) That‚Äôs good. Why do we need some artificial setting (company, other developers) to be introduced while learning about TDD/DDD. Well, it‚Äôs just me having fun, after taking a writing class . Yes, that‚Äôs right, over the last months, I took a writing class (as in writing fiction). Due to the lack of time, I failed at actually participating in the class by making the homeworks. However, I was in the read-only mode and learnt a lot about stories, narrations, introducing the context to the reader etc. I‚Äôm still far to even be able to write a 2-pages fiction story, but I‚Äôm some steps forward in having fun while creating such narrations. I wanted to make this class but I wanted to put it in an imaginary situation which would be close to the programmers learning from the class. What can be closer than imagining ourselves as starting a new job as a programmer? That‚Äôs how it evolved. I imagined a company called BioRubyCorp . The main character was just hired to be a Ruby programmer. Jane is an existing developer at the company. She‚Äôs the developer who knows about all the stuff but she‚Äôs got too many things on her plate, so you (the narrating character played by me) were hired to help her. At the beginning you don‚Äôt know much about the company itself, from the business perspective. You only know that they operate in the Genetics domain, which is a domain completely foreign to you . In fact, it‚Äôs a domain also foreign to me, so I‚Äôm just learning along the way. Frankly speaking, this is the surprising side-effect to me, that learning about DNA, nucloetides became a fascinating topic to me! In order to tell the full story, I need to tell you about the ongoing Junior Rails Developer class that we‚Äôre running for the last 3 months. It‚Äôs an online, Slack-based class which has its roots in the Rails learning program which I developed over the 5 years of teaching Rails at University of Wroclaw (BTW, students of this university are regular medal-winning participants of team-programming contests in the world). The first student, who was taken to the Junior Rails Developer class was Janek. One of the first exercises to do in the class are the ones taken from exercism.io. Some of them are about the DNA domain. \nDuring the class, we help the student. What struck me was that even our team was suggesting the solutions based on the shortest-code instead of a one which would be more domain-revealing . That made me contemplate what‚Äôs better in such exercism context.\nWhat also bothered me was that even though the requirements were written with a nice domain language, the code was just strings and lists. Primitive obsession!\nI wanted to see if there‚Äôs a way of implementing it with more abstractions. You can see the result in this free video: I was quite happy with the result. I also became fascinated with this domain! So that‚Äôs one part of the story - why the Genetics domain. There‚Äôs the main character - me/you and Jane. There‚Äôs also the (unnamed yet) domain expert who helps us with understanding the domain. This kind of narration helps me build a typical situation in a company. It builds a good foundation for the next requirements and the planned plot twists. To be honest, the story is not fully prepared and planned. You can also influence it by suggesting me what kind of plot twist would be ‚Äútypical‚Äù in such situations. Which requirement to implement? How to deal with deadlines? How to deal with an empty budget? The real goal of the class is to teach you how to combine Rails with TDD . Other things, like DDD and mutant are also crucial. I don‚Äôt want to show the only one and true way. I‚Äôm going to present more techniques over time. I like to think of this class as a TV series, which is easy to consume in short episodes every day. I hope this gives you more context now :) Buy here: https://vimeo.com/ondemand/arkencyruby . Imagine the first day at BioRubyCorp and start watching the videos! Thanks,\nAndrzej Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-05-20"},
{"website": "Arkency", "title": "One more step to DDD in a legacy rails app", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/2016/05/one-more-step-to-ddd-in-a-legacy-app/", "abstract": "Recently I picked up a ticket from support team of one of our clients. Few months ago VAT rates have changed in Norway - 5% became 10% and 12% became 15%. It has some implications to platform users ‚Äî event organizers, since they can choose which VAT rate applies to products which they offer to the ticket buyers. You‚Äôll learn why I haven‚Äôt just updated db column. This app is a great example of a legacy software. It‚Äôs successful, earns a lot of money, but have some areas of code which haven‚Äôt been cleaned yet. There‚Äôs a concept of an organization in codebase, which represents the given country market. The organization has an attribute called available_vat_rates which is simply a serialized attribute, keeping VatRate value objects. I won‚Äôt focus on this object here, since its implementation is not a point of this post. It works in a really simple manner: VatRate objects are Comparable so you can easily sort them; pretty neat solution. Event organizer, who creates eg. a ticket, can choose a valid VAT rate applying to his product. Then, after purchase is made, ticket buyer receives the e-mail with a receipt. This has also side-effects in the financial reporting, obviously. I could simply write a migration and add new VAT rates, remove old ones and update events‚Äô products which use old rates. However, no domain knowledge would be handed down about when change was made and what kind of change happened. You simply can‚Äôt get that information from updated_at column in your database. We have nice domain facts ‚Äúcoverage‚Äù around event concept in the application, so we‚Äôre well informed here. We don‚Äôt have such knowledge in regard to the Organization . I simply started with making a plan of this upgrade. Two things worth notice happen here. Event data contain originator_id , I simply passed there my user_id . Just to leave other team members information about person who performed the change in the event store ‚Äî audit log purpose. The second thing is that I leave old VAT rates still available. Just in case if any event organizer performing changes on his products, to prevent errors and partially migrated state. The amount of products which required change of the VAT rates was so small that I simply used web interface to update them. Normally I would just go with  baking EventService with UpdateTicketTypeCommand containing all the necessary data. All the products on the platform have proper VAT rates set, organization has proper list of available VAT rates. And last, but not least, we know what and when exactly happened, we have better domain understanding, we started publishing events for another bounded context of our app.\nIf you‚Äôre still not convinced to publishing domain events, please read Andrzej post on that topic or even better, by watching his keynote From Rails legacy to DDD performed on wroc_love.rb . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-05-21"},
{"website": "Arkency", "title": "See how we create books. Live", "author": ["Wiktor Mociun"], "link": "https://blog.arkency.com/2016/06/see-how-we-create-books-live/", "abstract": "We are in the middle of a process of converting our bestselling book Rails Meets React from CoffeeScript to ES6. This book turned out to be a huge help for many Rails developers seeking a sane way to create their front end applications. Many people are not using CoffeeScript and could use the information from the book. This is our main request for improving this book, we got from you all. And we are providing this upgrade to all people, who will ever buy Rails Meets React The process of rewriting the content may be interesting for people wanting to convert their React codebase from CoffeeScript to ES6. So, I thought it may be a good time to experiment with video streaming. There is one thing I like about gaming live streams. The streamer has a chance\nto interact with the audience in a real-time. There is a chat where everyone can\nask a question. This is a great way to connect with an audience. That makes me\nreally interested in trying this out. I started to think about how exactly the whole stream could look like and I got an idea. I want to show you the part of our book-creation process . The tooling, the automation, the content creation. We spent a bit of time on making whole technical process around our books quick and easy. We don‚Äôt want to keep it only to ourselves. Here‚Äôs the plan of things I want to show: The stream will start at 2pm UTC this Friday . You can watch it on YouTube . I will answer your questions on chat , during the event. This is our first attempt to making this kind of live streaming event. And I hope it won‚Äôt be the last! :) There are much more things we would love to share with you! And yes! The stream will be saved and published later on our YouTube channel . Psst‚Ä¶ and there will be a special offer for people watching the stream. ;) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-06-10"},
{"website": "Arkency", "title": "Porting a Rails frontend from CoffeeScript to ES6 and JSX - examples", "author": ["Tomasz Patrzek"], "link": "https://blog.arkency.com/2016/06/porting-a-rails-frontend-from-coffeescript-to-es6-with-jsx/", "abstract": "We are working on a new version of our bestseller book - Rails meets React.js . Currently, the book is based in CoffeeScript and we want to port it to ES6. The update will be free for everyone, who bought the CoffeeScript version Here are some examples showing a process of porting from CoffeeScript to ES6 and JSX. In our previous version of this book, we didn‚Äôt use JSX because it does not fit well with CoffeScript.\nWith ES6 it is different, JSX fits here very well. Before: After: Note ES6 template literal ` http://www.example.org/stats/${this.context.user.id} ` which we are using to construct string url. Instead of using React.createClass we are now using ES6 class syntax: Before: After: Since getInitialState does not work with classes syntax, we are initializing state in a constructor. Like in this example: Before: After: In class syntax, React doesn‚Äôt bind all methods automatically. So, we are binding them in our constructor. Before: After: defaultProps , propTypes and contextTypes must be defined outside the class body. Here are some examples: Before: After: The same goes for propTypes : Before: After: Code written in ES6 with JSX can be pretty clean.\nI‚Äôve used ESLint which helped me to keep syntax clean and free of errors. Here you can find a Blogpost how correctly configure it for your editor. If you are interested in learning how to use React with Rails, with the ES6 syntax, we are working on a new version of Rails meets React.js . It will be free for everyone who bought previous CoffeeScript version. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-06-09"},
{"website": "Arkency", "title": "Is Rails a good fit for writing rich frontend applications?", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2016/06/is-rails-a-good-fit-for-writing-rich-frontend-applications/", "abstract": "This is a question I‚Äôve asked myself months ago, working as a consultant for a client with a quite unique app. Does it make sense to stick with Rails? Maybe it‚Äôs a good time to drop it? Ruby on Rails worked well in the app for quite a while. It was enterprise system for a rather interesting clients: They were accustomed to desktop-like experience of their tools. They were using classic spreadsheets software like Microsoft Excel back then. This was the first project I‚Äôve worked on where the particular way of how user interface works was the real business requirement. When I started working on that project, we were far away from the ideal user experience. In fact, it was a simple Rails application, with HTML views being rendered on the backend with some data tables. I (and, of course, the rest of my team) needed to find a way to implement new features, maintain the codebase and avoid regressions and find a way to achieve this desktop-like behavior of an app. Not to mention the domain was absolutely crazy in terms of complexity and we were having a client which were taught to talk CRUD. It was quite tough to cope with this challenge! One of our first decisions when working with this app was to stop serving HTML from the backend and build UI on the client side with JavaScript. We knew that if we‚Äôd ever want to achieve our desktop experience, it was a way to go. We‚Äôve got quite a nice expertise from working in previous projects (for me it was  writing highly interactive games working in the browser) so we were quite comfortable with this process. So we‚Äôve started working, writing features and replacing static HTML views + jQuery with apps (in a hexagonal sense). It was actually quite challenging since the app was big and we were struggling with our tooling. We‚Äôve discovered React this way and it‚Äôs such a great fit in our toolbox that we‚Äôre specializing in it now ;). But there was still a major problem. While we were experienced both with JS and Rails, we were still creating our Rails backend in a way that was slowing us down. This was the time when I was struggling with answering ‚Äúyes‚Äù to the question on the subject. The problem is, Rails is great in what it does - which is writing apps with views served by the backend. It, of course, has great tools to write code which only serves data which can be consumed by JS. But still, there were bigger problems which came with convention. With sequential IDs generated by default in Rails we were tightly bound to our backend - every action which will be eventually saved needed to be consulted with backend right away. It was a problem since it was introducing problems like slowness of feedback for an user - every saved resource, most clicks on buttons was calling backend to make the result. Of course, we‚Äôd be able to work without backend - but the resulting code was messy and sprinkled with conditionals about things being persisted or not. We‚Äôve found the solution to this problem by introducing UUIDs. This way we were able to do much more on the frontend without consulting our results with the backend all the time. What‚Äôs more, by using UUIDs and passing them in our POST/PUT/DELETE calls we were able to simplify our Rails backend even more, by not reading responses after every change. Without being bound to the backend, we were a little step closer to work in a very similar way desktop app works ;). There were many different fundamental problems in Rails conventions we needed to solve to achieve desktop-like experience (you should aim for it too in your apps!) and boost productivity while working on our app.  UUID was the first fundamental change. We also needed to get rid of Sprockets limitations (to have a proper modularization of our JS code), include real-time updates to our JS front-end and much more. The result was spectacular. After months of hard work (it was a big, legacy app) the client came back to us saying that potential buyers were shocked about how good the experience is. This was a moment of a real joy to our team - we succeeded with the task. I wish you the same feelings we had back then. During these months, and to this date I was collecting such gems of knowledge which needed a lot of groundwork to research and implement correctly. Those techniques I‚Äôve learned helped me to write dynamic, rich code better and find the answer to the question of Rails being a good fit. And my answer is: Yes. But you need to be aware of what you need to change to make it work in an optimal way . I‚Äôve hand-picked the most useful techniques I‚Äôve learned and decided to write a book about it. Those are techniques used today in every app we‚Äôre writing and needs a dynamic user interface. Some of them are coming from my colleagues in Arkency (so, unfortunately, I‚Äôm not that smart :() and I‚Äôve decided to describe them in this book. It is called Frontend-friendly Rails - because this book is all about connecting frontend and Rails backend in the most optimal way. By implementing those techniques your Rails backend can be friendly and not stand in a way when you‚Äôre implementing your rich JS UI. The book is (almost, some editorial work needed) already finished, so I got a list of techniques described in the book. Switch your Rails application to frontend-generated UUIDs. This process comes with many benefits, allowing your frontend to be free in terms of consulting with the backend. It allows you to write certain advanced backend communication techniques (like auto-save, for example) in a way easier way than with sequential IDs. But there are certain technological changes: For example how to do it in an unobtrusive way, or how to ensure UUID will be generated if not provided by the frontend. I describe the whole process of transforming your model to be UUID-based, without losing data and in a database-agnostic way. Setup the Cross-Origin Resource Sharing. This is a challenge often overlooked when the team decides to go to the higher level with their frontend - which is separating it completely from Rails. I describe how to setup it - and why we‚Äôre having this problem in the first place. Prepare JSON API endpoints for your API. A well-thought format of API responses can make your API easier to use by your frontend and third parties, not to mention extendability. JSON API is a great standard of responses, so you do not need to reinvent the wheel. I describe the process of introducing JSON API-compliant responses to your controllers in a way that is not touching your old API endpoints (if you have any) at all. This makes it a way safer way for legacy applications. Create a living API - beyond request-response cycle. This technique is all about making your frontend even more interactive - by introducing live updates so you can see what other users are editing in your application in real-time. I describe how to do it using Pusher library since I have the biggest experience with this tool - but the mechanism is the same with any other tool. I also describe how to make such mechanism non-distinguishable from the current user actions, resulting in a way easier maintenance of such code. Consequences of frontend decisions. This is not a technique, but more like a philosophical essay ;). I deconstruct the thinking behind two basic flows of how frontend app may work - bound to the backend or not bound to it - code snippets included. Knowing those differences allowed me to upgrade my thinking about JS frontends to the higher level. Using modern Javascript with Rails. Quick guide to tooling in JavaScript. Constructing the Node.js assets pipeline - step by step. Those three chapters are in fact connected together. The biggest boost of productivity that you can have while writing your frontend app is switching to modern JavaScript tooling and standards. Using tools like Webpack to achieve modularization of your code, or Babel to use the newest flavor of JavaScript are bringing spectacular productivity gains. In this part I show why it is beneficial to you, what new tools you‚Äôll be using and what they do and how to build the whole JS assets pipeline which replaces Sprockets - step by step. By finishing this chapter and following steps you‚Äôll have working assets stack with production-ready builds, ES2015 and CoffeeScript support and a complete testing stack. I find this knowledge extremely helpful and most of our projects have this knowledge applied. I‚Äôve also included a handful of blogposts we‚Äôve written about making Rails more friendly while working with rich frontends in a form of bonus chapters. I‚Äôm very happy about contents of this book and had tons of fun writing it - and I‚Äôm sure it‚Äôll make you a better frontend developer. (this announcement was initially sent to the Arkency mailing list - the place where all our announcements go first. If you‚Äôd like to know early about what we‚Äôre cooking at Arkency, please subscribe ) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-06-15"},
{"website": "Arkency", "title": "Mutation testing of Rails views", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/06/mutation-testing-of-rails-views/", "abstract": "Thanks to mutation testing we can get much higher confidence while working with Ruby/Rails apps. There is one area, though, where I‚Äôve been struggling to make mutant to work - the Rails views. People use erb or haml and they‚Äôre both not a proper Ruby code, they‚Äôre templating languages. In this post, I‚Äôm showing a trick which can help make Rails views covered by mutation testing coverage. It was 7 or 8 years ago, when I was using another approach to Rails views, called Erector. We wrote a whole project using it. It had some issues, but overall I loved the idea that I can have views written in Ruby, given how elegant our language is. Since then, I haven‚Äôt used Erector much, but today I‚Äôve tried to use it, just to learn that there‚Äôs another library which sounds like a better version of Erector - it‚Äôs called fortitude . I have created a simple Rails 5 app and then created a simple Capybara test which will be used by mutant: with the following routes: and the controller: Then I created a fortitude view: Now when I run mutant with: I get a nice mutation coverage report: and the mutated code: This means, that the view did get covered by mutant and it was mutated to see what‚Äôs the coverage.\nWith this example, it showed me, that I have no test requiring that it‚Äôs a <p> tag. I‚Äôm not sure if that‚Äôs really useful, but at least this technique can be applied in situations where we need to take care of Rails views as well :) Obviously, if you want to apply it to existing Rails views, they need to be converted to Fortitude first, which may not be the best choise for every project‚Ä¶ You can struggle with Rails views or ‚Ä¶ you can make your Rails more frontend-friendly and go with JavaScript-based applications. That‚Äôs one of our favourite ways at Arkency in the last years. Marcin has just released a new book describing the techniques we‚Äôve been using. Our new book is called ‚ÄúFrontend friendly Rails‚Äù and during this week (until Friday night) it‚Äôs on a discounted price 40% off with the code FF_RAILS_BLOG . Click here to buy the book! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-06-21"},
{"website": "Arkency", "title": "Use ES2015 with Rails today - what & how", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2016/06/use-es2015-with-rails-today-what-and-how/", "abstract": "This is a content which was previously published as a mailing campaign in the Arkency Newsletter . This promotes our new book - Frontend Friendly Rails which is available on sale now. Use FF_RAILS_BLOG to get 40% discount on the book, which you can buy here . ECMAScript 2015 is the new standard of JavaScript, popularised mostly by communities around React.js view library. While Angular people chose TypeScript for their language of choice and Ember is (mostly) unopinionated about it, React people tend to use ES2015 extensively. ECMAScript 2015 is the new standard of JavaScript. Just like Ruby has version 2 or 1.8, ECMAScript 2015 is the new version of the language. It‚Äôll be supported by every browser soon. Right now it‚Äôs not, but you can still use it today thanks to so-called transpilers or source-to-source compilers that understand new syntax and transforms it into the old standard of JS. Since it‚Äôs hard to chase all JavaScript novelties if you don‚Äôt sit in it, it‚Äôs understandable that ES2015 can be a new thing to you. I‚Äôd like to present you what you can gain by using it today and what‚Äôs the best way to do it with Rails. ES2015 is an evolution, not revolution - it adds features to the language. Old JavaScript code is automatically ES2015 code - no changes needed. Just like Ruby 1.9 code is automatically Ruby 2.0 code. One of the most common struggles people have with JavaScript is its ‚Äòunfamiliarity‚Äô. JavaScript is object-oriented, but in a different way than most language - it‚Äôs object model is based on prototypes , not classes . This is nor simpler nor more complex model of object orientation - just different. In fact, it has the same capabilities that classical ‚Äòclass‚Äô object model has. To aid developers migrating from languages like Ruby, C# or C++, ECMAScript 2015 provides a support of classes you already know and love. Compare: To: Much familiar syntax is a great addition to the language. You can stick with it if you want, or learn the underlying prototype model later - it usually pays off and makes some ‚Äòweird‚Äô edges of JavaScript much more understandable (like the concept of context in functions). Other problem people tend to had with JavaScript is a visibility of variables and the concept of hoisting . Old variables in JavaScript has the function scope - that means whenever you‚Äôll define them they‚Äôll be bound to the scope of a function. Consider this: In most languages console.log(f) would throw an error since f is undefined. But since JS variables tend to be scoped in a function scope and there is a concept of hoisting , the function behaves more like this function: This breaks familiarity with other languages you know. It is because most languages use so-called block scope - so variable is visible in the block it is defined and nowhere else. ES2015 fixes this by introducing a new type of variables which are block scoped - say hello to let and const : The difference between let and const is that if you define const , you can‚Äôt change it later (because it is constant ): Such additions to the language are making JavaScript more friendly and familiar to developers coming from Ruby and other languages. This is a great thing because, well, no matter you like it or not, we all end up writing JS eventually‚Ä¶ Aren‚Äôt we? :) JavaScript is burdened by its past - and certain unhappy decisions made that you must live with. One of the most annoying is the concept of default context. If you forget to use var , let or const in an assignment to the variable, you‚Äôll define a global variable: That‚Äôs unexpected and it sucks . It allows you to create global variables by an accident - or shadow existing global functions with accidental values. Ouch! There is a concept of strict mode in JavaScript. It breaks backwards compatibility in favor of providing better defaults - like fixing this default context issue. Fortunately, since most people tend to forget to switch strict mode on, certain ES2015 features like classes or modules enable it by default. Also tooling behind transpiration today are producing ‚ÄúES2015 modules‚Äù by default so strict mode is enabled for free - you don‚Äôt need to remember about adding it by yourself. Next thing, coming from the way how JavaScript works under the hood is the idea of context binding to functions. If you write  a class in Ruby, no matter how you call the method, the context ( self or @ ) will be the object from which you called this method. This is not the case in JavaScript. Due to its prototypical nature, they decided to compute the context when a function is called. This is not necessarily a bug (for me it‚Äôs a feature), but it is very surprising to many: In this case calling fn set the context to default one - so window . Since window.x is undefined and the number is added here, the result is NaN . Specifying context works different than in most languages that are using lexical binding of context. This is also the reason of the pattern you may often see in jQuery code: This that pattern is because context is not lexical scoped. This is powerful concept, really, but often you just want to refer to the lexical context, no matter what. Fortunately, ES2015 provides lexical-scoped functions, being also a very handy shorthand for defining functions in place - the feature is called ‚Äòarrow functions‚Äô: Not only it‚Äôs more concise than function syntax (which comes in handy if you don‚Äôt care about the context at all), but also provides a nice feature of having a lexical context. This has tremendous effect on typical frontend code that is being written - making it easier to read, more concise and less surprising. Arrow functions are one thing that is making writing typical code in JavaScript less tedious. But there are more features that are making writing code more pleasant. First feature that was lacking for a long time is string interpolation . ES2015 provides it by wrapping your string content with backticks: Unpacking objects and arrays is so common operation that ES2015 provides a special syntax for it called destructuring . Just see it in action to see how useful it is: Looking innocent, this saves you a lot of tedious writing. There is also a change to defining functions. You can supply default arguments and use spread operator to work with variadic functions: The last addition that is extremely useful is enhanced object notation . It‚Äôs better to see it by an example: There are many more. Those are only that I‚Äôm using in my day-to-day work, making my work more pleasant. There are generators, iterators, new for syntax and so on. That being said, there is a lot of sugar added to JavaScript - a sugar which makes you way more productive once you master it. Last, but not least. Before ES2015 JavaScript had no syntax for building modules. There were technologies and standards that allowed your code to be modular (CommonJS, RequireJS‚Ä¶). But with ES2015 modules became first-class citizens, having its own syntax. It allows your code to hide implicit details of implementation, relying only on the public API. Not to mention it makes your dependency control way easier: You can use destructuring, make default or name imports: This allows you to structure your code way better and with concise syntax. Previous solutions like using IIFE was quite verbose - here you have nice syntax and (soon) native support for modularization of your code. As you can see, ES2015 brings much into the table. Not only it‚Äôs more convenient to use, but also comes with many great opportunities (better stdlib, modularization, TCO after being implemented by browsers natively) for today and future. Unfortunately, having all of it with Rails is not that super-easy. First of all, Sprockets will support ES2015 starting from version 4. If you have a new version of Rails, you‚Äôre probably using version 3, which only has an experimental support. Even with this support, load system of Sprockets kinda doubles your work if you want modularise your code. What‚Äôs more, this technology is developing in a very rapid pace, so tooling is constantly evolving and the best of tools are available on Node.js-based stacks. Does it mean we‚Äôre doomed and we can‚Äôt use this stuff? Of course we can. You can use experimental support in Sprockets if you want or use many gems that are trying to add the support for it. But the most robust solution for me is to add a separate, Webpack-based stack. There is SurviveJS book about it (grab it - it‚Äôs cool!), which is also teaching React.js which is quite cool technology. There are also many articles in the web. In fact, for me it‚Äôs so important I‚Äôve made a big chapter about it in my ‚ÄúFrontend-friendly Rails‚Äù book. It seems people are struggling with it, so I‚Äôve decided to make a step-by-step process of constructing the whole stack from zero to a complete solution. And you can buy it now. Download the free chapter Finally I managed to write this book :). It‚Äôs a set of good practices and techniques I‚Äôve worked during my work on couple of projects I‚Äôve worked so far. This book is about making Rails more friendly to your frontend, making it easier and faster to write, as well as more powerful and maintainable. I had tons of fun writing it and I‚Äôm using those techniques in my day to day work. I hope you‚Äôll find it useful too in your projects - hours of development in Arkency proved me those are battle-tested solutions to real problems. It‚Äôs the beta version of the book. If you buy it now, you‚Äôll get all updates for free. Just enter FF_RAILS_BLOG as a coupon code to get 40% discount for this book . The original price is $49 and you‚Äôll get it for less than $30! Book has 97 pages of exclusive content now + bonus chapters, so it‚Äôs 154 pages in total. If you don‚Äôt like it, we have an eternal no-questions-asked refund policy - just drop us an e-mail and you‚Äôll get refunded. Click here to buy the book! Download the free chapter The following topics are covered in the book: I‚Äôm available to you for all questions about this book. Just drop me a comment I‚Äôll try to clarify everything you may need to make a decision whether this book is a good choice for you or not. Techniques I‚Äôve described in the book made my work better and allowed me to write better Rails API applications. I hope you‚Äôll find the book as useful as I‚Äôm finding those techniques. Click here to buy the book! Download the free chapter Is listening a your kind of consuming content? You can grab a 30-minute podcast where we discuss what you can find inside the book and what were our motives to write it: You can also download it in the mp3 format and see the shownotes here . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-06-15"},
{"website": "Arkency", "title": "Rails Refactoring Podcast #6 - Frontend Friendly Rails", "author": ["Wiktor Mociun"], "link": "https://blog.arkency.com/2016/06/rails-refactoring-podcast-6-frontend-friendly-rails/", "abstract": "In this episode of Rails Refactoring Podcast, Wiktor and Marcin are discussing\nnew Frontend-Friendly Rails book. It tells about improving frontend\ninfrastructure provided by Rails. We are covering few topics from the book: Download MP3 file . You can subscribe to Rails Refactoring Podcast on RSS or on iTunes . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-06-24"},
{"website": "Arkency", "title": "API of the future", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2016/06/api-of-the-future/", "abstract": "The ‚ÄúFrontend-friendly Rails‚Äù book is now live! Here is what Ryan Platte (one of the readers) wrote after reading the book: I‚Äôm very experienced with Rails, and I‚Äôve built production apps in React. But faced with starting a new Rails+React integration, I didn‚Äôt look forward to arguing with Sprockets or undoing other Rails ‚Äúopinions‚Äù. Frontend Ready Rails pointed me to a clean setup with easy-to-follow steps to do it right the first time. And every step is explained thoroughly so I understand the reasoning behind each part of the advice. With this book, I basically got an experienced pair to step through this setup with me. I recommend it to anyone who wants to integrate React into their Rails app the right way. Click here to buy the book! Use the `FF_RAILS_BLOG` coupon to get 40% off! API is a constantly evolving topic. Today, the most of APIs we‚Äôre all using are REST, JSON-based APIs. That was not the case a few years ago, when XML was a king in terms of response formats and techniques like XML RPC was quite popular. But is the current state of API is here to stay? Do REST-based APIs scale well? Do they provide an optimal experience for integrated clients? I don‚Äôt know the answer to this question. What I know is that there are companies that are challenging this idea - and you can  try out their solutions today. Maybe those solutions will be a better fit for your project? Relatively recently open sourced Falcor is a technology Netflix uses to build their backend solutions. Results for them are spectacular - they claim that they were able to remove 90% of their networking backend code by using it. It‚Äôs a pretty impressive result! As every approach in this list, Falcor is using an underlying language for its inner workings - in case of Falcor it‚Äôs the most non-intrusive choice which is JSONGraph . The advantage of this choice is that you don‚Äôt need to incorporate another technology - JSONGraph is just JSON. The aim of Falcor is to provide the same experience regardless of the technical means needed to fetch the data. As quoted in the main page: You code the same way no matter where the data is, whether in memory on the client or over the network on the server. Falcor works different than a typical REST API - while in REST you have a resource per endpoint, in Falcor you have just one endpoint. This is what Andrzej was advocating too in a slightly different context.  The main reason for that is to reduce network latency inherent to an every request - with Falcor the client can hit the endpoint just once due to flexibility it provides. Falcor was an internal Netflix solution and it‚Äôs blatantly visible - it provides the biggest gains if you happen to host an infrastructure with many small services being the separate apps - just like Netflix does. What‚Äôs more, you get many application-level features that you would need to implement by yourself in case of the REST API, like caching, batching requests or request deducing. I find Falcor very interesting in topologies with many small apps - because it provides a way to provide a coherent way of fetch data regardless of its source. Unfortunately, you can‚Äôt use Falcor together with Rails - you need to build your Falcor solution on top of Rails, instead on integrating it together within the same app. Before Netflix released Falcor, Facebook came up and open sourced their own technology for building APIs. What‚Äôs more, it‚Äôs not only a technology, but an architecture/framework as well. It‚Äôs called Relay , it is intended to be used with React.js and it allows you to fetch your data in a more controlled way than a typical REST API connection looks like. In a typical REST API application, your client can only assume the structure of the data - of course you can make your API configurable, so the client can ask only for a specific fields from the response and so on. But still, client is just issues the AJAX request, hoping that what comes back is the response format it expects. Relay takes a different approach. There is a concept of schema in your backend which you define. A client can issue a query , defining what expects from the server to be returned. A server validates whether the query is valid (so it can be processed) and returns the response expected by the client. The main difference is that now client not assumes how response should look like, but defines it by itself. This way you can avoid variety of very hard to test situations like breaking API changes not reflected on the client, and so on. The language used to describe your queries is a custom Facebook solution and is called GraphQL . It works very well for queries which can be easily shaped as a graph. And (surprise!) Facebook has the ideal data for it - relationships are natural because those are people relationships. As Falcor, since it is a solution tailored for applications with an extreme performance needs you have many performance improvements, like built-in caching. Unfortunately, while performant, it also introduces a lot of code if you‚Äôd like to mutate your data through it. There is a concept of mutations which encapsulates the logic of mutating your data, as well as invalidating all necessary caches - and you need to write it by yourself. Ouch! It also provides very nifty features like optimistic updates (your view gets updated instantly, and if error on the server occurs it gets rolled back), retrying failed requests, queuing mutations‚Ä¶ it‚Äôs really sophisticated. I‚Äôd recommend this solution if you happen to have data suitable to be represented as a graph - so rather small ‚Äòdata nodes‚Äô with many relationships between them. Relay has a rather steep learning curve and entry cost - but it pays off in terms maintainability, performance and elasticity it provides. What‚Äôs fortunate, you can use GraphQL and Relay together in Ruby - there are libraries like graphql-ruby and graphql-relay-ruby that can help you with building a solution, using Rails. There are many other new approaches to build an API ( Flux over the wire , the big come back of Datalog thanks to Clojure/Datomic ‚Ä¶), but what is very clear from most of them is that those solutions are relying heavily on the cornerstone of the computer science - which is a graph. Graph is a very simple structure, consisting of nodes and edges . Under the hood, a RDBMS like MySQL and PostgreSQL can be represented by a graph, too. There are graph databases which are optimised to store and query data that way. It‚Äôs nothing new. But right now topologies of our systems, as well as processing power enables us to revise this idea again. I‚Äôm looking forward to more ideas like this - it‚Äôs always beneficial to provide new, interesting solutions. The wind of change is also visible because majority of those solutions are based on just a single API endpoint. It‚Äôs cool because you can just add it as a separate endpoint and continue serving your old REST API without major problems. Are they better than plain old REST APIs? Of course it‚Äôs complicated. They work wonders for both Facebook and Netflix - but it is because those are developed with their needs in mind. What is the best choice for your applications? There are rules of thumb, but the definite answer is unknown for your project. You know the best! Download the free chapter I‚Äôd recommend going for those techniques if you happen to be quite happy with your current REST API solution and want to upgrade your experience. Unfortunately, I find people struggling with creating robust APIs in Rails tailored for their rich UI needs. No wonder Rails has some to do with this - it is a framework designed for request-response cycle applications with HTML views served by the backend. To use it better, you may need to upgrade Rails defaults to something better. In our new Frontend-friendly Rails book we describe the process of such upgrade. You can learn from it a set of independent techniques that are beneficial for API-based Rails projects, without resorting to new technologies like Grape, Rails-API or Swagger. You can build great APIs using just standard Rails - something that I want to emphasize. Those techniques serves me well to this day. I implement them in my projects and they‚Äôve got a status of being battle-tested - they served thousands of users and made my code just more maintainable and allowed my frontend to blossom. Not to mention you‚Äôll see how easy you can improve your backend-frontend communication even more by adding real-time to it or knowing and distinguishing patterns you can use to write integrations around them. It‚Äôs all about making Rails more friendly for your sophisticated frontend application written in JavaScript (knowledge applies to mobile clients too). The book consists of 99 pages of an exclusive content. With bonus chapters (which are hand-picked selection of blogposts we‚Äôve written during years about the topic) it‚Äôs 154 pages. The book is a set of techniques - you can treat chapters as complete solutions or summaries of a particular topic/technique. There are also benefits and all necessary theory explained, as well as step-by-step descriptions of a process - perfect for convincing your boss/teammates to improve your codebase. From this book you‚Äôll learn: Click here to buy the book! Use the `FF_RAILS_BLOG` coupon to get 40% off! I found a ton of value in applying these techniques in projects I‚Äôve been working on. I hope this book will be as handy for you as those techniques are for me - to this day! Graph-based solutions are great opportunities to think about redesigning your API. While powerful and backed by big companies, you still need to consider they‚Äôll be a good fit for your data and your kind of topology. Netflix‚Äô Falcor works best in micro services world where Netflix sits. Facebook‚Äô Relay works best in graph-based scenarios where fetching data related in a complicated way is a common use case. Choose your technology wisely and think above REST API! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-06-24"},
{"website": "Arkency", "title": "Domain Events Schema Definitions", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/07/domain-events-schema-definitions/", "abstract": "When we started three years ago publishing Domain Events\nin our applications, we were newbies in\nthe DDD world. I consider the experiment to be\nvery successful but some lessons that had to\nbe learned the hard way. At the very beginning, we were just publishing events.\nWe didn‚Äôt think much about consuming them.\nWe haven‚Äôt yet considered them a very powerful\nmechanism for communication between the application\nsubsystems (called Bounded Contexts in DDD world).\nAnd we didn‚Äôt think much about how those events\nwould evolve in the future. Nowadays, one of our events has 18 handlers. And I believe\nthis number will continue growing. We also started using domain events in many smaller\ntest i.e. tests for one class or one sub-system. So at some point in time, it became necessary that what\nwe publish in code, what we expect in tests and what\nwe use to set up a state in tests has the same interface.\nAll those events should contain the same attribute names\nand the same types of values in them. For that I used classy_hash gem which raises\nuseful exceptions when things don‚Äôt match. I tried an approach in which the event schema is validated\nin a constructor phase (new/initialize) but later decided against\nit. In a few very rare cases we might be OK with an event\nwhich is not completely full (not all attributes are present).\nWhen we get historical events\nfrom event store we don‚Äôt want (or need) to verify the schema as well. So instead when you want to verify the schema (in 97% of cases)\nyou should just use the strict method to create the event\ninstead of new . classy_hash supports nullable keys (value is nil), optional keys (value not present),\nmultiple choices, regular expressions, ranges, lambda validations, nested arrays and hashes. I know some people who use dry-types for defining events‚Äô schema\nand they were happy with that library as well. With 220 domain events that we already publish, with every new\nthat I add, I remember to define its schema. That way it‚Äôs much\neasier for every other team member to know what they can expect\nin those events just by looking at their definition. Check out our rails_event_store gem if you want to start publishing domain events as well. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-07-10"},
{"website": "Arkency", "title": "Cover all test cases with #permutation", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/06/cover-all-test-cases-with-permutation/", "abstract": "When dealing with system which cooperate with many other\nsubsystems in an asynchronous way, you are presented with\na challenge. Due to the nature of such systems, messages\nmay not arrive always in the same order. How do you test\nthat your code will react in the same way in all cases? Let me present what I used to be doing and how I changed my\napproach. The example will be based on a saga but it\napplies to any solution that you want to test for\norder independence. This solution however has major drawbacks It will eventually test all possibilites. Given enough runs on CI.\nAnd you can reproduce it if you pass the --seed attribute. But generally it does not make our job easier. And it might miss some bugs\nuntil it is executed enough times. It was rightfully questioned by Pawe≈Ç, my coworker. We can do better. We should strive to test all possible cases. It‚Äôs boring to go manually through all 6 of them.\nWith even more possible inputs the number goes high very quickly. And it might be error prone.\nSo let‚Äôs generate all of them with the little help of #permutation method. If you enjoyed this blog post you will like our books as well. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-06-25"},
{"website": "Arkency", "title": "Always present association", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/07/always-present-association/", "abstract": "Recently my colleague showed my a little trick\nthat I found to be very useful in some situations.\nIt‚Äôs nothing fancy or mind-blowing or unusual\nin terms of using Ruby. It‚Äôs just applied in a way\nthat I haven‚Äôt seen before. It kind of even seems\nobvious after seeing it :) Now you can just do: without wondering if order.meta_data is nil because\nif this associated record was never saved then build_meta_data will create a new one for you. Same goes with reading such attributes. You can get nil but you won‚Äôt get NoMethodError from calling ip_address on an empty association ( nil ). It has some downsides, however. Reading (event an empty) ip_address can trigger a side-effect in saving the meta_data . MetaData can not have non-null columns unless you set all of them\nat the same time. Otherwise, when ip_address can be null but user_agent cannot, setting only\none of them will cause troubles. The same problem can occur with validations on MetaData . But if you don‚Äôt have such situations in your code and just have\nmultiple attributes that are either optional or all set at the\nsame time, then why not. Check out more patterns that can help you in maintaining Rails apps in our Fearless Refactoring: Rails controllers ebook Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-07-12"},
{"website": "Arkency", "title": "Implementing & Testing SOAP API clients in Ruby", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/07/testing-soap-api-adapters/", "abstract": "The bigger your application, the more likely you will need\nto integrate with less common APIs. This time, we are going\nto discuss testing communication with SOAP based services.\nIt‚Äôs no big deal. Still better\nthan gzipped XMLs over SFTP (I will leave that story to\nanother time). I am always a bit anxious when I hear SOAP API. It sounds\nso enterprisey and intimidating. But it doesn‚Äôt need to be.\nAlso, I usually prematurely worry that Ruby tooling won‚Äôt\nbe good enough to handle all those XMLs. Perhaps this is\nbecause of some of my memories of terrible SOAP APIs that\nI needed to integrate with when I was working as a .NET\ndeveloper. But SOAP is not inherently evil. In fact, it has\nsome good sides as well. We are going to use savon gem for the implementation\nand webmock to help us with testing. The plan is to\nimplement a capture functionality for a payment gateway.\nIt means that goods were already shipped or delivered to\nthe customer and the reserved amount can be paid to the\nmerchant. Let‚Äôs see the implementation first and go through it. The example is not long but sufficient enough\nto discuss a few aspects. There is a static configuration that we don‚Äôt need\nto bother ourselves with right now. It contains API URLs\nand API keys. In Rails app they usually differ per\nenvironment. Development and staging are using the pre-production\nenvironment of the API provider. Our production env \nis using API production host. In tests, I usually use\npre-production config for safety as well. But thanks to webmock\nwe should never reach this host anyway. We use Savon gem to communicate with the API. I explicitly\nconfigure it to use TLS instead of the obsolete SSL protocol\nfor safety. Depending on your preferences you might set\nit to log the full communication and to which file. I find\nit very useful to have full dump during the exploratory phase.\nWhen I just play with the API in development to see how it\nbehaves and what it responds. Having full output of the XML\nfrom requests and responses can be a lifesaver when debugging\nand comparing with documentation. The most important part of the initialization is: It tells Savon where to find WSDL - an XML file\nfor describing network services as a set of\nendpoints operating on messages . It can be used to descripe messages/types: This is for example what we need to send: and this is what we will receive: What is a GoodsShippedStatus ? So as you can see the whole API is defined based on primitives\nwhich build more complex types which can be parts of even more\ncomplex types. The best thing about using SOAP APIs with WSDL is that the client\ncan parse such API definition and dynamically or statically define\nall the methods and conversions required to interact with the API. Also, even when the API documentation written by humans is incorrect,\nyou can peek into the WSDL to see what‚Äôs actually going on there.\nIt helped me a lot a few times. In next part, we build a Hash with keys matching the names\nfrom the WSDL definition of the type. The signature is a cryptographic digest of all the other\nvalues based on a secret that only me and the payment gateway\nshould know. That way the gateway can check the integrity of\nthe message and that it is coming from me and not somebody else.\nSo it plays a role of authentication token as well.\nI extracted the implementation into HashGuard class which\nis not interesting for us today. Finally, we call goods_shipped API endpoint which is also\ndefined in the WSDL so Savon knows how to reach it and\nhow to build the XML with the data that we provide. The result of the API call is also automatically converted for us\nfrom XML to Ruby primitives such as numbers, strings, arrays\nand hashes. So we can extract the interesting part and\nsee if everything worked correctly. I am going to test this code based on the underlying\nnetworking communication protocol. In other words,\nwe will stub the HTTP requests with the XML being sent. This is on purpose. I want to be able to switch to\ndifferent gem or a library provided by the payment\ngateway authors without the need to change the\ntests. If I just stubbed Ruby method calls, I would not have\nthe ability to change the implementation without\nchanging tests. I would be just typo-testing the\nimplementation. That way I check if we send proper\ndata over the wire and how we react to response\ndata. It does not matter if I use Savon or handcraft\nthose XMLs and URLs myself. First, we stub getting the WSDL . I downloaded it\nmyself and saved under spec/fixtures/pg.wsdl.xml .\nThey are usually quite a long files, so I prefer to\nkeep their content outside of the specification.\nIt remains the same and does not depend on any\nparameters that we could pass so it does not bring\nanything valuable to the spec. Then we stub the GoodsShipped request that we issue.\nIt contains the static data coming from the configuration\nand the provided order_id . I have taken the XML\nstructure of the file from savon logs while playing\nwith the API. Sometimes you have the correct \nXML structure provided as part of the API documentation. Notice the body: <<-XML.split(\"\\n\").map(&:strip).join part. The XML generated by savon is not pretty formatted.\nI like my XMLs in tests to be human readable. So I use\nthis little trick to compact my XML into the same format\nas savon will generate. It has no indentation. We also stub the response. In this test, we are checking\nthe successful path. So the status is ‚ÄúOk‚Äù. In such\ncase, our adapter should return the transaction_id from the response. That would be 8c2ee655b5114 . If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-07-18"},
{"website": "Arkency", "title": "The quotes from the Post Rails Book Bundle books", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/07/the-quotes-from-the-post-rails-book-bundle-books/", "abstract": "Given that I now looked at all the Post Rails Book Bundle (Psst, the offer ends on Friday!) books, I‚Äôve decided to pick some quotes which may shed more light into the whole message. This may help you understand the different points of views reflected by each author. \nSome of the authors are very opinionated and use a strong language, whiles others seem to quite positive towards Rails, but show their ways of extending The Rails Way. What the books share is the view that The Rails Way may not be enough. They also share the constructive approach - they all present some alternatives.\nHowever, the books differ in ‚Äúwhat‚Äôs next‚Äù. Thanks to the bundle you can have a wide perspective on the possible alternatives. Rails apps are not supposed to be monolithic. If your Rails app is monolithic, it is fucked. Explaining why a conventional Rails architecture fails is simple: There is no architecture.\nThe fatal delusion that three abstraction layers, called ‚ÄúMVC‚Äù, are sufficient for implementing complex applications is failure by design. On an architectural level, everything in Rails is crying for separation and encapsulation. But those cries go unheard, mostly. I believe the puristic Rails Way isn‚Äôt appropriate for projects with a complexity greater than a 5-minute blog. Full stop. I also believe that writing an application with J2E specifications sucks. I don‚Äôt want to be constantly thinking about types and interfaces and builders and how to wire them together. The monolithic design of Rails basically led every application I‚Äôve worked on into a cryptic code hell. Massive models, controllers with 7 levels of indentation for conditionals. Callbacks, observers and filters getting randomly triggered and changing application state where you don‚Äôt want it. You have to go into what‚Äôs fucked up with Rails in order to figure out the difference between where Rails cheats on OOP, but shouldn‚Äôt, and where Rails cheats on OOP, but totally gets away with it. Identifying those differences are crucial if you want to figure out what it is that Rails gets right but OOP theory gets wrong. And that‚Äôs the real question driving this book. One very important example: Rails is not good at telling you what Rails is doing. We‚Äôre going to see several places where Rails misinforms you, in both its code and its documentation, about its own design. The question that matters here is ‚Äúwhere and why does Rails break OOP and get away with it?‚Äù But to get to the answer, we need to have a clear, articulate discussion, which means we have to dispell inaccuracies about Rails which Rails itself has propagated. When you started working with Rails some years ago, it all seemed so easy. You saw the blog-in- ten-minutes video. You reproduced the result. ActiveRecord felt great and everything had its place.\nFast forward two years. Your blog is now a full-blown CMS with a hundred models and controllers. Your team has grown to four developers. Every change to the application is a pain. Your code feels like a house of cards. Let‚Äôs talk about controllers. Nobody loves their controllers. Developers burnt by large Rails applications often blame their pain on ActiveRecord. We feel that a lof of this criticism is misplaced. ActiveRecord can be a highly effective way to implement user- facing models, meaning models that back an interaction with a human user. It‚Äôs rare to find a Rails app with a test suite run below 3 minutes. Even more, it‚Äôs not uncommon to have a build taking 30 minutes. You can‚Äôt be agile this way. We should focus on getting the tests run as quickly as possible. It‚Äôs easy to say, but harder to do. This book introduces techniques, that make it possible. I‚Äôve seen a project, for which a typical build time went from 40 minutes, down to 5 minutes. Still not perfect, but it was a huge productivity improvement. It all started with improving our controllers. What I noticed is that once you start caring about the controllers, you start caring more about the whole codebase. The most popular way of having better controllers is through introducing the layer of service objects.\nService objects are like a gateway drug. In bigger teams, it‚Äôs not always easy to agree on refactoring needs. It‚Äôs best to start with small steps. Service objects are the perfect small step. After that you‚Äôll see further improvements. Ruby on Rails is an awesome tool for crafting web applications from scratch. Built-in solutions allow you to quickly prototype your application. Asset pipeline is a great help with enhancing the user interface of your app with JavaScript sprinkles.\nBut Rails conventions are not that helpful when it comes to creating complex applications living in the user‚Äôs browser. Such applications require a bit different defaults than classical request-response solutions.\nIn this book I‚Äôd like to show you how you can prepare Ruby on Rails to be an awesome foundation for the backend solution of your frontend application. My story with modularity started over two years ago when I was charged with the rewriting of an existing web application into something more configurable and modular. I‚Äôve been working on a few modular applications since then while still creating regular apps when modularity was overkill.\nI wrote this book because I couldn‚Äôt find any documentation when I created my first modular application. I was studying open source apps and even though reading code is awesome to learn something, you don‚Äôt always understand what‚Äôs going on. This is the book I wish I had at that time. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-07-20"},
{"website": "Arkency", "title": "respond_to |format| is useful even without multiple formats", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/07/respond-to-format-is-useful-even-without-multiple-formats/", "abstract": "You might think that if a controller action is only\ncapable of rendering HTML, there is not much reason\nto use respond_to . After all, this is what\nRails scaffold probably taught you. There is, however, one very annoying situation in which\nthis code will lead to an exception: when a silly client\nasks to get your page in XML format. Try for yourself: You will get an exception: The client will get 500 error indicating that the\nproblem was on the server side. This problem will be logged by an\nexception tracker, that you use for your Rails app.\nHowever, there is nothing we can do about the\nfact that someone out there thinks they can get\na random page in our app via XML. We don‚Äôt need\na notification every time that happens. And the\nbigger your website, the more often such random\ncrap happens. But we also don‚Äôt want to ignore those errors completely\nwhen they occur. There could be a situation in\nwhich they can help us catch a real problem i.e.\na refactoring which went wrong. How can we fix the situation? Just add respond_to section indicating which formats we support.\nYou don‚Äôt even need to pass a block to html method call. Alternatively, you can go with respond_to :html .\nBut that itself is not sufficient and requires\nusing it together with respond_with . After such change, the client will get 406 error\nwhen the format is not supported. And missing template exception leading to 500 error code\nwill occur only when you really have a problem with the template\nfor supported MIME types (HTML in our example). P.S. If you haven‚Äôt heard yet, Post-Rails Way Book Bundle is ending soon. With 55% discount you can buy 8 products that will help you a lot. Especially\nif you work with bigger or legacy Rails applications. Enjoy! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-07-21"},
{"website": "Arkency", "title": "Two dimensions of a Rails developer's growth", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/07/two-dimensions-of-a-rails-developers-growth/", "abstract": "Yesterday I was contacted by a developer through our Post Rails Book Bundle intercom. He wasn‚Äôt sure about whether the bundle is right to him. I quickly jumped to a phone call (how old-school!) with him and it turned into a very interesting discussion! One topic that was relevant here is how to grow as a Rails developer . In my opinion, there are two main ‚Äúdimensions‚Äù of growth. Both need to be improved continuously . The first direction is the ‚Äútechnology‚Äù part - learning languages, learning frameworks, libraries, APIs. This is quite clear, as that‚Äôs what expected from us in the daily jobs. We need to write new features to our Rails apps - we need to know Ruby, we need to know the standard libraries of Ruby. We also need to know Rails, not just the scaffolding, but also all the details of associations, validations, controllers, routes, view helpers. The better we know it, the faster we implement the features. This is very helpful in debugging too. If you know the frameworks you‚Äôre using it‚Äôs easy for you to spot the problem. This is often what makes one programmer N times faster than other. Not in the ‚Äúnew features time‚Äù, but the debugging time. However, there is a trap. If you‚Äôre focused too much on the technology dimension, you will follow too much The Framework Way . You will focus on using the specific solutions for the technology. You may apply some tricks which are only known in this community. Heck, you may even add some ‚Äúhacks‚Äù so that it‚Äôs  in the spirit of that technology. Rails Conditional validations anyone?\nController filters with :except anyone?\nActiveRecord + Single Table Inheritance anyone? There‚Äôs another direction, often neglected by developers. It‚Äôs the ‚Äúgeneric‚Äù skills as a programmer: Those are the things that make you be able to talk to programmers from other technologies . Can you talk to a PHP/Symfony programmer?\nCan you talk to a Java/Spring/Akka/Scala/Clojure developer?\nCan you talk to a .NET programmer?\nCan you talk to a JavaScript/React.js developer? The problem with learning in this dimension of your growth is that it seems hard to apply in your current job. It‚Äôs hard to apply in your current Rails project. It‚Äôs nice to talk about DDD, but then you come back to your 1000-line controller with a complex filters algebra ! What a change! On one hand, this direction of growing, helps you long-term. You no longer need to worry what happens if Rails is really dead. You‚Äôll use your generic skills to learn other languages and frameworks. The confidence you get by knowing that you can always switch and it‚Äôs going to be easy is priceless! I like working with Rails, I like working with React.js. But I‚Äôm not worried too much what happens if they disappear. The programming patterns that I‚Äôm using are any-technology-friendly. I can do DDD in any language. I can build more layers in any framework (well, I‚Äôm not sure about Angular ;) ). So, it‚Äôs great to grow as a developer with patterns, architectures and stuff. But how can we apply it in the current projects? How can we use it with Rails? This is where I believe the Post Rails Book Bundle shines! The books and videos are exactly this - a nice combination of growing within a technology but by applying timeless patterns. From the 2 of Giles books, you can learn why Rails is not so OOP and why it matters. From my ‚ÄúFearless Refactoring‚Äù book you can learn how to extract new classes and new layers (forms, adapters, repos, services) even this week in your project ! It‚Äôs all verified techniques. You can watch my Rails and TDD videos to see an almost like science-fiction approach to building a Rails app, with the DDD-infected way of thinking , while being mutation covered ! You can read and apply the Trailblazer techniques by Apotonick, as they come with libraries and gems, making it even easier to apply. They come with similar layers, as described in my book! You can (and should!) apply the modularizing/namespacing techniques from the ‚ÄúGrowing Rails‚Äù book. You can extract gems (yes!) from your code, thanks to the ‚ÄúModular Rails‚Äù book. It‚Äôs all exactly the combination of the two worlds of growing. It‚Äôs all the best from the two worlds! Happy growing as a software developer :) Post Rails Book Bundle Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-07-22"},
{"website": "Arkency", "title": "Phases of refactoring complex Rails apps", "author": ["Marcin Grzywaczewski"], "link": "https://blog.arkency.com/2016/07/phases-of-refactoring-complex-rails-apps/", "abstract": "Refactoring is a on-going process that is necessary in big Rails applications. Without it, you can quickly get into trouble - not only your code quality will suffer, but pieces of your architecture - models, controllers and views will get more and more coupled together. This is not a nice separation you had at the beginning of the project - it can quickly transform into an entangled mess of callbacks, going by relationships through half of the system to do stuff, and horrible things like that. Sure, you can live with that. But every feature you‚Äôll add, every bug fix you‚Äôll make will be harder and harder. You can fight with it. Add some gems, clean up one piece of entangled code. But those are small improvements - they can eventually sum up to a big improvement, but it‚Äôs unlikely. Not to mention time won‚Äôt stop - there is a requirement from the business that new features will get delivered. And they aren‚Äôt aware of problems you can experience in your code. Even if they are, it is often resulting in a loss of trust. Just think about it - would you trust a car mechanic which says something like ‚Äúyou know, this fix of brake system will take more time because I‚Äôve made mess in the engine after fixing it before‚Äù? I would not. I‚Äôd say it‚Äôd be very unprofessional :). And this lose of trust will snowball and get you into trouble - more control, more meetings. Even less things done. You can avoid all those problems by understanding simple (but brutal :() truths. The first truth is that the Rails Way does not scale. It works cool for CRUD apps. It works great at the beginning of the project. It also works well for simple domains. That‚Äôs why you hear that big, scaling projects are using Rails - but in fact it‚Äôs not because Rails Way is scaling. It is because they did a lot of work to make it right, or their business domain is simple enough to be fitted into the ‚ÄòCRUD‚Äô approach. And the second truth - you won‚Äôt get far in terms of results if you refactor your code without a plan. Small improvements are great and they‚Äôre better than doing nothing. But in fact major problems are solved by modeling them away . We call this way ‚Äòthe New Way‚Äô or ‚ÄòPost-Rails‚Äô. You don‚Äôt need to model those problems away by yourself. You have years of experience of software developers & architects to support you. Rails is far away from good OOP principles - and when problems happen, it‚Äôs very wise to resort to them. In Arkency we have a ‚Äòframework‚Äô for escaping from framework ;). There are certain techniques that are powerful and fixes the first fundamental problem of Rails frameworks. We think that is a great refactoring plan you can apply to your project right now - and we want to share it with you. Controllers are tricky in Rails. They break the single responsibility principle in a rather brutal way. They orchestrate your models to do stuff. They take care of HTTP request parameters processing. They set up shared state across your actions. They take the responsibility for orchestrating rendering of views. They choose over many response formats based on the content type & accept headers of your request. We think it‚Äôs fine to give those HTTP responsibilities to a controller, but it‚Äôs very restricting to orchestrate your business logic within it. The first step to make your complex Rails app better is to get rid of this coupling. From variety of reasons - keeping the business logic separated will make you able to just extract supportive pieces into a gem, for example. The second reason is that controllers have the big sin of taking away the power of instantiating your own objects. To regain control, the following pieces needs to be introduced - form & service objects. Form objects are all about taking away the params processing responsibility out of the controller. In form objects you‚Äôre making proper type coercions for your attributes, as well as providing simple validations - like presence, numericality & length validations. In Arkency it‚Äôs usually implemented using Virtus & ActiveModel libraries, but we‚Äôre looking forward to use dry-types instead of Virtus for them. What you do with form objects is just wrapping params object from controller with them: And then use it in your controller: But having only form objects is not very helpful. Form objects are far more useful if combined with the service object pattern. You can read more about form objects (point #3) in this great article - but in Arkency there is no save! method in form objects. While form objects are extracting the input handling responsibility from your controllers, service objects are all about extracting business logic out of them. They don‚Äôt need any  supportive technologies - they are just plain old Ruby objects. Then, in your controller, instead of: You do: Since SubmitArticle is a plain object, you can inject your dependencies - a thing which is very useful for testing. There are many things you can do further with service objects - this is the most simple implementation of such object, still relying on implicit rendering of Rails views. You can read more about this pattern on our blog. Those two patterns can serve you long. With even more complex patterns there are many other techniques, which will get described shortly. Next techniques you can use need a proper understanding of the business domain - a thing you need to learn to crunch properly. But this is only Arkency way, right? In fact, we‚Äôre not the only one inspired by proper design and architectural patterns. We write about them (a lot!), but there are other great developers sharing the same goal with us - make Rails a better fit for complex applications. That‚Äôs why we have organized a book bundle with those authors. Officially we‚Äôve finished it on Friday, but due to many requests, we‚Äôve reopened the Post-Rails-Way Book Bundle today, just for one day! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-07-25"},
{"website": "Arkency", "title": "Command bus in a Rails application", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/09/command-bus-in-a-rails-application/", "abstract": "Using commands is an important part of a DDD/CQRS-influenced architecture. In this blogpost I‚Äôd like to show you how to use the Arkency Command Bus gem within a Rails application. Let‚Äôs first look at what a command may look like: As you see, it‚Äôs just a data structure. We‚Äôve used DryTypes here, but you can use whatever you want, as long as it may help you define the expected params and have some basic ‚Äúvalidations‚Äù. Now, let‚Äôs look how it‚Äôs used from a Rails controller: The execute method is defined in the ApplicationController as it is used from many controllers: So, there‚Äôs a CommandExecutor class which is responsible for dispatching commands. In this case, we declare a dedidacted command handler, called AddCostCodeHandler . What is the CommandHandler class which we inherit from? We use a full-CQRS approach here together with event sourcing and aggregates. Let‚Äôs look at the aggregate here: This means, that we‚Äôre publishing a successful CostCodeAdded event, which can be used in other places of the system. One main place may be a read model - to help us retrieve the data from the system.\n(In CQRS read models serve as the Query part) How are the events then connected? In which, the CompanyCostCode is just a normal ActiveRecord class: And on the read side of the application, it‚Äôs used from a controller, as any other ActiveRecord objects collection. Check out more patterns that can help you in maintaining Rails apps in our Fearless Refactoring: Rails controllers ebook Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-09-15"},
{"website": "Arkency", "title": "How we save money by using DDD and Process Managers in our Rails app", "author": ["Jakub Rozmiarek"], "link": "https://blog.arkency.com/2016/10/how-we-save-money-by-using-ddd-and-process-managers-in-our-rails-app/", "abstract": "It‚Äôs obvious that e-commerce businesses depend on an ability to sell goods or services. If an e-commerce platform is unable to sell, even temporarily, it becomes useless. It suffers from short-term revenue loss, but also it appears as untrustworthy to both it‚Äôs customers and merchants. Unfortunately only the biggest players can afford to become an independent payment service provider. All other platforms have to depend on external providers and these providers have their own problems: infrastructure failures, errors in code, DDOS attacks etc. To ensure the ability to operate a platform needs at least two independent payment providers. There are numerous interesting challenges related to having more than one payment provider, but let‚Äôs focus on one of them only: how to detect that a gateway is failing and be able to use another one instead. One simple yet effective solution is to use a process manager that reacts to a set of business events, maintains its internal state and makes a decision to switch to the backup gateway when some condition is met. Let‚Äôs assume the system publishes these business events: Each of these business events include payment_provider_identifier in their data so the information is available which provider was related to the event. With these types of events it‚Äôs possible to detect if there is a problem with the payment gateway that is currently in use. To achieve this a process manager can be used. It will react to these events and keep track of a number of failures. If the number exceeds a certain threshold a command is issued to switch to a backup provider: Let‚Äôs trace what the process manager does when one of the events it‚Äôs subscribed to happens. After a failure, when PaymentFailed or CreateRemoteTransactionFailed occurs the process manager finds a related State record that keeps a current number of failures for the provider and increases the counter.\nNext it checks for the current state. If the number of failures defined in configuration wasn‚Äôt exceeded the state is not critical so nothing more happens.\nIf the state is critical a command is sent to the command bus that triggers provider switch. A command handler takes care of it and also notifies appropriate people about the fact. Then the counter is reset. When there was a successful payment and PaymentSucceeded was published it just finds the state record and nullifies the counter. To sum up: if too many problems pile up it makes a decision to switch to a backup provider. Some may argue that failed payment isn‚Äôt actually a failure, but experience with payment gateways shows that it‚Äôs a common problem when a provider seem to work normally but all or most of the payments are declined because for example, an underlying acquirer is having issues. Of course, this solution is prone to false-positives. It can happen that a group of customers have their payments declined because lack of funds etc. and no successful payment happens in the meantime. The process manager records it as a problem with payment provider and triggers a switch.\nHowever, in platforms with heavy traffic it‚Äôs very unlikely. And even if it does happen it‚Äôs still better than the inability to sell goods for a period of time before someone notices and manually selects a backup gateway. Make sure to check our books and upcoming Smart Income For Developers Bundle . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-10-11"},
{"website": "Arkency", "title": "Minimal decoupled subsystems in your rails app", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/09/minimal-decoupled-subsystems-of-your-rails-app/", "abstract": "There are multiple ways to implement communication between two separate\nmicroservices in your application. Messaging is often the most recommended\nway of doing this. However, you probably heard that ‚ÄúYou must be this tall to use microservices‚Äù .\nIn other words, your project must be certain size and your organization\nmust be mature (in terms of DevOps, monitoring, etc.) to split your app\ninto separately running and deployed processes. But that doesn‚Äôt mean\nyou can‚Äôt benefit from decoupling individual subsystems in your application\nearlier. Those subsystems in your application are called Bounded Contexts . Now, imagine that you have two bounded contexts in your application: And the use-case that we will be working with is described as: When Season Pass Holder is imported, create an account for her/him and send a welcome\ne-mail with password setting instructions. . The usual way to achieve it would be to wrap everything in a transaction, create a user,\ncreate a season pass and commit it. But we already decided to split our application\ninto two separate parts . And we don‚Äôt want to operate on both of them directly. They\nhave their responsibilities, and we want to keep them decoupled. So here is what we could do instead: It certainly sounds (and is) more complicated compared to our default solution.\nSo we should only apply this tactic where it benefits us more than it costs. But I assume that if you decided to separate some sub-systems of your applications\ninto indepedent, decoupled units, you already weighted the pros and cons . So now,\nwe are talking only about the execution. You might be thinking that there is big infrastructural cost in communicating via domain events.\nThat you need to set up some message bus and think about event serialization.\nBut big chances are things are easier than you expect them to be . You can start small and straightforward and later change to more complex solutions when\nthe need appears. And chances are you already have all the components required\nfor it, but you never thought of them in such a way. Do you use Rails 5 Active Job, or resque or sidekiq or delayed job or any similar tooling,\nfor scheduling background jobs? Good, you can use them as message bus for asynchronous\ncommunication between two parts of your application .\nWith #retry_job you can even think of it as at least one delivery in case of failures. So the parts of your application (sub-systems, bounded-contexts) don‚Äôt need at the\nbeginning to be deployed as separate applications (microservices). They don‚Äôt need\na separate message bus such as RabbitMQ or Apache Kafka. At the start, all\nyou need is a code which assumes asynchronous communication (and also embraces\neventual consistency) and uses the tools that you have at your disposal. Also, you don‚Äôt need any fancy serializer at the beginning such as message pack or protobuf.\nYAML or JSON can be sufficient when you keep communicating asynchronously\nwithin the same codebase (just different part of it). We are going to use rails_event_store ,\nbut you could achieve the same results using any other pub-sub (e.g. wisper +\nwisper-sidekiq extension). rails_event_store has the benefit that your\ndomain events will be saved in a database. Domain event definition. This will be published when Season Pass is imported: Domain event handlers/callbacks. This is how we put messages on our background\nqueue, treating it as a simple message bus. We use Rails 5 API here. Imagine this part of code (somewhere) responsible for\nimporting season passes. It saves the pass and publishes PassImported event. When event_store saves and publishes the Season::PassImported event, it will also be queued\nfor processing by IdentityAndAccess::RegisterSeasonPassHolder background job\n(handler equivalent in DDD world). These are the events that will be published by Identity and Access bounded context: This is how Identity And Access context reacts to the fact\nthat Season Pass was imported. (ps. if you think that we should not use exceptions for control-flow,\nor that exceptions are slow, keep it to yourself. This is not a debate about\nsuch topic. I am pretty sure you can imagine exception-less solution) Reminder how when an event is published we schedule something to happen\nin the other part of the app. Season Pass Bounded Context reacts to either UserImported or UserAlreadyRegistered by saving the reference to user_id . It does not have direct access to User class. It just holds a reference. Now imagine that the needs of Identity And Access grow a lot. We would like to extract it\nas a small application (microservice) and scale separately . Maybe deploy much more instances\nthan the rest of our app needs? Maybe ship it with JRuby instead of MRI.\nMaybe expose it to other applications that will now use it for authentication and managing\nusers as well? Can it be done? Yes. Switch to a serious message bus that can be used between separate apps, and use a proper\nserialization format (not YAML, because YAML is connected to class names and you won‚Äôt have\nidentical class names between two separate apps). Your code already assumes asynchronous communication between Season Passes and Identity&Access\nso you are safe to do so . Make sure to check our books and upcoming Smart Income For Developers Bundle . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-09-30"},
{"website": "Arkency", "title": "To blog or to write code?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/10/to-blog-or-to-write-code/", "abstract": "Four years ago when I started writing first posts on Arkency blog, I didn‚Äôt have many expectations. I knew I wanted to share my knowledge and thoughts. Not much beyond that. My 1st blog post took me 8 hours to write . I was excited but at the same time, I felt incredibly ineffective. Like any other skill (testing, programming), writing takes the time to get better. And to be able to harvest the benefits from invested time. I wish more people blog. Even in our company, not everyone is writing regularly. Why would I love to see more bloggers? There are hundreds, thousands of developers working every day on solving unique problems. Finding interesting solutions. And they are never shared. The world does not know. I am inspired by the collaboration spirit of open source apps and libraries . So much time and effort saved because we can re-use existing solutions and we don‚Äôt need to re-implement databases for every new project. Blogging is for me part of the same movement but on a different level. On the level of knowledge . Having the tools is one thing. Knowing how to use them properly is another. But when I was starting I didn‚Äôt know all those things. I just wanted to be like the best programmers that I know about. Fowler, Feathers, Martin, Katz and many others. They inspired me hundreds of times and challenged me to be a professional. So I started to blog. And then many things happened. Other developers started to read us. There were CTOs of bigger and smaller startups among them. They respected our technical skills and wanted to hire us . Right now most leads who contact us regarding collaboration are technical. They are developers, engineers who already know what we do, how we do it and how we work. We don‚Äôt need to sell our expertise. This is a very comfortable situation to be in. We are vouched by people who are already in the client‚Äôs organization. We start our collaboration from an entirely different level of trust. It‚Äôs easier for us to charge higher rates . Our expertise is provided for free to others (as blog posts and books). It can be verified. If potential customers don‚Äôt agree with some techniques, we can explain them. It doesn‚Äôt work so good with existing clients (rising rates in such situation is a hard process) but with new leads it does. The more knowledge we have written down, the easier we can present the broad range of problems that we already dealt with. Recruitment got easier . People want to work with good developers. So when they perceive you as experts, they are more likely to apply to you. Blog posts are like a magnet. You can present yourself to the world. We speak more at conferences. Who do you think is more likely to be invited as a speaker at a conference? An unknown person or a developer with 100 blog posts. Who do you think is more trustworthy to deliver valuable knowledge, in the eyes of conference organizers? And conferences give you exposure which gives you leads, candidates and more. We got workshop materials :) Right now we are preparing ourselves for our second Rails DDD workshop. I‚Äôve been reviewing what I would like to discuss as part of every module. Many of these topics we‚Äôve already written about on our blog. I could re-use those materials as a starting point and save time. Many of our posts were starting points for our books. In other words, it‚Äôs easier to sell knowledge that you already have written down. We have an excellent way to share knowledge between our whole team. We have our internal saying ‚Äúthere is Arkency blog post for that‚Äù. We use it when someone asks a question, and the answer is a link to our post about this topic. Or when you google something and find our post as a first result :) So far I wrote 70 blog posts. My boss is already at 150. But it started with the first one. Blogging for busy programmers , which is part of our bundle will help you start with it. It doesn‚Äôt matter if you want to charge more, sell products, recruit more people or find more clients. Your blog can be a potent tool for achieving those goals. But‚Ä¶ We admire blogging programmers for their knowledge, for how they share it. We would like to work with them and achieve same things they achieved. But all those limitations‚Ä¶ Can we handle them somehow? It is easier to blog once you realize you are playing a long-term game . One, two or five blog posts are valuable but they won‚Äôt change your life. Let me show you a few examples from own blog posts We all want the benefits of blogging , but we don‚Äôt know how to overcome our limitations . And this is where ‚ÄúBlogging for busy programmers‚Äù will help you. To cut the scope, to promote, to worry less about haters, to ship it and to finally become a proud blogger helping people every day (even when you are not writing). Available as part of our Smart Income For Developers Bundle in all three available packages. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-10-17"},
{"website": "Arkency", "title": "Hourly Billing is Nuts", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/10/hourly-billing-is-nuts/", "abstract": "When it comes to my salary, I am an ordinary guy who earns $ X per hour . I‚Äôve been working this way since I joined Arkency, and I never thought much about it. I‚Äôve been happy that if I need vacations or break, I can just take them (properly communicating it with the clients, of course) without much hassle. When I feel great, and I am inspired to work and work and work, I can just log more hours without thinking much about it as well. Just like monthly salary, hourly salary is a safe way to live. But is it the most rewarding way to live? Hourly Billing is Nuts challenged my perception that hourly billing is the best way possible. I started to notice the bad parts as well, not only the good ones. I don‚Äôt agree completely with some parts of the book, but it got me thinking. I won‚Äôt summarize the book here because it‚Äôs better if you just read it yourself. So just a small challenge . At some point, you started working as a programmer with skills X and let‚Äôs say your current salary reflects that (no matter if you are an employee or freelancer). You read tons of books, invest in your skills, gain lots of experience and reach level 2X. How do you prove that you should be paid 2X now? Not to your boss but your customer‚Ä¶ How? How can they see that you are worth twice the salary? As you can imagine, it‚Äôs problematic‚Ä¶ So I‚Äôve been thinking how value pricing (pay per feature) changes things here. And I came to the conclusion that the change is mostly about moving this risk (and potential reward related to it). What do I mean by that? When billing hourly the risk for an agency is minimal , and most of it is on the side of the customer. If the estimations are incorrect (and they always are), and things take longer, it‚Äôs the customer who pays more. This can be mitigated some way (for example by ‚Äúwe won‚Äôt pay you for more than Z hours‚Äù rule) but if it is, you are already beyond hourly billing and taking some part of the risk on your side. On the other hand, when you are doing fixed price project the whole risk is on your side . When things take longer customers don‚Äôt care (unless deadlines apply). It‚Äôs your time and your cost only. So the risk shifted. What if by being a better developer, using the proper technology you can do the project in half the time it would take other companies? In hourly billing model, the reward is received by the customer . You did things cheaper, and they still have the budget to pay you for other features that you could work on during that freed time. In fixed price environment the reward is yours . You get the same amount of money, and you can work (or not :) ) on a different task/project in the freed time. Of course, you might argue that when you get 2X better, you can charge 2X more per hour. I agree, assuming you know how to convince your (new or existing customers) that you are now two times better. This can be hard. But is it hard in the value pricing model? Not really. You charge like you used to, and you implement things faster. You monetize the free time however you want. The price is not directly correlated to the time you spend on the feature but to the value, it brings to your customer. And that‚Äôs only a fraction of my thoughts after reading Hourly Billing Is Nuts. For years I‚Äôve been comfortable with my hourly billing model, and I never challenged it consciously . Now I started to consider pros and cons of both approaches actively and wonder which one I should use with which customers. And which let me easily charge more when I get better . P.S. Hourly Billing is Nuts is available as part of our Smart Income for Developers bundle in all packages. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-10-20"},
{"website": "Arkency", "title": "The freelancer in you that wants to be FREE", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/10/the-freelancer-in-you-that-wants-to-be-free/", "abstract": "Do you sometimes as a developer/freelancer/company owner have the feeling that you are being used? Not adequately paid? Treated like a resource? That considering all the effort, time, investment, the school you went through you should earn more and live a happier life? How many things do you know and use in your daily job? Try to list all of them. Here is my list: And that‚Äôs not even everything . I bet the list for you is very similar in length (no matter what is your main programming language and whether you are frontend or backend dev). You want things to You don‚Äôt want to stress over things every time. Most of the people finish their job after 5, go home and don‚Äôt give a fuck. I‚Äôve been learning every week since 8 years after I finished studying at the university. I bet you are as well crunching knowledge on a daily basis. Educating yourself and improving your skills. With the impostor syndrome so prevailing in our industry we always feel behind, we need to know more and we spend so much time and money and energy reading and educating in the ever-changing landscape of technologies . And what do we get for it? A-holes who tell us that our estimations are too big, managers who try to micro-manage . In the meantime, other programmers earn 2-10x more, work with nice people, on nice projects, for companies that make money, can spend time with their family and have a much happier life . What went wrong? Let me tell you what usually goes wrong in a situation like this. We invest 99% of our time in tech skills (do you remember the long list a moment ago) and 1% of our time in business/sales/soft skills. Boom! Want a better life? You need to start attracting better clients into your company. Nicer people, happier people, successful people that will respect your authority. People who treat collaboration with you as an investment into their company, not as a cost. How to do that? The Independent Consulting Manual will help you find answers. It will teach you how to: And when you balance your skills to 80% tech and 20% business you will be truly an independent, happy consultant . This book will be available as part of our Smart Income for Developers Bundle next week. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-10-18"},
{"website": "Arkency", "title": "Developers as DDD bounded contexts representatives", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/10/developers-as-ddd-bounded-contexts-representatives/", "abstract": "Recently, I‚Äôve been preparing to my webinar about introducing DDD to existing projects/teams. One DDD part which is not always easy to explain is actually the main strategic pattern - bounded contexts. For the webinar purpose I came up with the idea of showing that in our programming teams we also have bounded contexts. What‚Äôs better, most of us, represent different bounded contexts. In this blogpost I‚Äôd like to highlight some of such contexts. The thing I‚Äôd like you to focus on is that the different types of developers - they all use different vocabulary when they play different roles. It‚Äôs worth noting it just for the purpose of a successful communication but also to learn that it‚Äôs the main point of DDD - find the domain language. There‚Äôs definitely more such types/contexts. If you can name some - feel free to do it in the comments! Please note, that each of us can play any of the roles at any time. However, sometimes some roles are more natural for each of us. Note the language that we‚Äôre using. Even though we can have some fun describing the personas, all the contexts are important at specific times. What contexts do we see here? This is all for our internal needs - to safely/efficiently deliver software. Now look similarly at the actual project you‚Äôre working on. What subdomains do you see? If you‚Äôre working on some kind of e-commerce, you‚Äôll probably see: and many others. It‚Äôs not uncommon to see ~30 potential bounded contexts. Are they clearly represented in your system? Each of them deserve a dedicated module. It‚Äôs truly bounded if they don‚Äôt talk to each other directly. They either communicate with events or there‚Äôs a layer above (app layer) which orchestrates them. Each of them should have a separate storage (conceptually) and never ever look at each other storage directly. When I first encountered DDD - this all was a mystery to me. How to actually achieve this? Now, after seeing this happen in our projects it all seems much simpler. Each context is either a separate module/directory in the repo or it‚Äôs a separare microservice/repository. When I work on Accounting features, I‚Äôm not bothered by the concepts of other contexts. I‚Äôm only surrounded by things like accounts, revenues, profits. This makes me much easier ‚Äúto get into the zone‚Äù. Heck, thanks to the CQRS (I consider this to be part of the bigger DDD family) techniques, I don‚Äôt need to bother too much about how it displays on the UI. The ‚Äúread‚Äù code is also separated. Pssssst, if you‚Äôre interested in applying DDD in your Rails projects, consider coming for 2 days to Wroc≈Çaw, Poland and attend our Rails DDD workshops . The next edition is 24-25 November, 2016. This is how the first 2 hours of the workshops look like in the first edition - a heaven for the agilers - post-it notes everywhere ;) It‚Äôs a technique called Event Storming - we visualize a system with events/commands/aggregates - each having a different color of a post-it note. Our #Rails #DDD Workshop is happening right now! pic.twitter.com/qzMbL8tNwt Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-10-23"},
{"website": "Arkency", "title": "The esthetics of a Ruby service object", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/10/the-esthetics-of-a-ruby-service-object/", "abstract": "A service object in Ruby has some typical elements. There are some minor difference which I noted in the way people implement them. Let‚Äôs look at one example. This example may be a bit unusual, as it doesn‚Äôt come from a Rails codebase. The service object actually comes from the engine of this very own blog. The blog is nanoc-based and the service object is used to locally generate a new draft of a blogpost. You can read the first part of the story in the blogpost where I talked how this service object was extracted from a previous script. The final result was the following: Today, I was trying to optimize and automate my blogging flow. This involved: The code ended up looking like this: When I tweeted it (just the call method), I‚Äôve got one interesting reply from my friend who is not working with Ruby: @andrzejkrzywda this may actually convince me to use #ruby ‚Ä¶. followed by this: @psmyrdek @andrzejkrzywda not lines, rather the pure simplicity in code. This awesome. which got me thinking and inspired to write this blogpost (thanks Pawe≈Ç!). What we see here, is a typical run/execute/call method (I‚Äôve settled with ‚Äúcall‚Äù) which orchestrates other parts. The naming is a bit verbose but also quite explicit in describing what it does. There‚Äôs something Ruby-specific which makes the code appealing to certain developers. This was probably the part which brought me to Ruby back in 2004 (and I still didn‚Äôt find a programming language which would be more esthetically appealing to me than Ruby!). The lack of braces is one thing. Then there‚Äôs the dynamic typing , resulting in no type declaration. Less verbose thanks to that. There‚Äôs also a design choice here - there‚Äôs lack of params being passed . The ‚Äúcall‚Äù method doesn‚Äôt take anything, nor the other methods. However, in fact, they do use some input data, but those 2 variables are set in the constructor method. This means that we can access them via ‚Äú@title‚Äù and ‚Äú@date‚Äù instance variables. There are additional 7 private methods here, which are using the instance variables. The topic of service objects in Rails apps was so fascinating to me that I wrote a whole book about it. One realisation I‚Äôve had over my time spent on service objects is their connection to functions and to functional programming. Some people call them function objects or commands. My architectural journey led me to discover the beauty of Domain-Driven Development and CQRS (Command Query Responsibility Segregation). At some point, all those pieces started to fit together. I‚Äôm now looking at code in a more functional way. What I was doing with my ‚ÄúRails Refactoring‚Äù actions was actually about localizing the places where data gets mutated . In fact, my current Rails/DDD teaching how to build Rails apps feels almost like Functional Programming. So, the question appears - is this service object functional? I‚Äôm not aware of all FP techniques, but being explicit with input/output of each function is one of the main rules, as I understand. Which means, that the 8 methods of my service object are not functional at all. (the part of this object which mutates the whole world around - file system, git repo, operating system - is also not helping in calling it functional). But let‚Äôs focus on the input arguments part. What if we explicitly add them? Given that this post is about esthethics and it‚Äôs always a subject to personal opinion - I‚Äôd say it‚Äôs worse now. It‚Äôs more verbose, it‚Äôs even too explicit. But there‚Äôs one part which makes this new code better. As a big refactoring fan, I can tell that when each method is explicit about the input it needs, the code is much more friendly towards extracting new classes and methods . In this specific situation, the estethics won over the being refactoring-friendly :) What‚Äôs your take on the esthetics here? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-10-23"},
{"website": "Arkency", "title": "Running bash command from Ruby (with your bash_profile)", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/10/running-bash-command-from-ruby-with-your-bash-profile/", "abstract": "Whenever I try to automate some of my daily tasks, I end up with a mix of Ruby and Bash scripts. This is the time when I look up to the differences between system , exec , %x , backticks and others. However, there‚Äôs additional thing with actually executing a bash script not just shell script. Yesterday I‚Äôve been optimizing my blogging flow. One part of it is to open my favourite editor with the current draft file. Until yesterday I did it manually. I‚Äôve had this in my bash_profile : so just typing ia content/posts/a_long_path_to_the_file.md was opening the editor. Now, I have a script which not only generates the draft file, but also git pushes it, opens the browser to preview it and opens the editor. See my previous blogpost to read more about this specific Ruby service object The thing is, if you just use system it‚Äôs not enough. You need to invoke bash in a special mode -ilc to actually get the bash_profile loaded. Otherwise, the ia alias is not recognized. So, I ended up with this: Which works great so far. It helped me speeding up my blogging process and hopefully will result in more blogposts ;) Happy blogging! BTW, if you want to improve your blogging skils, my ‚ÄúBlogging for Busy Programmers‚Äù book is now part (for a limited time) of the Smart Income For Developers bundle . Check it out! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-10-24"},
{"website": "Arkency", "title": "Overcommunication is required for async/remote work", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/10/overcommunication-is-required-for-async-slash-remote-work/", "abstract": "At Arkency we‚Äôre working remotely. Being able to work remotely is just a side-effect of working asynchronously. Let‚Äôs talk about what it means in practice. Async means working at the time you choose and not the time, the rest of the team insists on. Async means not being blocked on work of other people. It also means not blocking other people on your work. Remote work doesn‚Äôt need to be async. You can still have rules about certain time of working. In that case, blocking on each other is less of a big deal. You can always interrupt others to remove blockades. As in the office. Async is where the fun begins. Async is where productivity begins . I sometimes hear people saying - ‚Äúworking remotely is not for me‚Äù. I prefer to use other words. Async work, like remote work is a skill. You can choose to learn but you may be fine without learning it. You don‚Äôt need to have a special talent to work async/remotely. It‚Äôs about skills you can practice, it‚Äôs up to you. You control it. Async work is actually a set of skills. One of the most important skills is overcommunication. It‚Äôs hard to work async without overcommunication. Actually, overcommunication is also a big term and not very precise. At the very minimum, I want to overcommunicate to the rest of the team what I‚Äôm working on. It should be super-easy for others in the team to say - Andrzej is working on feature X. I can do it in several ways: That was the minimum. What‚Äôs close to the maximum?\nWhen you let others to help you if your solution is wrong. You can do it only if you present not only what you work on, but also how you‚Äôre planning to work on it. Additionally, communicating along the way, how it goes. How can I do it? Working async doesn‚Äôt mean we force ourselves to work at different times. It‚Äôs almost always the case, that someone else is working at the same time.\nMy team members can see my messages. They can read it but don‚Äôt have to. They can choose not to reply. They can choose to reply now or later. It‚Äôs up to them. Some chances are that if I‚Äôm doing something stupid (and I often do), then can quickly tell me. If I‚Äôm going over the scope of the ticket they can correct me. In our case, it‚Äôs not only project members who can help me. Also people from other teams stay interested in what is going on. They become familiar with the project (good for possible future rotations!). They can help me too. They can also learn from me, if I did something nice. Overcommunication is like the best parts of live pairing, without the uncomfortable ones. It‚Äôs like being surrounded by friends who want to help me, if only I give them the chance to help me. I choose to give them the chance. Happy overcommunicating! We wrote a book called ‚ÄúAsync/Remote‚Äù where you can find more about the required skills for async work, not only over-communication. Those are things like ‚Äú1-size stories‚Äù, ‚Äúsingle backlog‚Äù, ‚Äúasync standups‚Äù. The book is now part of the ‚ÄúSmart Income for Developers‚Äù bundle. It‚Äôs a good deal. Go check it out, it‚Äôs only valid this week. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-10-26"},
{"website": "Arkency", "title": "The typical Ruby bugs with changing the last line in a method", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2016/10/the-typical-ruby-bugs-with-changing-the-last-line-in-a-method/", "abstract": "In Ruby, we don‚Äôt have to type return at the end of the method. As many other things in our favourite language, this is implicit, not explicit. The value of the last line in a method is also automatically the value of the whole method. Which is a good thing as it helps making the code less verbose and more poem-like. But it also has some dangers. Only this year, we‚Äôve had two different situations where experienced developers introduced bugs into a system. The first story started when business wanted us to remove one existing feature from the application. This feature was something about analytics code in the view. The intention was to no longer track something. There was something like this in the code. The developer looked at the code. It was quite clear that the last call should be removed, so he did that. Apparently, it was crucial that the value returned by this method was actually used in the view. It went through several layers, so it wasn‚Äôt that easy to spot.\nThe result? The result was actually very bad - the setup_which_is_meant_to_stay_here call returned a hash with a lot of information about internals of our system. And it all went to the front page of one of the systems. Which we learnt only a few hours later. The second story happened just recently, in my project. There‚Äôs a place in the UI (react+redux), where we register new customers. Submitting the form creates an ajax request, which goes to the backend, which then calls a special microservice (bounded context) and then we get the response back to let the UI know that all is good with some additional information to display. It was all good and working. But then, we‚Äôve had a need to extend the existing backend code with publishing an event. The code was a typical service object in a Rails app: After extending the service object, it looked like this: The thing is, this service object was used from a controller, like this: We haven‚Äôt noticed the problem at first. The visible difference was that the UI now showed a failure message, but it was actually adding the customer to the system! And the exception under the hood was something about IOError , which didn‚Äôt help in debugging it. As you see, two different stories, but the same problem - changing the last line of a method. Be careful with that :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-10-27"},
{"website": "Arkency", "title": "Safer Rails database migrations with Soundcloud's Large Hadron Migrator", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/12/safer-database-migrations-with-soundclouds-large-hadron-migrator/", "abstract": "When I first started using Rails years ago I fell in love with the concept of\ndatabase migrations. Perhaps because at the time I was working on commercial\nprojects in C# which lacked this and I could feel the difference. The fact\nthat for many years the concept remained almost the same with some minor\nimprovements speaks for itself. Ruby on Rails continues evolving all the time but\nmigrations remain simple. However, there is an elephant in the room . Some DDL operations on MySQL database (such as adding or removing columns)\nare locking the whole affected table. It means that no other process will be\nable to add or update a record at that time and will wait until lock is released\nor timeout occurs. The list of operations that can be performed online (without lock)\nconstantly increases with every new MySQL release so make sure to check the\nversion of your database and consult its documentation. In particular, this has been very much improved in MySQL 5.6 . With lower number of records, offline DDL operations are not problematic. You can live with 1s\nlock. Your application and background workers will not do anything in that\ntime, some customers might experience slower response times. But in general,\nnothing harmful very much. However, when the table has millions of records changing it can lock the table\nfor many seconds or even minutes . Soundcloud even says an hour , although\nI personally haven‚Äôt experienced it. Anyway, there are tables in our system of utter importance such as orders or payments and locking them for minutes would mean that customers can‚Äôt buy,\nmerchants can‚Äôt sell and we don‚Äôt earn a dime during that time. For some time our solution was to run the costly migrations around 1 am or 6 am when\nthere was not much traffic and a few minutes of downtime was not a problem.\nBut with the volume of purchases constantly increasing, with having more merchants from\naround the whole world, there is no longer a good hour to do\nmaintenance anymore . Not to mention that everyone loves to sleep and waking up earlier just to\nrun a migration is pointless. We needed better tools and better robots to\nsolve this problem. We decided to use Large Hadron Migrator created by Soundcloud. How does it work? That‚Äôs the idea behind it. The syntax is not as easy as with standard Rails migrations because you will need\nto restore to using SQL syntax a bit more often. If you need to migrate big tables without downtime in MySQL you can use LHM\nor upgrade MySQL to 5.6 :) If you are still worried how to safely do Continuous\nDeployment and handle migrations please read our other blog posts as well: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-12-23"},
{"website": "Arkency", "title": "Patterns for dealing with uncertainty", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/12/techniques-for-dealing-with-uncertainity/", "abstract": "In programming we are often dealing with an uncertainity. We don‚Äôt know,\nwe are not sure if something happened or not. Or we are not sure about\nthe result. Especially when it comes to networking/distributed systems but also in other\ncontexts. What I find interesting is that in many cases the techniques\nused to handle such problems are very similar. You tried to send something from computer A to computer B. It didn‚Äôt work.\nWhat do we do? One of the most often technique is to try to do it again.\nVery simple isn‚Äôt it? We say at least once delivery because computer B can receive our message\nmultiple times in case 1st time already worked but computer A wasn‚Äôt sure\nabout it so it sent it again. Sometimes it is enough that we know a message reached point B. But often\nit is not enough. We also need to know that the message was successfully\nrecognized and processed. When it worked point B sends a confirmation\nto point A. Notice that what is just a delivery on a higher level (message reached point B)\nrequires a delivery and confirmation on a lower level (packets consisting the\nmessage reached point B and acknowledge of it reached point A back). Sometimes we prefer speed and smaller usage of resources over certainty or\nreliability. In such case, we always send the message only once. Either it\nwill reach its destination or not. So the strategy is based on lack of\nretries. This can be used in many scenarios where the transmitted value is only valid\nfor a very short time anyway and next transmission will include a new\nversion anyway. A mobile phone sending GPS positions every second. A computer game sending\nplayer‚Äôs positions constantly. A thermometer sending current temp. In such\ncases, the logic behind those systems can probably use a previous value as\na good enough substitute of the new one, if it wasn‚Äôt received. A new value\nwill be sent quickly anyway. We need timeouts because we cannot wait indefinitely for a confirmation of\na message. If the message was lost or the confirmation of it was lost\nwaiting longer won‚Äôt change the situation. When we reach the timeout we can schedule a retry or just move on depending\non the previously discussed strategies. Of course, timeouts can cause false negatives. Our system reports a timeout\nwhich is treated as a failure and one second later we can receive a message\nsaying that everything went OK. But we received it too late. In such case\nsometimes we don‚Äôt need to do anything (retry was already scheduled) but\nsometimes we might need to compensate. We cancelled an order and now that\nwe received a payment confirmation after a timeout we need to also refund\nthe payment. That is an example of a compensation. Idempotence is a way to correctly handle duplicated messages received due\nto retries and at least once delivery strategy. The idea is that when we\nreceive the same message multiple times it does not cause additional\nimportant side-effects and the client is informed that the operation was\nsuccessful. The repeated message may cause minor, unimportant side-effects. Maybe it will be\nlogged again, maybe some technical metrics will be increased again but\nbusiness-wise there is no visible effect. For example, you can receive an information that a payment was successful.\nSo you trigger a state transition in your app, schedule order delivery,\nemail to customer etc. And 1 second later you receive from payment\ngateway the same information that the payment was successful. You don‚Äôt\nsend to the customer the products twice, you don‚Äôt report twice that much\nrevenue for your startup and you don‚Äôt send another email. You silently\nignore the information, but you respond with information that everything\nwas OK. Sometimes idempotence can be achieved easily (state machine that can go\nfrom state X to X without doing anything), but usually, it requires\nthe effort to detect such situation and handle it properly. There is often no point in continuing to retry at the same rate i.e.\nevery second. If the situation does not improve, with every retry\nit is less likely that the system that we try to cooperate with will\nself-heal. It‚Äôs better to back off and keep trying but less and less\noften. 1 second, 1 minute, 1 hour, 1 day, etc‚Ä¶ Also, some systems randomize retries to avoid a situation where thousands\nof affected devices try to repeat something at the same time causing a\nself-denial of service attack. Imagine a networking problem which causes\nmillions of devices running a chat application to disconnect immediately.\nIf they all try to reconnect instantaneously at the same time with the\nsame non-randomized strategy then your servers may not be able to handle it. But when\nsome retry after a second, another group after two seconds, and another after\nthree seconds, then the load on your server might be more tolerable.\nEspecially considering that initiating a connection can often be one\nof the most expensive operation when systems try to sync their state. RAM is volatile, our application/database processes can die,\nservers can be turned off. That‚Äôs why for really important data\nbefore sending it over the wire we first save it into a more safe\nspace. A disc. That way in the worst case we can have a list of messages that\nwe wanted to send but never had the time to, or were not yet\nconfirmed. If something bad happens, we can re-read the list of messages\nand send them again. Almost every messaging system that is supposed to be reliable will\nsend messages to either a disc or replicas running on other machines\nfirst before confirming that the message was queued. When we keep sending messages over the wire we can number them incrementally.\nOne, Two, Three, Four, Five‚Ä¶ When the other part receives them it can spot gaps and out of order messages\nand request replies or re-order them. It can also confirm multiple messages\nup to certain number with one reply ( i am at 5 ) instead of confirming each\none separately ( got 1 , got 2 , got 3 ‚Ä¶). Sequence numbers don‚Äôt\nneed to be global. They can be defined per connection, session, stream, etc. Often the content of a message does not uniquely identify it. For example, we\nmay receive an order for 2 iPhones. Many people could order 2 iPhones. So to\nhandle idempotency it might greatly help if every message/request is sent\nwith unique client side generated UUID. If the client repeats the message it\nwill use the same UUID. That makes the recipient‚Äôs job of detecting\nduplicates much easier. Correlation numbers are a mix of sequence numbers and UUIDs. Say a client\nsends an order (UUID 321), and later informs about a successful payment (UUID 609\ncaused by UUID 321) but both messages can get lost. When the server receives UUID 609 and sees that it is correlated to something with UUID 321\nbut it has not yet received 321 it knows that it cannot process 609 immediately.\nIt can save that information and wait for retry of 321 and only when it receives\nit, the server will process 321 and then process 609. In other words, correlation numbers can help you with retries/duplicates/out-of-order\nmessages which are related to the same business process. Imagine a payment gateway. Your e-commerce system assumes that certain transactions\nwere successful. But that may be an incorrect or incomplete list. If your system was down\nor had reliability problems it could have dropped some messages about successful payments.\nIf your system was down too long maybe all retries failed and you will never know\nabout this payment (which you probably should refund, or deliver or pay taxes of). Unless there is a reconciliation process. It means that the payment gateway system\nexposes API or downloadable file with a list of all transactions. Ideally, as immutable,\nappend-only list of transactions. In such case even long time later you can compare\nthe list of payments in your system, the list in their system and find discrepancies. This is a way of keeping and synchronizing data in your system in a special way.\nMultiple independent nodes can even disconnect completely and when they reconnect\nlater and merge their data together you can be sure that all will reach the same\nstate. I think that if we, as humanity, ever go into stars we will need more of such structures\nto effectively exchange data after disconnecting and reconnecting between multiple space\nships :) These techniques are either already used by databases of your choice or directly\nin your applications. There are probably much more of them but these I am most\nfamiliar with and are most popular, I think. But I am certain I missed some of\nthem. Let me know in comments about other techniques that you know about. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-12-04"},
{"website": "Arkency", "title": "Why would you even want to listen about DDD?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2016/12/why-would-you-even-want-to-listen-about-ddd/", "abstract": "You might have heard about this thing called DDD which stands for Domain-Driven Design .\nThe name does not reveal much about itself. So maybe you wonder why should you listen\nabout it. What‚Äôs so good in it? What problems does it try to solve? If you look at the cover of the book (often referred to as the Blue Book ) which brought\na lot of attention to DDD you will see the answer. The subtitle says ‚ÄúTackling Complexity in the Heart of Software‚Äù. That‚Äôs what DDD is all about.\nManaging, fighting and struggling with complexity. Building software according to certain\nprinciples which help us build maintainable code. So‚Ä¶ If every 3 months you start a new simple Rails application, a new prototype which may or may\nnot is successful then probably DDD is not for you. You don‚Äôt accumulate enough complexity in 3\nmonths probably. If you work on short projects (in terms of development and time to live)\nfor example, because you work for a marketing agency and that‚Äôs the kind of applications you develop\nthen DDD is probably not for you. When is DDD most useful in my opinion? In the long term. When you work on years-long projects\nwhich are supposed to have even more years-long time of usage. When the cost of maintenance\nand expanding is much more important than the cost of development. But even there you start to\nintroduce the techniques gradually when the need arises. When you see the complexity reaching a certain level. When you understand the domain better. DDD is just a name for a set of techniques such as: As with every programming technique, you don‚Äôt need to use all of them. You can cherry pick those that you benefit most from and start using them at the beginning. In my projects, the most beneficial were Bounded Contexts, Domain Events, Sagas. So if you are wondering‚Ä¶ Are DDD books for me? Is Arkency‚Äôs DDD workshop for me? Should I invest my\ntime and money into learning those techniques? Then the first questions you should ask yourself is Because if not then you can watch DDD from distance, with curiosity, but without much commitment to it.\nYou simply have other problems in life :) But DDD was one of the 5 most important books for DHH so definitelly you will benefit from learning it as well. Join our upcoming DDD workshop in January to spend 2 days practicing those techniques in Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-12-26"},
{"website": "Arkency", "title": "Event sourced domain objects in less than 150 LOC", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/2016/12/event-sourced-domain-objects-in-less-than-150-loc/", "abstract": "Some say: ‚ÄúEvent sourcing is hard‚Äù . Some say: ‚ÄúYou need a framework to use Event Sourcing‚Äù .\nSome say: ‚Ä¶ Meh. You aren‚Äôt gonna need it. Let‚Äôs use Payment as a sample here. The ‚Äústory‚Äù is simple. Customer place an order.\nWhen an order is validated the payment is authorized. We do not just create it.\nCreate is not a word our business experts will use here (hopefully). The customer\nauthorizes us to charge him some amount of money.\nRead this Udi Dahan‚Äôs post . The payment logic is pretty simple (for a sake of this example, in real life it is much more complicated).\nCustomer authorizes payment for specified amount. We send the authorization to the payment gateway.\nAfter some time (async FTW) payment gateway will respond with OK or NOT OK message.\nIf payment gateway informs us about successful payment it means it has been able to charge the customer and\nthe money is waiting reserved for us.\nSuccessful payments could be then captured (what means asking payment gateway to give us our money). Ok, so we have our business logic. First, we need to define our domain events. Then let‚Äôs use them to implement our Payment domain model. With a little help from RailsEventStore & AggregateRoot gems we have now fully functional event sourced Payment aggregate. RailsEventStore allows to read & store domain events. AggregateRoot is just a module to include in your aggregate root classes. It provides just 3 methods: apply , load & store . Check the source code to understand how it works. It‚Äôs quite simple. The typical lifecycle of that domain object is: Let‚Äôs define our process. To help us use it later we will define an application service class that will handle all ‚Äúplumbing‚Äù for us. Now we need only an adapter for our payment gateway & instance of RailsEventStore::Client . Complete code (149 LOC) is available here . Of course, it is an additional effort. Of course, it requires more code (and probably even more as I have not shown read models here).\nOf course, it required a change in Your mindset. I‚Äôve posted Why use Event Sourcing some time ago. The audit log of all actions is priceless (especially when you deal with customers money). All state changes are made only by applying domain event, so you will not have any change that is not stored in domain events (which are your audit log). Avoiding impedance mismatch between object oriented and relational world & not having ActiveRecord in your domain model - another win for me. By using CQRS and read models (maybe not just a single one, polyglot data is a BIG win here) you could make your application more scalable, more available. Decoupling different parts of the system (bounded contexts) is also much easier. This is a very basic example. There is much more to learn here, naming some only: If you are interested join our upcoming Rails + Domain Driven Design Workshop . Next edition will be held on 12-13th January 2017 (Thursday & Friday) in Wroc≈Çaw, Poland. The workshop will be held in English. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2016-12-30"},
{"website": "Arkency", "title": "On upcoming immutable string literals in Ruby", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/01/on-upcoming-immutable-string-literals-in-ruby/", "abstract": "Today I checked one of the solutions made by our Junior Rails Developer class student. As part of the course they make Rails apps but also learn from smaller\ncode examples delivered by exercism.io. I found there an opportunity for him to learn more about a few things‚Ä¶ That‚Äôs his code which inspired me for a reflection. I recommended reading about #tap ; how one could use a Hash to remove some duplication.\nI also thought it could be a good occasion to talk about how Ruby String‚Äôs are mutable\nbut in some time string literals will be immutable . The keyword here is literals . Let‚Äôs focus on a very small part of the code: We could easily refactor it to: But not when string literals are enabled to be immutable. Obviously, because s = \"\" is a string literal. In such case we would need to go with a less elegant solution probably: because So just be aware that in upcoming Ruby versions s = \"\" and s = String.new might not be equal. And in the case when you\nare building a new string via multiple transformations or\nconcatenations the 2nd version might be preffered. I wonder if some time later ruby (4 ?) will make all Strings\nimmutable (not only those created via literals) and introduce StringBuilder class like .Net or Java has? Happy 2017! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-01-02"},
{"website": "Arkency", "title": "Run your tests on production!", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/2017/01/run-your-tests-on-production/", "abstract": "Running your tests on ‚Äúproduction‚Äù? On the real, live system?\nSounds unorthodox? There might be situations, where it‚Äôs actually a good idea. Read on! One of the projects we‚Äôre working on is composed of multiple microservices . We have a lot of good lower level tests, but at some point, we felt we lack proper integration tests. Also, our UI code needed some more coverage. In a project comprised of microservices, there are many integration surfaces, so many things can go wrong and setting up the test environment is not trivial . To start the whole app locally we need to spin up 8 ruby processes, not counting databases, message queues, and assets compilation. Building a dedicated infrastructure to launch all the required processes in test environment was a no-go for us at the moment because that would easily consume way too much time. Such an environment would also be a permanent source of debugging why a certain thing works here and not there . So we thought why don‚Äôt we try running some test cases on our production app. Something in the direction of synthetic monitoring . Of course, we don‚Äôt mean to run every single test case - that would easily be an overkill. We want to cover the ‚Äúcritical‚Äù scenario, a typical use case . If you‚Äôre a coffee grinder salesman, you certainly don‚Äôt want this to be broken. You also don‚Äôt want to learn it the hard way by noticing a cliff on your sales chart. Chances are such tests will catch a big part of integration, UI, infrastructural issues . Handling all the corner cases would, of course, stay in lower-level tests. The important thing to note - it was easier for us to give it a try because we don‚Äôt yet have real customers using our app - just the QA - so one could argue it‚Äôs not really a production app. True. But chances are we‚Äôll stay with this approach for longer and there are considerable benefits involved. An immediate objection is probably ‚Äúaren‚Äôt you worried that the test actions will somehow affect regular users‚Äô experience?‚Äù . For example when some test data would become visible to other users? Understandable. Our project is better suited here because it‚Äôs a multitenant system. Which means there are completely separate tenants using our app and normally they don‚Äôt deal at all with each other. So this was easy - we just need to run the tests on a new tenant¬†every time and this shouldn‚Äôt affect other tenants‚Äô experience. But one could say, what if your tenant isolation code is buggy and sometimes there‚Äôs data leaking from one tenant to another. We‚Äôre not perfect, so it‚Äôs perfectly imaginable. But honestly I‚Äôd rather discover this issue while running our tests than later when some real user crashed on another tenant‚Äôs data, where there could be more severe consequences. Of course not everybody works on a multitenant system, but many apps have some specific ways to limit the visibility of certain data to some users. In our case, this was particularly easy. In your case, it may be harder or involve some trade-offs. I can also imagine situations where it‚Äôs totally infeasible. But it‚Äôs probably worth giving a thought before completely dismissing the idea. Another concern that comes up quickly is how we‚Äôre going to clean up the data created by running the tests. This is not the test environment, so we cannot clear the test db or rollback the test transaction. Initially, I thought we‚Äôll need some sophisticated procedure that would clean up the data after every build. This would also be error prone & dangerous - what if you accidentally delete the wrong data? I discussed this with my colleagues and an interesting idea came up. ‚ÄúJust don‚Äôt clean up‚Äù . Sounds even sillier. But: During our discussions there even was an idea of performing a real credit card payment in such a test. To clean it up, you could issue a refund (another use case covered by the way). Ok, there could be non-returnable costs involved, like transaction fees. But, if the reliability of a large app is at stake, why not? Consider the cost of your time lost battling bugs that could have been avoided, or users not being able to work with the app. The bottom line is: a lot is possible, it‚Äôs just trade-offs everywhere . Why not the good old Capybara ? Luckily it can run tests against a remote server. Here‚Äôs a snippet with the base class for the tests: run_server = false tells capybara not to spin up a server itself. This code does not need to live in our rails app. It can (and it probably should) be a separate repo . This way you don‚Äôt unnecessarily load any of the rails stuff, so it hopefully makes the test startup a little faster. Basically, apart from the test cases, the only thing you will need is to add this to the Rakefile : How about the reliability of tests executing the whole stack, involving a complicated JS client code? Frequent random failures can be frustrating, especially that debugging is harder with such high-level tests. It‚Äôs a question for us too. So far we‚Äôve had a tolerable amount of random builds and we hope that it stays this way (or if it doesn‚Äôt, that we can do something about it). We‚Äôll keep you posted! What if you don‚Äôt need to test a web page, just an API? Actually we have one scenario, where we chose to interact with the API directly. What to use to fire http requests? You don‚Äôt need Capybara in such case, nor any other test specific tool. Just use a normal http library, and continue with your assertions. We went for rest-client because it handled file uploads in an easy way. There are basically three three ways you can run such tests: Our goal is to make it all run automatically and see nice green notifications on our slack channel (or wherever else). Since it‚Äôs the production server being tested, we cannot run the tests in the regular build process after a push, because the code is simply not yet there in production. Typically your push would trigger a build on CI and after it passes (of course if you also got continuous delivery set up), it would then trigger a deploy. Only then we can run the tests. So basically you just need to trigger a build of the repo containing the tests, whenever the app was deployed successfully . If you happen to use CircleCI , there‚Äôs an API endpoint that let‚Äôs you trigger the build whenever you want, all you need is to post to this url: So you need to do that whenever there‚Äôs a successful deployment. If you happen to host your app on heroku , you can use heroku‚Äôs free ‚ÄúDeploy Hooks‚Äù addon to post to that url. Here‚Äôs how to do it from CLI: Let us know what you think, what are your experiences. Of course, it‚Äôs not the only way to do it. You could, for example, run the tests against the staging server, which would have its pros and cons. We‚Äôre still experimenting with the idea too - we‚Äôll keep you posted. PS. If you‚Äôre looking for some more interesting ideas that can foster a great development environment, have a look at our book Async Remote . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-01-13"},
{"website": "Arkency", "title": "Why Smalltalk instead of Ruby", "author": ["Richard Eng"], "link": "https://blog.arkency.com/2017/01/why-smalltalk-instead-of-ruby/", "abstract": "Hello, it‚Äôs Andrzej Krzywda from Arkency here. Today we have a special guest post from Richard Eng, the man who is behind the Smalltalk Renaissance. I‚Äôve been fascinated with Smalltalk since my University. In a way, my Ruby code is Smalltalk-infected all the time. Many of the great things in programming come from Smalltalk, like MVC, IDE, refactoring. Smalltalk has its history, but it‚Äôs also still in use in huge, production systems. As Ruby developers we often look at the new, shiny languages. Maybe we can also look at something less new? Maybe we can get inspired with Smalltalk? I‚Äôll leave you with Richard and his strong arguments for Smalltalk, enjoy! :) Ruby is a popular language, largely because of Rails. Ruby borrows OOP from\nSmalltalk, but otherwise is a very much different language. I‚Äôm going to argue\nthat Smalltalk is still technically a better choice than Ruby. Ruby appeals to programmers because of its clean, simpler syntax. However,\nSmalltalk is the ultimate in clean, simple, and minimalist. Its syntax can be\nsummarized on a\npostcard! It has three principal features: objects, lambdas (closures), and reflection.\nThe language is exceptionally easy to understand. Ruby, not so much. That‚Äôs why I always recommend Smalltalk to beginners. It was designed for\nteaching programming to children . Alan Kay et al. had the right idea. Despite its simplicity, Smalltalk loses nothing in terms of programming power.\nIt was used by the U.S. joint military to write a million-line battle\nsimulation program called JWARS (which incidentally outperformed a similar\nsimulation called STORM written in C++ by the U.S. Air Force). Smalltalk is\npowerful enough that it has served the commercial industry for over three\ndecades! Powerful languages do not need all kinds of special features, which is the kind\nof thinking that went into C++, Scala, Rust, Swift, and yes, Python and Ruby,\ntoo. This is why I‚Äôm also a fan of Scheme, Forth, and Go‚Ä¶small, simple,\nminimalist. Smalltalk is synonymous with its ‚Äúlive coding and debugging‚Äù IDE, which is the\nmain reason for its incredible\nproductivity .\nTwice as productive as Ruby. More than three times as productive as JavaScript! Smalltalk‚Äôs ‚Äúimage‚Äù persistence is also a huge timesaver: you can save the\nentire execution state of an application and resume execution at a later time.\nThis is awfully convenient for maintaining the continuity of your workflow. Smalltalk has a fabulous facility for Domain-Specific\nLanguages .\nIt‚Äôs easier and more pleasant than Lisp macros or Ruby‚Äôs approach. Compared to Smalltalk, Ruby‚Äôs handling of blocks is also a bit wonky. Citing Lambda the Ultimate : In Smalltalk, there is easy syntax for constructing a closure and passing it as\nan ordinary argument and picking up that argument with an ordinary parameter and\nthen for invoking the closure and/or passing it on. Ruby, semantically, has the\nsame capabilities for constructing, passing, and using closures. However, Ruby\nuses oddball syntax for these things, for no benefit that I can discern. There\nis a specially distinguished argument position available in a call, specifically\nfor passing a closure. If you are going to construct a closure in that position,\nthe syntax is simple. But the closure in its simplest syntax is not an\nexpression. It only goes in that special position in the syntax, or has to have\na word put in front of it, ‚Äúlambda‚Äù, to be turned into an expression that could\nbe put in an ordinary argument position or any other expression context (e. g.,\nright side of assignment). If a method is expecting to receive a block as an\nargument, the designer has to choose between having it take the block as an\nordinary argument or as the special block argument (either can be chosen, in the\nmonomorphic case). And if the method needs to take two blocks, at least one of\nthem has to be passed as an ordinary argument, so the decision has to be made\nwhether to pass one as the special argument and if so, which one. Conversion in\nboth directions is available in all contexts where it makes sense, between the\nspecial argument or parameter and an ordinary expression. I think this\nexceptionalism in Ruby‚Äôs syntax imposes significant extra conceptual load to\nunderstanding the syntax. Smalltalk‚Äôs treatment of blocks is plenty economical,\nwhether they are being passed as arguments right at the point of construction or\nnot. I guess I can see why this exceptionalism arose; it avoids having the closing\nparenthesis of the argument list coming right after the end of a block, which I\ncan see would look ugly. But, so much twisting and turning and squirming for a\ntiny increment of beauty. No programming language is perfect, of course. Of all the complaints I‚Äôve ever\nheard about Smalltalk, only a small handful are even remotely valid, in my\nopinion. Most are based on\nignorance. Ruby is mostly famous for its Rails framework. Smalltalk has its own ‚ÄúRails,‚Äù\ntoo. It‚Äôs called Seaside (aka the ‚Äú Heretic Web\nFramework ‚Äù).\nSeaside is based on reusable, stateful components. The key feature that supports\nthis is called a continuation . A continuation is a snapshot of a program‚Äôs\ncurrent control state. It allows you to ‚Äújump‚Äù to another execution and when you\nreturn, the current state is restored. This provides a conventional\n‚Äúcall/return‚Äù mechanism for your web application. It can help resolve issues\nsuch as the double request and back button problems. If you‚Äôve never used Seaside, I urge you to try it. You just might find a real\ngem of a web framework (ahem). Ruby and Python are often hailed as great solutions for startups. I argue that\nSmalltalk is just as good if not\nbetter. For rapid prototyping, Smalltalk is unmatched; using Smalltalk is a Zen-like\nexperience. I hear that a lot from Smalltalkers. All of the above wouldn‚Äôt be possible if Smalltalk wasn‚Äôt a nice, simple\nlanguage, IDE, and runtime all rolled into one and tightly coupled. Bret Victor in his wonderful talk, ‚ÄúThe Future of Programming,‚Äù points to\nSmalltalk as a harbinger of how software creation will evolve four decades in\nthe future (from 1973): Why are we still using file-based tools??? Are we Neanderthals? To be sure, Smalltalk isn‚Äôt as popular as Ruby. This is the reason for my\nSmalltalk\nevangelism. However, popularity should not blind you to other possibilities, to other better ways to write software. Read my seminal article on Smalltalk (which has been viewed by over 25,000\npeople around the world!): How learning Smalltalk can make you a better\ndeveloper . You can make software creation easier, more productive, less stressful. You can remake the future of software engineering. The remedy to the malaise\nof today‚Äôs antediluvian style of programming using loosely-integrated,\nfile-based tools is very, very simple: give Smalltalk a\nchance . Mr. Smalltalk: https://medium.com/p/domo-arigato-mr-smalltalk-aa84e245beb9 Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-01-20"},
{"website": "Arkency", "title": "yield default object", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/02/yield-default-object/", "abstract": "There is one programming pattern that I sometimes use\nwhich I wanted to tell you about today. I called this technique yield default object . I am pretty sure it was already presented\nsomewhere, by someone, but I could not find any good reference. Imagine code like this: It doesn‚Äôt matter where it is located (TestActor btw), what it does,\nor what‚Äôs the full context (selling in a boutique/box office/museum).\nWe will be looking at it from a mechanical perspective. So the code is all good, and provides some nice defaults that we can\nuse in our tests. However, sometimes we would like to override those\ndefaults. What options do we have? We could use default named arguments: I don‚Äôt know about you, but that feels a little verbose to me.\nOn the other hand the code is very grep-able and if you later\nwant to rename something it should be pretty straight-forward\nto rename any argument and find where they are being used. After years of using Ruby and Rails, that‚Äôs something that I\nvalue highly. It would also work very nice with code-editors that will be\ncapable of showing nicely all arguments that you can pass to\nthe method. We can use the double splat operator and handle any named arguments. It‚Äôs definitely less verbose, but you might not get any errors\nin case of typos (depends on how the StartNewSession constructor\nis implemented, whether it will silently ignore additional\nattributes or not). And you won‚Äôt get autocomplete. Here is another approach. One that I wanted to show you. Instead of passing the attributes around we are passing the whole\ncommand object built with defaults by yield -ing it to the caller. For me there is certain appeal to this solution. And if you use the fluent interface as well you could have: Of course the number of solutions to this problem is infinite and\nyou could mix and match those approaches (for example by yielding attributes\ninstead of the object). Which approach do you use in your test? P.S. Yes, I know about factory_girl , you don‚Äôt need to mention it. Check out our video course Hands-on Ruby, TDD, DDD - a simulation of a real project which contains 51 short videos, each one discussing a small refactoring or technique. Use discount code YIELD_DEFAULT to purchase with 50% discount.\nThe offer expires on Fed 10, 2017. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-02-02"},
{"website": "Arkency", "title": "Fluent Interfaces in Ruby ecosystem", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/01/fluent-interfaces-in-ruby-ecosystem/", "abstract": "You already used fluent interfaces multiple times as a Ruby\ndeveloper. Although you might not have done it consciously.\nAnd maybe you haven‚Äôt built yourself a class with such API\nstyle yet. Let me present you a couple of examples from Ruby\nand its ecosystem and how I designed such API in my use-case. Probably most well-known example would be Active Record Query API\nused for building SQL statements. Most of the time it will return ActiveRecord::Relation as a result.\nBut there are a couple of methods like first! , to_a , to_sql which break\nthe pattern, evaluate the statement and return a result. But if you think about it, usually the fluent interface must give you\na set of methods that will allow to either return the built object or\npass an object to it for interacting. Rspec Mocks is another well-known example of fluent API. On each own, the methods sound silly. What would with(\"John\") mean?\nBut in the context they are readable and make perfect sense. I wonder if could say that certain built-in Ruby classes adhere to a fluent interface?\nFor example String or Enumerable have plenty of methods that you can chain\nand they return the same class. But Martin Fowler said: Certainly chaining is a common technique to use with fluent interfaces, but true fluency is much more than that. I could not find a more elaborate statement of what he meant exactly. But my guess would be that\nfluency comes with a combination of Domain Specific Language. The line between convenient method chaining and fluent interface might be a bit blurry. Some time ago we built an Insights Panel for a marketplace platform where Merchants could\nsee some stats about their customers. The data is based on the marketplace Google Analytics\ndata and fetched via API. But it limits the data only to the customers of certain merchant;\nwithout leaking global stats. Google Analytics API can be queried in thousands of possible ways. If you have at least\none domain with GA, I encourage you to give Query Explorer a try . You can get a ton of useful knowledge from it. Which days of week people buy most, which hours,\nwhere are they from, what devices do they use etc. Google Analytics allows you to do a lot\nwithin its interface, but I find the query explorer sometimes to be much easier. Maybe\nbecause you can easily map its concepts into SELECT/WHERE/GROUP BY üòä Going back to the fluent interfaces‚Ä¶ Here is the code that I used for building the query.\nWhat we usually display in most cases is product sold over time . So that‚Äôs the default\nconfiguration we set up in the constructor. Then we can use the fluent API to change the values easily. The API could be even further refined into: or But the current form was good enough for our needs and readable enough. The other most common usage is to group customers and their purchase stats\nin total, without a timeline. In that case, we often want to display from most\nto least buying groups. That is a common use-case so we have a dedicated method\nfor it. I think that‚Äôs how fluent interfaces evolve over time. They get better names,\nbetter chains, more out of the box, good defaults, and dedicated names. After all you could write in Rspec receive(:method).exactly(1).times but it is \nmuch easier to understand receive(:method).once . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-01-26"},
{"website": "Arkency", "title": "How to unit test classes which depend on Rails models?", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2017/02/how-to-unit-test-classes-which-depend-on-rails-models/", "abstract": "Let‚Äôs say you have such a class:\n(this code is borrowed from this Reddit thread How can we test this class without loading Rails? One way to unit test it is by using the repository object and the concept of replacing the repo object with an in-memory one in tests. Note that the service now takes the repo as the argument. It means the controller needs to pass the right repo in the production code and we use the InMemory one in tests.\nObviously, if your implementations of the repos diverge, you have a problem :) (which best to mitigate by having integration tests which do run this code with Rails) You can read more about the setup here: InMemory fake adapters Rails and adapter objects - different implementations in production and tests It‚Äôs worth noting here, that it may be better to treat a bigger thing as a unit than a single service object. For example you may want to consider testing CreateSomethingService together with GetAllSomethings, which makes the code even simpler, as the InMemory implementation doesn‚Äôt need to have the :somethings attribute. Unit tests vs class tests Services - what they are and why we need them This setup has its limitations (the risk of diverging), but it‚Äôs solvable. The benefit here is that you don‚Äôt rely on Rails in tests, which makes them faster. If you like this kind of approaches to Rails apps, then you will enjoy more such techniques in my book about Refactoring Rails Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-02-05"},
{"website": "Arkency", "title": "A potential problem with PStore and Rails", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2017/02/a-potential-problem-with-pstore-and-rails/", "abstract": "Today, I‚Äôve noticed an interesting post about PStore in Ruby . This reminded me a recent story we‚Äôve had with PStore. Let me start by saying that I like PStore. It‚Äôs a simple solution which can definitely work. I like to think about persistence as something that can be easily replaced if needed. At least in theory ;) In our projects , we often use the pattern of repository. As long as you provide another repository implementation which has the same API, persistence should still work. In one of our >5.years projects, we‚Äôve been migrating servers to a better machine. As part of this, we‚Äôve made one app to work on several nodes instead of 1 as it was so far. The database node was already separated, all cool. During the migration, the developer followed the Capistrano file (code never lies, that‚Äôs the beauty of Capistrano!) and noticed that as part of the deployment we link to the pstore file. After quick investigation, he noticed that one module of the app doesn‚Äôt use the relational database, but used PStore. It‚Äôs a very rarily used module and a very small one (as in 2 ‚Äútables‚Äù). This made the server migration a bit more complex. We either need to have a network-based file system now, so that 2 nodes can use it, or we need to refactor the module (write a new repo object) to use database. Refactoring sounds easier. The story here is that while PStore was a nice experiment here, there were infrastructural consequences of this decision :) Accessing the filesystem is not the thing that cloud providers like to give us nowadays ;) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-02-06"},
{"website": "Arkency", "title": "Prototypes in Ruby and the strange story of dup", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/03/prototypes-in-ruby/", "abstract": "Today I was working on a feature where I had to create a few similar\nActive Record objects. I was creating a read model for\nsome financial data. Most of the attributes of created objects were the\nsame but a few were different. In one refactoring step I removed the\nduplication in passing attributes by using a prototype. Although at that\nmoment I haven‚Äôt thought of the object in such way. The code before refactoring looked similar to this: There were more columns and more entries\n(between 2 and 5) being created for the financial ledger. I could have extracted the common attributes into a Hash\nbut I decided to go with a slightly different direction. I used dup method\nwhich is available for every Object in Ruby. Including in ActiveRecord .\nWhen using dup be aware of its differences from clone ,\nespecially in ActiveRecord case .\nThose semantics changed a few years ago in in Rails 3.1 . The most important difference for me turns out to be the record identity. #dup is like I want a similar (in terms of attributes) but new record and #clone is like i want a copy pointing to the same db record . It is true that every Object has dup implemented so you might be tempted to believe you\nactually, can duplicate every object. And so on, and so on‚Ä¶ Unfortunately, the truth is a bit more complicated.\nThere are so-called immediate objects (or immediate values ) in Ruby which\ncannot be duplicated/cloned. unless‚Ä¶ you are on recently released ruby 2.4‚Ä¶ in which you can call those methods but instead of returning actual duplicates\nthey return the same instances. Because there is only one instance of nil , false , true , 1 , 1.0 , etc in your ruby app. Rails framework extends every Object with duplicable? method which tells if you can safely\ncall dup and not get an exception sometimes. It‚Äôs interesting how duplicable? is implemented . First, they start by saying you can dup an Object . And then it is dynamically checked if that‚Äôs actually true for some known exceptions\nsuch as nil etc. As can see the return value of nil.duplicable? will actually depend on\nthe Ruby version you are running on. true or false is not hardcoded (what\nI expected) but rather dynamically probed. In the case of TypeError exception,\nthe method is overwritten in that specific class. However, for some reason for a few classes, a different strategy is used\nby explicitly returning true or false without such check. But the most interesting part is around Symbol . Because Ruby 2.4 is a bit weird and a literal symbol can be duplicated\nbut dynamic one cannot. Unless the dynamic one is the same as a literal\none created before it‚Ä¶ yep‚Ä¶ Frankly, I am not sure if that‚Äôs a bug or expected behavior.\nI see no reason why the dynamic symbol could not return itself\nas well. Maybe dup is suppose to preserve some constraints\nthat I am not aware of‚Ä¶ I enjoyed reading the explanation why ActiveSupport even attempts to implement\nthose methods. Quoting the documentation itself. Most objects are cloneable, but not all. For example, you can‚Äôt dup methods: Classes may signal their instances are not duplicable removing dup / clone or raising exceptions from them. So, to dup an arbitrary object you normally\nuse an optimistic approach and are ready to catch an exception, say: Rails dups objects in a few critical spots where they are not that arbitrary.\nThat rescue is very expensive (like 40 times slower than a predicate), and it\nis often triggered. That‚Äôs why we hardcode the following cases and check duplicable? instead of\nusing that rescue idiom. So it is a performance optimization inside the framwork‚Äôs critical paths. Remember\nthat optimizing exceptions in you web application most likely won‚Äôt have any\nmeaningful impact . Check out our Junior Rails Developer course. You will enjoy our upcoming Rails DDD Workshop (25-26th May 2017, Thursday & Friday, Lviv, Ukraine. In English)\nwhich teaches you techniques for maintaining large, complex Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-03-07"},
{"website": "Arkency", "title": "Ruby code I no longer write", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/02/ruby-code-i-no-longer-write/", "abstract": "When we learn programming languages and techniques we go through certain phases: etc. Similarly with other things we enjoy in our life such as ice cream, pizza and sunbathing :)\nWe learn to enjoy them, we try too much of it and learn the consequences. Hopefully\nsome time later we find a good balance. We know, how much of it, we can use without hurting ourselves. I think we can have a similar experience in programming for example when you find out about\nmetaprogramming, immutability, unit testing, DDD. Basically anything. We often need to hit\nan invisible wall and realize that we overdosed. It‚Äôs not easy at all to realize it and learn\nfrom it. After 8 years of using Ruby and Rails, there are certain constructs that I try not to use anymore\nbecause I believe they make maintaining large applications harder. Here is a piece of code I wrote some time ago. I am sure that, as most developers, you will find it‚Ä¶ unusual. After all\nthe code can be written as: However, there are certain problems with this construct. It‚Äôs a bit more insecure. If kind comes from external sources we\naccidentally allow calling other methods which end in _pdf . Granted,\nyou might think that in such case this should be prevented by different layer\nand perhaps you would be right. But a much bigger issue for me is that this code is hard to refactor, hard to grep .\nIf I try to find usages of zebra_pdf method before refactoring it, I won‚Äôt find out that pdf_of_kind is using it. If your codebase is small or you don‚Äôt have too many of such\nconstructs, it doesn‚Äôt hurt much. But the larger the code is, the more you use it,\nthe more you will find it is hard to change easily. Perhaps you read it as a sign that\nI miss static typing and you would be right. After so many years with Ruby, I miss the\npowerful refactoring tooling that comes with statically typed languages. Rubymine can do\na lot, but there are limits to its features. The surface of pdf_of_kind method is infinite. There is an infinite number of things it\ncan do. I don‚Äôt handle infinite very well :). If you look more deeply at the code you will\nrealize that it can do 3 things possibly. Run two methods or raise an exception in case of an incorrect\nargument (invalid kind ). However, to find it out you need to look a bit more deeply. With the first\nimplementation that I showed you, you quickly and easily see the limited scope of the function. Here is another similar example. The purpose of this code is to map certain data computed in a report to salesforce columns\nthat will be updated. Easy peasy. However, again the number of columns that we will update is limited and predefined.\nThere is no need for the mapping to be dynamic (computed based on gsub and underscore )\ninstead of static (defined as A => B once). How would I write this code now: Now the fill_keys_with_values method even seems unnecessary and could be refactored\ninto using inject and the inheritance seems excessive. If I ever want to find out where is currency_iso_code used, I will know about\nits usage in AccountMapper . What makes this refactoring possible? The fact that we have a limited and predefined\nnumber of attributes that we need to map between. If the number was unlimited then\ndynamic transformation would be the only possible solution and the right one. But\nthere is no need for it if you know upfront about all possibilities. Similarly, I try to avoid ActiveSupport extensions to String such as constantize or underscore to find ruby classes or build CSS classes.\nI prefer explicit mapping from Abc::Xyz to abc_xyz or whatever. That way when\nI Remove Xyz class I can also find the mapping and remember to remove other parts\nof code related to what I am removing. On one hand Rails conventions are convenient, but on the other hand, whenever I refactor\nsomething I need to search for Abc::WhateverXyz , WhateverXyz , whatever_xyz and\nprobably, even more, variations to be sure that everything is going to work. The more\ndynamic and meta your code is, the more such cases you will have. So I try to limit\nthose situations when the mapping is, in fact, predefined and limited. There are even more conventions in Rails which make refactorings harder. For example\nthe usage of instance variables when rendering views that we described in Fearless Refactoring: Rails Controllers . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-02-17"},
{"website": "Arkency", "title": "Why classes eventually reach 50 columns and hundreds of methods", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/03/why-your-classes-eventually-reach-50-columns-and-hundreds-of-methods/", "abstract": "There are dozens of small or bigger problems that we can have in our code. Like diseases, they affect our applications and make them harder to maintain, expand and enjoy. They give us a headache and they give bugs to our customers. As a result, we (programmers) read a lot to find out more about the symptoms (code smells) and treatment (refactoring techniques, other languages, other architectures). One of the most common issues that I see is that classes tend to grow bigger and bigger. In terms of Rails Active Record models, they get new columns/attributes and methods around them. I usually see it in User class, but not only. It really depends on what your system is all about. If you work on an e-commerce application it can be Shop or Product class. If you work on HR application, it can be Employee . Think about the most important classes in your system and there is a big chance they keep growing and growing in columns and responsibilities. Reading about ‚ÄúBounded Contexts‚Äù, on of the strategical DDD pattern, allowed me to be better understand one of the potential cause of the problem. We hear the same word from many people, used in different situations and we identify the word with the same class. After all, that‚Äôs what we were often thought in school. You know, the noun/verb mapping into classes/methods. Let me show you 2 slides from one of my favorite presentation . It was an eye-opener for me. So‚Ä¶ This is what we often think we have. A class (or entity). Just Product . Possibly with dozens of attributes and methods. This is what we could have. Sales::Product , Pricing::Product , Inventory::Product . Separate classes with separate storage (just separate tables for start), with their own attributes. When doing such split there are multiple heuristics you can apply to determine how to split a class. Think which attributes change together in a response to certain actions. Check who changes those attributes. Quantities can change because of customer‚Äôs purchases but pricing is only changed by Merchants. If you are working on discounts or promotions which are bound to certain conditions, perhaps that itself is reaching a level of complexity when you realize I have a Pricing context in my application . When you catch yourself working on tracking inventory status, writing business rules about which delivery services provider should be used for which products, how it depends on weight or other product attributes then‚Ä¶ Maybe you just started implementing an Inventory module and it owns certain information about products, which help make all those computations. So perhaps it is time to have Inventory::Product . I know that understanding and putting boundaries in your application can be hard. Especially when you are just starting to work on an application. There is a lot of changes, a lot of new knowledge coming every day. But over time we (developers) should become more careful, more attentive to what is happening to our system. Sometimes we just add one more attribute, just one more column because we don‚Äôt see how it fits the whole system at that time. I understand it. But before a class reached 50 columns, there were 40 occasions to review its design, its responsibilities. To see whether some of those attributes and methods now form a cohesive, meaningful object which could be extracted into a separate class. To wonder whether our systems is reaching a level of complexity when we should seriously think about its modularization. It does not necessarily mean micro-services. You can start with namespaces in your monolith. You can start be splitting those obese classes into smaller ones. I don‚Äôt recommend doing it in early stages of the project. It is easy to get things wrong unless the business which came to you already have years of experience from developing previous versions of their software. My recommendation would be to stay vigilant, try to notice those patterns in your application. Introduce modularization when the business is a bit more mature, but perhaps before you have 50 columns problem :) P.S. If you want to learn more about using DDD techniques to solve problems in Rails apps check out our upcoming workshop, 25-26th May 2017 (Thursday & Friday), Lviv, Ukraine, in English . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-03-31"},
{"website": "Arkency", "title": "Reliable notifications between two apps or microservices", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/04/reliable-notifications-between-two-systems/", "abstract": "Let‚Äôs say you have 2 systems or microservices (or processes). And one of them needs to be notified when something happened in another one. You might believe it is not so hard unless you start thinking about networking, reliability, and consistency. I would like to briefly present some patterns for how it can be done and what do they usually bring to the table. It all works nicely until system B is down and non-responsive. In such case, it won‚Äôt be notified about what happened in B so we have a discrepancy. Assuming we have some kind of error reporting (and it worked at that moment) a developer can be notified about the problem and try to fix it manually later. This, however, could be easily fixed, couldn‚Äôt it? Let‚Äôs just contact system B inside the DB transaction, instead of outside. Some developers believe this a perfect solution, but they forget about one corner case that can still occur. Imagine that system B received your message (HTTP request) but you didn‚Äôt receive a response (because networking is not reliable).  In such case, there will be most likely an exception in system A. It will rollback a DB transaction and pretend that nothing happened. But system B assumes it did happen. So we have a discrepancy again. Also, this situation might not happen just because the response did not get back. There are other cases where the final effect is the same. HTTP request was sent, but an application process was killed, or server turned off. Or there was a bug in a code (if there is such code) between sending the request and committing the DB transaction). I believe however that all those situations combined are less likely than server B just being unavailable. So probably this is better than v1. But still not perfect. a) System A takes jobs from queuing system and sends them to system B. Jobs can be retried in case of failure. or b) System B takes jobs from queuing system and processes them Jobs can be retried in case of failure. In this situation, we introduced an external queuing system such as Kafka, RabbitMQ or redis. I called it external because the storage mechanism is using a different database then the application itself (which assume SQL DB). Also depending on the situation, it might be your system (but another process, like a background workers solution) taking jobs from the queue and pushing them further. Or it might be that another micro-service (system B) takes the jobs and processes them. Notice that by introducing a queueing system in the middle and retries we changed the semantics from at-most-once delivery to at-least-once delivery. It‚Äôs still not all roses, however. We don‚Äôt contact a separate system directly now, but we contact a separate database. With exactly the same potential pitfalls. What if we rollback after pushing to the queue? What if we pushed to the queue, but we didn‚Äôt receive a confirmation and rolled-back in SQL? All the same situations can happen. But because we assume those servers running queues are closer to us, we also assume the likelihood of such problems happening is much lower. But still not zero. In my system, it happened 10 times in one month. Also, the assumption that both DBs are very close to each other is not always correct in modern world anymore. If you use hosted redis or hosted X there is a big chance they are going to be in the same region, but not necessarily the same availability zone. To summarize. Thanks to retries we are safe from system B failures but we can still encounter problems on our side. Ultimately the only safe solution is to use only one database only which would be the same SQL database. a) System A (another thread or process) takes jobs from the internal queuing system and sends them to system B. or b) System A (another thread or process) takes jobs from the internal queuing system and moves them to the external queuing system, where system B takes them from. In this case, we save jobs info about what we want to notify external system about in the same SQL DB we store application state in. We can safely commit or rollback both of them together. Then we either have background workers pulling from the same DB (internal queue) and communicating with system B or pushing those jobs to the external queue such as Kafka or RabbitMQ (one reason for that is there might be more systems than just B interested in this data). I am tempted to say that this gives you 100% reliability but probably that‚Äôs not true and I am just missing a case where it can fail :) Anyway, this is probably the safest solution. But it requires more monitoring. Not only you watch for system B, for the external queue, but now you also need to watch the thread or process moving data from the internal to the external queue. How do you solve those problems in your system? Which solution did you go with? I think some apps just ignore them and handle such issues manually (or not at all), because they are not crucial. But many things that I work on, handle monetary transactions, so I am always cautious when thinking about such problems. As you can see there are many ways system A can notify B about something (notice that we are talking about notifications, where A is not immediately interested in a response from B, just that it got the message and they are both in sync about the state of the world). You can do it directly, you can introduce external queues, you can have internal queues in the same DB or you can even go with both queues if you find it worthy of the cost of DevOps. Are you working on more advanced Rails Apps? Register for our upcoming workshop in May, in Lviv to learn and practice more techniques, beyond service objects, which will help you organize your code. Examples of internal queues: Examples of external queues: Other: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-04-09"},
{"website": "Arkency", "title": "All Rails service objects as one Ruby class", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2017/04/all-rails-service-objects-as-one-ruby-class/", "abstract": "I review many Rails applications every month, every year. One visible change is that service objects became mainstream in the Rails community . This makes me happy, as I believe they do introduce some more order in typical Rails apps. Service objects were the main topic of my ‚ÄúFearless Refactoring: Rails controllers‚Äù book, along with adapters, repositories and form objects. Today I‚Äôd like to present one technique for grouping service objects. I was reminded of this technique, when I‚Äôve recently came back to the development of the simple Fuckups application. This application started as a simple CRUD, typical Rails Way. Over time, though, I‚Äôve added the service layer. \nIn this application I went with keeping the service objects, as one class, instead of the usual - one service object == one class.\nThis is not my usual technique and I‚Äôm not sure I‚Äôm recommending it. However it has some nice features, so I thought it would be worth sharing. Basically, the whole service layer (or as I like to call it in more DDD-style, the application layer) is one class here. The class is called App . The app layer defines its own exceptions so they‚Äôre declared at the top. After that it has a number of public methods, each responsible for handling one user request. I‚Äôve followed one important rule here - the service layer knows nothing about http-related stuff . This is left to controllers to handle. Rails is great at this. From a Rails controller point of view, this is quite simple, leaving the controllers very thin: As you may notice, the authentication (who are you?) part is handled at the controller level, while the authorization (can you do that?) is part of the service layer. Authentication is the usual dilemma, as it‚Äôs not clear where it belongs. I like to think about it as the app layer. But given the usual coupling between authentication (hello Devise) and Rails controllers, this is usually the last part to decouple, if ever. Authorization feels much more in the app layer, that‚Äôs why it‚Äôs here too. What‚Äôs inside the report_fuckup method then? Well, so this is not the usual service object, as it‚Äôs extended with more things (event_store). Let me first start with the more typical stuff. All the service objects accept params which are primitives, or at least are not Rails objects . This is important to decouple them at this level from Rails. Passing user_id is more than enough, as we can retrieve the data on our own.\nThe first part is authorization. We need to ensure you belong to the organization where you try to report the fuckup to. (It might be a good idea somewhere in the future to submit fuckups to foreign organizations, but it‚Äôs not in the scope yet).\nThen we use the normal ActiveRecord associations to create the database record. There‚Äôs no validations here, so nothing to check. Depending on ActiveRecord here is another dilemma. It‚Äôs not perfect here. It would be nicer if we just called fuckuops_repo.create but it‚Äôs not there (yet?). I left the persistence layer here, without any repo objects. Mostly due to lack of time for this effort, as it would be nice here. The last part is the unusual part. This is where the app starts to become beyond service objects . This is where the app starts to be more Domain-Driven Design in its architecture. We publish an event here and store it. Events were not meant to be in the scope of this blogpost, but as a sneak-peek, here they are for this app: And here is an example test at the app layer: If the app itself sounds interesting to you, it‚Äôs free to use at http://fuckups.arkency.com/ . If you like this style of organizing the Rails code, then you may like my book: ‚ÄúFearless Refactoring: Rails controllers‚Äù . If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-04-09"},
{"website": "Arkency", "title": "Conditionality is filtering. Don't filter control flow, filter data.", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/04/conditionality-is-filtering-dont-filter-control-flow-filter-data/", "abstract": "I am not that smart. I didn‚Äôt say it. Michael Feathers did.\nBut it got me thinking and I know it inspired my colleagues as well. Conditionality is filtering. Don‚Äôt filter control flow, filter data. A few weeks ago our client decided to change the URL structure of some of the\nmost important pages. There were a few reasons to do it. They don‚Äôt matter so much right now.\nThe business just decided to prioritize one kind of benefits over another kind. They turned\nout to be more important in the long term. Instead of slugs provided by organizers such as /wrocloverb2016/ we now generate them ourselves based on the name of the event, its id, browser language,\nsome translations so it looks like /e/wroc-love-rb-2016-tickets-111222333 . But, of course, for a few months, we need to support old URLs and continue to\nredirect them to new URLs. We implemented it as redirects in routing.\nUnfortunately, they need to query database to find matching events to know\nthe new URL, the redirect should point to, but I think that‚Äôs acceptable. Our platform also needs to support multiple languages and the URLs are a bit different\nin every language (the word ‚Äútickets‚Äù is translated). And we don‚Äôt need this redirection\nfeature for new events which will only use the new URL structure. This is the solution. The line that I wanted to show you is: We use routing-filter gem and when the\nURL path is /whatever/ then path_params[:locale] is nil and we use the default language \nof current country (which we know based on the domain). If it is /es/whatever then path_params[:locale] is es and we know that the user wants to see the page in\nSpanish. We normally recognize it and set I18n.locale in ApplicationController but it works in routing as well if you need it. So there is nothing super unusual or fantastic in this line of code,\nexcept that I originally wanted to write it as: But the tweet which stayed in my mind and the Don‚Äôt filter control flow, filter data phrase\nmade me do it differently. I decided to filter data . All data. Even if \"e\" or slugged is\nnever empty. A bit more in a functional style. It is a trivial example but if you want to see how far you can go with it,\nI recommend watching Norbert‚Äôs talk . For bonus, you\ncan learn more about scurvy ;) No wonder that Enumerable methods are\noften favorites of best Ruby developers. I think one of the main reasons is that they often allow us to\navoid a bunch of if-statements. Instead, we can easily filter the data we work on. There is a similar pattern in Redux reducers, especially when they are combined . When an action is dispatched, the reducer does not think whether todos or counter should\nreact to it. The action is passed to both of them and sometimes one of them decides to do nothing\nand keeps its current state. It filters out uninteresting actions. BTW. Our book Fearless Refactoring: Rails controllers contains a chapter\nabout Extract routing constraint technique (as well as many others) that you might be interested in, as well. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-04-21"},
{"website": "Arkency", "title": "What's inside the Rails DDD workshop application?", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2017/05/whats-inside-the-rails-ddd-workshop-application/", "abstract": "An integral part of our Rails DDD workshops is an application which we use during the teaching and exercises process. Many people have asked what‚Äôs inside the app, so I have prepared a small sneak-peek. Let‚Äôs start with the UI to see the scope of the app. There are typical Rails scaffold CRUD UIs for customers and products, respectively: In the above screens we can manage and prepare customers and products, which will be used in other parts of the system. The order list screen lets us review the orders, which is the main part of the system: As you can see, there are some features of what we can do with the order: pay, ship, cancel, history. Creating a new order screen displays the existing products and lets us choose a customer. This screen simulates the payment, to show how we can integrate with external API. The history view shows the events related to that order, which makes debugging easier, we can the whole history here. Given that this app helps learning DDD, you could expect some interesting domain layer, right? In this case there are 2 domain-rich bounded contexts, each of them represented as a Ruby namespace: and there are Products and Customers which we could probably also call like Catalog and CRM respectively, but here they are just CRUD contexts, without much logic. We‚Äôve used the Product and Customer ActiveRecord-driven CRUDs to represent how such things can cooperate with domain-rich bounded contexts. We also have one saga (or process manager, depending on the definition), called Discount . There‚Äôs also a projection, called PaymentsProjection . In the spirit of CQRS, we handle the ‚Äúwrite‚Äù part with Commands. Everything is based on events, through which the different contexts communicate with each other. There are aggregates for Payment and for Order . All the domain logic of the application is fully tested. The app is a nice example of a non-trivial code which is using the RailsEventStore ecosystem of tools. The code is hosted on GitLab. Once you get access there, you will also see a list of Issues. Each issue is actually an exercise to let you practice DDD, based on this app. Several of those exercises are what we expect you to do, during the workshops (with our support and help). I hope this blogpost answers some questions and can help you evaluate whether our Rails DDD workshops are of value to you. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-05-03"},
{"website": "Arkency", "title": "The vision behind Rails, DDD and the RailsEventStore ecosystem", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2017/05/the-vision-behind-rails-ddd-and-the-railseventstore-ecosystem/", "abstract": "Arkency became known for our DDD efforts in the Rails community. DDD together with CQRS and Event Sourcing helped us dealing with large Rails apps. At some point we also started open-source tooling to support introducing DDD in Rails apps. This blogpost aims to highlight where we started, where we are and what is the vision for the future, for the RailsEventStore ecosystem. The journey with DDD at Arkency started probably around ~6 years ago, when we started using technical patterns like service objects (in DDD we would call them application services), adapters and repositories. This phase resulted in writing the ‚ÄúFearless Refactoring: Rails Controllers‚Äù ebook which is all about those patterns. Those patterns helped, but didn‚Äôt solve all of our problems. We could say, that service objects were like a gateway drug - they enabled us to isolate our logic from the Rails app. The patterns from the book are helping with one big mission - how to separate the Rails part from your actual application. Then we also help to structure your application with the app/infra layer and the domain layer. This is the real value of that phase. The next phase, the DDD phase is then more about how to structure the domain. If you want to watch more about this journey from service objects to DDD - watch our conversation with Robert, where we talked a lot about this evolution. When I met Mirek and when Mirek has joined Arkency it was a fast progress with our understanding of DDD. You can read books, read blogposts, even try to write some simple prototypes, but having access to someone who already knows all of it is just priceless.  Our adoption of DDD, CQRS and Event Sourcing was at full speed. In one of our biggest client projects, we have introduced the concept and the implementation of an Event Store. At the beginning it was just a simple table which stores events, wrapped with ActiveRecord. This enabled us to publish events and subscribe to them. Also this created the Event Log capabilities. This was the time, when we thought we could help other people with existing Rails apps to introduce domain events, which we believed (and still believe) to be a great first step to better structure in Rails apps. We‚Äôve started publishing more blogposts, but we also started 2 open-source projects: With HttpEventStore our vision was to make it easy to use the so-called Greg‚Äôs Event Store (or GetEventStore, or GES) from within a Ruby or Rails app. We have released some code and it gained traction. Some people started using it in their production apps, which was great. We also got a lot of help/contributions from people like Justin Litchfield or Morgan Hallgren who became an active contributor. With RailsEventStore the main goal at the beginning was to be as Rails-friendly as possible. The goal was to let people plug RES in very quickly and start publishing events. This goal was achieved.\nAnother goal was to keep the API the same as with HttpEventStore, with the idea being that once people need a better solution than RES they can quickly switch to HES. This goal wasn‚Äôt accomplished and at some point we decided not to keep the compatibility. The main reason was that while HES was mostly ready, the RES project became bigger and we didn‚Äôt want it to slow us down. Which in the hindsight seems like a good decision. Fast forward, where we are today. The ecosystem of tools grew to: RailsEventStore is the umbrella gem to group the other gems. The CommandBus is not yet put into RES, but it will probably happen. We have also established development practices to follow in those projects with a strong focus on TDD and test coverage. We‚Äôre using mutant to ensure all the code is covered with tests.  It‚Äôs described here: Why I want to introduce mutation testing to the rails_event_store gem and here: Mutation testing and continuous integration . Education-wise we encourage people to use DDD/CQRS/ES in their Rails apps. It‚Äôs not our goal to lock-in people with our tooling . On one hand, tooling is a detail here. On the other hand, an existing production-ready tooling makes it much easier for developers to try it and introduce it in their apps. Arkency people delivered many talks at conferences and meetups, where we talk about the ups and downs of DDD with Rails. We also offer a commercial (non-free) Rails/DDD workshops . A 2-day format is a great way to teach all of this at one go. As an integral part of the workshop we have built a non-trivial Rails DDD/CQRS/ES applications which shows how to use DDD with Rails, but also with the RailsEventStore ecosystem. The workshop comes with an example Rails/CQRS/DDD application which does show all the concepts. The application also contains a number of example ‚Äúrequirements‚Äù to add by using the DDD patterns. Also, there‚Äôs a video class which I recorded (about 3 hours) which is about using Rails, TDD and some DDD concepts together. Hands-on Ruby, TDD, DDD - a simulation of a real project As for our client projects, we now use DDD probably in all of them. At the beginning we‚Äôve only used DDD in legacy projects, but now we also introduce DDD/CQRS/ES in those projects which we start from scratch (rare cases in our company). In majority of those apps we went with RailsEventStore. CQRS or DDD are not about microservices , but the concepts can help each other. In some of our projects, we have microservices which represent bounded contexts. This adds some infrastructure complexity but it also does bring some value in the physical separation and the ability to split the into smaller pieces. To summarise where we are: Things are changing really fast so it‚Äôs hard to predict anything precisely. However, all signs show that Arkency will keep doing DDD and Rails apps. This naturally means that we‚Äôll do even more education around DDD and about solving typical problems in Rails apps. We‚Äôll also work on the RailsEventStore ecosystem of tooling . We want the tooling to stay stable and to be reliable. I put education at the first place, as our offer it‚Äôs not about ‚Äúselling‚Äù you some tooling. We do have the free and open-source tools in our offer, but we care more about the real value of DDD - using the Domain language in the code, shape the code after discussions with Domain Experts. The tooling is irrelevant here. It helps only to provide you some basic structure but the real thing is your app. We want to focus on helping you split your application into bounded contexts. We want to help you understand how to map requirements into code. That‚Äôs the big value here. If our tooling can help you, that‚Äôs great. We have already gathered a small but very passionate community around the DDD ideas. The important thing here - it‚Äôs a community around DDD, not a community around RailsEventStore or any kind of specific tooling. We‚Äôre learning together, we help each other. At the moment the community doesn‚Äôt have a central place of communication, but we‚Äôre thinking about improving this part. Even further in the future? One thing which I was sceptical in the past is microservices . Whenever we were suggesting any ideas how to improve Rails apps, microservices were rarely among the techniques. The thing is - microservices represent an infrastructural split, while what‚Äôs more important is the conceptual split. This has changed a little bit recently. I see the value in well-split microservices. After understanding the value of Bounded Contexts, aggregates, read models - I can now see much better that the the split is the same as with Bounded Contexts. If you do more DDD, you‚Äôll notice how it emphasises good OOP - the one were attributes are not publicly exposed, where object tell, don‚Äôt ask. Where messages are used to communicate. Where you can think about aggregates as objects or read models as objects. You will also notice how good OOP and good Functional Programming are close to each other and how DDD/CQRS/Event Sourcing exposes it. Aggregates can be thought as functions. They are built from events and they ‚Äúreturn‚Äù new events. A lot is being said about functional aggregates . Read models can be thought as functions - given some events, they return some state. Sagas can be seen as functions, given some events, they return commands. Rails + DDD + CQRS + ES +OOP + FP == that‚Äôs a lot of buzzwords, isn‚Äôt it? It‚Äôs good to be able to name things to communicate between developers and understand the patterns by their name. But the buzzwords is not the point. Again, it‚Äôs all about delivering business value in a consistent manner. Let me throw another buzzword here - serverless . It‚Äôs a confusing name for a concept that is relatively simple. It‚Äôs about Functions as a Service, but also about a different way of billing for the hosting. How is that relevant to Rails and DDD? Well, if you work on a bigger Rails app, then hosting is a big part of your (or your client) budget. Whether you went with a dedicated machine or you went cloud with Heroku or Engine Yard or anything else, this all cost a lot of money, for bigger traffic and bigger data. Making your Rails app more functional by introducing Aggregates, Read models, sagas enables you to benefit from lower costs using the serverless infrastructure. Splitting your app into smaller infrastructural pieces also enables you to experiment with other technologies which are trending in our community recently - Elixir , Clojure, Haskell, Go, Rust. Instead of having a big debate whether to start a new app in one of those languages (and probably risking a bit), you can now say - ‚Äúlet‚Äôs build this read model in Elixir‚Äù - this is something much easier to accept by everyone involved! This part a bit science-fiction so far, but as part of my preparation to the next edition of the Rails/DDD workshops in Lviv , I started researching those topics more. At the workshop, we‚Äôll have a discussion about it. I‚Äôm not sure about you, but I‚Äôm very excited about the state of the Rails and DDD ecosystem and I‚Äôm excited about the upcoming possibilities. I‚Äôm very happy to be part of the changes! Thanks for reading this blogpost and thanks for supporting us in our efforts! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-05-07"},
{"website": "Arkency", "title": "Passive aggresive events - code smell", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2017/05/passive-aggresive-events-code-smell/", "abstract": "Today, while sitting on our Rails/DDD workshops led by Robert in Lviv, I was thinking/preparing a design of the new aggregates in my project. Robert was just explaining aggregates and how they can communicate (with events). During the break, I asked Robert what he thinks about it and he mentioned a term, that I missed somehow. The term was coined by Martin Fowler in his What do you mean by ‚ÄúEvent-Driven‚Äù? article. Here is the particular quote: ‚ÄúA simple example of this trap is when an event is used as a passive-aggressive command. This happens when the source system expects the recipient to carry out an action, and ought to use a command message to show that intention, but styles the message as an event instead.‚Äù In my case, it was a situation, where I have a Company aggregate and when it receives an external request to ‚Äúchange_some_state‚Äù it has to delegate it to its ‚Äúchildren‚Äù objects. Those objects are just value object in the aggregate, but they are also aggregates on their own (as separate classes). The design was split into smaller aggregates with hope of avoiding Your Aggregate Is Too Big problem. I agree that with the approach I have planned my events are a little bit passive-aggresive and they sound more like commands. I will either live with that (but be aware of the trap) or I will consider using the Saga concept here (events as input, command as output). BTW, the whole article by Martin Fowler is worth a read . How do you deal with such problems in your DDD apps? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-05-25"},
{"website": "Arkency", "title": "Self-hosting Event Store on Digital Ocean", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2017/05/self-hosting-event-store-on-digital-ocean/", "abstract": "Recently in one of our projects, we have decided that it would be a good idea to switch to EventStore . Our current solution is based on RailsEventStore (internal to each Bounded Context) and an external RabbitMQ to publish some event ‚Äúglobally‚Äù. This approach works, but relying on EventStore sounds like a better approach. For a long time, we felt blocked, as EventStore doesn‚Äôt offer a hosted solution and we were not sure if we want to self-host (in addition to the current heroku setup). Luckily, one of the Arkency developers, Pawe≈Ç, was following the discussion and quickly timeboxed a solution of self-hosting Event Store on Digital Ocean. It took him super quick to deliver a working node. This enables us to experiment with partial switching to EventStore. I have asked Pawe≈Ç to provide some instructions how he did it, as it seems to a very popular need among the DDD/CQRS developers. Here are some of the notes. If it lacks any important information, feel free to ping us in the comments. Those are the instructions for the basic setup/installation. You can now start experimenting with EventStore. For production use though you‚Äôd need to invest in reliability (clustering, process supervision and monitoring) as well as in security. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-05-24"},
{"website": "Arkency", "title": "Handling SVG images with Refile and Imgix", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/06/handling-svg-images-with-refile/", "abstract": "My colleague Tomek today was responsible for changing a bit how we\nhandle file uploads in a project so that it can support SVG logos. For handling uploads this Rails app uses Refile library. And\nfor serving images there is Imgix which helps you save bandwith\nand apply transformations (using Imgix servers instead of yours). The normal approach didn‚Äôt work because it did not recognize SVGs\nas images. So instead we had to list supported content types manually. There is also a bit of logic involved in building proper URL for\nthe browser. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-06-01"},
{"website": "Arkency", "title": "Testing cookies in Rails", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/2017/06/testing-cookies-in-rails/", "abstract": "Recently at Arkency I was working on a task, on which it was very important to \nensure that the right cookies are saved with the specific expiration time. Obiovusly \nI wanted to test this code to prevent regressions in the future. Firstly I thought about controller tests , but you can use only one controller in\none test (at least without strong hacks) and in this case it was important to check\nvalues of cookies after requests sent into few different controllers. You can now think, that controller\ntests are ‚Äúgood enough‚Äù for you, if you don‚Äôt need to reach to different controllers. Not quite, unfortunately .\nLet‚Äôs consider following code: And controller test: Note that the cookie time has expiration time of 30 minutes and we are doing second call\n‚Äúafter‚Äù 35 minutes , so we would expect output to be: So, we would expect cookie to be empty, twice. Unfortunately, the output is: Therefore, it is not a good tool to test cookies when you want to test cookies\nexpiring. My second thought was feature specs , but that‚Äôs capybara and we prefer to avoid capybara if we can\nand use it only in very critical parts of our applications, so I wanted to use something lighter than that.\nIt would probably work, but as you can already guess, there‚Äôs better solution. There‚Äôs another kind of specs, request specs , which is less popular than previous two, but in this\ncase it is very interesting for us. Let‚Äôs take a look at this test: With this test, we get the desired output: Now we would like to add some assertions about the cookies. Let‚Äôs check what\ncookies class is by calling cookies.inspect : Great, we see that it has all information we want to check: value of the cookie,\nexpiration time, and more. You can easily retrieve the value of the cookie by calling cookies[:foo] . Getting expire time is more tricky, but nothing you couldn‚Äôt do in ruby. On HEAD of rack-test there‚Äôs already a method get_cookie you can use to get all cookie‚Äôs options.\nIf you are on 0.6.3 though, you can add following method somewhere in your specs: It is not perfect, but it is simple enough until you migrate to newer version of rack-test . In the end, my specs looks like this: With these I can test more complex logic of my cookies. Having reliable tests\nallows me and my colleagues to easily refactor code in the future and prevent\nregressions in our legacy applications (if topic of refactoring legacy applications\nis interesting to you, you may want to check out our Fearless Refactoring book ). What are your experiences of testing cookies in rails? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-06-02"},
{"website": "Arkency", "title": "Acceptance testing using actors/personas", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/06/acceptance-testing-ruby-using-actors-personas/", "abstract": "Today I‚Äôve been working on chillout.io (new landing page coming soon).\nOur solution for sending Rails applications‚Äô metrics and building dashboards.\nAll of that so you can chill out and know that your app is working. We have one, almost full-stack, acceptance test which spawns a Rails app, a thread listening to\nHTTP requests and which checks that the metrics are received by chillout.io when\nan Active Record object was created. It has some interesting points so let‚Äôs have a look. The test has higher-level abstractions, which we like to call Test Actors.\nIn our consulting projects we often introduce classes such as TestCustomer or TestAdmin or TestMerchant , even TestMobileApp and TestDeveloper etc.\nThey usually encapsulate logic/behavior of a certain role.\nTheir implementation detail varies between project. Sometimes they will use Capybara and one of its drivers. That can usually happen\nat the beginning when we join a new legacy project, which test coverage is not\nyet good enough. In that case, you can build helper methods that will navigate\naround the page and perform certain actions. This style allows you to build a story and hide a lot of implementation details.\nUsually, defaults are provided either in terms of default method arguments: or as instance variables filled by previous actions which is useful if you have a multi-tenant application and most of your scenarios\noperate in one tenant/country/shop/etc but sometimes you would like to test how\nthings behave if one merchant has two shops or if one customer buys in two different\ncountries/currencies etc. The instance variables will usually contain primitive values. Either identifier (id or slug) of something that was done or a value filled out in a form which can be later used to find the relevant object again. but sometimes it can be a simple struct if that‚Äôs useful for subsequent method calls. In some cases, those actors will directly (or indirectly through factory girl) create some Active Record models. That is the case where we don‚Äôt have UI for some settings because they are rarely changed. In other cases an actor will build a command and pass it to a service object or command bus . This is a case where we feel that we don‚Äôt\nneed (or want to because they are usually slow) to use the frontend\nto test the functionality. I like this approach because such actors can remember certain default attributes\nand fill out the commands with user_id or order_id based on what they did.\nThat means you don‚Äôt need to keep too many variables in the test. These personas\nhave a memory. They know what they just did :) If an actor plays a role of a mobile app which uses the API to communicate with\nus, then the methods will call the API. So let‚Äôs get back to the acceptance test of our chillout gem which is done in a similar style and see what we can find inside. Let‚Äôs start with TestEndpoint which plays the role of a chillout.io API server. It can run a very simple rack-based server in a separate thread. When there is an API request to /metrics endpoint it saves the payload on in a Queue , a thread-safe collection. It is also capable of checking whether there is something received in the queue. Ok, but what about TestApp ? There is more heavy machinery involved. We start a full Rails application with\nchillout gem. The bbq-spawn gem makes sure that the\nRails app is fully started before we try to contact with it. It can do it based on a text which appears in the command output (such as INFO  WEBrick::HTTPServer#start: pid=400 port=3000 ). It can do it based on whether you can connect to a port using a socket. Or in our case based on whether it can send and receive a response to an HTTP request, which is the most reliable way to determine that the app is fully booted and working. There is also TestUser ( TestBrowser would be probably a better name) which sends a request to the Rails app. Together the story goes like this: If you enjoyed reading subscribe to our newsletter and continue receiving useful tips for maintaining Rails applications, plus get a free e-book as well. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-06-07"},
{"website": "Arkency", "title": "Test critical paths in your app with ease thanks to Dependency Injection", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/2017/06/test-critical-paths-in-your-app-with-ease-thanks-to-dependency-injection/", "abstract": "Dependency Injection is one of my favorite programming patterns. In this short blogpost, I‚Äôll present you how it helps testing potentially untestable code. Imagine that your customer wants to easily identify orders in the e-commerce system which you are maintaining. They requested simple numeric identifier in a very specific 9-digit format which will make their life easier, especially when it comes to discussing order details with their client via the phone call. They want identifier starting with 100 and six random digits, e.g. 100123456. Easy peasy you think, but you probably also know that the subset is limited to 999999 combinations and collisions may happen. You probably create a unique index on the database column, let‚Äôs call it order_number to prevent duplicates. However, instead of raising an error if the same number occurs again you want to make a retry. Let‚Äôs start with a test for the best case scenario And the simple implementation: The code looks fine, but we‚Äôre not able to easily verify whether retry scenario works as intended. We could stub Ruby‚Äôs Kernel#rand but we want cleaner & more flexible solution, so let‚Äôs do a tiny refactoring. Random number generator is no longer a private method, but a separate class RandomNumberGenerator . It‚Äôs injected to OrderNumberGenerator and the code still works as before. Instead of a default RandomNumberGenerator , for the testing purposes we pass simple lambda. Lambda pops elements from crafted array to cause intended unique index violation. As you can see, apart from being more confident about the critical code in our application due to having more test scenarios, we gained a lot of flexibility. Requirements related to order_number may change in the future. Injecting a different random_number_generator will do the job and core implementation of OrderNumberGenerator will remain untouched. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-06-07"},
{"website": "Arkency", "title": "Dogfooding Process Manager", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/2017/06/dogfooding-process-manager/", "abstract": "Process managers (sometimes called Sagas) help us with modeling long running processes which happen in our domains. Think of such process as a series of domain events. When enough of them took place (and the particular ones we‚Äôre interested in) then we execute a command. The thing is that the events we‚Äôre waiting for might take a longer time to arrive, during which our process manager has to keep track of what has been already processed. And that‚Äôs where it gets interesting. Consider following example taken from catering domain. You‚Äôre an  operations manager. Your task is to suggest your customer a menu they‚Äôd like to order and at the same time you have to confirm that caterer can deliver this particular menu (for given catering conditions). In short you wait for CustomerConfirmedMenu and CatererConfirmedMenu . Only after both happened you can proceed further.\nYou‚Äôll likely offer several menus to the customer and each of them will need a confirmation from corresponding caterers. If there‚Äôs a match of CustomerConfirmedMenu and CatererConfirmedMenu for the same order_id you cheer and trigger ConfirmOrder command to push things forward. By the way there‚Äôs a chance you may as well never hear from the caterer or they may decline, so process may as well never complete ;) Given the tools from RailsEventStore ecosystem I use on a daily basis, the implementation might look more or less like this: This process manager is then enabled by following RailsEventStore instance configuration: Whenever one of the aforementioned domain events is published by the event store, our process manager will be called with that event as an argument. Implementation above uses ActiveRecord (with dedicated table) to persist internal process state between those executions. In addition you‚Äôd have to run database migration and create this table. I was just about to code it but then suddenly one of those aha moments came. We already know how to persist events ‚Äî that‚Äôs what we use RailsEventStore for. We also know how to recreate state from events with event sourcing. Last but not least the input for process manager are events. Wouldn‚Äôt it be simpler for process managers to eat it‚Äôs own dog food? My first take on event sourced process manager looked something like this: When process manager is executed, we load already processed events from stream (partitioned by order_id ). Next we apply the event that just came in, in the end appending it to stream to persist. The trigger with condition stays unchanged since it is only the State implementation that we made different. In theory that could work, I could already feel that dopamine kick after job well done. In practice, the reality brought me this: Doh! I forgot about this limitation of RailsEventStore . You can‚Äôt yet have the same event in multiple streams. By contrast in GetEventStore streams are cheap and that‚Äôs one of the common use cases. Given the RailsEventStore limitation I had to figure out something else. The idea was just too good to give it up that soon. And that‚Äôs when second aha moment arrived! There‚Äôs this RailsEventStore::Projection mechanism, which let‚Äôs you traverse multiple streams in search for particular events. When one is found, given lambda is called. Sounds familiar? Let‚Äôs see it in full shape: Implementation is noticeably shorter (thanks to hidden parts of RailsEventStore::Projection ). Works not only in theory. And this is the one I chose to stick with for my process manager. I cannot however say I fully like it. The smell for me is that we peek into the stream that does not exclusively belong to the process manager (it does belong to aggregate into whose stream CustomerConfirmedMenu and CatererConfirmedMenu were published).\nAnother culprit comes when testing. Projection can only work with events persisted in streams, so it is not sufficient to only pass an event as an input to process manager. You have to additionally persist it. Would you choose event backed state for process manager as well? Let me know in comments! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-06-21"},
{"website": "Arkency", "title": "What I learnt today from reading gems' code", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/06/what-i-learnt-today-from-reading-gems-code/", "abstract": "Today I was working on chillout.io client and while I was debugging some parts, I had a look at some Ruby gems. This is always an interesting experience because you can learn how other developers design their API and how different it can be from your approach. So here are some interesting bits from sidekiq code. Quoting the documentation: Sidekiq::Client normally uses the default Redis pool but you may\npass a custom ConnectionPool if you want to shard your\nSidekiq jobs across several Redis instances‚Ä¶ I generally don‚Äôt like globals as a gem consumer but sometimes they are convenient and provide the convention over configuration magical feeling. The nice thing about this global is that you don‚Äôt need to use it. It is easily overridable with such constructor. If you have specific requirements, your own connection pool, special redis connection, multiple clients and multiple connections etc, etc, you can still get the work done. Going further with global which you don‚Äôt need to use. With this code, instead of you can do Again. No one forces you to use the class method. If for any reason, the first approach works better than the second, if you need to have a new instance with specific constructor arguments, do it. Sidekiq can handle both. This redis=(hash) setter can handle a Hash with redis configuration options or a Sidekiq::ConnectionPool instance. Quoting the documentation: Sidekiq has a similar notion of middleware to Rack: these are small bits of code that can implement functionality. Sidekiq breaks middleware into client-side and server-side. So the sidekiq client is the app (usually a Rails app) responsible for pushing jobs and scheduling them. Sidekiq server is the worker process that execute on a different machine for processing jobs in the background. Sidekiq needs to know which mode it is in, and it needs to have the ability to have different configurations for both of them. Especially considering that usually it is the same Rails application running either in client mode (http application server such as puma or unicorn) or server mode (worker process executed with sidekiq command). The configuration can be set such as: So the configure_server method yields the block only when the if-statement evaluates we are in a server process. It uses block for lazy configuration. It is not evaluated when unnecessary (in the client). server_middleware yields for nicer readability, I believe. Especially in the case of many middlewares. BTW. chillout.io client uses a middleware to schedule sending metrics when a background job is done. ActiveSupport::TaggedLogging wraps any standard Logger object to provide tagging capabilities. There is one method which brought my attention: I‚Äôve never seen this super if defined?(super) but it turns out it is useful to dynamically figure out if the ancestor defined given method and you should call it or this is the first module/class in inheritance chain which defines it. Also, check this out. new is not used to create a new instance of TaggedLogging (after all it is a module, not a class) that would delegate to the logger as one could expect based on the API. Instead it extends the logger object with itself. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-06-23"},
{"website": "Arkency", "title": "Tracking dead code in Rails apps with metrics", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/06/tracking-dead-code-with-metrics/", "abstract": "When you work in big Rails application sometimes you would like to remove certain lines of code or even whole features. But often, you are not completely sure if they are truly unused. What can you do? With chillout.io and other monitoring solutions that‚Äôs easy. Just introduce a new metric in the place of code you are unsure about. After you add a graph to your panel, you can easily configure an alert with notifications to Slack, email or whatever you prefer, so that you are pinged if this code is executed. Wait an appropriate amount of time such as a few days or weeks. Make sure the code was not invoked and talk to your business client, boss, CTO or coworkers to make the final call that the feature should be dropped. Now you have the arguments. We all know that unused code is burden for our whole team because we keep supporting it, refactoring (yes, sometimes we do renames or upgrades and we spend time on code delivering no value, don‚Äôt we?). Even because it keeps appearing in search results or occupying space in our mind. As Michael Feathers greatly explained No, to me, code is inventory.  It is stuff lying around and it has substantial cost of ownership. It might do us good to consider what we can do to minimize it. I think that the future belongs to organizations that learn how to strategically delete code.  Many companies are getting better at cutting unprofitable features in their products, but the next step is to pull those features out by the root: the code.  Carrying costs are larger than we think. There‚Äôs competitive advantage for companies that recognize this. Or as Eric Lee put it : However, the code itself is not intrinsically valuable except as tool to accomplish some goal.  Meanwhile, code has ongoing costs.  You have to understand it, you have to maintain it, you have to adapt it to new goals over time.  The more code you have, the larger those ongoing costs will be.  It‚Äôs in our best interest to have as little source code as possible while still being able to accomplish our business goals. Or as James Hague expressed it: To a great extent the act of coding is one of organization. Refactoring. Simplifying. Figuring out how to remove extraneous manipulations here and there. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-06-23"},
{"website": "Arkency", "title": "The easiest posts to write for a programming blog", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/07/the-easiest-posts-to-write-for-a-programming-blog/", "abstract": "Here are the 4 easiest types of posts that you can write about on your programming blog: The purpose of this list is not for you to always write easiest posts. But to know that not every post needs to be a bible, guide or very deep dive in. Shorter forms are welcomed nicely in programming communities and does not require tremendous amount of time to read. They have their own, valuable place in your blogging style. Especially at the beginning of your blogging journey when you are not an expert yet. And especially when you want to build a habit and keep writing regularly, but you don‚Äôt feel inspired or don‚Äôt have the time for anything very long. I wanted to elaborate a bit more about show some code and explain technique presented by Andrzej in the video. I want you to remember, it does not need to be your code . Sometimes you don‚Äôt feel inspired by the code you wrote recently in your daily work. Don‚Äôt force yourself . Think about your favorite gem, package, library. Go to github (usually) to check its code. Find lib or src or other directory with code. Find an a filename which sounds important or interesting and start reading. Don‚Äôt assume you are going to understand the code easily, but try to get an understanding what this class/function/file does and how it fits in the whole library that you already like and use. Spend as long doing it as you want. Usually we don‚Äôt like jumping to unknown code as programmers and reading someone else code for the first time can be challenging. However‚Ä¶ most popular open source libraries usually maintain good quality because of the whole community effort . When you browse code of a library you use and like, you already know more or less its top-level API exposed for programmers. So this is not a completely strange code to you. I guarantee the feeling won‚Äôt be the same as diving for the first time into a new, legacy project that your company got :) Although that can be interesting as well . The feeling will be more refreshing. More curiosity instead of worrying . After all, you don‚Äôt need to maintain this codebase. You are just passing by, trying to learn something new and interesting that will make you a better programmer. Something worth sharing with your readers. You will be doing a few things at the same time: Helping others and helping yourself at the same time. Everybody wins. Here is how I did it last time . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-07-14"},
{"website": "Arkency", "title": "How to safely store API keys in Rails apps", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/07/how-to-safely-store-api-keys-in-rails-apps/", "abstract": "Inspired by a question on reddit: Can you store user API keys in the database? I decided to elaborate just a little bit on this topic. Assuming you want store API keys (or passwords for SSL ceritifcate files) what are your options? What are the pros and cons in each case. If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-07-14"},
{"website": "Arkency", "title": "Using influxdb with ruby", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/07/using-influxdb-with-ruby/", "abstract": "InfluxDB is an open-source time series database, written in Go. It is optimized for fast, high-availability storage and retrieval of time series data in fields such as operations monitoring, application metrics, and real-time analytics. We use it in chillout for storing business and performance metrics sent by our collector . InfluxDB storage engine looks very similar to a LSM Tree. It has a write ahead log and a collection of read-only data files which are similar in concept to SSTables in an LSM Tree. TSM files contain sorted, compressed series data. If you wonder how it works I can provide you a very quick tour based on the The InfluxDB Storage Engine documentation and what I‚Äôve learnt from a Data Structures that Power your DB part in Designing Data Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems First, arriving data is written to a WAL (Write Ahead Log). The WAL is a write-optimized storage format that allows for writes to be durable, but not easily queryable. Writes to the WAL are appended to segments of a fixed size. The WAL is organized as a bunch of files that look like _000001.wal. The file numbers are monotonically increasing and referred to as WAL segments. When a segment reaches a certain size, it is closed and a new one is opened. The database has an in-memory cache of all the data written to WAL. In a case of a crash and restart this cache is recreated from scratch based on the data written to WAL file. When a write comes it is written to a WAL file, synced and added to an in-memory index. From time to time (based on both size and time interval) the cache of latest data is snapshotted to disc (as Time-Structured Merge Tree File). The DB also needs to clear the in-memory cache and can clear WAL file. The structure of these TSM files looks very similar to an SSTable in LevelDB or other LSM Tree variants. In the background, these files can be compacted and merged together to form bigger files. The documentation has a nice historical overview how previous versions of InfluxDB tried to use LevelDB and BoltDB as underlying engines but it was not enough for the most demanding scenarios. I must admin that I never really understood very deeply how DBs work under the hood and what are the differences between them (from the point of underlying technology and design, not from the point of APIs, query languages, and features). The book that I mentioned Designing Data Intensive Applications really helped me understand it. Let‚Äôs go back to using InfluxDB in Ruby. For me personally, influxdb-ruby gem seems to just work. The difference between tags and values is that tags are always automatically indexed. Queries that use field values as filters must scan all values that match the other conditions in the query. As a result, those queries are not performant relative to queries on tags. However, InfluxQL query language (similar to SQL but not really it) really shines when it comes to returning data grouped by time periods (notice GROUP BY time(1d) ), which is great for metrics and visualizing. where Time.at(1499212800).utc is 2017-07-05 00:00:00 UTC and Time.at(1499299200).utc is 2017-07-06 00:00:00 UTC . Using the gem you can easily query for the data using InfluxQL and get these values nicely formatted. For dashboards and graphs, monitoring and alerting. For business metrics: And performance metrics (monitoring http and sidekiq): Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-07-10"},
{"website": "Arkency", "title": "Handle sidekiq processing when one job saturates your workers and the rest queue up", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/07/sidekiq-slow-processing-one-job-saturates-workers-rest-queue-up/", "abstract": "I saw a great question on reddit which I am gonna quote and try to provide a few possible answers. Ran in to a scenario for a second or 3rd time today and I‚Äôm stumped as how to handle it. We run a ton of stuff as background workers, pretty standard stuff, broken up in to a few priority queues. Every now and then one of our jobs fails and starts running for a long time - usually for reasons outside of our control - our connection to S3 drops or as it happened today - our API connection to our mail system was timing out. So jobs that normally run in a second or two are now taking 60 seconds and holding a worker for that time. Enough of those jobs quickly saturate our available workers and no other work gets done. The 60 second timeout hits for those in-process jobs, they get shuffled to the retry queue, a few smaller jobs process through the available workers until the queued jobs pull in enough of the failing jobs to again saturate the available workers. I‚Äôd think this would be a pattern that other systems would have and there would be a semi-obvious solution for it - I‚Äôve come up empty handed. My thought was to separate the workers by queue and balance those on different worker jobs but then that still runs the risk of saturating a specific queue‚Äôs workers. Here are your options: Lower your timeouts Keep monitoring averages and percentiles of how long it takes to finish a certain job in your system (using chillout or any other metric collector). This will give you a better insight into how long is normal for this task to take and what timeout you should set. Prefer using configurable, lower-level network timeouts provided directly by libraries over Timeout module. Pause a queue. Keep the troublesome job on a separate queue. Use Sidekiq Pro. When lots of jobs are failing or taking too long, just pause the queue . Great feature. Saved our ass a few times. Partition your queues into many machines or processes. Have machine one work on queues A,B,C,D and machine two work on queues E,F,G,H. Use Circuit Breaker pattern. Circuit breaker is used to detect failures and encapsulates logic of preventing a failure to reoccur constantly Keep your queues in two reverse orders I am not sure if that‚Äôs possible with Sidekiq but it was possible with Resque. Most of our machines were processing jobs in normal priority: A,B,C,D,E,F,G. But there was one machine configured to process them in reverse: G,F,E,D,C,B,A. That way if job D started being problematic then A-C was covered by most machines and G-E was covered by the other machine. Because even if jobs in last queue are least important in your system, you generally don‚Äôt want them to be starved but rather keep processing them albeit more slowly. Increase number of threads per worker. If most of your tasks are IO bound (usually on networking) then you might increase number of threads processing them as your CPU is probably not utilized fully. Let me know if you have other ways to handle such situation. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-07-14"},
{"website": "Arkency", "title": "How to keep yourself motivated for blogging?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/07/how-to-keep-yourself-motivated-for-blogging/", "abstract": "You got your programming blog, but you don‚Äôt blog too much? You don‚Äôt feel like doing it? What can you do about it? There is a reason why you started or want to start blogging. Write it down and remember why you are doing something, what you are doing it for. It might be: Whatever your goal is, remember about it. There is something you are trying to achieve here. There is a dream you have. Don‚Äôt forget the dream. For me this is Independence. I like to organize my time, my tasks and doing things that I like ;) Without anyone telling me what to work on, how fast, in which way. This drives me. When I remind myself why I am doing what I am doing, why I am blogging, recording, writing a book, coding right now, it gives me a boost. There is more energy and less apathy. Say you want to deliver 20 blog posts in a year. Around one every second week. Set up a reminder in Trello or Google Calendar or printed calendar, whatever you use. It‚Äôs important for you to see a scheduled block of time for writing or a deadline for delivery. You need to find your style of work. Do you like to schedule an hour or two, focus on a single task and be done with it? Is that your style? Or do you prefer to work in many smaller chunks of work? Here is one technique that I sometimes use: It works for me because: In other days I use a different technique. I schedule a dedicated two-hour block of time and I immerse myself in writing. Find out what works best for you. The number 1 reason I am lazy and I don‚Äôt want to do anything, including writing a blog-post? Not enough joy. I sometimes forget to have fun myself during a whole week. It happens sometimes when we chase our goals and dreams but we don‚Äôt appreciate what we have, we don‚Äôt stop to notice and use what we already achieved. It‚Äôs the routine killing us. You know the drill: What‚Äôs the recipe? Only one thing works for me. Consciously scheduling time for joy. Remembering what I like and doing it. I grouped my favorite activities into a few categories: I noticed that often I don‚Äôt balance those activities properly. For example, I like learning more about programming and sales, I always have a few books about these topics around myself. But‚Ä¶ sometimes it feels that I am forcing myself to read them. On the other hand, when I started listening to an SF book, I finished 17 hours of audio material in a few days. I just forgot that I enjoy a different topic and a different medium. This happens to me more often than I would like to admit. We love doing something and we forget about doing it. We forget to schedule time for joy and remaining sane. We forget we are more complex and enjoy variety. We default to the same way of resting such as: because it usually works. Perhaps because it worked for so many years on us. But at some point, it often loses its magic. Yes, you enjoy the TV show but only when the episode is great, not average. And frankly, because of recommendations, it is easier to find historically good TV-shows and movies and hard to find fresh ones which are up to your taste. Art is strange in that way. You can use IMDB and other platforms to watch top 100 or top 250 best movies and enjoy them a lot. But then anything next is just‚Ä¶ not so good :) For me and computer games, it was hard to enjoy anything after The Witcher 3. So last week I was like‚Ä¶ fuck it. I gotta do something different, go somewhere different and break my habits. I scheduled 2-hour ride to a different city, spending half a day in an aqua-park and enjoyed water sliding like a baby. The next day it was ice skating. I noticed that I often tend to focus too strongly on intellectual ways of having joy, which can be extremely hard after hours of coding, instead of social and/or physical. It‚Äôs good to have goals, it‚Äôs good to want to improve, be better at coding, find a better place, support your family. But when we forget about joy, when we only feel guilty about not working, not doing more, we lose a lot. Especially we lose motivation. I noticed that surprisingly when you give yourself more slack, more freedom, you come more energized to your goals and challenges. Sometimes the real answer is to forget about blogging, do something different that you love and come back later. I know it sounds trivial. That‚Äôs why I tried to provide specific examples how I can completely screw it up. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-07-17"},
{"website": "Arkency", "title": "Non-coding activities in a software project", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/2017/07/non-coding-activites-in-a-software-project/", "abstract": "Recently in our project, we came up with a list of non-coding activities. Those are the tasks that need be done quite regularly and might be easy to be forgotten. If we tend to forget them, then there‚Äôs a risk that someone else will introduce a process around those activities. Sometimes it may mean new people will be brought so that they ‚Äúmanage‚Äù those activities. In my opinion, the more can be done by a developer the better, because we don‚Äôt introduce non-technical people to the communication loop. I will probably keep updating this list, as this may serve our team in the longer run. If you feel that we miss something important here, feel free to comment, thanks! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-07-17"},
{"website": "Arkency", "title": "nil?, empty?, blank? in Ruby on Rails - what's the difference actually?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/07/nil-empty-blank-ruby-rails-difference/", "abstract": "There are plenty of options available. Let‚Äôs evaluate their usefulness and potential problems that they bring to the table. but it is not included in Enumerable . Not every object which iterates and returns values knows if if it has any value to return This is where Rails comes with ActiveSupport extensions and defines blank? Let‚Äôs see how. This is convenient for web applications because you often want to reject or handle differently string which contain only invisible spaces. !!empty? - is just a double negation of empty? . This is useful in case empty? returned nil or a string or a number, something different than true or false . That way the returned value is always converted to a boolean value. If you implement your own class and define empty? method it will effortlessly work as well. Provided by Rails. Sometimes you would like to write a logic such as: but because the parameters can come from forms, they might be empty (or whitespaced) strings and in such case you could get '' as a result instead of 'US' . This is where presence comes in handy. Instead of you can write The implementation is very simple: If you are working in Rails I recommend using present? and/or blank? . They are available on all objects, work intuitively well (by following the principle of least surprise) and you don‚Äôt need to manually check for nil anymore. If you liked this explanation please consider sharing this link on: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-07-20"},
{"website": "Arkency", "title": "Monitoring Sidekiq queues with middlewares", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/07/monitoring-sidekiq-queues-with-middlewares/", "abstract": "Sidekiq, similarly to Rack, has a concept of middlewares. A list of wrappers around its processing logic that you can use to include custom behavior. In chillout we use it to collect and send a number of metrics: how long did it take to process a job Obviously it is nice to notice when a certain jobs starts to work much slower than usually. how long did it take between scheduling a job and starting a job This is useful to know if your Sidekiq workers are not saturated. Ideally the numbers should be around 1-2ms, which means you are processing everything as it comes and have no delay. Depending on what your application does a second or two of a delay might be good enough as well. But if the number is getting higher it means you are having problems and maybe you need more machines, threads or just investigate a temporary issue. If it is one job causing you problems, check out your options in Handle sidekiq processing when one job saturates your workers and the rest queue up . I used to think that number of unprocessed jobs is a good metric, but I think this is better. I doesn‚Äôt matter if you have 1 or 10_000 jobs waiting if you can start all of them very quickly because you have enough workers and the jobs are processed very quickly. The delay before processing is a better indicator than queue size. Because you don‚Äôt know if you have 1000 jobs which take 10ms each, or 1 job which takes 10 minutes to finish. And all you care about is the effect on other jobs waiting in queues. did it finish successfully or with a failure So that one can monitor a failure rate queue and job names To have granular metrics per jobs and queues. The code is very simple and nicely explained in Sidekiq documentation so if you want to build your own logging or monitoring, it‚Äôs not hard. Effect (click to enlarge): Testing middlewares is also easy: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-07-19"},
{"website": "Arkency", "title": "How to quickly add graphs and charts to Rails app", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/2017/07/how-to-quickly-add-graphs-and-charts-to-rails-app/", "abstract": "When how to visualize data in your Rails app there are certain factors that you need to consider. I am gonna propose you use Google Charts. Interactive and maintained by Google. This methods returns the data in format such as: Obviously it is up to you what data and how you want to visualize :) This is just a simple example. And that‚Äôs it. If your needs are simple, if you don‚Äôt need chart which dynamically changes values, if you just want draw a diagram, that‚Äôs enough. There are certain refactorings that you may want to apply once your needs get more sophisticated if you want to treat JavaScript and frontend code as first class citizen in your Rails app. You can read more about creating bar charts using Google Charts and check out tons of available configuration options Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-07-20"},
{"website": "Arkency", "title": "When DDD clicked for me", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/when-ddd-clicked-for-me/", "abstract": "It took me quite a time to grasp the concepts from DDD community and apply them in our Rails projects. This is a story of one of such ‚Äúaha‚Äù moments. Imagine a scenario which goes like this: but it triggers a shitload of effects: In another project that I worked on, the effects of a successful payment were: In both cases, the teams experienced the same problems: DDD helped me realized that my teams tried to do too much during HTTP requests and too many unrelated concepts were coupled together. That Payments, Delivery, Reporting and other concerns were too coupled. But it also gave me tools and solutions to fix the situation: In this case, I realized that I can refactor the payment process as: when both subsystems have success and it receives PaymentPaid , OrderCompleted , it can trigger CapturePayment command (in credit card payments the process has 2 phases: authorization, which reserves the money, and capturing, which actually confirms you want to receive them). when the Payment is Paid but we could not complete our Order and got OrderCompletionFailure for a brief moment of time (temporally) we have a discrepancy between two sub-systems. But DDD made me realize this is a natural situation. More importantly, DDD helped me realized this is a daily routine in businesses. There is never 100% agreement with the money you got and Orders you shipped/delivered. It just takes time. This might be obvious for you if you work on e-commerce system selling normal goods. But in systems dealing with virtual goods (book, coupons, accesses, videos, streaming) I noticed that the teams rarely make that distinction. The discrepancy can be easily fixed by triggering release/refund command for the payment. Just as you would do in normal business when you got the money but for some reason, but you could not send products to a customer. What does it do for your system design, how does it split the responsibilities? Subscribe to our newsletter to always receive best discounts and free Ruby and Rails lessons every week. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-03"},
{"website": "Arkency", "title": "How to add a default value to an existing column in a Rails migration", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/how-to-add-a-default-value-to-an-existing-column-in-a-rails-migration/", "abstract": "You probably know that you can easily set a default when adding a new column in an Active Record migration. But what about adding a default to an existing column? Fortunately in Rails 4 and 5 there is an easy way to achieve that. Use change_column_default method. The Api is: or Check out an example: Providing nil results in dropping the default. However, it‚Äôs better to use :from and :to named arguments. It will make the migration reversible which is sometimes useful. BTW. There is a similar method named change_column_null which (as you probably guessed right now) allows you to easily set or remove the NOT NULL constraint. means that state cannot be NULL (null -> false). means that state can be NULL (null -> true). If you want, you can also set a new value for records who currently have NULL . If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring big, complex Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-03"},
{"website": "Arkency", "title": "My first 10 minutes with Eventide", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/my-first-10-minutes-with-eventide/", "abstract": "Recently I find out about Eventide project and it looked really nice for me based on the initial documentation so I wanted have a small look inside. Here is what I found in a first few minutes without knowing anything about Eventide so far. What‚Äôs Eventide? Microservices, Autonomous Services, Service-Oriented Architecture, and Event Sourcing Toolkit for Ruby with Support for Event Store and Postgres Sounds good to me! My first thought was that there is quite interesting structure of directories: I was not sure where to look first to find the business logic but from the names you can quickly figure out what the project is about. I looked at start.rb but that didn‚Äôt tell me much: I could not easily navigate to an interesting place in RubyMine so I just started poking around. The first place that I felt I am on a known ground was around files in lib/account_component/messages/commands/ which include commands such as: commands are for telling services/handlers what to do. They‚Äôre just data structures. That‚Äôs important. So we have our input. Let‚Äôs see where it is coming into. In lib/account_component/handlers/commands.rb and lib/account_component/handlers/commands/transactions.rb you can find handlers and the logic for processing those commands. I won‚Äôt show the whole code. It‚Äôs pretty interesting. Just the most important snippets. What I recognized immediately was: The other interesting parts are: But at that point I could not easily navigate to follow method to check its implementation. I will probably find out later how it works. Anyway Opened is a domain event. Let‚Äôs see it: Events are written to streams. All of the events for a given account are written to that account‚Äôs stream. If the account ID is 123, the account‚Äôs stream name is account-123, and all events for the account with ID 123 are written to that stream. Classic thing if you already learned about event sourcing basics. Here is an interesting thing: A handler might also respond (or react) to events by other services, or it might respond to events published by its own service (when a service calls itself). Events and commands are messages in EventIDE. Apparently there is also one more class of messages: Replies. I haven‚Äôt yet figured out what Replies are used for. It seems interesting. I wonder where is the logic for changing account balance or checking if the funds are sufficient for withdrawal. Let‚Äôs find out. It‚Äôs interesting that even though the model is event sourced you don‚Äôt see it when looking at it. Let‚Äôs find the place responsible for rebuilding model state based on domain events. So far we haven‚Äôt looked at these files in controlers directory. I wonder what‚Äôs there. It looks to me like these are helpers that help you build exemplary data, maybe test data. Maybe some kind of builders. Subscribe to our newsletter to always receive best discounts and free Ruby and Rails lessons every week. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-03"},
{"website": "Arkency", "title": "My fruitless, previous attempts at not losing history of changes in Rails apps", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/my-fruitless-previous-attempts-at-not-loosing-history-of-changes-in-rails-apps/", "abstract": "Some time ago I was implementing a simple Inventory with products that could be available, reserved and sold in certain quantities. There were certain requirements that I tried to maintain: I was also experimenting with keeping the main business logic decoupled from DB and Active Record as well as a few more coupled approaches. Let me show you some parts of a solution that I came up with about 30 months ago. It‚Äôs not a whole solution but just a few snippets around the reservation logic. Good enough to get a feeling of the whole solution. I imagined that the Product protects some rules such as that you can‚Äôt reserve more of given product than you already have. Quite simple, quite logical. The product keeps track of all the quantities and implements the business logic. In the case of reservation that‚Äôs just: However, because I wanted to keep history what happened I came up with what I called ProductHistoryChange . It was a structure where I kept note about the changes made to a Product. It looked like this: Nothing fancy. 3 fields for what changed in the quantities and 3 fields about current values of the quantities after applying the changes. Here is how I imagined using those classes together: The solution was not that bad, it had good and bad points. There were however certain points that I didn‚Äôt like about it: Here is something that I understood over time‚Ä¶ I was trying to implement: However, my implementation was‚Ä¶ not the best. It took me quite some time, a lot of reading and experimenting to find out better ways to achieve it. Now, this kata is the base for for many exercises from our Domain-Driven Rails book and Rails/DDD workshops and you can see several different approaches to the problem. They progressively go from more Rails-way spectrum to more DDD-way of solving the before-mentioned problems so you can see for yourself how the solution changes but the logic remains the same. Here is part of one of the solutions from Event Sourcing spectrum. Subscribe to our newsletter to always receive best discounts and free Ruby and Rails lessons every week. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-07"},
{"website": "Arkency", "title": "That one time I used recursion to solve a problem", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/that-one-time-i-used-recursion-to-solve-a-problem/", "abstract": "Some time ago I was working on a problem and I could not find a satisfactory solution. But it all turned out to be much simpler when I reminded myself about a tool that I rarely use: recursive function. You see‚Ä¶ The iterators from Ruby‚Äôs Enumerable and queries from ActiveRecord are so nice that you barely need anything different than each to solve a problem. And here I was trying to use each in 5 different ways, in various different approaches failing again and again. The story goes like this. Your platform sells a ticket to an event. It might be for example a race or a marathon or other something completely different. Some of those competitions are pro or semi-pro and they are announced and purchased even 2 years upfront. The attendees (or should I rather say sportspeople) need to provide sometimes quite a lot of additional, mandatory data that the organizer asks for. It can be the name of your team, your age, your personal best record, etc. That kind of data. Which can also change over time‚Ä¶ Also for many other kinds of events, the platform offers additional services such as insurance (in case an accident prevents you from going to an event) or postal delivery (lol, yep). Some kinds of those services require plenty of additional data as well. We don‚Äôt ask for them before purchase because that would lower the conversion. So we ask about them after a purchase, on the very first page after you pay. However, some customers (so happy that they bought a ticket) don‚Äôt provide this data immediately. This can have various consequences. It might mean their insurance is not valid yet, it might mean that the organizer does not have all the necessary information about them. Or it might mean we can‚Äôt send them those tickets via postal like they wanted. But we don‚Äôt want our customers to experience these problems. We want them to be happy and get what they paid for. So we have implemented reminders. From time to time they should receive an email to fill out missing data. But not too often (because that‚Äôs annoying) and not rarely (because they might miss them). Also‚Ä¶ The reminders work best close to the purchase (just after it, when customers still remember what‚Äôs all the fuss about) and close to the date of the actual event (you are more concerned about providing proper data 1 week before a match or a race than 1 year before yet). The reminders are obviously not sent anymore when you provide all the necessary data. I wanted to implement an algorithm that would schedule reminders progressively less often going from the moment purchase towards the event date. And similarly in the other direction from the event date back to sales date. And they both should meet somewehre in the middle. On a timeline, it would look like this. At the beginning all of my approaches were similar. Almost identical. I split the timeline in half. I iterated from left to right, doubling the distance in time for every reminder (up to 1 month). Similarly, I iterated from right to left, doubling as well. But there was a big problem with that solution. If often happened that two reminders in the middle were either: I tried about 3 different approaches and all failed the same way. You see‚Ä¶ The problem was that I tried to iterate from left to right, from right to the left and combine the solutions around the middle. What worked for me instead? Iterating from both sides at the same time. It works like this: The whole solution is around 12 logical lines of code in total :) Because the time distances between the purchase and the event are no bigger than a few years, I was not worried about possible stack-overflow. In longer periods of time, we need about 6 method calls to compute reminders for a whole year. Subscribe to our newsletter to always receive best discounts and free Ruby and Rails lessons every week. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-11"},
{"website": "Arkency", "title": "How to connect to and use ActiveRecord outside of Rails?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/how-to-connect-to-and-use-activerecord-outside-of-rails/", "abstract": "Let‚Äôs say you have a Ruby script and you would like to use ActiveRecord library inside it. Is it possible without Rails? Is it hard? It‚Äôs possible and not hard at all. Make sure your pure Ruby script starts by requiring active_record : I like to use connection strings instead of YAML/Hash with separate key-values to describe the connection. I prefer to use environment variables to set them because if I want to run the script on a different database, I can change it easily by setting an environment variable outside of the script. So this works nicely for me. That‚Äôs it. This one-liner does the job. You can now normally use the models in your script. There is no ApplicationRecord so you can‚Äôt inherit from it (unless you define that as well, yourself). If the database does not already have the tables you are planning on using, you can create them with Active Record migrations. If you always want to have a clean state when you run the script, pass force: true to drop a table before creating it. Set verbose to false if you don‚Äôt want to see the output of SQL statements creating those tables. If you want to see the SQL queries and commands executed by your script, don‚Äôt forget to set a logger. Use STDOUT instead of a file, to output directly on the screen. If you want to use different DB than the default: If you want to always use specific activerecord version you need to create the Gemfile and run bundle install . Execute the script with bundler: If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring big, complex Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-19"},
{"website": "Arkency", "title": "What I learned from reading spreadsheet_architect code", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/a-word-about-spreadsheet-architect/", "abstract": "Recently I heard about spreadsheet_architect gem and I wondered a few things after reading its README. It all lead me to look into its code and wonder‚Ä¶ What started my curiosity was the API: I was like wow, why would anyone add such a thing to a model ? Why is it a good idea to extend my ActiveRecord class (which most likely already has tons of responsibilities) with methods responsible for generating reporting files. Dunno. Doesn‚Äôt sound like the best use of Single responsibility principle . But I was like‚Ä¶ There must be a way to avoid it. After all, this gem can work with normal classes as well as documented: or another way: So probably including SpreadsheetArchitect is not mandatory. I was thinking about checking the code to see how it gets the list of records and implementing a compatible interface inside a different class. I was pretty sure the method was #to_a because why not? What else could it be? So I hoped for this kind of workaround: or if we want to dynamically build and pass the collection for generating a report. Then I could define methods such as def spreadsheet_columns which configure what is displayed in PostsReport instead of Posts because that sounded better to me. Less coupling, and bigger separation. Probably easier if we need to support multiple reports. So I started looking into the code, how it works internally, to confirm my guesses. It turned out the logic was implemented in get_cell_data . Let‚Äôs see what inside. It had 80 lines of code including this: Ok, so it turned out it is not just to_a but rather where(nil).to_a . I am not exactly sure why would where(nil) be necessary. At this point, I decided that working around the API is probably not worth it and too hard. But I got curious how can it work? Because you see‚Ä¶ Yep, Ruby has private classes and they are used here. So how can this gem work with relations? Let‚Äôs try something‚Ä¶ Nothing out of ordinary here: But check this out: Even though this is ActiveRecord::Relation , methods which are dynamically executed report that self is the Post class. Let‚Äôs investigate further: Let‚Äôs see how this Rails magic works: In other words before Post#to_xlsx is called, Post.current_scope is set temporarily and as a result Post.where(nil).to_a called by the SpreadsheetArchitect is limited in scope and does not include all records. That‚Äôs how it works. via GIPHY Subscribe to our newsletter to receive weekly free Ruby and Rails lessons. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-18"},
{"website": "Arkency", "title": "Testing deprecations warnings with RSpec", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/testing-deprecations-warnings-with-rspec/", "abstract": "Recently at Arkency we‚Äôve been doing quite a lot of work around the RailsEventStore ecosystem. We see RailsEventStore as the way to help Rails teams start doing DDD without needing to build the infrastructure for CQRS/ES. The growing number of teams which use RES + a growing number of contributors made us do some cleaning. One of the small changes which was introduced was a very simple refactoring. It would be simple if not the fact, that this was changing the public API, so it requires a proper handling with a deprecation warning. In the RailsEventStore you can pass a handler to any event. We expect that this handler responds to call . If it doesn‚Äôt respond to that, we used to raise MethodNotDefined exception. However, this can be a confusing name, so we decided to rename that to InvalidHandler to be more explicit about the problem. It‚Äôs very unlikely that someone was relying on this exception in their project using RES. Still, it‚Äôs a good practice to not crash the client code, after an upgrade. We want to keep it working, but give a warning message. Also, we don‚Äôt want any new code to depend on this old class. What we did: This is the whole implementation: This works and is fine. However, how can we ensure that this works? By writing tests of course! We have 3 requirements here: which nicely turns into this RSpec code: The first spec goes like that: The second spec: The third one was a bit more complex. We rely on .warn method which is built-in in Ruby. Its result is to output a message to $stderr. There are several ways to approach this. We can either mock the .warn method, or we can wrap the whole thing with some kind of UIAdapter which just happens to have .warn as the implementation detail (but we still need to test the new class). The last solution is to make sure the effect is valid - we see some output on $stderr, which can be done by introducing a FakeStdErr class. Then the spec looks like this: It‚Äôs also worth noting, that most developers will rely on the RailsEventStore gem, which is the umbrella gem for all the ecosystem here. However, RailsEventStore is only a simple wrapper over the RubyEventStore gem. In particular it means we wrap the public exceptions with a code like this: This gives us the control that from a RailsEventStore perspective the RubyEventStore can be an implementation detail. People who are using it with their Rails apps don‚Äôt need to be aware of the details. However, RubyEventStore is something that can be used in any project, not only Rails (think Hanami, Sinatra, etc). This way the feature is covered fully with tests. Mutant is happy (100% coverage) and we can feel secure that other changes don‚Äôt break this simple functionality. Obviously, we could debate whether this kind of a feature (deprecations) deserves the tests. It‚Äôs a hard question and not easy to answer. We should probably decide whether the new tests make our further work harder. One thing which we follow from the beginning of the project is to have 100% mutation coverage to ensure the quality and to encourage the contributors to follow the TDD techniques. RailsEventStore is a tool on which serious projects rely on and we want to ensure that it works the best possible way. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-27"},
{"website": "Arkency", "title": "The === (case equality) operator in Ruby", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/the-equals-equals-equals-case-equality-operator-in-ruby/", "abstract": "Recently I‚Äôve been working on adding more exercises to DevMemo.io about Ruby‚Äôs Enumerable module. And I try to balance learning most popular APIs (which you might already know) with some less popular but very useful. And my attention was captured by Enumerable#grep . As you can see it works with any class which implements === operator. So I was curious as to which classes implement it and in what way. Let‚Äôs see. === returns true if obj is an instance of mod or one of mod ‚Äôs descendants. Of limited use for modules, but can be used to classify objects by class. Basically implemented as Basically implemented as: Returns true if obj is an element of the range, false otherwise. Invokes the block with obj as the proc ‚Äòs parameter just like #call . For most of other objects the behavior of === is the same as == . You can define your own class and it‚Äôs own === which might be as complex (or as simple) as you want. And you can use instances of such class as matchers in case..when statements or as arguments to Array#grep . Of course this is only for the cases when you don‚Äôt feel that checking those conditions is a responsibility of the tested object and you don‚Äôt want to implement it as a method in its class. As Ruby allows you to define singleton method which affect only a single object‚Äôs behavior, you don‚Äôt even need a class. But frankly, I would rather go with Proc in such case. If you don‚Äôt want to forget about Enumerable#grep try DevMemo.io . We‚Äôve been recently working on a Beta version which includes scheduling flashcards repetitions and reminders. Also, subscribe to our newsletter to receive weekly free Ruby and Rails lessons. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-23"},
{"website": "Arkency", "title": "Interview with Sergii Makagon about hanami-events, domain-driven design, remote work, blogging and more", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/interview-with-sergii-makagon-about-hanami-events-domain-driven-design-remote-work-blogging-and-more/", "abstract": "You might not have heard about Sergii but he got my attention some time ago. Because, you see‚Ä¶ In one year Sergii wrote about 40 articles on http://rubyblog.pro/ and I started to see them mentioned in various places. Then I saw he started doing some open source work on Hanami::Events and that tipped the balance. For a long time I wanted to interview some Ruby developers and I decided to start with him as I met him personally before :) Meet Sergii Makagon :) I was interested in Hanami because they follow Clean Architecture principles. That‚Äôs exactly how I structured my current Rails project.\nI‚Äôve started my pet-project with Hanami and I like it! I decided to look for opportunities to contribute.\nLuca Guidi (founder of Hanami), suggested that for Hanami 2.0 it would be good to have Pub/Sub functionality implemented with Wisper gem. I like event-driven systems, and I‚Äôve been using Wisper for a while, so I created an example of possible implementation. Luca suggested some improvements, but in general, he liked it. After some iterations and discussions, he created a repository and Anton Davydov (core dev of Hanami) and I started to work on a gem called Hanami::Events . I enjoy working with Anton, it‚Äôs a good way to learn something new. I think the long term goal for this gem is to be included into Hanami framework and provide flexible a solution for event-driven systems. We tried to keep our focus on flexibility. For example, developers can extend the gem by any custom adapter, or use existing ones. For now, we have MemorySync, MemoryAsync and Redis adapters included into the gem. We have ideas to implement DB adapters and Anton wants to add Kafka adapter as well.\nThis gem is still in an early stage but I think we have created a good foundation for future features. There are only two hard things in Computer Science: cache invalidation and naming things. ‚Äì Phil Karlton I feel like we‚Äôve been struggling with the second one. Naming things is hard. Especially considering the fact that we have ‚ÄúMany Meanings of Event-Driven Architecture‚Äù . I like that talk by Martin Fowler. If somebody says ‚Äúwe have an event-driven system‚Äù, usually we need to ask more questions to clearly understand the role of events in the system and how they‚Äôre being used. We had to refactor code couple times just to change naming conventions. Also, Anton suggested creating a definition for each keyword. It was a really good idea. We had a lot of good gems to get inspiration from (wisper, rails_event_store).\nSpeaking about API for libraries, I prefer to start with something really simple, like MVP in a startup world.\nMy idea was to solve one specific problem and make sure it works perfectly. When I have a simple well-tested solution I can add more features. In case of Hanami::Events it was as simple as: broadcast (or publish ? :) and subscribe methods. You can not go wrong with those two.\nThen we started to add more features: more adapters, allowed to inject custom logger, etc.\nIn the early stage, Anton did all hard work and set that foundation for us. By the way, it‚Äôs a good idea to start using own gem when it‚Äôs on early stage. Anton tried to create Hanami app with Hanami::Events and found couple bugs which we already fixed. I use Rails in my everyday project. With proper architecture - a framework is not a vital part of a project. We could switch to Hanami if we needed to.\nI want to try Hanami in a couple more pet projects to see pros and cons of it and then I will consider using it in production.\nFrom what I‚Äôve seen so far it looks like a promising production-ready framework. Hanami feels really similar to Rails, especially if you use it with a basic MVC approach (I know that some Ruby devs will not agree with this statement, but I describe my own experience here :). I think it makes developers feel more confident and comfortable with a framework. I should mention that some ideas looked unusual at the beginning, but when I became more familiar with it, I noticed a lot of benefits.\nFor example, in Hanami, controllers are Ruby modules that group actions. Actions are objects that respond to the #call method.\nThe View layer is split into two pieces: template and view. Where a view is a testable object that‚Äôs responsible for rendering a template.\nHanami has nice documentation that should help dive into all the details. Speaking about Hanami-way of writing apps, I think it‚Äôs defined in description of the framework and Architecture Overview : Hanami prioritizes the use of plain objects over magical, over-complicated classes with too much responsibility Hanami is based on two principles: Clean Architecture and Monolith First. Clean Architecture is a hot topic. Robert C. Martin (Uncle Bob) is publishing his book ‚ÄúClean Architecture‚Äù in October. I‚Äôve pre-ordered one and really looking for it. I‚Äôve been using this approach for a while I want to learn more from Uncle Bob. When they say ‚ÄúMonolith First‚Äù, they refer to Martin Fowler‚Äôs article which suggests to start with Monolith and split it to microservices only if there‚Äôs a need for that. Usually, that happens when developers understand domain and complexity behind it so they can split it properly. Sure. I read all 3 books on DDD and visited your workshop where I learned how to actually use DDD in Rails projects. It totally changed my way of thinking about designing applications. Your last book Domain-Driven Rails is a really good one too. I like the practical aspect of it. DDD brings a lot of benefits. First, most important and underestimated is ubiquitous language. I‚Äôve seen many times where developers and project managers have misunderstandings because of different vocabulary for the same object. Also, I‚Äôve seen developers use different names for the same things in code, which is even worse. I use the idea of Entities, Value objects and Repositories. Having those little PORO‚Äôs in your core domain is really beneficial.\nThe idea of layered architecture works out pretty well too. It allows to decouple elements of a system. Also, I like to use events as a way to communicate between some parts of the application. I must admit that I don‚Äôt use everything that DDD offers, I try to pick just what I need.\nI have an architecture which is more similar to Clean Architecture, but as I dive into different architectures, I see more similarities. In apps that I create these days I usually have three layers: As you mentioned in your book, it‚Äôs just layers of the same cake :)\nBut then we should keep in mind that layered cake should be sliced to pieces as well. That‚Äôs where bounded contexts come into play. I try not to overcomplicate apps from the beginning, that split should happen naturally. The most important thing is not to miss that moment. I can not miss opportunity to answer with ‚ÄúIt depends‚Äù :)\nBut it really does. Sometimes, if I know upfront that it‚Äôs going to be just a small app: portfolio website, ‚ÄúProof of concept‚Äù project, etc, I follow YAGNI principle and try not to over engineer things.\nBut if it‚Äôs an app that supposed to grow to something bigger - it‚Äôs a good idea to invest time into flexible architecture. Usually, I keep an eye on 2 sources of complexity that can lead to changes: business requirements and existing codebase. Business requirements: When I start working in a new domain - things might look simple in the beginning.\nFor example, one of my previous projects was related to child day care. Sounds easy, right? Just create a simple web app for day care centers. But as we started to dig into that domain, we figured out that it‚Äôs huge. Schedules and notifications for parents. Payment collection and benefits calculation. Absence calculation, etc, etc. Every time we received a request from the business, we revisited our existing implementation, how it fits to new requirements? Which words did they use to describe new business processes? It brings us back to the ubiquitous language. It‚Äôs just great how much we can learn from domain experts. Usually, after a discussion with them, I feel like I‚Äôm super overwhelmed, but new knowledge allows developers to architect solution properly. If you see that new words and definitions appear in requirements, make sure you understand the meaning behind it. Is that a separate entity, or just an existing one but used in different context?\nLong story short: I try to keep an eye on complexity and new knowledge. Listen to domain experts. Revisit what I have. Constantly. Codebase: Sometimes we make decisions that are not exactly accurate because of many reasons: lack of domain knowledge, lack of programming skills, lack of time, etc.\nThat‚Äôs why I try to revise existing solutions from time to time. We can have a relatively simple domain, but business rules can be implemented in a way that it makes it hard to follow. In that case, we want to refactor it to the smaller pieces. Knowledge of object-oriented design should help here. I found that Sandi Metz rules help a lot. It sounds too simple to be true, right? But it helps. The benefit I see is that it shows areas where you probably have an opportunity to refactor code and extract nice little classes with single responsibility out of one huge God object.\nWriting clean code helps to prevent future messes. It‚Äôs much easier to add if..else statement to a method with 50+ lines of code than add it to a method with 5 lines of code. It will be too obvious that you‚Äôre just trying to hotfix something, instead of providing a solid solution. I like to work remotely. I worked remotely as a freelancer for different companies around 4 years. At my previous company, I worked in the office. At my current one, I work from home 2-3 days a week. Speaking about traps. I think remote work is not for everyone.\nI know that there are people who like small-talks and communication in general. It‚Äôs going to be hard for them to just stay at home and talk only through the internet when needed. From what I‚Äôve seen, many developers are introverts, so remote work looks good to them because all communication is happening online. But at the same time, they‚Äôre losing their soft skills. In general, soft skills are really important in any field, not just in development. Also, sometimes working remotely I felt like I was disconnected from all exciting things that were happening in the company, community, etc. Basically, it was my responsibility to set new challenges and goals for myself, but it creates sort of comfort zone which is hard to break. In this case, it‚Äôs a really good idea to find meetups, go to conferences, try new technologies, find a new source of inspiration. It applies not just to remote developers, but when I worked remotely I felt that more often. To work from home efficiently, a developer should be self-disciplined and should be able to stay focused. At home, we usually have a lot of distractions that might pull us from code to something else. Without self-discipline, it can be a problem.\nI like to use Pomodoro technique with 50 minutes of work without any distractions and 10 minutes of break. 4-5 sessions a day allow to accomplishing A LOT, often much more than 8-9 hours in the office with distractions.\nProper sleep and physical activities are important too. I found for myself that I like to be in office 1-2 days a week. It allows me to stay connected to the team and have enough communication. Thanks! In software development, we have to learn constantly. Because everything‚Äôs changing: languages, frameworks, tools. I like to learn and I like to share what I learned, that‚Äôs my motivation. I‚Äôm happy to see that people find my blog useful. They leave really nice comments and provide awesome feedback too. I learned a lot from discussions around those blog posts. Since I started my blog I used it many times to explain something to other developers or to refresh my knowledge of the topic. I‚Äôm a Ruby developer, but I try to cover topics that are not framework-related or tool-related. I switched my focus more to the approach and techniques that I can use in any object-oriented language. For example, Object-Oriented Design, Domain-Driven Design, Clean Architecture - those do not require us to use Ruby. It‚Äôs more about the structure, domain, an approach in general. I like it because those things are not ‚Äònew hotness‚Äô (that will fade out next week) in the world of frameworks and languages. It‚Äôs more about tackling complexity. We‚Äôre dealing with complexity in any project and it‚Äôs our responsibility to know how to do that properly. Speaking about time, I prefer to write on Sundays. It‚Äôs good to have it scheduled so it becomes like a good routine. I enjoy having that time. Sunday works because during a week I can come up with the idea for the future post and gather all information. The more I write, the less time it takes. I like the practical approach. It‚Äôs great to start with a simple pet-project and add more and more advanced features. It will walk developer from planning phase and development to deployment stage. Pet-project should be in a domain that is interesting to a developer. It will help to keep him engaged to the project. Junior developers are usually focused on tools. Let‚Äôs say they want to know Rails, React, because that will give them a job, that will help them to get something done. It makes sense to teach them that tool. It helps developers to feel more comfortable and safe in terms of the job. Once they have a good understanding of a framework it‚Äôs good to dig into language and get a better understanding that a framework is just a tool. Of course, we have different tools for different tasks, so it‚Äôs good to know pros and cons of those tools. That will allow them to pick proper tool for a task. Usually, at that moment ‚ÄúJunior Rails developer‚Äù become a Ruby developer :) Very often junior developers need guidance with soft skills as well. It‚Äôs good to be able to communicate with team mates on a good level, not to have a problem with stand-ups and other meetings, to be able to show own strengths during an interview, etc. When a developer knows framework and language, it‚Äôs good to help him to write not just code that works, but code that can be considered as clean code. At that moment I would show the ideas of Object-Oriented Design, Patterns and some general ideas of possible architectural decisions. It will help a developer to pass code review stage with fewer iterations. At that moment developer can be considered as a middle-level developer, I think. But all companies are different so it‚Äôs a good idea to have something like this: https://github.com/basecamp/handbook/blob/master/titles-for-programmers.md Yes, Pivorak is amazing. They created the unique environment for developers. It motivates, it makes you feel like you‚Äôre part of something big. They bring amazing speakers.\nI met Piotr Solnica, creator of dry-rb there. Nick Sutterer (creator of Trailblazer) did a workshop on Trailblazer for us. I met Michal Papis - core developer of rvm. I learned a lot from my Ukrainian colleagues as well. It‚Äôs definitely worth visiting. There is also RubyC conference - it‚Äôs a big annual Ruby-conference in Ukraine. It‚Äôs a really good one too. A lot of new ideas and interesting speakers there. I know that in Kyiv there is a Ruby Meditation meetup which is quite popular too. I haven‚Äôt had a chance to visit it yet, but I would love to. I feel like we have a strong Ruby community in Ukraine. I will be happy if more local meetups like Pivorak appear in Ukrainian cities in the nearest future. I prefer to release fewer features but more often. Of course ‚Äúmore often‚Äù depends on the structure of a company and the process of software delivery it has.\nIn general, I try to rely on tests and additional checks, for example, linters and code quality tools. If everything‚Äôs green I can pass it to QA for regression and manual testing. I like the idea of feature toggle but it should be used really carefully. It looks good on paper, but when you have a complex system and too many toggles, the code can be really messy and contain a lot of duplication. It makes much harder to debug issues and follow the flow of the application. It‚Äôs a powerful tool that should be used wisely and only if it‚Äôs needed. The last part of the question regarding monitoring is really broad and interesting. On previous projects, I had experience of using tools like NewRelic and Nagios and it was good. Sometimes even simple Heroku dashboard was enough to get an understanding that everything‚Äôs fine. Depending on the project, the key metric can be different. Size of the project is an important factor as well. We have a lot of things to monitor and measure these days. It‚Äôs good to have systems that provide visibility and help to understand what‚Äôs going on in our system. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-29"},
{"website": "Arkency", "title": "inject vs eachwithobject", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/inject-vs-each-with-object/", "abstract": "Recently I‚Äôve been adding new exercises to DevMemo.io and some of them were about Enumerable#inject (also available as aliased method reduce ) and some of them were about Enumberable#each_with_object . And I‚Äôve been thinking about a small guideline for when to use which. Let‚Äôs see the differences with some examples. Imagine you have a collection of objects and you want to build a new Hash using them and perform some kind of mapping. In such case, each_with_object is very convenient. On the other hand inject is less convenient: because inject requires that the memoized value provided for subsequent block calls ( hash which initially is {} ) is returned by previous block calls. So even though you constantly operate on the same object, you always need to return it in the last line of the provided block. each_with_object on the other hand always calls the block with the same initial object that was passed first as first argument to the method. But let‚Äôs say you already have an existing object that you would like to modify. In such case, it would be usually preferable to just use each over each_with_object but each_with_object can be a bit shorter if you still need to return the changed object. All three versions below generate the same result. I would say that each is preferable if you mutate an existing collection, because usually you don‚Äôt need to return it. After all, whoever gave you that object as an argument, wherever it comes from, that place in code probably still has reference to this object. This time you are not mutating internal state of an object but rather always creating a new one. The operation that you use always returns a new object. The most simple example can be + operator for numbers. There is no way to change the Integer object referenced by variable a into 3 . The only thing you can do is assign a different object to variable a or b or c . It‚Äôs an obvious example. But Date is a less obvious one. If you want a different date, you cannot change the existing Date instance. You need to create a new one. So that was a small introduction. What does it have to do with inject . If your initial object is immutable, inject is the way to go. or or This time we will be creating a new object every time but not because we can‚Äôt change the internal state. This time it‚Äôs because a certain method returns a new object. Hash#merge merges two hashes and returns a new one. That‚Äôs why we can use it easily with inject . Compare it with Isn‚Äôt it a bit irritating that the order of arguments passed to the block for inject and each_with_object is reversed? Soon we will be launching Ruby‚Äôs Enumerable course on DevMemo.io . Try it and subscribe if you like it. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-29"},
{"website": "Arkency", "title": "Why we follow the Rails repo structure in Rails Event Store", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/why-we-follow-rails-repo-structure-in-rails-event-store/", "abstract": "A complete Rails Event Store solution consists of following gems: Until recently, each of the gems lived in a repository of it‚Äôs own. That‚Äôs how you usually develop a gem and what majority of tools assume. Bundler for example in its release-helping Rake tasks provides a code for tagging git repo with gem version number for a RubyGems release. Each gem in it own‚Äôs repository provides a separation and some gems like aggregate_root or ruby_event_store can be used completely on their own. This split however has several drawbacks if you‚Äôre already a contributor or wishing to become one: In short it was easier to start that way but it‚Äôs painful in the long run. If you look at Rails repository you immediately notice the code layout: It is also worth noting that each of the components gets the same version number as Rails release. It might be tempting to keep component versioning separate but it simpler from end-user perspective to refer to only one number (i.e. when reporting issues). Being sold to the idea of monorepo we had to figure out ‚ÄúThe How‚Äù. For sure we wanted to keep most popular repo alive (and base for others to be merged in). We could think of 3 possible approaches: For us, from contributor perspective, working file history is more important than preserving original commit identifiers. So we chose approach involving git filter-branch . The migration involved running following snippet on each repo: Finally each of rewritten repos where merged into destination one with following git pull modifier: Sidenote: gsed stands for GNU Sed. BSD Sed available on MacOS caused me some trouble . All these changes were mostly for contributors and maintainers convenience. If you‚Äôre a happy Rails Event Store user you might be wondering if that change should be on your radar. Last but not least Rails Event Store got new website . I encourage you to check it out and consider using RES to support your business. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-30"},
{"website": "Arkency", "title": "DATABASE_URL examples for Rails DB connection strings", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/database-url-examples-for-rails-db-connection-strings/", "abstract": "Recently I‚Äôve been configuring RailsEventStore to run tests on many\ndatabases on the Travis CI. We do it using DATABASE_URL environment variable\nbut I couldn‚Äôt find good examples easily. So here they are. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-08-31"},
{"website": "Arkency", "title": "How to find records where column is not null or empty in Rails 4 or 5", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/rails-how-to-find-records-where-column-is-not-null-or-empty/", "abstract": "We all know that especially in legacy applications sometimes our database columns are not that well maintained. So we need to query for, or exclude rows containing nil / NULL and empty strings ( \"\" ) as well. How can we do it in ActiveRecord? Let‚Äôs say your Active Record model is called User and the DB column we are going to be searching by is category . That‚Äôs simple. Still easy. This not() clause is going to only apply to one where . You are not going to negate all previous conditions. In other words you can safely use it like this: to get SQL statement like this: If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring big, complex Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-04"},
{"website": "Arkency", "title": "Making tmux work for you", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/making-tmux-work-for-you/", "abstract": "When it comes to my developer toolset, I like solutions that I can easily understand and tweak . I guess I‚Äôm not the only one. I believe that for a lot of people one of the (many) reasons to join the Ruby bandwagon was that you could do virtually everything without a big, fat IDE, that I never know what it‚Äôs doing and how. Just your editor and console. First class citizens. And I still like this kind of workflows. But, let‚Äôs be honest, sometimes there are moments, that you wish your tools did a little better when it comes to helping you with tedious tasks . You can often find an editor plugin that does what you need. And it‚Äôs fine. But sometimes you‚Äôre like: do I really need to pull another plugin to do this simple thing? . After all, we‚Äôre programmers. We can program basically anything, why not our developer environments. It seems to me, that often what we need are just the proper building blocks. tmux is one of such building blocks, that can really help you a lot when it comes to automating your developer workflows. Generally one could say it‚Äôs a screen on steroids. It let‚Äôs you manage terminal sessions and interact with them programmatically. On Mac OS you can install it with brew install tmux . It‚Äôs a powerful and versatile tool - a lot of people us it for a lot of different things. I‚Äôm neither a heavy, nor a longtime tmux user, but let me present a couple problems where it paid off for me. I‚Äôm not encouraging you to adopt ‚Äúfull tmux workflow‚Äù, whathever that means, but maybe it‚Äôll show how you can easily solve a specific problem yourself. Sometimes you work on a project with one rails server and probably a background worker. Not a big deal to launch when you start your work in the morning. But what when you have 10 microservices ? You don‚Äôt want to do it by hand. Of course, there are so many tools that can help you with that, pow, puma-dev, docker, whatever. But you don‚Äôt always wanna learn and employ a new tool to do a dead simple thing like launching a server. You wanna keep it simple, and not spend a day on configuring a specific solution. How did tmux help me with that? I simply wanted something that will just ‚Äúprepare‚Äù terminal windows for me, so that I can later interact with them in a ‚Äútypical‚Äù way, like ^C to kill the process, then press up and enter to start it again, rather that running the processes in the background, etc. I often want to tweak my workflow step by step, rather than by revolution. So tmux has this nice command ‚Äúsend-keys‚Äù. What does it do? Yup, it sends a particular key sequence to the terminal session . As simple as that. Let‚Äôs have a look at this sample tmux invocation: One tmux binary invocation let‚Äôs you specify multiple tmux-commands: Once you launch your script, you can: You‚Äôll find your tmux config in ~/.tmux.conf : By the way, if you sometimes happen to work with multiple repos, you might wanna have a look at this shell-one-liner: multigit - it lets you run (git) commands on all ‚Äúsibling‚Äù repos. ‚Ä¶to come in another blogpost - but the building blocks are the same, no additional magic needed :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-07"},
{"website": "Arkency", "title": "How to setup development work with RailsEventStore and mutant", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/how-to-setup-development-work-with-railseventstore-and-mutant/", "abstract": "As Arkency we‚Äôre making our efforts to inculcate Domain Driven Design into Rails community. You should be familiar with Rails Event Store ecosystem. We use it in our customers‚Äô projects with success since quite some time. One of the cornerstones of the Rails Event Store is keeping it 100% covered with tests . Rails Event Store can become an important part of the application. There‚Äôs no other way than keeping it well covered with tests. Mutant is the tool which supports us in achieving that. One of the missing parts of our ecosystem were RSpec matchers. In each customer‚Äôs project, we wrote custom matchers. Some of you who already use Rails Event Store will do the same sooner or later. Based on our experience we decided to provide matchers. They can be used out of the box via rails_event_store-rspec library. I‚Äôll try to describe it better in a separate post. For each of the RailsEventStore parts, we provide Makefile , for convenience use. After cloning the interesting repository run make install and you are ready to go. If you want to run tests, just run make test . To run mutant you do make mutate . The second one runs tests as an initial phase. Yet, I‚Äôve got tired of constantly switching between my code editor and terminal. Running make mutate and switching back to code since it takes some time for the Mutant to complete the run. I used Guard in past to track file changes and run tests but I remember that it was integrating with the code a bit too much. Adding to Gemfile and .Guardfile to a repository is such integration for me. I don‚Äôt want to force people to install stuff which is not necessary for contributing . Anarchy, you know. I reminded myself that jest is using something under the hood for watching file changes. And there it is, please meet the Watchman - A file watching service . We can read on the project page that Watchman exists to watch files and record when they change. It can also trigger actions (such as rebuilding assets) when matching files change. Sounds good enough. I went through the docs. I‚Äôve figured that there‚Äôs dedicated command for running Makefile tasks. It‚Äôs called watchman-make . If you‚Äôre a Mac user, simply brew install watchman . More in the installation section of the docs. What‚Äôs the magic command to use with any part of RailsEventStore then? -p stands for the patterns to watch, we want to run a test on any spec or lib file. Just figured out that the second pattern includes a first one. -t is for specifying the build target, in our case, it‚Äôs mutate task from Makefile The tool is powerful. You can specify many targets responding to several patterns. But that‚Äôs not our case. It might be cool to get some notification on success or failure. Current setup is good enough for me and improves my workflow. Yet I don‚Äôt want to spend more time on that now. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-06"},
{"website": "Arkency", "title": "What I learnt from Jason Fried about running a remote/async software company", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/what-i-learnt-from-jason-fried-about-running-a-remote-slash-async-software-company/", "abstract": "I‚Äôm running Arkency for 10 years now. Arkency is a remote company and an async company. We have created our own unique culture of working and collaborating. When we started, there weren‚Äôt really many sources to look up to in regards to remote work. Basecamp (former name: 37 Signals) is such a company. They have existed before Arkency. They were kind enough to be very opinionated about the way they work, they blogged about their efforts. Not only Rails was an inspiration from Basecamp to Arkency, also the remote work was. It was a true pleasure to me, to watch the Jason‚Äôs Fried talk on their way of working, now after 17 years of running Basecamp. The whole video along with the transcript is available here , but in this blogpost let me list my lessons I‚Äôm taking from Jason + my comments on them. This part was said by Jason as a response to (a very common) question how to get emotions in remote companies. I fully agree with this statement. We read books which tell us stories about past, present and future, we follow heroes and significant characters, all in the form of writing. Writing can be emotional, if one has the writing skill. At Basecamp, they hire only people who are good writers. This is also a rule at the Arkency recruitment. We want to see how people communicate, before we hire them. It‚Äôs not a hard requirement, but a blog, a book or even a Twitter can help us decide how good at written communication you are. Many developers are introverts. What I learnt over the years working with the Arkency people is that introvert/extravert split doesn‚Äôt matter in remote/async companies. People who would never say a word during a live meeting, are communicating super-precisely when they are allowed to take their time to form their thoughts. Writing doesn‚Äôt promote extraverts nor introverts, this is an orthogonal skill. A writing skill is not about grammar nor about a rich selection of words, although they do help. The writing skill in our context is to send the right message to other people. Some people prefer bullet points, some are able to build nice stories. It‚Äôs all good, there‚Äôs no one way of expressing yourself. As with every skill, we can always improve. Was my writing clear to others? Am I getting some signs, that my writing was too cryptic and not understood? Are other people involved in my writing? Do they reply? Am I too abstract? Am I starting the right conversations? Sometimes, not writing is the skill. Choosing where to direct people‚Äôs energy is also important. Are we discussing minor points for too long? Is this getting us closer to our goals? In every situation, where passionate people collaborate, some form of conflict or disagreement will appear sooner or later. This is not bad on its own. In fact, this is an opportunity to remind ourselves, how much we care. We care so much that even though we respect each other, we are ok to disagree, for the better good.\nWhat‚Äôs important here is again - the writing skill. We usually disagree in writing. Writing is wonderfully emotional, but also wonderfully asynchronous. I can take my time to think, why we disagree.\nMost importantly, we discuss ideas. We shouldn‚Äôt let our egos to dominate the conversation. It‚Äôs all about ideas, reasoning, finding what‚Äôs best. It‚Äôs clear, as Jason put it, that companies should be more interested in working with people who prefer to conflict ideas, not conflicting other people. Jason and Basecamp are against meetings. That‚Äôs a similar approach at Arkency. As developers, we usually work on deep problems. This requires our focus. Meetings are an interruption. In the spirit of Jason‚Äôs talk - if companies are products, then meetings are bugs. If you need access to some information and it‚Äôs not there for you, this is a bug in the company. If you need to wait for someone with answers to your questions, this is a bug. Remote/async companies (but also office-based companies!) usually have great documentations. Whenever you spot a missing information, it‚Äôs a good moment to fill the gap. Developers have the concept of the Scout rule, always leave the code in a better shape. People in companies can follow the same rule in regards to documentation. Always leave it in a better shape. At Arkency, we constantly come up with some ideas. Sometimes one sentence is enough to express an intent. Jason‚Äôs point on long pitches got me thinking. New ideas are meant to change the current situation. Is my description clear enough? I have a lot to improve here. Jason said that at Basecamp almost everybody has access to almost everything. This is similar at Arkency. However, Jason made an interesting point - when people get access to everything, they feel obliged to follow everything. This is not always the best way to spend people time - to follow all teams/projects/channels/ideas. In some companies, people write summaries on what they did or will do every day. In other companies it maybe every week. The point here is to let other people know what is going on. As Jason clarified it - they‚Äôve had it different when there were 20 people than now when there‚Äôs 50 people. I see the same at Arkency, we keep the company intentionally small (15-20 people) not to get caught in the communication problems. This is probably the main take-away of the talk. As a software company we‚Äôre familiar with the concept of product. A product has features, it has bugs, it may have a backlog of things to do. This can all help shaping the company. A company, similarly to a product needs a constant process of iterating on. This was just a short list of the Jason‚Äôs tips, go watch the whole talk . It‚Äôs worth a mention that Basecamp released a book, called Rework where they share many such thoughts. At Arkency, we have released a book which reflects our process based on Async and Remote, called Async Remote . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-11"},
{"website": "Arkency", "title": "How mutation testing causes deeper thinking about your code + constructor for an included module in Ruby", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/constructor-for-a-included-module-in-ruby/", "abstract": "This is a short story which starts with being very surprised by mutation testing results and trying to figure out how to deal with it. Consider this module from aggregate_root gem which is part of Rails Event Store . In two places it uses @unpublished_events = [] . However, besides normal specs, we also check out mutation testing coverage using mutant . If you are not familiar with the technique, in short, it works like this: If the specs continue passing, it might indicate that you have a missing test. However, one thing we realized over time is that mutant often tries to tell you something deeper and point out a potential, higher level problem with your design. With every mutation detected it‚Äôs good to dig a bit deeper and ask yourself why 5 times :) Let‚Äôs discuss a simple example. Mutant decided to change into Introducing nil in random places is a great technique to discover untested or unused code. After all, if changing an assignment to nil does not break your code, why do you need it at all? Either you don‚Äôt need it and you can remove the line of code, or you miss a spec that properly verifies this line of code. My first reaction was fuck you, that doesn‚Äôt make any sense . Why would you change an empty array to a nil? I think about @unpublished_events that it is an Array . You can see that in and in: so fuck off mutant, mkay? üò§ Then I calmed down and started thinking about it. üòú I changed the code manually from: to I run the tests and they passed. I don‚Äôt know what I was thinking, obviously, they passed, mutant already verified that they pass in such case. That‚Äôs why I am here. But I didn‚Äôt believe so I made this change manually and of course specs passed. I was confused. I looked into those specs to find out if we were missing some cases and we did not. So why was everything working? - I wondered. And I quickly realized. Because of the getter with a default Array: 3 places in this code which need @unpublished_events always access them via this getter: So even though @unpublished_events is nil , it will work because upon reading it will become an empty array and work nicely with << and size methods. Then I asked myself‚Ä¶ Why are we using this unpublished_events getter at all? Why do we even need it? Why not use @unpublished_events everywhere? And the answer was‚Ä¶ Because we don‚Äôt set @unpublished_events = [] in a constructor. You see, usually, you use the library in two ways. You load historical domain events when you edit an object. or we may skip loading historical domain events for new records and go straight into changing their state and saving in DB. In this second case, we want to append new domain events to @unpublished_events collection but it is a nil . Using the unpublished_events getter workarounds this problem. Ok, so we don‚Äôt set @unpublished_events = [] in a constructor. But why? Why don‚Äôt we do that? Because AggregateRoot is a module and not a class. This led me to next two questions: should AggregateRoot be a module that you include or a class to inherit from? I prefer that it is a module. can we have constructors for modules? It turns out we can with the little help of prepend which is available in Ruby for years now. Check it out. You might be thinking why not simply: But there are some problems: another developer might forget to call super in a class constructor to trigger AggregateRoot#initialize and @unpublished_events will be nil . Inside AggregateRoot#initialize we don‚Äôt know how many arguments we should provide for a parent class constructor If we try to workaround the previous problem by not calling super from our module we get into problems in different situations. It seems to me that while prepending Constructor can work it does not seem to be very intuitive. If I had many instance variables to set I could consider it. But with one or two, I think I am gonna stay with a getter and a default value. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-13"},
{"website": "Arkency", "title": "Physical separation in Rails apps", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/physical-separation-in-rails-apps/", "abstract": "I‚Äôve been just following an interesting discussion within our Rails/DDD community . The particular topic which triggered me was a debate what kind of separation is possible within a Rails app which aims to be more in the DDD spirit. I remember, back in the school, I was taught about the difference between the physical separation of components vs conceptual separation. This was a part of a software engineering class I took. I hope I got it right. If we translate it into Rails terminology + some bits of DDD, we want to have the conceptual separation at the level of Bounded Context. A Bounded Context is a world in its own. It has its own ‚Äúmodel‚Äù (don‚Äôt confuse it with the ActiveRecord models) which represents the domain. Your typical Rails app can have 5-50 different bounded contexts, for things like Inventory, Invoicing, Ordering, Catalog. If you also go CQRS, then some of your read models (or collections of them) can also be considered bounded contexts, but that‚Äôs more debatable. Anyway, once you have the bounded contexts (some people like to call them modules or components), that‚Äôs a conceptual concept. Often you implement it with Ruby namespaces/modules. But what options do you have for a physical separation? You can just create a new directory for the bounded context files and keep the separation at this level. It can be at the top-level directory of your Rails app or at the level of lib/ .  This usually works fine, however you need to play a bit with Rails autoloading paths to make it work perfectly. This is probably the default option we choose for the Rails apps at Arkency. Another level is a gem. It can be still within the same app directory/repo, but you keep them in separate directories, but declare the dependency at the Gemfile level. The same as above, but you also separate the repos. This can create some more separation, but also brings new problems, like frequent jumps between repos to make one feature. This is a way, probably most in the spirit of The Rails Way. Which is both good and bad. Technically it can be a rational choice. However, it‚Äôs a tough sell, as usually people who jump into Rails+DDD are not so keen to rely on Rails mechanisms too heavily. Also, many people may think of that separation as at the controllers level, which doesn‚Äôt have to be the case. BTW, splitting at the web/controllers level is an interesting technique of splitting your app/infra layer, but it‚Äôs less relevant to the ‚Äúdomain‚Äù discussions\". I like to split the admin web app, as it‚Äôs usually a separate set of controllers/UI. The same with API. But still, this split is rarely a good split for your domain, that‚Äôs only the infra layer. Anyway, Rails engines can be perfectly used as a physical separation of the domain bounded contexts. If you don‚Äôt mind the dependency on Rails (for the physical separation mechanism) here, then that‚Äôs a good option. It‚Äôs worth noting that gems and engines were chosen as the physical separation by our friends from Pivotal, they call it Component-Based Rails Applications . This approach relies on having multiple nodes/microservices for our app. Each one can be a Rails application on its own. It can be that one microservice per bounded context, but it doesn‚Äôt need to be like that. In my current project, we have 6 microservices, but >15 bounded contexts. I wasn‚Äôt a big fan of microservices, as they bring a lot of infrastructure overhead. My opinion has changed after I worked more heavily with a Heroku-based setup. The tooling nowadays has improved and a lot is offered by the platform providers. It‚Äôs worth noting that you can separate the code among several repos. However, you can also keep them in one monorepo. With a heroku-based setup, it seems to be simpler to keep them separated, but one repo should also be possible. Microservices also allow another separation - at the programming language level. You can write each microservice in different languages, if it makes sense for you. It‚Äôs an option not possible in previous approaches. This is a relatively new option and probably not mostly considered. Especially that the current serverless providers don‚Äôt support Ruby out of the box.\nServerless is quite a revolution happening and they can change a lot in regards to the physical separation. What‚Äôs possible with serverless is to not only separate physically bounded contexts, but also the smaller building blocks, like aggregates, read models, process managers (sagas). I‚Äôm not yet experienced enough with how to use it with Rails, but I‚Äôm excited about this option. As with microservices, this gives an option to use a different programming language, but at a lower scale. While, I‚Äôd be scared to implement a whole app in Haskell, I‚Äôm super ok, if we implement one read module or one aggregate in Haskell. In the worst case, we can rewrite those 200 LOC.\nAnother big deal with serverless is the fact that they handle the HTTP layer for you. Does it mean ‚Äúgood bye Rails‚Äù? I‚Äôm not sure yet, but possibly it can reduce the HTTP-layer of our codebases to a minimum. The nice thing with a DDD-based architecture of your application is that it mostly works with whichever physical separation you choose. It‚Äôs worth noting that those physical mechanisms can change over time. You can start with a Rails engine, then turn it into a microservice and then split it into several serverless functions. How about you, how do you physically separate the modules/bounded contexts of your Rails apps? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-14"},
{"website": "Arkency", "title": "Rails components ‚Äî neither engines nor gems", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/rails-components-neither-engines-nor-gems/", "abstract": "There has been a very interesting discussion today on #ruby-rails-ddd community slack channel . The topic circulated around bounded contexts and introducing certain ‚Äúcomponent‚Äù solutions to separate them. There are various approaches to achieve such separation ‚Äî  Rails Engines and CBRA were listed among them. It was, however, the mention of ‚Äúunbuilt gems‚Äù that reminded me of something. Back in the day, we had an approach in Arkency in which distinct code areas were extracted to gems. They had the typical gem structure with lib/ and spec/ and top-level gemspec file. Each gem had its own namespace, reflected in gem‚Äôs name. In example Scanner with scanner . It held code related to the scanning context, from the service level to the domain. You were able to run specs related to this particular area in separation.\nIn fact, at that time we were just starting to use the term Bounded Context. Yet these gems were not like the others. We did not push them to RubyGems obviously. Neither did we store them on private gem server. They lived among Rails app, at the very top level in the code repository. They‚Äôre referenced in Gemfile using path: , like you‚Äôd do with vendored dependencies. That way much of the versioning/pushing hassle was out of the radar. They could change simultaneously with the app that used them (starting in the controllers calling services from gems). Yet they organized cohesive concept in one place. Quite idyllic, isn‚Äôt it? Well, there was only one problem‚Ä¶ Code packaged as gem suffers from Rails code reload mechanism. While that rarely bothers you with the dependencies distributed from RubyGems that you‚Äôd never change locally, it is an issue for ‚Äúunbuilt‚Äù gems. Struggle with Rails autoload is real. If you keep losing battles with it ‚Äî go read the guide thoroughly. That was also the reason we disregarded the gem approach. The solution we‚Äôre happy with now does not differ drastically from having vendored gems. There‚Äôs no gemspec but the namespace and directory structure from a gem stay. The gem entry in Gemfile is gone. Any runtime dependencies this gem had, go into Gemfile directly now. What differs is that we no longer have require to load files. Instead, we use autoload-friendly require_dependency . With that approach, you also have to make sure that Rails is aware to autoload code from the path your Bounded Context lives in. And that‚Äôs mostly it! As an example you put Scanner::Ticket into scanner/lib/scanner/ticket.rb as: If you wish to painlessly run spec files in the isolated directory there are certain steps to take. First, the spec helper should be responsible to correctly load the code. Then the test files should require it appropriately. Last but not least ‚Äî it would be a pity to forget to run specs along the whole application test suite on CI. For this scenario we tend to put following code in app spec/ directory: The solution picture above is definitely not the only viable option. It has worked for me and my colleagues thus far. No matter which one you‚Äôre using ‚Äî deliberate design with bounded contexts is a win . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-15"},
{"website": "Arkency", "title": "All the ways to generate routing paths in Rails", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/all-the-ways-to-generate-routing-paths-in-rails/", "abstract": "Have you ever considered how many ways there are to generate a routing path in Rails? Plenty. Let‚Äôs see. Imagine that your config/routes.rb contains: So the generated routes are: And in the admin panel, on the list of exercises ( Admin::ExercisesController#index action which renders admin/exercises/index.html.erb ) you want to link to a single exercise. What are your options? There are some obvious ones and less obvious ones. You can just use edit_admin_exercise_path(exercise) or edit_admin_exercise_path(exercise.id) or edit_admin_exercise_path(id: exercise.id) . If I recall correctly, all of them are going to work. I think this is the most often used option. Of course instead of _path sometimes you are going to need _url . You can use [:edit, :admin, exercise] . In some places I‚Äôve seen developers using [:admin, exercise] for admin_exercises route, but I‚Äôve never seen this syntax being used for edit_admin_exercise routes and similar ones. I don‚Äôt expect my coworkers to use this version and during refactorings I most likely won‚Äôt find this usage. You don‚Äôt see this version used very much since Rails introduced named routes. But that will work as well. It probably accepts strings and symbols as values for action/controller keys. If you are linking between actions in the same controller you can omit the controller parameter. I always liked this syntax for one reason. It does not include controller‚Äôs name. That means if you ever decide to rename Admin::ExercisesController to Moderator::ExercisesController or Admin::PublicExercisesController and make some changes in your config/routes.rb so that the routes and controllers follow the same naming convention, at leas you would not need to change the links that go between views in the same controller. One less thing to worry about when doing refactorings. You can always provide a hand-crafted String as the path. LOL üòã . I would rather not do it, ever :) Usually we use named routes such as edit_admin_exercise which are available via methods. But the other options are worth knowing about and they can be useful when you need to generate URLs to various actions or controllers more dynamically. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-18"},
{"website": "Arkency", "title": "Custom application configuration variables in Rails 4 and 5", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/custom-application-configuration-variables-in-rails-4-and-5/", "abstract": "You probably know that you can configure Rails in config/application.rb and config/environments/development.rb etc. But you can also leverage that for configuring your own custom settings for your application. As with normal rails configuration, your own settings in config/environments/* take precedence over those specified in config/application.rb . If you have multiple settings that you would like to group together, you can prefix them with x.config_name . or you can just use Hash To read those preference, you can use Rails.application.config or its shorter version Rails.configuration . It will work anywhere in your application, including models and controllers. What‚Äôs the magic that allows you to define the keys in such way? Let‚Äôs see if the setters and getters are implemented using method_missing ? Indeed they are. Let‚Äôs see the code. That‚Äôs quite a simple and small implementation. If it recognizes that the method name ends with = it sets the value. If it does not end with = , it sets ActiveSupport::OrderedOptions.new as the value and returns it. So the 1st level ( x.one , x.one= ) is managed by Rails::Application::Configuration::Custom class and its implementation. The 2nd level ( x.one.two , x.one.two= ) is managed by ActiveSupport::OrderedOptions . Let‚Äôs see what‚Äôs there. And that‚Äôs how it works on the 2nd level. If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your everyday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring big, complex Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-16"},
{"website": "Arkency", "title": "Composable RSpec matchers", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/composable-rspec-matchers/", "abstract": "While developing RSpec matchers for RailsEventStore we figured out that in some cases it would be good to compose multiple matchers together. The core scenario would be checking whether given event is part of a collection. It‚Äôs a common case in our customers‚Äô applications. We want to verify if a certain domain event has been published. We make an assertion on a given RailsEventStore stream and check whether the event is in place. But let‚Äôs go with a simple example: It should work, since it is possible to build an expectation like: But nope, not gonna happen: Can you see what happened? There‚Äôs an RSpec matcher in actual collection rather than expected domain event instance. We quickly figured out that our custom matcher is missing some behavior. The one which allows composing it with other matchers. We dived into the RSpec‚Äôs codebase and found out that RSpec::Matchers::Composable mixin is our missing block. As docs state: Mixin designed to support the composable matcher features of RSpec 3+. Mix it into your custom matcher classes to allow them to be used in a composable fashion. It works in a quite simple manner by delegating === to #matches? . It allows matchers to be used in composable fashion and also supports using matchers in case statements. Adding include ::RSpec::Matchers::Composable to our BeEvent matcher class made the test passing. That was quick . Sometimes you expect your domain event to contain data provided by a database. It‚Äôs hard to expect specific value. You can build an expectation using kind_of built-in matcher: or include : There a cases where we want to be very precise about domain event data. In such situation, we add strict matcher. Strictness applies both to domain event data and metadata. If you want to be strict for data, but give more room to metadata, you can go with compound and expression: It‚Äôs also a part of RSpec‚Äôs Composable mixin, together with or . There‚Äôs a very good post on RSpec blog explaining Composable matchers . It‚Äôs 4 years old now, but still a good read. Especially if you‚Äôre curious how RSpec internals work. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-18"},
{"website": "Arkency", "title": "Which ruby version am I using ‚Äî how to check?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/which-ruby-version-am-i-using-how-to-check/", "abstract": "Are you not sure which Ruby version you are using right now? Wondering how to check it? Say no more. Here are two simple ways to check for it. Run irb and type: Just type ruby -v Are you using RVM? Run rvm current and get the answer Are you using rbenv? Just run rbenv version Do you want to know where your ruby binary is installed? It can also sometimes reveal the version you are using as it is usually part of directory structure. Just run which ruby . If you want to know even more about your current ruby setup, there is a command for that as well! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-29"},
{"website": "Arkency", "title": "How to overwrite tojson (asjson) in Active Record models in Rails", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/how-to-overwrite-to-json-as-json-in-active-record-models-in-rails/", "abstract": "Let‚Äôs say you have a model in Rails with certain attributes and columns. When you serialize it with to_json , by default Rails will include all columns. How can you add one more or remove some from now appearing there? In the simplest way you can define the as_json method on your model. You can call super to get the standard behavior from ActiveRecord::Base and then add or remove more attributes on that result. Imagine that you have Event class. And initially the JSON looks like. You might want to remove created_at , updated_at and add one new field such as is_single_day_event . You can do it that way: And now when you call event.to_json you are going to get If the logic around serializing your objects gets more complex over time or you need multiple representations of the same model, I suggest you decouple JSON serialization from ActiveRecord. You can do it by using of the gems such as ActiveModel::Serializers or similar. Potential gems for you to investigate If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your\neveryday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring big, complex Rails applications. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-09-26"},
{"website": "Arkency", "title": "One simple trick to make Event Sourcing click", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/one-simple-trick-to-make-event-sourcing-click/", "abstract": "Event Sourcing is like having two methods when previously there was one. There ‚Äî I‚Äôve said it. But it isn‚Äôt my idea at all. It was Greg that used it first, in a bit different context. When explaining CQRS he used this exact words: Starting with CQRS, CQRS is simply the creation of two objects where there was previously only one . The separation occurs based upon whether the methods are a command or a query (the same definition that is used by Meyer in Command and Query Separation, a command is any method that mutates state and a query is any method that returns a value). You can have quite a similar statement on event-sourced aggregate root. The separation occurs based upon whether the method: Not convinced yet? Let the examples speak. Below is a typical aggregate root. In the scope of the example there are only two actions you can take ‚Äî via public register and supply methods. In event sourcing it is the domain events that are our source of truth. They state what happened. What we need to do is to make them a bit more useful and convenient for decision making. This is the sourcing part. In this step we‚Äôve drawn the line between making a statement that something happened (being possible to happen first) and what side effects does it have. Notice private registered and supplied methods. Why make such effort and introduce indirection? The reason is simple ‚Äî if the events are source of truth, we could not only shape internal state for current actions we take but also for the ones that happened in the past. Instead of loading current state stored in a database, we can take collection of events that happened in scope of this aggregate ‚Äî in its stream. At this point you may have figured out that event_store dependency that we constantly pass as an argument belongs more to the infrastructure layer than to a domain and business. What if something above passed a list of events first so we could rebuild the state? After an aggregate action happened we could provide a list of domain events to be published ( unpublished_events ): More or less this reminds the aggregate_root gem that is aimed to assist you with event sourced aggregates. The rule of having two methods when there was previously one however still holds. Have a great day! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-02"},
{"website": "Arkency", "title": "Event-sourcing whole app ‚Äî opinions", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/event-sourcing-whole-app-opinions/", "abstract": "You might have heard the ‚ÄúEvent-sourcing your whole app in an anti-pattern‚Äù meme. It‚Äôs not uncommon to get exposed to it once you find yourself speaking with another practitioner of this technique. I had ‚Äî just recently, on a Rails DDD training that we‚Äôve organized. To be honest I‚Äôve heard about it before. This time however it provoked me to dig deeper. To find the reasons which led to formulate it. To understand the context in which it makes most sense. No surprise that the original quote comes from the godfather of CQRS, Greg Young : The single biggest bad thing that Young has seen during the last ten years is the common anti-pattern of building a whole system based on Event sourcing. That is a really big failure, effectively creating an event sourced monolith. CQRS and Event sourcing are not top-level architectures and normally they should be applied selectively just in few places. The above except is extracted specifically from A Decade of DDD, CQRS, Event Sourcing that Greg gave at DDD Europe 2016. Main takeaways: After learning from Greg, let‚Äôs examine what Udi Dahan says on similar topic : Most people using CQRS (and Event Sourcing too) shouldn‚Äôt have done so. On the audit logs, that come for free, as a side-effect, with Event Sourcing: Who put you in a position to decide that development time and resources should be diverted from short-term business-value-adding features to support a non-functional requirement that the business didn‚Äôt ask for? (‚Ä¶) you can usually implement this specific requirement with some simple interception and logging. About the ‚Äúproof of correctness‚Äù in Event Sourcing: While having a full archive of all events can allow us to roll the system back to some state, fix a bug, and roll forwards, that assumes that we‚Äôre in a closed system. We have users which are outside the system. If a user made a decision based on data influenced by the bug, there‚Äôs no automated way for us to know that, or correct for it as we roll forwards. Not only me ‚Äî also Andrzej decided to research some pros and cons: Being honest about a technology or a technique is not only knowing ‚Äúthe good parts‚Äù but also what pitfalls to be aware of. There is No Silver Bullet . In the end you‚Äôll have to make a judgement ‚Äî in the context of your application and the business domain it supports. It is however good to know your options first and what price tag does each of them comes with. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-10"},
{"website": "Arkency", "title": "Could we drop Symbols from Ruby?", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/could-we-drop-symbols-from-ruby/", "abstract": "Don‚Äôt know about you, but I personally have been hit a least a dozen times by bugs caused by strings vs symbols distinction. That happened in my own code, and it happened when using some other libraries as well. I like how symbols look in the code, but I don‚Äôt like the specific distinction that is made between them and strings. In my (perhaps controversial opinion) they introduce more problems than they solve. So I was thinking‚Ä¶ Maybe we could drop them? Sounds radical right? But I don‚Äôt think rewriting thousands of Ruby libraries to remove every :symbol is a viable strategy. So maybe there is a different option? Maybe symbol literals could become frozen, immutable strings. How could that work? It‚Äôs hard for me to describe a solution very well in long paragraphs. So I thought I would rather try to demonstrate the properties that I imagine and let the code speak for itself. So‚Ä¶ Imagine a world in which‚Ä¶ This is what I started with. My goal. I don‚Äôt want to care anymore if I have a string or symbol. Of course, nothing is that easy. We need more properties (test cases) to fully imagine how that could work. Usually my use-case is about taking something out of a hash or putting into a hash. Let‚Äôs express it. That would make my life easier :) For that we need: Whenever you put or get something out of a Hash (or Set ) Ruby uses Object#hash as input to a hashing function. If two objects are equal they should return the same hash . Otherwise Ruby won‚Äôt properly find objects in a Hash. Let me show you an example: You defined a Value Object. A class that is defined by its attributes (one or many) and which uses them for comparison. But because we haven‚Äôt implemented hash method, Ruby doesn‚Äôt know they can be used interchangeably as Hash keys. If two objects return the same hash on the other hand that does not mean they are equal. There is a limited number of hashes available so conflicts can rarely occur. But if two objects are equal, they should return the same hash. Usually, you compute the hash as hash of array of all attributes XORed with a big random value to avoid conflicts with that exact array of all attributes. In other words, we want: But that was a digression. Let‚Äôs get back to the merit. I would love: And for that we would need: But here is the thing. It might be that computing a Symbols‚Äôs hash is 2-3 times faster than a String‚Äôs hash right now . I don‚Äôt know why. Maybe Symbols, which are immutable have a pre-computed hash or can have a memoized hash value because it won‚Äôt change. I am not sure. But if that‚Äôs the reason, I can imagine that frozen, immutable Strings could have lazy-computed, memoized hash value as well. I believe there is a lot of libraries and apps out there that rely on that fact: So obviously that should be preserved. But I believe if symbols were strings and Ruby would internally keep a unique list of them, just like doing it today for us, everything would work without a problem. After all, the fact that you always get the same symbol is just a mapping somewhere in Ruby implementation from Historically, it was not even garbage-collected. Now it is. So with: somewhere there in Ruby internals, we could still get the same object when we ask for :foo : Let‚Äôs continue this journey. Here is a problematic area: String#to_s basically returns self in Ruby. So if Symbols were frozen strings this would break: because bar would be the same object as foo instead of a new string (like it is right now for Symbols). Here is another potential issue. There might be libraries out there checking if an object is a symbol. I was thinking how to solve it‚Ä¶ How could we distinguish :foo from \"foo\" if we really needed. I see two options. Make Symbol work like String without making it a String (either by adding all the methods or making it an alias Symbol = String ).\nAnd another option. Make Symbol inherit from String so Symbol < String . With that would be true. But‚Ä¶ would also be true. The difference could be that Symbol#to_s would be redefined to return new, identical, unfrozen String instead of the same one. So maybe something like that. I doubt that‚Äôs gonna happen. Probably too many corner cases right now to introduce such a change. But if we could drop Fixnum and Bignum , maybe we can drop Symbol ? Would we even want to? What‚Äôs your opinion? Do you need Symbol class in your code? Or do you just like the symbol notation? I will leave you with a quote by Matz Symbols are taken from Lisp symbols, and they has been totally different beast from strings. They are not nicer (and faster) representation of strings. But as Ruby stepped forward its own way, the difference between symbols and strings has been less recognized by users. And if you think that would be a bad idea, let me tell you that he tried but failed. We tried & failed. Link:  Could we drop Symbols from Ruby? | Arkency Blog https://t.co/um0xnWSpcQ I guess too many libraries out there rely on checking if an object is Symbol or not. And in Smalltalk Symbols inherit from Strings: Hey, Just a comment on the article: In Smalltalk, Symbols actually do inherit from string: pic.twitter.com/YtQO91N0p7 Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-02"},
{"website": "Arkency", "title": "Two ways for testing preloading/eager-loading of ActiveRecord associations in Rails", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/two-ways-for-testing-preloading-eager-loading-of-activerecord-association-in-rails/", "abstract": "As a developer who cares about performance you know to avoid N+1 queries by using #includes , #preload or #eager_load methods . But is there a way of checking out that you are doing your job correctly and making sure that the associations you expect to be preloaded are indeed? How can you test it? There are two ways. Imagine we have two of these classes in our Rails application. An order can have many order_lines . We implemented Order.last_ten method which returns last 10 orders with one eager loaded association. Let‚Äôs see how can make sure that the lines are preloaded after calling it. Because we preload(:order_lines) we are interested whether order_lines is loaded. To check that we need to get one Order object such as orders[0] verify on it. There is nothing to check on orders collection that could tell us if the association is loaded or not. The test in Rspec would look quite similar ActiveRecord library has a nice helper method called assert_queries which is part of ActiveRecord::TestCase . Unfortunately, ActiveRecord::TestCase is not shipped as part of ActiveRecord. It is only available in rails internal tests to verify its behavior. We can however quite easily emulate it for our needs. Imagine a scenario in which you operate on a graph of Active Record objects but you don‚Äôt return them. You just return a computed values. How can your verify it in such case that you don‚Äôt have the N+1 problem? There are no observable side-effects, no returned records to check if they are loaded? . But‚Ä¶ aren‚Äôt they really? In this situation. How can you test that Order.average_line_gross_price_today does not suffer from N+1 queries? Is there a way to make sure order.order_lines.map(&:gross_price) is not triggering a SQL query when reading order_lines ? It turns out there is. We can use ActiveSupport::Notifications and get notified about every executed SQL statement. If you go that way make sure to create enough records to detect potential issues with eager loading. One order with one line is not enough because with and without the eager loading the number of queries would be the same. In this case only when you have 2 order lines you can see the difference in a number of queries with preloading (2, one for all orders and one for all lines) vs without preloading (3, one for all orders and one for every line separately). Always make sure your test is failing before fixing it :) While using this approach is possible, it tells me that it could be nice to split the responsibilities into two smaller methods. One responsible for extracting the right records from a database (IO-related) and one for transforming the data and doing the computations (no IO, side-effect free). You can check out db-query-matchers gem for RSpec matchers to help you with that kind of testing. If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your everyday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-16"},
{"website": "Arkency", "title": "Using state_machine with event sourced entities", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/using-state-machine-with-event-sourced-entities-ruby/", "abstract": "Our event sourced aggregates usually have a lifecycle and they need to protect some business rules. Often they start with guard statements checking if performing given action is even allowed. I was wondering if there was a nice way to remove those conditional and make the code more explicit. I wanted to experiment with porting the code from our book to use state_machine gem and see if the results are promising. My starting point was a class looking like this: I removed some parts which are not interesting for this discussion. As you can see, often methods start with some state checks. Sometimes it‚Äôs one state that the Order must be in: Sometimes it‚Äôs two or more: Sometimes we want idempotency instead of an error: So when you try to expire an already expired Order , we will do nothing. This can be in case our app is expiring in a reaction to a message coming from a message queue and we could get duplicated messages. So I was wondering if using state_machine would make the code more readable and expressive. I didn‚Äôt even need to make the rules exactly the same. I just wanted to get a feeling how it could look like and If I enjoyed it more or less. One of the things I noticed recently is that the more versions of code I have solving the same problem the better I understand their good and bad sides. I learn what works for me and what not. So let‚Äôs see a version using state_machine It is interesting. If a method can be called in one state only you can define it inside that state definition: If you try to call it in another state, you will get NoMethodError . On the other hand, the exception won‚Äôt be as good as our custom exception which clearly shows line number. It‚Äôs easier to have a look at that line of code and understand that a method was called in an invalid state compared to understanding a NoMethodError without any meaningful info. It is possible to define a method in 2 states only: But the benefits are not so big in my opinion. You don‚Äôt see this method around other methods available for draft or submitted states and this meta-programming statement is not much better than my custom if statement. I also experimented with: but that was the worst. Computers can understand that but for humans like me, it‚Äôs unbearable. I can‚Äôt easily recall all possible states in and remove expired and shipped from them, to understand where could that method be allowed or not. Notice that I used only a very small subset of the gem‚Äôs features. That‚Äôs on purpose. I looked into event transitions and transition callbacks but I could not find a nice way for them to play with the expectation that the only way to change a state of an event sourced object is via applying events. I could define expire method that could transition into expired state: But that would set the state directly, instead of indirectly via a domain event that I would later save in a database. So at the end, I decided that it‚Äôs probably not worth in many use-cases to use this particular gem with our way of writing event sourced entities. The benefits are small and most features of the library cannot be used easily without introducing problems. Perhaps there is a different library out there for defining state machines that could play nicer with event sourcing and aggregate_root . But I haven‚Äôt found it yet. The struggle for a nice code involves a lot failed experiments. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-15"},
{"website": "Arkency", "title": "Make your Ruby code more modular and functional with polymorphic aggregate classes", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/make-your-ruby-code-more-modular-and-functional-with-polymorphic-aggregate-classes/", "abstract": "For the last year, I‚Äôve been using Rails (as I do for the last 10 years), but last year it was almost exclusively Rails ‚ÄúNot the Rails Way‚Äù. We have successfully combined Rails with Domain-Driven Design, CQRS and Event Sourcing. Over the last year, most of the business logic state in my apps was persisted using events, not the ‚Äúlet‚Äôs just store the last state and forget the history‚Äù ;) In short, I was using a lot of Event Sourcing. Event Sourcing is not easy and it does take time to get used to it. We are event sourcing our aggregates. Aggregates are like the more important objects in our systems, like Order, Product, User, Project etc. Aggregates are just normal objects, but if you combine them with event sourcing they take this specific shape. We have developed a library called AggregateRoot (part of our RailsEventStore ecosystem of tooling), which helps a lot with Event Sourcing. Once you start modelling your domain (aka what objects we should have and how they communicate), you will arrive to the problem of finding ways of making your aggregates smaller. Today, I‚Äôd like to share an experimental technique with you. For now it‚Äôs just a proof of concept, but hopefully soon this will be something I will be able to use in my apps. Usually, I‚Äôd have a class like Order and it would have several methods, like: Depending on the current state of the order, we‚Äôd disallow certain actions, either via exception or whichever else favourite technique for returning the result: In a way, this is like a state machine and could be implemented with some help of the state_machine gems or similar. I wanted to try out a different attempt. For each state of the order let‚Äôs have a separate class and use polymorphism to call the methods. The initial proof of concept looks like this: As you can see after every call to the public method, we call the apply method - this is provided by the AggregateRoot gem. Under the hood it makes sure the event is published and then it calls the appropriate private method. Those private methods usually just set the state. However, in my spike, they actually also return a new kind of object, depending on the state. It‚Äôs still not production-ready, but I find it a good start for more research. I like how the classes become small. Most importantly I like how all the if statements disappear and I don‚Äôt even need to signal the result anymore. In pure OOP, calling other objects is sending messages to them. In Ruby it‚Äôs actually exactly like that. Under the hood, we have the send mechanism for that. If no such method exists, then the message can‚Äôt be delivered. In a way, this is relying on the message-driven approach in Ruby objects. Another nice side-effect (pun intended) is the fact that the objects are now immutable. They don‚Äôt change existing state, they return a new state instead. Which brings OOP and FP nicely together in a nice domain modelling example. If you want to learn more about aggregates and event sourcing with Ruby/Rails consider buying our ‚ÄúDomain-Driven Rails‚Äù book. If you wonder where such a OOP/FP merge can bring us, consider learning more about Serverless , which is like a DDD/Microservices/FP heaven. Thanks for reading! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-16"},
{"website": "Arkency", "title": "Relative Testing vs Absolute Testing", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/relative-testing-vs-absolute-testing/", "abstract": "Recently I had one of these small aha! moments when something strikes you and a concept clarifies in your head. This time it was about testing. I am not even sure if this is something new or obvious. I know it‚Äôs something that so far I haven‚Äôt been paying attention to consciously and it made testing unnecessarily harder for me . This recognition of the difference between both approaches comes purely thanks to using mutation testing. When you really try hard to make sure every part of code is tested properly it makes you think twice sometimes. In the simplest, most basic terms I can describe absolute testing as one in which you use equality comparison assert_equal(...) / expect(...).to eq(...) with a specific value. And relative testing is when you use greater-than or less-then or not-equal types of assertions. But also when you compare equality but with non-specific values. When you compare equality with other values generated by your code rather than an exact value expected and provided by your test. But that‚Äôs just superficial explanation. It‚Äôs sometimes much more subtle than that. Here is what I realized recently. When we decide to test a part of the code based on the requirements we often lean towards relative or absolute testing. We are primed with our own thoughts, expectations and first tests. Then we reach a case or obstacle in testing and it‚Äôs hard for us to move on. But what could help us is switching the mindset from relative to absolute testing or the other way. When you want to implement an object to be equal by values instead of equal by reference you need to overwrite the == method. But you should also implement the hash method which is used when putting the object in or out of Hash (or Set ) as part of the hashing function. This is part of a class that I have implemented. Let me present you some of the tests. The hash depends on event_id : The hash depends on event‚Äôs data : The hash depends on event‚Äôs class: The event‚Äôs hash does not collide with the hash of an Array keeping event‚Äôs class, event_id and data. Thanks to proper implementation I can put events into Set or Hash and they are properly recognized even if I don‚Äôt use the same instance but rather a new instance with identical values. Now if you look at all the tests carefully you will realize that in not a single place have I specified the exact value that hash function should return. Every test case is based on relativity here. The hash value must be equal to another hash value also generated by my code. Or not equal to another hash value generate by my code. But it never in plain sight says that the hash in a certain situation should be equal 8061336739304082551 . Every part of the implementation code is there for something. For example the XOR. The XOR ( ^ ) operator is there so that we can avoid this collision: It‚Äôs just a random, big value that I generated. You could change a random bit in this number and the code would still work. If you have an experience with mutation testing you know that it routinely checks for off-by-one errors by mutating numbers in your code to a bigger or lower number and checking whether the code fails. But if you mutate BIG_VALUE to BIG_VALUE -1 or BIG_VALUE +1 the code still works and no test fails. All of those values are as good as any other. What does it mean? I have no explanation for my tests why I want this number over another number. What am I missing? One absolute test. You might scream üôÄüôÄ but that test has no value üò±üò± . Run it on your computer, CI, ruby 1.9 - ruby 2.5, JRuby, Rubinius and the value might differ and your test can fail. What could that tell you? Maybe, whether your hashing function is stable in a distributed, heterogeneous environment. Imagine that you write data in certain order: and you expect it to come back in the same order. Now imagine that somewhere in the implementation of #publish_event method there are lines such as: that makes our events in stream indexed from 0. So first event‚Äôs position is 0, next one is 1, and then there is 2‚Ä¶ Here is the thing. From the point of our API, it does not matter how they are internally numbered and stored in the database. If we went with: it would work equally well and the tests would be still passing. Our data would be indexed from -7 which sounds silly, but for computers, it works. So yeah. What‚Äôs the difference üòâ ? Why does the test still work? Because your tests are relative. They compare the order of written events and the order of read events and make sure they are identical. Write A, B to X . Read X and get A, B back. Why did I choose to have the numbers indexed from 0? Because I am used to? Because that is usually the default (in most languages). Because of aesthetics, I guess‚Ä¶ So how can I justify this -1 in the code? I need one non-relevant test but rather an absolute one. Granted it‚Äôs not important for the public api of the client that I am testing: And you might scream üôÄüôÄ but that test has no value üò±üò± . But this kind of things matters when you want your code to use the same convention between multiple releases/versions. If some data is written in a previous version of the code and some data is written in the next version of the code they might be inconsistent. So even though this implementation detail (exact position number) is not exposed to the clients of this API, you might still want to pinpoint it to a single specific number and keep consistent between releases . I don‚Äôt recommend doing a lot of those tests. I usually try to design my tests so that they operate on the same layer for setup/preparation and for verification. But checking the implementation detail of one layer below, in this case, can be beneficial because these values get persisted forever. This example is similar to the previous one. Imagine that inside read_stream_events_forward you have code similar to: but mutation testing tells you that if you remove order(\"position ASC\") the code still works and returns the rows in proper order. That‚Äôs usually the case because with a small amount of data, the DB will often return those records in the same order they were inserted. But you don‚Äôt want to rely on that (p.s. even messing with auto-incremented IDs saved on DB might not help you because DBs will often use internal row ids). You want to be explicit. You know you are doing the right thing by explicitly specifying the order but it might be hard for you to create a test setup presenting the situation in which not providing the order fails the tests. Damn DBs. But I realized one thing. I don‚Äôt need to use relative testing which checks that writing A, B, C in that order leads to reading A, B, C in that order. I can capture the SQL statement generated by Active Record and verify it. Instead of checking if I got the right results I can check if I generated the right SQL query. Absolute instead of relative. When you get stuck in testing (especially if you want to make sure the last 5% keeps working as expected as well) it might be a sign that you hit the wall with your current approach. You might have been testing only using Relative Testing or only using Absolute Testing. Stop for a moment and consider if the other approach makes it easier to achieve your goal. I usually try to not cross the boundaries, not test too much implementation details because that makes refactorings harder. I prefer testing units over classes (remember: There is no such rule that there should be one test class per class ). But sometimes when all you have is relative, it might be good to introduce more specificity. And vice-versa. If you enjoyed the article, subscribe to our newsletter so that you are always the first one to get the knowledge that you might find useful in your everyday Rails programmer job. Content is mostly focused on (but not limited to) Ruby, Rails, Web-development and refactoring. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-19"},
{"website": "Arkency", "title": "Application Services ‚Äî 10 common doubts answered", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/application-service-ruby-rails-ddd/", "abstract": "You might have heard about the Domain-Driven Design approach to building applications. In this approach, there is this horizontal layer called Application Service. But what does it do? What arguments does it take? How many operations can it perform? How does it cooperate with other parts of our application such as controllers and models? So many questions, not so many answers. I decided to write down what I imagine this layer to look like based on the books and articles that I‚Äôve read and on my personal feelings after years of experimenting. In DDD-flavored applications, your domain logic lives mostly in the form of aggregates (and some bits in domain services). But that logic and object‚Äôs behavior is usually persistence-agnostic. It means that it is not our domain objects responsibility to worry how they are persisted. So where does it happen? In Application Services. That‚Äôs their primary role but not the only one. Let‚Äôs dissect Application Services. Application Service get‚Äôs domain objects from repositories and saves changed domain objects to repositories. If you migrate from Rails-way towards more Domain-Driven approach and you don‚Äôt have repositories yet but you continue using ActiveRecord this will look similar to: One of the simplest rules that you can follow to make your code more testable and easier to refactor in the future is to avoid calling save! (or save ) from your models. Don‚Äôt: it is not a responsibility of the domain object to save itself. It makes testing harder, slower and does not allow you to easily compose multiple operations without constantly saving to DB. So even if you use ActiveRecord, just don‚Äôt call save! from within the class. Only the application service should do it. The repository is often just one of the dependencies that our code need. Others can be I believe it can. But the other object ( Product ) should come from the same bounded context as the object we are updating ( Order ). There are opinions saying that it application services should not be doing it and it should only read and update one object. I am not convinced, however. There is also a fraction claiming that the 2nd object should not be another aggregate but rather a read-model. I believe it can be an aggregate from the same bounded context. It is not recommended as it increases coupling between objects (aggregates) that are being updated at the same time. Potential issues to consider: I believe it‚Äôs best if the service receives a command. The command can be implemented using your preferred solution - pure ruby, dry-rb, virtus, active model, whatever. Having commands can be beneficial if you want to easily see what‚Äôs supposed to be provided. It is more explicit, and it makes it more visible. The more attributes are provided by a form or API the more likely having this layer can be valuable. Also, the more complicated/nested/repeated the attributes are, the more grateful you will be for having commands. Commands can perform simple validations that don‚Äôt require any business knowledge. Usually, this is just a trivial thing like making sure a value is present, properly formatted, included in a list of allowed values, etc. The interesting aspect of defining a closed (not allowing every possible value) structure is that it also increases security and basically acts similar to strong_parameters . I think the controller fits nicely to the responsibility. It has access to all the data from HTTP layer such as currently logged user id (from a session or a token), routing parameters, form post parameters etc. And from all that, it can build a command by passing the necessary values. Ideally, I believe the layer which contains application service should define the interface of the command and a higher layer could implement it. What‚Äôs the difference? If we implemented the command in a higher layer (ie controller) then it could use objects it already has access to such as cookies, session, params etc. Because of Ruby‚Äôs duck-typing mechanism, this can work but, due to the lack of interfaces, it is not easy for the ApplicationService to describe the exact format it expects the data from the command. For simple commands that‚Äôs not a big issue, but the bigger they get and the more nested attributes they include the harder it is. One thing I considered as a substitute for interface was‚Ä¶ linters :) Such as rack linter or this . In other words shared examples distributed as parts of the lower layer (implementing an Application Service) that could be executed in the tests of the higher layer (controller). in case Test Unit frameworks this would be just a module with test methods to include: But I didn‚Äôt find this separation worthy and I never went that way. I would in a language with interfaces where defining the structure without implementation is pretty fast. In this version, the controller must copy the necessary data to a provided implementation of the command, which might be less convenient when there are more arguments. or I think it can and I would even say that it‚Äôs often beneficial to group many operations together in one Application Service instead of scattering them across multiple classes. Because usually, the use-cases need the same dependencies to finish and have a similar workflow. If you go with commands, you can even hide all those internal methods as private. I believe there is none, really. It‚Äôs just the names comes from 2 different communities (DDD vs CQRS) but they represent a similar concept. However, Command Handler is more likely to handle just one command :) Ideally, nothing. The reason most Application Services return anything is that the higher layer needs the ID of the created record. But if you go with client-side generated (JS fronted or in a controller) UUIDs then the only thing the controller needs to know is whether the operation succeeded. This can be expressed with domain events and/or exceptions. Nope. That should stay in your Aggregates and Domain Services. Yes, although I am not yet sure what‚Äôs the best approach for it. I consider a separate command vs a commands container for a batch of smaller commands. Handling many smaller operations can be useful when there are multiple clients with various needs or when you need to handle less granular (compared to standard UI operations) updates provided via CSV/XLS/XLSX/XML files. However, this naive approach can lead to a poor performance (reading and writing the same object multiple times) and it does not guarantee processing all commands transactionally. It will be up to your implementation to balance transactionality and performance and choose the model which best covers your application requirements (including non-functional ones). Generally, you can choose from: Here is an example of how the balanced one transaction per one object approach can be implemented by grouping commands related to the same object. No matter which approach, you go with, it can be beneficial when transforming your UI from a bunch of fields sent together into more Task Based UI. Here are some links worth reading to see other author‚Äôs perspective on services: If you still have some questions and doubts, leave them in a comment or buy our book and join the Rails/DDD community to discuss with other Ruby devs doing DDD. We wrote many articles about doing DDD in Ruby and Rails such as When DDD clicked for me or My fruitless, previous attempts at not losing history of changes in Rails apps which show our journey into better, prettier code that can handle complex Rails apps. For a free weekly content about DDD and working with big apps join our newsletter . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-23"},
{"website": "Arkency", "title": "A bug that only appears once a year", "author": ["Anton Paisov"], "link": "https://blog.arkency.com/a-bug-that-only-appears-once-a-year/", "abstract": "There are some bugs that only appear under certain circumstances. Today was the day I‚Äôve got one of those (there is a hint in this sentence). I pushed a small change and got a red build as a result. I already had the corresponding test fixed so red build was not something I was expecting. An exception I‚Äôve got was from a check in TicketTransferPolicy which had nothing at all to do with my changes. And so the investigation began. Hint: failing test was not related to extended deadline. I‚Äôve looked into the failing test and here‚Äôs the line that instantly got my attention: This was an instant ‚Äòaha‚Äô moment when I‚Äôve realized, today‚Äôs the day when we have 25 hours in the day. In my opinion, the best solution here is to use Time.current.advance(days: 1, hours: 1) in the test instead of 25.hours.from_now , this approach is more consistent with the code we‚Äôre testing.\nChanging 25 to 26 would also work ;) Thanks, DST :P If you enjoyed that story, subscribe to our newsletter . We share our every day struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. You might enjoy reading: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-28"},
{"website": "Arkency", "title": "Make your JSON API tests clean with linter", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/make-your-json-api-tests-clean-with-linter/", "abstract": "Recently, one of our customers requested that mobile devices should communicate with backend via JSON API . We started implementing an endpoint for registering customers. We used JSON Schema describing JSON API as a part of custom RSpec matcher. To be sure that both request and response body are following the schema. As you might notice, json-schema gem was used to validate the schema, but that‚Äôs an implementation detail. Let‚Äôs take a look at the test: We‚Äôre posting data regarding customer like email, password, and some agreements. Then we validate if request and response meet our expectations. Especially if they are valid with JSON Schema. This test is a bit cluttered, don‚Äôt you think? Wouldn‚Äôt it be better to make schema validations more transparent and focus on important things? By important I assume business information, not infrastructural things like proper headers or following the schema. The spec above is a request type of spec. Request specs provide a thin wrapper around Rails‚Äô integration tests, and are designed to drive behaviour through the full stack, including routing (provided by Rails) and without stubbing (that‚Äôs up to you). If it‚Äôs a full stack test, probably we can get access to the Rails app via app method. #<UsersApp::Application:0x007fc86503cc7... > . Yes, we can. Let‚Äôs write a middleware which wraps the Rails app instantiated in a spec example to validate request & response. We can do this since Rails application is a Rack object . According to Rack documentation, it provides a minimal interface between webservers that support Ruby and Ruby frameworks. To use Rack, provide an ‚Äúapp‚Äù: an object that responds to the call method, taking the environment hash as a parameter, and returning an Array with three elements: - The HTTP response code - A Hash of headers - The response body, which must respond to each Linter implementation: Linter wraps Rails app, takes env hash as input to the call method and then performs checks. At first glance it verifies Content-Type header whether one matches application.vnd.api+json . Then it verifies request body whether it‚Äôs compliant with JSON Schema, same goes for the response. If any deviation occurs, an exception with RSpec look-a-like error is raised. If everything goes fine, the response is returned so we can check other expectations in our spec example. To make linter working, app method has to be overridden as I mentioned before: Our complete spec now looks like: For me this test is now more compact & business value oriented. If you enjoyed that story, subscribe to our newsletter . We share our every day struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. You might enjoy reading: If you want to learn how to support JSON API standard in your Rails application, try our Frontend Friendly Rails Book . Would you like me and my coworkers from Arkency to join your project? Check out our offer Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-10-30"},
{"website": "Arkency", "title": "On ActiveRecord callbacks, setters and derived data", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/on-activerecord-callbacks-setters-and-derived-data/", "abstract": "We‚Äôve already written in Arkency a few times about callbacks alternatives in Rails and what kind of problems you can expect from them . But I still see them being used in the wild in many scenarios so why not write about this topic a bit one more time with different examples. This is a pattern that I‚Äôve seen quite often. Predefined (existing) methods from ActiveRecord::Base are used to set some attributes. Often with the combination of accepts_nested_attributes_for being used to edit some objects deeper in the tree. And then derived-data must be recomputed such as maybe taxes, maybe sums, counters, discounts, it varies between applications. An example can be that sales tax in US depends on the shipping address. So when you set it, you would like to have taxes recalculated (in Order , Sale , Cart , or whatever you call it in your app). The usual reason why those calculations are kept in a database is that we don‚Äôt want them to change in the future when prices or taxes or discount amounts change etc. So we want to compute and store the derived data based on current values. The other reason is that it might make calculating reports on DB faster and/or easier. In such case instead of using update_attributes! or attributes= to set the address and then calling update_tax to trigger recalculations, it‚Äôs better to have a single, public, intention revealing method such as shipping_address= setter. I must say that having a public interface, in which no matter the order of calling methods or their arguments, I always end with a correct state (or an exception clearly indicating an incorrect usage of the object), is a must-have for me. Write your objects so that either the order does not matter, or you prevent the wrong order by keeping an internal state. I believe the word I am looking for is commutative . It also makes refactoring flows much much easier. If you decide to change your checkout process so that you provide discount before or after the address, it won‚Äôt matter because your code will always properly recalculate the derived-data. If you decide to split a big screen which allowed changing 10 values into 2 smaller screens you will feel safe things still work just fine. Another typical example looks like this: It‚Äôs the same issue as before; just automated a little bit more, because when you call save! the method will get called automatically. But you still can‚Äôt write your tests like: Instead, you need to trigger re-computation manually or save: which is no fun at all (at least for me). How can you avoid it? Add meaningful, intention-revealing methods which you are going to use such as add_line , remove_line , update_line etc which will re-compute derived data such as amount or tax . Make those domain operations explicit instead of hiding them somewhere in callbacks. Remember that you can often overwrite Rails setters or getters to call super and then continue with your job. This is especially useful when you have many calculations that need to be evaluated again. Today I had 6 such values üòâ. I believe the root problem of those issues is that by default there are so many public methods in ActiveRecord subclasses. Every attribute, every association, all of that is public and anyone can change it from anywhere. In a situation like that, you as a developer need to be responsible for making the real API used by your application smaller, limited and predictable. I gotta say when I watched some code from other languages (or maybe frameworks would be a more accurate depiction as this is not Ruby‚Äôs fault) it is much more common to have encapsulated methods which protect rules and trigger computing of derived data. In Rails, it‚Äôs rather common to set anything in columns and worry about it during validation phase or when it‚Äôs time to save the object and heavily rely on callbacks for that. I prefer my objects to be OK all the time. I hope I won‚Äôt lose you here. But do you use React.js? Do you know how it is best when render is a pure function based only on component‚Äôs state and props ? The result of calling render in React is derived data which should always give the same result for the same arguments. You can think about methods like these in the same way: There are values you can set such as column_1 or column_2 And there are derived values that you get such as sum , discounted , tax and total which are automatically recomputed. I am sure you that is obvious to you, it‚Äôs just ActiveRecord does not make it easy for you at all, so we need to make some effort. If you follow us and read the Domain-Driven Design ebook you might recognize that the refactorings that I mention, they bring us closer to having nice Aggregates which protect their internal rules all the time. If you enjoyed that story, subscribe to our newsletter . We share our everyday struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. Also worth reading: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-12-13"},
{"website": "Arkency", "title": "Using ruby parser and AST tree to find deprecated syntax", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/using-ruby-parser-and-ast-tree-to-find-deprecated-syntax/", "abstract": "Sometimes when doing large refactoring or upgrades we would like to find places in the code for which grep or Rubymine search is not good enough. These are usually cases where you would like to use something more powerful. And we can do that. I am upgrading this old app to Rails 4.1 and the official guide mentions this case : Previously, Rails allowed inline callback blocks to use return this way: This behavior was never intentionally supported. Due to a change in the internals of ActiveSupport::Callbacks , this is no longer allowed in Rails 4.1. Using a return statement in an inline callback block causes a LocalJumpError to be raised when the callback is executed. Of course, the same code could look like: or be even more complex/nested. I did not want to look at all possible files and callbacks to figure out whether there is a statement like that or not. I decided to use a ruby parser and check the AST for blocks which have a return statement. I am not super skilled in using this gem or its binaries. I know it can be used for rewriting Ruby in Ruby because my coworkers used it for doing big rewrites across big Rails apps. But I‚Äôve never played with it before myself. This was my first approach. And I think it went fine :) I started by having a small ruby example showing more or less that kind of code I was trying to detect and parsing it to see what it looks like. and it gives us: the result has overwritten inspect method so the output looks a bit unusual. But here is what it is. And that‚Äôs all I needed to know. I can read a node‚Äôs type and it can be for example :block or :return symbols. I can iterate over children (1st level) with children . There is probably much more you can. I wanted to iterate over all descendants but I couldn‚Äôt find an easy way to do it. Nevertheless, children was good enough for me. I decided to write a recursive algorithm which will look for :block nodes and inside them for :return nodes. Since this was quite a simple query I didn‚Äôt mind writing it by hand. Looking for X inside Y when Z is something would be less trivial. When I was writing it my first thought was that XPath queries could be an interesting way of expressing such queries. After all that would be just //block//return query, I believe. Maybe there is a gem for that. I don‚Äôt know, if you do, let me know. Anyway, it seemed to work on my artificial example so I was hopeful :) The only thing left for me to do was checking it out on all files in my Rails project. And it worked! It found usages such as: or or All of them had a return statement inside a block. But in the end, none of them were callbacks, so I didn‚Äôt have to change anything. BTW, all of that - not needed if you have very good code coverage and you can just rely on test failures to bring broken code to your attention after Rails upgrade. If you enjoyed that story, subscribe to our newsletter . We share our everyday struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. Also worth reading: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2017-12-22"},
{"website": "Arkency", "title": "Process Managers revisited", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/process-managers-revisited/", "abstract": "I‚Äôve been telling my story with process managers some time ago . In short I‚Äôve explored there a way to source state of the process using nothing more but domain events. However I‚Äôve encountered an issue which led to a workaround I wasn‚Äôt quite happy about. With the release of RailsEventStore v0.22.0 this is no longer a case! Let‚Äôs remind ourselves what was the original problem to solve: You‚Äôre an operations manager. Your task is to suggest your customer a menu they‚Äôd like to order and at the same time you have to confirm that caterer can deliver this particular menu (for given catering conditions). In short you wait for CustomerConfirmedMenu and CatererConfirmedMenu . Only after both happened you can proceed further. You‚Äôll likely offer several menus to the customer and each of them will need a confirmation from corresponding caterers.\n If there‚Äôs a match of CustomerConfirmedMenu and CatererConfirmedMenu for the same order_id you cheer and trigger ConfirmOrder command to push things forward. The issue manifested when I was about to ‚Äúpublish‚Äù events that process manager subscribed to and eventually received: I wanted to group those events in a short, dedicated stream from which they could be read from on each process manager invocation. Within limitations of RailsEventStore version at that time I wasn‚Äôt able to do so and resorted to looking for past domain events in streams they were originally published to. That involved filtering them from irrelevant events (in the light of the process) and most notably knowing and depending on such streams (coupling). Recently released RailsEventStore v0.22.0 finally brings long-awaited link_to_stream API. This method would simply make reference to a published event in a given stream. It does not duplicate the domain event ‚Äî it is the same fact but indexed in another collection. From the outside it looks quite similar to publish_event you may already know. It accepts stream name and expected version of an event in that stream. The difference is that you can only link published events so it takes event ids instead of events as a first argument: Just like when publishing, you cannot link same event twice in a stream. Now you may be wondering why is that this API wasn‚Äôt present before and just now became possible. From the inside we‚Äôve changed how events are persisted ‚Äî the layout of database tables in RailsEventStoreActiveRecord is a bit different. There‚Äôs a single table for domain events ( event_store_events ) and another table to maintain links in streams ( event_store_events_in_streams ). It was quite a big change along v0.19.0 release and a challenging one to do it right. Overall, our goal was to make streams cheap. This opens a range of possibilities: Generally when people are wanting only a few streams its because they want to read things out in a certain way for a particular type of reader. What you can do is repartition your streams utilizing projections to help provide for a specific reader. As an example let‚Äôs say that a reader was interested in all the InventoryItemCreated and InventoryItemDeactivated events but was not interested in all the other events in the system. Its important to remember that the way you write to your streams does not have to match the way you want to read from your streams. You can quite easily choose a different partitioning for a given reader. indexing events by type interim streams How would our process manager look like with link_to_stream then? Below you‚Äôll find store method which takes advantage of it. Whenever process manager receives new domain event it is processed and linked to the corresponding CateringMatch$ stream. If the process is complete, we trigger a command. Otherwise we have to wait for more events. Processes like this happen over time and it‚Äôs nice addition to be able to inspect their state. Is it nearly done? Or what are we waiting for? It‚Äôs not uncommon that some processes may never complete . A stream browser that now ships with RailsEventStore helps with that. Mount it in your application, launch the app and navigate to the stream you‚Äôre interested in: Isn‚Äôt it funny that as creators we mostly learn about new people using what we‚Äôve created from github issues when we break something or make it harder than necessary? We‚Äôd love to hear from you when things are going well too üòÖ Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-01-09"},
{"website": "Arkency", "title": "Building custom search is hard and boring", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/building-custom-search-is-hard-and-boring/", "abstract": "Have you ever had to implement a search page for your client? I did and I‚Äôve gotta say it‚Äôs often one of the most boring tasks. Not always, but often. There is a big list of features that the client usually expects other similar pages have. Especially when it comes to e-commerce websites. It needs to be fast, it needs to allow filtering, grouping and limiting by various attributes. On the backend side that usually means building sophisticated SQL queries, which is never a fun task when the search is highly dynamic and based on a variety of options available in the UI. Alternatively, you can use ElasticSearch or Lucene/Solr, but I‚Äôve realized that while they are often super fast there is a lot of quirks that you need to learn when you would like them to just work. So, on one hand, there is all this complexity and challenge which you might like as a developer. On the other hand, there is rarely anything novel about the task of building a search page and you might feel demotivated about doing it. Not to mention the deadlines. How can you build something sophisticated and have so many features (that are usually wanted and needed) in a week or two? Let‚Äôs try to list a few usual requirements: Now, separately these features are usually not that hard, but the challenge comes from the fact that our customers usually want all of that combined and working together. For every such feature/requirement, you need to implement it on the backend, implement a frontend component and connect them together. That‚Äôs a lot of work. And this is just a start. After these basic requirements, there come new ones, those specific to the domain and application that you work on which require more custom logic and display. That‚Äôs what interesting. That‚Äôs what you would like to focus on. Here often lies the competitive advantage. For quite some time I also struggled with this conundrum on how to build great search pages quickly and painlessly for our customers. to be continued‚Ä¶ Are you also feeling the pain of building search pages from scratch every time? Or maybe you just want to learn how to deal with it upfront? We have a video course that can help :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-02-14"},
{"website": "Arkency", "title": "Testing React.js components with Jest in Rails+Webpacker+Webpack environment", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/testing-react-dot-js-components-with-jest-in-rails-plus-webpacker-plus-webpack-environment/", "abstract": "Around a month ago, I worked on a task, which required a more dynamic frontend behavior. I worked on a component with 2 selects and 2 date pickers and depending on what was selected where the other pickers or select inputs had to be updated based on some relatively simple business rules. I decided to implement it using React.js and it was fun and pretty straight-forward to finish it. Also, working with http://airbnb.io/react-dates/ turned out to be a very pleasureful experience. But that‚Äôs not what this post is about. I wanted to test my component. The integration between Rails asset pipeline (which you can find in almost all legacy Rails apps) and Webpack (which is what anyone wants to use nowadays) is called Webpacker . Thanks to it you can organize, manage, compile your new JavaScript code with Webpack and have it nicely integrated into whole Rails app deployment process. For testing, I wanted to use Jest, which I prefer more than its alternatives. There are already guides on how to achieve that and that‚Äôs what I started with. But it was not good enough for me to end up with a working solution. I had to do much more manual work and googling than I expected. So I decided to document the process hoping that it will make other people‚Äôs life a bit easier. BTW. I don‚Äôt have a PhD in Webpack so forgive me if all of that is obvious to you. You have webpacker installed in that Rails 5 app with React.js integration Add to your Gemfile Run: Run: Now we can configure commands for running jest and tell where we keep our test files by adding those lines to package.json . This project uses MiniTest and test/* directories for Ruby tests and I decided to add my tests in similar test/javascript location. I added a very simple check in test/javascript/sum.test.js to verify that this step of the configuration is working correctly. Run yarn test and you should that it works. It doesn‚Äôt mean much but at least I knew I had jest installed and working for most simple cases. That‚Äôs something. This step is required to have import directives working. Run: Now, here is something other tutorials did not mention, but which is mentioned in https://github.com/facebook/jest#additional-configuration If you‚Äôve turned off transpilation of ES modules with the option { ‚Äúmodules‚Äù: false }, you have to make sure to turn this on in your test environment. And indeed. Webpacker initially creates a .babelrc configuration: which disables transpilation of ES modules. So we need to overwrite the configuration for test env: Why by default (webpacker‚Äôs default) does the configuration say \"modules\": false and what does it mean? That‚Äôs a really long story‚Ä¶ It is necessary for tree shaking, webpack 2 can do it only with ES6 modules syntax. What is tree shaking? It‚Äôs eliminating unused code that is not imported . Wait, there are many modules syntaxes? Yes, there are and only some of them are statically analyzable. I guess you know that and it is obvious if you come from JS community but coming from Ruby community I really needed to educate myself as to what and why to understand what I am doing here. Also, there is no need to use babel-preset-es2015 as recommended in some articles. Without any configuration options, babel-preset-env (which we have in config) behaves exactly the same as babel-preset-latest (or babel-preset-es2015 , babel-preset-es2016 , and babel-preset-es2017 together). For more information on that check out: https://babeljs.io/docs/plugins/preset-env/ To verify that I could use import I used test/javascript/sum.test.js : And it worked. Bear in mind that I already had lodash and moment.js installed with yarn . P.S. have nothing to do with These two env s have 2 different meanings. That‚Äôs confusing when you see: \"presets\": [[\"env\"] is about https://babeljs.io/docs/plugins/preset-env/ and \"env\":{\"test\": (which can overwrite presets ) is about https://babeljs.io/docs/usage/babelrc/#env-option (supporting BABEL_ENV and NODE_ENV environment variable for overwriting configuration). The moduleDirectories setting can be used to tell Jest where to look for modules. I configured it to use like that: I verified that I can use JSX syntax in test/javascript/sum.test.js with: and it worked. Nothing crashed. For testing react components I like to use Enzyme More on that in http://airbnb.io/enzyme/docs/installation/ and https://github.com/airbnb/enzyme/blob/master/docs/guides/jest.md and https://github.com/FormidableLabs/enzyme-matchers/tree/master/packages/jest-enzyme#setup In package.json I added No idea, why this line is needed. That was not enough yet for testing my React components though. I still had some failures. Based on webpack instructions instructions. I didn‚Äôt go with Mocking CSS modules . That was not necessary for me. Let‚Äôs configure Jest to gracefully handle asset files such as stylesheets and images. Usually, these files aren‚Äôt particularly useful in tests so we can safely mock them out. In package.json we add: I created: test/javascript/__mocks__/fileMock.js with: and test/javascript/__mocks__/styleMock.js with I seriously did not expect that such configs will be necessary‚Ä¶ With that I could finally test my first component: Notice that I needed import 'react-dates/initialize'; only because I am using react-dates component. Your first component will most likely not need it. That‚Äôs more or less how started testing our React.js components with Jest in Rails apps that use Webpacker to integrate with Webpack. I definitely thought more of those things would work out of the box and without me having to understand all of that details. Let me show you full .babelrc : and almost full package.json : after a couple of other problems that we discovered later and had to handle as well. This also turned out to be a bit more tricky than I expected. The reason for that is that running assets pre-compilation (asset pipeline+webpack via webpacker integration) or running tests uninstalled devDependencies and then we could not run jest because there was no such binary. Rails probably called yarn with some options which lead to uninstalling jest . I totally did not expect that behavior. Here is our current config: I think eventually I will either move JS testing before assets pre-compilation and rails testing. Alternatively, I am going to split this one long job into a workflow with two separate jobs. Especially considering that 90% of commands (some not listed for clarity) do not affect JS testing at all. Another thing that trolled me a little was setting NODE_ENV globally to production , which I tried in the beginning. This caused more issues than problems that it solved. Do no do it üòâ That‚Äôs it folks. Please test your JavaScript . P.S. I hope at least some of those configs won‚Äôt be necessary with Webpack 4 more configless approach . If you enjoyed that story, subscribe to our newsletter . We share our everyday struggles and solutions for building maintainable Rails and React apps which don‚Äôt surprise you. Also worth reading: And don‚Äôt forget to check out our React books Rails meets React and React.js by example . Both helped thousands of React and Rails developers. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-02-11"},
{"website": "Arkency", "title": "How Algolia built their frontend search widgets with React.js by following redux principles", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/how-algolia-built-their-frontend-search-widgets-with-react-dot-js-by-following-redux-principles/", "abstract": "When Algolia built their first version of Frontend Search Component called Instantsearch.js I was not particularly fond of this solution. Some parts of our integration looked like this: Or this: I found it very hard to follow what‚Äôs going on and to build a mental model on how the solution works. The more custom logic search logic and custom components you wanted to add the more slippery it felt. Those available components were good, but extending the solution felt hard. On the good side, these components and APIs still work properly years after we‚Äôve built them so that‚Äôs nice. However, sometime later when I wanted to implement a new search solution for another customer, I was pleasantly surprised with what they did with their 3rd generator of components called (no surprise) react-instantsearch . I believe they learned their lesson on how much state they need to juggle from having all those components and that when their customers want to extend the search with even more components it would be nice to keep the state in the same place and unify the solution. Also, every time the state changes it‚Äôs likely that some HTTP requests need to be made to get a fresh list of search results. Not to mention that some component‚Äôs state needs to be updated when the new results come (ie to display the number of available results, or refresh pagination or list of available categories and so on). It‚Äôs no wonder to me that they decided to use react and follow the redux principles to implement the new solution. You can see that when browsing the code of react-instantsearch package. The whole solution is based on core which contains ie createStore.js and InstantSearch.js and createInstantSearchManager.js etc, and then you have components which are just presentational components without the logic. I bet that even if you look at compiled sources of those files you can still pretty much understand how they look like and what they display. Here is an extract from Stats.js file. Stats component displays how many results were found and how quickly. Then there are connectors . Connectors are higher order components. They encapsulate the logic for a specific search concept and they provide a way to interact with the whole Instantsearch solution. You can use them when you want to customize the UI or display some components using a different toolkit or library like Material UI or Antd . For example if you you don‚Äôt like that fact that <Hits> widget creates a <div> tag to wrap all results into, you can create a custom component to render the search results which uses <ul> tag and use the customHits connector to subscribe for data changes to update when there are new search results available. You can find this connectors documented at https://community.algolia.com/react-instantsearch/connectors/connectHits.html and also check out the others. And then there are widgets . Widgets are container components (presentational components connected using the connectors). They provide the out of box working experience that makes building the search page so fast. Here is for example a Menu widget which can be used to filter categories etc https://community.algolia.com/react-instantsearch/widgets/Menu.html and you can have a look how it works at https://community.algolia.com/react-instantsearch/storybook/?selectedKind=Menu&selectedStory=default&full=0&addons=1&stories=1&panelRight=1&addonPanel=storybooks%2Fstorybook-addon-knobs Now that‚Äôs something I can understand much more easily then what they had in their first solution :) It‚Äôs a really flexible solution which provides a lot of out of box working functionality but also allows you to change some of the gears without reimplementing everything. And if you ever feel the need to implement a custom component because those provided are not good enough you can do it too. You basically need to implement 3 methods: That‚Äôs just React and Redux you already know and like. And all together the code looks this way at the end: Are you also feeling the pain of building search pages from scratch every time? Or maybe you just want to learn how to deal with it upfront? We have a video course that can help :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-02-21"},
{"website": "Arkency", "title": "The anatomy of search pages", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/the-anatomy-of-search-pages/", "abstract": "What kind of components do we need to use to make a great search page? Let‚Äôs have a look. I gathered some examples of the most typical components. How long do you think it would take your team to implement all of them. At the beginning, the task does not seem to be quite daunting. But the complexity comes from the fact that many of those components influence the search query, search results, and other components. As an example when a user keeps writing the search query, the list of categories (and their counters) is refreshed to reflect only those categories which contain search results limited to the query. When you search for ‚Äúpillow‚Äù, the categories which don‚Äôt contain pillows are not displayed anymore. And that‚Äôs where the complexity of implementing search pages comes from and often increases with every added component. It‚Äôs not that there are many of them, it‚Äôs not that you need to implement the backend to support that kind of filters. It‚Äôs that you need to handle all the interactions between the components in any order and handle the state of every one of them. But‚Ä¶ Do you know what technology appeared some time ago to make handling frontend interactions and forms easier? React.js and‚Ä¶ to be continued‚Ä¶ Are you also feeling the pain of building search pages from scratch every time? Or maybe you just want to learn how to deal with it upfront? We have a video course that can help :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-02-19"},
{"website": "Arkency", "title": "Your search index is a read model (and API üòâ) in the Searching bounded context", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/your-search-index-is-a-read-model-and-api-in-the-searching-bounded-context/", "abstract": "For me, one of the biggest revelation from adopting DDD was the discovery that one model of your data is often not sufficient. That with bigger and more complex applications you might need to have multiple, slightly (or hugely) different models which take a different perspective to look at the data. For years, I‚Äôve been modeling thinking that there is one way I can organize my classes and my data. One way, which would be convenient for all parts of the application. Only years later I realized that requirements usually diverge over time and trying to have a single model to cover for the requirements coming from different people, different parts of the organization, from admins, customer service, merchants, customers, board, etc might be doomed from the beginning. I realized that it might be simpler to duplicate some data, but organize them differently, in a way which makes it easier to answer the needs of a certain stakeholder or certain class of features. The same argument goes for data stores, but I think with data stores it is more embraced in certain circles. We understand that there is no single DB good at everything and at some point, we just need to add a different one. It‚Äôs never an easy decision (at least for skeptical guys like me). And it requires careful judgment whether the benefits outweigh the additional operational cost. If there is one place where it turned out to be a good choice many times, I would say it‚Äôs when you need to implement fast full-text search in your app. In DDD, a derived model that you build based on what‚Äôs happening in your canonical model is called a read model. Read model as the name implies is read-optimized. Usually, a read model will contain data from multiple tables and there will be duplication. Also, a good read model is tailor-made for a single purpose, ie. a single screen. That means when you design the search index it is good to think about: I think about search indexes (as in Elasticsearch or Algolia) as my read models optimized for searching and displaying search results. Those indexes are often built asynchronously in a reaction to what‚Äôs happening to the write model stored in a primary data-store. If you use domain events then often you are going to update the index within a handler reacting to published events. Here is an interesting idea. What if the data in the index could be exposed via an automatically provided API directly to your frontend (JS and/or mobile) clients for direct consumption. That would mean you don‚Äôt need to implement the search API on your own and you could save the time for building it. It means that the frontend team working on the search would probably need very little API from the backend team. You would communicate via the data in the indexes. Sometimes that‚Äôs not good enough. When it is not, the frontend can communicate with the backend to run a query using the index. Then the backend could further limit the results based on some criteria or enhance the returned data with attributes from the database. You can even decide on the index schema up front, fill a test index with some fake data and start working separately on it. The frontend team on building a wonderful search experience and a pretty design. The backend team on reacting to data changes in your app which should lead to updating the index. As a result, both teams can work in parallel. And that‚Äôs what you get out of the box when using Algolia. Your mobile or JS client (written in React.js or React Native) can use an API token to obtain search results from allowed search indexes and it does not even need to communicate with the backend. It can talk directly to Algolia, which is hosting your search data in their data-centers. Are you also feeling the pain of building search pages from scratch every time? Or maybe you just want to learn how to deal with it upfront? We have a video course that can help :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-02-23"},
{"website": "Arkency", "title": "Rails Event Store - better APIs coming", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/rails-event-store-better-apis-coming/", "abstract": "Rails Event Store v0.26 is here with new, nicer APIs. Let‚Äôs have a look at some of those changes: subscribe used to take 2 arguments: handler (instance or class or proc) and event_types (that the handler was subscribed to). This can be now used as I think this named argument to: makes it much more readable. We also made it possible to subscribe Proc in much nicer way. Instead of: you can now pass the block directly. I really didn‚Äôt like the API that we had for temporary subscribers. It looked like this: It was inconvenient because there was no idiomatic way to pass two blocks of code. One for the subscriber and one for the part of code during which we want the temporary subscribers to be active: Interestingly, ActiveSupport::Notifications have a similar limitation: Here is the new API that you can use. It‚Äôs a chainable API which could be used in controllers or imports to find out what happened inside them: Of course, you can still pass the subscriber as a first argument. It does not have to be a block. AggregateRoot now allows to easily define handler methods (reacting to an event being applied on an object). So instead of using underscored method names such as def apply_order_submitted(event) which follow our default convention, you can just say on OrderSubmitted do |event| . That‚Äôs how it was (and is still supported): That‚Äôs the new way: The nice thing about on OrderSubmitted do |event| is that it makes your codebase more grep-able when you are looking for where OrderSubmitted is used. We have some other interesting ideas on how to make the code using Rails Event Store more readable and easier to follow and adapt to your needs: If you enjoyed that story, subscribe to our newsletter . We share our everyday struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. Also worth reading: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-03-03"},
{"website": "Arkency", "title": "Ruby Event Store - use without Rails", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/ruby-event-store-use-without-rails/", "abstract": "Ruby Event Store v0.27 is here with some nice improvements. Let‚Äôs have a quick look. We‚Äôve always built our ecosystem of gems with the intention of not being coupled to Rails. So the majority of features are implemented in ruby_event_store gem and a few other features such as async handlers integrated with ActiveJob are in rails_event_store . Every dependency is taken in a constructor and can be swapped to something different. Until now we had one coupling which prevented you from using ruby_event_store easily. The rails_event_store_active_record gem (which provides an implementation for a repository to save events) depended on rails because it provided a migration generator to create necessary tables for storing events. rails_event_store_active_record now integrates with rails optionally and can work without it. So you can use ruby_event_store together with rails_event_store_active_record without rails . Here is how: Add ruby_event_store too your Gemfile : As you are not using rails and its generators, please create required database tables which are equivalent to what our migration would do in whatever way you manage DB schema in your project. You can now use RES in your app. Since the beginning events had to inherit from RubyEventStore::Event . That is no longer the case, however. Now, any object can be used as events, provided you tell us how to map it to columns that we store in the database. To do that you can implement a custom mapper. This new ruby_event_store version comes with RubyEventStore::Mappers::Protobuf mapper which you can use to store messages generated with google-protobuf gem. Add RES and protobuf to your app‚Äôs Gemfile: Configure protobuf mapper: Define your events in protobuf file format i.e.: events.proto3 and generate the Ruby classes: You can now use those structures when publishing events with Rails/Ruby Event Store The work is not yet finished. We are still working on enabling this feature for async handlers but we think this a good start. We believe this will make much easier to use RES and exchange events between multiple applications and/or micro-services. You can now serialize your events however you want: protbuf, messagepack, Apache Avro, BSON, Thrift, Cap‚Äôn Proto. It‚Äôs up to you. You just need to implement a custom mapper with 3 methods. Here is an example of how you could serialize your events with messagepack. Implement the mapper: Pass it as a dependency: and now you can publish an event: If you enjoyed that story, subscribe to our newsletter . We share our everyday struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. Also worth reading: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-03-27"},
{"website": "Arkency", "title": "What I've learned at Arkency and why I am leaving", "author": ["Piotr Macuk"], "link": "https://blog.arkency.com/what-ive-learned-at-arkency-and-why-i-am-leaving/", "abstract": "The story I‚Äôd like to tell you starts at the beginning of 2012. At that time I‚Äôve run my side project Konfeo ‚Äì event registration and management system . After three years of working on that, first partially and then full time, I‚Äôve decided that I need to split my time to enhance and diverse income sources. I joined Arkency in 2015. It was a very good decision. I‚Äôve joined to quite large legacy Rails project with a complex domain. I‚Äôve started to cooperate with more developers working on the project and with the client directly. I‚Äôve learned how to use new tools like Heroku , MongoDB , Sidekiq with its pros and cons. Arkency is known for its Domain-Driven design approach to work with Rails legacy apps. I‚Äôve learned a lot of that architecture and started to use that design in my work. I even wrote a post on our blog about one of DDD tools: repositories . The style of work at Arkency is very good planned. We use Trello with Kanban like process. Even most complicated features are easy to accomplish because we split each feature into tiny tasks and work with one baby step at a time approach. The work is processing with remote and async style . It means that we don‚Äôt need to work at client‚Äôs place and at client‚Äôs time. All communication in the project and in the company is doing via Slack and Trello asynchronously. The one synchronous moment in a week is our Friday weekly meeting but it is also recorded and may be consumed in an async way. There are at least two interesting and important habits I‚Äôve learned at Arkency. The first one is Monday security update (thanks Pawe≈Ç ). Every Monday I update my all projects and infrastructure. For code, I use tools like bundle-audit and bundle outdated . For servers, I update packages and apply security patches. The second habit is starting from the middle approach (thanks Andrzej ). It‚Äôs like MVP for every task. The work should be time-boxed, shippable and done with next iterations in mind. There is a proverb for this approach: ‚Äúdone is better than perfect‚Äù :) So the question is why am I leaving such great company? As I said at the beginning my plan was to enhance and diverse income sources. After 3 years and around 2.500 hours of work, I had to decide what should be my next step. I had two challenging projects to work on and both projects need more and more attention. Additionally, I‚Äôve started to practice Aikido a few months ago and feel more need to no rush myself and have some time to develop my inner peace. I‚Äôve decided to go full time with my own SaaS business Konfeo which is profitable more and more each month. Is the decision good? Time will show‚Ä¶ Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-03-28"},
{"website": "Arkency", "title": "How to use Algolia without coupling to ActiveRecord::Base", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/how-to-use-algolia-without-coupling-to-activerecord-base/", "abstract": "In my video course , I present using Algolia with Rails using the more direct integration provided by algoliasearch-rails gem. Like many gems in Rails ecosystem, the integration relies on ActiveRecord::Base and its callbacks. And while it certainly can be very convenient and fast to add to your app, there is also a certain amount of magic involved. Ie, when your classes are loaded, they send HTTP request to Algolia with the index settings. And for me, that‚Äôs a big no-no. I prefer the more explicit approach in which I treat those settings as database schema and update it in migrations so there is a history in the code. But Algolia made a good decision by splitting their solution into 2 gems. There is algoliasearch gem written in Ruby and not coupled at all to Rails. And there is algoliasearch-rails which integrates with the Rails ecosystem and ActiveRecord::Base in particular. And you are free to just not use it :) You don‚Äôt like Rails magic? You can opt out from it. I like it! https://github.com/algolia/algoliasearch-client-ruby The first thing you need to know is how to configure your search indexes. If you want to make sure not a single value changes over time due to some defaults that Algolia might introduce, then use get_settings method to obtain all possible config values with their defaults and provide all of them: If you have many indices to configure it‚Äôs OK to create a class for building those configs more dynamically: And that‚Äôs how you can work with index settings without putting them into ActiveRecord class like algoliasearch-rails does: That‚Äôs the 1st step to decoupling this code from ActiveRecord. Now we need something to use instead of ActiveRecord callbacks to trigger indexing of our records. We can use meaningful domain events and event handlers over callbacks. This can be done with RailsEventStore , or anything else that you use like RabbitMQ or Kafka or SQS . The premise is identical. Publish info about changes happening in your application and in reaction update the search index. There are 2 approaches that can you can go with. Full reindexing all the time or partial updates. Full reindexing is usually a safer approach. You use the domain events only as a trigger to gather all the data and send an updated version of your object to Elastic or Algolia . It‚Äôs easy to handle retries in case of a networking error because you can just build a new version of your object again based on latest data and send it. But the downside is that you need to have a way of collecting all the necessary data. Sometimes it might be simple and it could be just mapping your active record attributes to a proper json . But sometimes you might have an event sourced aggregate and you don‚Äôt want it to expose its internal fields. Or your search object (remember: read model) might have attributes coming from multiple objects from the write-side of your app. For example, the party in your search index can have data from the part, from its organizer, from attendees, from the venue, etc etc. If you go with full reindexing you are usually going to have a mapper converting the attributes and their format from your domain object to search object. As you can see, you put composed the solution together on your own, instead of putting everything into Event class like you would do with algoliasearch-rails gem: There is a class for index configuration, there are domain events triggering the indexing and re-indexing, there is a mapper for mapping attributes (ie Time to Unix timestamp in UTC), and there is a handler actually invoking the Algolia API. Everything under your control, and everything in plain sight. No direct coupling with ActiveRecord::Base , just your app reactively updating the search index. Partial updates, on the other hand, can be more convenient sometimes, especially when published events contain all the information necessary to perform an update, without the need to load domain object and map all fields. Imagine that you publish a domain event ( fact ) when someone moves a party to a different date. In such case you can add a handler reacting to the fact, which only updates 2 fields in the search index: Someplace in your code, you publish the fact: P.S. If your domain is all about events (conferences, parties, exhibitions, concerts etc) then I like to use the synonym fact for domain events (which you publish and save in a DB). Are you also feeling the pain of building search pages from scratch every time? Or maybe you just want to learn how to deal with it upfront? We have a video course that can help :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-02-28"},
{"website": "Arkency", "title": "Using singleton objects as default arguments in Ruby", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/using-singleton-objects-as-default-arguments/", "abstract": "Sometimes you would like to define a method which takes an optional argument, but the programmer might pass nil . And your code needs to distinguish between the value not being provided (default value) and nil . How can it be achieved? The usual solution for default value is to define them as nil or other empty/zero values which makes sense such as 0 or a string, an empty array, etc. But what if you need to distinguish between nil and no value being provided? What if you want to distinguish between: and Here is the solution. Define a single, unique object and use it as a default. And instead of checking if the passed argument is nil check if that‚Äôs the singleton object or not. using private_constant is not necessary but I like to remind Ruby devs that we can use it for ages and that we can have private classes that way as well. You could use a symbol ( :not_provided ) or number or anything else that‚Äôs unique in ruby, but in general methods (such as assert_changes described below) they could be valid objects to be provided as an argument. So the best way to solve it, is to use a unique object that nobody can pass as an argument. Here is how Rails is using it to implement assert_changes : I guess I prefer the rspec approach but I admire the assert_changes implementation which uses UNTRACKED object. Although, it‚Äôs kind of similar to using boolean arguments, which often is an indicator that 2 separate methods should be defined. So instead of foo(1, true) and foo(1, false) , it is often argued it‚Äôs better to just have foo(1) and bar(1) and I usually agree with this guideline. However, in case of assert_changes the usage of named arguments and singleton object seems OK to me. If you enjoyed that story, subscribe to our newsletter . We share our every day struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. You might enjoy reading: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-04-23"},
{"website": "Arkency", "title": "Correlation id and causation id in evented systems", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/correlation-id-and-causation-id-in-evented-systems/", "abstract": "Debugging can be one of the challenges when building asynchronous, evented systems. Why did this happen, what caused all of that? . But there are patterns which might make your life easier. We just need to keep track of what is happening as a result of what. For that, you can use 2 metadata attributes associated with events you are going to publish. Let‚Äôs hear what Greg Young says about correlation_id and causation_id : Let‚Äôs say every message has 3 ids. 1 is its id. Another is correlation the last it causation. \nIf you are responding to a message, you copy its correlation id as your correlation id, its message id is your causation id. \nThis allows you to see an entire conversation (correlation id) or to see what causes what (causation id). Now, the message that you are responding to can be either a command or an event which triggered some event handlers and probably caused even more events. In Rails/Ruby Event Store this is also possible. Recently we‚Äôve released version 0.29.0 which adds #with_metadata method that makes it even easier. BTW, that was our 52nd release . of course, if you don‚Äôt publish many events, it might be easier to apply it manually, once. Now, keeping that correlation and causation IDs in events‚Äô metadata is one thing. That‚Äôs beneficial and if you want to check why event X happened you can just easily do it, but it‚Äôs not where the story ends. Imagine that you have a global handler that registered which reacts to every event occurring in your system and building two projections by linking the events to certain streams: What would that give you? Given an event with id 54b2 you can now check: That makes it possible to verify what happened because of X ? I hope that in the future we can automate all of it when some of the upcoming Ruby Event Store features are finished: If you enjoyed that story, subscribe to our newsletter . We share our every day struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. You might enjoy reading: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-05-14"},
{"website": "Arkency", "title": "Rewriting deprecated APIs with parser gem", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/rewriting-deprecated-apis-with-parser-gem/", "abstract": "In upcoming Rails Event Store release we‚Äôre going to deprecate existing reader methods. They‚Äôll be replaced in favor of fluent query interface ‚Äî popularized by ActiveRecord. In order to make this transition a bit easier, we‚Äôve prepared a script to transform given codebase to utilize new APIs. Long story short: we had six different query methods that allowed reading streams of events forward or backward, up to certain limit of events or not and finally starting from beginning or given position in a stream. Example: We‚Äôve decided to change it to something like this: Deprecating APIs seems easy ‚Äî issue warning on old method call, maybe suggest new usage: It is however more burdensome for the end-user than it looks on first sight: Digging though codebase for usage and manual replace or maybe some sed trickery would help, sure. The thing is we can do better. We can rewrite Ruby, using Ruby. Enter excellent parser gem: It all begins with analyzing how the code we want to replace looks like in AST. Consider the aforementioned example: Here we‚Äôve learned that :read_events_backward is a message sent to what appears to be a client receiver. We can also see how arguments, positional and keyword, are represented as AST nodes. Next piece of the puzzle is a thing called Parser::Rewriter (or Parser::TreeRewriter in latest parser releases). It let‚Äôs you modify AST node in following ways: What are its arguments? Content stands for string with code. In our case that would be client.read.stream('Order$1').from(:head).limit(5).backward.each.to_a . With range it‚Äôs a bit more complicated. Let‚Äôs use ruby-parse -L to reveal more secrets: With -L switch ruby-parse was kind enough to describe to us those ranges in each AST node. We can use them to refer to particular locations in parsed code. For example following description teaches us that node.location.selector refers to area between client. and ('Order$1', limit: 5, start: :head) . What‚Äôs more, ranges can be joined. Calling node.location.selector.join(node.location.end) would get you range for read_events_backward('Order$1', limit: 5, start: :head) . Exactly what we‚Äôre looking for! All good so far, but how exactly you‚Äôd get that node for replace ?\nThis Parser::Rewriter class is a descendant of Parser::AST::Processor . Given parsed AST and source buffer, it will call our method handlers as soon as a matching tree is found: In the example above we totally disregard arguments passed to the read_events_backward method. This is fine since we‚Äôre focusing on first example in TDD flow and giving more specific test examples would drive this code to become more generic. Full infrastructure to get it going: To recap, we‚Äôve learned how to read parsed Ruby code in AST and how to use this knowledge in order to transform it to something new. And that‚Äôs just the tip of the iceberg! Full DeprecatedReadAPIRewriter script with specs to study in Rails Event Store repository . If you enjoyed that story, subscribe to our newsletter . We share our every day struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. You might enjoy reading: Using Ruby parser and the AST tree to find deprecated syntax ‚Äî when grep is just not good enough One simple trick to make Event Sourcing click ‚Äî the curious case of explaining Event Sourcing with Aggregate Root Process Managers revisited ‚Äî how coordinate events over time and put a barrier that breaks as soon as they all happened Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-05-14"},
{"website": "Arkency", "title": "Removing magic with magic", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/removing-magic-with-magic/", "abstract": "Some gems, like InheritedResources, help us by reducing the lines of code we have to write by providing definitions automatically. However, depending on how the gem is written, it can be done ‚Äúmagically‚Äù. In some cases, we want to remove such gems. By ‚Äúmagic‚Äù I obviously mean defining methods, or in this case, controller actions, without any explicit call to the functions provided by this gem. Such implicit behaviour makes life on legacy codebases harder. It‚Äôs harder to remove the feature (or model), because how do we know whether it is used or not? Similarly, it is harder to add feature correctly, because it‚Äôs easy to overlook some dependency or usecase which is available in our program. That can lead to sad bugs, where we have overlooked something and our code breaks. That said, I wanted to remove the gem from one of ours applications. There was only one explicit usage‚Ä¶ And BaseController was a parent for every controller used in admin panel, so the implicit actions could be in ANY of these controller. Well, that‚Äôs not easy to clean up. How do you safely remove such gem, if you don‚Äôt know where it is even used? Obviously one could just remove the gem and watch production burn, but you don‚Äôt have to be that brutal.\nYou can fight magic with magic :) This is the moment where you are actually thankful for having open classes in Ruby. I‚Äôve written following code and dynamically extended InheritedResources gem with additional behaviour, which would notify me where it is used. Now we only need to open one specific module and prepend our hack into it: As you can see, each time one of the methods is used, I get notification on Honeybadger, with exact controller and action where it is used. One could now use automatic rewrites using parser gem to add the necessary code, but in my case it was only a few actions where it was used, so it was not worth it. I‚Äôve just manually wrote couple lines of controller code and respectively changed the views in order to not use byzantine @resource variable name. After few weeks of this code in production, I‚Äôve removed the code and safely removed the gem :) If you enjoyed that story, subscribe to our newsletter . We share our every day struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you.\nYou might also like: Rewriting deprecated APIs with parser gem Using Ruby parser and the AST tree to find deprecated syntax ‚Äî when grep is just not good enough Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-06-21"},
{"website": "Arkency", "title": "Use channels, not direct messages ‚Äî 9 tips", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/use-channels-not-direct-messages/", "abstract": "There‚Äôs something about using direct messages (project related) that developers find uncomfortable. At Arkency, we avoid them at all costs, but we also see how hard is to change our client teams habits, sometimes. I thought it would be nice to describe the problem and find some solutions. I talk about it from the perspective of a remote/async/distributed team, but I suppose some problems are similar to in-office teams. My criticism towards direct messages doesn‚Äôt assume bad intents from the initiatiors of such conversation. I believe it‚Äôs mostly about not being aware of the consequences. In this post, I‚Äôm focusing mostly on project-related communication - where I believe all the team should be up to date. There are other types of conversations which might be fine keeping secret and hidden from others, if really required. remote teams rule nr 1 no direct/private messages related to work, use channels The main thing is the problem with transparent communication . Whatever happens in direct/private messages is not visible to others in the team. Sooner or later, it will leak that some people talk about a project in direct messages - others will fell left out of the conversation. If the conversation is about some work to do then others in the team are excluded of knowing what are the current priorities. Also, it implicitly creates a pressure on the receiver of the message, that from now on, only she/he is familar with the details provided here. The context (even if it‚Äôs not a clear task) is kind of assigned to her. Also, it‚Äôs not always clear - if I received a direct message instead of communicating on a public channel, does it mean I should keep it secret? Not clear. The second problem is the ASAP nature of DM which make them feel like requests. We talk a lot about the reasons for going async in Async Remote but to TLDR it here - async allows choosing the most effective time for a developer to work on difficult tasks. By allowing async work we send a clear message to the team that we trust them. Obviously not all kind of messages need to be treated as bad/sync, but the way they‚Äôre presented by default (notifications) is often making it sound like something to react to ASAP. Direct messages are the evolutionary kids of phone calls. Which developers usually also hate, by the way. One type is asking for the status update . ‚ÄúHey, how is it going with X feature?‚Äù. The second type is the ‚Äújust‚Äù tasks , ‚ÄúHey, could you just fix the sorting in the financial report UI, thanks!‚Äù Please repeat after me: Slack DMs do not replace GitHub issues/google docs Slack DMs shouldn‚Äôt be for private tasklists from other folks nor should it be a place for feature requests. Thanks for coming to my TED talk. The third type, I‚Äôm aware of is when a developer is clearly assigned to a part of the application and it‚Äôs only him dealing with it. In this case, the communication is usually specifying the details of the next bugfixes/features. The problems here are quite subtle - other people don‚Äôt know what is going on in this area. They are not able to participate. The whole project doesn‚Äôt feel like a collaboration anymore. The basic tips are the following: Instead, initiate the conversation on channel - even if it‚Äôs clear that for now only you and 1 other person would be involved. It helps to have dedicated channels (open to everybody) where certain topics are discused. General channels are not a good place for that If it‚Äôs a status question - you can take the 2 minutes to reply on the #standup channel and just link in the DM saying ‚ÄúHey, I thought others would benefit from knowing my status, so I posted there, I hope it‚Äôs fine‚Äù. I recently listened to the audiobook ‚ÄúDeep Work‚Äù . There are more such async-friendly advices. In short, the author recommends approach where you schedule ‚Äúinternet-time‚Äù instead of the more popular  (reversed) approach of scheduling ‚Äúoffline-time‚Äù. Instead, you can schedule in your own backlog/plan when you review your messages. Others would learn quickly that grabbing your attention this way is not effective. It‚Äôs important thought that you do reply to questions (ideally on channels) systematically. Your project has a backlog, but you can have one too. Schedule and prioritize your tasks, so that you have time for deep work, but also for communication. I use Nozbe for that. Todoist was also a good experience. The simplest way is to post to #standup frequently. But sometimes if your current task goes slowly - keep overcommunicating on the channel about what problems you‚Äôre encountering. It‚Äôs a varation of the rubber duck as we all know  it. This is the big one. Project managers need to trust your process. If you ask them to always add tasks to the backlog, show they would be handled quickly. Otherwise, they would look for tricks and they would be right. Our developers freedom to have a deep work should be accompanied by quick progress. The best way to do it is to improve the skill of extracting smaller stories and the ‚Äústart from the middle‚Äù techniques. At Arkency we promote the idea of 1-size user stories. It‚Äôs a bit idealistic and sometimes hard to achieve but that‚Äôs the goal worth aspiring to. Our culture of work assumes that each developer can work on any part of the application. This is sometimes called collective ownership. It‚Äôs especially important for the kind of projects we work on - projects where we‚Äôre involved for several years (we almost never work on 3-months projects).\nIf you‚Äôre the developer who gets assigned to one part only, try to explain why it‚Äôs risky for the team and what implications it has on you (big pressure, hard to go for vacation?).\nEven if you‚Äôre not that developer - look around. Is someone in the team always assigned to one part? Do they enjoy it? Do you both think it‚Äôs healthy for the project? Reach out to them and ask if you can help them. BTW, this is the kind of conversation where I would find it OK to do on direct message, even though it‚Äôs project-related. Just don‚Äôt keep it secret for too long. Aim for actions/discussions how to resolve the issue and move on. Maybe it‚Äôs #projectfoo-UI, #projectfoo-invoices or #projectfoo-performance - you name it, whatever makes sense in your context. This helps deciding where to start the conversation. (Using email) You have one inbox. (Sign up for Slack) You have 30 inboxes. (Slack adds threads feature) Your inboxes have inboxes. Do you know other techniques how to deal with the problem? Feel free to share in the comments. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-09-02"},
{"website": "Arkency", "title": "Doing more on reads vs writes", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/doing-more-on-reads-vs-writes/", "abstract": "What do you like more in your application? Handling complexity when reading the data or when updating it? Imagine you have a model such as Product with three potential date attributes: Also, three additional business rules: What this means is that a product can be: Of course, our e-commerce publishing system would like to query for products that can be displayed or purchased. There are multiple ways to implement such a solution. I am going to present two of them. I would say the most common solution deals with these business rules when reading the data. I would say this is a typical UI-driven development. The user can provide three different fields. Let‚Äôs have three different fields for storing these values. Nothing happens here on writes. Value A is stored as A. That‚Äôs it. The whole presented logic is verified when reading the data, when querying it. If you send your products to ElasticSearch or Algolia, you need to repeat similar logic in those queries as well. However, it is simple, and it works. Let‚Äôs try something different. In this solution whenever we change one of our three main attributes which can be provided from the UI, we immediately recompute their effective, derived values . So instead of having more complex reads, we have more complex writes. The reads are now stupidly simple. Also, you could send those derived dates to Elastic Search and the queries to it would be equally simple. While preorder_on and announce_on might be nil , their computed counterparts should not be (assuming a validation on publication_on ). The downside of this solution is that in many places you might need to remember to use preorder_on_computed instead of preorder_on . It could be tempting to reverse the nomenclature and use preorder_on_provided or a similar, longer name for the value coming from the UI. And to reserve preorder_on for the precomputed, not-null value which should be used for queries. Whichever way you go, make sure to communicate the pattern that you use with your team. There is potentially an even simpler solution lurking here. Instead of remembering preorder_on_computed and announce_on_computed we would add booleans such as is_visible and is_buyable . Once a day (think cron job or a scheduler ) we would query for all products which should become visible or buyable today and we would switch their booleans from true to false . Similarly, we would need to do it when the user updates one of our three main attributes. Depending on your preferences this might seem even easier than the previous solution. Or, it might look ugly or like an over-kill. There is, however, one potential benefit lurking here. With some slight modifications, we could detect if the product was just announced, hidden, pre-orders opened or closed. In such case, we could publish an appropriate domain event on our message bus. Thus making it an explicit event in our system that something important happened; potentially notifying other bounded contexts about a significant fact from our domain. It might be a critical reflection that there is something like a Calendar Bounded Context in your application. The fact that time passed, there is a new day, new business day, new week, new month, new year, etc. is a crucial event that triggers state changes in your system. Going even further in that direction, we could split our Product into two classes/models: Our write model does not need is_buyable or is_visible at all. But, it‚Äôs very beneficial for the read-model which can live as a separate table in the SQL DB, or it could be in the already mentioned Elastic Search. However, usually when working with ActiveRecord , we mix those two models together. In more complex apps, it might be a good idea to separate them. Subscribe to our mailing list to be notified about our upcoming 12-weeks Rails Events video class. You might also enjoy reading: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-07-13"},
{"website": "Arkency", "title": "Event scout rule", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/event-scout-rule/", "abstract": "Recently I¬†came up with a new name for an existing technique that I have been suggesting over the last few years. It is connected with applying domain-driven design, event¬†sourcing or event-driven¬†architecture to various existing applications. The idea is that when you start a new application¬†and you already know how to do events,¬†event-driven¬†architecture or event¬†sourcing,¬†then from a technical perspective,¬†it‚Äôs all quite easy. You simply add¬†a new feature and all is event-driven from the beginning. In the case of existing applications, however, it‚Äôs a bit more complicated, because there are no events. Building a¬†new feature in an existing application is hard, because when you get down to it, you realize you are missing many events that could prove useful once you decide to build this feature in the event-driven way right from the start. The feature¬†can be, for example, a read¬†model or improving performance of a read model or improving¬†performance of reading a¬†report. Another example is a feature changing the business logic. Still, the problem is that you have no events. The technique¬†I‚Äôm talking about is unintuitive and¬†that‚Äôs why I think it‚Äôs more a discipline than¬†something you do by intuition. It‚Äôs probably similar to TDD, which is also a discipline. The idea is that, when working on an existing system, at some point you decide to go event-driven. Then, in the course of work on a feature, you may realize that you know how to build it quickly and easily without being event-driven, and so you do it like that, not in the event-driven way. But you also add¬†publishing events to the existing code, even though you are not going to use them just yet. So in a way,¬†it‚Äôs an unused code or a dead¬†code. It serves the logging purpose in the application rather than anything else at that time. You publish an event,¬†for example,¬†you‚Äôve just changed the¬†registration procedure and¬†you can find the key moment in time,¬†let‚Äôs say it‚Äôs a service object, application service¬†or¬†aggregate, when you can clearly say the user was registered. You publish a ‚ÄúUserRegistered\"¬†event and you add some attributes to it: maybe a user ID is enough. You‚Äôre not using the UserRegistered event yet,¬†but if you do it regularly,¬†like every day,¬†every feature,¬†every week,¬†then after a year you will probably have a hundred of events in your application.¬†And one year from now you will thank yourself for doing it, after you realize that you can use these events to build new features. It means that you have been systematically building the event coverage . I call¬†this technique the event scout rule ,¬†because it‚Äôs similar¬†to the scout rule¬†that I hope many programmers are familiar with. The scout rule¬†says: leave the code¬†which you touched, or even just read or looked at, a little bit better. Namely, improve the naming, the structure or maybe just make some small refactorings, extract some new methods or a small class. Don‚Äôt do huge changes, but make the life of the next person dealing with the code slightly easier. Maybe it will be you one month from now. And it‚Äôs the same with¬†events: always leave the place with more events,¬†so that one day you can benefit from that. You or some other colleagues. It‚Äôs not an easy technique,¬†in fact it‚Äôs a discipline. It may not make sense at the beginning, because you don‚Äôt see the benefits right away,¬†but to my knowledge, it's¬†the only technique that allows you to say one day: \"Oh,¬†I actually have all or almost all the events ready! So I‚Äôll just add the three missing events as part of the feature.‚Äù This will be the first feature built fully in the¬†event-driven way. And that‚Äôs the goal. But we have to remember that events will not add¬†themselves, right?¬†You have to add them,¬†even though people will not see the benefits right away. Maybe it will also make the life of code reviewers a little bit harder. But still, it's¬†just adding¬†a few lines of code in certain places,¬†maybe passing the dependency to the event¬†store or event¬†bus. Sometimes, you will have to add human factor to the process, as it may turn out necessary to discuss why you are doing this. But once you start doing it and people start to see its value,¬†all doubts will disappear and you can continue doing it¬†every day,¬†every week,¬†every month. I said before that you will see the benefits of the technique after about a year, but actually it can be much¬†faster. If you are working on one area for most of the time, perhaps you will be able to see the benefits in one month‚Äôs time, which means the technique can bring short-term benefits as well. If you like this topic of adding events to legacy (Ruby) applications, then attending REScon might be a good idea. We‚Äôll show more advanced techniques how to gradually get out of the existing Rails Way architecture and turn it inot loosely-coupled event-driven application. As part of REScon we have 3 events (each can be attended/bought separately): All in beatiful Wroc≈Çaw, Poland. Happy event publishing! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-09-04"},
{"website": "Arkency", "title": "Big events vs Small events ‚Äî from the perspective of refactoring", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/big-events-vs-small-events-from-the-perspective-of-refactoring/", "abstract": "Today I‚Äôd like to write a few words about big events versus small events. This topic usually leads to very heated discussions. At the beginning, I was mostly keen on big events, but now, after working for several years in an event sourced system, I notice many problems connected with big events. And that is why now I am all for smaller events. To give you a sense of what I mean, here‚Äôs one example of a problem that I find very painful to deal with, namely the refactoring of our codebase. The thing is that when you publish events and you usually do it from an aggregate, then the aggregate contains some state related to the event or to the information needed to make the decision to publish the event. Quite often, usually at the beginning when you are still new to aggregates, you make them too big and they contain much more state than they need. They contain information that could actually be split in two aggregates. And with aggregates slightly bigger than necessary, you may end up enriching the original event with useful information that happens to be in the state of the aggregate. So at this time it seems to be a no-brainer because you simply append new properties. It can be some kind of an association. For example, you registered a new employee and you attach information on the department she was assigned to or you publish her manager_id and may also want to embedded the employee resume.  (BTW, see how the different Bounded Contexts are coupled here?) So you keep on enriching the event with whatever you have, which at the beginning sounds great because for some reasons you may find it very useful to have all or most of the information in one event, as it makes it easy to build a projection or a read model and saves you the trouble of reacting  to several smaller events. On the other hand, when you feel the need to do a refactoring of the original aggregate,  you need to split it into two parts. The events are a kind of a public interface, so other consumers rely on it. If you want to split an aggregate, suddenly you face a problem of not having all the information in one place, making the refactoring a bit more complicated. So you either try to keep the original event published somehow and build a projection only to publish the old event, just to stay with the old consumers without having to change them, or you change all the consumers or create event versions. All this is a bit problematic. To me the most important thing is that all this can make you want to postpone the refactoring. And I think that one of the most important thing about programming is that we should never worry about refactoring. If I see a better design, then I  should be able to do it quickly without any doubts or worries. And big events can be a source of worry. On the other hand, when events are small and you move logic from one aggregate to another, and the event comprises just an ID and one or maybe two attributes, then it‚Äôs much easier. Even if you move it from one aggregate to another, it doesn‚Äôt really change the semantics of the event and so the event stays untouched. It is just  published from a different place now, which makes it much easier to deal with. That‚Äôs the lesson I learned from my own mistakes made while working in event sourced systems or event-driven architecture. Based on my experience, I much prefer smaller events because they make refactoring a whole lot easier. Thanks for reading! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-09-10"},
{"website": "Arkency", "title": "Command sourcing ‚Äî why I am considering it", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/command-sourcing-why-i-am-considering-it/", "abstract": "If you‚Äôve been following me over the last years, you know that recently I‚Äôve been very interested in event¬†sourcing. Command¬†sourcing sounds very similar and indeed there are similarities. But on the other hand, it‚Äôs something completely different. We have been trying to introduce command sourcing to one of our projects and it requires us to address so many different aspects of it. Therefore, I will try to gradually get you familiar with what it‚Äôs about, why we use it and what kind of problems we‚Äôre trying to solve. Today, I‚Äôll try to stay simple and maybe in the next blog posts I will delve deeper into the subject. The idea of command sourcing is about persisting commands. A command is a user‚Äôs intent to change the system or other system‚Äôs intent to change our system state. We persist commands in a command store. In our case we just reuse an event store to kind of fake a command store. We just store another message type there. The persisted commands allow us to replay the whole state of our system. And here is one difference as compared with event sourcing, at least as compared with our approach to command sourcing and event sourcing: with event sourcing you never replay or source the whole system from events. You only do it per one read-model, usually per aggregate or per process manager, or you have a projection and load some state into memory. Usually it‚Äôs a small state and it‚Äôs fast and there are no performance penalties overall,¬†because it‚Äôs usually a short stream of events. But with command sourcing, at least the way we approach it, it‚Äôs different. We do assume that it causes a performance hit, so command sourcing is not something that we do very often, at least not in the first phases. It‚Äôs something that we consider doing with our system from time to time. Even if it takes one hour or two, we can do it under the hood and then just replay the system state. This way, we replay the whole system and all the users'¬†requests to¬†it. Obviously not all projects sound like a good fit for that. In our case we think it‚Äôs a good fit because we do complicated batch processing and batch calculations involving many edge cases, which means that sometimes things can go wrong. And the idea of command sourcing is to have a second weapon to use in situations when we notice that our calculations went wrong. If that is the case, we want to replay the system from all the commands and get a new state that is different from the current state because, hopefully, it contains fixes to the calculations. In a way, we can imagine that we can rebuild our system from one year of commands and get a new state. Obviously it has big implications and I‚Äôm going to cover them in the next episodes. For example, if any part of our system is leaking to other systems, then things can get complicated. Moreover, when we replay the commands, it‚Äôs not really the same system that we‚Äôre replaying, because we inject or connect different dependencies. So usually when we have some kind of communication with another system, we do it through an adapter object. It means that during replaying we don‚Äôt want to contact the other system. That is why during the replaying phase we  use an in-memory adapter which doesn‚Äôt really contact the third-party system. But then again, to be very specific, it‚Äôs about our project which doesn‚Äôt really contact third parties that much. And even if it does, it happens very sporadically and always under control. So this is not a big issue for us. There is also another problem that command sourcing solves as a side effect. If you already do event sourcing, then you probably know it‚Äôs cool and awesome, but there is one challenging thing about it. And this thing is called event versioning. Over the time of development, when you want to have one event, but then you realize that this event should actually have different properties, sooner or later you end up with the idea of an event version. So you have two different events or two versions of one event, which mean the same but have different properties, and your system needs to know how to deal with both of them, because they already live in the history and you never reject the history. Consequently, you end up with two different events. Obviously there are ways to deal with it, so it‚Äôs not the end of the world. You can convert those events into new versions of the events during loading or some other techniques. There‚Äôs a fantastic book by Greg Young called ‚ÄúEvent Versioning.‚Äù I think it‚Äôs free when you¬†read it online, so you can easily google it. \nWhen you end up with those different versions of events and you have command sourcing, you can always rebuild the system from the commands, and as a result, have the events in their newest version. Indeed this is a nice side effect, but it can¬†actually have a significant impact on the time and ease of development. So that‚Äôs one reason why we want to do it. But of course there are more and I will cover them later. For now, I hope I managed to explain to you what command sourcing is generally about. Thanks for reading! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-09-05"},
{"website": "Arkency", "title": "What I've learnt at RESCON", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/what-ive-learnt-at-rescon/", "abstract": "During 4‚Äì6 October 2018 I had a pleasure to organize and participate in RESCON . It was an opportunity to show and share what I‚Äôve learned over the years. I‚Äôve met new people that are into this topics and that gave me new perspective on things I work on. Without further ado, below are things I‚Äôve learnt on first ever RailsEventStore conference. There is a certain appeal in knowing the current shape of a domain event. It helps from a documentation point of view, it could drive better tooling as well.\nWithout schemas you rely on thorough test coverage. On hackathon I‚Äôve seen a tool to show focused diff of schema changes over time. High five to Ania and Mariusz! The topics that circulated were how to handle event versioning, the promise of upcasting in future RES versions and how to make JSON a default serializer (with a little help of schemas). One of our guests was Szymon Pobiega who works daily on NServiceBus. He was kind enough to run a quick demo of it‚Äôs dashboard. In the app you could drill down what messages were recorded in a system, how they connect together and visualize the whole business processes. That gave me much inspiration what could land in Browser some day. A stream is a simple grouping of events under one name. Simple yet powerful as it allows partitioning for a particular reader . And organizing events into streams in RES is quite cheap now with link operation. You will primarily use a stream per aggregate but you‚Äôre not limited to. We use streams to manage process manager state ‚Äî by linking the events process is reacting to into it‚Äôs stream. We also started using it for different projections ‚Äî grouping events by correlation id or collecting events to be processed for particular read model, that is a report. This technique can be used to decouple infrastructure from a domain code. Instead of relying on a clock and modifying it, you send an event describing what time-related just happened in your domain ‚Äî MonthClosed , TwoWeeksBeforeEditionReached , etc. The infrastructure to deliver it when it needs to be done stays on infrastructure layer. We‚Äôve seen it first in workshop app. Then Szymon took us on a journey through several possible applications and how they‚Äôve done the infrastructure part on RabbitMQ. Lastly, on hackathon, David showed us the Metronome to tick with time-related events relevant for a app he works on. There are philosophical difficulties when considering sending past message to your future self. What kind of message it is? Can you reject it? I‚Äôve definitely have a food for thought and that is on our RES roadmap. David shared a joy for finding a good name and I would totally use that one as well. I‚Äôve learned about deployments of different RES versions and the struggles to keep it up-to-date. It is especially tricky if you implement a lot of custom extensions on top of such moving target. That‚Äôs the downside of current before-1.0-release situation and we‚Äôre aligned to make it better soon. It was not the first time I‚Äôve seen such extensions. That keeps me convinced RES is a great base to extend (when stable), yet we might miss some conventions to keep you more productive from day one like Rails does. It seems the choice nowadays is to lean on dry-rb ecosystem. That usually comes with a sentiment that Virtus was much more pleasant to work with. Some of the extensions fill the void of RES not having first-class command support ‚Äî it really seems to be the missing part. Btw. if you wish to get featured on Community send me a link to your article or code and I‚Äôll make sure it gets there! If you haven‚Äôt seen it live already, I‚Äôd recommend watching Andrzej‚Äôs keynote on The Vision of Rails Event Store! This also makes a good moment to announce next year‚Äôs edition ‚Äî there will definitely be one. Can you imagine where RES will be 12.months.from_now ?. If you want to make sure you don‚Äôt miss it, subscribe here . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-10-08"},
{"website": "Arkency", "title": "3 ways to make your ruby object thread-safe", "author": ["Robert Pankowecki"], "link": "https://blog.arkency.com/3-ways-to-make-your-ruby-object-thread-safe/", "abstract": "Let‚Äôs say you have an object and you know or suspect it might be used (called) from many threads. What can you do to make it safe to use in such a way? Here is the most basic approach which is sometimes the easiest to go with and also very safe.\nMake your object state-less . In other words, forbid an object from having any long-term internal state.\nThat means, use only local variables and no instance variables ( @ivars ). To be sure that you are not accidentally adding state, you can freeze the object. That way it‚Äôs going to raise an exception if you ever try to refactor it. Since your object does not have any local state it can be freely used between multiple threads.\nIt‚Äôs ok for example to use the same instance when processing different requests in a threaded application server such as Puma. You can assign MyCommandHandler.new to a global variable or pass as a dependency to objects created in different threads and things should be fine. If someone from your team tries to refactor the code to: they are going to get an exception can't modify frozen MyCommandHandler unless they remove make_threadsafe_by_stateless in which case it‚Äôs a conscious decision, not an accidental one. For years I struggled a bit when thinking about thread-safety and which objects can be used between threads and which can‚Äôt be and whether I need to make something thread-safe or not. Later I realized it‚Äôs not as much about a single object‚Äôs properties but rather about a graph of objects. Imagine a situation like this: If CMD_HANDLER is used between multiple threads, then its dependencies are as well.\nThat means that thread-safety is more a property for a graph of objects (object and its dependencies and their dependencies etc) rather than a property of a single object. In this case, it‚Äôs not enough that MyCommandHandler is stateless and thread-safe. Its dependencies should be as well for the whole solution to work properly. If you know that an object can be used between multiple threads you can compartmentalize its state per thread. For that you can use ThreadLocalVar from concurrent-ruby project: ThreadLocalVar: Shared, mutable, isolated variable which holds a different value for each thread which has access. Often used as an instance variable in objects which must maintain different state for different threads SUBSCRIBERS can be used from within different threads because its state is different for every thread that uses it. @subscribers.value is a different Array for every thread. This might be useful and what you want/expect. But it also might not. In RailsEventStore we use this pattern to keep a list of short-term handlers interested in events published by the current thread. For example, an import process can collect stats about the number of ProductImported and ProductImportErrored events that occur when parsing and processing an XLSX file. In this approach, the object‚Äôs state is shared between all threads but the access is limits to a single thread at once. Although to be honest, I am not sure if synchronize is needed for a method which does not change the state such as notify ‚Ä¶ ( discussion on reddit ) But instead of going that way, you might prefer to use already existing classes such as Concurrent::Array and going with the previous method. A thread-safe subclass of Array. This version locks against the object itself for every method call, ensuring only one thread can be reading or writing at a time. This includes iteration methods like #each. Your object does not need to be explicitly made thread safe if: Here, in case our application threads use CMD_HANDLER.call(...) then we don‚Äôt need to worry about thread-safety because every time we need MyCommandHandler , we instantiate a new object with the whole dependency tree. The dependencies ( repository, adapter ) can use any of the mentioned techniques to be thread-safe as well, or they can be new instances as well. And here lies the reason that many classes are not thread-safe in Ruby by default. They are simply not expected to be used from multiple threads. The authors did not imagine such use-case for them. That‚Äôs OK. The bigger issue, in my opinion, is that it‚Äôs often hard to find info about classes coming from some gems about their thread-safety. If you enjoyed that story, subscribe to our newsletter . We share our every day struggles and solutions for building maintainable Rails apps which don‚Äôt surprise you. You might enjoy reading: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-09-18"},
{"website": "Arkency", "title": "Serverless Slack bot on Lambda with Ruby (and what‚Äôs the less pleasant part about it)", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/our-slack-bot-now-on-lambda-with-ruby/", "abstract": "We love sharing knowledge at Arkency. Education is in our DNA. We‚Äôre happy when our readers and customers are benefiting from that as well. And we‚Äôve set a Slack bot celebrate on each such occasion! SlackProxy, which is the name of our application, notifies us whenever we make a sale from our e-commerce solution, that is DPD . This is an extremely rewarding experience when launching a new product but also a reminder to keep up improving existing ones. Initially SlackProxy was a Rails application deployed on our internal infrastructure, then moved to Heroku. Technically it is nothing more than a proxy that transforms incoming webhooks from DPD into formatted messages posted on dedicated Slack channel. See yourself, this one of the controllers: Nothing much interesting in the controller. The notification part is in the SaleNotifier which does the formatting and posting with help of a library to chat with Slack API. When AWS announced Lambda support for Ruby I was really excited about the possibilities it opens . Not that those possibilities were unreachable before ‚Äî with Ruby it is just more fun. I knew what would be the first thing we happily move there and we already had most of the code üòÖ In fact the way traffic shapes for SlackProxy is an ideal candidate for a Lambda deployment ‚Äî huge spikes for several launch days and more peaceful pings on other days. Nothing latency‚Äìcritical as well. Lambda functions may be triggered by several AWS events. Be it a repository event from CodeCommit, an upload to S3 or and update from SQS. For us, web developers, a request coming to an API Gateway sounds most familiar. It is a good entry point to explore Lambda. I figured that an ‚ÄúAPI Gateway to Rack‚Äù adapter would be a natural glue for any Ruby web application and was relieved to find it contributed by AWS . After all, Rails application is just a an elaborate mechanisms to turn env into [status, headers, body] . Some resistance against Lambda has formed around the opinion that ‚Äúyou cannot run this in development‚Äù. I find it hard to defend when the boundary of you application ends on Rack. We already manage that well with existing tooling. And in production you may need different set of checks anyway. Without any further ado here‚Äôs a rewrite of a notifier in form of a simplest Rack application: This comes with a handy config.ru that rack-lambda handler expects: That makes it trivial to run such app in development with rackup . You can find full source code with unit and mutation tests at slack_proxy . So far it did not mention how we get this code deployed. There are several options possible: First option is fine for exploring the environment. It gets you up to speed without bothering much. In the long run, being accustomed to Continuous Delivery I‚Äôd favor CodePipeline. I did not figure it out just yet. At the moment we rely on SAM CLI, as described in Ruby announcement post . The biggest obstacle for me so far was getting familiar with AWS services involved (IAM, API Gateway, Certificate Manager) and making sense out of the documentation. That is not something Lambda specific and I guess you‚Äôd have to face it when dealing with any AWS service. This was far for me from the Heroku-like experience. What could be also problematic for particular deployments is getting some required dependencies . It might be more desirable to lean on AWS ecosystem more deeply in that case (i.e. consider Dynamo storage). Should you try AWS Lambda with Ruby after all? Yes, go explore it! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-12-13"},
{"website": "Arkency", "title": "Modeling passing time with events", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/modeling-passing-time-with-events/", "abstract": "Learning new ideas can be a real struggle. Getting familiar with new concepts, nomenclature and understanding the context in which this new skills can be applied takes time. When it finally clicks and you‚Äôve connected all the dots ‚Äî the joy is tremendous. I remember a nice discussion we had over burgers during wroc_love.rb conference in 2015. Folks shared their experiences of getting through The Blue and The Red books ‚Äî what was the most difficult to grasp and what made a breakthrough for them.\nFor me the turning point was a chapter on Domain Events. This is when it all made sense. When I shared this experience, Alberto Brandolini who was sitting next to me silently nodded. Eventstorming is a technique for collaborative exploration of complex business domains. I was lucky to participate in a few of such workshops, including the one Alberto facilitated after his wroc_love.rb talk . Events, as the name suggests, play a key role in this discovery process. An event, in this context, represents a fact. Something that has had already happened in the domain. Thus we name it in past tense and in the language of the business (Ubiquitous Language). The other day my coworker Robert shared a technique he picked from a DDDx talk . In this video (with a completely non-searchable title) Greg Young presents how to model time in CQRS/ES system by sending your future self messages. And it turns out events are great for that purpose. The concept is a bit tricky at first. You schedule an event to be delivered to you at some time in the future. An event is described in past tense but it did not happen yet.\nWhen the time comes, a message in form of this event is received notifying you that something just happened. You cannot reject it, as it belongs to the past and represents a fact. Yet you sent it into the future. It can be mind boggling. An application we worked on with Robert allowed requesting invoices. As a customer you were allowed to make a delayed payment in such scenario. When due date was missed and payment still wasn‚Äôt there, a credit note was sent. At the time we were implementing this, ActiveJob and Sidekiq were out of our sight. With Resque at hand that led us to: A credit note is sent after given time has passed. In our business that was two weeks since invoice was requested ( invoice.credit_note.utc ). We enqueue an event Payments::CreditNoteScheduled to be delivered at that time to the receiver, Payments::SendCreditNote . Finally that event is appended to the event store log as a confirmation of scheduling in Resque/Redis. A true receiver and an event handler is credit_note_scheduled method of a Payments::InvoicesService . The glue of Payments::SendCreditNote background job is to make this idea possible in the given infrastructure. The code above is no longer in that application. The feature of invoices in that shape has been completely removed. Yet when digging through git history and restoring it for the purpose of this post, it struck me that something wasn‚Äôt quite right. It wasn‚Äôt about the infrastructure, although it can be much improved and overall less distracting. Something wasn‚Äôt compiling in my head when looking at the events and their relation to time: It doesn‚Äôt indicate passing time! Today I would name this event differently. TimeToPayForInvoiceExpired already conveys the message better for me. Naming is hard and all models are wrong. But some are useful and this technique comes useful in one more aspect. Imagine organizing a commercial workshop. For such to happen you need participants and a venue. All is fine when you‚Äôre fully booked. But with not enough people attending you might not be even able to cover the costs. So a decision is made that workshops will be conducted only when enough (let‚Äôs say 16) people register. You also sign a deal with a venue that a cancellation can be made two weeks prior to the workshop date. How would you design a cancellation process for that one usecase? How would it look like in a design tool that is a test-driven design flow? Let‚Äôs say we‚Äôre past an eventstorming session exploring this workshops domain.\nHere are some of the domain events we‚Äôve discovered: And some commands than can be triggered: This is a stub of our process: And a first test case for a cancellation: That is already a good looking test. But how do we simulate time in a way that the infrastructure doesn‚Äôt interfere with a domain? I think you already know where this is going‚Ä¶ With this technique we can model the process completely with just events and commands. No scheduling infrastructure nor time mocking is required.\nFor correctness we‚Äôd like to add an integration test later that verifies no drift with infrastructure. That one however serves different purpose than being a driver for design. What amazes me the most in learning is that how others can influence us and expand what we already know. Take it further on a foundation we have. I had a pleasure to participate in RESCON conference where Szymon Pobiega in a self-describing talk Sending Messages Into The Future explored the ideas I wrote about even more with great examples and a proposition of a RabbitMQ scheduling infrastructure. And just a day after, on a RailsEventStore hackathon, David Saitta presented his take on Metronome ‚Äî a calendar-like bounded context. I hope this post can someday be a turning point for you! Btw. If you like the idea of modeling time with events and would like help bring this into broader audience with RailsEventStore, there as issue related to this topic you could help us with. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2018-12-20"},
{"website": "Arkency", "title": "Patterns for asynchronous read models in infrastructure without order guarantee", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/read-model-patterns-in-case-of-lack-of-order-guarantee/", "abstract": "When we focus on the model in CQRS architecture, we put most effort into write model.\nNot only this is the place where the business operations are implemented and breakthroughs in understanding domain are happening ‚Äì we also consider it the part of the implementation where we should put a lot of our technical attention to. Different implementations of aggregates? Persistence in the model or not? Messaging? Different kinds of transaction boundaries and transaction guarantees between multiple databases? All of these are exciting topic, but read model part, often considered as an easy job for ‚Äújunior‚Äù developers also pose challenges in implemention. In ideal scenario, read models are rebuilt from the Domain Events , in order of their publishing, and no errors are happening when processing them. But today, let‚Äôs focus on a more legacy scenario. We do have some rails app and it became de facto a standard to have some kind of backround jobs processing system, like Sidekiq, which give you at-least-once guarantee , but doesn‚Äôt give you guarantees about the order of processing the messages. Not having an order guarantee can be a problem, if you‚Äôre not paying attention to the implementation of the read model. For example, simple read model like this: Would be usually unsatisfactory because if these events: were processed in different order, the outcome would be incorrect. That‚Äôs why I‚Äôd like to describe a few techniques which can be useful when working with such legacy application. First case can be if you know that some value is nil initially, it will be set by some domain event, and it is a field which, when set, never changes. Let‚Äôs look at these events: And following read model handler: Thanks to the [state, domain_event.data[:warehouse_id]].compact.first , even if the messages will arrive out of order and the event with location: nil will be processed as last one, the location will be remembered correctly as \"Wroc≈Çaw, Poland\" . Sometimes, we have a data type which forms a linear order and we only want to remember the biggest or the smallest value. In that case, let‚Äôs look at the following example: Remembering the biggest/smallest value is easy thing. We just need to always pick the smallest out of the previously stored, and the one from the event we are currently processing. It can be easily extended to have nth value in order (by remembering a list of values instead of only the smallest one). A truly eventually consistent thing! We just want to remember current value, but we don‚Äôt want to be fooled by messages arriving out of order. This is actually the problematic case from the example in the beginning of this post: In that case, we need to remember two fields, for each value. We can think of it of course as two columns in the database table, but it can also be some kind of compound value in blob storage.\nFirst one remembers actual newest value. The second keeps track of the timestamp, for which that value was definitely true. Now, if we want to remember only the newest one, we just always have to check whether the domain event we are processing have really some newer data than we actually already have. All of the previous examples were based on updating the read model. What about creating the record for it? For example, what will happen if we will have some creation fact processed twice? It would be a shame to create two different records in that case, because further queries and updates will use one of the records, and we probably don‚Äôt really know which one. Again, solution is simple ‚Äì having unique index on a field generated before running a handler (like frontend generated UUID), will cause database to throw an error. Usually we want our handler to be idempotent in that case, and just ignore such error (but only this, very specific one). Second problem is when the read model is particularly short lived, and we will process the events in following order: The bad thing is, that the second processing of the SomethingCreated fact, added the row for the second time. Logically, there should be none, because the read model was created and deleted afterwards. The solution is to use soft-deletes, meaning, instead of removing the record from the database, just anonymize them and set deleted boolean flag to true. That way, second processing of first fact will again raise error due to uniqueness violation and unwanted record won‚Äôt be created. These patterns were meant to be taken under consideration in a legacy system with at-least-once delivery, but without order guarantee. Not always there‚Äôs a need for that. Sometimes we can get order guarantee by having linearized writes and remembering last processed position or having a queue infrastructure with only at most one consumer processing element from given queue at the time. This poses challenges on its own, but all I wanted to show is that read-models are not so trivial and in reality there are some nuances in their implementation. Also, if all of this sounds interesting and you would like to know more about our approaches to legacy rails apps and architecture, consider joining our Rails Architect MasterClass . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2019-02-14"},
{"website": "Arkency", "title": "Optimizing test suites when using Rails Event Store", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/optimizing-test-suites-when-using-rails-event-store/", "abstract": "Using domain events in DDD make it easier to tackle complex workflows. If we are working in a monolith infrastructure, it may cause our event store to have thousands handlers and running all of them in test environment is short way for long test suites. However, there‚Äôs a trick which may allow you to increase the speed of your test suite by disabling unnecessary handlers. The idea is to reinstantiate an event store configuration before each test. Developer will have the control over what will and what will not be run from the test file, by using rspec test metadata feature. You can probably find respective features in other testing frameworks as well. But let‚Äôs start with the basics. If you are having only one instance of event store, there‚Äôs a chance that you have global configuration file. In one of our projects, it looks like this: There is a map, from each event to list of handlers it should trigger, and the method handlers , which converts to the opposite mapping ‚Äì from each handler, to list of events on which it should react. The first optimization may be, that we want to disable some handler, which is computing very long ‚Äì for example, PDF generation. That should give a major speed up, but because handler for generating order receipt pdf is not running at all, some tests will probably fail. That‚Äôs why we want to tag the specific tests in which we are interested in that handler. There may also be situations, where you want to disable some handlers, but only in specific tests: Last but not least, if you are working on fresh bounded context, in which you don‚Äôt have your usual legaccy baggage, you may want to disable whole default handlers configuration and enable the handlers selectively. The final code: I hope that those of you who work with bigger applications and Rails Event Store, will find that snippet useful. If you would like to become better in working with complex rails applications and have more holistic knowledge about software architecture, consider joining our Rails Architect MasterClass . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2019-03-04"},
{"website": "Arkency", "title": "A scary side of ActiveRecord's find", "author": ["Jakub Kosi≈Ñski"], "link": "https://blog.arkency.com/a-scary-side-of-activerecords-find/", "abstract": "Recently I was refactoring a part of one our projects to add more domain events to the Identity bounded context so that we can have better audit logs of certain actions performed on identities in our system. I started from extracting a service that was responsible for consuming commands. I started with something like this: At first sights, everything looked OK. I had some tests that was verifying the intended behaviour so I deployed the code to the test environment. Then I realized that something is wrong - when I was trying to update the name of my test account, I received a uniqueness validation error on the email field. What?! I started to debug logs and it turned out that I was actually updating the first identity and not the one identitfied with command.identity_id . I looked back at the test suite - everything looks correct, my test cases where I update the name & the email pass, so what‚Äôs wrong here? Then I looked at the ActiveRecord‚Äôs find method sources: That super was really interesting, so I started the console and just run the following snippet: Now I realized what was going on - my code was just iterating over all records in the DB table and try to evaluate given block on each record. Thankfully validations have detected this behaviour on the test environment quickly, but it might be really dangerous if the code would be run on production and there would be no uniqueness validation - I would just update all reacords in DB.\nThe other thing is that my test cases were also not smart enough to detect this issue - I should just create more than one identity in tests & try to update at least two of them. You might ask what was the solution? The solution was really obvious - I just forgot to add tap to my find call: I am considering reporting this as a bug since when you pass arguments & a block to find the arguments will be silently ignored. I think such calls should at least issue a warning that your arguments are ignored due to the block so that you can easily find out why your code does not work as intended. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2019-03-19"},
{"website": "Arkency", "title": "Using streams to build read models", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/using-streams-to-build-read-models/", "abstract": "Building read models sometimes pose a technical challenge, especially if given infrastructure doesn‚Äôt provide order guarantee and the model has to be eventually consistent. Read models are considered the easy part, so we would like to be able to implement them quickly and move to the more interesting tasks. One of the simplest ways to ensure order is to use dedicated read model streams. Thanks to them, we will be able to spare ourselves a migration of data, so our implementation will be ready as soon as we will finish the code. Let‚Äôs assume that our read model is meant to keep track of some information about football match participants.\nTo build dedicated stream for a read model, we create a linker: And we add that linker as a handler to all mentioned events. Note that thanks to rescue RubyEventStore::EventDuplicatedInStream , no errors will be raised in at-least-once strategy. We want beforementioned order & consistency guarantees, so we want to make use of database locks to ensure that. For that reason, we could either use named locks, or just create separate structure for them: Of course, we also need to have a unique index on stream_name attribute to prevent race-condition on creating. Then, we need a builder and some data structure: Depending on our preferences, we may consider a Model to be just a data structure which Builder knows how to build, or a Model may know how to build itself. On such granularity, it doesn‚Äôt really matter. If we already have a status table for each model, we can easily add simple caching. Our cache would be invalidated by appending new event to the stream and it would be stored either as a ‚Äúsnapshot‚Äù (json, marshal) in Status ActiveRecord, or as a separate collection of tables. As always, the choice is up to you. At that point, we got one really nice feature: we don‚Äôt need to rebuild our read model ‚Äì it will be build on-demand. However, we still need to link all the events to the dedicated read model streams, which is as painful as having to build all the read models. Fortunately, we can solve this inconvenience by creating Catchup class, meant to be called always before Builder is called: Thanks to it, before building given read model for the first time, all old domain events will be linked, so we are free from doing the data migration. Requirements obviously change, and from time to time we need to add some other domain event to be linked to all read model streams. The solution for that is simple and preserves all previous invariants ‚Äì use the linker: As you saw, that approach has multiple of benefits, the biggest one being that we don‚Äôt have to run any potentially long migrations. Of course, it comes at a price being a little bit time consuming at each retrieval (but we can solve that one with snapshots) and even more time consuming at the first retrieval. As they say, your mileage may vary, but for many of the use cases, this is more than enough. This pattern is especially well suited to read models which are ‚Äúretrieved‚Äù in the background like reports being send over the e-mail or API. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2019-04-11"},
{"website": "Arkency", "title": "How many Ruby programmers are there in the world?", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/how-many-ruby-programmers-are-there-in-the-world/", "abstract": "According to JetBrains research (as of 2019), there is 300.000 professional Ruby programmers, while there is over a 1.000.000 programmers who use Ruby. A few days ago, I‚Äôve had a chance to sit with Artem Sarkisov, a product marketing manager from JetBrains, responsible for the RubyMine IDE. Artem‚Äôs perspective on the Ruby community is quite unique, as he looks at it from the marketing perspective. Here is this part of the conversation where we talk about the size of the Ruby community: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2019-06-07"},
{"website": "Arkency", "title": "Heuristics for choosing bounded context for an event handler", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/heuristics-for-choosing-bounded-context-for-an-event-handler/", "abstract": "Some time ago I was implementing a feature. As part of this I was, of course, writing a bunch of event handlers.\nAt some point, I‚Äôve realized I didn‚Äôt put much thought when choosing the bounded context to which the event handlers should belong. It was mostly driven by intuition or some mechanical routine that upfront design. Let‚Äôs consider the popular example, of ‚ÄúOrder with shipping‚Äù and few different approaches. Of course, one solution is: The other however is Which one to choose?\nBoth fact and commands are something we usually consider a ‚Äúpublic API‚Äù (by a public, I mean public for bounded contexts within an organization, not public to the whole world. Or published language, how some would probably call it). The first suggestion of my colleagues was: do what your context map tells you.\nIn that popular case of some eCommerce, there‚Äôs a high chance that Ordering and Shipping are in upstream-downstream relation, Ordering being upstream one.\nTherefore, the Shipping should ‚Äúadjust‚Äù to Ordering, so it makes sense that the handler is in the Shipping bounded context. On the other hand, we can imagine that we want refund the order when we receive information from the shipping company that the package was destroyed, so we make a handler: In this case, we have an event handler in the same BC as published domain event, because again, we want Ordering to know as little as possible (preferably nothing) about Shipping . The other suggestion given by Andrzej was: none of them.\nInstead of scheduling PrepareShipment command in either one of these bounded contexts, we can extract a process manager which manages the whole ‚Äúorder flow‚Äù. Why would we like to do that? Firstly, you end up with no coupling between Ordering and Shipping (at least when it comes to that particular flow). The whole coupling is in the OrderFlow process manager, and this is a place you want to go when you want to understand how the whole flow is working. Secondly, as told nicely in a talk by Bernd Rucker about process managers , it allows you to achieve less coupled code in more complex scenarios. Imagine that you want to add pretty packaging if the buyer had a ‚ÄúVIP status‚Äù.\nIn that case, you either need to have information about which buyers are VIP in the Shipping BC (which sounds like a lot of work to do and adding complexity only to make one conditional work) or you add a conditional in the handler, like so: As a result, you end up with domain logic in the event handler (which is sometimes fine, but it‚Äôs always best to have as little of it as possible in the handlers). By having a process manager for the order flow, process manager have to know about the VIP status of the buyer, but it sounds far more reasonable than forcing Shipping to know it (especially that there can be some additional actions in the other BCs done only if the buyer is a VIP). Having said that, these were two heuristics, there are possibly more.\nWhat are your heuristics when deciding about a place where a given event handler resides?\nDo you use the ones mentioned above?\nShare your opinion :) Thanks to @pawelpacana , @szymonfiedler , and @andrzejkrzywda for the discussion Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2019-09-23"},
{"website": "Arkency", "title": "10 lessons learnt from the Ruby Refactoring Kata - Tennis Game", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/10-lessons-learnt-from-the-ruby-refactoring-kata-tennis-game/", "abstract": "Over the last ~2 months, I‚Äôve been scheduling some time to work on a specific Ruby code which is designed to be a good starting point for a refactoring. Those exercises are called Refactoring Katas. The one I picked up is called Tennis Game. I‚Äôve had around 20 coding sessions with this specific kata and I have recorded all the session on our Arkency YouTube channel. There‚Äôs a whole playlist of all the videos for this kata. This blogpost is a written summary of my lessons, thoughts, observations after those 20 coding/refactoring sessions. The responsibility of this code is to return a string with a Tennis game result. The input comes as sequential calls to the won_point method. The result is returned via score method. Those 2 methods create the public API of the TennisGame1 object. At the first glance the tests look quite cool - the code is declarative, you should be able to see the expectations. However, while working on this code for longer I came to the conclusion that those tests are not really that perfect. Let me quote the extreme programming definition of what I mean here: The above definition makes most sense when applied to a situation when you think you know what is the perfect design and then you try to apply it. Usually, the code would tell you why your vision may not be so perfect. The solution is to follow the code. In this Refactoring Kata, you can see my initial attempts to actually understand what the code does, by changing the code. Even though, I don‚Äôt understand the domain yet - I‚Äôm following the typical code smells to restructure the code. It‚Äôs purely technical at this stage. I have no idea what the code really does (I‚Äôm trying to guess) but I know that certain technical transformations will keep the behaviour the same, while allow me to look at the code from a different angle. Another refactoring lesson was the reminder that TDD (which I try to practice) is not only Red/Green, it‚Äôs also Refactor after the Green part. At first, I thought that the tests give me enough coverage, that I can do the initial refactoring safely. However, it‚Äôs only over time that I learnt what are the drawbacks of the current tests design. The main problem is this code: You see, this code plays through the game always in the same manner ‚Äî first add all possible points to player1 and only later add player2 points. \nFor certain implementations this might be a correct suite of tests, but if we switch to more stateful implementations, we‚Äôre lacking the coverage. I consider refactoring a process of learning. It‚Äôs learning of the domain, of the code and of the tests. When you look at it this way, maybe it was alright - I started refactoring and through this process I learnt about the problems with tests. However, this is only valid, if I don‚Äôt push my changes before I learn the lessons. If I do, I risk introducing breaking changes. I‚Äôm not a big fan of tennis, but I thought I knew enough about it to work on this code. In practice, this was hard. I kept forgetting what‚Äôs the meaning of Love , I had to constantly look up the possible results. I think this led me to overgeneralising the code sometimes. The names I used for method names, for object names - they were not really names that would appear in a conversation among the real fans of the game. That‚Äôs something what I‚Äôm trying to be more professional in my commercial projects. When I worked on accounting project, I took an online class on accounting. When I worked on a publishing project, I have studied the publishing industry, including the possible business models, what publishers struggle with, how publishers cooperate with authors. I talked to certain publishers. In this kata, I clearly failed at it. I wasted some time, because I couldn‚Äôt visualise the domain well enough. My domain vocabulary was very poor here - I kept using the words: game , score , result without learning some more. As you can see in the initial videos, I‚Äôm very aggressive in using the extract method technique. There are several reasons but the main one is to make the main algorithm, the main scenario as concise and clear as possible. This way I have the main method which represents the algorithm in an abstract way, but everything stays at the same level of abstraction. All the details are left to the other methods or even classes to be extracted. I use RubyMine and I learnt to trust its Extract Method tooling. It‚Äôs just an alt-cmd-m keystroke, type the new name and it‚Äôs done. I like to use modules to package my code. Sometimes, I don‚Äôt have control on the client calls, though. In such cases, I leave the public API untouched, but then delegate everything to the code behind modules. This is like building a facade/wall in front of my ‚Äúwell-packaged‚Äù code. The name TennisGame1 remained untouched in my initial commits, even though it‚Äôs a terrible name. However, over time, I moved more code into the Tennis module. Similarly as Extract Method, I found Extract Class useful. I usually follow the same pattern, where I create a constructor method which sets the state and then 1 or 2 public methods to retrieve the data. In a way, this is a function and can be implemented as a function too. However, what I learnt is that often those objects are just a temporary thing. They‚Äôre not the final result of the refactoring, more like a step in-between. There‚Äôs something about if conditions that I dislike. They often hide some important logic and I feel like ifs are sometimes a too primitive way of encapsulating this logic (often some state machines). The most dangerous code I usually see are the nested if statements. ‚ÄúNo one ever got fired for adding the n+1 if statement, right‚Äù? When I saw this initial code, trying to simplify it was my main goal: Here is the result after Extract Method, Extract Class and Replace If with Guard: Obviously the ugliness of nested ifs didn‚Äôt disappear, but starting from the top-level code allowed me to make the main algorithm more clear and let me deal with other nested ifs in more localized methods/objects. I‚Äôve been always excited by the idea of treating code as data. What I mean here is that some code doesn‚Äôt need to be code, because it‚Äôs actually data. In the video above you can see that I started refactoring some of the code (state machine transitions between possible results) into ‚Äúcode like data‚Äù direction. However, I never really finished it. It felt very ‚Äúprimitive‚Äù to represent those concepts as pure data without any behaviour. This is a topic I need to think more about. Maybe this example wasn‚Äôt a good fit. Or maybe the idea isn‚Äôt as good as I thought. Maybe I‚Äôm just missing some skills here. This lesson came too late ;) In my next refactoring kata I will try to ‚Äúhire‚Äù mutant to check my tests earlier than I did here.\nSomehow, I connect mutant with some ‚Äúbig deal‚Äù, while in fact it‚Äôs very easy to start with, especially in such katas. Mutant shows very nicely that the tests are far from perfect and that my merciless refactoring attempts might be too brave sometimes. The final code is not really that final. It‚Äôs more like a result of the most recent code experiment and I don‚Äôt consider it perfect in any way. Far from that. The last experiment was treating the tennis game as a state machine. Each state is another object (data structure maybe?). Each state knows which is the next state, depending on who won the next point. The nodes in the state machine don‚Äôt track the actual points count, instead it‚Äôs part of their identity/name to know what is the result. Each state does know (possible a smell?) what is their String representation, but the actual formatting (plus interpolation) happens at the TennisGame1#score method. Those lessons are not all, I just picked the ones I thought were the most important. It was a nice experience overall and I learnt a lot from doing the kata. I have recorded the YouTube videos along the way and it was nice to receive feedback from the audience which of my changes could be better (thank you everyone!). The sad thing is that I‚Äôm still not satisfied with the end result code, but on the more optimistic side is that I like the current solution better than the initial one. Such katas are a wonderful way of practicing outside of our commercial projects, while the lessons can be incorporated into our daily coding sessions. If you‚Äôd like to follow my next such coding sessions and/or watch my other software/Ruby/DDD/TDD-related thoughts, follow us on the Arkency YouTube channel , thank you!! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2019-09-23"},
{"website": "Arkency", "title": "How to migrate large database tables without a headache", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/how-to-migrate-large-database-tables-without-a-headache/", "abstract": "This is the story how we once migrated an in-house event store, that reached its limits, to Rails Event Store. My client used to say that the reward you get for achieving success is having to deal with even more complexity afterwards. That couldn‚Äôt be more true even if only applied to the ever-accumulating database tables when your venture gets traction. One very specific aspect of this success was reaching the count of several hundred millions domain events describing what happened in this business over time. What do you do when you realize you made some mistakes in the past and have to change the schema of such humongous table? Do you know and trust your database engine well enough to let it handle it? Can you estimate the downtime cost of such operation and most importantly ‚Äî can your business and clients accept it? At that time we used MySQL and we had no other option than to deal with it. You may be in a more fortunate situation, i.e. on Postgres :) There are a few problems with millions of rows and hundreds of gigabytes all in a single database table: some schema changes involve making a copy of existing table, i.e. when changing the type of used column and having to convert existing data some schema changes do not happen online, in-place and may lock your table making it inaccessible for some time, the longer the bigger this table is being aware of what happens when your database node runs out of free space while performing the operation you wanted and how does it reclaim that allocated space on failure being able to estimate how long the process will take and what is current progress of it ‚Äî that is being in control See more: Online DDL operations on MySQL . We knew we‚Äôd not be able to stop the world, perform the migration and resume like nothing happened. Instead we settled on small steps performed on a living organism. The plan was more or less as follows: create new, empty database table with the schema you wished to have add a database trigger which constantly copies new records to this new database table ‚Äî this component is responsible for reshaping the inserted or updated data so that it fits new schema with the trigger handling new records, start backfilling old records in the background ‚Äî there are several hacks to make this process fast once the copy is done ‚Äî remove the trigger, switch the tables and the code operating on them, possibly within a short downtime to avoid race conditions after successful switch all that is left is removing the now-old database table (as one would expect that is not as easy as it sounds) The devil is the details. We‚Äôve learned a few by making the mistakes you can avoid. Long story short ‚Äî we moved from a single table storing domain events to the schema that consists of two. The exact code of the trigger that translated custom event store schema into RES: One thing to remember is that you wouldn‚Äôt want a conflict between backfilled rows and the ones inserted by a trigger. So did we and set the auto increment to be large enough. Backfilled rows would get a value on an auto-incremented column set by us. Below is the script we have used initially to backfill existing records into new tables. Key takeaways: Iterating with OFFSET and LIMIT on MySQL just does not meet any reasonable performance expectations on large tables. Scoping batches on id scaled pretty well on the other hand. One INSERT adding 1000 rows works faster than 1000 inserts each with one row. It is also worth noting there is a point in which having larger inserts does not help. You have to find a value that works for you best.\n  Using activerecord-import is one option. Another is the bulk import which arrived with Rails 6 . Don‚Äôt forget to finally add them though. You may find yourself in a situations where a query that does full table scan on a table with several hundreds millions of rows ;) Plan how much disk space you will need to fit a copy of such a large table over an extended period of time. You may be surprised to actually need more than the old table due to different indexes. In case you run out if that precious disk space, be prepared to reclaim it and it does not happen immediately. With all events in the new tables we were able to do the switch. In order to avoid any race conditions we have decided on a deployment with barely noticeable downtime: Last but not least ‚Äî removing the leftovers. We‚Äôve learned, the hard way, it worked best with batches . Dealing with large database tables is definitely a different beast. Nevertheless this skill can be practiced and mastered like any else. Know your database engine better, execute in small steps and profit. Have you been in a similar situation? What did you do and which tools made your task easier? Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-01-08"},
{"website": "Arkency", "title": "Ruby ºs raise Exception.new or raise Exception ‚Äî they're both the same", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/ruby-raise-exception-dot-new-or-raise-exception-theyre-both-the-same/", "abstract": "TLDR: You can use raise Exception and raise Exception.new - they‚Äôre identical as a result and it‚Äôs 4 characters less. In my previous post ( OOP Refactoring: from a god class to smaller objects | Arkency Blog I‚Äôve used some code with raising exceptions: or The way I‚Äôm raising the exception here is that I raise it without calling .new . This may look as if I‚Äôm raising a class, not an object. Some people asked me if this actually works and if it‚Äôs the same. We already know the short answer, so let‚Äôs dig into why it‚Äôs the same. Is there some kind of magic involved? One place to go is to check the raise (Kernel) - APIdock documentation. OK, this gives some answer, but I prefer to check the code - let‚Äôs look at the Ruby implementation (in C‚Ä¶) BTW I‚Äôm wondering if it‚Äôs the first time, we‚Äôve posted some C code on the Arkency materials. In order to actually get the answer, I started exploring the C code, but my C reading is not the biggest skill I have. I prefer reading Ruby. Let‚Äôs look at Rubinius - the Ruby implementation of Ruby. rubinius/thread.rb at 75086f2b2cc92302b54176db0250ec6635adfcc8 ¬∑ rubinius/rubinius ¬∑ GitHub The main thing here is if exc.respond_to? :exception . Does that mean that both the Exception class and the Exception instances have this method? Let‚Äôs find out! rubinius/exception.rb at 0296620da5ce252266cccf3574ae3e756ab144e6 ¬∑ rubinius/rubinius ¬∑ GitHub This part is the main answer: So, exception is just an alias to new , at the class level. At the object level it just returns self . Thanks to Rubinius, for making it easier to explore such things! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2019-12-02"},
{"website": "Arkency", "title": "Legacy Rails DDD Migration strategy ‚Äî from read models, through events to aggregates", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/legacy-rails-ddd-migration-strategy-from-read-models-through-events-to-aggregates/", "abstract": "How to migrate legacy Rails apps into DDD/CQRS in a relatively safe way? Recently, I was answering a question on our Rails Architect Masterclass Slack channel. The question was related to a video which explained the strategy of extracting read models as the first step. The part which wasn‚Äôt clear enough was on the topic how the read models extraction can help in designing aggregates. Here‚Äôs my written attempt to explain this strategy: In the service objects introduce publishing events, so when there‚Äôs a RegisterUser service object it would have a line event_store.publish(UserRegistered) Build read models like UsersList as the reaction to those events (and only to those events). Note that this read models can use its own ‚Äúinternal detail‚Äù ActiceRecord, which resembles the original one, but it‚Äôs just for view purpose. Once you have all the events required for a UsersList view, you will see the pattern that the suffix (the subject the events start with) will suggest aggregate names. In our example that would be User aggregate (probably in the Access bounded context) Additionaly, the event names (the what was done) - the  verbs in passive - Registered , Rejected , Banned may suggest the method names in that aggregate This brings us to the potential design of the aggregate Once you learn more about the other flavours of implementing aggregates, business objects (objects which ensure business constraints), you will see that verbs can suggest the state changes and the polymorphism-based aggregates: See more aggregates flavours examples in our aggregates repo. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-02-24"},
{"website": "Arkency", "title": "Ultimate guide to 3rd party calls from your Aggregate", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/ultimate-guide-to-3rd-party-calls-from-your-aggregate/", "abstract": "If you ever wondered how to make 3rd party API call from Aggregate and not clutter it with dependencies, you may find this post interesting. Some time ago I faced that problem while implementing Payment aggregate. Everything looked quite simple until the time a real request to payment gateway had to be performed. I started wondering what is the right spot for that operation? Initially, I tried to do it in command handler. Let‚Äôs take a look at the snippet below. Command handler gets command, makes API call to the gateway and then event is applied to the aggregate. But what if another command happens and customer will be charged twice? That‚Äôs why we used aggregate pattern here, to guard the invariants. But this won‚Äôt work as expected with current implementation, since gateway request happens before aggregate gets called. Let‚Äôs change the order then: This looks good at first sight. But what if payment doesn‚Äôt get accepted because of invalid credit card data or random network error appear? We already applied an event, event handlers started processing it. In effect user has received e-mail about successful payment, he got link to download virtual products, etc. We could make compensating operation, of course. The question is if there‚Äôs a possibility to do so. We could try to do it other way around. Let‚Äôs expose Payment internal state via charged? method to command handler and make the decision there. Even more, CreditCardCharged event could be published from command handler too. Introduction of aggregate wouldn‚Äôt make any sense in such approach, it would be obsolete. What about passing gateway as a dependency and calling it inside Payment aggregate? Sounds tempting, let‚Äôs see: Payment class got cluttered and its responsibilities expanded. I‚Äôm not convinced that such technical details are the part of aggregate interests and I disliked this approach as soon as I implemented it. I started thinking how to make decision about the payment inside the aggregate but keep all the payment technicals out of it. Instead of passing gateway as a dependency, we pass a payment gateway call wrapped in lambda. The only thing we need to do is to check whether response is successful to decided whether apply CreditCardCharged event or not. We assume that payment gateway call returns Response object responding to success? method, but it‚Äôs not a topic of this post and I believe that you know how wrap gateways response into Value Object. Lambda gives us great possibility of currying arguments and getting some from inside of aggregate state. Let‚Äôs use two-step payment scenario like CC Authorization & Capture . Often you need to refer original transaction when capturing the real money. Just prepare request as a lambda with argument: As a bonus, you get nice and clean aggregate tests without messing with mocks, VCRs, massive fake gateway adapters. Aggregate can remain interested in single method of Response object only. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-02-25"},
{"website": "Arkency", "title": "Remote collaborative modeling", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/remote-collaborative-modeling/", "abstract": "Being remote means you don‚Äôt always have face-to-face contact with your customers, with the domain experts, or with the right people who know how the business you‚Äôre trying to model works. This must not be an excuse to skip the modeling part. It is even more important to have a good shared understanding of the domain and the problem you try to solve, to build and use the ubiquitous language. You don‚Äôt need any special knowledge, you don‚Äôt need to read books, you don‚Äôt need to take courses. Just start collaborating with the right people, the people who know the business or the people who have experience with old systems.  Start by asking questions and by actually listening to the answers, and then note them down. Use shared document, and if possible shared screen to collaboratively create the description of the domain problem. The tools here are not important - anything that is capable to show the history of changes and add comments / handle discussions would fit. The online documents shared and editable by the whole team would be a good start.\nFrom my experience, the tools to manage projects (Trello, Basecamp, Jira, etc) are not the best ones here. The discussion should be close to the text where the problem is described. Those tools allow you to comment under the main section and that is adding unnecessary friction when you want to understand the whole thing, with all different opinions.\nBTW if your collaborators are familiar with Github a Pull Request with a markdown file should work - you will get traceability of changes and PR conversations will allow for easy clarifications of misunderstandings. A shared screen will work only if you work at the same time, what not always will be possible, that‚Äôs why is it important to have a way to track changes and have discussions in the edited document. Use a shared screen to build trust, to help others to focus on the current part and to make sure everything is described in a way all of you understand and being consistent with ubiquitous language. A good idea might be to use techniques and tools used in Mob Programming - especially when you work in a larger group, with several domain experts. Put yourself in a driver role and let them guide you, follow their words and make notes - build the model based on shared understanding. And ask questions if you have any doubts. But we could do better than that. Some modelers have already tried that and worked on some techniques on how to make the gathering requirements better, how to have better, easier conversations with domain experts. We could learn from them and try which technique works best for us. The Event Storming has hit the DDD community a few years ago. It has changed the way we think about modeling session. It has introduced visual collaborative modeling techniques and has made us embrace the use of it. But the Event Storming is the hardest one to use online. According to Alberto Brandolini , there is no good way to do it online, the same way you do not organize ‚Äútoga party‚Äù online. There have been some attempts to make remote/virtual Event Storming but my feeling is all of them miss some important energy we get when standing together under an unlimited modeling space, working chaotically. The remote Event Storming misses the chaos, and in this case, it‚Äôs a bad thing. However things change‚Ä¶ maybe you will find a good way to use it, maybe you could think of some new tools that will ‚Äúembrace chaos‚Äù and allow you to get back that energy.\nI‚Äôve tried‚Ä¶ but I‚Äôve failed. Some said that it was a success. Try it for yourself. Share the experiences. Let‚Äôs learn from each other because you know‚Ä¶ ‚Äúwe‚Äôre learners‚Äù! Domain Storytelling is a collaborative modeling method developed by Henning Schwentner and Stefan Hofer .\nIt relies on a fact that telling stories is a fundamental way of human communication and business experts are the storytellers (at least most of them). It uses a simple pictographic language with three essential types of symbols:\nactors (usually named with a role identifier), work objects and activities to visualize the stories told by domain experts.\nDomain Stories are told from the perspective of actors, which play an active role in the Domain Story. Actors create, work with, and exchange work objects such as documents, physical items, and digital objects. The modeler should focus on true stories, no abstract generalizations, no hypotheticals, just concrete examples of what is happening in a domain. Sometimes three good examples are more helpful to understand the requirements than a bad abstraction. Start with drawing the default/happy case, model important variations and error cases as separate Domain Stories. Telling the stories are natural for humans so this method is easy to explain to your collaborators. It‚Äôs easy to understand, easy to follow and very easy to do it remotely. There is a dedicated online tool where you could draw Domain Stories https://www.wps.de/modeler but this is not the only one option. Anything that works for you and is easy to share will work. You could read more about Domain Storytelling here . Another technique is the Story Storming created by Martin Schimak . It is influenced by Domain Storytelling and Event Storming but also by other ways to map business requirements like Event Modelling and User Story Mapping. It starts with very simple, easy to explain notation. You use tree kinds of post-its: Subject, Verb and Object. And like in any language you build simple sentences. You describe the domain story with these simple sentences, using very specific, concrete examples. As a modeler you have to focus on language, the simplicity of the sentences enables domain experts to tell their stories in a natural manner. No need to learn a new technique, no barriers to start using it, no need for technical background.\nLearn more about Story Storming here . Remote work gives you an opportunity to avoid sync meetings. Yes, sometimes this is much easier to just have a shared screen and work together - especially when your domain expert is not very technically savvy. It would work better also at the beginning of the modeling when you need to establish basics of the language (ubiquitous language is not defined from the begging - this is something you need to build together) and gain trust from the business. So start with sync, and then it depends on you and your collaborators how you want to continue. Embracing async is not easy, it will cause some friction and misunderstandings but don‚Äôt let it stop you. With time your understanding of domain will grow, also you will learn how to work better with your domain experts and they will learn how to share the domain knowledge with you. Async modeling has also some advantages. It‚Äôs is slower. And that is a good thing. You have time to think about your model, about what is important and about examples you are working with. I‚Äôve shown you only a few techniques here, but there is more. To name a few: Go check them. Find the one suitable for your needs. Start using it. Share your experiences. Help others to understand good and bad parts of the techniques, but remember your mileage may vary :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-03-23"},
{"website": "Arkency", "title": "Remote Mob Programming ‚Äî review of ideas and the book", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/remote-mob-programming-review-of-ideas-and-the-book/", "abstract": "At Arkency, we subscribe mostly to the remote and asynchronous approach to software development. I described this method in detail in ‚ÄúAsync Remote. The guide to Build a Self-Organizing Team‚Äù, a book I co-wrote with Robert Pankowecki. Most of the insights and views expressed in the publication were based on our own experience but we also drew on some ideas borrowed from other companies. Truth be told, when we first started, there weren‚Äôt really that many remote companies to be influenced or inspired by. We learned a lot from open-source projects, the way they build their teams and how they collaborated asynchronously. In this post, however, I would like share and review some ideas from a book called  ‚ÄúRemote Mob Programming‚Äù written by Jochen Christ, Simon Harrer and Martin Huber, senior programmers at Innoq, which is dedicated to‚Ä¶ you guessed it‚Ä¶ mob programming and how it can be done remotely. I like this publication a lot although, to be honest, it‚Äôs super short - only 30 pages, including some additional resources. If you, however, like quick and concise reading that lets you focus on a number of key ideas, you should certainly grab it. There are some lessons to be learned from the book - It has certainly inspired me to look at the things we do at Arkency from a different angle. For those of you who don‚Äôt know - mob programming is a software development approach based on a whole team working together on a single task at the same time and at the same place. Some of you may be familiar with pair programming - it‚Äôs a situation when two people sit together in front of one computer and they program together, with one person typing and the other one navigating. It‚Äôs a very unique way of working, certainly not for everybody. I used to work this way at a London company, before I started Arkency. We paired all the time, it was a very exhausting and intense process but quite rewarding. I personally think it‚Äôs not a bad approach to creating software. Then there‚Äôs mob programming. It usually means working in front of one screen, but, unlike in pair programming, there are more than two people involved. The main idea behind  this method is that it can be done in the office. We have tried this approach a few times at Arkency, including one time when we had a co-working session at my house. There were five of us working on a big TV screen, moving around the keyboard. We were working on a sample application for our Domain Driven Design workshops, creating the initial design, adding tests, etc. As a method consisting in many people working at a single workstation, mob programming has some consequences, both positive and negative. Intuition may tell us that working like this could be a waste of time - what with five people working concurrently on the same task. But this approach has some benefits as well. We came to realize that mob programming is superior to anything we‚Äôve ever tried before, claim the authors of ‚ÄúRemote Mob Programming‚Äù in the introduction to the book. That‚Äôs quite a bold claim to make and it‚Äôs interesting to see that comparing office work to some other ideas, including remote work without mob programming, didn‚Äôt change the writers‚Äô minds. The three senior programmers did full-time mob programming every day for a year, which to me seems like enough time to draw conclusions. An interesting consequence of the approach was that for one year the team didn‚Äôt have to rely on daily stand-ups . They were not needed, as everybody already knew what had been done and everyone was on the same page all the time. Moreover, there were no code reviews . At Arkency, we don‚Äôt really do mandatory code reviews anyway. I know, it‚Äôs quite controversial. A lot of other companies, however, seem to construct their processes around code reviews so there is always some waiting involved. When developers create code, they are not allowed to push it into production before it gets reviewed. The one-year ‚Äúmob programming experiment‚Äù also meant no hand-overs before holidays . Normally, when one of the developers takes some time off, for example, to go on holiday, someone else has to step in and take over their task. At Arkency we do this through collective ownership but this is not a solve-it-all approach so I can understand why the method suggested by the writers makes sense. Finally, the authors address the problem of working in isolation - using remote mob programming approach can prevent programmes working at home from feeling alone. Right now many of us in the industry are working remotely full-time for the first time in our lives and there are lots of things that we can learn from this experience. Christ, Harrer and Hubethe give us a number of rules to follow when working remotely. Here are some I found most interesting: Remote everybody. This idea was also important for us when we wrote our ‚ÄúRemote Async‚Äù book. We didn‚Äôt claim that remoting every team member was absolutely  needed or required but we agreed it was something to consider. As the writers rightly point out, when one group of people is working remotely and another one in the office, there is information asymmetry that may lead to problems. Camera always on. I think this one is a controversial rule. Personally, I wouldn‚Äôt like to work in an environment where you are constantly monitored. It‚Äôs not that I have anything to hide but I don‚Äôt think it is important for programming activities. According to the authors, the cameras were on all the time and although it felt strange at first, after a few days it started feeling natural. It gave them a sense of presence in the team, almost like working together. I think that what they aimed at was to ‚Äúsimulate‚Äù the office. I would, however, avoid this, there are better metaphors with which to describe our work right now, for example open-source projects. Regular on-site meetings. According to the authors, they work together on-site once a month. That‚Äôs not something that we do at Arkency. We don‚Äôt do mobs, we do remote. On-site meetings may take place once in a while, but definitely not as often as once a month. Some projects never involve on-site meetings. Interesting as it may seem, I‚Äôm not a huge fan of this idea. I don‚Äôt think this is necessary but if there is the luxury of living not far from each other, it‚Äôs definitely worth giving a try. At Arkency, we have clients from the US, Denmark, Germany, which makes working on-site practically impossible. Same time. Innoq developers work six times a day and everybody is required to work at the same time. It‚Äôs not something that would be easy to do at Arkency. For us, the async part of the job is crucial. Our processes are based on the fact that we can work at different times. A short digression: we have tried mob programming. What I would like to explore after reading the book, is the concept of async mob programming where people can ‚Äúchecking in and out‚Äù at their own scheduled time. We‚Äôve been trying this approach with one project and it was very interesting. All the benefits mentioned in the book (no stand-ups, no coach reviews) were working to our advantage. I don‚Äôt think, however, it would be feasible to work six hours a day in the present situation, during the lockdown when we‚Äôre constantly interrupted by some external factors. Same shared screen. This rule makes sense, as it helps people to focus on their work. When one person is typing, another one is the navigator. The team mates rotate every 10 minutes. I think it‚Äôs a similar approach to the one we used for our mob sessions except that we were rotating every five minutes. It helped to keep the dynamics of the work. Small team. This rule aligns with my view that there is a number above which a team stops being productive. I don‚Äôt believe in teams comprising of 20 programmers - at some point adding more team members make communication more difficult and slows down a project. I think this also depends to a certain degree on the culture of the organisation. Group decisions. In software engineering you constantly compare different alternatives and decide for one. (‚Ä¶) Group decisions are superior over individual decisions. (‚Ä¶) In remote programming all decisions are group decisions and this way we minimize technical debt. As bold as this claim may be, I think that group decisions do reduce technical debt. At some point most software projects slow down and almost come to a halt because there is technical debt, there are some workarounds, morale goes down. In the case of pair working, which we practice a lot at Arkency, it helps a lot that there are two people, and they help each other to understand code bases, problems and solutions. Constant momentum. The authors claim that this approach often allows them to get into a rewarding flow. Working in a group helps to avoid procrastination and random activity online. There‚Äôs less temptation to stop working, which may be a big time saver. Learning from the team. This idea was huge when I was pairing at the London company. We got better by learning from each other. With mob programming, on-boarding only took weeks rather than years as the knowledge transfer was much faster. In half a year I became quite fluent at VIM or Python‚Ä¶and all of the team members were able to quickly learn new things. Trust. According to the authors trust is built by communicating actively. When we work remotely, the client doesn‚Äôt see us working and the management may be afraid of losing control over their team. If all members work at the same time, we can‚Äôt be sure about the productivity. Of course, when there are five people working on the same task, there can be some waste. This is similar to what we call over-communication in Async. Save the planet. Working remotely is environmentally-friendly. We don‚Äôt travel so there‚Äôs zero emission. We can observe this currently, as the climate change has slowed down during the epidemic, as we all have to stay at home. Enjoy more quality time with our families. This one is more about the remote aspect of work than mob programming but is still worth mentioning. Mob programming is certainly an idea to consider. At Arkency we might combine the different modes of working and try to mix some of the ideas of remote async work, mob programming and even live streaming. There‚Äôs definitely a lot to explore in this area. The lockdown has forced us to seek new effective ways of working and enhanced our creativity. Check out more and follow us on the Arkency YouTube channel, thank you! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-10"},
{"website": "Arkency", "title": "Rails connections, pools and handlers", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/rails-connections-pools-and-handlers/", "abstract": "In Active Record there are db connections, connection pools and handlers. To put it shortly: Connection Handler has many Connection Pools has many Connections The connection object is the one you most often directly interact with. You can get hold of it via ActiveRecord::Base.connection and, for example, use it to execute raw SQL: If you‚Äôre on PostgreSQL and inspect connection ‚Äòs class, here‚Äôs what you get: Connection pool is a bag with connections. Why do we need one? If you‚Äôre using threads (e.g. on Puma server), every thread needs a separate db connection. Otherwise their conversations with the db could mix up. On the other hand, you want to control the number of db connections, because it typically increases the resources needed on the db server. That‚Äôs why there‚Äôs a pool of connections. If a thread wants to use AR, it gets a connection from the pool. If pool size is exceeded, ActiveRecord::ConnectionTimeoutError is raised. See Object pool pattern . You can get hold of it via ActiveRecord::Base.connection_pool . It‚Äôs class: You can list all your active connections: Note: this array can be empty - this can happen for example when you just started a console session and haven‚Äôt yet interacted with AR. Max pool size is controlled by the pool option in config/database.yml . By default it can be overridden by ENV[\"RAILS_MAX_THREADS\"] . You can check max pool size via: This number is not to be confused with the number of currently active connections: When there‚Äôs another connection in the pool? Open up a thread and talk to AR: Note: you may want to read about with_connection method. Now a plot twist: you can have many connection pools in your Rails app. Connection handler manages them. When can there be multiple connection pools? For example when there‚Äôs an AR model which makes its own establish_connection - usually to another database. Note: each connection pool has its own max pool size. You can use connection handler to get hold of all your pools: So if you wanna traverse the whole hierarchy, you end up with: You can do it right here ! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-10"},
{"website": "Arkency", "title": "Catchup subscriptions with Rails Event Store", "author": ["Miros≈Çaw Prag≈Çowski"], "link": "https://blog.arkency.com/catchup-subscriptions-with-rails-event-store/", "abstract": "The usual way of handling domain events in Rails Event Store is using the included pub-sub mechanism. And in most cases, especially when you have majestic monolith application it is enough. It is also easiest to grasp by the team and use in legacy applications implemented using Rails Way. But sometimes it is not enough, sometimes this way of handling events become an obstacle. Let me define the domain problem. The feature we want to implement is described by requirements: At the beginning it looks very simple. And a first, naive, implementation of this requirements could look like: Let‚Äôs start with new Rails application with RailsEventStore template: Then let‚Äôs create a modules (bounded contexts) because we don‚Äôt want to end up in Big Ball Of Mud quickly: Next, we need to define domain events, implement the blogging logic (omitted here because that‚Äôs different topic for another post): Define subscriptions, to connect our domain events to handlers: And finally our handler to send articles to external indexing service: The Index::PushArticleToIndex is an asynchronous event handler. It is inherited from ActiveJob::Base and implements perform(event) method. This will allow to use it by RailsEventStore::ActiveJobScheduler and schedule sending to an index asynchronously, without blocking the execution of main logic. Because we do not want to fail publishing our new article just because indexing service is down :) Some background jobs adapters (i.e. Sidekiq) use Redis to store information about scheduled jobs. That‚Äôs why we should change default dispatcher in Rails Event Store client to RailsEventStore::AfterCommitAsyncDispatcher . It ensures that the async handlers will be scheduled only after commit of current database transaction. Your handlers won‚Äôt be triggered when transaction is rolled back. Rails Event Store uses call(event) method to invoke an event handler‚Äôs logic. By default you need to pass a callable instance of handler or lambda to subscribe method. But this is not the same when using RailsEventStore::ActiveJobScheduler . If you want to have your handler processed by this scheduler it must be a class and must inherit from ActiveJob::Base . Otherwise (thanks to the RubyEventStore::ComposedDispatcher ) it will be handed by default RubyEventStore::Dispatcher . This solution has a drawback. Let‚Äôs imagine that your blogging platform becomes extremely popular and you need to handle hundreds of blogposts per second. Thanks to async processing you might even be able to cope with that. But then your index provider announced it has ended his ‚Äúamazing journey‚Äù and you need to move your index to a new one. Do I have to mention that your paying customers expect the platform will work 24/7 ? ;) Catchup subscription is a subscription where client defines a starting point and a size of dataset to handle. Basically it‚Äôs a while loop reading through a stream of events with defined chunk size.\nIt could be a separate process, maybe even on separate node which we will spin up only to handle incoming events and push the articles to the external index provider. The catchup subscription is easy to implement when you read from a single stream. But your Blogging context should not put all events into single stream. That‚Äôs obvious.\nIn this case we could use linking to stream feature in Rails Event Store. Change the Rails Event Store initializer: And instead of PublishArticleToIndex use LinkToIndex : This time we will use a simple synchronous handler. Thanks to RubyEventStore::ComposedDispatcher we do not need to change anything. It won‚Äôt match the handlers handled by RailsEventStore::ActiveJobScheduler so the default RubyEventStore::Dispatcher will trigger the event handler. When started, the catchup subscription should start reading from the last processed event and handle read events (pushing them to the external index). Here a sample implementation: Let‚Äôs start with cons of this solution. The obvious one is you need a separate process :) Maybe with separate deployment, separate CI etc. Probably more time will pass between publication of article and indexing it. So why bother? The external index is basically a read model of your articles metadata. Tailor made, aligned with capabilities of external index provider. Recreateable. This is what makes a difference here. To rebuild an index from scratch all you need to do is to remove the stored starting point for the catchup subscription and wait some time. The indexing will start from beginning and will go though all published articles until index will be up to date. I‚Äôve mentioned a scenario where you change your external index provider. Using a catchup subscription it will be quite easy. Just create new instance of the subscription process with different index adapter. Run it and wait until it catches up and indexes all the published articles. And then just switch your application to new external index and drop the old subscription. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-22"},
{"website": "Arkency", "title": "How to work remotely and not get crazy", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/how-to-work-remotely-and-not-get-crazy/", "abstract": "Today I would like to talk about how to work remotely and how we do it at Arkency.  We‚Äôre living in crazy times right now, almost everyone seems to be working remotely, so I thought it would be a good idea to share some experiences we‚Äôve had with remote (and asynchronous) work since we first started working like this around 15 years ago. I think some of the ideas I‚Äôm going to share are universal and can be applied to any profession, not just programming or software development. It is interesting to see how well the whole concept of remote work seems to be doing right now that everyone is embracing it. For some people, it‚Äôs a completely novel approach to going about their work, while others have been practicing it for some time. But even within one team, there can be people who have and who haven‚Äôt done it, which, of course, may create some difficulties. In this post I would like to discuss a few techniques that can be useful in organizing project work involving teams comprising of at least 2 people. And although these people might work from home, they don‚Äôt necessarily have to work at the same time. This is the big difference between asynchronous and remote work, the former meaning working from different locations, the latter - working at different times. Sometimes working times of the individual team members may overlap but very often they don‚Äôt. It‚Äôs important especially now, when few of us have the luxury of working six hours straight. While working remotely, we have to juggle work and family life, handling daily activities and running errands. There are intermittent periods of work and breaks. It‚Äôs a new situation which means, for example, that it is not so easy to set up a call or turn on a camera and talk to our co-workers. People can be interrupted or disconnected. The Internet connection is not that great nowadays because everybody goes online now. To make things easier for ourselves and our colleagues, it‚Äôs important that we divide our projects into smaller tasks, especially considering that people are less available or productive now. To avoid interruptions and unexpected stoppages, it is advisable to split tasks into the smallest possible units, or tickets, as we call them. A ‚Äúsize 1‚Äù ticket is an abstract concept, one that may represent e.g. 1 to 2 days of work. Ticketing makes our work more transparent and efficient, as it helps us to see the flow of work within a single task. During one week, for example, a developer might be expected to complete one or two tickets. This approach can give us a sense of closure and achievement. What‚Äôs more, there are some techniques that can be used to split tickets into even smaller units. This approach is especially useful in the remote and asynch environments as it creates an impression that everything is going well despite the difficult times. Many programming teams apply the concept of ‚Äúsprint‚Äù or ‚Äúiteration‚Äù, which represent work units lasting from 1 to 2 weeks. These units have their start, end and scope. However, despite the advantages of such a solution, I think that  in the remote settings it is more efficient to gradually move towards the concept of the stream of work. Instead of thinking in terms of what can be done in one week, we just focus on organizing the work that needs to be done while someone else (project managers, product owners, etc.) organize the tickets somewhere in the backlog. True, project managers still need to think of the calendar, timelines and deadlines, but from the perspective of developers the start and end of a sprint is not that important. It‚Äôs enough to focus on whichever ticket needs to be done - grab it, complete it and then move to another.  This is what the stream of work looks like from our perspective. As developers we will always have backlogs and there will always be more work than we can handle so it‚Äôs important from psychological and motivational point of view that we don‚Äôt look at everything that needs to be done but rather that we think in terms of timeboxes. Now that the workflow has become so irregular and interrupted, it‚Äôs time to make the most of the time slots available to us any given day. This requires a mindshift in the way we perceive our project. For each such a slot of time, we should consider what kind of useful results we can deliver and what can we do to push our project forward. As for the backlog, it needs to be sorted in a way that allows team members to take the first ticket available. Tickets should be ordered (by product owners) according to their priority so that everyone knows which tasks have to be done first. It‚Äôs crucial that we try to understand the task and plan our work so that we can squeeze the most out of the time we have. For example, we may use a three-hour slot one day, and then finish a task the following day. Of course, it‚Äôs possible that we will be able to complete a small task within one work session. This approach requires something that is not common at all companies, i.e. that all developers are capable of doing all the tasks. It gives rise to many questions, e.g. how to split the backlog between developers. At Arkency we do mostly web development and API so we prefer that our developers to be full-stack, which means that one ticket represents something meaningful and functional, something that our client can actually use. Although our clients have their own dedicated project managers, Arkency developers are capable of doing project management work on their own. We have enough business knowledge and are sufficiently familiar with their vision and priorities that we can access specifications and move tickets into the backlog, organizing the latter in a way that is useful to others. When a developer starts a PM session, they always make sure that they prepare at least 5 tickets so that other team members don‚Äôt have to change their context. In our backlog, we usually have 10 tickets which are sorted and prioritised and underneath them there is a separation line below which there are other random ideas and tickets. The line is serves as a border between what is prioritized and what is not. Another important issue in the remote work settings are customer meetings. The big question is: how to organize meetings or calls with customers when these are required to agree on the next portion of work that needs to be completed? Although I try to avoid calls when possible, certain calls and meetings need to be held. Usually, the busiest of our clients are not accustomed to the async way of working, which also means they are not huge fans of writing things down and explaining themselves in writing. So sometimes, when we do make a call with businesspeople, we put in writing what they say, we agree on what and how needs to be done, we brainstorm ideas with them, etc. And then we try to summarize those meetings in terms of documentation or notes so that everyone can have access to them. Sometimes we also record such meetings but watching them may be too time consuming and reviewing notes proves to be more effective. Sometimes the notes and specifications we write down during meetings become part of the backlog and help us to create a specification ticket which is not ready to be worked on directly but can be used for extracting actual tickets. In such a situation project management work involves extracting work from specifications. Once we work with the tickets, it may be necessary to communicate certain things and to ask more questions. Comments are added to the tickets which are there for everyone to see. There is some integration between communication channels, so that a comment added to the backlog can also be seen e.g. on Slack. When we work remotely, it‚Äôs very important that we over-communicate. This might be very difficult for developers because we tend to be introverted. Programmers have this rule: ‚ÄúDon‚Äôt repeat yourself‚Äù and this sometimes applies to real-life communications, too. And here we have to write a Git commit message to explain what we‚Äôve done and then repeat it on Trello or Slack. Over-communication also means that when we see a problem that is related to the area we‚Äôre working on, we don‚Äôt keep it to ourselves. At Arkency, for example, we have a special Slack channel for summarizing all our projects. We don‚Äôt do daily stand-ups at Arkency, as this technique makes more sense in the office environment than in the remote settings. It‚Äôs easier to be done when everybody starts work at the same time. If you have this luxury and are able to keep discipline, then a kick-off meeting each day could be a good idea. But when people work asynchronously, it‚Äôs not easy to enforce the same time for a stand-up. Some companies enforce daily stand-ups on Slack channels before work, which is not a bad idea, but in case of async work, one don‚Äôt really think in terms of days of work, but rather in terms of work sessions. There‚Äôs more focus on the current session, what needs to be done now and what can be done later. Sometimes communicating during stand-ups every three days is enough and sometimes I post two times a day on our stand-up channel. Another important matter that needs to be considered in the context of remote work is the organisation of our home office. During the time of lockdown we are forced to work from the same house or apartment as the rest of our family. I have the luxury of having a separate room at my house but it wasn‚Äôt so when I first started. Right now I can enjoy some isolation and if necessary I can always use headphones. Still, working from home can make it more difficult to have a call or meeting as there is always some noise around you. And we‚Äôre not always comfortable turning our cameras on. I think that focusing on the techniques I mentioned above is useful because they reduce the need for calls while allowing us to have better communication than office-based teams. Remote and async work is different than office-based work, more things are put in writing and more discipline is required. It‚Äôs good to have a set of tools (e.g. Nozbe, Todoist), which allow us to record things that need to be done later. Also, asking questions in the async environment works a bit different than in the traditional office settings. We can‚Äôt expect that our questions will be answered straightaway, so we have to be respectful and let other people decide when to answer. Unfortunately, this sometimes may cause us to feel ignored within a team, as no one responds to our inquiries. If we feel frustrated because there is no one replying, it may mean that it‚Äôs time to review the techniques used. Although we shouldn‚Äôt expect an immediate answer, if our question is more or less valid, we have every right to expect that someone (be it another developer or a project manager), will finally provide us with an answer. There has to be a clear policy in place on how this problem will be dealt with. In healthy situations, however, the number of questions between team members should be reduced because more and more information is available - everyone has access to everything specification, documents, wireframes, etc. Follow us on the Arkency YouTube channel , thank you! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-04-20"},
{"website": "Arkency", "title": "What surprised us in Postgres-schema multitenancy", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/what-surprised-us-in-postgres-schema-multitenancy/", "abstract": "You can implement multitenancy in various ways . In one of our projects we went for schema-based multitenancy, where each tenant has its own PostgreSQL schema - i.e. its own namespaced set of tables. This approach has many pros and cons , but we found it viable in certain situations. Apartment is a popular gem assisting with that (currently not actively maintained though). I like this particular feature of Postgres, but one has to admit it introduces a little bit of complexity - after all it‚Äôs not a conventional feature everyone uses. The thing is that complexity compounds . One unconventional feature is not a big deal, but if there‚Äôs more of them, interesting things start to happen . Here are some of the things that surprised us when we were implementing schema-based multitenancy. Chances are you already use some of the Postgres extensions like pgcrypto or hstore . Now if you want to switch schemas, Postgres extensions need to be moved to a separate schema (e.g. extensions ). The thing is that they need to be installed in one specific schema that is available in the search_path . Normally they reside in public schema, which will no longer be in search_path if you go multitenant. Typically you move the extensions to a new extensions schema which will always stay in search_path , regardless of the current tenant. This is a just minor annoyance , but it‚Äôs worth making sure you‚Äôre authorized to make the operations in your particular DB setup. In our case (managed DB on Digital Ocean) it was a little tricky. PgBouncer is a popular tool to control the number of connections to your DB server. It runs in a couple pool modes: session mode , transaction mode , statement mode . Transaction mode is sometimes the recommended default. Anything else than session mode won‚Äôt let you use search_path to switch tenants (nor any other postgres ‚Äúsession features‚Äù). If you try to use search_path in Transaction mode, you can even unknowingly mix tenant‚Äôs data . Switching to session mode is the obvious solution, but it has its own set of consequences. I‚Äôm aware not many people use Delayed Job nowadays. It was used in the project we dealt it, though, and it has shown an interesting situation. Delayed Job is used to perform jobs by a background worker, just like Sidekiq. The difference is that the jobs are stored in a plain SQL table. Now if you go multitenant, you need to decide where to put the job that belongs to a specific tenant. Should every tenant‚Äôs schema have its own table with jobs? Then you need to have N workers running in parallel (where N is the number of tenants), or you can make one worker somehow query all these tables. Alternatively you can go for a shared table with the jobs and put it to a shared schema - which is what we did. You can do it by explicitly prefixing the jobs table name with the schema name. Got comments? Reply under this tweet . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-10-01"},
{"website": "Arkency", "title": "Multitenancy with Postgres schemas: key concepts explained", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/multitenancy-with-postgres-schemas-key-concepts-explained/", "abstract": "PostgreSQL schemas let you hold multiple instances of the same set of tables inside a single database. They‚Äôre essentially namespaces for tables . Sounds like a compelling way to implement multitenancy ‚Äî at least in specific cases . But if you‚Äôre like me, you‚Äôd rather know exactly how it works before you rush to implement it. Let‚Äôs explain some basic concepts first: schema itself, search_path and session . As I said before schema is basically a collection of tables (and other db ‚Äúobjects‚Äù). Schema is not the most fortunate name for it, as it can be confused with db schema in the sense of db structure. Namespace would be a more adequate name. Actually namespace is how PG calls it in some contexts - e.g. in pg_namespace catalog. Let‚Äôs open psql and play a little. Even before you create any schemas yourself, there‚Äôs already one schema there - the default schema, named public . You can see it by listing all schemas with the psql command \\dn . Alternatively, if you run select * from pg_namespace; you‚Äôll also see all schemas ‚Äî along with some internal ones. public schema is where all your tables live by default - if you don‚Äôt specify any specific schema. So if you create a table CREATE TABLE things (name text) it just ends up in the public schema, which you‚Äôll see when listing all the tables with \\dt (of course provided you haven‚Äôt changed the search_path - see next section). Now let‚Äôs create a new schema: CREATE SCHEMA tenant_1 . You should now see another entry if you list all existing schemas with \\dn : Now we can create a table inside this schema. You can do it by prefixing the table name with the schema name and a dot: CREATE TABLE tenant_1.things (name text) . You won‚Äôt see it when listing all tables with \\dt (again, provided you haven‚Äôt yet changed the search_path ). To list the table, run \\dt with an additional param: So at this moment we should have two tables named things which live in separate namespaces (schemas). To interact with them, you just prefix the table name with the schema name and a dot: To get rid of the schema, run DROP SCHEMA tenant_1; . It‚Äôll fail in this situation though, because there are tables in it. To remove it together with the tables, run DROP SCHEMA tenant_1 CASCADE; . So far we accessed other schemas by using their fully qualified name: schema_name.table_name . If we skip the schema name, the default schema is used ‚Äî public . Now, search_path is a Postgres session variable that determines which schema is the default one. Let‚Äôs check its value: As you can see, it has a couple comma-separated values. It works similarly to PATH in a shell ‚Äî if you try to access a table, Postgres first looks for it in the first schema listed in the search_path ‚Äî which is \"$user\" . If it cannot find it there, it looks in the second one ‚Äî public . If it‚Äôs not there, then we get an error. Now \"$user\" is a special value that makes it actually look for a schema named after the user. Personally I‚Äôve never used it. It‚Äôs just there by default. The ability to use multiple schemas also looks like a feature I‚Äôd rather not use, but sometimes you have to - e.g. to handle Postgres extensions - more here . To change the search path, run: SET search_path = tenant_1 . If you now run SELECT * FROM things , it will access tenant_1.things . To get back, you typically do SET search_path = public ‚Äî or whatever is your default. Sidenote: in Rails you can set your default schema via schema_search_path option in database.yml . A good question to ask: what is the scope and lifetime of this variable. This is a session variable, i.e. it affects the current Postgres session and is discarded when the session closes. Sidenote: there‚Äôs a variant ‚Äî SET LOCAL which works for a transaction instead of a session, but personally I‚Äôve never had to use it. Another sidenote: search_path resolution is actually more complex ‚Äî apart from aforementioned session variable, it can be also permanently set for the whole DB or role. Now it makes sense to explain what exactly a Postgres session is ‚Äî when it‚Äôs initiated and closed. Postgres session, depending on context, might be referred to as Postgres connection or backend . Under normal circumstances, whenever you establish a new connection to Postgres (e.g. via psql or from Rails), a new Postgres session is instantiated. It‚Äôs closed when the connection closes. That perhaps explains the interchangeable usage of session and connection . Actually when you get hold of a DB connection in Rails, you don‚Äôt always get a new DB connection ‚Äî because there‚Äôs a connection pool ( read more here ). Connection pool is there to limit the number of DB connections Rails app can make. From the perspective of Postgres sessions, a crucial fact is that the DB connection is not necessarily closed if a Rails request releases it ‚Äî¬†it stays in the pool and might be used by a different request later. The consequence is that if you SET a Postgress session variable in one request, it‚Äôll be set in the next one ‚Äî if it happens to run on the same DB connection. That‚Äôs why you have to make sure you always SET the desired search_path before you do any DB work in a request or background job. Every session gets a separate OS process on the DB server, which you can check yourself by running e.g. ps aux | grep postgres . You can also see the sessions by querying SELECT * FROM pg_stat_activity ‚Äî¬†there‚Äôs a lot of useful data in it. Now it‚Äôs worth saying that even if Rails makes an actual new connection to the DB, it usually means a new connection on the DB server, but not always ‚Äî¬†it depends on what stands inbetween the Rails app and the DB server. If you happen to have PgBouncer in your stack, sessions are managed by it. If PgBouncer runs in any mode different than session mode , you can even end up mixing tenant‚Äôs data .  ‚Äî read more here . If you want to know what session you‚Äôre currently on, you may use pg_backend_pid() . It‚Äôs basically the PID of this session‚Äôs backend (OS process). Feel like giving a comment? Reply under this tweet . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-10-07"},
{"website": "Arkency", "title": "Rails multitenancy story in 11 snippets of code", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/rails-multitenancy-story-in-11-snippets-of-code/", "abstract": "Let me tell you the story of how we implemented Postgres-schema based multitenancy in one of the projects we dealt with. It was an existing single tenant system ‚Äî though definitions vary . We were meant to multitenantize this system. That was the main precondition that made us pick schema-based approach, but overall, the decision is not obvious . We also went for the Apartment gem , as it was the most mature and popular ‚Äî but it‚Äôs not currently maintained, so we ended up on one of the forks . You can tell a story with a wall of text, but it‚Äôs not optimal for everyone. For example, children would rather see some pictures. When it comes to programmers, I guess a lot of you will prefer a snippet of code rather than a wall of text. So let me build the story around those. Please bear with the greek letters ‚Äî I just wanted to use something more fun than plain digits :) Just to keep everyone on the same page ‚Äî¬†the simplified snippet above shows the most common solution to switch the tenant‚Äôs schema in a Rails app. You basically set a special Postgres session variable whose scope is the current DB connection. This typically happens in a piece of middleware. Apartment integrates with Rails via middleware that switches the tenant as depicted in the first snippet, e.g. according to the subdomain of the current request. Just make sure you plug it in ‚Äúhigh enough‚Äù in the middleware stack, so that any middleware that happens to contact the DB, is embraced by the tenant switch. In our case it was ActionDispatch::Session::ActiveRecordStore . This is how you configure Apartment, to always append extensions schema to current search_path (which changes as you change tenants). extensions schema needs to contain your PostgreSQL extensions, like hstore or ltree , if you happen to use them. This is how you move the extensions to the extensions schema. You probably need to move them, because typically they reside in public ‚Äî the default schema. This may be more tricky than the above snippet ‚Äî e.g. because of roles and ownership. Make sure you can do it on your DB setup. It‚Äôs worth double checking what happens if you access the DB from another app process ‚Äî you‚Äôd assume you‚Äôre on another DB connection with an independent search_path ‚Äî but this might not be the case, when, for example, you run PgBouncer in anything else than Session Mode. More here and here . This is what you can do when you‚Äôre on a SQL backed background job queue, like Delayed Job. You tell it to always put the jobs in a shared schema ( public in this case), by using a fully qualified table name, which overrides search_path . This little snippet tells a couple things. First ‚Äî a short reminder that your migrations will need to be run against every schema separately (consider time, errors and rollbacks). Second ‚Äî if you need something like a global migration, you can make an ugly if. Third ‚Äî employing Postgres schemas is sometimes at odds with Rails assumptions, which leads to some nuances. Mostly solvable, though. For example, what exactly should your db/structure.sql contain. If you have any handcrafted in-memory caches in your app, make sure to invalidate them on tenant switch. That might be the case when you‚Äôre transitioning an existing system. If you spawn threads inside your requests or background jobs, make sure to set the search_path on their connections too. That should be pretty rare, but you don‚Äôt want this to surprise you. Now this piece of code is even weirder ‚Äî why would you set up another connection to the same DB? But I‚Äôm sure you know pretty well that a lot of weird things can be found in the legacy systems we deal with. We had such a situation, actually with a legitimate reason to it. It basically results in another connection pool , where you need to set the search_path too. Now this is a PoC ‚Äî a way of switching tenants alternative to the default way described in the first snippet. If you‚Äôve followed along, you could see that a lot of the issues we had were related to statefullness of the search_path . Or, more precisely, to the fact that current state lives in a DB connection. Keeping this piece of state in-process could make the solution more robust. We haven‚Äôt tried this in production, but¬†I can imagine going for it under special circumstances. Have a comment? Leave a reply under this tweet . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2020-10-14"},
{"website": "Arkency", "title": "4 strategies when you need to change a published event", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/4-strategies-when-you-need-to-change-a-published-event/", "abstract": "Event versioning ‚Äî you‚Äôll need it one day anyway. You publish an event. You handle it in some consumers, perhaps aggregates or process managers. Soon you realize you need to change something in the event‚Äôs schema‚Ä¶ Like adding an attribute, filling up a missing value, flattening the attribute structure. What do you in such a situation? Meaning you just loop over the past events that need to change and mutate their payload in the database. You might be tempted to do it using RES internals: But please do not. There‚Äôs an api exactly for this purpose described in the RES docs : This way or another, mutating past events is what most people intuitively do if they weren‚Äôt previously exposed to the topic of event versioning. But, I believe in most cases you should not go for this strategy. It seems like it‚Äôs just fine, but: This strategy can be still fine in situations like: the event was only published on a staging environment so far. Or when you control the consumers and the change is trivial ‚Äî where triviality can mean anything depending on your team. A lot of people are fine with it for e.g. event name change, some other people will say changing a key/value in the payload is trivial too (like user_id: 12345 -> approved_by: \"someone@example.com\" ). I.e. you just let your event be this or that. Newer events will have the new attribute, older events simply won‚Äôt. It‚Äôs not easily seen if you don‚Äôt have schema validation set up. But if you do, you typically need to explicitly weaken the schema. If you happen to use dry-struct , that can mean using attribute? to let older events stay without the new attribute without causing validation errors when loading events. In any case, this pushes the complexity of handling different event shapes to consumers and results in a lot of defensive code. Also, with schema validation set up, it weakens the writes ‚Äî which is completely unnecessary (unless you choose to validate only on write, which has its own pros and cons). The downsides of weak schema are clear. It still sometimes makes sense ‚Äî for example as a hotfix to gain time for a cold fix. I.e. whenever an old event is read, you transform it on-the-fly. You can do it by defining a mapper in RES. Now, whenever an old event is read from the event store, it‚Äôs transformed and the consumer receives it in the new shape, so it doesn‚Äôt need to handle both of them. Upcasting is often a good default strategy when events are already in production. But, what if the necessary transformation is not practical to achieve on the fly? I.e. publish/append new events into a new stream, leave the old stream untouched, switch to the new stream. Like permanent upcasting with the price of new event records. Arguably most expensive operationally, but it can handle complicated scenarios and doesn‚Äôt wipe out the history. It may be helpful to compare this strategy to git rebase . This is far from what can be said on the topic. If you want to know more, make sure to check Versioning in an Event Sourced System by Greg Young. Got comments/questions? Ping or DM me on twitter or reply under this tweet . Special thanks for Pawe≈Ç for crunching the topic together. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-01-11"},
{"website": "Arkency", "title": "Low-boilerplate process manager", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/low-boilerplate-process-manager/", "abstract": "Process Manager ‚Äî you can think of it as something that takes events on input and produces a command on output. You might have found yourself in situations, where you felt you should be implementing one. A simple example that often happens: you‚Äôre familiar with event handlers , but at some point it seems like you need an event handler that would activate after two distinct events happen ‚Äî not just one . Often people are tempted to propagate attributes in events. It‚Äôs rarely a good idea. It might be a good place to employ a process manager. Typically, to implement it, you handle specific events and store process manager state in an plain ol‚Äô active record ‚Äî but it‚Äôs not the only way to do it. Since introduction of linking events in RES, another approach is viable: event sourced process managers. Their advantage is that you don‚Äôt need to set up another db table. You might have already read about them in a blogpost by Pawe≈Ç . What I‚Äôm giving you here is another implementation of it. A simple implementation that hopefully illustrates the essential parts. You can take it and tweak it. It utilizes linking to streams and RES projections: How it‚Äôs wired up: Exactly three things: 1. ‚ÄúCapture‚Äù the event that is needed to determine the process manager‚Äôs state (put it into the process manager‚Äôs stream): 2. Fetch all the events currently linked to the PM‚Äôs stream and build the current state from them: 3. If the conditions needed for the process to complete are met, execute the piece of code. Got comments/questions? Ping or DM me on twitter or reply under this tweet . Did this piece help you? Can we improve something? Let me know, your feedback is very valuable! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-01-07"},
{"website": "Arkency", "title": "How well Rails developers actually test their apps", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/how-well-rails-developers-actually-test-their-apps/", "abstract": "Here are the results of our State of Testing in Rails apps survey results. I have distilled the most interesting numbers for your reading pleasure and efficiency. Detailed charts here . I have highlighted what stands out as interesting for me.  Number of surveyees: 142. Thanks for being one! Want to see detailed charts ? Jump into this twitter thread . Also, it‚Äôs the best place to comment or ask further questions. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-01-13"},
{"website": "Arkency", "title": "Gradual automation in Ruby", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/gradual-automation-in-ruby/", "abstract": "It‚Äôs the simplest piece of Ruby code you‚Äôll read today. I originally stumbled upon it here , where it‚Äôs referred as do-nothing scripting . I have yet another name for it, though: Puts-First Automation or Puts-Driven Automation . You want to codify a manual process like setting up another instance of your e-commerce app. It may involve several steps with varying potential for automation (like seed the db, set up a subdomain, set up admin account). The original example is in Python. This is how I once did it in Ruby. You can see why it‚Äôs called Puts-First Automation ‚Äî at first you puts what has to be done, and you‚Äôre happy to have your steps documented. Then you gradually automate, only when you think it‚Äôs worth it. You can see here how one step is done manualy, and the other is automated. Send me a gist showing how you do it and I‚Äôll link your example here. DMs open . Got comments? Reply under this tweet . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-01-12"},
{"website": "Arkency", "title": "The Goodies in Rails Event Store 2.x", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/the-goodies-in-rails-event-store-2-dot-x/", "abstract": "But‚Ä¶ what is Rails Event Store and why would I use it? It‚Äôs a Ruby library to publish, consume and store events in your Rails application. Events are an architectural pattern that helps you decouple your code, record what‚Äôs happening inside your app, avoid the callback hell and many other kinds of hells . Most often, when you try this pattern, there‚Äôs no going back . We‚Äôve released 2.0.0, 2.0.1, 2.1.0. High fives for Pawe≈Ç , Mirek and Rafa≈Ç for all the hard work. Some of the goodies inside: Read on for details. Read the release notes for even more details, specifically for migration guide (2.0 requires DB migrations). No more explicit db record to indicate that the event belongs to the default stream all . Before: every time you published an event, at least two db records were created: After: unless you add your event to a specific stream, no record is added to event_store_events_in_streams . (keep in mind that you still probably link all events to type and correlation streams) A lot of users have asked for this. Let the code speak for itself: Now, your event can have two timestamps: regular timestamp and valid_at . But why ? Keyword: Bi-Temporal event sourcing . You can have one timestamp denote when the event was appended to the store, and the other when it should count as in-effect. Example: you have some kind of policy that is created at one day, but should only be valid at some point in the future. You can query by either of the timestamps: Read more: https://blog.arkency.com/rails-multiple-databases-support-in-rails-event-store/ As a side-effect you can now pass AR model classes to be used by repository. Useful for example when for any reason you had to change default table names: Client#subscribers_for(event_type) returns list of handlers subscribing for given event type. Useful in specs and diagnostics. Speaks for itself. Also introduced in 1.3.1. Upcasting is a technique you can use when you need to change an already published event. Read more here . Now RES helps you with that: Some people prefer to explicilty define event type, to avoid having event type depend on class names, which may cause bugs on class name changes. This was already possible with: But the drawback was that you then needed to use SomeEvent.event_type wherever you‚Äôd normally use SomeEvent : Now, with this change you can avoid that. Configure the event type resolver: And now you can stick to the plain way: Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-01-18"},
{"website": "Arkency", "title": "3 tips to tune your VCR in tests", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/3-tips-to-tune-your-vcr-in-tests/", "abstract": "In this post I describe 3 things that have grown my trust in VCR. These are: Read on to see why I‚Äôve specifically picked them. VCR is a tool which I‚Äôd classify as useful in snapshot testing. You record a snapshot of an interaction with a System Under Test . Once recorded, these interactions are replayed from stored files ‚Äî snapshots. VCR specifically records HTTP interactions and stores results of such in YAML files called ‚Äútapes‚Äù. A tape consists of series of requested URL, request headers, response headers and returned body. There may be multiple requests and responses stored in a single tape. When added to project, VCR installs globally and intercepts all HTTP requests made in a test environment. When there‚Äôs no tape recorded for an interaction, an error is raised, i.e.: For an interaction to be recorded, a living HTTP endpoint with data to record must exist. This is usually is your staging or test service instance. Recording is no different from regular data manipulation ‚Äî querying or modifying. By default VCR is tuned to store gzipped response data in gzipped-and-base64-encoded yaml-friendly string. This data is not decompressed and definitely not greppable: Problem: Solution: From now on recorded gzipped responses will be decompressed. Caveat: This option should be avoided if the actual decompression of response bodies is part of the functionality of the library or app being tested. Another default in VCR states that if there are unused interactions recorded on a tape, they will be silently skipped. No error is raised if the tape has a GET request to https://example.net and this request is not actually made. Documentation says: The option defaults to true (mostly for backwards compatibility) I am sure for majority of the projects on VCR this backwards compatibility is not an important argument. I found myself quite puzzled when I was inspecting a tape (of a legacy application) with multiple duplications in recorded yaml. I initially assumed that the code was making all those requests for some bizarre reason. That simply wasn‚Äôt true. When I disallowed unused interactions, there was a handful of errors. After removing the duplicates and the obsolete ones the test suite was green again. Pull Request showed following stat: Quite a lot of unused YAMLs. To try it yourself, set: Finally I wanted to make some well-placed and precise assertions with webmock on HTTP interactions for new functionality. Recording full snapshots is fine, as long as your test data stays stable. I noticed that some tests had intentionally very limited matching scope to avoid trouble of matching pre-recorded body with always-changing test data: That can be addressed for example with webmock and composing rspec matchers. The problem was that VCR already hijacked all interactions and disallowed webmock to take it over. The solution was to only enable VCR when the cassette was inserted (via rspec metadata). Or rather to disable VCR when there was no cassette: That worked beautifully. The caveat is you have to explicitly enable VCR when not using vcr: in test metadata: Not a big deal. If I used this, I‚Äôd probably extract the whole block as the with_cassette helper method: All above tweaks finally led me to following snippet of configuration: I hope you found some of these useful. Catch me up on twitter and let me know what you think about it. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-02-13"},
{"website": "Arkency", "title": "Recording real requests with WebMock", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/recording-real-requests-with-webmock/", "abstract": "‚Ä¶to get an experience almost like with VCR , but without it ‚Äî thankfully. If you‚Äôre my-future-self and you‚Äôre just looking for the piece of code to paste, it‚Äôs here: Caveat: it‚Äôs global, i.e. it‚Äôll apply to all the test cases you‚Äôll run. If you want it for just one test case, run just a single one. (As far as I tried, there‚Äôs no easy way to cleanly make it per-example with an around hook, because that would mean we need to store and restore the list of WebMock callbacks. You can get them from WebMock::CallbackRegistry.callbacks , but I couldn‚Äôt find a way to set them back, apart from WebMock::CallbackRegistry.reset which empties the list of callback, which is fine only when there were no other callbacks in the first place.) WebMock is a library for stubbing/mocking HTTP requests in your Ruby code. Once you enable WebMock and try to make a HTTP request in your test, you‚Äôll get an error like: I really appreciate that WebMock gives you a copy-paste-able snippet to stub your request in a test case, but this snippet is only useful to some extent ‚Äî to stub your request when you don‚Äôt care what it returns. When your code cares about the returned response, you need to define the body and you need to take it from somewhere. Usually people play with the real service and take an example response body from it. This can get quite tedious when there‚Äôs a lot of requests. There are tools that help you with this too ‚Äî¬†if you run your test again a real service, VCR will record any HTTP interaction to a yaml file called ‚Äútape‚Äù, which can then be replayed in subsequent test runs without hitting the real service. But personally I‚Äôd rather avoid VCR and limit myself to WebMock, unless I‚Äôm forced too ‚Äî for reasons that could make another blogpost. Shortly ‚Äî it‚Äôs easy to start with, but tends to be painful to maintain. (Actually, while I‚Äôm in the mode of sharing opinions, I‚Äôd limit the use of WebMock too ‚Äî preferrably just to test the adapters . Domain tests can stub/mock/fake the adapters. Otherwise the tests quickly get noisy.) But we can use WebMock like VCR to some degree. WebMock‚Äôs after_request callback can be used to get hold of any outgoing request (once you allow them with WebMock.allow_net_connect! ) and print it to stdout. Sounds promising, but if you only go with: ‚Ä¶you‚Äôll no longer see these ready-to-copy stubbing snippets, but we can have them back with RequestSignatureSnippet#stubbing_instructions , which is what the original snippet is about. Here are some other pieces about WebMock and VCR we‚Äôve published recenty: 3 tips to tune your VCR in tests and Testing cursor-based pagination with Webmock . If you want to comment or discuss, feel free to reply under this tweet . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-03-08"},
{"website": "Arkency", "title": "Discord-to-Slack bot hosted on repl.it", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/discord-bot-talking-to-slack-hosted-on-repl-dot-it/", "abstract": "So we started using Discord alongside Slack recently. The selling point were voice channels and screen streaming. It made voice conversations much smoother and  more async friendly and made us overall closer to each other. More context here . Basically, whenever someone joins a voice channel it means: But since we‚Äôre using Slack as the primary means of communication, not everyone in our team is used to having Discord open at all times. We need one thing: an integration that would notify us on Slack, whenever someone joins a voice channel on Discord : We‚Äôll need to: Now the code takes the discordrb gem and uses it to listen to voice channel updates (basically any event related to voice status, like self-mute or screen share). The updates get filtered and posted to the Slack webhook. First, some basic setup of Discordb that will puts whenever something related to voice happens on Discord: Now we only care about a specific set of events ‚Äî when someone joins, leaves or changes a voice channel. That‚Äôs how we can filter them out in the main method: Now we need to actually send these notifications to slack. It‚Äôs just a plain post to the configured webhook: Run it from my local machine‚Ä¶ Works! You can find the full gist here . Special thanks for Pawe≈Ç for working on it together as a short coffee-break-hack. Click, click, click, the app is live üéâ I have to admit it was fast. Personally I really like it when services and tools improve DX, eliminate friction and are more approachable. Want to discuss? Leave a reply under this tweet . Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-02-18"},
{"website": "Arkency", "title": "Testing cursor-based pagination with Webmock", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/testing-responses-from-http-apis-with-cursor-based-pagination-and-webmock/", "abstract": "Once upon a time I was working on importing orders from a phased out Shopify shop instance into an existing system. The application was already interacting with a new shop instance. The business wanted to extend the reporting and gain insight into legacy orders from previous shop, in addition to existing one. The developers already implemented a tailor-made adapter to interact with Shopify API, which wrapped shopify_api gem. \nSo far so good ‚Äî I thought, and jumped straight into the core of the problem I had to solve. The low-level interaction details were abstracted away, allowing me to move focus elsewhere. \nYet something was odd when I‚Äôve been using this adapter against legacy shop endpoint. Some product variant resources were not to be found in the API although I could look them up in the admin UI. The part of the adapter in question looked like this: It got me thinking. Why do we use this particular value as the limit? And how many variants do we actually have in each of the shops? Turns out that Shopify by default returns up to 50 items of the collection in the API response. The new shop had not much over 50 variants. Increasing the limit to fit existing variant count was surely a pragmatic way to overcome a similar problem in the past.\nHowever the legacy shop had over 400 variants. And the limit of 250 turned out to be the maximum one can set ‚Äî for a reason. In general, the bigger the query set, the more time is spent: preparing (querying the database, serializing results into JSON objects, streaming the response bytes) consuming it (receiving bytes and parsing it into something useful) Cursor-based pagination is the one where you navigate through a dataset with a pointer, marking the record where you left, and a number of records to read in a given direction. In contrast to offset-limit pagination there‚Äôs no situation where changing a part of the dataset prior to the cursor affects the next set of results. To quote a great explanation from JSON API specification: For example, with offset‚Äìlimit pagination, if an item from a prior page is deleted while the client is paginating, all subsequent results will be shifted forward by one. Therefore, when the client requests the next page, there‚Äôs one result that it will skip over and never see. Conversely, if a result is added to the list of results as the client is paginating, the client may see the same result multiple times, on different pages. Cursor-based pagination can prevent both of these possibilities. In SQL databases there are some interesting performance implications as well. Shopify API exposes cursor-based pagination. The page_info parameter is our cursor, limit drives the number of results and we only move forward. This is how it looks like from API client gem perspective: At this point I could have improved the API adapter and call it a day: I did not üò± In my worldview this ShopifyClient adapter is an abstraction of every 3rd party interaction we could have in this application. There may be reasons out of which I would change the implementation of the adapter. At the same time I would not like to change how the application interacts with the adapter. When testing, I would like to extensively test how the adapter interacts with the 3rd party API on the HTTP protocol level. On the other hand, I would not like to exercise each piece of the application with that level of detail when it comes to 3rd party ‚Äî only that it collaborates with the adapter in a way that is expected. Before you ask: the reason why would I test HTTP interactions of the adapter despite the presence of convenient shopify_api gem is to keep options open in the future: when its time to change the adapter I‚Äôd like to do it with confidence and without hesitating too much how it affects the rest ‚Äî keeping HTTP interactions in check gives me that context switching ‚Äî I already had to jump into very details of Shopify API and to this particular code, I‚Äôm sure months from now I‚Äôll not have all that cache in my head, thus making future changes more costly than now dependencies graph ‚Äî each application dependency constrains it more, the scope of the gem is much bigger than the needs of the application I work on and I‚Äôd not hesitate to drop the gem as soon as it becomes a trouble (i.e. its activeresource dependency ) Testing paginated responses can be tricky. We need at least two requests for subsequent pages to verify that paging works as expected. The URL and query parameters must match (looking at that maximum per-page limit). Finally the response must be in shape and it can be a lot of records to fake or replay for two pages of results. Today I‚Äôd like to show you how I specifically approached this with webmock gem. There are other fine tools one can use instead. Unfortunately I may not have enough patience or forgiveness to use them. A TDD practitioner would begin with a failing test and fill in the implementation, which in turn makes a ‚Äúred‚Äù go into ‚Äúgreen‚Äù. We already have a non-paginated adapter implementation and the spec is consciously blank for educational purpose. Let‚Äôs execute following: Despite no expectation to fulfill, this triggers following error: That‚Äôs very useful error to have. It tells that: We need expectations on the URL and query params. Let‚Äôs stick to that, dropping with(...) part completely. It is a GET request so no body is posted, but we need body to return as a response. This is something webmock cannot provide for us and where I usually fallback to curl : Here‚Äôs a little cheating ‚Äî I don‚Äôt actually want to have 250 resources in as response in the test. Just the single one, but still in shape of the collection: The response looks more or less like this (with majority of the attributes removed from output for brevity): There‚Äôs one more thing to look at. Response headers! Among the various key-values, there‚Äôs the one we‚Äôre looking for. A link . It‚Äôs value reveals what is the link to the next page of results: With all that knowledge, let‚Äôs improve the spec and pass the first webmock expectation: Our non-paginated Shopify adapter would pass this, a paginated one too. We need to introduce more expectations. Knowing the value of Link header, let‚Äôs assert on that: Client should follow the URL from link header in order to get the next set of results. This link contains the cursor in form of the page_info parameter. The result of following the link is the second page with the resource we‚Äôre looking for. Translating all this into a spec: We‚Äôve now covered full interaction with a paginated endpoint: All clear and explicitly stated in code, as opposed to VCR-recorded interaction in YAML fixtures. I hope this post gave you some useful insight how to use webmock and how a cursor-based pagination can be approached. You can use Request callbacks to reach zen in web mocking. No need to mess with curl when you can print all the request and response data using Webmock . Please remember to enable real requests to get what you want. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-02-09"},
{"website": "Arkency", "title": "Use ActiveAdmin like a boss", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/use-activeadmin-like-a-boss/", "abstract": "ActiveAdmin is widely used administration framework in Rails applications. This post explains how to hook more sophiscicated domain logic into it. An ActiveAdmin resource can be generated to perform CRUD operations on the application‚Äôs models. However, there are scenarios where: Taking this into account, classic CRUD approach is simply not enough. Below, there‚Äôs a short example extracted from sample Order management app What‚Äôs happening here: One can say ‚Äî but it‚Äôs just a status column update . Nope, that tiny column update is a result of business operation taking specific constraints into account. Moreover, data remain consistent because there‚Äôs no longer a possibility to ‚Äújust‚Äù update the column or many of them.\nThat updated column is a representation in read model which is not meant to deliver domain behaviour, only data for display ‚Äî Implementing Domain-Driven Design, Vaughn Vernon . I find this useful, especially if one wants to avoid multiple checks and conditions (eg. IF statements) in code to determine whether certain actor (admin in our scenario) is capable of doing certain operation. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-03-19"},
{"website": "Arkency", "title": "7 simple ways to make remote work feel less lonely", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/n-simple-ways-to-make-remote-work-less-lonely/", "abstract": "4+ years ago I joined Arkency, a remote-first company. It was so remote that I didn‚Äôt even see my teammates‚Äô faces for almost half a year. The most surprising thing about the new job, was that, even though it was remote: Nowadays most of us need to work remotely. I guess not everyone is happy about that, partially because of all that accidental human interaction lost. Here‚Äôs how we‚Äôve always dealt with that: Let‚Äôs say your name is Dorothy. Now create a channel named: #dorothy . Treat this channel as your own blog, which is just published internally in your company, with your teammates comments inlined. Post anything related to your life: going on a hike, read something interesting, buying a piece of hardware and needing help ‚Äî post it and let others chime in. Good place for occasional photos, too. When working on a project, don‚Äôt defer communication until you‚Äôre stuck or until you achieve something noteworthy. Turn yourself into --verbose mode and keep actively communicating whatever you‚Äôre doing to progress with your task at hand. The benefits are threefold: Example: one day I had to set up assets compilation on CI. I would probably do it sooner or later with more or less googling and that was the plan. I just kept communicating what I‚Äôm doing step on a channel where everyone has access, but I didn‚Äôt expect anyone to read. I didn‚Äôt even assume anyone is around. By default I‚Äôm just communicating my moves for the purpose of rubber-duck-debugging. But perhaps someone is around, has already dealt with that and doesn‚Äôt mind being interrupted at the moment ‚Äî in such situation they can offer to help and this is what happened in this particular situation and I probably saved an hour or two. It wouldn‚Äôt happen though, if I weren‚Äôt overcommunicating , i.e. turn myself into --verbose mode. The default was just to keep doing it myself and no one would be offended if there wasn‚Äôt any response to my messages. We‚Äôve shared a lot of content on why meetings are suboptimal in most situations. We mostly mean meetings that are large, compulsory and regularly scheduled. But we fancy a lot any ad hoc calls, with 2-3 participants max, whenever someone feels like they need it and the other one doesn‚Äôt mind. It can take anything from 5 mins to 3 hours. We sometimes ask one another: ‚ÄúGot a good interrupt for a call?‚Äù. Whenever you think it makes sense and the other party doesn‚Äôt mind it. Especially when exploring a new area. Often it can save you a lot wrong turns and you can build the context together. After such a session both of you are ready to continue working in this area on your own. If you‚Äôre not comfortable with working on a teammate‚Äôs environment via a remote-control tool just try switching after commiting to a branch. If you‚Äôre not comfortable with full-blown pairing style like TDD ping-pong, just go for anything on the spectrum between proper pairing and plain voice consultation. Even passive watching is fine, provided you pay attention to the moment when your partner is losing attention. If you don‚Äôt feel like pairing all the time, try intermittent pairing . Jump in and out of pairing mode whenever you think it makes sense. This may blow your mind, but 83% messages on our Slack are sent to public channels , only 17% in DMs. This is completely different from most other companies, where percentage of public messages can even be a single digit. In particular we keep most of project-related communication on a channel and let everyone see it (even people that aren‚Äôt assigned to this project currently). This way it‚Äôs easier to see what other folks are up to and jump in to help on a good occasion. Our dedication to communication via public channels helps many things. It‚Äôs so much easier to be a part of a project you‚Äôre not directly assigned to. This is a recent experiment we‚Äôre running and it‚Äôs going great so far. We‚Äôve started using a Discord server alongside Slack, just for the purpose of using its voice channels. It‚Äôs like walkie-talkie. Voice channels are way more async-compatible because you don‚Äôt have to synchronize the moment when you jump on the call. You just enter a voice channel whenever you don‚Äôt mind company and just mute yourself (or enable push-to-talk). Probably you‚Äôll be alone for some time, but at some moment someone else may join with random interactions happening. More in these tweets: How Voice Channels are changing the way we work , Another hopefully game-changing thing in the way we work , Discord-to-Slack bot . Here‚Äôs the one we‚Äôre building: Arkademy . You‚Äôre getting access to courses like Rails Architect Masterclass , Blogging for Busy Proggrammers and you‚Äôre joining a community because it can be a game changer. As a part of this, you‚Äôre getting access to Arkademy Discord server, where you can try some of these techniques ‚Äî¬†overcommnicating your steps on your personal channel, joining voice channels ‚Äî especially if you cannot do it at your regular workplace.", "date": "2021-03-17"},
{"website": "Arkency", "title": "Code review of an Order object implemented as a state machine", "author": ["Andrzej Krzywda"], "link": "https://blog.arkency.com/order-code-review/", "abstract": "Let me show you an example piece of code - an Order class. This class is used in our sample DDD/CQRS/ES application . We‚Äôre in the process of improving this app so this is a good opportunity to document certain opinions and changes. As always, this class was nice and simple at the beginning, but over time it grew and became less readable. There are now several responsibilities of this class. Some of those I will leave for another discussion (like coupling the domain code with event code). Today I want to focus on the concept of a state machine. This Order object has now 5 possible states. How come it grew to such size? As always - one by one. 5 is probably still OKish but we can imagine what can happen if we extend it even more. It‚Äôs worth noting that this class uses AggregateRoot which helps in the event sourcing part of this object. What are the requirements for this state machine? More or less this: Draft is the initial state. The happy path then switches to submitted , then to paid . \nThe less happy paths include expired and cancelled , both are leaf states. The challenge with state machines is that it‚Äôs not easy to represent them in code in a readable manner. Whenever the number of states and transitions grows it‚Äôs becoming harder to read such code. State machines consist of states and transitions. Somehow we need to represent them in the code. In this implementation, we put the transition as the main ‚Äúdimension‚Äù. The method names show the possible transitions. However, they show possible transitions for all the states. This leads to a problem, that in each of those methods we now need to ‚Äúdisable‚Äù the impossible transitions. We could do it with just an early return in such cases without using exceptions. The problem with this code is that it‚Äôs hard to easily say, what is the possible flow in this state machine. The code is infected with other responsibilities which make it all less readable. BTW, why do we use exceptions here? Because one responsibility of this object is to communicate ‚ÄúWHY‚Äù a certain change is not possible. An early return only communicates a boolean information - possible or not. A custom exception brings more context. What are the possible directions of improvement here? The first direction is the most tempting here. Reducing the size via reducing the number of possible states would help. It would help as it decreases the scope of other problems too. That‚Äôs the direction that is most beneficial - it does improve the root of the problem and thus reduces other problems. How can we reduce the number of states here? Stay tuned :) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-03-20"},
{"website": "Arkency", "title": "Disadvantages of Pull Requests", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/disadvantages-of-pull-requests/", "abstract": "Pull requests with blocking reviews (sometimes mandatory) are widespread in our industry. A lot of developers believe pushing straight to the main branch should be prohibited. Sometimes it‚Äôs unavoidable (in a low-trust environment), but often people work with PRs just because everyone else does . And nobody ever got fired for it. But what are the costs of working in such style ? And what are the alternatives? I wrote this post to gather the disadvantages of a typical PR flow, so that you can make a better informed decision ‚Äî by knowing all the potential costs involved. You can judge yourself how each particular aspect applies to your specific work setting. If you have anything to add, contact me on twitter or submit a pull request to this blogpost . How ironic üôÉ PRs promote developing code in branches, which increases the time and the amount of code staying in divergent state , which increases chances of merge conflicts. And merge conflicts can be terrible, especially if the branch waited for a long time. Now branches are unavoidable, even if you only commit to master and never type git branch . You‚Äôre still creating a logical branch at the very moment you change a file in your local working directory. But it‚Äôs up to us: Limiting these factors can make merge conflicts happen less frequently. When I have a feature to implement, I often like to work like this: (I believe this is what Kent Beck meant saying For each desired change, make the change easy (warning: this may be hard), then make the easy change ) for each desired change, make the change easy (warning: this may be hard), then make the easy change Think of Google-Docs-style realtime collaboration vs. emailing each other subsequent versions of a document. Of course, we prefer the realtime style and a lot of applications evolve towards it. But where do our code integration techniques fall on this spectrum? By allowing small atomic commits we‚Äôre closer to realtime collaboartion with all the benefits of it. From personal experience, it feels like using Google Docs compared to emailing each other Word documents. Also refactoring much much more often since it‚Äôs extremely cheap and fast. Which helps with => https://t.co/jdN1eSmJPl I bet you can relate: PRs tend to promote reviewing bigger chunks of code . I also like to increase reviewability of my changes by annotating them (and by mixing purposes as rarely as possible ‚Äî separate refactors from changing behavior). This way I can suggest where the reviewer spends the most attention. Why is programming so much more fun compared to other engineering disciplines? Perhaps because it‚Äôs so quick to build something and see the result of your work. If you‚Äôre building skyscrapers or airplanes, it takes years to see the product you‚Äôve been working on. This is also the reason why a lot of programmers find UI (or game) programming more enjoyable ‚Äî because you can code something up and quickly see a fancy effect on the screen, which makes you want to do more. Now PRs make this feedback loop longer . You code something up but it‚Äôs nowhere close to being integrated and working. You now have to wait for the reviewer, go through his remarks, discuss them, change the code‚Ä¶ There are a lot of processes for managing programmers‚Äô time, quality of the code, but I believe it pays off to also manage programmer‚Äôs energy and momentum . Working on a project now without a laborious PR review process, and it feels like I‚Äôm flying. Perhaps it‚Äôs familiar to you: Why is that? In order to properly review a non-trivial piece of code, I need to focus, get deep enough and build enough context ‚Äî almost to the extent that the PR author did while coding. The reviewer has to do it without the privilege of being able to put my hands on the keyboard and without the same amount of time. Proper review takes the same amount of focus as actual coding. Why not pair-program instead? The superficiality of reviews is amplified by the following factors: Remarks made by the reviewer can fall anywhere on the spectrum of whether they should block merging or not: from a mistake that‚Äôll bring production down to cosmetic suggestions or opinions (with more of the latter ones). How do we account for this variety? Typically, any PR comment makes a dispute that needs to be resolved before merging. Let‚Äôs say the reviewer has an idea how to make a piece of code in you PR better. They explain it in a comment. The original author has to understand it first, agree with it, and then is expected to implement it. Often it‚Äôs better to let the original author merge his thing, and let the reviewer implement his remark post-factum? Faster, less costly, less frustrating. PRs promote more words instead of action . Compare these two developers: Which developer will faster learn to code responsibly ? The first one knows that whatever they commit, first lands on a branch, and doesn‚Äôt affect anything. Then there‚Äôs the review, so if they commited anything blatantly wrong, perhaps the reviewer will catch it. The second one knows that every line they write can screw up things for other developers or even bring production down. They watch their step, they know they are the only one responsible for this change. It shortens the delay between making a mistake and seeing the effect of it . People argue that you need PRs because of junior programmers. Probably yes, but do you consider how fast can such a person stop relying on reviews on develop his own sense of responsibility? I believe refactoring is an activity that should be performed continuously. It‚Äôs good to follow boy scouts‚Äô rule: always leave the place better than it was before . Over time it can lead to nice codebases. With PRs, though, this rule is harder to apply: if I see a piece of code worth fixing while working in this specific area, I can now: The latter imposes some additional effort ( tax ) which means that some refactorings won‚Äôt be attempted. Some discussion with interesting arguments: Hypothesis: PR-based workflow kills continuous refactoring. I see a piece of code I can improve while working on a feature and‚Ä¶ no, I‚Äôm not doing it now to avoid having irrelevant stuff in the PR (and reducing reviewability). Mandatory PR reviews can induce way more negative emotions than needed. Let‚Äôs say someone nitpicks on a PR because they had to point out something. The original author takes it personally. We all have limited emotional budgets ‚Äî it‚Äôs better not to waste it on avoidable stuff. In rare cases it can lead to absurd behaviors, e.g. developers making arrangements behind the scenes: I‚Äôll approve your PR, and you‚Äôll approve mine . You obviously sometimes need migrations while working on a branch. What do you do if you then have to switch back to another branch locally? Cumbersome. With a Rails app, how do you manage switching between a bunch of different branches that have different DB schemas/migration states? Blowing away the whole DB every time gets old fast. Here‚Äôs a contribution from @PavelKaczor : No one would argue against PRs if we were sure that they substantially contribute to overall product quality. In fact, are we sure PRs increase overall product quality at all? Probably yes, but how much? Probably not much because deciding factors are not the ones that are impacted by the PRs. How do PRs affect the architecture and the design of the system (division of responsibilities between services, how the services communicate with each other, etc.), how do they affect requirements gathering / domain distillation process? If we don‚Äôt see positive impact on these factors then why should we invest time and energy on activities that do not matter. In worst case scenario we could end up with overall product quality decreased due to uproductive consumption of time and energy and distraction from activities that do matter. How often do PRs focus on implemenation details? Is it a big deal if the implementation of a service is not so clean, optimal as one could imagine, especially if the service is an independent application (aka micro-service)? How much time do you spend on discussing, cleaning the requirements before starting the implementation of a new feature? How does this activity influence the quality of the produced code? Think about the universal Pareto‚Äôs Law (80/20 Rule) and try to follow it. Concentrate on processes that contribute 80% of your product‚Äôs value. I‚Äôm not suggesting a total outlawing of PRs. Gerald Weinberg says you can only make the 10% percent change in your organization. What are examples of such changes? If your manager thinks pair-programming is a waste of time, perhaps they can be convinced by a meme: Super excited that you liked my last drawing. Here is a new one! ü§ó pic.twitter.com/vULVb0uoFo Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-04-12"},
{"website": "Arkency", "title": "How to delete jobs from Sidekiq Retries", "author": ["Tomasz Wr√≥bel"], "link": "https://blog.arkency.com/how-to-delete-jobs-from-sidekiq-retries/", "abstract": "Hi future-me! This is just a list of snippets I might be looking for next time I suddenly have to deal with a huge list of failing Sidekiq jobs being retried over and over. (similarly, there‚Äôs &:kill , &:retry ) (similarly, there‚Äôs Sidekiq::DeadSet , Sidekiq::ScheduledSet ) (warning: the details depend on your RES async scheduler implementation) Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-04-14"},
{"website": "Arkency", "title": "Write once, publish in many places, keep SEO happy", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/posting-the-same-entry-on-many-blogs-can-you-do-it/", "abstract": "One of the questions that we frequently receive on arkademy blogging course is about reusing and sharing the same post multiple times: When you got your own blog, do you blog only on this website, or you can blog on platform like Medium or dev.to too? If you blog on many sites, do you centralize all blog‚Äôs titles somewhere or do you duplicate the content on your blog? Before diving into technical aspects of reusing your blog content, let‚Äôs examine the reasons why you‚Äôd may want to do it in first place. Blogging is great. When you‚Äôre a developer starting this journey, it is unfortunately very easy to fall into a trap of doing your own blog platform or engine first. Endless layout tweaks, picking fonts, code example highlighting, comment system and whatnot. It is interesting for sure! When you‚Äôre just beginning blogging it may quickly wear off all your initial enthusiasm. It‚Äôs a way to procrastinate instead of writing too. Instead ‚Äî start on an existing platform. Accept all of the platform limitations. Let it help you taking away the distractions for writing. You‚Äôre not deciding on service permanently after all. Write your first ten or twenty blogposts there. And only then consider moving to your own website. When moving, most of the time you can export and take all of your posts. Then re-publish them on your website. You have your own blog. Maybe it is this fancy static-site kind of blog. Or it‚Äôs a Wordpress installation. It doesn‚Äôt matter. What matters is getting your message through. You can write for your own sake but your posts won‚Äôt help anyone if people can‚Äôt reach them. Re-posting your content on blog platforms like Medium or dev.to gives you additional exposure: chances are some people do not look outside those post aggregators platform can drive you some traffic from your content being featured, recommended or categorized You‚Äôre a specialist and blog about some very intricate topics. Perhaps those topics and your content align well with someone‚Äôs else business. You decide to collaborate with them and now your post is featured on their blog. For example: ‚ÄúOptimist‚Äôs Guide to Pessimistic Library Versioning‚Äù presented on schneems blog and featured on a third party . The drawback of duplicating content on the web is the SEO. Doing so will hurt the way search engines rank your content. Unless you‚Äôre very explicit what is the original post and what is duplicate. \nTo do so, use canonical link element . Duplicate posts are indexed by search engines less frequently . For example this is an article originally posted on arkency blog, that is also on Medium . Inspect its source to find rel=\"canonical\" link buried inside: Besides telling the machines what is duplicate and what is not, I cared for the reader too. There was a note at the end telling about the source: Do blogging platforms actually allow you to place canonical link reference? In the past Medium allowed it. There‚Äôs a way to do in on dev.to for sure. Happy blogging! Now, a plug üîå. This post is a part of #5days5blogposts challenge on ARKADEMY.DEV . \n\nIn this challenge we encourage you to publish five blogposts in five subsequent days. We help you generate topics to write about, structure the content and review the final effect. You get the support from our community of teachers and learners on Discord. Join ARKADEMY.DEV now to start your blogging journey!", "date": "2021-04-19"},
{"website": "Arkency", "title": "'render' is not your final word in your Rails controller's action", "author": ["Jakub Kosi≈Ñski"], "link": "https://blog.arkency.com/render-is-not-your-final-word-in-your-rails-controllers-action/", "abstract": "Today I have a quick tip for you. Suppose you are using some library that performs some logic in your controller and render some templates once some invariants are not met. \nAssume the library is old and was created once you were not using JSON API but was only rendering HTML templates.\nNow you need to add similar logic to your API controller. You may think that you should now modify the library to handle JSON responses, but that‚Äôs not the only solution you can use. Remember that the render method in your controllers is not returning the control flow from your action and you can still modify the response or perform some operations once you call render . \nYou only cannot call render more than once in a single action as that will raise the DoubleRenderError exception. This means you can enhance your action without touching the library. Let‚Äôs assume your library is exposing a module with a method that is rendering a template in case on an exception (I‚Äôm not going to discuss if the library is well-written here, this in only an example): and you API controller looks like this: The problem with using the library in your controller is that when the check_active method returns false , it also means the HTML template with 200 OK status will also be rendered in the response. \nYou can always create some JSON template to overwrite the default HTML template provided by your library, but this will still return 200 OK status (and you should not return successful status code if your response is not successful). In order to handle this, let‚Äôs just modify your response status directly later in the flow: Now, as long as you create a JSON template to overwrite the default one (e.g. app/views/activation_check/error.json.erb ), you will return the JSON response with proper status code in your controller without modifying the original library. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-04-16"},
{"website": "Arkency", "title": "How to speed up Netlify build, twice", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/how-to-speed-up-netlify-build-twice/", "abstract": "Netlify is a platform to build and deploy static sites (i.e. nanoc or jekyll). We use it extensively for many of our sites at Arkency, like this blog . The platform acts both as a Continuous Deployment pipeline and a hosting for generated HTML files, with Content Delivery Network. That is a quite convenient combination of features. The workflow to publish changes on a site starts from a push into a git repository. That triggers the build process for a given revision of the repository. That build process runs within a sort of stateless container. That is pretty typical model if you‚Äôve ever had anything to do with a CI/CD in a cloud. The container is stateless itself, but it needs some state in order to make build process quicker. That state is cache, which can be: Netlify is smart enough to figure out that our Nanoc or Jekyll sites have runtime dependencies: Those dependencies are automatically cached in order to save us much time fetching them over and over again on each build. That is pretty typical too, when setting up CI/CD and it is great to have it working out of the box. Netlify as a platform goes ‚Äúwide‚Äù, to attract many developers and provide great initial experience for most of us (there‚Äôs far more ‚Äújamstacks‚Äù than just Nanoc or Jekyll). In order to squeeze performance to the very last drop, you have to go ‚Äúdeep‚Äù instead. For example nanoc is very good at avoiding unnecessary recompilation and will only recompile an item if it is deemed to be outdated. For this to work, it maintains a set of files: Netlify has no idea that these, along with generated output from previous builds, are crucial for nanoc to work its best. Same goes with parceljs ‚Äî¬†a very pleasant-to-work-with asset bundler with blazing fast bundle rebuilds. In order to achieve it, this tool maintains its own cache in .cache directory of the project. Again, it is too niche and specific for Netlify to optimize for, they go ‚Äúwide‚Äù. Luckily Netlify left the door open for developers to augment their build process and let them go ‚Äúdeep‚Äù. This is done with build plugins . Building such plugin is rather straightforward: Pretty sweet balance of possibilities yet without an overly verbose API. Here‚Äôs an overview of build events: It turned out that plugins to take the most of nanoc and parceljs were quite simplistic: Don‚Äôt be mistaken ‚Äî those several lines of code were quite powerful and have cut the build time in half . If you‚Äôre on nanoc or parceljs, perhaps with a starter-kit for static sites that we use in arkency, check these out: Now, a plug üîå. This post is a part of #5days5blogposts challenge on ARKADEMY.DEV . \n\nIn this challenge we encourage you to publish five blogposts in five subsequent days. We help you generate topics to write about, structure the content and review the final effect. You get the support from our community of teachers and learners on Discord. Join ARKADEMY.DEV now to start your blogging journey!", "date": "2021-04-21"},
{"website": "Arkency", "title": "Rack apps mounted in Rails ‚Äî how to protect access to them?", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/common-authentication-for-mounted-rack-apps-in-rails/", "abstract": "Sidekiq, Flipper, RailsEventStore ‚Äî what do these Rails gems have in common? They all ship web apps with UI to enhance their usefulness in the application. Getting an overview of processed jobs, managing visibility of feature toggles, browsing events, their correlations and streams¬†‚Äî nothing you could not do in code or Rails console already. But never really wanted to do there üòâ In Rails apps we add those UI apps with mount in routing: In production application you‚Äôll want to protect access to this Sidekiq dashboard. Let‚Äôs assume this Rails application is API-only. There‚Äôs no Devise nor any other authentication library of your choice there. Fair scenario to rely on HTTP Basic Auth, as illustrated with wonderfully commented example from Sidekiq wiki : Let‚Äôs transform this example a bit to not rely on Sidekiq::Web.use . That‚Äôs very convenient to provide such interface from a library. I want to show you something else here ‚Äî Sidekiq::Web is a Rack application and can be treated as such. Little explanation: Right. Wasn‚Äôt it supposed to be about protecting access? Imagine now that aforementioned Rails application includes all those UIs for Sidekiq, Flipper, RailsEventStore at the same time. How can we have common protection for them without boring copying and pasting same wrapper again and again? Let‚Äôs extract (bad word detected) a factory! Fun fact is that a Proc#[] is an equivalent to Proc#call .\nThe last line can be as well written as: And with all those UIs in place we receive: In the future we could swap Basic Auth to one or another authentication mechanism. The with_dev_auth factory would remain useful and probably survive them all. Now, a plug üîå. This post is a part of #5days5blogposts challenge on ARKADEMY.DEV . \n\nIn this challenge we encourage you to publish five blogposts in five subsequent days. We help you generate topics to write about, structure the content and review the final effect. You get the support from our community of teachers and learners on Discord. Join ARKADEMY.DEV now to start your blogging journey!", "date": "2021-04-21"},
{"website": "Arkency", "title": "Explaining Rack ‚Äî desugaring Rack::Builder DSL", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/explaining-rack-desugaring-rack-builder-dsl/", "abstract": "Yesterday I wrote a post highlighting Basic Auth and how can we protect Rack applications mounted in Rails with it.\nToday when discussing some ideas from this post with my colleague, our focus immediately shifted to Rack::Builder . On one hand Rack interface is simple, fairly constrained and well described in spec . And ships with a linter to help your Rack apps and middleware pass this compliance. A Rack application is a Ruby object (not a class) that responds to call. It takes exactly one argument, the environment and returns an Array of exactly three values: The status , the headers , and the body . On the other hand your first exposure to Rack is usually via config.ru in Rails application: Behind the scenes, this file is eventually passed to Rack::Builder . It is a convenient DSL to compose Rack application out of other Rack applications. In yesterday‚Äôs blogpost we‚Äôve seen it being used directly: Whether you use Rack::Builder directly or via rackup files, you‚Äôre immediately associating Rack with the use and run DSL. \nAnd that triggered an honest question from my colleague ‚Äî how does this DSL relate to that rather simple Rack interface? To add even more nuance, some Rack apps are called a middleware . What is a middleware? In simple words ‚Äî it‚Äôs a Rack application that wraps another Rack application. It may affect the input passed to the wrapped app. Or it may affect the output if it. An example of a middleware that makes everything sound more dramatic: A composition of such middleware and our previous sample HelloWorld application with config.ru would look like this: When executed, it would return very dramatic greeting: Now back to the question that started it all: How does this DSL relate to that rather simple Rack interface? The last example of composition via Rack::Builder can be rewritten to avoid some of the DSL: A single run is needed to tell a Ruby application server what is our Rack application that we‚Äôd like to run. The use of use is on the other hand just optional. If this post got you curious on Rack, a fun way to learn more about it is to check the code of each middleware powering your Rails application: Happy learning! Now, a plug üîå. This post is a part of #5days5blogposts challenge on ARKADEMY.DEV . \n\nIn this challenge we encourage you to publish five blogposts in five subsequent days. We help you generate topics to write about, structure the content and review the final effect. You get the support from our community of teachers and learners on Discord. Join ARKADEMY.DEV now to start your blogging journey!", "date": "2021-04-22"},
{"website": "Arkency", "title": "Zeitwerk-based autoload and workarounds for single-file-many-classes problem", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/zeitwerk-based-autoload-and-workarounds-for-single-file-many-classes/", "abstract": "Rails has deprecated its classic autoloader by the release of version 6.1. From now on it by default uses zeitwerk gem as a basis for new autoloading. The previous autoloader is going to be dropped in future releases. That‚Äôs good news ‚Äî the classic autoloader had several, well-documented, but nevertheless tricky gotchas . This welcomed change brings back some sanity. Unfortunately the initial scope of zeitwerk features did not include one, that I‚Äôd welcome the most ‚Äî an ability to host several classes in a single file. Several years ago I‚Äôve described a pattern that we‚Äôve been using for ‚Äúcomponents‚Äù (or rather a way to express bounded contexts) in Rails apps. Here‚Äôs an example of one of such components ‚Äî the Scanner context. It‚Äôs a top-level, autoloaded directory in a Rails app. Let‚Äôs focus on scanner/lib/scanner/domain_events.rb . This file hosts several, rather small classes that describe domain events in the Scanner subdomain : This worked with classic autoloader mostly due to require_dependency placed in the bottom of Scanner module file: In zeitwerk-based autoloader there is no place for require_dependency anymore. The limitation is known and described in the migration guide. What are your options when you have similar code structure and intend to migrate beyond Rails 6.1? It is totally fine for zeitwerk if multiple classes in a single file are nested under a common module, which maps to a file name: I‚Äôm not a fan of excessive nesting and would like to keep namespaces as flat as possible. With Scanner::DomainEvents::TicketScanned it is already 3rd level and quite verbose. What doesn‚Äôt suit me however may be totally fine for you. Another option is to bow to single-file-per-class philosophy. Yet to keep things organized we can group those related classes within a directory. And make sure this directory does not imply unnecessary namespace with collapsing , We need to tell autoloader to keep scanner/lib/scanner/domain/events collapsed. The pro is that namespace is kept intact as in Scanner::TicketScanned . The classes are also grouped, although not in a single file. Then con is obviously a class-per-file religion. Which may be totally fine for some and there‚Äôs nothing wrong with that. Remove zeitwerk, explicit require list. ;) Perhaps now opting out of autoloading in Rails is easier than ever. With zeitwerk you can tell the autoloader to ignore particular directories . Feel free to expand this article or ping me on twitter with comments. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-04-22"},
{"website": "Arkency", "title": "Rails console trick I had no idea about", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/rails-console-trick-i-had-no-idea-about/", "abstract": "Tweaking .irbrc to make interactive console comfortable is a highly-rewarding activity. You gets instant boost of productivity and there are less frustrations. There were numerous posts and tips featured in Ruby Weekly on this topic recently. I‚Äôve been making my .irbrc more useful too. The harder part was always distributing those changes to remote servers. For example to have those goodies available in Heroku console. And not only for me. There are multiple ways to achieve that, duh. The one that stick though was close to the app code: You‚Äôd open the console first and load the helpers next to the IRB session with: And then Kuba showed me a neat trick that made this load step completely obsolete: Now, whenever you load bin/rails c , the command_bus and event_store methods will be present in the IRB session. That‚Äôs it. That‚Äôs the trick I did not know about for years. You‚Äôre welcome. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-04-28"},
{"website": "Arkency", "title": "How to balance the public APIs of an open-source library ‚Äî practical examples from RailsEventStore", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/how-to-balance-the-public-apis-of-open-source-library-practical-examples-from-railseventstore/", "abstract": "On twitter, Krzysztof asked an interesting question: what are our thoughts on carving a well-balanced API for an open-source library that RailsEventStore is: @arkency what are your thoughts on carving a well balanced API the an #opensource library #railseventstore ? This post is a whirlwind tour on RailsEventStore interfaces, components and decisions behind them. In case you‚Äôre still wondering: Rails Event Store is a library for publishing, consuming, storing and retrieving events. It‚Äôs your best companion for going with an Event-Driven Architecture for your Rails application. In other words ‚Äî it is a storage for events. And a reader of events. On top of relational database. And a pub-sub mechanism. And a few more bits you could implement by your own, like you would implement a web framework. But nobody implements web frameworks on a daily basis, right? The most straightforward way to start using RES is to instantiate the client: That requires no additional configuration or upfront decisions. Perhaps it‚Äôs your first time with this library. You‚Äôre evaluating it and exploring its usefulness in context of the project you‚Äôre working on. At this point any obstacle from getting to the core functionality is a friction to eliminate. We provide sensible defaults to lean on. This facade channels most library interactions. It is the entry point. Quite liberal on the inputs in some ways, like accepting both event or collection of events: On the other hand, an entry point is the best place to validate the input and catch mistakes early on: This further allows collaborators (i.e. broker or repository) to expect and act on a valid input. Also to have much stricter APIs down the stack . Like no longer accepting item-or-array-of-items or string-or-symbol primitives in this RBS description: Since facade is the thing a developer would interact with the most, its worth taking care of ergonomics . For example, a fluent interface on the reader : Starting with batteries included is great. Over time your project will likely diverge from the defaults ‚Äúfor everyone‚Äù. At this point it would be better if the library leaves some room for that. If RailsEventStore::Client.new was the frontend, here‚Äôs the backend view. All of the assumptions, defined as default arguments: You can change any of these components. They‚Äôre dependencies. Passed via initializer, not hardcoded. In need for in-memory event repository for faster tests? Check. Working on a DynamoDB storage backend ? Check. Not an ActiveRecord fan or integrating with ROM and Hanami? Check. Same could go for subscriptions ‚Äî i.e. persistent storage over in-memory from evaluating configuration file. Or to replace a broker (the in-process pub-sub bus). Perhaps you just need the mapper to encrypt the payload, before it is persisted and announced on the message bus. It is hard to please everyone with a single choice of configuration. But it seems easy to leave some room to diverge . After all each component can have its set of linters , a shared test suite to ensure its playing well with others. Little known fact is that RailsEventStore is a tiny wrapper on RubyEventStore. It is more of a configuration preset for RubyEventStore. An umbrella for bringing several gem dependencies together. But it also comes with several component implementations that are enabled by the presence Rails framework: You can opt-out from these framework integrations . Chances are you won‚Äôt ‚Äî you‚Äôve chosen Rails for a reason. \nBut in case you need it, compose your own event store from the ground up with RubyEventStore . Think of it this way: RailsEventStore is a specialization of RubyEventStore. Can there be HanamiEventStore, on top RubyEventStore? Sure! Catch me up on twitter to discuss more ‚Äî my DMs are open. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-04-27"},
{"website": "Arkency", "title": "Decorate your runner session like a pro", "author": ["Jakub Kosi≈Ñski"], "link": "https://blog.arkency.com/decorate-your-runner-session-like-a-pro/", "abstract": "Pawe≈Ç described some tricks you could use to tune-up your Rails console. \nI want to tell you about the runner method you can use to enhance your runner sessions. Decorating runner sessions is a little bit less convenient as we don‚Äôt have a module like Rails::ConsoleMethods that is included when runner session is started. So adding some methods available for runner scripts is not that easy.\nHowever you can still add some code that will be executed at the start and the end of your runner sessions. For example you can send some notifications so you don‚Äôt need to look at the terminal to check if the script evaluation has been finished. You can also measure time or set some Rails Event Store metadata \nto easily determine what has been changed during your session. Here is an example we are using in one of our projects to log the elapsed time, set some RES metadata and send Slack notifications. You can just add it to your config/application.rb . With such snippet, each time you run some script (or evaluate some inline Ruby code) with rails runner it will set the metadata for your RES instance and it will send a Slack notification\nat the beginning and the end of the runner session. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-04-28"},
{"website": "Arkency", "title": "When in doubt, signal!", "author": ["Jakub Kosi≈Ñski"], "link": "https://blog.arkency.com/when-in-doubt-signal/", "abstract": "When you are writing frontend applications you often need to fetch some data. Sometimes, especially when you‚Äôre working with a single-page app, some requests could be cancelled as you would not use the response anywhere, e.g. when your visitor clicks on some link and changes the page during the pending request that would no longer be needed on the next page.\nIf you are using fetch to perform AJAX requests, you can use AbortController to cancel pending requests in such situations. AbortController and AbortSignal are features available in most modern browsers , there are also some polyfills available for those unfortunates who need to support IE. Using AbortController to cancel fetch requests is easy. You just need to create a controller instance and pass it‚Äôs signal property to fetch . Then, when you need to abort a request, you just need to call the abort() method: Once you call controller.abort() , the signal passed to the fetch call will cancel the request and throw an AbortError . You can catch it using regular try {} catch (e) {} block: If you are using React in your application, you can write a simple hook that will encapsulate all logic related with AbortController for easier request handling: This hook could be used in your function component that is fetching some data. When such component is unmounted, all pending requests will be cancelled. You should remember to handle the AbortError in your state management so that you won‚Äôt update the state when your component is unmounted. An example component might look like this: With such implementation you‚Äôll fetch the data on initial render and call setProfile only if the request was successful. If you unmount the component, the request will be cancelled an AbortError will be catched and ignored (so you won‚Äôt try to update the state of an already unmounted component). As we are throwing all errors other than AbortError , you should wrap your component with an Error Boundary to prevent crashes in case of any other error that might occur during fetching the data. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-04-28"},
{"website": "Arkency", "title": "Fighting the primitive obsession with Value objects", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/fighting-the-primitive-obsession-with-value-objects/", "abstract": "My previous post on read models intended to address something different, but I decided to focus on read model part and leave the other topic for a different \none. There‚Äôs one thing which I dislike in the implementation. Using primitives to calculate the scores. It accumulates the score in scope of a given skill so we can count the average and so on. This example is simplified,\nas you may suspect, the original is more complex. How can it be done differently? By introducing Value object . Before diving into the code, we should establish the\ncorrect definition of it. I like characteristics of Value object which Eric Evans put in his \n‚ÄûDomain-Driven Design: Tackling the Complexity in the Heart of Software‚Äù book: Probably the most common example of Value object you‚Äôll meet is the Price or MonetaryValue which represents the\ncombo of BigDecimal and a String representing the currency. I‚Äôll do something different then. What we got here, we are able to compare two different AnswerScore by their values thanks to == , eql? and hash methods on our own: Same results will give us the .eql? operator since we alias it. Ok, you can compare two objects, what now? And there‚Äôs also an id, shouldn‚Äôt this be an Entity ? Nope, it shouldn‚Äôt,\nwe treat this id to distinguish scores of different skills. Adding two scores of two different skills wouldn‚Äôt make much\nsense, right? Imagine adding money in dollars and pounds sterling without distinguishing the currency. Let‚Äôs implement + operator on the object then. And there it is, we won‚Äôt be able to add anything wrong to our score: Works great, but returns BigDecimal and we want to add more AnswerScore object to each other to cleanup and simplify\nour projection: This won‚Äôt work, we don‚Äôt have a NullScore , we should implement it: It just returns first real Value object , after addition. Great starting point for our projection than hacking\ninternals of AnswerScore to provide that behaviour. Getting back to the AnswerScore . We need to return a Value object from our AnswerScore rather than raw BigDecimal value. Adding two scores is no longer a score, we should return ScoreSum , probably. How it rolls: What this gives us: Our projection won‚Äôt work now. Because current implementation in Rails Event Store framework doesn‚Äôt allow that. Initial implementation worked because we used the Hash to maintain our state and we were\nmutating on and on the same instance\nof itüò± Does the same, and even looks less magical, at least to me. And the NullScore is obsolete now, we do map ‚Äî reduce and there it is. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-05-01"},
{"website": "Arkency", "title": "Semantic blind spot in Ruby case statement", "author": ["Pawe≈Ç Pacana"], "link": "https://blog.arkency.com/semantic-blind-spot-in-ruby-case-statement/", "abstract": "Some time ago I‚Äôve stumbled upon an article on case statements in Ruby. The author presents there an example of case statement with ranges : The ranges actually read well. I‚Äôd even write similar case statement myself. And yet an avid mutant user may tell you there‚Äôs a ‚Äúflaw‚Äù hidden there. Can you spot it? Let‚Äôs pick one branch of that conditional for a closer look. Be it this one: Assume we also have 100% line coverage, reported by simplecov for that example. That‚Äôs rather easy to achieve: What would mutant report here, given that 100% line coverage? That drop of the coverage (as mutant sees it) can be attributed to conditions being shadowed by earlier branches. It doesn‚Äôt really matter if the lower-bound of the range in condition is a bit off. The test still pass. The mutant gem is a way to automatically detect such semantic gaps: An automated code review tool, with a side effect of producing semantic code coverage metrics. Think of mutant as an expert developer that simplifies your code while making sure that all tests pass. You can perform such code mutations in small scale without mutant ‚Äî manually. It‚Äôs a matter of changing the lower-bound in range condition and re-rerunning the tests. So what‚Äôs the semantically reduced case statement, that passes under mutant‚Äôs scrutiny? It appears to be this one: Would you call it a good middle ground? Is the case statement still useful in this form? You can find the code used in this post on my github . Happy mutation testing! Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-05-02"},
{"website": "Arkency", "title": "Limit your automatic retries", "author": ["Rafa≈Ç ≈Åasocha"], "link": "https://blog.arkency.com/limit-your-automatic-retries/", "abstract": "Recently I‚Äôve seen such code: As you can see, the build_state may raise an RubyEventStore::WrongExpectedVersion which, if raised, is always automatically retried. This error is raised if there are two concurrent writes to the same stream in RubyEventStore but only one of them is allowed to succeed.\nOne of them will successfully append its event to the end of the stream and the second one will fail.\nThat second request with automatic retry implemented (like in above snippet) will retry after failing, and most likely succeed this time. But what if it doesn‚Äôt? What if there is so much concurrent writes, that this request will fail over and over? What if there is a bug in the code which will always raise that error and we always retry? We have created an infinite loop, which deployed to production can bring our system down in seconds. :) At least for these reasons, the code should always be retried limited number of times (one should be enough ;) ). An example could be: The less, the better for your system resilience (thus I recommend only one retry at most ). Instead of automatically retrying, it‚Äôs better to let it fail the background job and retry it with exponential backoff (sidekiq has that built-in, other background job libraries may have too). The same applies to frontend, especially because user, tired of waiting, could already close the tab and we are still trying to prepare a response for him ‚Äì it‚Äôs better to fail the request and let the frontend decide whether to retry it again or not. By letting it fail we have faster requests, and therefore one less reason to have a production outage related to request queuing. Remember that if your worker continues working on this failed task, it is not working on other tasks. And some failure reasons (like network error on third parties) have a high chance of happening again. All of the above applies also to integration with third parties. Even if HTTP request has failed because of a transient failure (like a timeout), it has a high chance of happening again if retried just after failing. Maybe your third party is temporarily overloaded, maybe they are just in the middle of the deployment, maybe they have a temporary bug on which their team is already working. Let it fail, and retry later. Why there were retries in the first place? Often they are added because the code failed once, the error came up to error reporting software and someone decided that if it has failed, then we need to retry it again. Noone likes to get a notification for an error about which you can‚Äôt do anything, therefore it‚Äôs best to just ignore these transient errors. Instead of reporting them, it‚Äôs better to add a metric to your monitoring system each time your request succeed or fail and add an alert threshold when it fails too often. This is possible in all open source monitoring systems (like Grafana+InfluxDB) or proprietary ones (like NewRelic). Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-05-19"},
{"website": "Arkency", "title": "How to build a read model with Rails Event Store Projection", "author": ["Szymon Fiedler"], "link": "https://blog.arkency.com/how-to-build-a-read-model-with-rails-event-store-projection/", "abstract": "Recently I faced interesting challenge in one of our customer‚Äôs application. Imagine that you take a test after which\nyou get a personalised reports about your skills level. Existing mechanism for that was time and resource consuming.\nPeople had to wait for e-mail delivery with PDF-generated report several hours due to several constraints, which I would\nprefer not to dive into. The solution was obvious ‚Äî lets progressively build read model every time someone answers the question. After the test\nis done, the report will be available instantly in a web ui. Nothing fancy, a typical domain event powered by Rails Event Store , with a schema\ndefined, keeping identifiers of involved entities and score calculated by the domain service which publishes the event\nabove when its job is done. Next building block is the asynchronous handler. Why asynchronous? Not to waste time on participant‚Äôs request‚Äîresponse\ncycle and lower their satisfaction from using our application: What happens here: Vaughn Vernon in his ‚ÄûImplementing Domain-Driven Design‚Äù book describes read model this way: Denormalization is not a popular technique in the Rails world. What it gives? Complex, often many queries replaced with\nsimple lookup for a single record which contains all the data to be displayed in a pre‚Äîformatted manner. Please, have a look at the read model implementation: It‚Äôs mostly obvious . One might think that, there‚Äôs already with_lock or simply lock! method in ActiveRecord . Yes,\nit is. However, it won‚Äôt work for the not‚Äìyet‚Äìexisting records because it uses lock for update and on first write there‚Äôs\nno update operation, but create. So, in many cases ActiveRecord::RecordNotUnique errors would appear if two or more\nconcurrent threads would try to insert the row for the first time. Thanks to pg_advisory_xact_lock ( key bigint ) ‚Üí void we can obtain an exclusive transaction-level advisory lock, waiting if necessary . Yet another reason to use PostgreSQL . What if another field is required or there was a bug in the calculations? Not a problem, read models can be thrown out\nand rebuild with ease, because all the history behind them is known ‚Äî thanks to domain events. Btw. You might be also interested in other posts on read models on our\nblog. Now, a plug üîå. Join ARKADEMY.DEV and get access to our best courses: Rails Architect Masterclass, Anti-IF course, Blogging for busy programmers, Async Remote, TDD video class, Domain-Driven Rails video course and growing!", "date": "2021-05-01"}
]