[
{"website": "Octo", "title": "\n                Design your Silverlight application for TDD !            ", "author": ["Nicolas Raynaud"], "link": "https://blog.octo.com/en/design-your-silverlight-application-for-tdd/", "abstract": "Design your Silverlight application for TDD ! Publication date 09/06/2011 by Nicolas Raynaud Tweet Share 0 +1 LinkedIn 0 There  is one major flaw you must have encountered whilst working with  Silverlight for a while: its incompatibility with unit tests! Unless  you’ve been using Silverlight solely for drawing circles and rectangles,  surely you must have tried to use the Silverlight Toolkit unit test project template to try to  put some TDD in your project ! …most probably you were disappointed by  it because it’s fairly poor… This project is specific to Silverlight (the Microsoft test assembly is specific) Running  tests is only possible by launching the test application wrapping up  your test classes. And let’s be frank, the usability of this interface  is quite poor. There is no integration in Visual Studio. It’s impossible to properly analyze the tests results and metrics or even to navigate through tests and code like you’d want to. It’s  also impossible to run a specific test or test class without having to  pollute your tests with additional tags. And then you will still have to  run the application to see it play the selected tests. It’s  almost impossible (or fairly complicated) to integrate those tests in  your continuous integration build process. Statlight (an open source project) is trying to  provide a way out but this remains highly dependent on your  configuration, and frankly, i never had it working on a TFS build. (this  can be explained by the fact that it still needs to open a window to  run your tests while your build process is highly relying on commands  only) All  those issues can find an explanation in the fact that the Silverlight  framework is a subset of the .NET one making them incompatible with each  other. However,  even though these frameworks are different, they’re also very similar.  Thanks to this, we will be able to go around the limitation stated above  and set up a solution to properly unit test our Silverlight libraries. This  solution relies on two parts: structure of the Silverlight projects and  then unit testing them within the visual studio framework. First, architecture your projects One  of the major secrets of a long running application resides in its  architecture. This goes the same for the GUI (Graphical User Interface).  We’re often tempted to consider the GUI as a whole monolithic bloc: “GUI is easy, not complex enough to structure it” “anyway, if I decide to change my user interface, I’ll probably change the whole layer, so it’s ok to mix everything here” Good  practices don’t stop at the service layer ! A good structured GUI will  allow you to optimize its testability while abstracting models from  views. I’m not here to remind you of the MVVM pattern ,  the idea here is to go one step further by physically abstracting the  different parts to leverage flexibility. I’ll  be taking the example of an application exposing web services to a  Silverlight light client. (It could as well be a Silverlight only  application, its structure would remain the same). Silverlight projects architecture The above architecture is composed of 3 RIA projects and 1 Web project: Octo.Bank.Web.Ria.Views This  library holds (like you could guess) the application views, either  controls or pages. This will help us isolate the GUI behaviors (you  know, the dirty fixes you do to please the user). All references to  converters or ViewModels will be handled through Binding to  “StaticResources”. Octo.Bank.Web.Ria.ViewModels Here,  we’ll isolate the GUI intelligence (or even business rules) using  ViewModels, or also converters for example. We’ll also add the service  references to this project (proxy generation) because this is what our  models will use to manage data. If you could test only one project, this  is THE library you’d want to test ! Octo.Bank.Web.Ria.Main This  is our main application, and actually the only “Silverlight  Application” project. It references the other libraries and will use  them through application resources (App.xaml). To be able to run the  application without issues, the trick here is to add the ServiceReferences.ClientConfig  of the ViewModels project as a link in order to have the service  information at runtime. Without this, your app will start and crash at  the first service client instantiation, the service references not being  present in the application. Octo.Bank.Web The  Web project is exposing our web services (of course tested as well :)).  It also exposes one xap file : the main Silverlight project (which will  contain the two other libraries). Then, unit test Silverlight in Visual Studio Now  that we structured our application, we can clearly identify the  critical library that needs testing: ViewModels. Because this is a  sensible library, we want to take advantage of the Microsoft unit test  framework of VS 2010, for all the reasons I mentioned earlier. Here  follows, imho, one of the most clean way to do so: 1. Create a standard .NET unit test project 2. Remove all the dll references except  ~QualityTools.UnitTestFramework reference. 3. Add the following references using the browser to get them from the  Silverlight SDK folder you’re using (ex.: C:\\Program Files  (x86)\\Microsoft Silverlight\\5.0.60401.0) system.dll System.Core.dll System.Windows.dll System.ServiceModel.dll and activate the library local copy, this will force the test framework into using them instead of the default .NET ones. 4. Add your favorite tools libraries (RhinoMocks.. Moq…) in their Silverlight version. 5. Add the Silverlight libraries you want to test, here Octo.Bank.Web.Ria.ViewModels Unit test project local copy of DLLs Congrats, You’ve just made this unit test library running with the VS 2010 Unit Test framework under a Silverlight runtime! Continuous Integration & Code Coverage You  are nearly there, almost ready to start developing (yes, because you’re  using TDD remember ? :) ! A couple of things left to do to make sure  you maximize the use of the framework: 1. Add your test library to your CI build Using TFS up to 2008 <TestContainer Include=”$(OutDir)\\%2aTests.dll” /> 2. Activate code coverage metrics for your tested DLL Using VS2010, open local.testsettings, go to Data & Diagnostics, check the Code Coverage option and via Configuration, select your Silverlight ~Views assembly. In conclusion We  know how much a testing framework impacts the development life-cycle. A  bad productivity tool (like the SL Toolkit way of testing) and your  whole TDD cycle (red, green, red, green…) is jammed ! This is why,  even if this solution has an additional cost (manually referencing  libraries), I find it an acceptable one when I compare it to the shift  in efficiency I get in my every day job. And this is without mentioning  the comfort of having the CI build run the tests and the code coverage  metrics … Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Project support and tagged Best Practices , design pattern , development , Silverlight , TDD , Testability , tips&tricks . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 8 commentaires sur “Design your Silverlight application for TDD !” Adrian Bolboaca 13/06/2011 à 12:02 Your solution is quite correct, but I would add the fact that you can also test view specific pages with the silverlight toolkit, and run the tests in browser. It's really the only way I found to automatically test the relation between the views and the view models. When you test a view the system obliges you to do a lot of thinks so you could expose the needed view information for testing.\r\n\r\nAlso I am very curious how you test without Silverlight Toolkit the asynchronous calls from the view models to the domain service.\r\n\r\nI did not see much about testing the DomainService, that has its particularities because you need to insert a layer so you could remove the high coupling between the DomainService and Entity Framework. This can be especially tough. Nicolas Raynaud 14/06/2011 à 10:28 Hi Adrian,\r\n\r\nYou're right, the Silverlight Toolkit can still be used to test View related scenarios. What I wanted to show here is a way of testing SL libraries with our most efficient tools for that (VS testing framework).\r\n\r\nAbout the asynchronous service calls from the ViewModels, what I do is use the generated interface as the type of my service client in the viewmodel. With this I can mock the service calls in my unit tests. The services can then be injected at runtime in a different manner than at testing time. This could be another article !\r\n\r\nUsing this interfaces really helps decoupling your ViewModels from the rest of the application and especially how your service proxies are implemented.\r\n\r\nHope this helps ! gorgoroth 14/06/2011 à 11:01 Ce n'est pas mieux ou pire que Selenium\r\nEntre la peste et le choléra on a toujours le choix\r\nc'est ça la démocrachie Sofian Djamaa 14/06/2011 à 11:28 @gorgoroth & @Adrian: I think there's a misunderstanding with the aim of this article. This article is not introducing a general way to test the application (Silverlight Toolkit does it already).\r\n\r\nWhat Nicolas highlighted is the ability to unit test Silverlight code (ViewModel that embeds Silverlight framework) and integrate those test to a CI build (without a task defined), a local environment (using Run All Tests feature) and having code metrics (code coverage, cyclomatic complexity and so on...). Silverlight Toolkit doesn't provide such features at the moment.\r\n\r\nI wouldn't compare this approach to Selenium tests but to MSTest tests. It's like comparing Selenium to JUnit :) Kai G. 11/07/2011 à 15:39 Hello Nicolas,\r\n\r\nI just tried out your simple tutorial and have got a problem when I try to add the reference:\r\n •system.dll\r\n •System.Core.dll\r\n •System.Windows.dll\r\n •System.ServiceModel.dll\r\nthe project always replaces the files, with those from the .net framework. Even when I select to use a local copy, the file get replaced within the bin folder when I build the project. When I try to add System.Core VS says this library is already referenced, even when I just deleted the library. Any idea?\r\n\r\nBest regards from germany\r\nKai G. Nicolas Raynaud 11/07/2011 à 15:53 Hi Kai,\r\n\r\nHave you tried editing your unit tests .csproj file and setting the path of the libraries yourself?\r\n\r\nHopefully, it should do the trick!\r\n\r\nRegards,\r\n\r\nNicolas. Kai G. 12/07/2011 à 13:01 Hello Nicoals,\r\n\r\nthank you for the very fast response. Your hint worked quite good, I could add all librarys except one: System.Core\r\n\r\nWhen I want to add it, VS says:\r\n\r\n\"System.Core, Version=2.0.5.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e\" is incompatible with .NET Framework 4\r\n\r\nIn order to add it you should to change the project's target to a compatible framework first.\r\n\r\nYou know how to solve this issue?\r\n\r\nThank ou very much.\r\n\r\nKai G. oyewole shina 23/05/2012 à 10:39 please i am student, and i want to know more on website structure, and architecture Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-06-09"},
{"website": "Octo", "title": "\n                Octo @ ScalaDays            ", "author": ["Rémy Christophe Schermesser"], "link": "https://blog.octo.com/en/octo-scaladays-en/", "abstract": "Octo @ ScalaDays Publication date 18/05/2011 by Rémy Christophe Schermesser Tweet Share 0 +1 LinkedIn 0 Octo will be attending the ScalaDays this year ! The ScalaDays is the main conference of the Scala language. Scala is a multi-paradigm programming language that runs on the JVM ( wikipedia ). On this occasion, Rémy-Christophe Schermesser will present a lecture on the comparison between the Ruby and Scala. A summary of the conference is available here . Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged Ruby , Scala , ScalaDays . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-05-18"},
{"website": "Octo", "title": "\n                Not “yet another SOA blog post”            ", "author": ["Matthias Feraga"], "link": "https://blog.octo.com/en/not-yet-another-soa-blog-post/", "abstract": "Not “yet another SOA blog post” Publication date 01/05/2012 by Matthias Feraga Tweet Share 0 +1 LinkedIn 0 I had a job opportunity recently to work as an SOA architect , on SOA projects , in the SOA team of a consulting company. As many companies still make it their marketing, at OCTO we get annoyed with the negative effect of SOA term usage in lots of situations. Such situations include confusing marketing purposes , global IT strategy of a company , or SOA compliant architectures of many kinds. These approximations tend to hide weaknesses behind this term. We rather prefer to discuss on concrete solutions with respect to customer needs, and SOA is all but concrete with its abstractions. Most discussions about SOA are sterile, because nobody is giving the same meaning to this acronym. Talking about it generates more confusion and misunderstanding than bringing real life problems to light. The goal of this article is not to propose one more SOA definition, but to analyze recurrent perceptions of what SOA could be . These perceptions are indifferently observed at various actors , as IT end users, software vendors and consulting companies. Do not feel offended if you recognize yourself in one of the given cases. We all forge our perception of what SOA really means, as a mix of all these points of view, and more. SOA as a prerequisite Gartner: “ To go to cloud, […] you cannot bypass services. You have to build on SOA strategy. “ We often hear that SOA is a prerequisite for other architectural paradigms such as Complex Event Processing, mobility or Cloud Computing. Nevertheless many business units buy a SaaS solution themselves: this represents the major part of SaaS purchases. These business units reasonably can’t have an SOA strategy as they bypass the IT department . Moreover it is really easy to expose a service (i.e. a unit action based on a single record: a command, an employee, etc.) from a cloud solution. On the contrary, it is not as easy to consume services from it: As any software packages, SaaS solutions are rather passive against other parts of the Information System. To consume an external service, we would have to plug on a hook (or make a trigger to fire) at the right place (a transaction, an update, etc.), which is not trivial with software packages. Even if you can plug in at the right place, not all software packages offer a web services toolbox to handle serialization, headers, security, etc. PaaS and IaaS solutions permit to consume remote services without restrictions. But the problem is somewhere else. It is technically hard to expose on premise services to the cloud , because of difficulties associated with IPV4 NAT , firewalls and other security issues. Paradoxically we want to open systems that have been willingly built as fortresses. The key to the Cloud is not an SOA strategy; it is Tunneling (i.e. network tunnels that remain open between on premise and cloud systems). Tunneling techniques differ between cloud providers, but the idea is the same. Of course having cross-applications exchanges based on a unit logic is a good enabler for cloud integration. But in most cases, services providers are hosted on the cloud, and services consumers are on-premise applications . As a result on premise service-orientation is not a prerequisite to take advantage of cloud computing. The everything-is-a-service syndrome Another perception of SOA is to consider everything as a service in any architecture layer: business services, logical services, IS services, technical services, web services, infrastructure services. Improper use of the word service is all the most disturbing, that it is already used to define many aspects of IT: IT services, IT transformation services, outsourcing services, consulting services, etc. Many software vendors and consulting companies define their SOA reference architecture similar to the following: Some software vendors make everything a service, and give the illusion to reduce the complexity with more abstraction . With their so-called SOA infrastructures , they largely participate in this confusion : Concepts that have been existing for a long time are simply re-branded , including ESBs internal components The extension of ESB – outside of its role of bus – with services containers capabilities is very confusing. After years these containers are marginally used. The business logic is still in application servers. On the contrary, instead of having ESB embedding applications, the trend is for applications to embed their ESB… Inside of the ESB, what was an integration flow is now often referred to as service that contains integration logic. The attempt of defining new SOA patterns is no more than a revamping of well known integration patterns , with … more complexity. SOA as a set of technologies only standardized the transportation layer (SOAP, HTTP, security on it, etc.). Although this is crucial, transportation is just one of the features provided by an integration middleware (routing, transformation, orchestration, etc.). Even in a world of well defined services, we would still need a middleware in many cases . The one-size-fits-all architecture solution Some says SOA is an architectural paradigm. Some even says SOA is a philosophy. SOA is often perceived as a ultimate architecture methodology that embraces all aspects of the enterprise . What was IT governance is now SOA governance , but the purpose is the same: manage lifecycles to align IT and business sustainably. Many organizations, especially consulting companies, have renamed thousands of architects as SOA architects . Does it mean such people are not able to design a batch anymore? Probably not. Architects have always been architects regardless of IT trends. The same goes with the organization: a transverse SOA team is unsuitable, as everything is not simply a well-grained service. Counter-examples are : a batch, a software package, a mainframe, an appliance, etc. Most architectures defined as SOA are based on web services, that are synchronous by definition. But the more dependencies on external web services your application has, the less available, robust and testable it is. Remember to use asynchronous messaging as possible. SOA can’t be a finality: we have to compose Information Systems with heterogeneous technologies and various paradigms. Use the right solution as the right time to get the best of each architecture. The mythical reuse rate A great promise of SOA definitions is to reduce IT costs by making more reuse of business services. In real life, there is a big difference in reuse regarding the nature of the service : Services that are largely reused by nature . This is often the case for referential services as products, customers, providers, etc. We also find some highly reusable business services as order management, stock management, accounting transaction, etc. I have seen such services reused more than ten times. Services that are so specific that chances of reuse are very low: core-business services (e.g. digital right management), B2B and partners interactions (e.g. market data, carrier), domain-specific services (e.g. recruitment), public services (e.g. specific law), etc. The myth around services reuse lead to various problems, in which: Making any service reusable prior to its effective reuse is over-engineering . In order to make reusable services, we have to create usable services first . Who has never faced such unusable services? tens of input parameters, complex data structures, performance bottleneck, insufficient availability, etc. are common characteristics of an unusable service) Inappropriate reuse is worst than no reuse at all: it creates a bad-coupling . A well-known anti-pattern is to reuse many services from within a batch… Trying to extend a service beyond its role results in a bad granularity Do not reuse services if it does not appear natural in the first place. If you spend to much time finding out how to reuse a service, it will probably end badly. An alternative to reuse services at all cost is to reuse patterns : a same way to do, but different implementations and different deployments. The top-down holism A common vision of SOA starts with higher level business processes. These processes rely on business services, themselves implemented through IT components. And finally IT components are hosted on technical infrastructures. All these concepts are considered commutable boxes. In this holistic approach, run-time operational constraints are not given enough weight : constraints that make however foundations of our Information Systems (complexity, productivity, maintainability, performance, etc.) Many speeches pretend that as a global top-down approach, SOA tied with BPM promises to reach business agility. This is not so simple: On one hand, we have predictable processes with a relatively slow variation (e.g. HR and supply chain, when not core-business). IT organizational agility is sufficient enough here: changes can fit in normal application lifecycles, without any particular abstraction or decoupling mechanism. On the other hand, we have rather unstructured processes. Neither SOA nor BPM brings an out-of-the-box solution for that kind of processes that relies on knowledge workers (e.g. CRM, risk management, personal services, healthcare, etc.). These processes can be tooled up with new-generation Case Management, collaboration workflows, or can be executed manually. The most important is that the end-user keeps control on what happens . I don’t think that Enterprise wide – long running – business processes are fast changing. On the contrary, the more operational a process is, the faster he can change. If SOA is fostered to change a process more easily in – let’s say – five years, this is over-engineering. The entire paradox is here: bringing more complexity to reduce complexity . Oracle says to start the SOA revolution with a corporate-wide center of excellence, when IBM says to start with infrastructure foundations. Both approaches lead to even more IT centralization , and are certainly not bringing agility. SOA initiatives often hide a lack of agility , by using technological abstraction as an organizational workaround. Are you activating the right lever to gain agility ? Service-Orientation as a software design paradigm Forrester: “ SOA refers to a software design approach where systems are composed of various and sometimes shared services. The point is better interoperability and the ability to save time and money through the reuse of code. ” Building application as a heap of services is a theoretical vision. Everything is not so isolated in applications. Totally separating parts of a same application as silos leads to maintenance difficulties, development overhead and performance issues. Gartner: “ By 2015, SOA will be used in more than 80% of applications “. Actually it doesn’t matter to have SOA in applications . We already have Object-Orientation as an effective software design paradigm. What could make the difference is having Service-Orientation between / around application, as an integration paradigm . Geeks apart, who really cares about SAP or Salesforce.com internal architectures? What matters is what services they expose, not what services they are built with. That’s why systems are generally not built as a heap of services. An interesting exception is to resort to SOA for denormalization and performance reasons (LinkedIn architecture for example). Each service is finally an application , with a certain amount of processing and its own storage system . LinkedIn chose to brand its particular architecture SOA , and that’s another perception of SOA again. This architecture orientation brings a lot of complexity and difficulties regarding: storage, testing, errors management, consistence, transaction management, availability, etc. As it addresses a major performance requirement , this choice of denormalization should not be made a priori. Such an architecture style is particularly relevant when an organization wants to foster independent feature teams . The SOAP-only shortcut For many people, SOA and SOAP/REST are equivalent. This shortcut especially concerns some developers, as SOAP is more tangible as SOA. They are no ambiguity around SOAP and this term permits concrete technical debates. However there is often confusions between various combinations of transportation protocols (HTTP, JMS, pure TCP…) and representation formats (SOAP, JSON, pure XML…). UDDI failed to standardize services registry around the single SOAP protocol. It illustrates that there will never be a single protocol that unifies systems. At the moment something is normalized, there are growing initiatives to challenge it. We can name it a message, an event, a request-response, an API call or a service call : it remains a flow between systems. That’s why components can be exposed and consumed through technologies such as RPC, messaging protocol (e.g. pure JMS, AMQP), proprietary protocol (e.g. BAPI/iDoc, SWIFT), streaming API, all with their pros and cons. The most important is that exchanges are not necessarily synchronous . What are we talking about exactly? I didn’t say that these perceptions are wrong as it would assume there is an SOA reality. My own vision of SOA has been a mix of some of these perceptions. Until I realized SOA has too many different meanings according to one’s pains and interests. As a result of all these different opinions, I give no particular meaning to the term SOA. Actually, I am more and more convinced that there is no way to align on a consensual definition of SOA . The SOA subject seems a little outdated, but believe me, it’s here and crawling everywhere . Standardization organizations and other IT consortiums are still working hard on writing down SOA reference documents (see recent examples of OASIS and Open Group ). Many software vendors and consulting companies are also pushing it intensely. When dealing with a new consulting position, I carefully avoid the direct debate about SOA. Each time, I have to make an effort to understand what SOA really means in context. Projects often involve people from different companies, and different cultures as well. However nothing should be implicit about SOA and architecture. My last recommendation is to drill-down into the debate as quickly as possible: use precise words, talk about precise notions as integration patterns , component granularity, errors management, deployment typologies, etc. Can’t we turn the page once for all? Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles and tagged ESB , SOA . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-05-01"},
{"website": "Octo", "title": "\n                What’s new in Apache Cassandra 0.7+            ", "author": ["Jordan Pittier"], "link": "https://blog.octo.com/en/whats-new-in-apache-cassandra-0-7/", "abstract": "What’s new in Apache Cassandra 0.7+ Publication date 17/05/2011 by Jordan Pittier Tweet Share 0 +1 LinkedIn 0 It’s been a while since we last blogged about Apache Cassandra. Let’s catch up with the new features available from version 0.7+. Online schemas updates Before version 0.7 Keyspaces and ColumnFamilies had to be described in the configuration file of Cassandra. Adding/removing/updating a Keyspace or a ColumnFamily required a rolling cluster update. This is no longer necessary with Cassandra 0.7 thanks to new methods added to the API. Schemas can now be changed without restarting the cluster. Here is a small example with the CLI : 11:55 root@sd-28364 ~ # cassandra-cli -h localhost\r\nConnected to: \"Test Cluster\" on localhost/9160\r\nWelcome to cassandra CLI.\r\n[default@unknown] create keyspace myTestKS with replication_factor = 1;\r\n[default@unknown] use myTestKS;\r\n[default@myTestKS] create column family myTestCF with comparator = UTF8Type and rows_cached = 100; Secondary indexes Secondary indexes are indexes on columns value. It’s now possible to fetch rows, not only based on their row key, but also by the value of their indexed columns. Prior to version 0.7 “inverted indexes” had to be maintened manually, which was cumbersome and “non-atomic”. This feature eases the writting of advanced queries. Here is an example based on the “myTestCF” columns familly set in the previous CLI session : [default@myTestKS] update column family myTestCF with column_metadata=[{column_name: myIntColumn, validation_class: LongType, index_type: KEYS}];\r\n[default@myTestKS] set myTestCF['key1']['myIntColumn'] = 2;\r\n[default@myTestKS] get myTestCF where myIntColumn = 2;\r\n-------------------\r\nRowKey: key1\r\n=> (column=myIntColumn, value=2, timestamp=1305288027744000)\r\n1 Row Returned. For more details about Secondary Indexes, see the Apache Cassandra Wiki and this good blog post by Jonathan Ellis . Other Changes There is a lot of other changes, the exhaustive changelog is worth reading. Among the changes : Support of expiring columns. Columns can be inserted along with a Time To Live. The column will automatically be marked for deletion after the TTL has expired. Numerous performance optimization including read repair and more efficient use of row cache. Some API changes. It is now considered best practice to use third party libraries such as Hector (in Java) or Pycassa (in Python) to query Cassandra instead of using “low level” Thrift objects. All in all, on a personal note, Cassandra seems production ready for “the rest of us”. But wait, there is more to come with the promising 0.8 branch scheduled in a couple of days : CQL Cassandra Query Language is a basic SQL-like language for data management in Cassandra. It will be possible to write queries like these : SELECT column1,column2 FROM myTestCF using CONSISTENCY.QUORUM where KEY > \"aaa\" AND \"column1\" = 5\r\nUPDATE myTestCF SET column1 = value1, column2 = value2 WHERE KEY=\"aaa\"\r\nDELETE column1 FROM myTestCF using CONSISTENCY.ALL WHERE KEY IN (\"aaa\", \"aab\"); Distributed Counters Those are columns of type “Long” that can be incremented or decremented. Yeah, that’s right, they are just counting, but they are doing it fast. Counters have long been awaited in Cassandra and they seem to fit ideally “realtime analytics” . See this presentation by Twitter on their use of counters. Performance enhancement of Compaction Starting with version 0.8+ sstables compactions are multithreaded. Compactions should complete faster. Several compactions are done in parallel which should bound the proliferation of new sstables while a previous long compaction is taking place. This should be really helpful for write-heavy workload. Datastax’s product Recently, Datastax (commercial leader in Apache Cassandra) have announced two new products. First is OpsCenter (free for non-production use only), an administration console for managing, monitoring and operating Cassandra and Hadoop cluster. Then is Brisk, an open-source Apache Hadoop and Hive distribution that utilizes Apache Cassandra for many of its core services. Looking forward to it ! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles and tagged Apache , Cassandra , NoSQL . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-05-17"},
{"website": "Octo", "title": "\n                IT : commodity or asset?            ", "author": ["Julien Cabot"], "link": "https://blog.octo.com/en/it-commodity-or-asset/", "abstract": "IT : commodity or asset? Publication date 08/05/2011 by Julien Cabot Tweet Share 0 +1 LinkedIn 0 The Information system is a corporate asset, providing a competitive advantage, seen as a critical commodity. What matters with Information system? Is it an asset or a commodity? IT as an asset A cross-business lines customer database gives a competitive advantage in cross-selling operations. A exotic trading tools brings to the  bank new hedging opportunities. Many examples illustrate the business advantages of the information systems. IT as a commodity For many business, information management and processing become vital, like electricity, capital, etc.  The Information system has to be robust and cheap. For example, back office banking system have to be cheapest as possible to allow low operating costs. Many examples illustrate the advantages of a low cost information system. Actually, The IT is both an asset AND a commodity ; each application of the Information System may be either a commodity or an asset. The principle of the pyramid How to use the pyramid? While you are defining your IS strategy or benchmarking your IT, remind of the pyramid of applications to be sure to have the right vision and the right key drivers. Un version française de cet article est disponible ici Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-05-08"},
{"website": "Octo", "title": "\n                Inbound Marketing: Get FREE traffic on your website!            ", "author": ["Marion Duzac"], "link": "https://blog.octo.com/en/inbound-marketing-get-free-traffic-on-your-website/", "abstract": "Inbound Marketing: Get FREE traffic on your website! Publication date 05/05/2011 by Marion Duzac Tweet Share 0 +1 LinkedIn 0 A revolution is occurring in the Marketing’s world! Today consumers are in control. Inbound Marketing is beating up the traditional Marketing methods aka Outbound Marketing. Join the movement! Outbound Marketing: Game over? About ten years ago, Marketers mainly used the following methods: telemarketing, e-mail blasting and advertising . Those Outbound Marketing techniques – successful at that time – consisted in pushing a message out . What do they have in common? They are all interruption-based techniques . Worse so: they are intrusive. Therefore consumers found ways to block them. To do so many tools were developed: spam filters, Caller ID or Do Not Call Registry and so on. Advertising can now be avoided when watching a movie or listening to music thanks to the emergence of TiVo/DVRs, video content online, mp3 players… On the contrary Inbound Marketing represents a set of actions to attract consumers to your website using free traffic sources . So, is Outbound Marketing dead? Not yet. In Europe, marketers still love sending out direct e-mails. 86% continue to use it or plan to do so in 2011. But the use of Outbound Marketing is surely going to decrease in the near future and will progressively be replaced by Inbound Marketing for obvious reasons: – Efficiency : consumers can now stop those interruptions. – Costs : leads generated by Inbound Marketing cost 62% less than those generated by Outbound Marketing. – Trend : where can marketers meet consumers? Inbound Marketing answers this question: Fish where the fish are . Let the consumers find you… But be sure to be at the right places! OK before going any further, I have to admit that the title of this article is a bit of an exaggeration. The traffic sources used by Inbound Marketing are free but creating and managing them requires work and thus human resources. What is true though is that Outbound Marketing techniques mean recurring costs (each new campaign has to be funded) while Inbound Marketing enables to start a virtuous circle where traffic and lead generation costs are paid off on the long run. Inbound Marketing… Action! If you fancy transforming your website into a Marketing Hub, start by positioning yourself at the strategic spots. Where can you find your consumers? Three magic places: – blogosphere – search engines (not to say « Google ») – social media To summarize, the purpose of Inbound Marketing is to be found on those three spheres. It enables you to attract Internet users, to maximize inbound links and thus get a better page rank. What is the secret recipe? Create rich, valuable, infectious content. Step 1: create a blog . You already have one? Sure you do. But do you publish anything else than articles? Written by your staff? Do you have videos, podcasts, comics or any other form of content? Talking about content… Do you often publish experts’ interviews? Consumers’ feedback? Diversity is good! Step 2: improve your SEO and do not only focus on SEM . Spend some time on improving your website general organization, page titles (keywords at the beginning, not at the end), page content, URL names and so on. Google is fighting Black Hat SEO (Fear the Panda !) and Inbound Marketing is going in the same direction by emphasizing quality content and legitimacy. Step 3: adapt your content to the right social media . For example, create a fun Facebook page, a LinkedIn Group about an important stake for your business, answer questions on Quora, try to get a first position on Digg or StumbleUpon. Step 4: create a network . This can sound pretty lame but many of us do not spend enough time and energy doing this right. You can start off by spotting the most influent blogs in your industry. Then interact with them by commenting their articles and when it is appropriate mention your own blog: “We also had a passionate debate on this topic, see this article”. Being active on influent blogs will help you gain both visibility and authority. This is good as Google ranks you based on relevance and authority. Some figures to conclude In France, Inbound Marketing is not yet a really famous topic but in the US a book has already been written about it: Inbound Marketing . The authors’ blog is full of rich content – of course – and of interesting figures. Here are some convincing ones : – 57% of businesses have acquired a customer through their company blog. – 41% of B2B companies and 67% of B2C companies have acquired a customer through Facebook. – The number of marketers who say Facebook is “critical” or “important” to their business has increased 83% in just 2 years. – Companies that blog get 55% more web traffic. The more you blog, the more pages Google has to index, and the more inbound links you’re likely to have. The more pages and inbound links you have, the higher you rank on search engines like Google—thus the greater amount of traffic to your website. Which is why we repeat: Blogging is good. Those figures reveal, among other things, that Inbound Marketing enables to increase traffic but more so: to acquire consumers. To be honest… That’s the point. Using efficient call to action buttons can increase your conversion rate from 0.5% with a lame one (like “Contact us” without a form to fill in) to 5% with an efficient one like a webinar, a white paper or a free trial. (Those figures come from Inbound Marketing, Chapter 8) . Like for all Marketing actions Inbound Marketing’s success will be determined using analytics. Know the famous quote from Edwards Deming? In God we trust; all others must bring data. The final step then is to choose the right metrics, analyze and… be delighted with the results! Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged Blog , Google , Inbound Marketing , leads , Marketing , Outbound Marketing , SEM , SEO , social media , traffic . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-05-05"},
{"website": "Octo", "title": "\n                PowerShell v2 – My Best Practices            ", "author": ["Maxence Modelin"], "link": "https://blog.octo.com/en/powershell-v2-my-best-practices/", "abstract": "PowerShell v2 – My Best Practices Publication date 10/05/2011 by Maxence Modelin Tweet Share 0 +1 LinkedIn 0 After working more than a year on different projects using PowerShell (v1 and v2), I would like to share with you some Best practices that I could identify. I decided to write this article when I realized that I was always using the same tips/tricks and asking people working with PowerShell to use them as well. Some of the tips that I’ll give may seem stupid and/or quite common (not to say mandatory), in the development world… But scripting is not really part of the “development world” and I realized that it’s much easier to write dirty codes. Sorry for the people working with PowerShell v1 for 2 reasons: first, they (sadly) still work with v1 :-) and second they won’t be able to apply all the tips from this article… Last point: as development tool, I use PowerGui . So let’s start! Always use a logger on your functions Feed back: A few weeks ago, I wrote a short “one shot script”… For the first time I decided not to include any logging function (I was, maybe, a bit too confident…). The punishment came straight away: after the first deployment I was not able to know what didn’t work properly and it took me awhile to understand why. The point: So even if you know it, force yourself to do it. Logging is not an option. Example: The first time that I wrote a logger in PowerShell, it took me a long time. But since then I never stopped to improve it and even added options. I copy hereafter, a light version of it (using the log4net library), just to give you an overview. function New-Logger\r\n{\r\n<#\r\n.SYNOPSIS\r\n      This function creates a log4net logger instance already configured\r\n.OUTPUTS\r\n      The log4net logger instance ready to be used\r\n#>\r\n     [CmdletBinding()]\r\n     Param\r\n     (\r\n          [string]\r\n          # Path of the configuration file of log4net\r\n          $Configuration,\r\n          [Alias(\"Dll\")]\r\n          [string]\r\n          # Log4net dll path\r\n          $log4netDllPath\r\n     )\r\n\r\n     Write-Verbose \"[New-Logger] Logger initialization\"\r\n\r\n     $log4netDllPath = Resolve-Path $log4netDllPath -ErrorAction SilentlyContinue -ErrorVariable Err\r\n     if ($Err)\r\n     {\r\n          throw \"Log4net library cannot be found on the path $log4netDllPath\"\r\n     }\r\n     else\r\n     {\r\n          Write-Verbose \"[New-Logger] Log4net dll path is : '$log4netDllPath'\"\r\n          [void][Reflection.Assembly]::LoadFrom($log4netDllPath) | Out-Null\r\n\r\n          # Log4net configuration loading\r\n          $log4netConfigFilePath = Resolve-Path $Configuration -ErrorAction SilentlyContinue -ErrorVariable Err\r\n          if ($Err)\r\n          {\r\n               throw \"Log4Net configuration file $Configuration cannot be found\"\r\n          }\r\n          else\r\n          {\r\n               Write-Verbose \"[New-Logger] Log4net configuration file is '$log4netConfigFilePath' \"\r\n               $FileInfo = New-Object System.IO.FileInfo($log4netConfigFilePath)\r\n\r\n               [log4net.Config.XmlConfigurator]::Configure($FileInfo)\r\n               $script:MyCommonLogger = [log4net.LogManager]::GetLogger(\"root\")\r\n\r\n               Write-Verbose \"[New-Logger] Logger is configured\"\r\n\r\n               return $MyCommonLogger\r\n          }\r\n     }\r\n} And to use it: $log = New-Logger -Configuration ./config/log4net.config -Dll ./lib/log4net.dll\r\n$log.DebugFormat(\"Logger configuration file is : '{0}'\", (Resolve-Path \"./config/log4net.config\")) Limits Ok ok… If you really DON’T want to log… But follow what’s going on during your script execution you can use the Write-Host / Write-Verbose… The cmdlet Write-Verbose “something” will write on the console when the switch parameter -Verbose is passed as argument at your script execution¹, or if you set $VerbosePreference to “Continue”. For more details Implement the -WhatIf switch parameter Feed back: How can I run a script, for testing purpose, without impacts? -> With the WhatIf parameter  WhatIf option simulates the behavior and writes it to the console, but without doing anything. You can do dry run test using this parameter. The point: Definitely the first testing step… Trying it, means adopting it. Example: A basic one: to apply it on a cmdlet New-Item -Path \"C:\\Program Files\\example.txt\" -ItemType File –WhatIf To apply it into your script: function Test-WhatIf\r\n{\r\n     [CmdletBinding()]\r\n     Param\r\n     (\r\n          [switch]\r\n          $WhatIf\r\n     )\r\n     New-Item -Path \"C:\\Program Files\\example.txt\" -ItemType File -WhatIf:$WhatIf\r\n} Limit: Sometimes you have to make a complex system just to enable it Use Modules to share your functions… Feed back: On the PowerShell v1, the only way to use common functions was to use the Cmdlet: . .\\myFunctions.ps1 Definitely not the best, but I did run into limitations when I wanted to know what was imported or not. On top of that, it was executing the script and importing it. Ok it works but it’s not its primary role. The point: PowerShell v2 came with some new features: one of them was the module feature. A module is a script file containing functions and it provides you with a way to share your functions. The module file extension in PowerShell is “.psm1” file. Once imported, it can be managed with standard cmdlet (Get-Module, Remove-Module, Import-Module, New-Module), and Force the reimport. Example: Import-Module $MyNewModule.psm1 –Force I use to add at the beginning of a module file: Write-Host “importing MyOwnModule” This helps to check if the module has been imported several times (as it shouldn’t be) or just once… Cf. http://www.simple-talk.com/sysadmin/powershell/an-introduction-to-powershell-modules/ … And to share your paths Feed back: I don’t know how do you manage your path, but it took me time to figure out how to manage them in my scripts. In fact I started to reach some limits when I started to interact a lot between scripts: they use the same paths but declared inside ps1 scripts… To avoid this issue, I decided to start to export them in a psm1 file and set this as a Best practice. On top of that it helped me during my tests (just copy/paste a new file to set testing locations). The point: Set a scope to your variable: at least the “script” scope but most of the time I use the “global” scope. Example: 2 way of setting variable scope Set-Variable -Name MY_FOLDER_PATH -Value \".\\MyFolder\" -Scope Global Or $global:MY_FOLDER_PATH = \".\\MyFolder\" I definitely prefer the first way of writing it… But it’s really a personal point of view :) Limits: It’s important to be aware of variable naming convention (another best practice), so as not to get variable value overwritten problems caused by global scope. Another point: bad scope could be the source of unexpected exceptions: you add a new path (as a variable) and forget to add a scope to it… Believe me, it was dreadful to understand this the first time… Always keep your script independent from the place you run it Feed back: PowerShell sets your current execution folder to the one from which the script is called. If you always run your script manually from the same folder: you don’t care. If you run it through a GUI (like PowerGui), you have to always set your default folder in your script location, from RunOnce or a BAT you have to set the execution folder, etc. Once you know it, it’s easy to deal with. But that’s not a convenient way to use relative path. It would be much better if your script was independent from where you run it, wouldn’t it? The point I advise you to follow this rule: add at the beginning of the script a variable set by “$myInvocation.MyCommand.Path”. Example: Set-Variable -Name SCRIPT_PATH -Value (Split-Path (Resolve-Path $myInvocation.MyCommand.Path)) -Scope local -ErrorAction SilentlyContinue Then, instead of Set-Variable -Name MY_OTHER_FOLDER_PATH -Value \".\\..\\..\\MyOtherFolder\" -Scope Global use: Set-Variable -Name MY_OTHER_FOLDER_PATH -Value \"$SCRIPT_PATH\\..\\..\\MyOtherFolder\" -Scope Global Prefer Exception Feed back: I started to work with PowerShell v1, when the error management was not easy. Options were to use : trap: but I’ve never been able to make it work properly (have a look here to see how it was crapy before) test after a cmdlet execution its error variable (which was giving you the chance to “catch” an error and throw it with a nice log message) Resolve-Path -Path \"./Test\" -ErrorAction SilentlyContinue -ErrorVariable Err\r\nif ($Err)\r\n{\r\n     Write-Host \"The path was wrong :(\"\r\n} Hopefully since PowerShell v2, the block try{}catch{}finally{} appears. The point: Always catch exceptions. No need to go as far as in a program and catch specific exceptions but it’s useful to catch them to be logged, and then apply a rollback if needed.  You can add the function name in the exception message to help you when you read the message to determine where it comes from. Lately I have discovered that the $_.InvocationInfo.PositionMessage property indicates where the error originates from. Example: try\r\n{\r\n       . \".\\myScripts.ps1\"  # this script contains an error!!!\r\n}\r\ncatch [Exception]\r\n{\r\n        Write-Host \"$($_.Exception.ToString()). $($_.InvocationInfo.PositionMessage)\"\r\n}\r\nfinally\r\n{\r\n        Write-Host \"End of this block\"\r\n} Be aware of the advanced help content: “Get-Help about_*” Feed Back: To discover this official manual pages, I needed to read a book² on PowerShell… And I have to admit it was pretty useful and interesting. Theses detailed pages describe how to write or use some cmdlets. The point: You can find a lot of information on internet about PowerShell (… too often it’s about PowerShell v1). So when you don’t know (or even worst: when you don’t have or do have just a limited access to the web): you can use this local help. Example: Get-Help about_try_catch_finally Ps:  That’s how I’ve just learned that you could catch several exception type: catch [System.Net.WebException],[System.IO.IOException]\r\n{\r\n} Limits: It can take some time to find help about a specific subject…. Write help on functions Feed Back: I do not write help description on each function (yes I know I should…), and I never had to use the Get-Help on a function that I did… The point: But writing comments is part of our job (for other people and even ourselves sometimes), and if we have to write them, it’s better to do it by the book To get more help on this: about_Comment_Based_Help Example: <#\r\n.SYNOPSIS\r\n         About your script\r\n.OUTPUTS (with a S at the end....)\r\n         output returned by your script or your function\r\n.PARAMETER MyParam\r\n         MyParam description\r\n#>\r\nFunction Test-MyFunction\r\n{\r\n       [CmdletBinding()]\r\n       param($MyParam)\r\n\r\n       Write-Output $MyParam\r\n} Then you can interrogate your script with Get-Help Test-MyFunction -Detailed Limits: If you make a spelling mistake writing your help description, the Get-Help cmdlet won’t work on it… For example:  if you forgot the ‘s’ at ‘.OUTPUTS’ the Get-Help function won’t show anything. Keep the format Verb-Noun to name your functions Feed back: Function naming conventions are not that important… Until the moment when you need to fix a bug and you have to reread your whole script… Trust me on this one ;-) The point: Use the Get-Verb cmdlet to get common verb to use  During a module import, function names will be checked: you can deactivate warnings with the switch parameter named “ -DisableNameChecking ” Example: At the beginning I started to name my logger function: ConfigLogger (I’m coming from C# world), ConfigureLogger, Configure-Logger (I started to get the Verb-Noun rule), Get-Logger (after discovering the PowerShell v2 module extension .psm1 and its warnings during the function name checking -> it didn’t want to accept Configure) And finally New-Logger: which represents what it does: create a new instance of a log4net logger object. By the way, I think the various naming conventions I used represents my own evolution in PowerShell….) Adopt a high way attitude… to write your variable Feed back: Almost as stupid to say as “follow the pattern Verb-Noun”, I know… But that helps when you read a script to be able to know what variables are… The point: It is not important to follow the CamelCase pattern or any other: the point is to FOLLOW and KEEP the same pattern to help yourself (and the others who would have to read your code) Example: For instance, I have used to write: A constant like : MY_NEW_CONSTANT Parameter like : Myparameter or MyParameter A variable like : myvariable or myVariable Conclusion In this post, I have tried to give you best practices that I could identify during my last projects. I hope they will be useful (maybe even used). If you find some other, feel free to share them here as well. ¹ To use -verbose on your script (or/and your functions) you have to add [CmdletBinding ()] in your script (see about_Functions_CmdletBindingAttribute : when you write functions, you can add the CmdletBinding attribute so that Windows PowerShell will bind the parameters of the function in the same way that it binds the parameters of compiled cmdlets) ² “Windows PowerShell 2.0 – Best Practices” from Ed Wilson Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Project support and tagged .NET , Best Practices , development , DevOps , PowerShell , Scripts , tips&tricks . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 3 commentaires sur “PowerShell v2 – My Best Practices” Clayton 18/05/2011 à 09:11 Purely as a matter of interest - Can I ask what advantages the log4net logger has over 'start-transcript' or do you already have a blog-post on that? Maxence MODELIN 18/05/2011 à 14:31 Hey Clayton\r\nI didn't know the Start-Transcript cmdlet... So thanks :)\r\nAfter reading the help and testing this cmdlet, it seems to help to log actions done in a PowerShell session. It could be used to log a script execution as well, but it's not as powerful as a logger framework is.\r\nLogger frameworks (like log4net) have been built to give you more logging options. It's easy to configure and to use. But you need to carry out a dll and a config file with your script. Bert 20/01/2015 à 15:34 I think it is verry easy to find about the \"about pages\".\r\nJust type \"get-help about\" and you get the list off all pages. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-05-10"},
{"website": "Octo", "title": "\n                Audit with JPA: creation and update date            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/audit-with-jpa-creation-and-update-date/", "abstract": "Audit with JPA: creation and update date Publication date 26/04/2011 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 When writing a business application with persistent data, some auditing capabilities are often required. Today, state of the art for persisting data involves using an ORM tool through the JPA interface. Being able to add two columns containing the creation date and the update date is a common auditing requirement. My colleague Borémi and I have had to answer this question. We have grouped and studied several implementations already used by other Octos. In order to help you choose the best tool for such need, I will present in this paper different solutions that we have compared. For the purpose of this article, I will take the example of a Product table with a single description column. Delegating to the database For a long time, creation and update date columns have been managed by triggers on the database side. Even if it is not the most state of the art technology, this implementation meets well the need. The dates generated by the triggers can be retrieved on the Java side by mapping the two target columns. The major risks of such an implementation are to put in such triggers too much logic or logic that can conflict with business logic coded in the Java code. If these two columns only have a technical purpose and if triggers are the standard way for filling them in, then using trigger is the most appropriate choice. Using @Version and a default value on the column Another natural way to implement this functionality is by combining two functionalities provided by JPA and every database. First, the creation date is implemented by defining a default value on a CREATION_DT column. This way, each time a line is created in the PRODUCT table, the current timestamp will be inserted, providing a creation date for that line. As DEFAULT CURRENT_TIMESTAMP is an SQL92 instruction, I’m confident that all major databases implement it correctly. This default value can be easily defined through the columnDefinition property of the column in JPA. Moreover, by specifying the insertable=false and updatable=false properties, we can guarantee that this column will never be modified by the Java code. So, adding this annotation on a dateCrea field provides a creation timestamp: @Column(name=\"CREATION_TS\", columnDefinition=\"TIMESTAMP DEFAULT CURRENT_TIMESTAMP\", insertable=false, updatable=false) . Then, the update date can be implemented by hacking the @Version column definition. The @Version annotation is used in the JPA specification to define a column used for the optimistic concurrency management. Each time an entity is modified, the value of that column is modified. That way, the JPA implementation can check before each merge operation if the version held by the entity manager is out of date or not. Fortunately, @Version can be defined on a TIMESTAMP column according to the JPA JavaDoc . The timestamp stored in that column is a very good estimator of the UPDATE_TS date (the latest modification date). To summarize, adding two attributes in each entity with the following annotations provides an easy way to get a create and an update date for that entity. @Version\r\n@Column(name=\"UPDATE_TS\")\r\nprivate Calendar dateMaj;\r\n\t\r\n@Column(name=\"CREATION_TS\", columnDefinition=\"TIMESTAMP DEFAULT CURRENT_TIMESTAMP\", insertable=false, updatable=false)\r\nprivate Calendar dateCrea; However, this method has a drawback: you will get an update date earlier than the creation date for a newly created entity . This behavior has been observed with Hibernate but is probably applicable to other JPA implementations too. Indeed, @Version is handled by the JPA implementation on the Java side: the timestamp is generated just before the generation of the INSERT SQL order. This order is then sent to the database where it is executed. The new line is created in the PRODUCT table and the timestamp for the default value of the CREATION_TS column is generated only at this time. Thus, the insertion timestamp is always generated several milliseconds after the update timestamp . From a human point of view these two dates are probably valid but this small difference should be taken into account by a program. For example, such a request would return 0. SELECT COUNT(*)\r\nFROM product p\r\nWHERE p.creation_ts=p.update_ts Looking for all the newly created records requires a slightly more complicated request such as this one SELECT COUNT(*)\r\nFROM product p\r\nwhere ABS(TIMESTAMPDIFF(SECOND, creation_ts, udate_ts))<=1 I have tried several workarounds to solve this problem but without satisfactory result. For example, it is not possible to avoid inserting the @Version attribute for newly created lines by adding an insertable=false instruction. This value is required by JPA in order to correctly manage the optimistic concurrency. Doing so leads to a JPA error. In brief, if you don’t go along with these limitations in your environment you need to choose another implementation. Using a base class for each entity The second implementation on a simplicity scale is to define a base class which handles the creation and update date generation. For that purpose, we have used the @PrePersist and @PreUpdate attributes of the JPA specification. A method with the PrePersist annotation is called each time the persist method is applied on the entity. We can use it to set the dateCrea attribute: @PrePersist\r\nvoid onCreate() {\r\n\tthis.setDateCreaTech(new Timestamp((new Date()).getTime()));\r\n} A method with the PreUpdate annotation is called each time before any update operation is performed on the entity data: a flush of the entity, a call on the setters, and the end of a transaction. We can use it to set the dateMaj attribute. @PreUpdate\r\nvoid onPersist() {\r\n\tthis.setDateMajTech(new Timestamp((new Date()).getTime()));\r\n} This schema on an Oracle documentation illustrates very well the transitions of the lifecycle where these methods are called. To summarize, we have defined a base class; all entities that need these two columns have to extend it. Please note that only the @Entity in a JPA meaning can extend such base class because only the events for @Entity (and not for embeddable class) are monitored. @MappedSuperclass\r\npublic abstract class BaseEntity {\r\n\t/**\r\n\t * Update date\r\n\t */\r\n\tprivate Timestamp dateMajTech;\r\n\t/**\r\n\t * Creation date\r\n\t */\r\n\tprivate Timestamp dateCreaTech;\r\n\r\n\t/**\r\n\t * @return the dateMajTech\r\n\t */\r\n\t@Column(name = \"UPDATE_TS\", insertable = false, updatable = true)\r\n\tTimestamp getDateMajTech() {\r\n\t\treturn dateMajTech;\r\n\t}\r\n\r\n\t/**\r\n\t * @param dateMajTech\r\n\t *            the dateMajTech to set\r\n\t */\r\n\tvoid setDateMajTech(Timestamp dateMajTech) {\r\n\t\tthis.dateMajTech = dateMajTech;\r\n\t}\r\n\r\n\t/**\r\n\t * @return the dateCreaTech\r\n\t */\r\n\t@Column(name = \"CREATION_TS\", insertable = true, updatable = false)\r\n\tTimestamp getDateCreaTech() {\r\n\t\treturn dateCreaTech;\r\n\t}\r\n\r\n\t/**\r\n\t * @param dateCreaTech\r\n\t *            the dateCreaTech to set\r\n\t */\r\n\tvoid setDateCreaTech(Timestamp dateCreaTech) {\r\n\t\tthis.dateCreaTech = dateCreaTech;\r\n\t}\r\n\r\n\t@PrePersist\r\n\tvoid onCreate() {\r\n\t\tthis.setDateCreaTech(new Timestamp((new Date()).getTime()));\r\n\t}\r\n\r\n\t@PreUpdate\r\n\tvoid onPersist() {\r\n\t\tthis.setDateMajTech(new Timestamp((new Date()).getTime()));\r\n\t}\r\n} This second implementation requires a bit more preparation but is very simple to apply: you just have to extend a base class. From a design point of view it applies a slight constraint to the business model by requiring a technical inheritance. Such design with a technical base class was highly criticized for the EJB 2.0 and I confess I have been a bit hesitant about using it. However, such a base class does not require any more dependencies than a traditional JPA entity would. It can be unit tested; it can run outside a container. For the simple need of these two columns such an implementation is finally very productive. Using an entity listener If you don’t want to introduce technical responsibilities into your business model or due to other constraints, you can add listeners on JPA events in an other way. The @PrePersist and @PreUdate annotations we have described in the previous implementation can be applied to a class dedicated to handle the timestamps. We have called it a TimestampEntityListener . Because the timestamps must be persisted in the table, they have to be declared on each entity. In order to share this code, both persistent fields have been placed in an embeddable class named TechnicalColmuns . Moreover, in order to be able to set these values in a generic way, an interface – hiding the business entities – is given as an argument to the TimestampEntityListener . The following listings show the interface, the embeddable class with the persistent fields and an implementation of an entity using this functionality. public interface EntityWithTechnicalColumns {\r\n\tpublic TechnicalColumns getTechnicalColumns();\r\n\t\r\n\tpublic void setTechnicalColumns(TechnicalColumns technicalColumns);\r\n} @Embeddable\r\npublic class TechnicalColumns {\r\n\t@Column(name=\"UPDATE_TS\", insertable=false, updatable=true)\r\n\tprivate Timestamp dateMaj;\t\r\n\t@Column(name=\"CREATION_TS\", insertable=true, updatable=false)\r\n\tprivate Timestamp dateCrea;\r\n\t/**\r\n\t * @return the dateMaj\r\n\t */\r\n\tpublic Timestamp getDateMaj() {\r\n\t\treturn dateMaj;\r\n\t}\r\n\t/**\r\n\t * @param dateMaj the dateMaj to set\r\n\t */\r\n\tpublic void setDateMaj(Timestamp dateMaj) {\r\n\t\tthis.dateMaj = dateMaj;\r\n\t}\r\n\t/**\r\n\t * @return the dateCrea\r\n\t */\r\n\tpublic Timestamp getDateCrea() {\r\n\t\treturn dateCrea;\r\n\t}\r\n\t/**\r\n\t * @param dateCrea the dateCrea to set\r\n\t */\r\n\tpublic void setDateCrea(Timestamp dateCrea) {\r\n\t\tthis.dateCrea = dateCrea;\r\n\t}\r\n} @Entity\r\npublic class Product implements EntityWithTechnicalColumns {\r\n@Embedded\r\n\tprivate TechnicalColumns technicalColumns;\r\n} The TimestampEntityListener can then be defined in order to set the dateCrea and dateMaj fields of the TechnicalColumns instance. public class TimestampEntityListener {\r\n\t@PrePersist\r\n\tvoid onCreate(Object entity) {\r\n\t\tif(entity instanceof EntityWithTechnicalColumns) {\r\n\t\t\tEntityWithTechnicalColumns eact = (EntityWithTechnicalColumns)entity;\r\n\t\t\tif(eact.getTechnicalColumns() == null) {\r\n\t\t\t\teact.setTechnicalColumns(new TechnicalColumns());\r\n\t\t\t}\r\n\t\t\teact.getTechnicalColumns().setDateCrea(new Timestamp((new Date()).getTime()));\r\n\t\t}\r\n\t}\r\n\t\r\n\t@PreUpdate\r\n\tvoid onPersist(Object entity) {\r\n\t\tif(entity instanceof EntityWithTechnicalColumns) {\r\n\t\t\tEntityWithTechnicalColumns eact = (EntityWithTechnicalColumns)entity;\r\n\t\t\tif(eact.getTechnicalColumns() == null) {\r\n\t\t\t\teact.setTechnicalColumns(new TechnicalColumns());\r\n\t\t\t}\r\n\t\t\teact.getTechnicalColumns().setDateMaj(new Timestamp((new Date()).getTime()));\r\n\t\t}\r\n\t}\r\n} Finally, this TimestampEntityListener should be registered in the META-INF/persistence.xml file. <entity-mappings xmlns=\"http://java.sun.com/xml/ns/persistence/orm\" version=\"1.0\" >\r\n   <persistence-unit-metadata>\r\n       <persistence-unit-defaults>\r\n           <entity-listeners>\r\n               <entity-listener class=\"com.octo.rnd.TimestampEntityListener\">\r\n                   <pre-persist method-name=\"onCreate\"/>\r\n                   <pre-update method-name=\"onPersist\"/>\r\n               </entity-listener>\r\n           </entity-listeners>\r\n       </persistence-unit-defaults>\r\n   </persistence-unit-metadata>\r\n</entity-mappings> This implementation requires still a bit more technical code but allows total isolation of the technical code from the business one. The business classes that require this functionality just need to implement the EntityWithTechnicalColumns interface. Similarly to the preceding implementation, please note that only the events for a JPA @Entity (and not the embeddable classes) are monitored. Another advantage of this implementation is that it is easily ported to an Hibernate implementation: The TimestampEntityListener is replaced by an EmptyInterceptor which is registered for example in a Spring configuration file like the class in the two following listings: public class TimestampIntercerptor extends EmptyInterceptor {\r\n\r\n    private static final long serialVersionUID = -7561360055103433456L;\r\n\r\n    @Override\r\n    public boolean onFlushDirty(Object entity, Serializable id, Object[] currentState, Object[] previousState,\r\n            String[] propertyNames, Type[] types) {\r\n\t\tif(entity instanceof EntityWithTechnicalColumns) {\r\n\t\t\tEntityWithTechnicalColumns eact = (EntityWithTechnicalColumns)entity;\r\n\t\t\tif(eact.getTechnicalColumns() == null) {\r\n\t\t\t\teact.setTechnicalColumns(new TechnicalColumns());\r\n\t\t\t}\r\n\t\t\teact.getTechnicalColumns().setDateMaj(new Timestamp((new Date()).getTime()));\r\n\t\t}\r\n        \r\n    }\r\n\r\n    @Override\r\n    public boolean onSave(Object entity, Serializable id, Object[] state, String[] propertyNames, Type[] types) {\r\n        if(entity instanceof EntityWithTechnicalColumns) {\r\n\t\t\tEntityWithTechnicalColumns eact = (EntityWithTechnicalColumns)entity;\r\n\t\t\tif(eact.getTechnicalColumns() == null) {\r\n\t\t\t\teact.setTechnicalColumns(new TechnicalColumns());\r\n\t\t\t}\r\n\t\t\teact.getTechnicalColumns().setDateCrea(new Timestamp((new Date()).getTime()));\r\n\t\t}\r\n    }\r\n} <bean id=\"sessionFactory\" class=\"org.springframework.orm.hibernate3.annotation.AnnotationSessionFactoryBean\">\r\n\t\t<property name=\"dataSource\" ref=\"dataSource\"></property>\r\n\t\t<property name=\"packagesToScan\" value=\"com.octo.rnd\" ></property>\r\n\t\t<property name=\"entityInterceptor\">\r\n\t\t    <bean class=\"com.octo.rnd.TimestampIntercerptor\"/>\r\n  \t\t</property>\r\n</bean> Delegating to a dedicated tool : Hades Such functionality has been implemented in a framework that integrates itself very well with Spring. Hades Framework will incidentally be merged into the new Spring Data framework. The number of functionalities provided by this framework is much larger that our particular requirement, but the Auditing functionality matches our need. In order to use it, our Product class has to implement the Auditable<U, PK> interface, or more easily to extend the AbstractAuditable<U, Integer> base class. U is the class describing the User who has modified the Product, and PK is the primary key type of the product. import javax.persistence.Entity;\r\nimport com.octo.rnd.hades.auditing.AbstractAuditable;\r\n\r\n@Entity\r\npublic class Product extends AbstractAuditable<Integer> {\r\n\r\n\tprivate static final long serialVersionUID = 1462823665190583909L;\r\n\tprivate String description;\r\n\t//Getter and Setter\r\n} In that case too, this base class includes only fields and JPA annotations. Hades behavior is implemented by an EntityListener configured in the META-INF/orm.xml file. <?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<entity-mappings xmlns=\"http://java.sun.com/xml/ns/persistence/orm\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/persistence/orm http://java.sun.com/xml/ns/persistence/orm_1_0.xsd\" version=\"1.0\">\r\n    <persistence-unit-metadata>\r\n        <persistence-unit-defaults>\r\n            <entity-listeners>\r\n                <entity-listener class=\"org.synyx.hades.domain.auditing.support.AuditingEntityListener\" />\r\n            </entity-listeners>\r\n        </persistence-unit-defaults>\r\n    </persistence-unit-metadata>\r\n</entity-mappings> Finally, you have to declare the Hades namespace in the spring application-context.xml file and add the following line: <hades:auditing /> . That’s all. With a small JUnit test, we can see the following Hibernate logs: Hibernate: insert into User (id, login) values (null, ?)\r\n21:24:22,952        TRACE BasicBinder:81 - binding parameter [1] as [VARCHAR] - UserTest\r\nHibernate: call identity()\r\nHibernate: insert into Product (id, createdBy_id, createdDate, lastModifiedBy_id, lastModifiedDate, description) values (null, ?, ?, ?, ?, ?)\r\n21:24:23,102        TRACE BasicBinder:70 - binding parameter [1] as [INTEGER] - <null>\r\n21:24:23,103        TRACE BasicBinder:81 - binding parameter [2] as [TIMESTAMP] - Wed Mar 02 21:24:22 CET 2011\r\n21:24:23,103        TRACE BasicBinder:70 - binding parameter [3] as [INTEGER] - <null>\r\n21:24:23,104        TRACE BasicBinder:81 - binding parameter [4] as [TIMESTAMP] - Wed Mar 02 21:24:22 CET 2011\r\n21:24:23,104        TRACE BasicBinder:81 - binding parameter [5] as [VARCHAR] - Product1\r\nHibernate: call identity()\r\nHibernate: select product0_.id as id0_2_, product0_.createdBy_id as createdBy5_0_2_, product0_.createdDate as createdD2_0_2_, product0_.lastModifiedBy_id as lastModi6_0_2_, product0_.lastModifiedDate as lastModi3_0_2_, product0_.description as descript4_0_2_, user1_.id as id7_0_, user1_.login as login7_0_, user2_.id as id7_1_, user2_.login as login7_1_ from Product product0_ left outer join User user1_ on product0_.createdBy_id=user1_.id left outer join User user2_ on product0_.lastModifiedBy_id=user2_.id where product0_.id=?\r\n21:24:23,120        TRACE BasicBinder:81 - binding parameter [1] as [INTEGER] - 1\r\n21:24:23,124        TRACE BasicExtractor:66 - found [null] as column [id7_0_]\r\n21:24:23,124        TRACE BasicExtractor:66 - found [null] as column [id7_1_]\r\n21:24:23,128        TRACE BasicExtractor:66 - found [null] as column [createdBy5_0_2_]\r\n21:24:23,129        TRACE BasicExtractor:70 - found [2011-03-02 21:24:22.996] as column [createdD2_0_2_]\r\n21:24:23,129        TRACE BasicExtractor:66 - found [null] as column [lastModi6_0_2_]\r\n21:24:23,129        TRACE BasicExtractor:70 - found [2011-03-02 21:24:22.996] as column [lastModi3_0_2_]\r\n21:24:23,130        TRACE BasicExtractor:70 - found [Product1] as column [descript4_0_2_]\r\nHibernate: update Product set createdBy_id=?, createdDate=?, lastModifiedBy_id=?, lastModifiedDate=?, description=? where id=?\r\n21:24:24,224        TRACE BasicBinder:70 - binding parameter [1] as [INTEGER] - <null>\r\n21:24:24,225        TRACE BasicBinder:81 - binding parameter [2] as [TIMESTAMP] - 2011-03-02 21:24:22.996\r\n21:24:24,227        TRACE BasicBinder:70 - binding parameter [3] as [INTEGER] - <null>\r\n21:24:24,227        TRACE BasicBinder:81 - binding parameter [4] as [TIMESTAMP] - Wed Mar 02 21:24:24 CET 2011\r\n21:24:24,228        TRACE BasicBinder:81 - binding parameter [5] as [VARCHAR] - Product 1 modified\r\n21:24:24,229        TRACE BasicBinder:81 - binding parameter [6] as [INTEGER] - 1 Notice that two extra columns createdBy_id and lastModifiedBy_id have been created. You can’t deactivate this functionality. Because I didn’t activate it in the applicationContext.xml file, the columns remain blank but they still exist in the database. Auditing the user that does the modification is however a very common requirement. So let’s have a quick overview of the corresponding configuration with Hades: I have defined an AbstractAuditable class that implements package com.octo.rnd.hades.auditing;\r\nimport org.joda.time.DateTime;\r\nimport org.synyx.hades.domain.AbstractPersistable;\r\nimport org.synyx.hades.domain.auditing.Auditable;\r\n//Other imports\r\n\r\n@MappedSuperclass\r\npublic abstract class AbstractAuditable<PK extends Serializable> extends\r\n        AbstractPersistable<PK> implements Auditable<String, PK> {\r\n\r\n    private static final long serialVersionUID = 141481953116476081L;\r\n\r\n    private String createdBy;\r\n\r\n    @Temporal(TemporalType.TIMESTAMP)\r\n    private Date createdDate;\r\n\r\n    private String lastModifiedBy;\r\n\r\n    @Temporal(TemporalType.TIMESTAMP)\r\n    private Date lastModifiedDate;\r\n\r\n\r\n    public DateTime getCreatedDate() {\r\n\r\n        return null == createdDate ? null : new DateTime(createdDate);\r\n    }\r\n\r\n    public void setCreatedDate(final DateTime createdDate) {\r\n\r\n        this.createdDate = null == createdDate ? null : createdDate.toDate();\r\n    }\r\n\r\n    public DateTime getLastModifiedDate() {\r\n\r\n        return null == lastModifiedDate ? null : new DateTime(lastModifiedDate);\r\n    }\r\n\r\n    public void setLastModifiedDate(final DateTime lastModifiedDate) {\r\n\r\n        this.lastModifiedDate =\r\n                null == lastModifiedDate ? null : lastModifiedDate.toDate();\r\n    }\r\n\t\r\n\t//Other getter and setter are classical ones\r\n} Then, I have defined an AuditorStringAwareImpl package com.octo.rnd.hades.auditing;\r\n\r\nimport org.synyx.hades.domain.auditing.AuditorAware;\r\n\r\npublic class AuditorStringAwareImpl implements AuditorAware<String> {\r\n\r\n    public String getCurrentAuditor() {\r\n\r\n        return \"AuditorString\";\r\n    }\r\n\r\n} I have configured these two beans in the Spring configuration applicationContext.xml . <hades:auditing auditor-aware-ref=\"auditorStringAware\"></hades:auditing>\r\n\t\r\n\t<bean id=\"auditingAware\" class=\"com.octo.rnd.hades.auditing.AuditingAwareImpl\">\r\n\t\t<constructor-arg>\r\n\t\t\t<ref bean=\"UserEm\" />\r\n\t\t</constructor-arg>\r\n\t</bean> \r\n\t<bean id=\"auditableProductDao\" class=\"org.synyx.hades.dao.orm.GenericJpaDao\" init-method=\"validate\">\r\n\t\t<property name=\"domainClass\" value=\"fr.bnpp.pf.personne.concept.model.Product\" />\r\n\t</bean>\r\n\t\r\n\t<bean id=\"auditorStringAware\" class=\"com.octo.rnd.hades.auditing.AuditorStringAwareImpl\"></bean>\r\n\t<bean class=\"org.springframework.orm.jpa.support.PersistenceAnnotationBeanPostProcessor\"></bean> I can then get the following logs: Hibernate: insert into Product (id, createdBy, createdDate, lastModifiedBy, lastModifiedDate, description) values (null, ?, ?, ?, ?, ?)\r\n21:50:18,759        TRACE BasicBinder:81 - binding parameter [1] as [VARCHAR] - AuditorString\r\n21:50:18,769        TRACE BasicBinder:81 - binding parameter [2] as [TIMESTAMP] - Sat Apr 02 21:50:18 CEST 2011\r\n21:50:18,770        TRACE BasicBinder:81 - binding parameter [3] as [VARCHAR] - AuditorString\r\n21:50:18,770        TRACE BasicBinder:81 - binding parameter [4] as [TIMESTAMP] - Sat Apr 02 21:50:18 CEST 2011\r\n21:50:18,771        TRACE BasicBinder:81 - binding parameter [5] as [VARCHAR] - Product1\r\nHibernate: call identity()\r\nHibernate: select product0_.id as id17_0_, product0_.createdBy as createdBy17_0_, product0_.createdDate as createdD3_17_0_, product0_.lastModifiedBy as lastModi4_17_0_, product0_.lastModifiedDate as lastModi5_17_0_, product0_.description as descript6_17_0_ from Product product0_ where product0_.id=?\r\n21:50:18,804        TRACE BasicBinder:81 - binding parameter [1] as [INTEGER] - 1\r\n21:50:18,811        TRACE BasicExtractor:70 - found [AuditorString] as column [createdBy17_0_]\r\n21:50:18,850        TRACE BasicExtractor:70 - found [2011-04-02 21:50:18.614] as column [createdD3_17_0_]\r\n21:50:18,851        TRACE BasicExtractor:70 - found [AuditorString] as column [lastModi4_17_0_]\r\n21:50:18,851        TRACE BasicExtractor:70 - found [2011-04-02 21:50:18.614] as column [lastModi5_17_0_]\r\n21:50:18,852        TRACE BasicExtractor:70 - found [Product1] as column [descript6_17_0_]\r\nHibernate: update Product set createdBy=?, createdDate=?, lastModifiedBy=?, lastModifiedDate=?, description=? where id=?\r\n21:50:20,331        TRACE BasicBinder:81 - binding parameter [1] as [VARCHAR] - AuditorString\r\n21:50:20,332        TRACE BasicBinder:81 - binding parameter [2] as [TIMESTAMP] - 2011-04-02 21:50:18.614\r\n21:50:20,333        TRACE BasicBinder:81 - binding parameter [3] as [VARCHAR] - AuditorString\r\n21:50:20,333        TRACE BasicBinder:81 - binding parameter [4] as [TIMESTAMP] - Sat Apr 02 21:50:20 CEST 2011\r\n21:50:20,334        TRACE BasicBinder:81 - binding parameter [5] as [VARCHAR] - Product 1 modified\r\n21:50:20,334        TRACE BasicBinder:81 - binding parameter [6] as [INTEGER] - 1 Instead of implementing manually the org.synyx.hades.domain.auditing.Auditable interface, we could have used directly the class AbstractAuditable provided by Hades. However, this class requires that the class U describing the user to be an entity. Moreover, retrieving a user entity in the AuditorStringAwareImpl leads to some problems. Loading order and dependency injection between the component managed by Spring and the listeners managed by Hibernate do not  work out of the box. Having a list of users in the database was largely overkill for our need. So I won’t get into such details in this article. Delegating to a second dedicated tool : Hibernate Envers Finally, Hibernate, the JPA major implementation, includes since its 3.5 version an auditing functionality. It was previously called Envers module . By just adding some Hibernate specific annotations on the classes and some properties in the configuration files, all changes are audited. For example on a product class you would add: import javax.persistence.Entity;\r\nimport javax.persistence.Id;\r\nimport javax.persistence.GeneratedValue;\r\n\r\n@Entity\r\n@Audited\r\npublic class Product {\r\n    @Id\r\n    @GeneratedValue\r\n    private int id;\r\n\t\r\n    private String description;\r\n} In order to use this functionality, you have to add in the META-INF/persistence.xml file <persistence-unit>\r\n<!-- Standard properties -->\r\n<properties>\r\n   <!-- other hibernate properties -->\r\n   <property name=\"hibernate.ejb.event.post-insert\" value=\"org.hibernate.ejb.event.EJB3PostInsertEventListener,org.hibernate.envers.event.AuditEventListener\"></property>\r\n   <property name=\"hibernate.ejb.event.post-update\"  value=\"org.hibernate.ejb.event.EJB3PostUpdateEventListener,org.hibernate.envers.event.AuditEventListener\"></property>\r\n   <property name=\"hibernate.ejb.event.post-delete\"  value=\"org.hibernate.ejb.event.EJB3PostDeleteEventListener,org.hibernate.envers.event.AuditEventListener\"></property>\r\n   <property name=\"hibernate.ejb.event.pre-collection-update\" value=\"org.hibernate.envers.event.AuditEventListener\"></property>\r\n   <property name=\"hibernate.ejb.event.pre-collection-remove\" value=\"org.hibernate.envers.event.AuditEventListener\"></property>\r\n   <property name=\"hibernate.ejb.event.post-collection-recreate\" value=\"org.hibernate.envers.event.AuditEventListener\"></property>\r\n</properties>\r\n</persistence-unit> Hibernate creates for you an audit table, with an _AUD suffix, for each audited class. Such implementation was overkill for our particular need: adding extra tables was not desirable. However, for deep auditing requirements, envers module is one of the best choices. Conclusion To conclude, we finally used a base class in our particular case. This is not the right tool for every situation but I trust this table will help you make your choice: Solution Pros Cons Delegating to the database Easy to implement, well known solution Introduces another technology with potential conflict of responsibilities Using @Version and a default value on the column Java solution with very few lines of code Severe drawback: creation date is later than the update date on newly created entities Using a base class for each entity or an entity listener Java solution entirely based on JPA standard Technical implementation falls entirely under the team responsibility Delegating to a dedicated tool (Hades or Envers) Java solutions based on tried and tested frameworks with lots of functionalities Each tool comes at a cost due to its complexity Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Java , JPA . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 8 commentaires sur “Audit with JPA: creation and update date” Jonathan 02/06/2011 à 08:27 Great article, thanks.\r\n\r\nI'm not sure how best to go about creating instances of an entity like one that extends your BaseEntity. Am I right in saying that if I use new MyEntity() that injection will not operate? I'm finding that if I inject MyEntity into the class which is creating and persisting instances like:\r\n\r\n@Inject\r\n@New\r\nprivate MyEntity myNewEntity;\r\n\r\nthat I get the same instance on the second invocation.\r\n\r\nHow would I create method-local instances of MyEntity and still have the injection processed?\r\n\r\nI'm sure I'm missing something obvious ... Marc Bojoly 08/06/2011 à 00:02 Hi,\r\n\r\nIt is not strictly speaking injection (in the @Inject meaning) that is used in the entities. Local instance of your class should be retrieved from a JPA entity manager and not injected. MArco 20/07/2011 à 23:50 Hi!\r\n\r\nI've found your article in google searching for a solution to my problem and I find it very usefull! \r\n\r\nI've adopted -Using a base class for each entity or an entity listener- solution. I've also turn on envers with @Audited annotation to audit my Entities changes.\r\n\r\nMy problem is this: how can I check if the entity was really changed by user before set new date in @PreUpdate method? If an user in the view choose to modify an entity and then save without any changes.. the time is overwrite by @PreUpdate.\r\n\r\nSo if I don't check this option, the date of modification is subsituted by new date, and envers create an audit of the row, because it see that the column is changed .\r\n\r\nI think that is a problem of passing the entity to and from the view by controller, because if I test it by java tests the @PreUpdate is not catched.\r\n\r\nThank you for your attention!\r\n\r\nMarco Borémi Toch 21/07/2011 à 21:39 Hi Marco,\r\n\r\nI don't understand the scenario you describe \"If an user in the view choose to modify an entity and then save without any changes.\" but are you a hundred percent sure your entity is not modified ?\r\n\r\nThe exact triggering conditions of the @PreUpdate are not part of JPA and are actually implementation dependent.\r\n\r\nIf you are using Hibernate, you can have it log which properties are in a dirty state. In order to do that, you have to log org.hibernate.persister.entity at the TRACE level.\r\n\r\nBasically, Hibernate compares the entity you merge with a snapshot which was taken when it was read from the database. This comparison is done on a field by field basis, hence even if you merge a detached deep copy of your original object, Hibernate does not consider your entity dirty and hence does not trigger the @PreUpdate. \r\nSo if you're using Hibernate, I'm guessing your entity has actually changed in some way. Patrice 28/10/2011 à 10:57 If you \r\n - absolutly need multiple audit column with datetime set from the database (creationdate, modificationdate, but often a third \"modificationtimestamp\" column \r\nused by synchronization tools and wich reflect all operation make to a row independently from the source : application, batch ...) \r\n - can't have multiple column with default value to current datetime (like mysql)\r\n - don't want to use datbase tools like triggers, \r\nYou can have a workaround with QueryRedirector to catch and modify orders : \r\nex : insert (....CREATIONDATE ...) values (....CURRENT_TIMESTAMP...). Darminto 06/12/2012 à 15:26 Annotations (Paperback)      I've had this book for 2 days and I'm already at cheaptr 4.  All I have to say is this is THE book to get if you want to learn Hibernate from scratch.  Believe me, you will not hibernate when reading this book.  Writing style is very clear and easy to understand.  This book reminds me of the Murach series books, but much better.  I also love the fact that the examples don't force you to use other miscellaneous helper tools (Ant, JUnit, etc) to get the examples working.  The author apologizes for not using those tools at first, but I think this is a good thing.  Had he made us use those tools, it would have complicated the learning process, not to mention having to learn how to use those tools.  Don't get me wrong, eventually you should learn those tools for large projects. The only thing I wish the book would cover more is how to use Hibernate with servlets or JSPs or other web front end technologies since now a days people want to learn how to make Hibernate work with their web applications.  But I understand wholeheartedly why the author didn't do a more deeper coverage.  Perhaps he should for his next book (hint hint). I also found some minor mistakes or omissions, not in the code, but in some of the explanations.  For example, reference to where the library zip files are located (page 50) is incorrect and to get Log4j to work, the author should have explicitly stated where the log4j.properties file needs to be saved(page 97).  He explicitly states where the other files need to be saved, but for some reason, he made an exception for the log4j properties file.  I had to use trial and error to figure that out (needs to be in the c:_mycode directory).  Sorry the only reason I'm mentioning these mistakes here is because the book's website at the time of this review doesn't appear to have a link to see/send errata and download sample code. I look forward reading the book to the very last page.  So far so good!  Without hesitation, I highly recommend this book. Javi 15/11/2013 à 12:24 Great. Very well explained çağla boynuegri 19/04/2017 à 07:57 Using an entity listener solution requires some code change to work. Here is my working modified version:\r\n\r\npublic class TimestampInterceptor extends EmptyInterceptor {\r\n\r\n    private static final long serialVersionUID = -7561360055103433456L;\r\n\r\n    @Override\r\n    public boolean onFlushDirty(Object entity, Serializable id, Object[] currentState, Object[] previousState,\r\n                                String[] propertyNames, Type[] types) {\r\n        if (entity instanceof EntityWithAuditColumns) {\r\n            EntityWithAuditColumns eact = getEntityWithAuditColumns((EntityWithAuditColumns) entity);\r\n            eact.getAuditColumns().setUpdatedTimestamp(Timestamp.valueOf(TimeMachine.now()));\r\n            for ( int i=0; i<propertyNames.length; i++ ) {\r\n                if ( \"auditColumns\".equals( propertyNames[i] ) ) {\r\n                    currentState[i] = eact.getAuditColumns();\r\n                    return true;\r\n                }\r\n            }\r\n        }\r\n\r\n        return false;\r\n    }\r\n\r\n    private EntityWithAuditColumns getEntityWithAuditColumns(EntityWithAuditColumns entity) {\r\n        EntityWithAuditColumns eact = entity;\r\n        if (eact.getAuditColumns() == null) {\r\n            eact.setAuditColumns(new AuditColumns());\r\n        }\r\n        return eact;\r\n    }\r\n\r\n    @Override\r\n    public boolean onSave(Object entity, Serializable id, Object[] state, String[] propertyNames, Type[] types) {\r\n        if (entity instanceof EntityWithAuditColumns) {\r\n            EntityWithAuditColumns eact = getEntityWithAuditColumns((EntityWithAuditColumns) entity);\r\n            eact.getAuditColumns().setCreationTimestamp(Timestamp.valueOf(TimeMachine.now()));\r\n            for ( int i=0; i<propertyNames.length; i++ ) {\r\n                if ( \"auditColumns\".equals( propertyNames[i] ) ) {\r\n                    state[i] = eact.getAuditColumns();\r\n                    return true;\r\n                }\r\n            }\r\n        }\r\n\r\n        return false;\r\n    }\r\n} Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-04-26"},
{"website": "Octo", "title": "\n                Mutation Testing, a step further to the perfection            ", "author": ["Nicolas de Nayer"], "link": "https://blog.octo.com/en/mutation-testing-a-step-further-to-the-perfection/", "abstract": "Mutation Testing, a step further to the perfection Publication date 18/08/2011 by Nicolas de Nayer Tweet Share 0 +1 LinkedIn 0 Mutation Testing Usefulness of unit tests is no longer discussed. They are essential in conception of a quality application. But, how can we assess their relevance ? A code coverage indicator up to 100% doesn’t mean the code is 100% tested. This is just a view of executed code during unit tests execution. The technique described here will allow you to have more confidence in your tests . This is a two step process : first mutants generation and then bloodshed of them. WTF? Mutant generation Goal of this step is to generate mutants classes from your business classes. What do you need? First of all, business code on which you want to evaluate your tests relevance. Next, a pool of mutations. A mutation is a change in source code. For example, the switch of an operator. Here are some examples: + ► – * ► / >= ► == true ► false. suppression of an instruction etc. Strictly speaking, generation consist in parsing every instruction of your code to determine if it is possible to apply some mutations. If true then it will generate a new mutant. In the next exemple, for the defined mutations pool and the specified code, process will give two mutant classes. Note that this process can use a lot of resources. When code to mute has a big amount of instructions and when mutations pool is consequent, number  of generated mutants quickly get big. Process of mutants generation is now done. Mutants are saved waiting for the next step: bloodshed ! Mutants bloodshed! Bloodshed is the best word I found to describe the second step of this technique. During the first part we have generated a big amount of mutants, but mutants are dangerous, Kill ‘Em All ! So goal of the game will be to eliminate as many as possible. What kind of weapon do we have? Unit tests of course! User’s guide: Make sure that all your tests of your unmuted business code pass Take all your mutants one by one Place them in front of a wall shot ( or in your classpath but it is really less classy) Shoot a salve of unit tests Count the deads Report / analysis For one given mutant, there are two possible results. All tests are still green At least one of the test is now red Usually we want tests to be green, but here we want them to be red, which is the color attesting the death of our mutant! Indeed if at least one test fails it means our tests are able to detect modifications of business code. On the other hand, if all tests are still green, the mutant outlives, it is invisible for our tests. One surviving mutant is potentially a missing test! Restrictions The principle is simple, but the full analysis can be tedious. Indeed as mentioned in the first part, you can have a very large amount of mutants. Lets take an example, 8000 mutants were generated in the first phase. During the carnage, 95% of them were killed (God rest their souls). There are still 400 mutants. Doing a manual analysis for each of them is expensive. And the fault may not be due to our unit tests. Indeed, as in all battles, there are enemies tougher and more cunning. Here they are called “equivalent mutant”. An equivalent mutant is a mutant that changes the syntax of the source code, but not its semantics. As a result, it will be impossible to create a unit test which can detect it. Source code: int index = 0;\r\nwhile(...) {\r\n    ...;\r\n    index++;\r\n    if (index == 10) break;\r\n} A mutation from “==” to “>=” will produce the following equivalent mutant: int index = 0;\r\nwhile(...) {\r\n    ...;\r\n    index++;\r\n    if (index >= 10) break;\r\n} In this example it is clear that the output condition of the loop remains the same. Which tool? This technique is not new, it was designed in 1971 by Richard Lipton . Its expansion was restricted by the heaviness of the process. The increase computer’s power has made the “mutation testing” more accessible. Ok, interesting concept. How can I use it on my project? Unfortunately available tools in Java’s world are far from being industrialized. The pioneer MμJava is not based on JUnit as antecedent to it. Its replacement MμClipse only supports JUnit 3 and is no longer maintained. Jester as for it, is laborious and requires a complicated configuration; plus is not maintained anymore. The best tool I could find is Javalanche. It incorporates and combines all the advantages of previous tools, ie: Selective mutation: A small number of mutations well selected is sufficient to have accurate results without generating too much noise. Mutant schemata: To avoid too many class version. The mutant schemata, the program keeps multiple mutations each guarded by a “runtime flag.” Ref. [3] Coverage data: All tests do not pass on each mutant. To avoid running irrelevant tests on a mutation, coverage data is collected. Then are executed only tests relating to the mutation. Manipulate bytecode: The generation of mutants is performed directly at the bytecode level avoiding recompilation and time-consuming. Parallel execution: It is possible to parallelize the process, WHAT. Automation: Javalanche, unlike other tools, requires very little configuration. Simply configure the test suite to run and the package of classes to be mutated. To prove what they say Javalanche designers have realized the following benchmark: In real life Javalanche seems to have been tried and tested out. So I submit it to constraints on real project. The results I obtained are positive despite some difficulties in realization. After a period of substantial ownership and resolution of some conflicts with my project classpath, here are my conclusions: for 10 classes and a total of 1000 LOC, the process takes 1 minute possibility of executing the process with both unitary and integration tests integration with frameworks like Spring and Hibernate is possible integration with mock frameworks (EasyMock) is possible, but with restrictions. Indeed, it was impossible to run the process in one time on both class A and B, if the tests for a Class B use a mock A. Without digging further, I think that by “proxyifier” classes too much, the tool is “lost” in the classpath (A, A mutated, A mocked, etc …) But most important is that it has revealed some non-tested and criticisms cases. Conclusion This technique is a step closer to perfection. But to use it, you need to be in a high quality approach. Testing should already be at the heart of your development process, otherwise results represent a too heavy load analysis. If you are one of those for whom coverage indicator has reached its limits, ponder this Dr. Robert Geist’s quote: If the software contains a fault, there will usually be a set of mutants that can only be killed by a test case that also detects that fault. Unfortunately, current tools, even Javalanche, do not seem quite industrialized. When will we have a Maven plugin that would allow us to follow the rate of killed mutants for each build? Wanna play? Want to see how it works? Here is a small sample describing how you can complete your unit tests with this method. Lets start  with a 100% covered class: The method initializes a Map and add an entry for each key passed as parameter by assigning the value 0. After a first pass, Javalanche gives us this report: In the left section, you can see that a simple class generates 8 mutants, but also that there are 3 survivors. In the right side, note that in line 18, there is the rub. Whether we increment or decrement the constant, tests continue to pass. Let’s see: Method’s tests: package com.octo.sample.mutationTesting;\r\n\r\npublic class MyClassUTest {\r\n\r\n\t@Test\r\n\tpublic void initMapToValueForKeys_WithNull_ShouldReturnAnEmptyMap() {\r\n\t\t// setup\r\n\t\tMyClass myCLass = new MyClass();\r\n\r\n\t\t// action\r\n\t\tMap myMap = myCLass.initMapToValueForKeys(null);\r\n\r\n\t\t// assert\r\n\t\tassertNotNull(myMap);\r\n\t\tassertEquals(0, myMap.size());\r\n\t}\r\n\r\n\t@Test\r\n\tpublic void initMapToValueForKeys_WithGoodParams_ShouldReturnAMapOfTheRigthSize() {\r\n\t\t// setup\r\n\t\tMyClass myCLass = new MyClass();\r\n\t\tInteger[] myKeys =new Integer[] {35, 84,8000};\r\n\r\n\t\t// action\r\n\t\tMap myMap = myCLass.initMapToValueForKeys(myKeys);\r\n\r\n\t\t// assert\r\n\t\tassertNotNull(myMap);\r\n\t\tassertEquals(3, myMap.size());\r\n\t}\r\n} Indeed, we check size of  returned map, but not the initial value. So we add a test: @Test\r\npublic void initMapToValueForKeys_WithGoodParams_ShouldReturnAMapWithGoodValues() {\r\n\t// setup\r\n\tMyClass myCLass = new MyClass();\r\n\tInteger[] myKeys =new Integer[] {35, 84,8000};\r\n\r\n\t// action\r\n\tMap myMap = myCLass.initMapToValueForKeys(myKeys);\r\n\r\n\t// assert\r\n\tassertNotNull(myMap);\r\n\tassertEquals(3, myMap.size());\r\n\tassertEquals(new Integer(0), myMap.get(myKeys[0]));\r\n\tassertEquals(new Integer(0), myMap.get(myKeys[1]));\r\n\tassertEquals(new Integer(0), myMap.get(myKeys[2]));\r\n} Lets relaunch Javalanche: Good job! Want to try? To realize this sample, follow these steps: set up maven and Ant get and unzip Javalanche unzip and build the sample (maven install) edit Javalanche.xml file which is at the project’s root. Changes the first property and have it point to the directory where you extracted Javalanche Then just run Javalanche by executing the following command: ant -f javalanche.xml -Djavalanche.maxmemory=1024m mutationTest Références [1] – Javalanche: Efﬁcient Mutation Testing for Java [2] – Should Software Testers Use Mutation Analysis to Augment a Test Set? [3] – Mutation Analysis Using Mutant Schemata Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Mutation Testing, a step further to the perfection” Maciej Gawinecki 25/11/2012 à 21:57 \" it has revealed some non-tested and criticisms cases.\" Can you elaborate on that more? Give some examples. I think that's the core. I don't care about performance, if mutation testing provides useless feedback. Henry 02/04/2021 à 12:16 muJava, Jester and javalanche are all old and unmaintained. For quite some time now the de-facto standard mutation testing tool for java has been PIT/pitest (https://pitest.org). Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-08-18"},
{"website": "Octo", "title": "\n                Hybrid Cloud and Amazon based IaaS – 2            ", "author": ["Stéphane Teyssier"], "link": "https://blog.octo.com/en/hybrid-cloud-and-amazon-based-iaas-2/", "abstract": "Hybrid Cloud and Amazon based IaaS – 2 Publication date 22/02/2011 by Stéphane Teyssier Tweet Share 0 +1 LinkedIn 0 Following the previous article (it is true :-)), we will consider the Amazon Web Services – AWS – to treat 3 themes related to hybrid IaaS Cloud : the connection between “on-premise” IT and a public cloud infrastructure, the policy for management of VM images and finally the licenses. What connection between “on-premise” IT and the infrastructure of public Cloud ? 2 complementary philosophies are possible to make communicate applications or components distributed on distinct infrastructures (in the geographical meaning): By Software interaction : create wrappers (web-services) to establish communications between different software blocks. These web services could be secured by SSL to ensure  confidentiality of exchanged data between the internal IT and public VMs. By integration and securing network : the different infrastructures are considered as an unique set from the network point of view. Using mechanisms like secured tunneling, every cloud platform is then an extension of the “on-premise” IT. Amazon, unlike other cloud actors, proposes Virtual Private Cloud service which can create an IPsec tunnel (authentication of both extremities and encryption of transported information) between an Amazon Data Center and your ‘on-premise’ IT. According to Amazon, this functionality requires 7 % of additional use of network bandwidth due to encryption and encapsulation. source : http://aws.amazon.com/ This service needs an advanced router, compliant with IKE  (Internet Key Exchange) protocol to exchange security attributes in relation with the VPN setup, or BPG (Border Gateway Protocol) Peering to define network routes between Amazon data center and your infrastructure. The main advantage of IPSec tunnels’ usage is beyond the transparency provided to applications,on Amazon side, VMs are not accessible from public Internet, but only by the VPN entry point. About the drawbacks, the setup of a VPN can imply important modifications of the network architecture (rules of filtering, sub-networks, distribution of bandwidth …). Currently, VPC service is in beta release with the following limitations: Some AWS like auto-scaling or Elastic Load Balancing are not compatible with VMs running inside a VPC. This will lead you to deploy machines configured by yourself to accomplish these functionnalities. IPV6 is not supported. The virtual router (on Amazon side) does not manage the concept of security groups – Amazon notion to determine who are the users allowed to administrate AWS resources (VM, storage  …). A single private cloud with a maximum of 20 sub networks can be created. What policy for the management of Amazon Machine Images ? To launch a VM, you need an image – Amazon Machine Image (AMI). So, the question is if a company must manage an hundred or more of VMs, how must it manage the AMIs at the origin of the VMs ? Amazon proposes 3 options : inventory of AMIs, Light coupling AMIs and “Just-Enough OS AMI”. The inventory of AMIs is the first approach. An AMI if associated to a “type” of VM : OS, technical stack and code are fixed. Several VMs can be started from the same image but suppose you have an “application server” VM and a second which is “database server” : you will have two different AMIs. As the company will adopt the Amazon cloud, the number of AMIs it will have to manage will increase. Problems will occur during updates : if you must apply a patch on an OS used by 30 AMIs, 30 AMIs should be created again once the update is done. Same thing for the technical stack and the applications! The second method is to introduce a light coupling between the image of a VM and its final state. Now, the application is loaded during boot time of the VM : an AMI which contains an application server will load JEE archives on a previously set location(S3, file server …). With this configuration, we avoid to bundle a new AMI to integrate new release of applicative binaries. For JAVA applications, this mechanism can simply be a Maven Command executed at startup to retrieve artifacts from a repository inside the company. In case of other technologies, we can imagine using a software versioning system (e.g : a SVN export) or a simple HTTP download (wget). To finish, the last solution is to create AMIs containing only OS and the mandatory environment to run an automatized configuration tool like Chef , Puppet , CFEngine . During the launch of the VM, a parameter will indicate which configuration should be used (list of required frameworks, the applicative code, …) and the VM will configure itself using scripts provided by the configuration tool. How to proceed for licenses ? The migration of licenses can be made in three distinct ways : Many editors have agreements with Amazon allowing the final user to perform a smooth transition towards execution on EC2 instances. This model called “Bring Your Own License (BYOL)” is based on the availability of pre-configured AMIs, in partnership with editors who have integrated licence mecanisms. The license you have used since 1 year, still usefull for the next 2 years will be available for certain AMIs on EC2. Among the partner editors we find Oracle, Sybase, Adobe, MySQL, JBoss, IBM and Microsoft. In addition to the “BYOL” model which is much close to the classical licensing model, Amazon proposes a “Pay-As-You-Go” model based on the “DevPay” service. In this case, licenses have been built with partners to integrate the “on demand” capability of EC2 : costs due to licenses are applied on the execution time of the VMs, that is to say the AMIs which contain the licensed software generate an extra charge of the hourly price. Support prestations are often included in these AMIs. Among the partners, we find : RedHat, Novell, IBM, Wowza. Finally, the third solution is to use an application in SaaS mode. Amazon introduces editors whose SaaS offers are hosted on AWS and provide migration kits to facilitate the transition to this model. Subscription is often monthly. We can list Mathematica , Quantivo ou Pervasive . Conclusion Several actors of public Cloud can intervene inside the same hybrid cloud. Reasons can be: financial: it can be interesting to launch VMs by a provider A and to store data using a provider B. historical: the offer of a provider A is well mastered by IT teams. POC or an hybrid cloud already are deployed on infrastructure of provider A. Yet, provider B has technical services which are unavailable elsewhere. others: the provider B guarantees a SLA greater than one of the provider A. … However, each platform has its specificities, its own services, its proprietary APIs and the possible answers presented in this article, based on AWS could not apply to other IaaS offers and reciprocally. Even though some initiatives of partnerships (between VMWare and Google for example) favor the development of standards, Cloud market is far from offering common services and APIs. Thus the multiplicity of used offers leads to increase the need of internal skills and an relative adhesion to the cloud  providers. Ideally, we will limit the number of CLoud actors, prefering partners apt to answer the forthcoming needs, in terms of services and guarantees. Tweet Share 0 +1 LinkedIn 0 This entry was posted in General -- DO NOT USE . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-02-22"},
{"website": "Octo", "title": "\n                Hybrid Cloud and Amazon based IaaS – 1            ", "author": ["Stéphane Teyssier"], "link": "https://blog.octo.com/en/hybride-cloud-and-amazon-based-iaas/", "abstract": "Hybrid Cloud and Amazon based IaaS – 1 Publication date 07/02/2011 by Stéphane Teyssier Tweet Share 0 +1 LinkedIn 0 In this first part, we introduce hybrid cloud computing, then relying on Amazon Web Services we will try to provide answers to questions related to hybrid infrastructures. Why an hybrid Cloud ? Among the reasons to set up an hybrid Cloud, we distinguish: optimization of an existing structure : no need to permanently maintain a set of machines sized to support temporary peak loads on demand provisioning : enhance providing delays for new material ressources starting a backup infrastructure : in case of a Disaster Recovery Plan (DRP) To these elements, we must a dd the necessity for a company to be convinced by the maturity of public Cloud . This problem of trust is one of the main obstacles (in French) to the adoption of public Cloud by companies. In this case, the idea is to make a first sketch of an hybrid infrastructure to acquire the required skills (ownership of the platform), gain insurance and trust before performing more important deployments. Nevertheless, all the applications are not good candidates to be moved from the internal IT infrastructure to a public Cloud: For hardware reasons: for example case of Mainframe systems. For strategic reasons: data are perceived as too important to be hosted outside the company. Or the application is too critical to risk an “adhesion” against the Cloud provider. For financial reasons : architecture of the application relies on building blocks which are not yet available in Cloud offers – the migration would need such adaptations that it is not interesting. … Thus, except small “standard” IT which may be fully migrated, hybrid cloud has interest for companies trying to test public Cloud or having to amortize a previously made infrastructure investment. Hybrid Cloud typologies In the rest of this article and the forthcoming ones, we define hybrid Cloud as the necessary cohabitation between an “on-premise” IT of a company A and a service proposed by a public Cloud. Moreover, we establish third party applications/services/infrastructure must be administered by company A. This make us compliant with people who think usage of public APIs like Google Maps or Twitter from an on premise IT does not constitute an hybrid cloud. Hybrid Cloud can be declined in three models Outsourcing applications Usage of  technical services Outsourcing infrastructure In addition, applications for enterprises can be segmented in three main categories according to their specificity : shared by all companies (mail, calendar, timesheets, …), common to distinct business lines but without differentiating issues (middleware, storage services, …) or strategical within a same activity domain (computation library, algorithm, innovating services, …). For more information on this subject, see USI session of Julien CABOT (again in French!). We will see that the base of the pyramid is adapted to application outsourcing whereas the top is more suitable for infrastructure outsourcing. Application outsourcing In this model, whole applications – not only components – are outsourced to the public Cloud. Even though the used Cloud  can be IaaS or PaaS if the company decides to keep a full control on its software, a SaaS offer is the best choice for applications common to all companies. No need of specific to insure standard functionalities like mail or room booking. The main architectural issues are provisioning and unprovisioning of users, choice of partners according to their SLA and service propositions. In addition, we  must adress integration strategies at data level : must we duplicate data ? what are the possibilities to get back data from the outsourced application ? by services : which interconnection must we set ? by MMI – setting up a portal … Usage of technical services In this case, we are not dealing with whole applications but only technical components which are located on the public Cloud. Usually, we find services like CDN (Content Delivery Network), MOM (Message Oriented Middleware), storage (like S3 for example) … Il ne s’agit plus d’applications entières, mais uniquement de briques techniques qui sont situées sur le cloud public. On retrouve classiquement des services de CDN (content delivery network), des middlewares de type MOM, du stockage (comme S3 par exemple) … This approach of hybrid Cloud is suitable for PaaS offers. Key points must concern latency and “adhesion” to API. Outsourcing infrastructure Finally, we consider running Virtual Machines outside of the on premise IT. In this case, we needto use more CPU, RAM – hardware ressource for a given timestamp. As they are low level services, they offer the highest flexibility. Thus, they are interesting for specific and innovating applications. Challenges are to keep control of the infrastructure (bandwidth, latency) and succeed to industrialize it (are format of disk images suitable for a later internalization ? appropriating control API, ..). Setting up a outsource infrastructure requires studies like What connection between on premise IT and public CLoud infrastructure ? What policy must be enforced to manage VM images ? How to proceed for licensing ? What are the ways to manage Cloud plateform access ? Is data compression usefull ? Where data must be placed ? Considering the Amazon case – Amazon Web Services – one of the main actor of IaaS, we will try to answer these questions. Some of them are adressed in the migration document edited by Amazon . To be followed … Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , General -- DO NOT USE . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-02-07"},
{"website": "Octo", "title": "\n                ANNOUNCEMENT: Java User Group in Lausanne on February the 10th            ", "author": ["Cyril Picat"], "link": "https://blog.octo.com/en/jugl-february-code-quality/", "abstract": "ANNOUNCEMENT: Java User Group in Lausanne on February the 10th Publication date 01/02/2011 by Cyril Picat Tweet Share 0 +1 LinkedIn 0 The Lausanne JUG holds a session dedicated to software quality on the Java platform in the presence of the leading software editors: Coverity Headway Software (Structure 101) Parasoft (JTest) Sonar XDepend During this event, each editor will demonstrate in 20 minutes its product’s features by analyzing the same application : IceScrum2. These demos will be followed by a panel discussion of 20 minutes where participants will be able to ask more open questions to all editors. Like all JUGs, this is a free event . Presentations will be given in English (Coverity, Headway, Parasoft) or in French (Sonar, XDepend) depending of the vendors. It is important you register for us to be able to plan the room capacity. The event will be followed by a buffet and will give you the opportunity to ask more detailed questions to editors. Be careful, the event does not take place at the usual place! It will take place at the Moevenpick Hotel in Lausanne , next to the underground station M2 Ouchy and very near the railway station (10 minutes). Lausanne JUG team, Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged Code quality , Coverity , Headway structure101 , Java , jugl , Parasoft Jtest , sonar , Suisse , XDepend . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-02-01"},
{"website": "Octo", "title": "\n                Last days to benefit from 20% reduction on your USI 2011 registration, june 28th&29th            ", "author": ["Céline Pageaux"], "link": "https://blog.octo.com/en/last-days-to-benefit-from-20-reduction-on-your-usi-2011-registration-june-28th29th/", "abstract": "Last days to benefit from 20% reduction on your USI 2011 registration, june 28th&29th Publication date 21/01/2011 by Céline Pageaux Tweet Share 0 +1 LinkedIn 0 In three years, USI has become the reference conference for « geeks » and « bosses » wishing to create Information Technologies that transform our society and our organizations. During the first editions, we had the great honor to welcome important persons such as Neil Armstrong, Eliyahu Goldratt, Juan Enriquez, Neal Ford, Chris Anderson, Martin Fowler… and many actors of our professions coming from Google, Microsoft, SAP, Mozilla,… This year again, we will develop 50 sessions on different subjects around 4 themes: Differently : inspirations coming from other domains (for eg. sciences, architecture, medicine), video games editors, Web masters, Big Mashups, DevOps. Technically : Cloud in practice, coding masters, Big Data, event architectures, mobile architectures, HTML5 vs owner. Humanely : social impacts of IT, animation dynamics, human factors, personal efficiency, agile and Lean transition. Shortly : prospective, evolution of our jobs per sector, future interfaces, IT at the speed of light, blend of Web and TV. And once again renowned personalities: Pattie Maes, associate porfessor, MIT André Compte-Sponville, philosopher and member of the Comité Consultatif National d’Ethique Michel Serres, philosopher and member of the Académie Française Simon Sinek, leadership expert and author of “Start with WHY” Benjamin Chaminade, international expert in “Talent Management”, Génération Y John Allspaw, VP technical operations, Etsy You only have 10 days (01/31/11) to register and benefit from our promotional offer : 20% reduction on your USI registration fee (€1 560 net of charge instead of €1 950; group registrations also possible). I REGISTER NOW TO USI 2011 AND BENEFIT FROM THE -20% REDUCTION Tweet Share 0 +1 LinkedIn 0 This entry was posted in General -- DO NOT USE , News and tagged USI 2011 . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Last days to benefit from 20% reduction on your USI 2011 registration, june 28th&29th” Enlast 09/04/2011 à 05:35 Do you wanna benefit Enlast, Please visit. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-01-21"},
{"website": "Octo", "title": "\n                How to: choosing between lightweight and traditional ESBs            ", "author": ["Matthias Feraga"], "link": "https://blog.octo.com/en/choosing-between-lightweight-and-traditional-esbs/", "abstract": "How to: choosing between lightweight and traditional ESBs Publication date 06/06/2011 by Matthias Feraga Tweet Share 0 +1 LinkedIn 0 Lightweight ESBs, compared to traditional ESBs as their name suggests, are an emerging type of simplified integration solutions that focuses on recurring needs. They do not bring additional integration features, and intend to address integration issues in a new way instead. Well-known solutions in this category can be counted on the fingers of one hand: ApacheCamel, MuleESB and SpringIntegration for now (note the impartial alphabetical order…). The goal here is not to compare the three of them but to appreciate what for sure they have in common: their efficient and productive nature . This article consists in a set of questions to address when dealing with applications integration. Many of these questions help indifferently to choose an exchange architecture, compare solutions or design flows. To cover the entire scope of applications integration the checklist breaks down in three parts as follows, focusing on what matters for lightweight ESBs: functional requirements, that means requirements for pure integration features, non-functional requirements as technical monitoring and deployment, and organisational aspects Functional requirements Let’s start with a little reminder: a reference stack of applications integration. Don’t panic! Off course they are many ways to represent it. This one is willingly simplistic for the proof of concept: this is typically the features covered by any traditional ESB. But who is really using all of these features at a time? Here we are: getting over the pitfalls of complex and heavy Enterprise Integration solutions. Connectivity and communication What has to be integrated? This simple question looks stupid but any integration initiative starts with it: validate the requirements concerning connectivity. If standard protocols only are used it is relevant to look towards lightweight ESBs. Nevertheless in the case some applications have to be integrated through a proprietary protocol, lightweight ESBs, and more generally open-source ones, could be very limited. For example, a common strategy is to use the ERP integration module and the protocol that comes with it, for performance, monitoring or errors management reasons. Same logic apply for legacy systems and sector-specific standards. Things are moving for Cloud integration, as MuleSoft announced its PaaS integration offer . A real lightweight ESB is finally available on the Cloud. Cloud integration patterns and trends were largely addressed in a previous article . It is well known that asynchronous flows are a preferred solution in many applications integration cases. Actually each time it is possible. Message Oriented Middlewares (MOM) such as JMS or AMQP are powerful on technical aspects but are very poor on all functional requirements. To bypass these limits, asynchronous messaging can be combined with a lightweight ESB to get the best of both worlds. Lightweight ESBs can interact with a distant Message Broker … …or embed it. Having such a choice is useful to tweak the robustness and availability of the entire solution. Routing and other messaging patterns Messaging patterns have to be chosen in view of business flows to implement. I invite you to identify the required patterns flow-by-flow against the complete list of Enterprise Integration Patterns . Although some patterns are overly complex most of them are fundamental to any integration solution. The three lightweight ESBs mentioned before are more or less strict implementations of EIP. More or less because not all patterns are supported. Note that to fill the gaps some higher level patterns can be home made from lower level patterns at the cost of … development. As underlined MOM are very poor concerning routing and transformation capabilities. Traditional ESBs require explicit configuration at various layers. This is where lightweight ESBs take place: bringing simplicity and efficiency by configuring only the required messaging patterns . Whether based on a Spring-XML or on a DSL configuration, lightweight ESBs rely on convention over configuration : for example an implicit protocol translator, an implicit router that does nothing special, etc. Transformation You may often ear “ no business in the integration middleware “. When it comes to data translation most organizations have well-defined rules on what to put in the integration middleware and what not. The only rule is that there are no rules! It depends on too many factors to be cast in stone and the point should be addressed systematically. Lightweight ESBs have the same mapping capabilities as traditional ones. Therefore same issues apply: whether to put the mapping logic on applications side or on the middleware side, whether to use Canonical Data Model or not, etc. Orchestration and more The term orchestration actually regroups two notions: technical orchestrations derive from the technical design of the flow (complex routing, errors management, etc.). Technical orchestrations only consist of Pipes & Filters or basic forms of Process Manager . business orchestrations are top-down implementations of business processes. Technically they use both Pipes & Filters and Process Manager too, but can involve more systems and potentially occur through a long period of time. Lightweight ESBs address technical orchestrations very well. However, only simple business orchestrations are supported as they do not provide any advanced mechanism for message correlation, hibernation, compensation nor business activity monitoring. Orchestration is one of the most influential criteria when comparing ﻿﻿lightweight with traditional ESBs. Once again it depends on the business value to put in the middleware layer. The lack of a business rules engine embedded in the integration solution is not penalizing as it intends to address different scenarios. Business rules engines schould be placed closer to the business logic with the most value, that means into applications. Some integration stacks include a Complex Event Processing (CEP) engine too. However, CEP addresses different integration scenarios. The precious time saved by the fast patterns detection (events recognition) of CEP would be nibbled by the complex events collection and notification through the various integration layers. CEP solutions provide their own API for message input and offer other forms of Message Router . That is why CEP do not require to be coupled to an ESB. Finally, lightweight ESBs do not offer business monitoring across multiple instances of a same flow definition. At most, what can be done is defining basic SLA such as timeout and overload protection. This is consistent with the type of orchestrations addressed. Non-functional requirements Errors management Although lightweight ESBs support the detection of errors ( Dead Letter Channel ) and anomalies ( Invalid Message Channel ), they do not offer any framework for resubmitting messages. This implies either resubmitting the message from the source system or performing a corrective action on the target system. ﻿That is mainly due to the fact that there is no native Message Store on the way. This is where lightweight ESBs still have gaps to fill. An alternative is to tool up with a custom error management to reproduce more or less what traditional solutions provide natively: The goal here is to tool up strictly with what is necessary for the errors management strategy. Deployment Time is coming to introduce another term: the integration framework . Both lightweight ESB and integration framework notions are very close. Integration framework could be considered as a deployment mode of a lightweight ESB. There is at least a big overlap between them. Once embedded in an application it takes the role of an Emissary . Such solutions are particularly powerfull to equip a Nova (a business domain central application) with mechanisms such as Messaging Gateway and Service Activator . The integration framework then acts as the integration module of a software package. This is typically the positioning of Spring Integration. Robustness and availability are greatly eased by embedding a lightweight ESB into an application: the integration system is not a bottleneck neither a single point of failure (SPOF) anymore. And over all there is no additional system in the infrastructure . One limitation with embedding the middleware into an application could be the impossibility to implement flows with a many-to-many cardinality (ie. N => P flows). But in real life flows with a many-to-many cardinality are very uncommon… when they do not result from a design problem. Lightweight ESBs can be indifferently embedded into an application or deployed as a standalone middleware. They support a large variety of light and heavy deployment technologies . The most pertinent way to deploy lightweight ESBs is using Spring. Avoiding dependency to an heavy container as JBI preserve their flexibility and ease of use. Performance The world (of ESBs…) is divided in two categories: ones with XML as the internal representation of messages, with systematic XML serialization and deserialization, ones with a custom binary internal representation or, depending on the context, no particular representation. The three lightweight ESBs aforementioned belong to the second group. That means they dot not suffer from serialization overhead. Added to the fact that their internal architecture consists of less layers and that these layers are more integrated together, they offer good performances. That being said there is not much surprises: performances are ones expectable from a JEE solution. Organisational aspects Architectural initiative The term lightweight ESB itself is a paradox as it is more efficient as a local services bus. Those solutions are perfect for a tactical integration project at the scale of a functional domain. The integration team is all the more reactive since it is interacting with fewer projects. It is recommended not to define peremptory rules but to diffuse best practices instead. A common shortcut is to expand the integration middleware to the entire organisation at any cost. Often by twisting it to host any kind of flows, or reusing flows that don’t have the same business meaning. Moreover the reuse rate of existing flows is significant inside a functional domain but decreases considerably beyond . Rightly or wrongly, the choice of an ESB is often made at Enterprise level, for strong infrastructure rationalization purpose. Even if the ESB is centralized, try at least not to centralize the integration teams too much. Keep teams autonomous and reactive instead. Technical competences Any JEE (Spring ideally) developer can do the job. This is a major point regarding the sourcing strategy : in most cases the deployment of a commercial ESB lead to outsource the expertise . Such alternative solutions are more affordable . The cost of access to the technology is lowered: developers can use their preferred development environment and capitalize on their JEE knowledge, without learning all the specificities of a vendor solution. Cost allocation and decision cycles The large scale deployment of ESBs always bring the same problem of cost allocation. The allocation on an Activity Based Costing principle is very difficult for shared flows. When confining the middleware to a defined perimeter it becomes a lot easier to identify entities in charge:  functional domains themselves. When a functional domain is responsible for its integration middleware, the time necessary to select a solution is reduced, as less actors are involved in the decision. The choice of an ESB for the entire Enterprise can take months and months. Since purchasing department and technical direction are not in the loop, a short decision cycle increases the time-to-market. Conclusion The main lack with lightweight ESBs is the impossibility to implement complex, long-living, business processes. That makes them inappropriate for an Enterprise top-down BPM approach . This limitation apart these light solutions cover all recurring integration requirements, in a more simple way . This is possible because they are built around integration best practices and conventions . If you can’t address your issue with Enterprise Integration Patterns , I invite you to double check your requirements. Also note that high level BPM scenarios are marginal in comparison to basic integration. Hopefully, there are more lightweight solutions to come, certainly among commercial ones. The reengineering effort to migrate traditional ESBs to the Cloud already lead vendors to simplify their solutions. ESBs are often deployed to integrate one and only one application (a shared repository, an ERP, a Nova) with the rest of the Information System. If integration middleware and application are tight coupled anyway, why not deploy both of them as a whole ? Lightweight ESBs can either be embedded into an application or deployed standalone. This is a precious option. Beyond these technical aspects, the use of a lightweight ESB can bring many organisational benefits , in which: Flexibility : since integration projects are here to fill the gaps between systems, they have to go through changes faster. The flexibility of a lightweight ESB make it a good candidate for agile projects . Cost transparency : most organizations are unable to identify and allocate costs associated with an integration project. Deploying lightweight ESBs locally make departments aware of their responsibilities. The ESB is not just a necessary IT cost anymore. (Re)internalization: it is much easier for a developer to become familiar with a lightweight ESB, giving the possibility to (re)internalize such technical competences. Lightweight ESBs offer a good trade off between the point-to-point spaghetti-ware and the over-centralized ESB . They are best suited for local initiatives at the scale of a functional domain or a business area. By keeping it tactical, the middleware does not turn into an administrative and technical bottleneck . Ultimately a motto for modern integration could be: make local successes! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged application integration , EAI , eip , enterprise integration , enterprise integration patterns , ESB , Intégration . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 9 commentaires sur “How to: choosing between lightweight and traditional ESBs” hans boot 07/06/2011 à 09:55 Il manque du texte dans Performance: The world (of ESBs…) is divided in two categories:\r\n•ones with XML as the internal representation of messages, with systematic XML serialization and deserialization,\r\n• (rien)\r\nC'est quoi la deuxieme catégorie? Matthias Feraga 07/06/2011 à 10:04 Merci c'est corrigé: ones with a custom binary internal representation or, depending on the context, no particular representation. James 07/08/2011 à 20:11 You should also mention MassTransit, NServiceBus and Rhino Service Bus in this category.  Additionally, each of these implements the notion of the saga to model complex, stateful long running business transactions. nalloukh 24/08/2011 à 15:42 J'ai aimé l'article, clair et concis.\r\nJ'aurais aimé savoir, si c'est possible, quel outil vous utilisez pour éditer les différents diagrammes?\r\nEncore merci pour cet article instructif. Matthias Feraga 24/08/2011 à 16:09 @James: thank you for your comment. I had a look at the 3 solutions you mentioned. It appears to me that these are not Enterprise-wide ESBs, as they are not providing many connecting, mapping and orchestration features. But I am certain they have their use cases! Thanks again. Matthias Feraga 24/08/2011 à 16:13 @nalloukh: Merci beaucoup pour votre retour.\r\nPour cet article j'ai utilisé Powerpoint.\r\nIl m'est déjà arrivé d'utiliser GoogleDocs Presentation, qui est largement moins abouti\r\nMa préférence reste de loin Visio, qui est le plus productif! bardamu 11/11/2011 à 16:07 Très instructif.\r\nMerci. which one to opt for MuleSoft ESB or IBM ESB 16/08/2013 à 18:46 Hi,\r\n\r\n    Can you please go over and give a brief comparison between MuleSoft ESB and IBM esb products. (or) May be like the problems with MuleSoft ESB for a very large enterprise. Thank you\r\n\r\nRegards\r\nP.Sharada Macarena 24/09/2018 à 13:41 Interesting post! Although I would like to open a discussion about WSO2 ESB, because In my IT department is working so good. Have you tried it? I invite you to read this post about it https://www.chakray.com/en/what-is-wso2-esb-the-enterprise-integration-solution/ Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-06-06"},
{"website": "Octo", "title": "\n                Securing a Flex/BlazeDS application with Active Directory Single-Sign-On            ", "author": ["David Hatanian"], "link": "https://blog.octo.com/en/securing-a-flexblazeds-application-with-active-directory-single-sign-on/", "abstract": "Securing a Flex/BlazeDS application with Active Directory Single-Sign-On Publication date 13/01/2011 by David Hatanian Tweet Share 0 +1 LinkedIn 0 Your CISO wants you to integrate your Flex/BlazeDS applications into Active Directory ? Sure, you could use the old fashioned LDAP way, but you certainly prefer remove the burden of typing once again their credentials from your users’ back. Get your users to love you by integrating you application to Active Directory’s SSO. Picture a web application with a  Flex client interface, calling business services through a BlazeDS channel. Before using said services from their Flex interface, users have to authenticate with their AD account. This article deals with the way to set up a transparent authentication by using the Kerberos protocol that AD implements. If things were just set up perfectly, BlazeDS would handle the Kerberos authentication all alone, unfortunately its current authentication capabilities don’t go further than the old login/password combination. But hey, a BlazeDS channel remains a Java web-service, so let us take advantage of this ! Enter Spring Security… Spring  Security framework provides Java applications with a whole bunch of  different ways to handle authentication. Starting from version 3, the  framework xml-based configuration got simpler. And, rejoice, Spring  Security supports Kerberos thanks to a recent extension. Now that every  component of our architecture is defined, let us make them  communicate together. Architecture This  diagram summarizes the interactions between the components of our  architecture to protect the BlazeDS  channel : Kerberos and Active Directory First published in the 80s by the MIT, the Kerberos protocol is an Active Directory’s default authentication mode which started off with Windows 2000. It replaces NTLM, which is not recommended by Microsoft anymore. The protocol is based on ticket exchanges with a domain controller. This diagram shows how those exchanges are orchestrated when a client accesses a Kerberos-protected service : (1) The user opens his Windows session with his AD credentials to use his workstation, as usual. (2) The workstation contacts a domain controller to check the credentials and get a session-related ticket called a TGT (Ticket Granting Ticket). (3) When the user accesses a Kerberos-protected service, Windows requests the Domain Controller for a ST (Service Ticket). (4) The DC sends to the workstation a ST encrypted so that it will only be used by the requested service. (5) The client can now authenticate himself to the service, no password is then requested. The DC identifies a service by its Service Principal Name (SPN). In AD, each service is registered as a user account, which is then mapped to a SPN during the generation of the service key with the ktpass tool. In our case, our web service’s SPN is something like HTTP/appserver.octo.com@AD-DOMAIN. Now, let us see how to integrate our service (the Flex application server part) with Kerberos. BlazeDS configuration Spring Security integrates in theory with BlazeDS through the Spring-Flex project. Unfortunately this support is currently limited to username/password authentication. Thus, we have no hope to take advantage of Spring-Flex for the Kerberos authentication. We detail our solution below. We use Spring Security URL filters to protect two types of resources : Static resources (html, css, …), including the swf file which is the client part of the application The BlazeDS endpoint (i.e. the server-side of the BlazeDS channel) Here is our Spring Security configuration file : <sec:http entry-point-ref=\"spnegoEntryPoint\" auto-config=\"false\">\r\n\r\n  <sec:intercept-url pattern=\"/**\" access=\"IS_ AUTHENTICATED_FULLY\"  />\r\n\r\n  <sec:custom-filter ref=\"spnegoAuthenticationProcessingFilter\" position=\"BASIC_AUTH_FILTER\" />\r\n\r\n</sec:http> The intercept-url tag is used to require the user to be authenticated. The custom-filter tag specifies that the authentication data will be processed by SPNego (see below). When accessing one of those URLs, Spring Securiy triggers authentication exchanges following the SPNego protocol. SPNego enables to negotiate the authentication mode used (for example NTLM or Kerberos) so that the client and the server reach an agreement on a mode they can both handle. The negotiation uses an HTTP 401 response from the application server, containing the SPNego headers which state that the server can handle Kerberos. Once the browser and the server have agreed on the use of Kerberos, the browser calls the Windows API to request the Domain Controller for a Service Ticket to get access to the application server. Then, the ticket is validated by the Spring Security Kerberos Extension by using the server’s own service key described in the Kerberos presentation above. If the ticket is validated, the user is authenticated and Spring Security authorization mechanisms (roles, ACLs) take place to grant or deny access to a given resource. The authorization phase is independent of Kerberos. But here’s the snag : using a HTTPS BlazeDS channel ( SecureAMFEndpoint ) requires to use the BlazeDS login mechanism consisting on the server of a LoginCommand class. So, we implemented our own LoginCommand ( PreAuthLoginCommand )… which does not bother to make any credentials check, the  authentication being actually handled by Spring Security before any access to the channel. public class PreAuthLoginCommand implements LoginCommand, LoginCommandExt {\r\n\r\npublic Principal doAuthentication(String username, Object credentials) {\r\n//On n’effectue aucune verification !\r\nlogger.info(\"doAuthentication \" + username + \", returning a PreAuthPrincipal\");\r\nreturn new PreAuthPrincipal(username);\r\n}\r\n\r\npublic boolean doAuthorization(Principal principal, List arg1) {\r\nlogger.info(\"doAuthorization \" + principal + \", returning true\");\r\nreturn true;\r\n}\r\n…\r\n} Getting further The article did not mention the client workstation configuration. It is very simple and consists of two points : Use a SPNego-Kerberos-enabled browser such asInternet Explorer, Firefox or Chrome Configure the browser to trust your application (thus allowing the application to receive Service Tickets from your browser). This configuration can be deployed on a whole set of workstations through Active Directory GPOs For further details see the “Links” section below. One last idea to improve your security architecture : give your users a way to logout from their Kerberos-related session in your application and ocasionnally get back to a password-based authentication. This will allow a user different of the AD account owner to authenticate. Links Spring-Security/BlazeDS integration through Spring-Flex. Works only with username/password authentication : http://static.springsource.org/spring-flex/docs/1.5.x/reference/html/#security The Spring Security Kerberos extension: http://static.springsource.org/spring-security/site/extensions/krb/index.html The extension’s manual (including Internet Explorer configuration) : http://blog.springsource.com/2009/09/28/spring-security-kerberos/ A guide to enable Kerberos in Firefox : http://grolmsnet.de/kerbtut/firefox.html A guide to enable Kerberos in Chrome : https://sites.google.com/a/chromium.org/dev/developers/design-documents/http-authentication Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Active Directory , Flex , security , Spring . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Securing a Flex/BlazeDS application with Active Directory Single-Sign-On” Raj 05/04/2011 à 11:19 Excellent article, Thanks !! thanan 21/01/2013 à 13:35 Hi,\r\n\r\nThis post is very informative. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-01-13"},
{"website": "Octo", "title": "\n                First steps with OpenERP            ", "author": ["Stéphane Teyssier"], "link": "https://blog.octo.com/en/first-steps-with-openerp/", "abstract": "First steps with OpenERP Publication date 31/12/2010 by Stéphane Teyssier Tweet Share 0 +1 LinkedIn 0 This article is an overview of the software OpenERP. Firstly, the company and the context in which the software evolves, are presented. Then the technical aspects of OpenERP are introduced: architecture, content module, management views and objects … OpenERP is … An open source ERP It is indicated in the name! OpenERP is an Enterprise Resource Planner . The scope of the tool covers (among others) the following domains: Customer Relationship Management – CRM Material requirements planning – MRP Project management Sale Purchase Marketing Human resources And of course, accounting OpenERP is licensed under the GNU AGPL (Affero GPL) which is an extension of the GNU GPL licence for network applications. In fact, you have the possibility to access the source code, modify it and use the software without paying a licence. A company OpenERP is promoted by the Belgian eponymous company, founded in 2005. At the beginning of 2010, the name OpenERP appeared in financial newspapers after a fundraise of 3 million euros, initiated by Xavier NIEL, the creator of Free – a famous french ISP. Beyond the standard business model of open source company – bug fixing, migrations, security alerts and training – OpenERP has a SaaS offer for 39 € / month / user. To ensure the support at a worldwide level, the company which has only 85 employees relies on a partner network. An ecosystem community oriented The environment OpenERP consists of 150 partner companies in the world but also of individual contributors (around 800) participating in the development of the application. The interaction between OpenERP and community contributors through Launchpad . Everyone can submit a bug and track correction, ask questions, suggest improvements, contribute translations, and finally access the source code. The contributors are divided into different categories: Core team Commiter : must have realised at least two working modules Drivers : active members Community : free access for everyone Thanks to this schema, more than 700 modules are available within the different branches – addons, extra addons and community addons. Technically In the rest of this article, we will refer to the version 5 of OpenERP; available on Windows, Linux and MAC. Architecture OpenERP is based on client /server in disconnected mode. Like any other ERP, it is mainly a data base – PostgreSQL – on which is added an application server called OpenERP server. This server supports the following protocols : Net-RPC et XML-RPC secured or not. This web-service server can be directly requested by a GTK client (provided by OpenERP), or a web server (called web-client). Server and client-web components are written in python. The framework OpenERP framework is called OpenObject. Its mechanisms are: Object-Relationship Mapping (ORM) integrated in Object Service (OSV). The aim is to avoid the developer to use SQL. Model-View-Controller (MVC) which manages inheritance of views. Report generation (PDF, HTML, ODT). Translations in .po files. Moreover all the OpenERP functionalities are grouped in modules whose state is maintained by OpenERP server. An OpenERP module The following structure is the advised one: Module name _terp__.py : meta file to descibe the module __init__.py : load the different python files XXX.py : business objects defined in python classes view : objets’views XXX.xml workflow : definition of workflows and processes XXX.xml wizard : screen sequences which handle OpenERP objects XXX.py : behavior of the wizard YYY.xml : view of the wizard i18n : translation files XXX.po security : access and user rights XXX.xml YYY.csv Python files define objects (database tables) with their behavior and functionalities whereas XML/CSV files load data (records in db tables). The module descriptor __terp__.py file is required to make your module usable. Here is an example with the base module (‘base’!) : {\r\n    'name': 'Base',\r\n    'version': '1.1',\r\n    'category': 'Generic Modules/Base',\r\n    'description': \"\"\"The kernel of OpenERP, needed for all installation.\"\"\",\r\n    'author': 'Tiny',\r\n    'website': 'http://www.openerp.com',\r\n    'depends': [],\r\n    'init_xml': [\r\n        'base_data.xml',\r\n        'base_menu.xml',\r\n        'security/base_security.xml',\r\n        'res/res_security.xml',\r\n        'maintenance/maintenance_security.xml'\r\n    ],\r\n    'update_xml': [\r\n        'base_update.xml',\r\n        'ir/wizard/wizard_menu_view.xml',\r\n        'ir/ir.xml',\r\n        'ir/workflow/workflow_view.xml',\r\n        'module/module_wizard.xml',\r\n        'module/module_view.xml',\r\n        'module/module_data.xml',\r\n        'module/module_report.xml',\r\n        'res/res_request_view.xml',\r\n        'res/res_lang_view.xml',\r\n        'res/partner/partner_report.xml',\r\n        'res/partner/partner_view.xml',\r\n        'res/partner/partner_wizard.xml',\r\n        'res/bank_view.xml',\r\n        'res/country_view.xml',\r\n        'res/res_currency_view.xml',\r\n        'res/partner/crm_view.xml',\r\n        'res/partner/partner_data.xml',\r\n        'res/ir_property_view.xml',\r\n        'security/base_security.xml',\r\n        'maintenance/maintenance_view.xml',\r\n        'security/ir.model.access.csv'\r\n    ],\r\n    'demo_xml': [\r\n        'base_demo.xml',\r\n        'res/partner/partner_demo.xml',\r\n        'res/partner/crm_demo.xml',\r\n        'base_test.xml'\r\n    ],\r\n    'installable': True,\r\n    'active': True,\r\n    'certificate': '0076807797149',\r\n} This file contains four lists: depends : modules to be installed before the current one init : files to be loaded during server startup update : files (re)loaded at each module update demo : files to be loaded when an installation is made with the ‘Load demonstration data’ option Furthermore, ‘category’ field determines the type of the module. We distinguish base modules, local ones (charts of accounts), … and especially profile modules which define installation templates for new data bases, at dependency level (required modules) or installation screens. Object Service Each business object must inherit osv.osv class to become persistent. The steps are: in __init__.py file: an import of the python file must be made (for example my_object.py will contain definition of ‘my_object’) in my_object.py : import osv and fields classes define ‘my_object’ class use the introspection mechanism to update the data base tables, calling ‘my_object’ constructor Here is an example with the ‘res.country’ class: from osv import fields, osv\r\n\r\nclass Country(osv.osv):\r\n    _name = 'res.country'\r\n    _description = 'Country'\r\n    _columns = {\r\n        'name': fields.char('Country Name', size=64,\r\n            help='The full name of the country.', required=True, translate=True),\r\n        'code': fields.char('Country Code', size=2,\r\n            help='The ISO country code in two chars.\\n'\r\n            'You can use this field for quick search.', required=True),\r\n    }\r\n    _sql_constraints = [\r\n        ('name_uniq', 'unique (name)',\r\n            'The name of the country must be unique !'),\r\n        ('code_uniq', 'unique (code)',\r\n            'The code of the country must be unique !')\r\n    ]\r\n\r\n    def name_search(self, cr, user, name='', args=None, operator='ilike',\r\n            context=None, limit=80):\r\n        if not args:\r\n            args=[]\r\n        if not context:\r\n            context={}\r\n        ids = False\r\n        if len(name) == 2:\r\n            ids = self.search(cr, user, [('code', 'ilike', name)] + args,\r\n                    limit=limit, context=context)\r\n        if not ids:\r\n            ids = self.search(cr, user, [('name', operator, name)] + args,\r\n                    limit=limit, context=context)\r\n        return self.name_get(cr, user, ids, context)\r\n    _order='name'\r\n\r\n    def create(self, cursor, user, vals, context=None):\r\n        if 'code' in vals:\r\n            vals['code'] = vals['code'].upper()\r\n        return super(Country, self).create(cursor, user, vals,\r\n                context=context)\r\n\r\n    def write(self, cursor, user, ids, vals, context=None):\r\n        if 'code' in vals:\r\n            vals['code'] = vals['code'].upper()\r\n        return super(Country, self).write(cursor, user, ids, vals,\r\n                context=context)\r\n\r\nCountry() osv.osv class defines specific attributes and methods. In addition, the fields of an object can be simple (boolean, integer, datetime, …), a reference to other business objects (many2one, many2many, One2Many) or associated with a specific function Python. For each field, the attributes string (text), required, readonly, help (tooltip help) or select (field defining a search criterion) can be specified. Among osv.osv attributes, there are: _name (required) : the object (db table) name _columns (required) : the list of all the object fields _defaults : initialization of fields _inherit : the inherited object. If the attribute _name is identical to that of _inherit, compatibility is ensured between views. Otherwise, a new table is created. There is also an attribute _inherits that allows multiple inheritance. _sql_constraints : SQL constraints with the following format (‘constraint name’, ‘SQL definition’, ‘message’) Every osv object has the methods : create, search, read, write, copy, unlink, browse (allows to use the dot notation to access object fields), default_get (get default values), perm_read (get the user id who created this object), fields_get (return a field list with their description), name_get, name_search, import_data, export_data. It is possible to override these methods – like in the ‘res.country’ example, for name_search, create and write functions. Besides, in most of the the prototypes of osv.osv object, there is a data base cursor to directly handle SQL requests. An object representation (screen) is an XML record in a specific table. Different types of views exist but we mainly use form and tree: form : display a single object. Used in edition mode. tree : display an item list. For example, res.country object has these views: <record id=\"view_country_tree\" model=\"ir.ui.view\">\r\n            <field name=\"name\">res.country.tree</field>\r\n            <field name=\"model\">res.country</field>\r\n            <field name=\"type\">tree</field>\r\n            <field name=\"arch\" type=\"xml\">\r\n                <tree string=\"Country\">\r\n                    <field name=\"name\"/>\r\n                    <field name=\"code\"/>\r\n                </tree>\r\n            </field>\r\n        </record>\r\n\r\n        <record id=\"view_country_form\" model=\"ir.ui.view\">\r\n            <field name=\"name\">res.country.form</field>\r\n            <field name=\"model\">res.country</field>\r\n            <field name=\"type\">form</field>\r\n            <field name=\"arch\" type=\"xml\">\r\n                <form string=\"Country\">\r\n                    <field name=\"name\" select=\"1\"/>\r\n                    <field name=\"code\" select=\"1\"/>\r\n                </form>\r\n            </field>\r\n        </record> Which display: Elements composing a view can be: an object field, a button, a separator, a label, a group of elements, notebooks … A set of attributes apply to these elements (readonly, required, invisible, string, widget (url, email, progressbar, …), …). Readonly, required and invisible attributes may have a value depending on object fields, to create dynamic views. As is, some invoice parameters are only editable when it is in draft state. To finish with views presentation, it is possible to make inheritance between views. The objective is to replace or add elements using eventually XPath queries. Configuration Through a module Data loading in OpenERP is commonly made by XML files. Each file must fits with the following structure: <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<openerp>\r\n    <data noupdate=\"1\">\r\n\r\n        <record id=\"entry_id\" model=\"table_name\">\r\n            <field name=\"field1_name\">value</field>\r\n            <field name=\"field2_name\" ref=\"referenced_id\" />\r\n            <field name=\"field3_name\" model=\"table_name\" search=\"[('field', 'operator', 'value')]\" />\r\n        </record>\r\n\r\n    </data>\r\n</openerp> An XML record must provide the id and model (name of the db table) attributes. As id can be absolute or relative, records defined in XML files can be modified by another module: inside a module named ‘module_A’, I create a new record whose id is new_entry_A <record id=\"new_entry_A\" model=\"table_name\">\r\n              ... in another module (module_B), I modify the record registered in data base whose id is new_entry_A <record id=\"module_A.entry_A\" model=\"table_name\">\r\n              ... In the case of relational fields, attributes ref and search can link XML records together. ‘Ref’ is to specify an absolute or relative identifier previously loaded. ‘Search’ is a criterion search in the form of a list of tuples: [(‘field_name’, ‘operator’, ‘value’), …] ‘Noupdate’ attribute defines the system behavior during a module update : will entries in data base be overriden by XML content ? This attribute is to be handle with care to avoid lost of user modifications. OpenERP also manage CSV files. In this case, the file name must be the one of the SQL table where insertion must be made. In addition, the names of columns in the CSV file must be identical to the columns of the table. CSV allows to load important quantities of data, like client references, products or financial accounts. Through GUI The GUI also allows the modification of all business entries, customization of views and definitions of objects: Moreover, changes made by users are recoverable through the module ‘base_module record’ that can generate XML files corresponding to changes made since the start of a recording session. Conclusion In this article , we have only addressed a few part of OpenERP aspects. Numerous concepts like workflows, processes, reports and user rights have not been presented. To be continued. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged ERP , Open source , Opensource . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-12-31"},
{"website": "Octo", "title": "\n                How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021            ", "author": ["Christophe Durand"], "link": "https://blog.octo.com/en/how-to-deal-with-an-inverse-conway-maneuver-a-talk-by-romain-vailleux-at-duck-conf-2021/", "abstract": "How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 Publication date 25/03/2021 by Christophe Durand Tweet Share 0 +1 LinkedIn 0 Romain Vailleux, OCTO Technology Companies are complex systems made up of humans and technological systems in perpetual interaction. “Theses socio-technical” systems are Romain’s favorite subject. Melvin Conway observed that the communication structures of organizations directly influence the design of technical systems produced by those organizations. In short: the organization chart of the company and the interpersonal relationships across people have more influence than designers and architects! The principle of an “Inverse Conway Maneuver” is simple: to transform your business at a lower cost,  first look at and influence how the teams interact. The rest will follow… Inspired by the bestseller Team Topologies: Organizing Business and Technology Teams for Fast Flow by Matthew Skelton and Manuel Pais, Romain gives some advice on how to put it into practice, reorganize your teams to better respond to the strategic challenges of your company and, at the same time, to design learning organizations where the individuals emancipate. The company: a complex socio-technical system Romain Vailleux, consultant at OCTO since 2015, is regularly contacted by business leaders who complain about the limits of their information systems: “My CRM is not omnichannel; our mobile application is late; my API project is going crazy….” The company is a complex system made up of humans and technological systems bounded by continuous interactions. This “socio-technical” system is twofold: The “Socio” side : human, sensitive, teams and their interactions The “Technical” side : machines, logic and rigor of processes and capital These two dimensions interact within a single complex system, oscillating around a point of rarely optimal equilibrium. This system actively and valiantly resists any attempt of transformation… Can you see the problem? How do we create balanced and durable changes in the socio-technical system embodied by our companies? Within an optimal spending of time and energy? Conway’s law Since the 1960s, Melvin Conway has also been interested in “socio-technical” systems. The law that bears his name ( Conway’s Law ) reads as follows: “Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization’s communication structure.” This law is cited extensively by OCTO consultants who spot these patterns in companies and consistently find that the structure of the company’s organisation influences the design of IT architecture and rarely the other way around. Inverse Conway maneuver This bold move is to use Conway’s Law to indirectly achieve our end goals: to transform your business at a lower cost, modify its communication structures to influence the emergence of optimal architectural designs. The real question to ask is then: what is the right organization to reach a given architectural target? In practice: Team first! The book Team Topologies: Organizing Business and Technology Teams for Fast Flow (Matthew Skelton, Manuel Pais) offers us a new useful modeling tool to study, discuss, and clarify the structure of communication between teams. To design optimal teams, two factors are essential : Limit the cognitive load (business, technical, infra) of the team to avoid overflow and maintain a learning capacity. Choose a team size which facilitates interactions. It is not easy to divide 150 people into several teams. Dunbar’s work and other significant numbers come in handy: 150 : the number of people with whom you can maintain a relationship / recall simultaneously ; 50 : the number of people with whom you can maintain a relationship of mutual trust ; 15 : the number of people with whom you can maintain a strong relationship of trust ; 5 ( +/- 2) : the number of simultaneous objects that you can instantly remember (to reconcile with the cognitive load). This is also the maximum number of people with whom you can maintain a close collaborative relationship. Focus on small teams and low cognitive load Finding the optimal balance between team size and efficiency means positioning the cursor between: Team synchronization versus inter-team synchronization Having all the skills in the team versus create dependency on other teams Strongly coupled software bricks versus tending toward a decoupled architecture The authors of Team Topologies: Organizing Business and Technology Teams for Fast Flow identify four types of teams : Stream-aligned team : flow and responsiveness, dedicated to achieving business objectives. Three other types of teams whose sole objective is to remove obstacles in the way of stream-aligned teams, and to minimize the cognitive load: Enabling team : disseminate good practices and increase the level of know-how of the teams (consulting / coaching / mentoring / training …) Complicated-subsystem team : bundle cutting-edge skills and specializations into a product made available to the rest of the organization Platform team : simplify the use of common bases that accelerate the implementation of products The 4 types of team of the model Team Topologies: Organizing Business and Technology Teams for Fast Flow Finally, 3 modes of interaction between teams are proposed in this model: Collaboration : Operating with strong interdependencies, two teams intensify their interactions in order to finely adapt the integration of their software products. The mode of integration is therefore designed rather ad hoc with the objective of efficiency and specialization. The collaboration mode is also suited for addressing  unique situations and uncertain contexts. X-as-a-Service : A mode of interaction favoring decoupling and standardization. A team makes its product available via standardized interfaces. The department’s production team adopts a “product” culture. Facilitating : A temporary relationship between two teams with the aim of transmitting skills in a sustainable way from one to the other. The 3 modes of interaction of the model Team Topologies: Organizing Business and Technology Teams for Fast Flow The target organization is then designed and assembled like Legos® by successively adding the teams and the appropriate modes of interaction. Example of a combination of the 4 types and 3 interaction modes In practice: assembly of a real case New target organization The organization is designed and assembled step-by-step, taking into account the challenges and constraints of the company: Issues & constraints Team Mode of interaction Resolve strong dependencies: “The PIM and CMS need to be synchronized very frequently and work in tandem.” Complicated Subsystem team “Products” Collaboration Take into account the corporate strategy: “Our core business is to provide a customer experience that reflects the quality and innovation of our products.” Stream aligned teams “Front Web” et “Front Mobile” Collaboration with Products Support transformation projects: “Omnichannel means making CRM information and OMS services available to all channels.” Complicated Subsystem team “CRM” et “OMS” X-as-a-service with “Products” Combine rare skills with a skills transmission mission: “We opted for the Xforce solution on several bricks. But experts in the market are few and far between. ” Enabling team “XForce” Facilitation Take into account the state of maturity of the application market: “The means of payment are multiplying. Developments in the payment block must benefit all sales channels. It must be able to be decommissioned / replaced easily if we change partners.” Complicated Subsystem team “Service Paiement” X-as-a-service with “Products” Sharing best practices: “In addition, we noticed that certain best practices in the implementation of the payment service allowed us to earn 34 conversion points.” Complicated Subsystem team “Service Paiement” Collaboration with Products “Front Web” and “Front Mobile” Your model will evolve through time The model is not set in stone; it will be initialized and then adjusted according to many evolving parameters: The mode of application integration The business strategy The maturity of application markets The maturity of the teams The existing architecture The law / risk (eg. Separation of responsibilities) Some takeways Bonus : The Sociotechnical Architect toolbox The talk video (French) is available here. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile , Archi & Techno and tagged conway . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2021-03-25"},
{"website": "Octo", "title": "\n                New Year’s Resolutions: Shed those excess pounds (from my Google inbox)!            ", "author": ["David Argelliès", "Alexis Nicolas", "Christophe Durand", "Natalie Schmitz"], "link": "https://blog.octo.com/en/new-years-resolutions-shed-those-excess-pounds-from-my-google-inbox/", "abstract": "New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Publication date 14/01/2021 by David Argelliès , Alexis Nicolas , Christophe Durand , Natalie Schmitz Tweet Share 0 +1 LinkedIn 0 TL;DR → Smart links to quickly clear out your Gmail inboxes Do you know your Do you know your current GMail storage usage ?   How much is dead weight? To ring in the new year, I decided to finally buckle down and clear out my GMail inbox . I spent a total of 10 minutes and, just by cleaning out old emails, my inbox dropped from a size 16GB to a mere 4GB ! There are more than 650 people at OCTO. If we all do a little inbox maintenance, we can quickly remove dozens of To from Google servers … Of course, great SaaS tools are available on the market to automatically clean up your inbox but they all require custom authorizations that aren’t necessarily in line with your (or our) company’s security policies. Fear not, there are other clever ways to free up space and lighten your footprint. Just between us, we have a few tips to share to help you painlessly sort through that mountain of old emails and clean house in just a few clicks. You can either tidy up your inbox on your own or, better yet, you can take a couple extra minutes to prepare personalized criteria for your company that can then be shared with your team. Below are some quick links that make use of smart sorting filters and save you time in cleaning out your Gmail inbox. These sorting filters allow you to select and delete (don’t archive ) emails you don’t want or need to keep. Start by clearing out the obvious Old Calendar/Event notifications (the link’s settings are before Dec 1-2020 but you can change the period in the search bar) All those “no-reply” emails (here the setting’s before June 30-2020 but you can change the period in the search bar) Then drop the dead weight… Attachments are the largest part of your inbox The greediest consumers of inbox data storage are media and the files (pptx, prez pdf) sent as attachments. Use these links to see what you’ve got kicking around: Emails with large attached files (here it’s 10Mo prior to Jan 1st-2020) Attached files older than a year and larger then 1Mo pptx files older than 3 months You can also unsubscribe from any newsletters or promotional emails by displaying emails with the Gmail “Promotions” tag that include an ‘unsubscribe’ link And, finally, take a moment to create sorting criteria adapted to your company—and then share it with your colleagues Start by answering a few simple questions: Do you use Google Group or other discussion groups that send email notifications? Do you use SaaS that sends email notifications? What are the typical keywords used in your company’s notification emails (for example, billing@, sales@, info@ etc.)? What email addresses usually send out your company’s notifications? and ABOVE ALL: which are the senders and keywords associated with messages THAT MUST NOT BE DELETED? Now you can quickly create clever sorting filters perfectly tailored to your business context: first, prepare sorting criteria directly in a Gmail page using search operators then, when you are happy with your search, just copy the URL of the ‘search results’ page. The search URL has integrated your search criteria and can now be directly shared with any Gmail user. Nifty! For example, searching for messages prior to Dec 1-2020 using keywords [my search] or [other search] but excluding [DO NOT DELETE] would show up like this in the search bar: before:2020-01-01 “my search” OR “other search criteria” -“DO NOT DELETE” And the corresponding URL would be: https://mail.google.com/mail/u/0/#search/before%3A2020-01-01+%22my+search%22+OR+%22other+search+criteria%22+-%22DO+NOT+DELETE%22 Custom Sorting IRL: OCTO Here is an example of some clever sorting filters that are adapted to our specific context here at OCTO: ‘Confluence’ notifications: We widely use Confluence, which sends useful notifications. But as with any notification, these become useless after a few days or weeks. Therefore we use search criteria based on [ from:confluence@atlassian.net ] The related link here is https://mail.google.com/mail/u/0/#search/from%3Aconfluence%40octo.atlassian.net Notifications from ‘Google Group’ : We use google groups for internal communication and therefore receive a lot of notifications and “ Récapitulatif Partiel ” (daily or weekly recap) Therefore we use search criteria based on [ “Récapitulatif partiel” ] from:name_of_the_google_group@octo.com [Standardized Keywords] we use in our standardized or automatic internal emails We then compiled this into an email, included all the sorting links, and sent it out to every Octo via a mailing list. That’s it! A few traps to avoid! ATTENTION/WARNING/AVERTO/HADHARI/ʻLELO AʻOAʻO/ATENCIÓN/KEIKOKU/VARNING If your Gmail is set to “Conversation Mode” : then when you delete a post from google groups, you also delete private replies that followed a public post. Remember to Delete and not Archive the messages that you want to permanently delete from your Gmail. Afterwards, deleted messages are still accounted for in your storage total as long as they remain in Trash . You’ll have to “Empty Trash” or wait a few days for Gmail to do it on its own. But take care! —after that, your deleted messages will be gone forever with no way to get them back if you’ve made a mistake. Be sure of what emails you’re deleting before sending them the way of the dodo. And please remember not to hold us responsible for any errors or faulty manipulations! Feel-good moment: Step back on the scale! Display the new data storage usage for Gmail again and bask in the warm glow of your accomplishments. If nothing has changed, try emptying your Trash and check back in a little while… Delayed gratification is all the sweeter! How many kilos of CO2 were shed? According to studies and assessments made by the ADEME dating back to 2011 , a single email has a carbon footprint of 19 grams of CO2 (Sorry… the page is only available in French for now). Sadly, most of this was emitted when the email was sent…But deleting them still counts! While we clearly won’t change the world by cleaning out our Gmail mailbox, our own OCTO Carbon Footprint study shows us that every initiative counts . Beyond that, there is another purpose to all this: by visualizing these hoarded GB of obsolete data, we can collectively become aware of our digital usages and of how it impacts the planet. So beyond deleting our emails, we should also strive to reduce traffic and network usage; to send less data, to fewer people; and, above all, to replace our digital toys a lot less often. Deleting our emails doesn’t always translate into real impact up the chain; while it could technically lead to fewer Google servers and fewer network components (both in terms of consumption and manufacturing), it remains to be proven. But it’s a step in the right direction. We find this to be an exciting and complex subject which we discuss in detail here . Sure, deleting emails gives you the warm-and-fuzzies and is helpful for the environment, but the real issue lies in rethinking our methods and changing the way we use our devices to become more sustainable at the office, with the ultimate goal of becoming better, greener citizens and workers in 2021. Frédéric Bordage wrote a useful list of best practices to follow to help lessen our digital footprint while at work / GreenIT.fr The ADEME website is also a must, including its page dedicated act at the office for the ecological transition and its publications , in particular the practical eco-responsible guide to the office ( le guide pratique écoresponsable au bureau ) ou Le guide pratique face cachée du numérique . (all doc in french) Next weightloss target: Google Drive “Tidying my inbox is cool and all, but my Google Drive seems a bit overweight too. Got any tips to help me sort through those files?” Google Drive, as you might imagine, is a bit more complicated. It’s not just sorting files but managing the different statuses and rights associated with each Google App folder and file. Unfortunately, GMail’s search operators don’t work in Drive at all. Google Vault can help but it’s mostly used by domain admin for legal backup purposes. For starters, you can: Sort your Google Drive files par size = https://drive.google.com/drive/u/0/quota (if necessary, click on “Size” at the top to display files from largest to smallest) Pull up your older Google Drive files https://drive.google.com/drive/u/0/search?q=owner:me%20before:2019-01-01 We’re still working on this but will provide more intel in a second blog post we hope to be posting soon. If we have any bright ideas, that is! That will be our good resolution for 2021, promise! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Change Management , Digitalisation , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2021-01-14"},
{"website": "Octo", "title": "\n                Technical Due Diligence–Safeguarding your IT Startup Investment            ", "author": ["Sylvain Fagnent"], "link": "https://blog.octo.com/en/technical-due-diligence-safeguarding-your-it-startup-investment/", "abstract": "Technical Due Diligence–Safeguarding your IT Startup Investment Publication date 11/12/2020 by Sylvain Fagnent Tweet Share 0 +1 LinkedIn 0 Or, how to invest then add value to your startup portfolio Translated from French by Natalie Schmitz. Original French article : Due diligence technique – sécuriser son investissement dans des startups IT Introduction While many companies are still reeling from the ongoing coronavirus crisis, startups are taking the hardest blows. The French government injected 4 billion euros into the sector to help keep them afloat. For BPIFrance , the talk of the season is on which horses to bet. The pandemic has bred frailty and imposed changes in course or even accelerations. These upheavals have, in turn, generated investment and buyout opportunities . Short- to medium-term positions can be leveraged, especially for firms and investors whose liquidities haven’t made like an iceberg and melted. Lockdowns have forced some startups to mutate and investments in equity to evolve while still retaining momentum . Digital transformation by necessity We’ve written 2 articles that try to give the best answer to how to safeguard an investment in a startup where IT is an integral part of the asset . And, more specific to investor companies, we’ll also talk about how to boost returns on startup portfolios and assets; how they can be accelerated; and how to get a leg-up in the day-to-day . To address these topics we’ve capitalized on our intelligence analyses and on our expertise gained during years of audits and technical due diligence reports. Before you invest, be sure to do your (technical) due diligence Before committing, investors—companies, Venture Capitalists—first look into business models and legal trappings (patents, field of business, territory of application) before focusing on a startup’s technical and digital competencies. This means verifying the startup has the technical capacity to scale up and to hedge the investment as much as possible . As we’ve gained experience and technical, methodological, UX, and product expertise over the years, we’ve also learned to get quick assessments during our audits and other technical DDs ( Tech DD for startups) by conducting our broad spectrum interviews (from operations and management to development teams) while performing a code audit. “Our main goal is to avoid financial commitments in companies that are at risk from a technical standpoint. To put it in IT terms, we say GO/NO GO on the investment.” Our 360° analysis includes everything from product roadmap to IT infrastructure, without bypassing organization, development industrialization, or security (as detailed below). This means we’re lightning fast at detecting macro trends that indicate whether an investee is or is not technically “robust” and whether their IT system is capable of scaling up.     We go into what we call “red flag mode”. As you know, a red flag is a warning signal that, in this case, tells us whether there might be something that could hinder the startup’s development down the line . In a nutshell, doing proper due diligence means: Detecting technical red flags, especially in terms of software production Checking that the startup is prepared to scale up Increasing value of the IT asset: “Build or Buy IT Asset” Providing a substantiated opinion complete with recommendations for the startup. An illustrated overview of technical due diligence . — OCTO’s Due Dil Deal : Check a startup for roadworthiness before you invest — – CEO : “Our “Startup” revolutionizes transatlantic travel thanks to the gig economy and our peer to peer blockchain dinghies. We only need 10M€ to get to the States. You in? “ – VC : ? – The seagull : ? – Developers team : “But…Will it float ?” Our Audit Framework We’ve split our Framework into 13 criteria that cover everything from roadmap to security, including documentation, team organization, code, and resilience. These criteria not only overlap but are interdependent. We broach them broadly during our interviews—all but the code audit which stands alone and can be operated independently. The code audit will occasionally hinder our post-interview analyses but most often it bolsters them or provides better accuracy. The combination of both code audit and interviews ultimately means our final diagnosis is reliable, and that we can be confident in the transparency of the teams we’ve met . While the end product of the audit process is a substantiated opinion submitted to the investor, we typically (and with the investor’s consent) also provide recommendations for the startup that was audited. These recommendations are then shared and can be included in a co-created roadmap between investor and startup. We’re under NDA (Non Disclosure Agreement) during these audits, which means there are strict requirements around the deletion of documents and audited source code, and that contracts are rife with IP (intellectual property) protection, non-compete, and anti-poaching clauses. And now, a detailed look at the Framework’s key themes Roadmap We always start by meeting the startup’s executives to hear their story: where they’ve been, why they started this company. Then we ask them where they want to go; which, in practice, means we ask to see their short- and medium-term roadmap . This step is crucial if we are to accurately gauge whether the startup is adequately sized and equipped to handle its roadmap from an organizational and technical point of view. Finally, we make sure that the startup’s technical teams are aligned with this roadmap . HR risks, methods & organization, development standardization Our years of technical DDs have taught us that the best startups have the most talented IT teams. They’re comprised of good organization, autonomy, and the ability to deliver quality software that is well-architected (state-of-the-art), well-mastered and in line with business objectives. These technical teams are usually also in line with the startup’s objectives and business goals. The most mature teams seem to be more humble and better equipped to adapt to a shifting environment. They also tend to be aware of the limitations of their organization and of their software (technical debt), constantly on the lookout for ways to improve them by consistently seeking out learning processes. “High levels of technical and organizational know-how means the team will be able to resolve any difficulties as they arise.” As we’ve said, the best startups have the most talented teams. It is therefore crucial to make sure, especially during the seed stage, that key individuals are properly incentivized (company equity, performance-based bonuses…) to keep them on for the next few years, at least. We’ve also found that a DIY approach, and automated operational processes and software delivery are strong drivers for success that allow development teams to regularly, frequently, and safely deliver code that goes straight into production. Automated testing is part and parcel of their dev cycle and we increasingly see QA-type profiles that contribute to raising the bar for software quality. Over the years, as we’ve gained experience and measured ourselves against some of the better teams we’ve come across, we’ve continuously improved our benchmark. Software architecture and code, IS management When performing a software audit, we usually combine automated (using tools) and manual (by core sampling the code) methods. Quick reminder: we’re still under NDA at this point, of course. Software source code is audited using automated tools, like SonarQube, that provide objective quality metrics . We then have expert software developers manually review and audit the code . During the code review analysis , depending on which technologies were used, we call on software development experts that have completed OCTO’s “software craftsmanship” training framework . Performance & scalability, resilience & recovery To evaluate a startup’s capacity for scaling up, we start by gauging whether its IT system can follow suit and at what cost . We measure whether the architecture, the components/the platform are able to technically withstand a load increase and what cost this will have on the infrastructure down the line (cloud infrastructures and/or partner and third-party service solicitations in particular). When necessary, we see whether performance testing should be implemented and inquire on a monitoring strategy to keep close tabs on costs and performances. In terms of architecture, we evaluate monitoring and automation practices —they should give teams the ability to quickly understand, respond, and adapt the platform should the need arise (scalability of infrastructure, on-the-fly management of services, unplugging components, absorbing peak loads, etc.). If need be, we evaluate all performance test protocols —we especially check the thoroughness of the measurement protocol and any related upgrades. As for resilience, we make sure to look into setting up and testing a BCP (Business Continuity Plan) and/or a BRP (Business Recovery Plan). PRA/PCA Data management and integration Where data is concerned—beyond the usual ponderings on technical/application integrity management, hosting, or backups—we take a look at the protocols set up around GDPR and, when applicable, question their seaworthiness: conditions for accessing data, processing, cross-referencing and consent, the right to be forgotten, GDPR reference persons within the startup, or GDPR-compatible API design … Security We don’t perform a security audit per se. We run thorough checks by evaluating management of data flows (data exchange, API security, analysis of incoming requests to detect attacks, etc.), of IS access (internet, databases, etc.), and by monitoring the obsolescence of embedded frameworks along with any relevant updates. If necessary, we can perform a quick audit by following OWASP recommendations (using ZAP, for example) which we typically supplement with a static code scan (Checkmarx) and manual analysis (see above). We can also call into question the security measures that are applied to employees—access to the premises, laptop encryption, use of password managers, whether a guest WIFI is available…. Conclusion The goal for any investing company is to avoid becoming financially committed to a technically risky startup. Our audit means we can properly assess the technical sustainability and risk of the startup. Our assessment relies on our 360° due diligence framework, whose main purpose is to provide an accurate look at the startup’s technical and organizational ability to scale up, so as to provide reasonable reassurance that the investment is secure . Doing due diligence means you can: Detect technical red flags, especially in and around software production Make sure the startup is prepared to scale up Increase the value of the IT’asset: “Build or Buy IT Asset” Provide a substantiated opinion complete with recommendations for the startup You might have heard there’s been a pandemic. In the wake of the crisis, we put our analysis framework under internal review and made some changes. We’re now able to assess a startup’s capabilities for evolution during this time of particular turmoil , with particular focus on its ability to adapt its product, IT, or organization. This type of crisis really brings the deep-rooted elements of a company’s DNA to the fore—its culture, its executives, its employees. We keep our position as legitimate experts on “software production” and how it, and organizations in general, have been affected by Covid. So…What now? Once they have received investments, it befalls us to help support startups on their path to growth. The next article will focus on investing companies , and try to provide answers to issues such as: how to add value to a startup portfolio, how to add value to those assets, and how to help with acceleration and be of general assistance day-to-day . Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged innovation , investment , portfolio , scaling , startup , VC , venture capitalist . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2020-12-11"},
{"website": "Octo", "title": "\n                A Journey To build a Business-Driven Data Science Capability            ", "author": ["Karl-Eduard Berger"], "link": "https://blog.octo.com/en/a-journey-to-build-a-business-driven-data-science-capability/", "abstract": "A Journey To build a Business-Driven Data Science Capability Publication date 24/02/2021 by Karl-Eduard Berger Tweet Share 0 +1 LinkedIn 0 Introduction We live in a world in which data is becoming a key business asset, playing an increasingly central role in the success of a growing number of companies. Data and the ability to transform data into business will become critical in all sectors in the coming years. A good mastery of data and its exploitation can be an important competitive advantage. This means that companies need to equip themselves with the technological and human resources to develop a skill set capable of leveraging data, applying analytical approaches and implementing new dedicated technologies. Why a Data Science Capability? Firms are in the process of setting up data-driven capabilities both internally (focus on the enterprise) and externally (focus on customers). Businesses will have to go through a massive digital transformation , including cultural and operational change . This is due to the fact that many companies still apply guidelines from the previous century while using 2020’s technologies. One way to update some guidelines on AI projects consists in building a Data Science capability that can help them to make this digital transformation a success. Business Stakes & Needs Currently, companies are aware of the importance of including AI in their processes. The stakes are increasing the financial Return On Investment, reducing risks or saving operational time. This awareness is translated in several ways: either by adding an AI vision in the definition of the top management’s strategic roadmap to increase market share, or by developing solutions based on AI in the search for disruptive innovation and information exploitation providing a competitive advantage, or by increasing human capabilities by offering decision support solutions or products. Despite a willingness to transform and add AI elements to current processes, companies may encounter difficulties that can minimize the potential associated AI  value . These difficulties can be found at every organizational level. At the top management level , there may be a lack of conviction at the sponsor level or a lack of awareness of the opportunities associated with AI, resulting in a lack of a clear global strategy for the deployment of AI-based tools and solutions. At the middle management level , there may be a willingness to develop AI solutions but, very often, a lack of structure and organization to deploy into production Proof of Concepts (POC) resulting in the multiplication of unused POCs and therefore a loss of investment. In addition, a lack of communication between managers can lead to redundant investment for the same project in different teams and geographies within an organization. At the executive team level , the absence of talent or the possibility to develop this talent can also cause a lack of capacity to carry out projects that are very promising and rich in learning. From a global perspective, there are major difficulties due to a lack of conviction about the possibilities offered by a data driven company because of a “guts-driven” habit or, ironic but true, a history of investing in technology in the past that was too expensive to allow new investment in new technologies. The introduction of a Business-Driven Data Science Capability can be a solution allowing an easy transition in the implementation of AI technologies at a lower cost and offering an approach facilitating the transformation towards a data driven company. What is a Business-Driven Data Science Capability (BDDSC)? A data science capability is an ecosystem that blends the data, skills, technologies, techniques, culture and processes needed to facilitate the integration of AI within an organization. The objectives of this capability are multiple and concern different themes around data , business and industrialization . From a Data Aspect Data is everywhere and must be handled rigorously and precisely in the same way as the development of a product or an offer. This is even more true while digitalization takes ever greater hold resulting in the emergence of devices/services that generate more and more data . In addition, these data can be leveraged by AI use cases to generate new business insights that can be used by companies to better control their activities and thus generate new data. This growth offers new perspectives, insights and new business action levers for all organizations. In fact, becoming data-driven is the new table-stakes for enterprise success (cf. Accenture and BCG publications). This is the reason why the capability must possess and share a global data lineage of the different data existing within the company. The lineage allows to locate all datasets (and their associated data dictionary) and their usages (if possible) within an organization and answers questions ranging from the origin of data to its journey through the organization. Today, most business enterprises are using various types of data lineage software which also includes ETL (Extract, Transform, Load) operations to clean and prepare data, therefore to get leverageable data. Having a clear vision of the Data within a company is essential because by knowing and mixing data from several entities, it is possible to create unsuspected business value. However, data lineage processes may be time consuming and therefore slow down the value generation process, particularly in constrained time environments. An alternative consists in identifying use cases first and assessing the related data acquisition complexity .  This alternative makes it possible to advance slowly but surely in the digital transformation while identifying the obstacles and needs. This identification makes it possible to define the necessary projects to be implemented to allow the organization to evolve at its own pace according to the available resources and thus establish a roadmap of the next actions to be performed. This approach is usually preferred but does not dispense the data lineage process during the use case realization time. From a Business aspect The BDDSC role consists in acculturating and sensitizing the business to the fact that AI is driving the new Industrial Revolution. AI adoption is growing faster than many had predicted. Research from a recent Global AI Survey commissioned by IBM indicates that 34 percent of businesses surveyed across the U.S., Europe and China have adopted AI. Therefore, the rise in AI adoption to the surge of new tools and services designed to help lower the barriers to AI entry. Those include new ways to fight data complexity, improve data integration and management and ensure privacy . To facilitate Business adoption, data scientists must master the vocabulary of the business as well as have an understanding of their challenges. This may lead to better identifying opportunities (resolve pain points or increase gain points) in which AI can have a definitive and irrevocable value. From an industrialization aspect The capability requires to work with strong IT professionals, having an experience of all existing barriers, making the industrialization of Data Science difficult. These difficulties may come from a lack of clear Business use cases, a lack of profitable Business goals and last but not least a lack of skills in the company as explained by Hervé Potelle & Laurent Leblond in Management & Data Science . To be able to handle these difficulties, and it is quite tough, it requires the BDDSC to have a good vision of dedicated technologies , to be close to Business teams and to acquire the appropriate skills to ensure the success of an industrialization. One Vision: Mandate & Objectives First of all, a Business-Driven Data Science Capability allows to develop a human-centric, data-led and technology-driven approach which is one of the keys to apply artificial intelligence and thus to get closer, step by step, to an intelligent enterprise. The main activities of a BDDSC can be decomposed into 5 major topics, all of which are equally important to each other: 5 main activities covered by a Data Science Capability Create New Value By leveraging on existing Data A first value creator factor is the Data itself , which can be considered as an asset and monetized. These new use cases bring value by expanding a company’s portfolio of offerings. This can include the rather rare case of straightforward selling of the data itself, selling of insights gleaned from data, offering new products or delivering analytics as a service. By identifying disruptive AI use cases Use case Identification. Before starting any Data Science project able to unearth these insights, it is crucial to define the expected impact the use case should generate. Niko Mohr and Holger Hürtgen from McKinsey Digital provides some valuable insights about how to achieve Business impact with Data . To illustrate with a non-exhaustive list of examples : for growth impact , use cases may focus on pricing strategies, churn prevention, cross- and upselling, and promotion optimization among many others. For internal process optimization , predictive maintenance, supply chain optimization, and fraud prevention can be cited as use cases example. To manage all these emerging new use cases, it is crucial to build a dedicated project portfolio with prioritization criteria. Complexity Estimation Another aspect to consider consists in assessing the complexity of a use case. Several questions must be addressed to be able to assess this difficulty: Are the required data available? Are the expected outcomes well-defined? Do we possess the required skills to be able to crack down the use case associated business problem? Are the algorithms able to scale up and to solve the problem in a humanly reasonable time? Are the Business Teams committed to the project? Is there an industrialization platform or process? Are the IT teams, responsible to look for the run of the use case, identified? Is there one team with devOps experience illustrated by Octo responsible for developing the POC and deploying it into production? If the answer of one of these questions is no, it might be interesting to have a deeper look and to investigate how to transform the “no” into a yes . One way to transform “no” answers to “yes” answers consists in building an agile culture of experimentation as explained by Octo in his publication to help the organization to evolve. Projecting value from a well-identified use case is necessary but not enough. More aspects have to be deepened such as the ability to prototype and industrialize the products and services resulting from the use cases. Transforming insights to actions and thus reaping the expected benefits is the best value that can be realized but it requires skills and technologies. These are the required means to make the Business-driven Data Science Capability operational, efficient and capable of achieving the objectives set by the business lines Untrap Original & Existing Data To untrap New & Existing Data, it is required to build a Data Supply Chain able to handle either internal data or external Data. Before defining the Data Supply Chain, let us have a deeper look about internal and external Data. Internal Data Internal Data is referred to as all Data located within an organization. It can be generated by sensors, various KPI dashboards, logs originated from various tables, data mart or data lakes. As mentioned earlier, a Data Lineage composed by various data dictionaries able to explain data and to indicate the data security level, a list of the corresponding Data Owners (people who possess the Data) and the location of the Data, should be provided to the BDDSC. In an ideal world, this should be easy to be obtained but in the real world, it is not always the case. Several major difficulties can be found such as either some ignorance of existing data due to management siloes, or a bad definition of data sensitivity level making sensible data accessible to all, or data stored on old databases making any data extractions complicated…. The goal of a BDDSC is not to resolve these problems, but it must be aware of it. Moreover, a BDDSC is not a data provider but should know the lineage of as much data as possible within the organization in order to be able to respond to the data-driven needs of the business. It must also know the data owners and the level of security associated with sensitive data and respect the procedures around the handling of this data. External Data External Data is referred to as all Data that is external to the company. They can be acquired either through partnerships with data providers or through open data. Some difficulties lie in identifying relevant data corresponding to existing or hypothetical future needs and also in the data cleaning/ingestion process to set up to deal with data quality problems. External data should be explored on purpose but never with a perspective of a possible undefined us  .It is important for the BDDSC to know the major data providers, to have knowledge of data that are exploitable in open Data and, if the need arises, to collect the available data for future purposes. Data Supply Chain A modern Data supply chain refers to the lifecycle process of Data All actions from its origin to its destruction through different steps such as storage, cleaning, use or transformation to name a few. The performance of machine learning models are highly correlated to data  quality and thus requires a clean and efficient Data supply chain to get the right data. Despite all the innovative aspects of this supply chain, it retains the same weaknesses of a classic supply chain, as mentioned by Katie Lazell-Fairman in “Learning from Machines: The Data Supply Chain” , especially for external data, and requires redesigns over time to meet sustainability and performance needs or most simply to upgrade the associated technology. To build a reliable data supply chain, several unavoidable steps have to be performed : Identify where the data comes from and define the sourcing strategy. Leandro DalleMille and Thomas H. Davenport in Harvard Business Review shows insights about how to balance an offensive et defensive strategy. Perform several quality checks with respect to quality principles such as accuracy, completeness, consistency, timeliness, validity, and uniqueness as explained by Datatrim . Generate a data pipeline to ensure that the right data preparation is performed on the suitable. Alan Marazzi shows how to build a data pipeline from scratch and provide some valuable lessons. Infuse State-Of-The-Art Technologies & Platform The Business-drive Data Science Capability must be up to date with the latest technologies and thus have a fast follower position in terms of R&D . It must also work with solutions that can easily be integrated into the organization’s existing technology architecture or propose a new architecture if the organization is too technologically backward. To define what technology fits the best to a data scientist is not as simple as writing down python code. The job of a Data Scientist has to be split into several aspects such as: programming tools, Data Analysis, Data Visualization, Machine Learning, Data Engineering among many others. In addition, each of these fields possess their associated and non-exhaustive technologies (tools or libraries) that are constantly evolving. There is no best of breed that can be cited due to the fact that most of these technologies are quite young and are still in development. To stay alert about the latest technological evolution, various authors communicate about the latest trends such as: Matt Turck updates regularly an AI technology landscape , Simran Kaur Arora on Hackr.io about Data Science tools , Kaggle also produces surveys to ask data scientists about the tools they used . The technological choice has to be performed by the BDDSC Data Scientists depending on their needs and their skills. However, a regular technological watch has to be performed continuously in case some product may become more mature than others. Embrace advanced Data Science A Data Science Methodology is an iterative process that follows a step-by-step sequence providing a clear project structure. By iterative process, it means a continuous cycle around the model construction. It gets continuously trained, evaluated and deployed. To improve the model relevancy, the end user has to provide feedback to the Data Scientist to ensure the identification of the right data, the right processes and the right KPIs to monitor. The Data Science methodology is composed by 4 main topics: Scoping Definition which allows to scope the project and to assess its impact and complexity, Data which handles data acquisition, quality and preprocessing, Model which focus on how to train a model until it fits Business expectations and Industrialization which consists in deploying the model. This methodology is quite classical and can be found with some variations in the literature like GeeksForGeeks , John Rollins from IBM Big Data & Analytics Hub , Lawrence Alaso Krukrubo also published some valuable insights on Towards AI among many others. Scoping Definition Scoping definition starts with understanding the business problem and thus  build an issue tree displaying all associated subproblems . Each issue will be analyzed to keep only those that can be answered with AI . To identify the best approach to solve the issue, an hypothesis tree is build using a Mutual Exclusive and Collective Exhaustive ( MECE ) policy. This method will intuitively lead to a resolution approach that can then be evaluated in terms of feasibility and temporality . To lower the risk of failure of the project, success metrics are defined during the scoping phase and will be regularly monitored during the project realization. In addition, to ensure that the project is meeting end-user expectation, it is required to apply agile methodologies and thus to organize weekly meetings with all important stakeholders of the project. Data Data is key to the success of Data Science realization. The major challenges rely in identifying the necessary data content , formats, sources and to initiate the data collection process . To facilitate this identification, it is key to identify the business experts likely to facilitate the understanding of the data. As a trivial way of assessing the informative power of the data, they must be able to answer questions related to the subject under analysis such as : ‘what’, ‘where’, ‘when’, ‘why’, ‘who’ ( also called the 5 W by towardsdatascience ) and ‘how’. Once the data are identified, feature engineering actions such as derivation of existing variables to create new data are performed on variables to identify relevant and meaningful insights to be validated or not by Business Teams, which must be continually informed of the progress of the analyses. Scientific approaches such as descriptive statistics and visualization can be applied to the data set to assess the content, quality, and initial insights. Exploring and understanding Data usually takes 80% of overall project time at the time of the first POCs. Model The construction of the model is performed in several major steps: the selection of the appropriate mathematical/algorithmic model, the selection of the model evaluation metrics and the consideration of feedback from end-users which allows an iterative improvement of the model’s performance. A large number of publications (like Xiuwenbo Wang on Kaggle or other publications cited earlier in this article) already deal with this topic and it is for this reason that we will not go into technical details. Model Selection Model selection focuses on developing models that are either descriptive or predictive. A predictive model tries to either predict a yes/no answer or a finite value based on the input variables. The data scientist will use a training set which is composed of historical data in which the outcomes are already known. The training set acts like a gauge to determine if the model needs to be calibrated. To identify the most suitable algorithm, it is required to test several Machine Learning algorithms families . Existing libraries ( scikit-learn , panda , pytorch among many others at least equally important) allow to quickly test a large volume of model while taking into account the complexities of variable selection, hyperparameter and other variable optimization constraints. To be able to identify the best model for the current data, the approach has to be agile and highly iterative until the model converges into satisfactory results. Model Evaluation Model evaluation is usually based on some technical KPIs ( confusion matrix , distance between realistic and predicted outcomes, segment homogeneity computation, ROC curves, …). Evaluation allows the model assessment and allows to verify if the Business problem is fully and appropriately addressed. To be able to verify the quality, a testing set, also historical data but not known by the model is used. To assess the quality, a confusion matrix is used to rate classification problems. To assess the regression problem, the difference between the predicted value and the real value is computed using several various metrics. An interesting and exploitable list can be found on scikit-learn . End-User Feedbacks End Users Feedbacks are necessary for Data Scientists to evaluate the model’s quality and check whether it addresses the business problem fully and appropriately because they are the best to provide feedback on the consistency of the results provided by the model. They can detect errors, measure its efficiency and share insights crucial for features engineering. They also can define when the performance of the model is enough for its industrialization. In order to facilitate feedback, an agile interaction mode with regular meetings with the business community allows to quickly identify ways of improving the model, thus enabling it to achieve performance scores sufficient to meet expectations. Last but not least, including end-users in the model feedback process also makes it easier to build a tool that perfectly meets their needs and facilitates the associated change management. Industrialization Deployment on production is performed when the model has been approved by business stakeholders, while keeping the sponsors informed of the progress of the project to avoid any negative tunnel effects. Deploying a model generally requires specific skills, dedicated technologies and a group of people responsible for monitoring the applications in production. The industrialization of a Data Science project is, at the time of writing this article, a subject that is both innovative and well-known, if not outdated. Innovative because the production of Machine Learning models is a recent phenomenon and raises new questions that must be answered with new technologies dedicated to it. This phase is tricky because the production environments at the time of writing this article are not necessarily all dedicated to Data Science, and the implementation must be performed step by step to evaluate the performance of the model and monitor its behavior in a production environment. well-known because the deployment (Build) and maintenance (Run) processes are already mastered by IT teams for other types of projects. It is this double state that makes the industrialization of a Data Science project complicated. When industrializing Data Science projects, we will mention two of the important aspects that must absolutely be taken into account: performance monitoring and end-user feedbacks collection Model Monitoring Model Monitoring is crucial to maintain the model operational despite any changes in Data nature or Data interpretation or to detect unexpected context. To be able detect any changes, two types of model monitoring exists as explained by Om Deshmukh in Anlytics Vidhya ,: proactive monitoring (test of various data samples which may lead to unexpected behaviors) or reactive monitoring (identification of root causes that led to the bug caught by the monitor). To ensure efficient monitoring, it is important to define relevant KPIs (Model performance, Data Distribution alternatives, …) and to set up sensors within the Data process. End User Feedbacks Collection End User Feedbacks maximizes the project’s chances of success as explained by Margaret M. Burnett and comes by collecting results from the model in production. The organization gets feedback on the model’s performance and observes the impact in a real-world environment. Analyzing this feedback enables the data scientist to refine/improve the model, increasing its accuracy, correcting any suspicious behavior or adding any new features. There is a lot to be written about the best practices for the production of Data Science projects. However, this is not the purpose of this article and further details will not be covered here. Imagine Modern User Experience This part is one of the most vital aspects that led to the success of the Data Science project and should be performed while scoping the use case. Much of the friction occurs right at the level where the user and the system meet (in other words, the Dashboard). User Experience starts with a good knowledge of users and a clear understanding of their objectives. If the user experience is not designed to answer user’s needs and objectives, the product outcomes will not be adopted by the end-users and all the work done will have been for naught due to a lack of users. Nick Kelly define User Experience as a secret ingredient to enhance Analytics success . To get the most suitable user experience, designers are performing Design Thinking actions, Persona definition process described by Jeremie Chaine , or even Theater as explained by Laure Constantinesco to understand end-user’s objectives and agile methodologies to ensure fulfilling expectations throughout the duration of the project. Therefore, it is important to equip Business / Functional leads with new tools, to delight end users with personalized journeys and to be able to inspire employees to innovate and co-create the new activity. To ensure the adoption of the product to any user, it is crucial to identify the UX issues that can be solved with UX expertise coupled with data-driven designs which aims to optimize the end-user design journey which can be defined by an article from Kate Moran and Kathryn Whitenton in “Analytics and User Experience” To pass all challenges, an agile methodology is applied meaning initiate a first design, defining success KPIs to track before and after design optimization . Moreover A/B and multivariate tests as explained by Antoine Pezé are helpful to improve UX experience with different designs and content variations. Once the data-driven design is set up, it is possible to drive new types of research by creating user surveys to get insights about the product itself and thus to identify optimization opportunities. Tom Hall  in UX Planet provides helpful recommendation for creating user surveys. Conclusion In this article, we have explained why a Business-driven Data Science Capability can bring value in digital transformation within an organization . We have also described the elements that constitute it, its core business and explained why it must remain close to the front and back office. The implementation of a Business-Driven Data Science Capability is not obvious and requires taking up many challenges at all levels (technological, human, business and organizational). If this article has interested you, if you would like to share your own experiences or if you would like to have more information on certain aspects covered in this article, please do not hesitate to contact us ! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles , Data Science , Digitalization , Methodology , Stratégie SI and tagged Advanced Data Science , Business Data Science , Business-driven Data Science , Data Factory , Data Science , Data Science Capacity , Data Science Organization , Data Strategy , Data supply chain , UX Data Science , Value Generation . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2021-02-24"},
{"website": "Octo", "title": "\n                Data+AI Summit 2020 – be Zen in your lakehouse            ", "author": ["Bartosz KONIECZNY"], "link": "https://blog.octo.com/en/dataai-summit-2020-be-zen-in-your-lakehouse/", "abstract": "Data+AI Summit 2020 – be Zen in your lakehouse Publication date 26/11/2020 by Bartosz KONIECZNY Tweet Share 0 +1 LinkedIn 0 In case you missed it, last week was held the first Data+AI Summit (formerly Spark+AI Summit) and we had a chance to participate. The talks will be published online but if you don’t want to wait, take a shortcut and learn our key insights! TL;DR Are you wondering why that title? It summarizes the major announcements from a data engineering perspective. Apache Spark will become more Pythonic thanks to the Project Zen initiative, and very probably, it will work on top of a lakehouse architecture. If you want to learn more about these and a few other topics, keep reading! Data architectures Data mesh Lakehouse and data mesh were two main data architecture concepts shared in the Summit. Let’s start with the latter one that we discovered in Zalando’s “Data Mesh in Practice…” presentation . In the beginning, Max Schultze listed all the problems of centralized data management and surprisingly, none of them was related to a particular technology! The most important issues were: the lack of ownership – when the data is, well, just the data. the data quality – a bit related to the lack of ownership. Since the data was flowing in the system managed by a central team, the team members couldn’t optimize the dataset for business purposes. the bottleneck – the centralized data team was in the center of all demands and, with the time, became a bottleneck in the agile data delivery Later, Dr. Arif Wider from ThoughtWorks explained how the data mesh paradigm solves these problems and makes everybody from the below picture happy. The first point solving the issue consists of changing the perception of the data. It’s not anymore a “dataset” but a product owned by a team. The team is there not only to maintain the technical pipeline but also to advertise and ensure its good usability. The second solution is the Domain-Driven Architecture applied to the data . The team responsible for the dataset has the best expertise to answer customer’s questions and provide a product of good quality. And finally, the Data Infrastructure as a Platform . The technical backbone for the data mesh has to be domain-agnostic, i.e., all teams should be able to use it seamlessly for batch/streaming data processing, ML models management or data analytics. “Seamlessly” is currently the keyword here because the goal is to avoid creating a new bottleneck in the organization. Lakehouse Let’s move now to the lakehouse paradigm. Databricks, which is the company advocating the principle, identified 10 main problems of the data lake approach and summarized them pretty well in the following slide: Ten issues and one solution present in a lot of the Summit talks, Delta Lake!  Delta Lake solves the reliability problems with the ACID transactions support, optimizes the performance with the Z-Ordering and small files compaction, and integrates with existing ACLs and RBAC strategies at a table, row or column level for the governance. Finally, it solves data quality issues with the schema management (e.g. merge schema feature of Delta Lake ) that can be extended with an additional data validation (e.g. Great Expectations ). In addition to that, this new paradigm uses the Bronze → Silver → Gold data classification, where the leftmost side stores the raw format of data and the rightmost exposes the dataset to the clients. The following picture summarizes the lakehouse concept pretty well: The picture above is also a great occasion to introduce the new component of Databricks service to handle data visualization scenarios, the SQL Analytics . And to make it possible Reynold Xin, one of Databricks co-founders and Apache Spark contributors, explained the implemented optimizations to the BI protocol used by the component, like: Photon Engine – vectorized C++ query engine, up to 10x faster than Apache Spark. SQL Endpoint scaling – the queries reach the SQL Endpoint that can create new compute instances at runtime. optimized short query path – if the query results can be returned within a few seconds, the client won’t need to ping the server to see whether the results are ready, as it happens in typical BI protocol exchange (picture below) Apache Spark – team unite and Project Zen From its very first days, the developers associated Apache Spark with Scala. But since then, a lot of years have passed, and the landscape has changed. In the last analysis shared by Databricks during the keynotes, Scala users’ proportion on top of Databricks platform decreased from 92% to 12%. The one for Python increased from 5% to 47%! However, PySpark is not yet at the same maturity level as Scala Spark. And in the Wednesday keynotes Reynold Xin shared the Project Zen initiative, whose goal is to make PySpark more Pythonic . And this transformation passes through different axes: type hints introduction to facilitate development in the IDEs and notebooks through auto completion feature – the previous one suggested not really related items to the written code documentation improvements – previous documentation was composed of a few long pages, not very helpful at a daily basis, especially for the newcomers improved error handling – no more need to scroll through the Java stack trace to find the problem in your code Pythonic UDF API – no more need to decorate the User-Defined Functions. The new approach also includes the type hints The news was shared during the keynotes and extended in the “ Project Zen: Improving Apache Spark for Python Users ” by Hyukjin Kwon. Apache Spark – internals and tips Despite the conference rebranding from Spark+AI Summit to Data+AI Summit, there were many insightful Apache Spark presentations! Everything started for us with “ What is New with Apache Spark Performance Monitoring in Spark 3.0 ” where Luca Canali shared his tips for monitoring Apache Spark applications. Apart from the visual Spark UI, you can go a bit deeper and monitor your application with Spark listeners , metrics, including the executor ones or plugins . And if you look for some examples, you should find them in one of these repositories https://github.com/cerndb/SparkPlugins , https://github.com/cerndb/spark-dashboard or https://github.com/LucaCanali/sparkMeasure After the monitoring part, we took a look at the migration journey from EMR to Kubernetes (EKS) workflows on AWS, presented in “ Migrating Airflow-based Apache Spark Jobs to Kubernetes – the Native Way ” by Itai Yaffe and Roi Teveth. They succeeded in reducing the cost of running Apache Spark data processing jobs by 30%, only by migrating them from EMR to EKS! Of course, it didn’t come without cons, but if you are looking for pricing alternatives to Spot Instances or Reserved Instance, you can take the EKS into account. In addition to them, we also appreciated the Facebook sharing on the data skew and joins optimizations in “ Skew Mitigation For Facebook PetabyteScale Joins ” by Suganthi Dewakar and Guanzhong Xu, and “ Spark SQL Join Improvement at Facebook ” by Cheng Su. In the first talk, you will discover the possible solutions to handle skewed joins, i.e. the joins where one key is way more popular than the others: Split the JOIN into 2 queries, one for the skewed key, one for the rest, and collect the final result with a UNION – can help but requires to scan data first to discover the skewed key(s). Use Apache Spark 2 adaptive framework – better since the management is automatic but can introduce an additional shuffles even for not skewed joins. Use an extended version Adaptive Query Execution skew join management addressing all of the above drawbacks The second of the talks covered all changes made on Apache Spark 3 regarding the JOINs. Among the changes ready to be released, you will find the shuffle join based on the code generation that improves the performances by 30% and also uses less CPU! It’s planned to be integrated into Apache Spark 3.1. Another exciting proposal, not merged yet, is the use of Bloom filters to scan the larger side of the join and eliminate the not matching rows before performing the join physically. The technical details are still under discussion, but the feature itself looks promising. Delta Lake – focus on MERGE If you looked for some technical details about Delta Lake this year, you won’t be deceived! Apart from Lakehouse architecture’s presentation based on this new storage format, Databricks engineers and Open Source community members shared a few tasty tips about Delta Lake. First, during Wednesday’s Meetup, Jacek Laskowski spotted a problem he encountered with the MERGE operation. From his feedback, we can discover that under-the-hood, the MERGE does 2 passes on the source data and that the Dataset has to be explicitly cached before invoking it! This behavior may change soon due to the mailing list exchange . This technical detail was later developed by Justin Breese, the solution architect from Databricks, in his talk “ Delta Lake: Optimizing Merge ”. In a nutshell, the MERGE operation executes in these 3 steps: Justin also shared a few interesting tips on how to optimize the MERGE: use partition pruning – so far not implemented as a built-in mechanism but can be easily achieved with explicit filtering on the columns used in the partitions and Z-Order index set the spark.databricks.delta.optimize.maxFileSize to at most 32MB for write-intensive workloads and to 1GB (the default) for the read-intensive ones unpersist cached and not used Spark DataFrame to free the memory consider using smaller workers than the big ones – according to the presented comparison, the MERGE operation executed on the cluster compose of 2xlarge nodes performed way better than with the 16xlarge use correct prefixes for your objects to avoid contention of the object storage And the “Optimize merge…” was not the single talk given by Justin. He was also a co-speaker of “ Delta: Building Merge on Read ” where together with Nick Karpov, they shared 2 different approaches for performing the MERGE operation. The first is called MERGE on write and consists of exposing the MERGED dataset to the downstream consumers. But it also has an alternative approach called MERGE on read , presented in the picture below: The idea is very similar to the Lambda architecture. The application adds the incoming data to the appendOnly table. But it’s not the exposition table. It’s only one of 2 tables composing the exposed dataset from the view table. And this view table is the result of the join operation between the most recent rows ( appendOnly) and the ones already processed in the Materialized view table. In addition to that, an Apache Spark job rebuilds this Materialized view table in the background by adding there the new rows from appendOnly table. As a result, the appendOnly table will only store the recently changed rows and the amount of joined data will be relatively small. Thanks to that, Apache Spark can use the broadcast join strategy to build the view table more efficiently. When to use this approach? The speakers presented some hints in this slide: But the MERGE is not the single amazing Delta Lake feature. Another one is the mergeSchema option presented by Mate Gulyas and Shasidhar Eranti from Databricks in the “ Designing and Implementing a Real-time Data Lake with Dynamically Changing Schema ”. If you’re working with streaming semi-structured format and would like to know how to manage the schemas in this context, this talk should shed some light on it! The keywords are “mergeSchema” et “metadata store”. As you can see, despite the fact of being held virtually this year, the event itself didn’t lose anything from its magic! And we covered here only the small part related to data engineering. If you want to discover more of the talks on your own, be patient and check whether a new Data+AI Summit 2020 Europe playlist didn’t appear in Databricks YouTube channel 🎬 Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data and tagged apache spark , bigdata , data mesh , delta lake , lakehouse . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2020-11-26"},
{"website": "Octo", "title": "\n                Terminal telekinesis            ", "author": ["Léo Jacquemin"], "link": "https://blog.octo.com/en/terminal-telekinesis/", "abstract": "Terminal telekinesis Publication date 30/05/2020 by Léo Jacquemin Tweet Share 0 +1 LinkedIn 0 Tl;dr In my experience, software developers’ skills regarding tasks in a terminal range from desperately slow to impressively swift. I believe that, for a large part, this is caused by an abundance of articles not discussing the real problems software developers are dealing with daily. Namely, how to feel productive on a terminal, after using omnipotent integrated software environments for so long. This article aims at bridging this gap by sharing carefully selected tips that can, in my opinion, dramatically increase the terminal productivity of those who have never heard of them. All the following tips are either natively present or very easy to install. I think they are of particular relevance to junior developers who are usually overwhelmed by the seemingly endless stream of new technologies they have to learn, although this list has proved helpful with seasoned terminal users too. Maxing out the cursor speed Instant search with backward incremental search Terminal teleportation with z Finding files with fd and words with rg Short man pages with tldr Perfect recal l with history Introduction As Nietzsche put it: If you stare long enough into your terminal, your terminal stares back into you. To be honest, I’m not convinced those were his exact words. But whether they were or not, one can hardly argue against the fact that the terminal is not equipped with a friendly user interface. As far as I’m concerned, I won’t be denying it any longer. For a long time, my terminal and I had a very conflictual relationship. I was mad at my terminal. I was mad because it made me feel slow and dumb every time I tried to perform even the most basic cd: cd  ../where/is/it/located/again/omg or searched  for a specific option: curl -X ’header? method? hostname?’ I seeked help from external sources, such as Internet articles. Alas, I failed at finding the material I required to get my self-esteem back.  Every piece of advice I found was missing the mark. Many tutorials throw unrelated Unix tips at you, as if there was any added value in aggregating all of them in one page, something Google has been doing since 1998. Far too many tutorials try to teach you Unix Piping Art. If you have never heard the term before, don’t feel embarrassed, I just made it up. It is the art of combining seemingly endless sequences of grep, sed, sort, cut and awk so one can rename all his summer pictures from .png to .jpg and email them properly filtered and gzipped to his best friend. All jokes aside, most of these articles are great. I mean it. However, they don’t really help with day-to-day programming tasks, such as spinning up multiple containers in the background, or quickly editing a configuration file far away from where you stand. I was full-blown frustrated for a long time. Until one day, one of my colleagues suggested that I brew install a little utility that would save me a lot of typing, thereby sparing me a lot of nuisance. All of a sudden, the ops inside of me was reborn. And so began my quest: fierce, restless, endless. The search for a lightning fast terminal experience. So here we go, dear reader. If you are frustrated at feeling slow, brace yourself . For I promise you this is the last time you will ever feel slow in your terminal. 1/ Maxing out the cursor speed Many web developers pride themselves at being KISS experts. In the spirit of simplicity, we shall start this productivity check-list with a genuinely simple advice: increasing the cursor speed. Switching back and forth from IDEs and terminals, one quickly realizes that the major developer experience paradigm shift is the absence of a mouse to fly from line to line. Keyboard-based navigation is thereby constrained by two things: your knowledge of epic time-saving shortcuts, and your cursor speed. Learning shortcuts can tremendously increase one’s terminal agility, at the expense of learning them, remembering them, and trying not to rely too much on them. Topping the cursor speed, on the other hand, will instantly increase your swiftness with the sole cost of figuring out how to do it on your operating system. This will have two interesting benefits as well. As a system-wide setting, doing so will give you a faster experience everywhere your computer requests you to fill out a text input. Besides, you will be able to cope with all these insane terminal-based editors like vim and emacs by just pressing an arrow key and waiting. Increasing the speed of the cursor – or of anything else in fact –  can be considered solid advice when it comes to productivity, after all. Believe it or not, it took me 2 years to realize it. In the aftermath, I set out on a complex calculation to estimate the time I spent looking at my cursor moving at the default speed rate in my terminal. I should never have done that. 2/ Instant search with backward incremental search This second command is probably my favorite. I think of it as the quintessential of the time-saving commands. Is your shell’s current configuration file filled with tons of aliases, half of which you don’t even remember setting up in the first place ? alias mkp=mkdir -p alias gcm=git commit -m alias loginpg=pqsl -h localhost -U postgres mydb alias mysrv=ssh there@142.93.98.118 -i ~/.ssh/mysrv It probably is. In fact, various popular shell frameworks will provide plugins, nicely packaged with their fair share of aliases, confusing the terminal apprentice into thinking that tons of aliases are actually a good idea. I’m going to make a bold claim here. Possibly along with making myself a few enemies in the process, but that’s fine. So mark my words for they may cause me trouble… Shell aliases are an anti-pattern. To illustrate all their pettiness, I could tell you about the time it took me 4 hours to understand why a slightly customized vagrant box was working correctly on everyone’s laptop but was failing silently on mine. But doing so would probably shed more light on my weak debugging skills than anything else, so I’ll pass. In my opinion, shell aliases are essentially redundant global variables that encourage indirection. They allow us to map a fully qualified and explicit long shell instruction into a random sequence of letters of our choice, in order to save us the trouble of typing them, remembering them, or both. Truth be told, they remind me of the infamous Singleton pattern, which has the somewhat confusing ability to be both a pattern and an anti-pattern at the same time, thereby making it the very first quantum design pattern in software’s history. Although they can actually come in handy in some situations. But what you’re probably looking for is a way to quickly execute commands you have already executed before , and there is a much better alternative, natively present in almost every shell called backward incremental search (or reverse search history). Distinctive sign: bck-i-search: _ Summoned by pressing: ^+r When entering this mode, your shell will quickly guess which command you’re looking for by displaying the most recent matches as you type. For instance, if you’re working on a configuration file that you opened with vim: vim ~/.mydir/config and typed a few other commands afterwards, what would be the fastest way to open up the same file again ? Many may have the reflex to use the arrow keys to quickly display the latest commands one after another. This works well. But one can accomplish exactly the same within a fraction of the time by invoking some incremental search goodness by pressing just 3 keys: > v im ~/.mydir/config bck-i-search: v How cool is that ! 3/ Terminal teleportation with z The cd command has been around for as long as computers had terminals – external ones – and screens. But a 70-year old built-in command should tip the wise developer off. Indeed, all odds are that a way more modern version has been developed since. In this case, the modern version exists. It is called z. z is a tool that tracks where you’ve travelled using cd and is able to guess where you want to go, allowing you to instantly jump into a directory with just a few keywords, provided you have visited it before. Prioritization is based on what they call “frecency”, which takes into account both the most frequent and most recent directory that match a given pattern. z works extremely well on its own. However, it will sometimes fail to get you exactly where you want to go in one go. Occasionally, you will land somewhere else. No tool is perfect. However, if you help it just a little by naming and ordering your directory with just a zest of z ™ in mind you will be able to jump right into any directory with just 3 key strokes at most! When you do, thank cd for his decades of service, but don’t forget to put it in the attic where it belongs! 4/ Finding files with fd, words with rg Searching for files and words in a project is something software developers do everyday. The developer experience for doing both in an IDE is hard to match. This is because IDEs index every source file we write. Although with great powers come… great limitations. Indeed, everything must be indexed first and this can be very slow, which is why we need something else. We need something that lets us find symbols such as parts of a word inside a directory. We need it to traverse each directory recursively by default, use regex patterns by default , and do all of this almost instantly by default. We need ripgrep . Ripgrep is the fastest command line search program I have ever seen in action. It is so fast that search results appear almost instantly on stdout. And even when they don’t, the speed is so impressive that I enter a weird hypnosis state from which I wake up when the job is done anyway. Either way, my consciousness perceives it as instantaneous. Oddly enough, ripgrep falls short when it comes to quickly finding a path inside a directory. For the sake of illustration, let us assume that there was a problem going on, involving a configuration file. You wish to quickly retrieve all the files that look more or less like a configuration file inside your project’s directory. How would you do it efficiently ? Finding paths inside a directory is a valid use case for a native command called find . One could for instance, write the following: find . -name “*conf*” One can go a long way with this command. Especially when paired up with the backward incremental search we discussed before. However at some point, one must inevitably address the elephant in the room, and have the courage to ask: what is wrong with the way the find accepts its options ? The command’s API does not seem to care for any of the unix standards most other commands duly respect. That alone should be sufficient to look for a more conventional alternative. If we strip out the in from find , it becomes fd . It is twice faster to type and works twice as fast. Plus, the output is colored by default. As for the find command, we must acknowledge that part of the reason for its unconventional usage is because its options are in fact not options, but predicates instead. Truth always come out. It has now. Let go of find and use fd instead! 5/ Short man pages with tldr Backward incremental search can do wonders for you most of the time but it suffers one major drawback: it cannot help you with commands you have never typed before. As it turns out, when one wants to know to get a quick documentation answer about a specific shell command, Google may not be the fastest way from point I to point K . Man pages are, and you can type man man to have more information about them if you’re that type of person. Man pages are truly great. Once you start getting used to them and are used to your $PAGER like less , you can retrieve relevant material very quickly without ever leaving your terminal. Furthermore, you’re always guaranteed to look at the documentation of the specific version you’re using, if your system is correctly set up. However, man pages suffer a major drawback themselves. They’re just static pages. They lay a program’s options down without any consideration of any sort for the relative importance of each of them, the way autojump and z do with recently visited directories. Alas, tldr summaries were not a thing then. Fortunately for us, they are now. tldr maintains an online database of the most useful program options for you to consult any time you’re sitting idly in your terminal. I don’t use it as often as bck-i-search: _ (which I use dozens of times a day), although when I do, it allows me to quickly find the 5 most popular usages of a program I’ve never used before. And I must give it some credit because up to this point, tldr almost never failed me. One of my favorite use cases is to pair it with Git, a very popular VCS software. From its humble beginnings, GIt API has been notoriously hard to understand. This is perhaps the reason why so many developers don’t take the time to appreciate it. To appreciate the power of tldr on the other hand, I can’t think of a better example than one of Git’s most confusing parts: the undoing of some work with the git reset command. > tldr git-reset - Unstage everything: git reset - Unstage specific file(s): git reset path/to/file(s) - Undo the last commit, keeping its changes in the filesystem: git reset HEAD~ - Reset the repository to a given commit, discarding all since then: git reset --hard commit I honestly think that this short abstract covers 10% of what this command can do and yet 90% of what it is used for. Pretty straightforward, as opposed to the 445 git-reset man pages. Another neat trick with this command is to use it as a learning tool. As a community-driven command, typing tldr will always give you what other people are using this command for, which will instantly give you a pretty good idea of its most relevant usages: something neither the man pages nor a Google search can give you with that accuracy. Try using tldr on both fd and rg to see more of it in action, and in the process, give this article the opportunity to refer back to itself , in a kind of meta-way. 6/ Perfect recall with history The Pareto principle states that 80% of the effects come from 20% of the causes. It is an empirical law that is confirmed by experience time and time again, with deep implications in many areas. Backward incremental search and tldr are already secretly piggybacking on the 80% part of it. But what about the other 20% ? From time to time, you will have to roll up your sleeves and write a specific command. One that you won’t be needing anytime soon. Perhaps even a master-piece of unix pipes of your own. However, one that will be tremendously useful at the time you’re crafting it, and therefore probably also useful another day in the future. History saves the last thousands unique entries you have typed. It is not to be confused with a journal log of everything you have typed chronologically. When piped into grep, it allows you to quickly pull out of the distant past something you know you have solved before. You don’t have to remember the exact command, or even the beginning of it. Just a tiny specific detail about it for grep to do its pattern filtering magic. What does it mean ? It means the terminal power-user that you are becoming is natively geared up with perfect recall abilities. Therefore, it is always a good investment to take the time to create a complex powerful shell command. For you can bring it back anytime you want by invoking history, and use a slightly different version fitting your slightly different problem. Conclusion In this article, I’ve shared a few tips I wish I had known when I started leveraging my terminal every day. All these tips proved invaluable to me, and I have yet to see a terminal power-user that would discard them all. Picking one of these tips up will definitely improve your terminal experience on some occasions. But if you pick them all, and add your own to the mix, you may come to realize that feeling fast is only half of the equation. What’s really important in the end, is not wasting time and energy on repetitive tasks, so we can focus on what truly matters in our field.  Rock solid quality code that solves challenging problems for our users. And even if you’re new to the game, my guess is you already know that it is a full time job. I’m not advocating to look for better ways to do things every second of the day. Sometimes, self-improvement must be put on hold and things are good enough the way they are. But after a while, it is a healthy practice to step out of your comfort zone, in search of lost time. You never know, you might stumble upon the time-saver of your life. Telekinesis is the faculty to move things around by the will of the mind. Well, I might not be able to execute shell commands without ever touching my keyboard yet, but one thing is for sure. I’m getting closer every day. Takeaways Increase the cursor speed Use ctrl+r to search through your shell’s history Use cd when you visit a directory for the first time, and then use z Use fd and rg instead of find Use tldr first and then man Use history with grep to retrieve anything you have typed before Drawings Courtesy of my colleague Aryana Pezé Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations , Infrastructure et opérations , Software Craftsmanship and tagged terminal , unix . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Terminal telekinesis” Laura Bergoens 14/10/2020 à 13:23 Great article from a great developer that cannot stand typing more than 5 keys to get the job done !\r\nIt's not impatience, it's efficiency at its quintessence Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2020-05-30"},
{"website": "Octo", "title": "\n                The Google Assistant for Android developers – PART 2            ", "author": ["Nicolas Telera"], "link": "https://blog.octo.com/en/the-google-assistant-for-android-developers-part-2/", "abstract": "The Google Assistant for Android developers – PART 2 Publication date 16/04/2020 by Nicolas Telera Tweet Share 0 +1 LinkedIn 0 This article follows a first article “The Google Assistant for Android developers – PART 1” . In the first article, we have discovered the “Solutions” part offered by Google by going through the making of a simple app. We have built an App Action allowing us to directly launch a specific feature of our app with a vocal command, then we improved it with the displaying of a Slice. Although really simple to build, we have quickly witnessed the limits of this part since it only allows us to launch a feature or, at best, display a few information of the app into the Assistant. But when we think about the Google Assistant, we think “conversation”. In this second article, we will go through the second possibility for Android development with the “Custom Conversations” by building a Conversational Action that will allow us to exchange with the Assistant and get answers built with an API. Conversational Actions App Actions allow us to interact with the Google Assistant. But we can’t really talk about conversations in the way that there is no real exchanges and there is no real start and end of the interaction. For instance, if I want to play a game or book a plane ticket, it will be difficult to get what I want with a single vocal command. Conversational Actions are more advanced actions that will allow us to build interactions with a lot more precision. They are started when the user invokes them and continue until the end of the conversation is identified while taking into account the different factors of a real conversation as adapting to the interlocutor or dealing with the absence of answer to a question. They work by exchanging JSON formatted data. When the user sends a vocal command to the Assistant, it triggers a series of steps in order to get adapted answer: the user input is transformed into text by the Assistant this text is used to build JSON requests these requests are sent to a “conversation fulfillment” by HTTPS request the conversation fulfillment parses the request into structured data, then triggers a webhook it returns the response of the webhook as JSON to the Assistant the Assistant processes this response to display an answer to the user There are two ways of building conversational actions: the Actions SDK with which it will be necessary to provide our own Natural Language Understanding (NLU) solution, or Dialogflow , highly recommended by Google. Here, we will only focus on the Dialogflow part and its web interface that will really ease our work. Dialogflow ? Dialogflow is an online service offered by Google and allowing us to build conversational interfaces by using machine learning. It is the most widely used tool to build Actions on more than 400 millions Google Assistants. By using this service, we will let a Dialogflow agent handle the transformation of our requests with NLU, training phrases that we provide and the context of the conversation. Once a request is identified, it will associate it to an event and extract parameters into Dialogflow entities. These events are associated to fulfillments, webhooks linked to Cloud Functions on Firebase into which we will be able to do all the logic needed by our request as calling a web service for instance. Let’s consider this case: the user wants to ask the Assistant for a fact about a number. He then must ask an adapted question from which an event will be associated by Dialogflow and an entity extracted as a number. From this event, a Cloud Function, taking this number as a parameter and making a call to an API to build an answer, will be triggered. Finally, this answer will be returned to the Assistant so that he just has to display and say it. Thus, the major part of the work that we will have to do for setting up this Conversational Action will be building the Cloud Function. Let’s do it! Prerequisites In order to start building a Conversational Action with Dialogflow, we have to activate some necessary parameters on our Google account : Web & App Activity Voice and audio recordings App info from your devices Then check “Include Chrome history” and “Include voice and audio recordings” into the “Web & App Activity” section. Some basic knowledge of JavaScript is also recommended to understand the code of the Cloud Functions. Finally, if we use Chrome with multiple associated accounts, it will be important to be logged with the same account on all the different consoles that we will be using next. Building a Conversational Action with Dialogflow The first step when building a Conversational Action is to go to the Actions Console . It is where we will be able to manage our actions, test them and at some point publish them. Let’s create our project. This will also create a project on Firebase on which to deploy the Cloud Functions to trigger from the webhooks that will be set. Once the project is created, we can see that there is a lot of different experiences already provided by Google and available as Built-in Intents. But careful, although very similar, these Built-in Intents shown here are not those of the App Actions from the previous article. They are used to link specific events into Dialogflow and thus hold different sets of data. These Built-in Intents allow us to tell the Assistant that our Action can fulfill a specific category of user requests (play a game, get the horoscope…). But in order to better understand what we do, we will directly select the “Conversational” card to set up an empty action to be associated with a custom Dialogflow fulfillment. Our first project being created, let’s add our first Conversational Action by clicking Build your Action > Add Action(s) > Custom intent > Build . Creating this custom intent automatically redirect us to the Dialogflow console on the creation of the agent to be validated. We can then see that our agent is well linked to our project on the Actions Console. Withing the Dialogflow console, there are the main sections that will interest us in this article: Intents here we can found the different intents that we wil associate with our Action (start and end of a conversation, permissions asking, questions and processing of the answers…) Entities here are the different entities that will be identified in our Action and used in our intents Fulfillment there are the fulfillments of our intents as webhooks Integrations it is here that we will be able to integrate our action to different services (Slack, Twitter etc.) but mainly configure the deployment of our modifications on the test console Start a conversation In order to be able to start a conversation, every Actions project must have an entry point as a “welcome intent” that will be invoked when the user utters the name of the Action. This intent is created by default by Dialogflow in the Intents section. We can also see a fallback intent that will be used when Dialogflow can’t match a specific intent with the received JSON request and that will tell the Assistant to answer something like “On more time?” . By clicking on our welcome intent, we can see the different responses by default. In order to have control on our Conversational Action from the start to the end of our conversation, let’s delete them all and add our own: “Welcome! What number do you want to know about?” . After every modification, whether it is on intents of fulfillments, never forget to save the modifications by clicking the SAVE button. Testing a Conversational Action We can now test our Action by going in the Integrations section and clicking on Google Assistant > Integration Settings > TEST . It is also useful to activate Auto-preview changes so that changes are automatically deployed to the Actions Console and the test simulator. We are then redirected to the Assistant simulator on which we can test our Action. On the left side, there is a smartphone simulator and on the right side, a little more advanced version with the possibility to display our Action on various surfaces as Smart Displays or even speakers. These different surfaces will be useful to test specific cases like the absence of vocal input. There are also some very useful tabs: REQUEST , RESPONSE , DEBUG and ERRORS . This is where we will be able to visualize the exchanged JSON requests and responses and access a first level of debugging of our interactions. All we now have to do is type “Talk to my test app” and press enter or just click the suggested input in order to trigger our “Welcome intent”. Note that once an Action is deployed, it is also possible to test it directly on a real device on which the Google account is authentified (you may need to configure its locale to English). Now, do the same thing again. It doesn’t work anymore? That is normal. We have just triggered a Conversational Action but we didn’t say that the conversation is over. Our Action is still running and is awaiting for the next input. That is why in order to run it again, we have to tell to the Assistant that we want to end it, for instance by sending “cancel” . Adding a conversational response with a simple webhook Now that our conversation start intent is ready, let’s add a new conversational response intent with a simple webhook based on an entity recognized by Dialogflow. Entities are categories of “things” that Dialogflow uses so that it can extract the value of parameters passed in the natural language requests. The idea here is to create a new intent, train it with specific phrases based on a recognized entity (here, the numbers). Let’s create a new intent “favorite number” (careful, a intent name is case-sensitive) and add the following training phrases to it: Dialogflow then automatically recognizes the @sys.number entity that it associates to a number parameter. Also, each time we save our changes, we can see that a machine learning model is trained with the new provided data. Let’s then define the number parameter as required for this intent and add a question to it ( “What number do you want to know about?” ). This prompt will be displayed to the user if the request does not contain this required parameter. All we have to do left is configure the webhook. To do so, we must first click on Enable webhook for this intent in the Fulfillment part of this intent (not to be confused with the Fulfillment section). Then go to the Fulfillment section. We can see two options: Webhook , allowing us to link a webhook uploaded as a Cloud Function on Firebase, and Inline Editor , more simplistic and allowing to directly type some Node.js. We will start with this Inline Editor in which we will type the following code: Here, we import the Dialogflow module for the Actions on Google, Firebase and the Dialogflow client then we handle the “number request” intent (which takes “number” as parameter) in order to display a simple response that ends the conversation. The last step is to click DEPLOY (warning, this can take a few minutes). Finally, our first simple conversational response is ready, let’s test it. Adding a conversational response with a custom webhook We have managed to build a response to our Action. However, it does not have much sense and we are quickly limited by the interface of the webhook editor on Dialogflow. We are then going to see how to deploy our own Cloud Functions in Node.js on Firebase so that we can make the most of our Action, which will also allow us to access the logs of our code. Let’s install the Firebase Command Line Interface (Firebase CLI) on a terminal: npm -g install firebase-tools Then authenticate with our Google account: firebase login To make things easier, the project containing the code of the previous webhook, and that we are going to improve later, is available on Github. We can now disable the Inline Editor in the Fulfillment section of Dialogflow then deploy our webhook manually by using the Firebase CLI. To do so, define the used project: firebase use --project The project ID can be found in the settings of the project on the Actions Console. Install its dependencies from the functions folder: npm install And deploy the project: firebase deploy --project It is also possible that Firebase or npm have to be updated or that Firebase authentication token must be refreshed. Now that our Cloud Function is deployed, we can see that its URL (visible in the Develop > Functions section on the Firebase console) is already set in the URL field of the Webhook part of the Fulfillment section in the Dialogflow console. Let’s just enable this webhook. If we go to the test simulator, we can still run our Action as if nothing happened! In order to make our Action a little more useful, we are going to call an API in our Cloud Function by using the number parameter passed by the user. Let’s update the index.js file by replacing that line: by this code: And deploy the updated Cloud Function. Now, our Action responds a fact about the number that we asked for. Keep the conversation going For now, the conversation is started and we have an exchange between the user and the Assistant. However, the end of this conversation is a bit harsh. We would want to make it a little more lively by pursuing it and by defining a proper end. We are going to use the follow-up intents . Those are going to be linked to our custom intents in Dialogflow and will offer various follow-ups like “Yes”, “No”, “Repeat” etc. We are thus going to ask the user if he wants to ask for a new fact after that the Assistant gave him one. First, let’s click on Add follow-up intent on our number request intent and add: a yes intent with a response “Ok so what other number do you want to know about?” , thus, when the user will answer with an input holding a word identified as associated with the @sys.number entity, our number request intent will be triggered again a no intent with a response “Goodbye!” and the “Set this intent as end of conversation” enabled so that this response is displayed before ending the conversation Then, we have to update the code of the Cloud Function to ask the user if he wants a new fact. Let’s then add the Suggestions dependency at the beginning of the index.js file: then update the conversation ending response: by the displaying of that same response followed by a question with a Yes / No suggestion: And deploy our Cloud Function to finally test it. Adding a rich response In order to finish our Conversational Actions basics tour, we are going to add a rich response to our Action. Rich responses allow us to display visual elements to enhance interactions with the Assistant. For instance, we can display images, carousels or even medias like a music player. We need to add a couple of visual elements dependencies and replace the displaying of the answer by a card containing this same answer and an image. Here’s our final code: The deploy and test. To see the result of our update, we need to test the Action on the right side of the simulator and select an adapted surface (meaning a surface with a display like Smart Display of phone), or simply test it on an Android smartphone. We now have a Conversational Action that covers the basics of a conversation with a start, exchanges with multiple choices of responses and a clear end, all with a visual response! Conclusion Where App Actions from the “Solutions” part only allowed us to extend the reach of our Android applications into the Assistant, Conversational Actions offer an impressive range of possibilities that we quicky want to “play” with for hours. Note that by using Dialogflow, we didn’t even have to touch a bit of Android code! We would have just wished for a bit more clarity as for the required configurations on our Google Account (voice recordings, Chrome history…). There are a lot more possibilities that exist and that we did not cover here so that we don’t overstep the goal of this article: discover the development of Actions on the Google Assistant. Among these possibilities, we can think of handling sounds, silences on speakers, unexpected inputs, data storage or different types of invocation (explicit or implicit). We are quite used to launch our favorite apps through the Assistant but few are the real interactions that go beyond the simple launching of a feature. As I mentioned I the introduction of the first article, Actions development still seems quite mysterious for Android developers. That is why I hope that these articles will have succeeded in demystifying it a little and that you will be excited to try it yourself! If you liked this second article, I invite you to go and check the Conversational Actions official documentation and the brilliant codelabs that can be found there, including an example of a Hangman game using a pretty advanced display. Goodbye! ‘Nicolas Telera’ left the conversation Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2020-04-16"},
{"website": "Octo", "title": "\n                Time series features extraction using Fourier and Wavelet transforms on ECG data            ", "author": ["Marisa Faraggi", "Karim Sayadi"], "link": "https://blog.octo.com/en/time-series-features-extraction-using-fourier-and-wavelet-transforms-on-ecg-data/", "abstract": "Time series features extraction using Fourier and Wavelet transforms on ECG data Publication date 23/11/2019 by Marisa Faraggi , Karim Sayadi Tweet Share 0 +1 LinkedIn 0 ABSTRACT This article focuses on the features extraction from time series and signals using Fourier and Wavelet transforms. This task will be carried out on an electrocardiogram (ECG) dataset in order to classify three groups of people: those with cardiac arrhythmia (ARR), congestive heart failure (CHF) and normal sinus rhythm (NSR). Our approach consists of using scaleogram (i.e. 2D representation of 1D extracted features) as an input to train a Neural Network (NN). We conducted the different tasks using python as a programming language. The results presented at the end are satisfactory and demonstrate the pertinence of the approach. INTRODUCTION Many data science problems e.g. weather prediction, stock market analysis, predictive maintenance, etc. comes with data that vary over time. We refer to those data interchangeably as time series (TS) or signals. Then we can apply the same extraction techniques for TS than for signals.  Whether it is for the classification or regression tasks, the feature extraction is an important step that will influence the accuracy of the intended predictions. In this article, we first present the pros and cons of using Fourier transform and wavelets on an ECG dataset. Then, we explain how a 2D view of the extracted 1D features can be used as an input image to train a Neural Network. An ECG is a graphical representation of the electrical activity of the heart, in other words it is an electric signal that varies over time. Usually an ECG was limited to a medical environment but recently some smartwatches capable of performing an electrocardiogram appeared on the market. This kind of signal seems a good example to start with a basic review of Fourier and Wavelet transforms. In order to gain some insight on the data we first begin by exploring the ECG data set which contains data collected from people classified into three groups, those with cardiac arrhythmia (ARR), congestive heart failure (CHF) and normal sinus rhythm (NSR) as shown in Figure 1. Figure 1: Representation of the three classes of ECG in our data, in line blue arrhythmia (ARR), in red congestive heart failure (CHF) and normal sinus rhythm (NSR) in green. Methodology In this part, we will present our methodology based on Fourier Transform (FT) and Wavelets (1) to extract features in order to classify the signals in three different classes: cardiac arrhythmia (ARR), congestive heart failure (CHF) and normal sinus rhythm (NSR). Beforehand, we should distinguish between continuous and discrete time signal in order to apply a discrete or continuous Fourier Equation . If the signal is a continuously occurring phenomenon, then we can represent it as a function of a time variable 𝑡 thus, 𝑥 ( 𝑡 ) is the value of signal 𝑥 at time 𝑡 , its domain is a subset of the real numbers. We speak then of a continuous signal. However, for a discrete-time signal, values are only defined at specific time-steps, it will be defined for example at every second, t = 1 s, t = 2 s, t = 3 s.  In our ECG example we deal with a continuous signal, so in the following we show how to apply Fourier Transform and Wavelets on this continuous signal. (1) It is worth to mention that these two techniques are the visible part of the iceberg because signal processing is a widespread field, Book . Fourier Transform Fourier analysis was developed to study periodicity in a signal and the main idea of this technique is to decompose the signal in its periodic components. Periodicity is a basic concept when it comes to signals, it means if the signal contains a pattern, which repeats itself after a specific period of time, we call it a periodic signal. The time it takes for a periodic signal to repeat itself is called the period 𝑃 , and the inverse of the period is named as frequency, 𝑓, for a signal with a period of 1 sec, its frequency is 1 Hertz (Hz). Therefore for a periodic signal the definition for the continuous Fourier transform F(k) of the time-domain description of a signal noted as 𝑥 ( 𝑡 ) is defined by: (1) Thanks to the Euler formula the equation (1) can be written in a different format using sine and cosine functions as : (2) After applying a Fourier transform on a signal we will obtain information about the component frequencies of the signal, F(k) in our notation. At this step we usually speak about space of frequency or frequency-domain description. The aim of applying Fourier transform is to decompose the original signal into a sum of its simpler signals, no matter how complex the original one looks. We refer to simpler signals, as the trigonometric functions sine and cosine. In general when we move from a signal in the time-domain to frequency-domain we speak of direct Fourier transform (FT) in the opposite case to change from frequency to time the equation is called the Inverse Fourier Transform. To illustrate the later equation with an example we will use a synthetic data set where we include a number of individual frequencies [2, 5, 3]. In Python, the FT of a signal can be calculated with the SciPy library in order to get the frequency values of the components of a signal. Figure 2: Synthetic data, in first horizontal box we plot the full signal in black, next boxes in lines red, blue and green are the individual components, corresponding to frequencies of 2, 5 and 3 respectively. In the vertical box we show the result of Fourier transform. In Figure 2 the black line corresponds to the full signal built as a linear combination of trigonometric functions plotted in lines blue, red and green, with frequencies 2, 5, and 3, respectively. From the original full signal these frequencies are unknown at this stage, just after Fourier transformation we get the frequency spectrum that compose the full signal, as plotted in purple in the vertical box. Towards a classification model: Going back to the example of the synthetic signal where it allows a clear decomposition in Fourier transform, we propose to collect all maxima positions peaks from the frequency domain, for example using the scipy library peaks , and use them as features. Doing that, the labeled signal ‘synthetic’ will have a row of associated maximum positions, here the peaks are easily identified as peak_1: (x_1=2, y_1=4 ), peak_2: (x_2=5 , y_2=1.5 ), peak_3: (x_3=3 , y_3=9 ). Figure 3: Schema representing the technique to extract features from a Fourier transform. Following the notation on the above table means that feature ft_peak _1_x is the x coordinate of the first frequency for the signal, and feature ft_peak _1_y is the y coordinate; feature ft_peak_ 2_x is the x coordinate of the second frequency detected and so on. Then our features in the example for the labeled ‘synthetic’ signal are : ft_peak_1_x , ft_peak_1_y , ft_peak_2_x, ft_peak_2_y, ft_peak_3_x,  ft_peak_3_y . In the case where our data set of signals allows a clear frequency identification we can feed a dataframe to train a classification algorithm. We can improve the problem description by adding more features exploring other signal processing techniques (concerning signal processing techniques you can delight with a large list of them in scipy .), like power spectral density (PSD) represented by the magnitude squared of the Fourier Transform. Even more, we can add statistical features like average, median, and variance of values. All these will be translated as more columns in the data frame. Having transformed the dataset into an exploitable data frame the next step is to choose a classifier algorithm to train, nevertheless we will continue with the study of another technique that best suits our data, follow us! Let us try the FT with a real signal extracted from the ECG dataset. In Figure 4 we plot as full signal an arrythmia example followed by its Fourier transform. Figure 4:  ARR ECG as full signal in green line, below FT result in purple color. In contrast to the very well defined set of frequencies obtained with synthetic data, in Fig. 4 the FT is less clear, we managed to identify some peaks but in some intervals they are not sharp. Why Fourier is not able to break down the signal clearly? To answer this we should pay attention to time. In the synthetic case we have a signal composed by three frequencies, and these frequencies are fixed on time, they do not vary. Fourier transform works when no variation in time happens, when we are in the frequency-domain we lose this dependency. In some way we can think of the Fourier transform as a trade-off between time information and frequency information. By taking a FT of a time signal, all time information is lost in return for frequency information. Our ECG signal is full of frequencies that vary on time, that is the reason why we can not resolve it clearly in frequency domain. Wavelets We saw in the last section that we need a method to handle signals whose constituent frequencies vary over time (e.g. the ECG data). We need a tool that has high resolution in the frequency domain and also in the time domain, that allows us to know at which frequencies the signal oscillates, and at which time these oscillations occur. The Wavelet transform fulfils these two conditions. In order to solve the problem of loss of knowledge from the temporal domain, the Wavelet transform modifies the shape of the simple sine and cosine functions of the Fourier transform. In a Wavelet the mother function is finite in time in contrast to Fourier where sine and cosine run from (-∞,+∞). Unlike a Fourier decomposition which always uses complex exponential (sine and cosine) basis functions (see Eq. 1 and 2), a wavelet decomposition uses a time-localized oscillatory function as the analyzing or mother wavelet, as shown in Figure 5. Figure 5: Real Morlet mother function. For a continuous signal, 𝑥 ( 𝑡 ) from one dimension, its transformed Wavelet into a 2D space is defined as: Being a a scale factor and b a translation factor applied in the continuous mother wavelet . We will call the wavelet by its mother wavelet name, for example shan for a Shannon kind function, and by two other parameters a and b , as in the case of shan1.5-1.0 in the first box of Figure 6, where a = 1.5 and b = 1.0 . b , is the central frequency parameter used to build the wavelet and it is related to the period by period = s/b , where s corresponds to the scale, it can be any array of values in increasing order. a , is associated to the bandwidth_parameter which selects how much the wavelet is sensitive to the frequencies around b . For complex mother functions, we show an example in the second box in Figure 6, where the mother wavelet cgau3 represents cgau for the complex Gaussian mother function and the number 3 is associated to the order of the derivative of the wavelet function. In summary, if we have a dynamical frequency spectrum, in other words if our signal has frequencies that change in time, then we need a tool with resolution not only in the frequency domain but also in the time domain, a Wavelet . In short, we should catch that wavelet transform is an efficient tool for analysis of short-time changes in signal morphology. Before to apply a wavelet transform on our ECG data set, let us a minute to show you what kind of functions we can find as a mother wavelet. A big difference with the Fourier transform, where sine and cosine are used as basis functions is that for wavelets we have a family of them: ‘Haar’, ‘Daubechies’, ‘Symlets’, ‘Coiflets’, ‘Biorthogonal’, ‘Reverse biorthogonal’, ‘Discrete Meyer (FIR Approximation)’, ‘Gaussian’, ‘Mexican hat wavelet’, ‘Morlet wavelet’, ‘Complex Gaussian wavelets’, ‘Shannon wavelets’, ‘Frequency B-Spline wavelets’, ‘Complex Morlet wavelets’. The choice of a particular mother function will depend a lot on the signal to be treated, we can even create our own wavelet function . As can be expected the Wavelet Transform comes in two different and distinct flavors; the Continuous and the Discrete Wavelet Transform similar to Fourier. Figure 6: Some of the members of the family wavelet functions used to compute the transform. First column are wavelet functions, second column corresponds to description of a and b parameters. Now we are ready to see in action a wavelet transform applied on one of the signal of our ECG dataset. For better visualizing the transformation we will use an 𝑠𝑐𝑎𝑙e𝑜𝑔𝑟𝑎𝑚 , a tool that build and displays the 2D spectrum for Continuous Wavelet Transform (CWT). An scalogram takes the absolute value of the CWT coefficients of a signal and plot it. Figure 7 : Top box corresponds to an interval of ECG signal class: ARR, the box below is the results after applying a Wavelet transform with a Morlet wavelet. Before diving into a machine learning subject it is worth understanding the scaleogram output. In vertical axe we plot the period (defined above), in the horizontal axe we show the scale, there is a relationship between scale and period mediated by the central frequency ,  a parameter of the chosen wavelet, frequency  = b/s . We can interpret each horizontal characteristic in the scaleogram as a frequency of the total signal. The fact of not seeing a continuous line in our figure corresponds to that said frequencies are not continuous in time. A very relevant point to our exercise is the fact that the scaleogram can be understood as a picture, an image as any other and then apply a model like NN to train a classifier as we will show you in the next section. Application Training the ECG classifier with scaleograms Interpreting ECGs is often difficult, then automatic interpretation of ECGs is useful for many aspects of clinical and emergency medicine, for example in remote monitoring, as a surgical decision support or in emergencies. In this section we perform a classification, as a rapid method of disease identification. The fact of transforming ECG signals into scaleograms allows us to use a visual computing model to classify the different types of diseases in our data set. As any image we can decompose the scaleogram in RGB spectra. An important fact to note is that each row is an ECG recording sampled at 128 hertz, this means that the scaleogram of a single signal-component produces an image of 127 by 127 pixels. Our data contains ECGs from three groups of people, ARR, CHF, NSR, the first two correspond to diseases and the third group are healthy people. To classify these groups, we propose to use basic neural network using TensorFlow 2.0, so we keep in mind that our images have a format 127 pixels to our input shape and decompose in 3 filters due to RGB, then our model has the following characteristics: Figure 8: Schematic representation of our Convolutional Neural Network. After running 10 epochs using a stochastic gradient descent as optimizer, and computing the loss with a sparse categorical cross-entropy, the accuracy metric shows a very good performance, as shown in Figure 9. Figure 9: Learning curve for 10 epochs, in blue and green lines we plot loss and validation loss, in orange and red we plot accuracy and validation accuracy. Last step to consolidate our model is to explore one metric more appropriate to a multi-class problem, the confusion matrix, represented in Figure 10. A Confusion matrix gives us a good idea of the performance of a classification model. Each line corresponds to a real/true class, each column corresponds to an estimated/predicted class. We compare true values against predicted values, then for good models we should expect a high number of elements in the diagonal, where all elements predicted correspond to the real ones. In our case we normalized the confusion matrix, so the maximum value expected in the diagonal is 1. From Figure 10 we can see that our model predict very well all the three classes because all values in the diagonal are very close to 1. After transforming our ECG data into scaleograms we reach a very good classification result for the three groups of persons. Now, if you have a smartwatch that performs ECG, at least you can know in which of these three groups you are, cross fingers in NSR . But remember, a doctor’s advice is always recommended! Figure 10: Normalized confusion matrix plot for classification results obtained with the CNN previously defined in Figure 8. Conclusion In this article we have seen two different ways to extract features from signals based on time dependence of frequency distribution, Fourier and Wavelet transforms. These functions transform a signal from the time-domain to frequency-domain and give us its frequency spectrum. We have learnt that Fourier transform is the most convenient tool when signal frequencies do not change in time. However, if the frequencies that make up the signal vary over time, the most performant technique is a wavelet transform. The last case allows us to explore the frequency domain of the signal as an image by formatting the output as an scaleogram and then take advantage of image classification techniques. Based on ECG data, we made a classification over three groups of people with different pathologies: cardiac arrhythmia, congestive heart failure and healthy people. With a very simple neural network we were able to get a precise model which quickly allows us to detect a healthy person from others with heart disease. Next steps : We believe that this work is the starting point to a large study of ECG signals,  next steps can be the cleaning of real signals and the inclusion of other pathologies in order to enlarge the classification scope of diseases. We also propose the use of low pass and/or high pass filters to clean the signal. In parallel the study of sound waves as ultrasound can be explored with theses techniques and of course anything that you challenge. Code: https://github.com/mnf2014/article_fft_wavelet_ecg/blob/develop/wavelet_article_octo.ipynb Links of interest: https://github.com/alsauve/scaleogram/blob/master/doc/scale-to-frequency.ipynb http://ataspinar.com/2018/12/21/a-guide-for-using-the-wavelet-transform-in-machine-learning/ http://nicolasfauchereau.github.io/climatecode/posts/wavelet-analysis-in-python/ https://www.mathworks.com/help/wavelet/examples/signal-classification-with-wavelet-analysis-and-convolutional-neural-networks.html#d117e8158 https://towardsdatascience.com/what-is-wavelet-and-how-we-use-it-for-data-science-d19427699cef Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data , Data Science and tagged bigdata , feature extraction , machine learning , time series . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-11-23"},
{"website": "Octo", "title": "\n                The Google Assistant for Android developers – PART 1            ", "author": ["Nicolas Telera"], "link": "https://blog.octo.com/en/the-google-assistant-for-android-developers-part-1/", "abstract": "The Google Assistant for Android developers – PART 1 Publication date 07/10/2019 by Nicolas Telera Tweet Share 0 +1 LinkedIn 0 Everybody knows about the Google Assistant , available on most Android and iPhone devices. As for “Hey Siri” , the well known “Ok Google” has entered common language. We probably all have already used it at least once if only to try it. However, the scope of its field of action and implementation seems to be mysterious for us developers. At least, it was for me until recently. Is it only a mobile feature? Can we use it in our application? What are the possibilities of interactions? We will answer these questions and more through this article and we will then focus on what interest us here: how to build an interaction with our Android application through the Assistant. The Google Assistant? The Google Assistant is a conversational interface that we interact with, mainly using our voice. Though popular on smartphones, it is present on more than a billion different devices such has speakers, smart screens, cars, TVs and connected watches. Its main goal is to improve the discoverability and the interactions of our applications and websites. We can for instance use it by a long press on the “Home” button of our smartphone or simply by saying “Ok Google” or “Hey Google” . It is based on Natural Language Processing (NLP) and artificial intelligence in order to transform a vocal input into a request that a computer program can interpret. To put it simply, when the request of a user matches a specific grammar, the Assistant extracts the request parameters into schema.org entities and generate Android Deep Link URLs using the mapping given into the actions.xml file. A lot of interactions, that we know of, already exist such as launching a video on YouTube or a song on Spotify, the displaying of an itinerary on Google Maps, the setting of a timer in our Clock application or the simple triggering of an internet search. But there are way more possibilities. Implementation on Android But then, can I, Android developper, build my own interactions on the Assistant? The answer is yes, but to a certain extent. Google offers to the developers the possibility to build their own interactions by using actions called App Actions that are intents holding a request to establish a link with our application. There are many possibilities to build these actions. We will focus on the two that are relative to the Android development for our application: The “ Solutions ” part which contains actions provided by Google to simplify everything for us. These actions are built-in intents that will allow us to build interactions in no time. However, they will not let us build conversations with the Assistant. See them as triggers, simple orders. Also, they can only be used to launch features in our applications with very few visual responses. The “ Custom Conversations ” part, way more interesting from a developer perspective. These actions are build by the developer through DialogFlow and allow us to create “real” conversations with the Assistant. They can also provide visual interactions that will never require the launching of the application. First, we will focus on the “Solutions” part in order to understand the concept of a simple App Action through its implementation in an application. Then, we will see the “Custom Conversations” part in a later article. Implementation of a simple App Action Before starting, it is important to know that by now, App Actions are still in developer preview. We will thus be able to build and test our actions but it will be impossible to trigger it through voice command in the Assistant. But don’t worry, we will still be able to visualize the result of our work into it. We will create a simple application that will be launched using one of the build-in intents provided by Google. The goal is to launch a feature by specifying its name. It will look like this: a main activity which is a hub leading to 3 features through 3 buttons one activity per feature displaying a title, a subtitle and an image Finally, we will improve our interaction with the Assistant by displaying a Slice holding the information of the requested feature. Let’s do this! Prerequisite In order to build an App Action for our application, we need to setup a few things. First, it is important to know that App Actions are only available starting Android 5 (API 21). Also, it is necessary to implement deep links into our application to allow Google to link our actions to our activities. We will not cover this part here, but it is easy and quick to generate these deep links through the App Links Assistant accessible from the “Tools” tab of Android Studio. Finally, it is primordial to have our application uploaded on the Google Play Console (a draft is enough) in order to be able to test our actions, and to be logged with the same account on the Console, in Android Studio and on our device / emulator. The application First, let’s build a basic application. The code being very simple and of no specific interest, it is not shown here but the code is available on GitHub. You will find there an AppActionsActivity which sets 3 listeners on 3 buttons allowing the launching of each feature. You will also notice the presence of an intent-filter in the manifest to handle deep links. Adding of an App Action Among the built-in intents provided by Google, we can find a lot of generic actions such as START_EXERCISE , CREATE_TAXI_RESERVATION or GET_ACCOUNT , each one enabling the launching of a specific feature in our application with the appropriate parameters. We will use here the most generic of them all: OPEN_APP_FEATURE . In order to do that, we need to create a new “xml” package into the “res” directory of our application and add a new actions.xml file there. This file will hold the structure of the actions that will be in our application and the different ways to access them (deep link or Slice) with the accepted and / or necessary parameters. So let’s add our action to the actions.xm l file: You can notice the different parts: intentName the name of the used built-in intent fullfilmentMode the mode to fulfill the action, here a deep link urlTemplate the URL template to use for our deep link with its parameters intentParameter the name of the parameter taken from the URL that will be passed to the intent sent to our application urlParameter the name of the parameter to map in the URL We now need to point to this file in our AndroidManifest.xml : Warning : at the moment, it is impossible to upload an APK or an AAB containing an AndroidManifest.xml pointing to an actions.xml file on the Google Play Console. You just need to remove it from the manifest before uploading it and then put it back locally to be able to test your App Actions. We still need to handle the received intent within our application. To do this, let’s create a private method in the AppActionsActivity , called from onCreate , to extract the data from the intent in order to check that its type is Intent.ACTION_VIEW , that it contains the necessary parameters if needed and, in our case, redirect to the specified feature. For the sake of this article, we decided to handle the redirection from the AppActionsActivity , but we could have decided to create one deep link per feature to avoid this redirection. Built-in intents only are pre-formatted entry points but the behavior resulting in the application is the responsibility of the developer, which offers great freedom. It is time to test our App Action. To do so, you need to install the “App Actions Test Tool” plugin and launch it from the “Tools” tab in Android Studio. By clicking the “Create Preview” button, Google checks the presence of an application sharing the same application ID on the Google Play Console and then generates the necessary deep links automatically. You can notice that our OPEN_APP_FEATURE App Action has been configured and that all we have left to do is type the name of the feature we want to launch before clicking “Run”. You can now see that the Assistant launches on your device / emulator and redirects us to our application then to the right feature. Note that if the asked feature doesn’t exist, the application will still be launched by default. Displaying of a Slice In order to make our interaction with the Assistant more visual, we are now going to add a step between the request to the Assistant and the launching of our application through the implementation of the Android Slices . Without going into the details of implementing Slices, we are going to see the configuration to use so that the Slice is sent to the Assistant. We need to add one entry point into our actions.xml : As previously with the DEEPLINK entry point, we can see: fullfilmentMode the mode to fulfill the action, here a deep link urlTemplate the URL template to use for our deep link with its parameters (it must respect this format: “content://{slice_authority}/…”, the slice authority being declared in the manifest) intentParameter the name of the parameter taken from the URL that will be passed to the intent sent to our application urlParameter the name of the parameter to map in the URL Thus, when you run the App Action from the test tool, the application will send a Slice holding the information of the requested feature (title, subtitle and image) to the Assistant and that will redirect us to the specified feature when clicked. It will also be necessary to grant the permission to access Slices to the Assistant at launch by asking the permission in an Application class: Conclusion In this article, we have seen what is the Google Assistant, mainly from the Android perspective. We have built a small application allowing, in the first place, to access a specific feature and then to display a Slice into the Assistant redirecting to the displayed feature when clicked. And all of this very easily using the “Solutions” part offered by Google and providing built-in intents that can automatically handle deep links. We will see the “Custom Conversations” part in a coming article in which we will focus on DialogFlow. During my research, I encountered some difficulties to which I still don’t have answers. For instance, I wanted to use only one FeatureActivity that would be configured with an extra holding the requested feature. But my activity always stayed configured on the first invocation (cache system?). Also, I had many instabilities with the Assistant on an emulator. For the time being, I would advise to test your App Actions directly on your device (you may need to configure its locale to en-US). Note that, when you App Action is ready, it is possible to submit it to Google through a form in order to be able to deploy it to production. Finally, I would like to remind Elaine Batista Dias ’s message, which I thank for her help, from her talk during the 2019 edition of Android Makers: “Google Assistant is still new, think outside the box.” See you in the next article . Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged android , androiddev , assistant , mobility . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 3 commentaires sur “The Google Assistant for Android developers – PART 1” laxman 03/02/2020 à 10:53 hi, How to create intent.actions.EXAMPLE_NAME and intent parameters and how to register schema.org and deep link in using action.xml file Thamilselvan K 03/02/2020 à 11:07 where do i write \"one\" value ? Asif 26/04/2020 à 05:14 Hi, Thanks for the above explanation but this isn't working.\r\n\r\n1. Open_app_Feature test tool says \"History\" where you have shown \"one\"\r\n2. I am getting this error in Test tool \"Reminder: You must be logged in to the same Google account in both Android Studio and your test device to access your preview.\" Though I am logged in to Android Studio By My account that is used in mobile and Play console too. My app is already published to Play store.\r\n3. The assistant says, no app is linked to this URL.\r\n\r\nPlease help.\r\nThank you. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-10-07"},
{"website": "Octo", "title": "\n                Seven shades of Git            ", "author": ["Léo Jacquemin"], "link": "https://blog.octo.com/en/seven-shades-of-git/", "abstract": "Seven shades of Git Publication date 14/09/2019 by Léo Jacquemin Tweet Share 0 +1 LinkedIn 0 Tl;dr Every time I stumble upon a Git article on the web, it always talks about how to use it, instead of why I should use it. Wait, isn’t that obvious? We use Git for versioning our code and sharing it with others. This is what Version Control Software (VCS) is used for, afterall. Thinking like this, it is easy to believe that you can Google your way through Git mastery by searching how-to’s every time you’re stuck on a specific Git-related problem. At least until you reach a point where you don’t really need it any longer, a point which I personally have failed to reach by a very long shot, by the way. I shared this mindset, one widespread among developers, until recently my colleagues and I had a long talk about collaborative work, code reviews , and why these are so hard. And after sitting down together, trying to figure out the perfect pull request, this article summarizes our ideas. I tried to make it into fifty shades of Git, but unfortunately fell short with only seven: Green branches : all commits should pass the tests Commit metadata: 50 char messages, description body to capture context Commit boundaries : typology, order and atomicity Branch status : always up to date with the development branch Code churn : use interactive rebasing to eliminate the churn before sharing More rebasing: useful tips during interactive rebases Unresolved : code review commits Introduction When I first started learning about web development several years ago, teaching myself how to build a chat in my bedroom, I quickly realized Git wasn’t going anywhere anytime soon . Nearly every tutorial I could find began with the same two words – the birth certificate of most software projects: git init At the time, I had a vague idea of what Git was used for. Yet, I was happy to blindly chain commands such as: git add .\r\ngit commit -m “save progress”\r\ngit push heroku master 1 Fast-forward to today, I’m still using Git everyday although my usage of it has dramatically shifted. What once was a convenient utility I used to push my code to the cloud has turned into an indispensable program, one that I use to present my work to the world. More specifically, I use it to present my work during code-review , where my co-workers are tasked to read my (hopefully multiple) commits and write comments, make suggestions and engage in conversations, with one noble goal in mind:  finding defects and improving the overall quality. But code reviews are hard… really hard. Now, depending on the level of expectation, from simple routine checks to complete bulletproof readings, the level of required concentration varies a great deal. The bootstrapping of my current project, several months ago, illustrates this perfectly. On the one hand, we had Git artists that would submit pull requests with brilliantly crafted, single-purpose, atomic commits. They would expect each commit to be read chronologically, and with great care. On the other hand, Git apprentices that happily opened massive 750+/500- pull requests that would typically look like this: 0be0f5f - Developed feature #42 (John) ef43abb - missing test (John) 18f47d8 - fixed typo (John) Between these two extremes, many developers such as myself were not really doing things consciously, and even less conscientiously. We never had put any thought into this. I reviewed my co-worker’s code slowly, painfully, every time the pull request was not trivial. I didn’t know where to start nor what to look for in order to be effective. Most of the time, I would just hover over the change set, half clueless about what to look for. Eventually, all these inconsistencies caused us much trouble, which is precisely what caused us to gather in order to figure out once and for all what would make a fantastic pull request. This is what we did afterwards, and, up to now, my personal feeling is that we’ve seen a tremendous level up from everyone, thereby making the whole code-review experience much more effective and, equally important, enjoyable. I consider the following guidelines to be best practices no matter where your team stands on the code-review scale. They can even prove beneficial beyond the code-review process, such as when browsing the distant Git history. These guidelines fit remarkably well with a certain utilization of Git also known as the GitHub Flow , where feature branches are first-class citizens. It is important to acknowledge that other models exist. Always adjust to your current workflow. 1/ Green branches A green branch is simply a Git branch in which all commits pass the tests, hence, are green. The reason why this is so important has to do with the three basic tasks we’re doing when facing other people’s code: reviewing it , to improve quality before we merge exploring it , to understand its history merging it , and fixing potential conflicts Let us explain why. For each aspect, green commits are so important, starting with the simplest during code review. Reviewing it Submitting a pull request with red commits is akin to explicitly asking to merge code that do not behave as expected, or that the tests are not to be trusted. Either way, the code is obviously not ready to be merged. Green commits by themselves don’t prove anything. It is possible that they are green for the wrong reason: false positives, flaky tests, misunderstanding of requirements. Red commits on the other hand, certainly prove that the original author does not believe its code to be correct. In the absence of any test, the indication of correctness solely relies on the author’s word that it works. I don’t mind giving feedback on untested code, but I would probably not call this a code review, nor would I give my Approved on GitHub. One could argue that it’s okay to have one red commit as long as the second one is explicitly labeled “Fix the tests”. I hope that by the end of this article, I will have changed your mind on that point. Exploring it Another important reason for having green commits is when we use Git to browse the history. Imagine you were to check out old commits in the so-called detached HEAD state , what would you do if your first noble reflex, to run the whole test suite, were to fail you? This is what happens when a ‘ yellow 2 ’ branch gets carelessly merged into development . People may have had a good reason to do it at the time – maybe the test was fixed in a subsequent commit. But how would you know if the commit message was not explicit? How can you branch off something from it in plain confidence? Besides, having red commit will defeat the purpose of the git bisect command. Indeed, this command lets run use a divide & conquer approach to quickly find a commit that introduces a bug. If some of your commits are red for the wrong reason, this will prove really challenging to use it effectively. Merging it Finally, the last reason why green branches are so important: when merging other people’s code into ours. In a perfect world, merges would always happen smoothly and Git would always automagically know what to do. But the world is far from perfect. In the real world, merge conflicts happen. If code reviews are hard, merge conflicts are even harder. First, conflicts require you to understand one of the multiple Git conflict-resolution layout styles which, depending on the layout you use, is arguably as hard as understanding monads . Then, you have to figure out which line to keep, which lines to drop, and, finally, which lines to actually merge by hand . By merging by hand , I mean understanding that this conflict, presented in diff3 : <<<<<<< HEAD\r\nfunction getUser(userId, opts) {\r\n||||||| merged common ancestors\r\nfunction getUser({ id }) {\r\n=======\r\nfunction getUserById(id) {\r\n>>>>>>> change-signature Should in all likelihood be resolved like this: function getUserById(userId, opts) { This implies to figure out that one side of the merge is concerned about making the signature more explicit (by diffing it with the common ancestor thanks to diff3 ) whereas the other side wants to allow for a second parameter and make the first parameter more explicit. Then, you need to figure out how to apply both changes at once! A tedious process that in practice is always both annoying and error-prone. Now, after doing so, you’ll often want to run the tests to ensure your merge still makes sense. Unless…one of two sides of the merge did not pass the tests in the first place. In this case, resolving conflicts can prove to be really challenging. The further you are to the ‘ Unit’ end of the automated test spectrum, the less likely you will be to face this issue. However, if you test close to your code, this will happen frequently, which in practice will often result in coding the whole function again in pair-programming with the original author. People who have experienced this will tell you how annoying it is, both for the reviewer and the author. Shade n°1 Each commit should pass the tests. 2/ Commit descriptions The second idea we came up with when figuring out what makes a git branch look great, were commit descriptions. By description, I mean commit messages and commit description bodies. For commit messages, we settled for classic recommendations from seasoned Git users. This is the template we used: Capitalized, short (50 char or less) summary More detailed explanatory text, if necessary.  Wrap it to about 72 characters or so.  In some contexts, the first line is treated as the subject of an email and the rest of the text as the body.  The blank line separating the summary from the body is critical (unless you omit the body entirely); tools like rebase can get confused if you run the two together. A properly formed git commit subject line should always be able to complete the following sentence 3 : If applied, this commit will … The Capitalized present-tense first verb ensures that commits will look like Git automatic commit messages such as when doing a merge or a revert . The length limit of 50 characters will help ecosystem tools like GitHub always display the full message instead of truncating it with ‘…’, which is nice to have. As do most habits, writing commit messages this way can take some time to get used to. However, I don’t think most of the value from a VCS tool like Git really lies with commit messages. Description bodies are by far the most overlooked Git “feature” of all time, in my experience. In fact, they weren’t part of the guidelines my team wanted to enforce on each commit, but I decided to add it to this article as a personal note. I believe that each commit, even the ones consisting of what appear to be minor changes, should always have a description body. I believe such a restrictive rule should be enforced because each single commit is always motivated by a reason. This reason is probably the most important piece of information after the code itself, and yet, by lack of proper description bodies, many commits fail to capture it. Eventually, the reason for the change will fade away its original author’s memory and nobody will ever remember it. One common objection to the idea of always having description bodies is that sometimes, they really feel overkill. Cosmetic changes fall in this category. We’ll address this objection in guideline 5 on code churn. Another objection is that, sometimes, the architecture of a project is so that features always go through the same series of steps, each one materialized by a different commit, which makes the description bodies quickly redundant. Here is an example of such well-defined steps that could occur: Performing a migration Adding a domain model Adding a repository method Adding a serializer And so on… I will admit that if your architecture is very mature and stable, and your codebase legacy-free, without a shred of dust, then it is true that commit messages such as: 0befe5d - Add a candidate table migration 1fe0f5c - Add a candidate model 2baef5b - Add a candidate repository 5fe0fbf - Add a candidate serializer can encompass all the context of each change set. However in practice, I have yet to see a codebase like this. This is what i’m used to instead: 0befe5d - refactor mig directory and add candidate migration 1fe0f5c - Added a candidate model and some cleanup 22eff5a - Fixed tests after rebase 2baef5b - added a method users repository to fetch candidates 5fe0fbf - Change serializers signatures Aside from the fact that these commits don’t follow the previous guideline regarding commit messages, there is nothing fundamentally wrong happening. The actual codebase could actually be very healthy,  but the least we can say is that there are a lot more going on here than just creating a migration, a model, a repository and a serializer. If those additional changes (refactoring, out-of-scope improvements…) were to be committed separately, maybe the branch history would resemble this: 1fe0f5c - Refactor the migration directory 0befe5d - Add a candidate table migration [x] 947ef42 - Leverage functional programming in model importer 1fe0f5c - Add a candidate model [x] 5fe0fbf - Add a method to user repository to fetch candidates 2baef5b - Extract candidate repo out of user repository 5fe0fbf - Update serializers to use jsonize2.js abc0fbf - Add a candidate serializer [x] Then, perhaps commits [x] 0befe5d, 1fe0f5c and abc0fbf could skip having a description body without any loss of context. But the other ones beg some serious questions: why was the migration directory refactored? (assuming refactoring such a directory have any sense) Why the apparent need to leverage functional programming in the model importer? Why introducing the jsonize2 library in the serializer? All these enigmas could probably be solved by the end of the day if their authors were still working on the project… but how about when they’re not around? How about when they are not working on the project anymore? One could argue that at some point, some commits are too old to be relevant. Therefore, description bodies are overkill because by the time they could be useful, the modified code gets obsolete and newer versions of it take precedence; versions for which we can always be sure to find the author around, if the need for explanations arises. To put it another way, “old” commits are virtually useless. I find this idea bothering for at least three reasons: First, how old is old? There are absolutely no guarantees that the author will still be around at all, nor that anyone on the team for that matter, will be able to provide detailed explanations of why a change was made. Second, being blocked on my task because I lack information that I can only get by taking a co-worker out of his zone is annoying at best, and unthinkable for other people. Last but not least, the refactoring argument. Every refactoring usually requires first to understand the code subject to a refactoring. After all, if we are to change the code without altering the behavior, we better understand the initial behavior in the first place. However, the code rarely says the whole truth about itself. Comments are notoriously hard to maintain, and self documenting code has limits. This is why we often find ourselves figuring out the code the hard way instead of simply reading it, just like we would read a book. This is where commit description bodies prove the most valuable to me. Understanding the whole truth about some code, why it was introduced in the first place, why it evolved this way. For all these reasons, I believe that providing context to each commit, past and future, is a game changer for a project’s long term success. Shade n°2 Each commit metadata should be templated (and mandatory). No matter which side of the argument you’re on, one thing remains true either way: description bodies are related to the content of the commit. As we saw, depending on the maturity of your codebase, a description body may or may not be mandatory. This brings us to our third recommendation: how should each commit be crafted? 3/ Commit scope : typology, order and atomicity As we just pointed out, the code rarely commits itself without human intervention. Each commit always has at least one reason to be. The third guideline is that each commit should have precisely one . This is somewhat tantamount to the SRP , applied to Git commits. The only problem with this principle is that nobody really knows what a responsibility is. Although it is true that a few categories of responsibilities have reached consensus, such as the ones we used in our fictitious example before, heated arguments still happen everyday over what a responsibility is or is not. In fact, it is even likely that the original author had something completely different in mind when he coined the term. For our purpose, we needed a more concrete definition. To specify what we meant by reason to be to avoid unnecessary debates. We came up with three useful criteria which we labeled typology, order and atomicity. Commit’s typology refers to the type of commit. There can be only two kinds of commits : structurals and behaviorals. This binary way of classifying changes to the codebase happened to have a lot of benefits, as it was very easy to distinguish between the two. Structurals commits were about changing the structure of the code without changing the behavior, a.k.a, refactorings. Behavioral commits were commits that directly changed the behavior, a.k.a, new business requirements from the product owner. Once commits are divided like this and commit messages are well written, the practice starts to feel more like Git grooming than Git hacking. This is a good sign. Let’s continue. As is turns out the order in which these commits are written can also impact both the development and the code review productivity. Many of us had trouble figuring out what refactorings were in or out of scope of their current task. Just like with pull requests, before getting on the same page, we all did things differently, and behavioral commits showing up randomly between refactoring commits were quite common. To readjust all these misalignments of expectations, we decided to follow a well-known craftsman’s adage that advocated first to make the change easy, then make the easy change, which became our second criterion. In practice, this led to the following history: 1fe0f5c - Add new algorithm to compute scoring (+250/-50) To be rewritten: 1febf5c - Extract a scoring service from game service (+100/-100) 5ab025e - Add Nash’s algorithm to scoring service (+250/-50) At this point, the history looks a little clearer, but that’s just splitting one commit in two. Nothing groundbreaking. Besides, the behavioral commit 5ab025e still contains a generous diff. This leads us to our third criteria regarding commit scope. Atomicity. When going the atomic way, one must legitimately ask at which points a commit gets objectively too small. If Git could speak, his answer would be straightforward: never . Git is designed to work well with the smallest possible diffs. Most of its job consists of tracking changes to the project’s directory and recording the tree as it grows 4 , which it does far more easily processing leaves of diffs than trunks. But Git is no human. There are two competitive forces that tend to make commits expand bigger. Unfortunately, we believe that only one of them is a valid reason. With behavior commits, we found that the line was easily drawn. Every method with its associated unit test seemed to be right candidates. Although remember, any block of code that gets in your way should be refactored and committed separately. Structural commits on the other hand, are a different ball game, although not for the reason one might think. Coming up with a list of recurrent refactorings we do all the time and decide that those should appear in separate commits is the easy part. For a comprehensive list of good atomic commit candidates, Refactoring could prove useful. The hard part is overcoming our own reluctances to making commits that atomic. Why? The reasons are unclear. However, I believe it is probably the same inner reason why we occasionally refrain ourselves from extracting one liners into a separate function. Though these reasons have no sound basis. Furthermore, going against them for months have had really surprisingly great results. The initial psychological discomfort was quickly overruled by the mental comfort of reading crystal clear git log outputs and the productivity gain that followed. Besides sometimes, even with the slightest modification, making a separate commit just makes more sense. For instance in the renaming of a file, high fan-in classes will cause a huge diff, and therefore a lot of noises when committed with another refactoring. Plus, if the renamed file includes more than 50% 5 differences with the source file, Git will not treat it as a rename but as pair of addition/deletion of files, thereby disconnecting part of the history, unless the –follow flag is passed. Building upon the previous example, such a history could be: Make the change easy 1febfff - Extract a scoring service from game service (+100/-100) ( struct .) 4febb5c - Extract a _computeScoring() method (+45/-40) ( struct .) 2fbaac5 - Replace Promise calls by async/await (+15/-12) ( struct .) Make the easy change 5ab025e - Add Nash’s algorithm to scoring service (+50/0) ( behavioral ) Ultimately, we would realize that the feature would be implemented by just adding fifty lines of code, which is much better than our initial monolithic commit with the diff 250+/50-. As you gradually adopt these guidelines regarding the ideal shape of a commit, you will most likely realize that coding first and committing then may not be the ideal flow of operations. The git add –patch command certainly proves helpful in many situations, but it won’t let you chop chunks of code beneath a certain threshold, and even so, crafting commits afte r will quickly feel awkward. Once you feel the awkwardness of trying to fit your diff in multiple commits after writing the code, it won’t be long before you find yourself hooked into what could be referred to as “Commit Driven Development”. Thinking about your next commit first , both its subject and description body, is the best way to ensure it will be as atomic and freestanding as possible –  if only for the fact that it will force you to think thoroughly about the problem instead of rushing to code a solution. Shade n°3 Each commit should be either structural or behavioral. 4/ Staying up-to-date with your development branch The three previous guidelines were somewhat agnostic of your git branching model. They could be considered solid advice for just about any commit. The following is specific to the GitFlow branching model, in which many feature branches originate from a main branch. When the feature is ready and peer approved after code review, the branch gets merged. It is considered a best practice by many teams to always rebase its local branch the latest version of the common branch. This way, a developer can ensure that its latest work is compatible with the latest version of the code. The purpose of this practice is twofold: First up is code review. Similarly to green commits, submitting your feature branch for review without ensuring first that it merges without conflict or that the resulting merge is functionally correct should be the responsibility of the team working on the branch, not the team reviewing the code. This may be obvious for many people but I’ve seen developers argue about who should fix a merge conflict, as if it was a matter of courtesy. I don’t think that it is the case, nor that it should be. Such decisions should not be driven by a misplaced sense of politeness, but by economics instead. In this case, it is exceedingly easier for the team on the pull-request side to know how to reconcile two conflicting pieces of code than it is for the other side. Hence, always ensure your branch merges integrates smoothly. Second is to have a linear history. When Git is seen as a tool to manage the source code history, we need to strive at making the output of: git log --oneline master to look like a poem instead of a rough draft. We want to be able to think of our application releases at three different levels of abstractions: The application level: v1.0 ---- v1.1 ---- v1.2 ---- v2.0 ---- v2.1 (origin/master) The development level: feat1 ---- feat2 ---- feat3 (origin/dev) And finally, the feature level : C1 ---- C2 ---- C3 ---- C4 ---- C5 (origin/my-feature) When accidentally merging a branch without rebasing it first, the history becomes harder to read and the mental representation above distances itself from what really happened. Shade n°4 Each pull-request should be based on dev. 5/ Removing the churn Assuming your commits: all pass the tests have a well-formatted commit message and a meaningful description body are either structural or behavioral in the right order as atomic as they can get on a branch that is up to date with your development branch should we stop now, before being accused of Git fanaticism? On the one hand, the previous guidelines make for some really awesome branches. On the other, we’d be remiss to have gone this far and not discuss this last piece of advice – one that will make your Git branches shine so bright that your co-workers will need sunglasses to review your work. This last one is best illustrated with an example. Consider the following history: 1febfff - Add a getByItemIds() method --------------------------------------- The order service needs to be able to fetch multiple products by item\r\nin one batch for performance reasons. 4febb5c - Refactor getByItemIds() to leverage lodash --------------------------------------- Most of the other repository methods leverage lodash in order to leverage the _(collection) syntax. This commit refactors the function to conform to what’s already present in the codebase. 54efa78 - Rename get into find to match semantics --------------------------------------- The current semantic for repository methods is that get should throw an error when nothing is found. The desired behavior here is to return an empty\r\narray [], hence the renaming getByItemIds() → findByItemIds() What is wrong with this branch? All the previous guidelines have been duly respected, yet this three commits have a serious problem. Each commit obsoletes the previous ones, for a reason that has nothing to do with a change of business rules. To put it another way, we are being presented the chaotic and tedious development phase the author went through. The problem being that this information has no added-value for the reviewer. When multiple commits affect the same group of lines in a row, we increase the code churn . A churn like this is perfectly okay during development. However, before sharing your code with the rest of the world, eliminating these redundancies is much appreciated. To make the magic happen, simply rebase your work on top of the latest version of development, using the interactive flag: git fetch -p git rebase -i origin/dev This will open up an editor asking you what you want to do: pick 1febf5c Add a getByItemIds() method pick 4febb5c Refactor getByItemIds() to leverage lodash pick 54efa78 Rename get into find to match semantics In our case, we would like to merge all commits, and adjust the resulting commit message. We will instruct Git to fixup 4febb5c into 1febf5c , to fixup 54efa78 into the result of the previous fixup , and finally to reword the result: reword 1febf5c Add a getByItemIds() method fixup 4febb5c Refactor getByItemIds() to leverage lodash fixup 54efa78 Rename get into find to match semantics After editing the commit message interactively, our history now looks much cleaner, with one single purpose commit: f42eabc - Add a findByItemIds() method --------------------------------------- The order service needs to be able to fetch multiple products by item\r\nin one batch for performance reasons. Interestingly, only the commit message changed. The description body did not since all subsequent commits were simple refactorings and added no business value. Shade n°5 Use interactive rebases to reduce the churn. 6/ More rebasing Once you engage in an interactive rebase, things can get messy faster than expected. Although a git rebase status command exists, reducing the cognitive load by any means necessary is a healthy practice. Here are some advice that will help you manage the complexity. The first one is always to do one action per interactive rebase 6 . The reason is because if one of your actions were to fail and get you to a point where it would be easier to just start over , all your previous actions would be lost. Therefore, it is always a best practice to chain multiple interactive rebases instead of doing it all at once. To say it differently, use interactive rebases a lot, but keep the interactive part to a minimum every time. During rebases, interactives or not, Git offerts a convenience command: git rebase --abort This commands allows you to cancel any modification happening with the current rebase. However, once the rebase has been done, the command does not work. To go back in the state you were at before doing multiple successful interactive rebases, simply hard reset your current local branch on the remote tracked one: git reset --hard origin/<my-branch> This command is useful when you are certain that you have messed something up. To ensure that you have not messed something up after doing multiple squash , fixup , edit and reword actions, simply ask for the diff between the result of your current branch and the one before rebase: git diff <my-branch-after-interactive-rebase> origin/<my-branch> If the diff gives you an empty output, you’re good to go. Otherwise, it probably means that you deviated a bit and need to double check on it before pushing to the remote. Now, no Git article would be complete without a scary and, equally importantly, a virtually useless Git command. One that the author would use to insinuate that even Hamano has got nothing on him. Sadly for me, I did not shape the following… it was given as it is by a co-worker and as turns out, it is quite useful. I call it the GitHub-diff command, as it is the diff that GitHub presents you when you open a pull-request: git diff --numstat HEAD $(git log --reverse --boundary --format=%h HEAD ^origin/dev | head -1) | awk '{plus+=$1;minus+=$2} END {print \"+\"plus\" -\"minus}' The goal of this command 7 is to find the diff between your branch and the main development branch. This is what GitHub presents you when your doing a pull-request. Let’s break it down into three parts. First, we need to find the forking point. We do so by requesting all the commits accessible from the tip of our branch but not from the development branch. Then, we ask Git to include the boundary (the forking point), reverse the order to feed it to the head utility and take the first line. $(git log --reverse --boundary --format=%h HEAD ^origin/dev|head -1) Then, we ask git the diff between this forking point and our HEAD in a specific format with –numstat . git diff –numstat HEAD ${forking point} Finally, we feed it to the awk program… and, well, man awk for the rest. What I like about this command is that it lets me precisely control the diff that I will send in each of my pull requests, to ensure they don’t get too large. I’m sure it can be used in many other creative ways that I haven’t thought of. Shade n°6 Sharpen your interactive rebases skills everyday. 7/ Code review commits For all the time we spent discussing these guidelines, there was one aspect of our workflow regarding Git, on which we could not reach consensus. The only commits that are allowed to land on the branch after the branch has been pushed, caused by the Changes requested label on GitHub. . On the one hand, pushing a “Code review” commit will have both the benefit of being quick and preserving the information that was modified because of peer review. It will also ensure that no code pushed to origin and collectively discussed will disappear from the radars. On the other hand, this can quickly stain the Git history with commits that can be both long and hard to read because they can get potentially lengthy, and whose sole context will be that they were caused by a code review. The naive fix would be to make multiple code review commits, but then cause a lot of churn, which, as per guideline #5, is something we would like to avoid. Consequently, it is up to you to decide what you do with these changes. I personally tend to favor the history’s readability and lose track of code reviews discussions. Although occasionally, I consider the first one to be more appropriate. Shade n°7 Figure out code-review commits for yourself ¯\\_(ツ)_/¯ Conclusion Past a certain degree of complexity, lone coders disappear. No single human mind can maintain millions of line of code on its own. Against popular belief, professional software development is a fundamentally collaborative discipline. Thanks to my colleagues, I realized that Git could be more than just a tool to save and share my work… that should be more. It should be used as a tool to make the result of beautiful years of hard collaborative work shine brightly. If you’ve read this far, I hope that this article has helped you realize that too. Footnotes 1 That one made me feel like a code ninja everytime I typed it 2 Half green, half red, shameless attempt to coin a technical term 3 Some strategies put this to the next level by adding semantics to the subject. Git semver and conventional commits are populars implementations of this practice 4 Figuring out the diff between two files is a hard problem. Hard as in NP hard . In fact at its inception , Git delegated this task to the unix diff executable. 5 The default value. man git-diff for more information about the heuristics Git uses. 6 It is the exact opposite of what we did demonstrating how to reduce the churn. 7 There are less dramatic ways to find the forking point, such as git merge-base HEAD dev Additional material The official Git documentation https://git-scm.com/doc About the different branching models: Git flow vs trunk-based development https://www.toptal.com/software/trunk-based-development-git-flow A great article on the advantages of a single big repository over many small ones. https://yosefk.com/blog/dont-ask-if-a-monorepo-is-good-for-you-ask-if-youre-good-enough-for-a-monorepo.html A 4-part article masquerading as a code review of Git itself: https://fabiensanglard.net/git_code_review/diff.php A great talk discussing numerous aspects of Git. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Best Practices , code quality , development . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-09-14"},
{"website": "Octo", "title": "\n                Accelerating NiFi flows delivery: Part 1            ", "author": ["Julien Assemat"], "link": "https://blog.octo.com/en/accelerating-nifi-flows-delivery-part-1/", "abstract": "Accelerating NiFi flows delivery: Part 1 Publication date 23/07/2019 by Julien Assemat Tweet Share 0 +1 LinkedIn 0 While working in different contexts with NiFi, we have faced recurring challenges of development, maintenance and deployment optimization of NiFi flows. Whereas the basic approach suggests to manually duplicate pipelines for similar patterns, we believe that an automated approach is relevant for production purpose when it comes to implementing a significant amount of ingestion flows relying on a limited set of patterns or, more simply, when it comes to deploying these flows on different environments of execution. The ability to reach the right level of automation for implementing NiFi flows implies principles such as versioning, code genericity, configuration centralization, automated generation, etc. Out of them, two concepts, “specialization” and “instantiation” of NiFi flows, have emerged and will be exposed in a two-post serie: Part 1 (this post): understanding the challenges, NiFi concepts and toolbox, implementing a specialization process through NiFi REST API Part 2: implementing the instantiation process and derived features About NiFi Apache NiFi is a real-time data ingestion platform that facilitates automation of data processing from various sources. Widely spread through its packaging into Hortonworks offering (Hortonworks DataFlow, now Cloudera DataFlow), NiFi can be a facilitator in building data pipelines at the edge of a data platform, for data ingestion or data exposure. Whereas handling streaming data flows is one of its primary strengths, do not expect NiFi to be used as an ETL. For instance, ingesting IoT data using MiNiFi (edge) or any message-based data flows are generally ideal use cases for NiFi, but ingesting flat files with a complex integration logic or data through JDBC will not be a good match. In other words, NiFi acts as a gateway for the data platform by abstracting the heterogeneous input or output ecosystem thanks to its rich library of connectors. It should be focused on message-based data flows and should not be used to build transformation pipelines within the data platform. Although NiFi comes with a GUI, abstracting part of the most technical aspects, it might turn into a complexity factor while tuning performance. Basics of NiFi Below are some concepts that will be used in this post and that need to be exposed for a clear understanding. Processor: a “Processor” is the elementary work unit in NiFi. There are different types of processors with various purposes such as creating, sending, receiving, transforming, routing, splitting, merging and processing data. It can be seen as an elementary piece of a pipeline with inputs and outputs through which data flow. NiFi comes with a rich library of prebuilt processors such as “PutHDFS” or “ConsumeKafka”. Process Group: a “Process Group” is a wrapper of a group of processors connected to each other to build a pipeline in which data will flow to reach a specific purpose. For instance, consuming messages from a Kafka topic to write them into HDFS is a process group that would, at least, be composed of two processors (“ConsumeKafka” and “PutHDFS”). Note that process groups can be nested into each other. A “FILE_INGESTION” process group as seen in NiFi UI What it looks like inside a process group in NiFi UI Note that the term “flow” is generally used to designate a data pipeline i.e. a process group. Therefore, “flow” and “process group” designate the same concepts in NiFi. Template: NiFi embeds a native “template” feature which aims at accelerating the development of flows by duplicating existing and saved patterns. This feature is being taken over and enriched with NiFi Registry, especially to facilitate version control. About templates, from HDF 3.1 blog series part 2: Introducing the NiFi registry : NiFi had support for using flow templates to facilitate SDLC [software development life cycle] use cases for a long time, but templates weren’t designed/optimized for that use case in the first place: no easy version control mechanism, not user friendly for sharing between multiple teams, no handling for sensitive properties, etc. But, now with the NiFi registry, you can get version control, collaboration, and easy deployment – significantly shortening the SDLC process, and accelerating flow deployment to achieve faster time to value. Understanding the challenges Need for adaptability During a project lifecycle and similarly to any other piece of code, NiFi flows go through a multiple environments chain (e.g. development, integration, preproduction, production). Each environment having specific settings, the properties of processors may vary based on the execution environment. Therefore, the ability to adapt NiFi flows properties on deployment is mandatory : this is the specialization process. Specialization can be defined in NiFi context as the ability to apply a configuration set to a NiFi flow and its processors to make it specific to a given environment or usage without editing its logic. Need for factorization While implementing data pipelines, similar patterns can be recurring. To optimize development and maintainability and to accelerate new use case implementation, generic NiFi flows can be created once and instantiated as many times as required : this is the instantiation process. Instantiation that can be defined in NiFi context as the ability, from a generic NiFi flow, to generate new ones that may then be specialized to meet a specific use case. How can specialization and instantiation help? Let’s take the example of a basic “Kafka to HDFS” pattern which consumes messages from a Kafka topic and writes into HDFS files: Basic pattern “Kafka to HDFS” Assuming that multiple data pipelines actually rely on this “Kafka to HDFS” pattern, at least two scenarios can be envisioned to handle such a case: Either redevelop from scratch this pattern for each pipeline or copy-paste an existing NiFi flow implementing a similar pattern to adapt it (NiFi template native feature) Or build a NiFi flow once (“master”) and then industrialize its instantiation and specialization Overview of the two implementation scenarios The first scenario consists in a decentralized model where the pattern is rebuilt each time from scratch or from copy-pasting an existing similar NiFi flow. The second one, much more interesting, is rather close to a centralized model with only one NiFi generic flow (master) built, maintained and synced with all the instances generated from the master. At the end, keep in mind that the number of replicated items is the same in both scenarios, only the way to build them is different. In the second scenario, which is the one detailed in the following sections, the automation of the creation of any new NiFi flows based on an existing generic one (instantiation) and its specialization improve developers productivity, increase factorization and reduce delivery and maintainability costs. Implementing specialization Specializing a NiFi flow consists in setting values to variables of processors of a NiFi flow so that it can be contextualized and used for a specific purpose without altering its logic. The first step is to understand how to update properties value of NiFi objects. NiFi comes with a UI for data engineers to design flows. It relies on calls to NiFi REST API, which is also directly available to any authorized users. This NiFi REST API is extremely rich and allows performing any actions usually done through the UI. Globally, the specialization process consists in orchestrating the right API calls to update a NiFi flow and make it operational. Back to our Kafka to HDFS flow example: ConsumeKafka processor reads messages from a given topic, sends them to PutHDFS processor, which writes them down into a given HDFS folder. In NiFi UI, it could look like below: A NiFi flow viewed from NiFi UI Let’s specialize this NiFi flow by setting the name of the input Kafka topic (framed in red below) in the ConsumeKafka processor. Here are the properties for this processor before updating: Processor properties before update Now, running the REST API call: curl -X PUT \\ -H \"Content-Type: application/json\" \\ -d '{\"component\": {\r\n        \"id\":\"2f9fb50e-0169-1000-8610-b6b24f84ffd5\", \"config\": {\r\n             \"properties\": {\r\n                 \"topic\":\"/dev/mata\"\r\n             } } }, \"revision\": {\r\n         \"clientId\": \"2f30251f-0169-1000-443e-571d9105df39\", \"version\":2\r\n     } }' http://localhost:8080/nifi-api/processors/2f9fb50e-0169-1000-8610-b6b24f84ffd5 And here is what it now looks like: Processor properties after update Thanks to NiFi REST API, we have been able to update the value of Kafka input topic name with a value for a pre-identified processor. This new value, along with the processor identifier, have been provided to this API call. This very same procedure can be used to update any property fields as long as the processor identifier and the property name are known. Orchestrating all these updates for all the required variables of processors into a specialization application is a step further to specialization process automation and to making the NiFi flow ready for execution. Note: be careful however, no consistency checks are run against the new value on NiFi end. Updating properties with inconsistent values can generate serious side-effects that may jeopardize NiFi service health. Centralizing processors configuration for specialization Going even further, the next step is about centralizing required information for specialization into a configuration repository that will act as a reference for the specialization application. Here is what the structure of such a repository could look like: Information Description Value Example Flow name Functional name used to identify a NiFi flow. INGEST_KAFKA_TO_HDFS Flow identifier Process group (flow) identifier given by NiFi. A flow in NiFi can be represented as a “Process Group” object, itself composed by one or several processors or sub process groups. This value is generally set by the instantiation stage that will be explained later. 6be93ac7-1753-9755-a6ce-955a3ff30abe Processors configuration Values to be applied to processors of NiFi flow. <JSON object> The diagram below depicts interactions between this configuration repository and the specialization application: Processors configuration information can be modelled as a JSON object such as: [{ \"KAFKA_CONSUMER\": { \"properties\": { \"topic\": \"/dev/mata\", \"group ID\": \"DEV\" } } }, { \"PUT_HDFS\": { \"properties\": { \"Directory\": \"/dev/mata\" } } }] In the JSON object, “KAFKA_CONSUMER” and “PUT_HDFS” are both processor names used to identify the processors to be updated. Note that processor names are not unique across NiFi or even within the same process group. Whereas processor names are defined by data engineers and so consistent across all environments where the NiFi flow is deployed, processor identifiers are generated by NiFi and may be different while passing from one environment to another. Since NiFi REST API uses identifiers to uniquely designate NiFi objects, it is recommended to use processor names in the configuration repository and implement a mapping process in the specialization application that will translate the processor name into its identifier in the execution environment. Translating a processor name into its identifier Now we know how to update a processor property and that a configuration repository stores all processor properties to be specialized, we need to industrialize the property update process so that it can be applied to a whole set of identified processors within a process group. But before going further with this, it is important to understand how to handle processors identification since object identifiers in NiFi cannot be known in advance. Object identifiers generation is actually rather simple: each object of any type in NiFi has got an identifier generated at creation. It is unique across the NiFi instance and this identifier allows to interact with NiFi objects through the REST API. Here is an example value of an identifier: 2f9fb50e-0169-1000-8610-b6b24f84ffd5 . Processor identifier viewed from NiFi UI The process group (the NiFi flow) itself and each of its processors has got a unique identifier. So, how to retrieve easily the processor identifier we are about to update properties of? A possible solution is about getting a processor identifier indirectly by using its name: Request processors configuration for a given NiFi flow known in the configuration repository Get processors configuration for this NiFi flow Use the REST API to request the list of processors within the NiFi flow (using the process group identifier from the configuration repository) Get the list of processors (name and identifier for each) within the NiFi flow Filter by processor name and get the corresponding process identifier Use REST API to specialize the processors using their respective identifier The diagram below includes these interactions: As a quick summary, such an approach would imply that: For a given process group, values of processor properties are stored and associated to a processor name, which is known in advance prior to the deployment (chosen by data engineers) contrary to the identifier (generated by NiFi) Each processor that is to be updated as part of the specialization process is named after a unique explicit name rather than the default name Conclusion Reaching the end of this post, we have been able to implement the specialization process of NiFi flows by using NiFi REST API and automate / massify it through a configuration repository. With the ability to specialize NiFi flows for a given usage, the next step is now to investigate instantiation process aiming at automating NiFi flows creation from a master / generic one. It will be introduced along with NiFi Registry within the part 2. Orchestrating both specialization and instantiation will result in a key applicative component to accelerate and optimize the delivery of NiFi flows. Sources https://pierrevillard.com/tag/nifi-registry/ https://fr.hortonworks.com/blog/hdf-3-1-blog-series-part-2-introducing-nifi-registry/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data and tagged bigdata , Cloudera , delivery , development , Hadoop , Hortonworks , NiFi . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Accelerating NiFi flows delivery: Part 1” Srikanth 05/12/2019 à 12:40 Hi,\r\n\r\nWe could not find part2 to this article?\r\nCan you please post the same Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-07-23"},
{"website": "Octo", "title": "\n                Android Material Components: Exploring MaterialShapeDrawable            ", "author": ["Pierre Degand"], "link": "https://blog.octo.com/en/android-materialshapedrawable/", "abstract": "Android Material Components: Exploring MaterialShapeDrawable Publication date 18/07/2019 by Pierre Degand Tweet Share 0 +1 LinkedIn 0 Material Components is the (not so) new library made by the Material team to replace the old support design library. It provides components to apply Material Design in your application with ease. Among these components, you can find the famous FloatingActionButton , the CardView or the BottomSheet . But there are also some less known, nonetheless powerful, components. And one of them is the MaterialShapeDrawable . Note: This article was written using the version 1.1.0-alpha07 > of the Material Components. MaterialShapeDrawable existed in the 1.0.0 stable but the API changed quite a lot in the version 1.1.0-alpha01. What is MaterialShapeDrawable ? According to the documentation: Base drawable class for Material Shapes that handles shadows, elevation, scale and color for a generated path. And by Shape , they are referring to the Material Guidelines shapes : Rectangular shapes with curved or angled edges and/or corners… In the build.gradle of your app, add the new dependency (or update it if you already use Material Components) implementation 'com.google.android.material:material:1.1.0-alpha07' We will start by creating a new MaterialShapeDrawable and setting it as the background of a TextView . <TextView\r\n  android:id=\"@+id/helloTV\"\r\n  android:layout_width=\"wrap_content\"\r\n  android:layout_height=\"wrap_content\"\r\n  android:layout_gravity=\"center\"\r\n  android:padding=\"8dp\"\r\n  android:textColor=\"@android:color/white\"\r\n  android:text=\"Hello world!\" /> val shapeDrawable = MaterialShapeDrawable()\r\nhelloTextView.background = shapeDrawable The drawable supports a couple of customizations like: Fill color shapeDrawable.fillColor = ColorStateList.valueOf(colorAccent) Strokes shapeDrawable.strokeWidth = resources.getDimension(R.dimen.stroke_width)\r\nshapeDrawable.strokeColor = ColorStateList.valueOf(colorPrimary) If your view has elevation, a shadow will be cast on API above 21 like any other drawable. But this drawable supports a compatibility version of shadows for devices running API lower than 21. In this case, you need to specify the elevation on the drawable itself: shapeDrawable.elevation = resources.getDimension(R.dimen.elevation) For now, this provides nothing more that a simple ShapeDrawable couldn’t do, apart from the compatibility shadows. So what’s the point? Introducing ShapeAppearanceModel A ShapeAppearanceModel is a model of edges and corners used by a MaterialShapeDrawable to render itself. Every MaterialShapeDrawable has a ShapeAppearanceModel . The default ShapeAppearanceModel has 4 rounded corners with a 0px radius and 4 flat edges. And of course, you can customize it in order to provide your own model: val shapeAppearanceModel = ShapeAppearanceModel()\r\nshapeDrawable.shapeAppearanceModel = shapeAppearanceModel\r\n// or\r\nval shapeDrawable = MaterialShapeDrawable(shapeAppearanceModel) Corners ShapeAppearanceModel has two out-of-the-box types of corners: rounded and cut. 8dp rounded corners 8dp cut corners You can change all 4 corners at once: val cornerSize = resources.getDimensionPixelSize(R.dimen.corner_size)\r\nshapeAppearanceModel.setAllCorners(\r\n CornerFamily.CUT, //or CornerFamily.ROUNDED\r\n cornerSize\r\n) Or you can change each corner individually: shapeAppearanceModel.setTopLeftCorner(CornerFamily.CUT, cornerSize)\r\nshapeAppearanceModel.setBottomRightCorner(CornerFamily.ROUNDED, cornerSize) different corners with strokes You can also create your own corners. ShapeAppearanceModel uses a CornerTreatment object for each corner. The previously shown ways of customizing the corners were just shortcuts to build instances of CutCornerTreatment and RoundedCornerTreatment . You can also write your own implementation of CornerTreatment if cut or rounded corners are not enough for you. For example, if you want an inside cut corner, you could use this implementation of CornerTreatment : class InnerCutCornerTreatment(cornerSize: Float): CornerTreatment(cornerSize) {\r\n  override fun getCornerPath(angle: Float, interpolation: Float, shapePath: ShapePath) {\r\n    val radius = cornerSize * interpolation\r\n    shapePath.reset(0f, radius, 180f, 180 - angle)\r\n    shapePath.lineTo(radius, radius)\r\n    shapePath.lineTo(radius, 0f)\r\n  }\r\n} The interpolation parameter is used for animations. Its value ranges from 0 to 1 and it’s used to compute the current radius of the corner. More example are shown later in this article. And apply it to your ShapeAppearanceModel : shapeAppearanceModel.setAllCorners(InnerCutCornerTreatment(cornerSize)) Edges Edges are handled the same way as corners, by giving to the model instances of EdgeTreatment objects. There is only one built-in edge treatment: the TriangleEdgeTreatment . It draws a triangle facing in or out of the shape in the middle of an edge. shapeAppearanceModel.topEdge = TriangleEdgeTreatment(edgeSize, false)\r\nshapeAppearanceModel.bottomEdge = TriangleEdgeTreatment(edgeSize, true) This can be useful to add nice looking tooltips into your app. Like for corners, you can build custom edge treatments. You can find a very good example in the code of the BottomAppBar of the material components. The BottomAppBar is using a MaterialShapeDrawable and the cradle for the FAB is a custom EdgeTreatment. Animations MaterialShapeDrawable supports animation of its treatments, whether it’s edge or corner, via the interpolation parameter. This attribute is ranging from 0 to 1. At 0, the treatments are not rendered at all (this results in square corners and flat edges). At 1, they are fully rendered. When building custom treatments, be aware of the interpolation parameter of getCornerPath method as this is the current interpolation of the drawable and you should take it into account when building the ShapePath of your treatment. seekBar.progress = seekBar.max\r\nseekBar.setOnSeekBarChangeListener(object : SeekBar.OnSeekBarChangeListener {\r\n  override fun onProgressChanged(seekBar: SeekBar, progress: Int, fromUser: Boolean) {\r\n    shapeDrawable.interpolation = progress.toFloat() / seekBar.max.toFloat()\r\n  }\r\n  override fun onStartTrackingTouch(seekBar: SeekBar) {}\r\n  override fun onStopTrackingTouch(seekBar: SeekBar) {}\r\n})\r\n\r\nanimateButton.setOnClickListener {\r\n  val seekbarRatio = seekBar.progress.toFloat() / seekBar.max.toFloat()\r\n  ObjectAnimator.ofFloat(shapeDrawable, \"interpolation\", seekbarRatio, 1f).apply {\r\n    duration = 1000\r\n    addUpdateListener {\r\n      seekBar.progress = ((it.animatedValue as Float) * seekBar.max).toInt()\r\n    }\r\n  }.start()\r\n} In this example, the interpolation property is either animated with an ObjectAnimator , or modified by the tracking of a SeekBar . XML usage with Material Theming The shapeAppearance attribute If you dug a bit in the source code of MaterialShapeDrawable , you probably noticed that this component supports XML inflation, but in a special way. You can leverage it by using Material components widgets that are using MaterialShapeDrawable internally: BottomSheet Chip MaterialButton (and subclasses like ExtandedFloatingActionButton ) FloatingActionButton MaterialCardView TextInputLayout On all theses widgets, you can use the app:shapeAppearance attribute and pass it a reference to a style containing the attributes of your custom appearance. <com.google.android.material.button.MaterialButton\r\n   android:layout_width=\"wrap_content\"\r\n   android:layout_height=\"wrap_content\"\r\n   android:text=\"MaterialButton\"\r\n   app:shapeAppearance=\"@style/CustomShapeAppearance\" />\r\n\r\n<style name=\"CustomShapeAppearance\">\r\n  <!-- Attributes for the custom appearance-->\r\n</style> Here is a list of all supported attributes to customize the appearance : cornerFamily (values: rounded or cut) cornerFamilyTopLeft cornerFamilyTopRight cornerFamilyBottomRight cornerFamilyBottomLeft cornerSize (dimension) cornerSizeTopLeft cornerSizeTopRight cornerSizeBottomRight cornerSizeBottomLeft If you want to implements the shape of the Shrine Material Studies , you can simply write: <style name=\"CustomShapeAppearance\">\r\n  <item name=\"cornerFamily\">cut</item>\r\n  <item name=\"cornerSize\">4dp</item>\r\n</style> You can change the shape of any supported widgets without writing code. But it can be tedious to remember to always specify the shapeAppearance if you want all your widgets to have the same shape. Fortunately, with the new Material Themes, you can control the default shape of all the Material widgets. There are 3 new theme attributes to control the default shapes: shapeAppearanceSmallComponent used by: Chip MaterialButton (and ExtendedFloatingActionButton ) FloatingActionButton TextInputLayout the items of NavigationView shapeAppearanceMediumComponent used by: MaterialCardView shapeAppearanceLargeComponent used by: BottomSheet By settings those attributes in your theme to a custom ShapeAppearance Style, every widgets impacted will be automatically properly shaped. How does it work internally? The default style of these widgets all include a shapeAppearance attribute referencing one of the 3 theme attributes. You can see an example for MaterialButton in the source code . If you want to learn more about how all the theming, styling and default attributes work, you can read Android Styles & Themes for developers. Here is how you would customise all the shapes of you app using Material Theming: <resources>\r\n\r\n  <!-- Application theme. -->\r\n  <style name=\"AppTheme\" parent=\"Theme.MaterialComponents.Light.NoActionBar\">\r\n    <!-- Other Theme Attributes. -->\r\n\r\n    <item name=\"shapeAppearanceSmallComponent\">@style/CustomSmallShapeAppearance</item>\r\n    <item name=\"shapeAppearanceMediumComponent\">@style/CustomMediumShapeAppearance</item>\r\n    <item name=\"shapeAppearanceLargeComponent\">@style/CustomLargeShapeAppearance</item>\r\n  </style>\r\n\r\n  <style name=\"CustomSmallShapeAppearance\">\r\n    <item name=\"cornerFamily\">cut</item>\r\n    <item name=\"cornerSize\">4dp</item>\r\n  </style>\r\n  <style name=\"CustomMediumShapeAppearance\">\r\n    <item name=\"cornerFamily\">rounded</item>\r\n    <item name=\"cornerSize\">16dp</item>\r\n  </style>\r\n</resources> <com.google.android.material.card.MaterialCardView\r\n    android:layout_width=\"match_parent\"\r\n    android:layout_height=\"wrap_content\"\r\n    android:layout_marginTop=\"24dp\"\r\n    android:clipChildren=\"false\"\r\n    app:cardBackgroundColor=\"#FFFFFF\"\r\n    app:cardElevation=\"8dp\"\r\n    app:contentPadding=\"16dp\">\r\n\r\n    <LinearLayout\r\n      android:layout_width=\"match_parent\"\r\n      android:layout_height=\"wrap_content\"\r\n      android:layout_gravity=\"center_horizontal\"\r\n      android:orientation=\"vertical\">\r\n\r\n      <TextView\r\n        android:layout_width=\"wrap_content\"\r\n        android:layout_height=\"wrap_content\"\r\n        android:layout_gravity=\"center_horizontal\"\r\n        android:text=\"MaterialCardView\" />\r\n\r\n      <com.google.android.material.button.MaterialButton\r\n        android:layout_width=\"wrap_content\"\r\n        android:layout_height=\"wrap_content\"\r\n        android:layout_gravity=\"center\"\r\n        android:layout_marginTop=\"16dp\"\r\n        android:layout_marginBottom=\"16dp\"\r\n        android:text=\"MaterialButton\" />\r\n\r\n      <com.google.android.material.textfield.TextInputLayout\r\n        style=\"@style/Widget.MaterialComponents.TextInputLayout.OutlinedBox\"\r\n        android:layout_width=\"match_parent\"\r\n        android:layout_height=\"wrap_content\"\r\n        android:hint=\"TextInputLayout\">\r\n\r\n        <com.google.android.material.textfield.TextInputEditText\r\n          android:layout_width=\"match_parent\"\r\n          android:layout_height=\"wrap_content\"\r\n          android:text=\"TextInputEditText\" />\r\n      </com.google.android.material.textfield.TextInputLayout>\r\n\r\n      <com.google.android.material.chip.ChipGroup\r\n        android:layout_width=\"match_parent\"\r\n        android:layout_height=\"wrap_content\"\r\n        android:layout_marginTop=\"16dp\">\r\n\r\n        <com.google.android.material.chip.Chip\r\n          style=\"@style/Widget.MaterialComponents.Chip.Action\"\r\n          android:layout_width=\"wrap_content\"\r\n          android:layout_height=\"wrap_content\"\r\n          android:text=\"Chip1\" />\r\n\r\n        <com.google.android.material.chip.Chip\r\n          android:layout_width=\"wrap_content\"\r\n          android:layout_height=\"wrap_content\"\r\n          android:text=\"Chip2\" />\r\n      </com.google.android.material.chip.ChipGroup>\r\n    </LinearLayout>\r\n  </com.google.android.material.card.MaterialCardView>\r\n\r\n  <com.google.android.material.floatingactionbutton.FloatingActionButton\r\n    style=\"@style/Widget.MaterialComponents.FloatingActionButton\"\r\n    android:layout_width=\"wrap_content\"\r\n    android:layout_height=\"wrap_content\"\r\n    android:layout_gravity=\"end\"\r\n    android:layout_marginTop=\"16dp\"\r\n    app:srcCompat=\"@drawable/ic_android_black_24dp\" />\r\n</LinearLayout> We can observe that some widgets like the Chip or the FloatingActionButton override the cornerSize of the theme. For the FAB, the cornerSize will be computed to always be 50% of the size of the fab. Wrap-up MaterialShapeDrawable is a very powerful tool used a lot across the Material Components library. You have learned how to use it directly in code to build very specific shapes and you also learned how to use it in your layout files and to customise your app theme to globally change all the shapes of your application. Material Components are open-source and you can find the code on Github . If you want to go further with Material Theming (colors, typography etc…), I highly recommend Styles, Themes, Material Theming, Oh My! by Anita Singh. I hope you will have fun experiencing with it in your own apps! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged android , androiddev , mobility . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-07-18"},
{"website": "Octo", "title": "\n                Cache me if you can – 2            ", "author": ["Léo Jacquemin"], "link": "https://blog.octo.com/en/cache-me-if-you-can-2/", "abstract": "Cache me if you can – 2 Publication date 19/07/2019 by Léo Jacquemin Tweet Share 0 +1 LinkedIn 0 Introduction – scope of the article This series of articles deals with caching in the context of HTTP. When properly done, caching can increase the performance of your application by an order of magnitude. On the contrary, when overlooked or completely ignored, it can lead to some very unwanted side effects caused by misbehaving proxy servers that, in the absence of clear caching instructions, decide to cache anyway and serve stale resources. In the first part of this series, we argued that caching is the most effective way to increase performance, when measured by the page load time. In this second part, it is time to shift our focus to the mechanisms at our disposal. To put it in another way: how does HTTP caching actually work ? To answer this question, we decided to consider the case of an empty cache that starts progressively caching and serving resources. As it gradually receives incoming HTTP requests, our cache will start behaving accordingly. Serving the resource from the cache when a fresh copy is available, varying over multiple representations, making a conditional request… This way, we can introduce each concept progressively as we need it. At first, our empty cache will have no choice but to forward requests to the origin server. This will allow us to understand how origin servers instruct our cache on what to do with the resource, such as if it is allowed to store it, and for how long. For this, we will examine each Cache-Control directive and clarify some of them that have been known to have conflictual meanings . Second, we will look at what happens when our cache receives a request for a resource it already knows. How does our cache decide if it can re-use a previously stored response ? How does it map a given HTTP request to a particular resource ? To answer these, we will learn about representation variations with the Vary header. This article is going to focus on knowledge that’s the most valuable from a web developer’s perspective. Therefore, conditional requests are only discussed briefly and will be the focus of another article. Without further ado, let us start with an overview of what we will be exploring. The HTTP caching decision tree Conceptually, a cache system always involve at least three participants. With HTTP, these participants are the client, the server, and the caching proxy. However, when learning about HTTP caching, we strongly encourage you not to think of the client as your typical web browser because these days, they all ship with their own HTTP caching layer. It makes it difficult to clearly separate the browser from the cache. For this reason, we invite you to think of the client as a headless command line program such as cURL or any application without an embedded HTTP cache. All precautions aside, let us now deep dive into the subject by taking a look at the following picture: the HTTP caching decision tree. This picture illustrates all the possible paths a request can take every time a client asks for a resource to an origin server behind a caching system. A careful examination of this illustration reveals that there are only four possible outcomes. Clearly separating these outcomes in our minds is actually very convenient, seeing as each important caching concept (cache instructions, representation matching, conditional requests and resource aging) maps to each one of them. Let us describe succinctly each one by introducing two important terms relating to the HTTP caching terminology: cache hits and cache misses. Hits and misses The first possible outcome is when the cache finds a matching resource, and is allowed to serve it, which, in the caching world, are indeed two distinct things. This outcome is what we commonly call a cache hit, and is the reason why we use caches in the first place. When a cache hit happens, it completely offloads the origin server and the latency is dramatically reduced. In fact, when the cache hit happens in the browser’s HTTP cache latency is null and the requested resource is instantly available. Unfortunately, cache hits account only one of the four possible outcomes. The rest of them fall into the second category, also known as cache misses, which can happen for only three reasons. The first reason a cache miss typically happens is simply when the cache does not find any matching resource in its storage. This is usually a sign that the resource has never been requested before, or has been evicted from the cache to free up some space. In such cases, the proxy has no choice but to forward the request to the origin server, fully download the response and look for caching instructions in the response headers. The second reason a cache miss can happen is actually just as detrimental, where the cache detects a matching representation, one that it could potentially use. However, the resource is not considered to be fresh anymore – we will see how exactly in the Cache-Control section of this article – but is said to be stale. In such case, the cache sends a special kind of request, called a conditional request to the origin server. Conditional requests allow caches to retrieve resources only if they are different from the one they have in their local storage. Since only the origin server ever has the most recent representation of a given resource, conditional requests always have to go through the whole caching proxy chain up to the origin server. These special requests have only two possible outcomes. If the resource has not changed, the cache is instructed to use its local copy by receiving a 304 Not Modified response along with updated headers and an empty body. This outcome, the third one on our list, is called a successful validation. Finally, the last possible outcome is when the resource has changed. In this case, the origin server sends a normal 200 OK response, as it would if the cache was empty and had forwarded the request. To put it another way, cache misses caused by empty cache and failed validation yield exactly the same HTTP response. To best visualize these four paths, it is helpful to picture them in a timeline, as illustrated below. At first, the cache is empty. The flow of requests starts with a cache miss (empty cache outcome). On its way back, the cache would read caching instructions and store the response. All subsequent requests for this particular resource would yield to cache hits, until the resource becomes stale and needs to be revalidated. Upon a first revalidation, it is possible that the resource has not changed, hence, a 304 Not Modified would be sent. Then, the resource eventually gets updated by a client, typically with a PUT or a PATCH request. When the next conditional request arrives, the origin server detects that the resource has changed and replies a 200 OK with updated ETag and Last-Modified headers. Knowing about cache hits and cache misses along with the 4 possible paths that every cacheable request could take, should give you a good overview of how caching works. Though overviews can only get you so far. In the following section, we will give a detailed explanation of how origin servers communicate caching instructions. How origin servers communicate caching instructions Caching HTTP headers: Cache-Control, Expires, Pragma Origin servers communicate their caching instructions to downstream caching proxies by adding a Cache-Control l header to their response. This header is an HTTP/1.1 addition and replaces the deprecated Pragma header, that was never a standard one. Cache-Control header values are called directives . The specification defines a lot of them, with various uses and browser-support . These directives are primarily used by developers to communicate caching instructions. However, when present in an HTTP request , clients can also influence the caching decision. Let us now take the time to describe the most useful directives. max-age The first important Cache-Control directive to know about is the max-age directive, which allows a server to specify the lifetime of a representation. It is expressed in seconds. For instance, if a cache sees a response containing the header Cache-Control: max-age=3600 , it is allowed to store and serve the same response for all subsequent requests for this resource for the next 3600 seconds. During these 3600 seconds, the resource will be considered fresh and cache hits will occur. Past this delay, the resource will become stale and validation will take over. no-store, no-cache, must-revalidate Unlike max-age , the no-store , no-cache and must-revalidate directives are about instructing caches to not cache a resource. However, they differ in subtle ways. no-store is pretty self-explanatory, and in fact, it does even a little more than the name suggests. When present, a HTTP/1.1 compliant cache must not attempt to store anything, and must also take actions to delete any copy it might have, either in memory, or stored on disk. The no-cache directive, on the other hand, is arguably much less self-explanatory. This directive actually means to never use a local copy without first validating with the origin server. By doing so, it prevents all possibility of a cache hit, even with fresh resources. To put it another way, the no-cache directive says that caches must revalidate their representations with the origin server . But then comes another directive, awkwardly named… must-revalidate . If this starts to get confusing for you, rest assured, you are not alone. If what one wants is not to cache, it has to use no-store instead of no-cache . And if what one wants is to always revalidate, it has to use no-cache instead of must-revalidate . Confusing, indeed. As for the must-revalidate directive, it is used to forbid a cache to serve a stale resource. If a resource is fresh, must-revalidate perfectly allows a cache to serve it without forcing any revalidation, unlike with no-store and no-cache . That’s why this header should always be used with a max-age directive, to indicate a desire to cache a resource for some time and when it’s become stale, enforce a revalidation. When it comes to these last three directives, we find the choice of words to describe each of them particularly confusing: no-store and no-cache are expressed negatively whereas must-revalidate is expressed positively. Their differences would probably be more obvious if they were to be expressed in the same fashion. Therefore, it is helpful to think about each of them expressed in terms of what is not allowed: no-store: never store anything no-cache: never cache hit must-revalidate: never serve stale Technically, these directives can appear in the same Cache-Control header. It is not uncommon to see them combined as a comma-separated list of values. A lot of popular websites still seem to behave very conservatively, sending back HTML pages with the following header: Cache-Control: no-cache, no-store, must-revalidate, max-age=0 When you stumble upon this, the intention behind it is usually pretty clear: the web development team wants to ensure that the resource never gets served stale to anyone. However, such cache-buster lines are probably not necessary anymore. Past work done in 2017 already showed that browsers are really rather compliant with the specification in respect to Cache-Control response directives. Therefore, unless you’re planning on setting up a caching stack with decades old software, you should be fine using just the directives you need. The most popular combinations will be analyzed in another article. public, private The last important directives we haven’t discussed yet are a little bit different, as they control which types of caches are allowed to cache the resources. These are the public and private directives, private being the default one if unspecified. Private caches are the ones that are supposed to be used by a single user. Typically, this is the web browser’s cache. CDN and reverse-proxies on the contrary, handle requests coming from multiple users. Why do we need to distinguish these two types of caches ? The answer is straightforward: security, as illustrated by the following example. Many web applications expose convenience endpoints that rely on information coming from elsewhere than the URL. If two users access their profile by requesting /users/me , at https://api.example/com , and their actual user id is hidden within a Authorization: Bearer 4Ja23ç42… .   token, the cache won’t be able to tell these are in fact two very different resources. Indeed, when constructing their cache key, caches do not inspect HTTP headers unless specifically instructed to do so, as we shall see in the next section. s-maxage The s-maxage directive is like the max-age directive, except that it only applies to public caches, which are also referred to as shared caches (hence the s- prefix). If both directives are present, s-maxage will take precedence over max-age on public caches and be ignored on private ones. When using this directive, the general rule is to always ensure that s-maxage value is below max-age’s . The reasoning behind this rule is that the closer you are to the origin, the more suitable it is to check frequently what the latest representation is. Imagine you were to cache for one day in the proxy, and one hour in browsers. Every time a browser would ask a resource to upstream servers, we could know in advance that the proxy will not contact the origin server for at least a day. Therefore, why not put the same TTL directly in the browsers ? As a conclusion, it is a best practice to always leave out a longer TTL in max-age than in s-maxage . stale-while-revalidate and stale-if-error These two directives are not technically part of the original specification but are part of an extension which were first described more than 10 years ago. Although their browser support is limited, some popular CDNs have been supported them for more than 5 years! Though stale-while-revalidate is pretty useful. As the name implies, it allows a cache to “[…] immediately return a stale response while it revalidates it in the background, thereby hiding latency (both in the network and on the server) from clients”. This caching extension proves really helpful for things like images, where reducing latency is critical for the user experience, and where having a stale version for a few seconds is often better than a painfully downloading image. As for stale-if-erro r, it allows a cache to serve a stale version if the origin server returns a 5xx status code. This gives developers a chance to fix potential issues during a grace period where clients are shielded from irritating error pages. Consider the case of a meteo third-party script. If the meteo server happens to be unreachable for a few minutes, it’s probably best to display a slightly outdated forecast during this lapse of time, than it is to see a portion of the page be blank (or a whole blank page if the code does not handle third-party scripts loading failures) What we don’t know yet After examining these Cache-Control directives, we now understand how applications that are distributed on the web, tend to leverage HTTP caching mechanisms in multiple ways, depending on what they need. Though what we don’t yet understand is what  cache softwares actually do with the response they receive. They will most likely have to store it somewhere in order to retrieve it later. That’s the core idea of any caching system afterall. Under normal circumstances, this certainly looks like what we would call an implementation detail. It should be merely enough to know that resources are indeed stored some way. Yet in this case, learning just a little more is actually critical. Neglecting the mechanisms that govern how caching softwares map objects from the HTTP responses space to their storage space can have really unexpected consequences, such as serving a brotli encoded chinese document, to a user who does not understand chinese, using a browser unable to decode brotli ¯\\_(ツ)_/¯ How caches store and retrieve resources Caching HTTP headers: Vary Albeit unlikely to happen, since most browsers can decode brotli – and since most people know how to 說中文 –  the previous situation can still easily occur. To understand why this is the case, one must consider how caches store their representations. By virtue of what they try to achieve, most caching softwares ought to be able to quickly retrieve simple text documents. To do so, a very simple yet powerful strategy is to use a key-value store. This strategy fits well in-memory representations. Therefore, the question one must answer when designing is the following: how to construct a cache key from an HTTP response ? What we are looking for here is a way to uniquely identify a resource. Conveniently, this is exactly why URI s – Uniform Resource Identifiers – were invented in the first place! But URIs don’t tell the whole truth about resources. They never describe them entirely, if only for the fact that resources change over time. Websites get rebranded, new content gets published and users update their profile. Granted,  not for the same reasons or at the same frequency, though all resources will eventually change. In fact, the entire Conditional request specification is based on this sole observation: nothing is permanent except change . Philosophical quotes aside, there is, however, another time-independent reason why resources changes. Indeed, any moment, resources may be available in multiple representations. This is why we have Content-Negociation. The HTTP request headers Accept, Accept-Language , Accept-Encoding , Accept-Charset (and a few other headers who are not strictly speaking part of content negotiation) add another dimension on which representations can differ. As such, the problem of finding a good cache key becomes more complicated. Since all these representations share the same URI, caches must have a way to distinguish them in order to serve the right representation at each client, honoring content negotiation. And since only origin servers know what different representations are available, it is again the origin server’s responsibility to indicate to a cache based on which headers it will generate a different representation. To do so, the origin servers must add a Vary header containing the value of the request headers that cause different representations to be generated. When caches see a response coming from an origin server with, for instance, the header Vary: Accept-Language , it will examine the value of the Accept-Language header, such as fr-FR , and use this value to construct a more specific cache-key, perhaps like https://example.net/home.html_ fr-FR . The actual implementation strategy is of little importance to us. Altering the cache key might not even be the best way to do it. It somehow has to use the value of the header to differentiate representations. The Vary header can actually point at more than one header, when resources are available in multiple representations. Selecting a cache key when multiple headers are involved is not really much more complicated than with only one header. The real problem when varying over multiple dimensions is the combinatorial explosion . Unfortunately, there are no ways around this. If you are to cache and serve your resources in multiple representations, you have to pay the cost of a large storage. If you decide to lower your vary cardinality, some of your users will receive cache hits for responses that won’t match their requests. On the other hand, if you vary properly on everything, and as you should have, yet do not have enough storage space, chances are your users won’t be seeing cache hits anytime soon. Now, it is important to know that this is only a problem if you decide to use a public cache, for which two different requests coming from two different users are running the same code, at the proxy level. If you decide to leverage the browser’s cache only, then you can skip the Vary header altogether and serve resources in as many representations as you want. This is because each browser’s cache will only cache representations matching the user’s preferences. This is good news! But let’s not get ahead of ourselves just yet. As we said, caches use the value of the header as its input to generate a more specific cache key. But what is to say that all these values are well formatted ? Absolutely nothing! This is the rather inconvenient consequence of RFC father ’s robustness principle . HTTP servers are indeed very liberal in what they accept . However there is hope. Considering the case of an origin server that can only produce a representation in two different languages, caches must be able to regroup incoming Accept-Content values such as fr, fr-FR, fr_FR .. into something such as FR . Otherwise, just like before with the combinatorial explosion, the number of representations will explode, but in this case, for a misguided reason. The process by which all these representations are regrouped is called normalization and is often done at the cache. Many caches offer configuration utilities or their own languages to deal with these situations. Sometimes, the functions are even already written, or snippets can easily be found on the Internet. The following pictures illustrates the process for the infamous User-Agent header. Fastly , a popular CDN, sampled 100 000 requests and found that the Accept-Encoding header was expressed in 44 different ways ! As for the User-Agent header, they found a shy of… 8000 different ones! Without normalization, chances are that the cache will never see any hit. This wraps up the section about representation variation. At this point, we know how to instruct caches to store our resources, and have learned to leverage the Vary header to prevent accidents from happening when using public caches. We have now covered enough of the specification to be able to cache resources effectively. Common misconceptions By now, you should have a thorough understanding of how HTTP caching works. Freshness control, resource’s representations and cache hits are no longer mysterious concepts to you. And if you start to feel empowered by all this knowledge, we have some good news for you: we’ve covered a large portion of the specification, and you now know pretty much all that’s necessary to be up and running. But make no mistake. Caching is a complex topic. Experience has shown us that, unless you’re dealing with it on a day-to-day basis, what may be crystal clear today will quickly turn into something rather blurry after a few weeks.  Therefore, we decided to conclude this second article by dispelling two common misconceptions that are all too easy to make. Freshness-control and validation This might seem obvious after reading the previous sections but it is worth repeating many times. Freshness control and validation ( which we have slightly discussed in the beginning ) are two very distinct mechanisms that serve two very different purposes, and involve HTTP requests between different pieces. Freshness control always happen in a cache and is solely based on time Validations always happen in the origin server and are based both on time and on identifiers (ETags) This is something we find important to remind ourselves. It means that once the cache has received temporal instructions, it can – and best believe it will – serve resources without ever contacting the origin server until the timer expires. For instance, if you’re web application’s HTML file reaches a browser and the HTTP response happens to include the header Cache-Control: max-age=86400 the browser will happily serve the same version of your app for a day. In this case, the browser would serve it for one day without any possible action from you or anyone, except the user, if one ever decided to flush his browser’s cache. If you’re thinking everyone can make mistakes, and one day is not so bad, well, brace yourself: the maximum max-age value is… 31536000 seconds! That is to say, one year. This is the reason why HTML files are very dangerous to cache like this, and should generally be declared with Cache-Control: no-cache Freshness and most recent representation Another misconception is to believe that cache hits and freshness have anything to do with having the last available version of a resource. This is what we all try to achieve, but one can never truly know if the resource it has been served from a cache is indeed the most up-to-date version. In fact, this holds true even in the absence of cache. It has to do with the nature of distributed applications: other people’s actions can change the things we are interacting with at any time. When querying the state of the application, the ETag header must always be used to always let the server know what our current understanding of the application’s state is. And if it does not match the server’s, 409 Conflict are expected to be received on the client side. Conclusion Along this article, we have described how caching actually works. Now would be a good time to spin up a local dev server and fiddle around with these two core headers : Cache-Control and Vary to see them in action . We started by giving an overview of how caching works, illustrating the four possible paths that a request can take : the happy path (cache hit) and the 3 possible ways to have a cache miss : empty cache, failed revalidation and successful revalidation. This overview alone gives the possibility to understand how complex caching topologies can fit together. Then, we went deeper and looked at all the most useful Cache-Control headers, and clarified some subtle differences that are all easily missed. We also looked at the Vary header and the fundamental difference between resources and representations, to avoid serving the wrong representation to the right client. Finally, we took some time to review it all through the angle of common misconceptions you might encounter, and hopefully helped you to avoid them. In the next article, we’ll apply all of this knowledge to set up a local lab environment in which we will set an innocent node.js app on fire with a load-testing tool, right before rescuing it with the help of a popular caching software. Stay tuned! To go further: The official specification about the material we covered (and other things) https://tools.ietf.org/html/rfc7234#section-5.3 Google Web’s Fundamental https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching#defining-optimal-cache-control-policy About the Cache-Control header: https://developer.mozilla.org/fr/docs/Web/HTTP/Headers/Cache-Control About the Vary Header: https://www.smashingmagazine.com/2017/11/understanding-vary-header/ https://www.fastly.com/blog/best-practices-using-vary-header https://www.fastly.com/blog/getting-most-out-vary-fastly https://www.fastly.com/blog/understanding-vary-header-browser Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 4 commentaires sur “Cache me if you can – 2” Stefan 04/08/2019 à 13:24 That was a very well written read! Thank you! David Okwii 29/08/2019 à 16:23 Hi, thanks for the comprehensive article. Learning alot. Please consider adding back links to previous and future articles in the series so it's easier to follow. Also would love the next article to show how to configure catching in popular web servers Nginx and Apache. Léo Jacquemin 13/09/2019 à 16:26 Thanks David for your feedback. I added the missing link. Stay tuned, part three is coming in the next few months! Léo Jacquemin 13/09/2019 à 18:48 Thank you Stefan Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-07-19"},
{"website": "Octo", "title": "\n                Comic – Infrastructure as Code (IaC)            ", "author": ["Aryana Peze"], "link": "https://blog.octo.com/en/comic-infrastructure-as-code-iac/", "abstract": "Comic – Infrastructure as Code (IaC) Publication date 05/07/2019 by Aryana Peze Tweet Share 0 +1 LinkedIn 0 Lire la BD en français Hello ! Deploying a new release to production is more complex than simply checking that the app’s code functions correctly. We must also make sure that it will run correctly on the existing production servers, and with the production environment’s infrastructure. And in order to do this, the Ops leave nothing to chance…! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , General -- DO NOT USE , Infrastructure et opérations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-07-05"},
{"website": "Octo", "title": "\n                Comic – Introduction to networks: IP addresses (part 2)            ", "author": ["Aryana Peze"], "link": "https://blog.octo.com/en/comic-introduction-to-networks-ip-addresses-2/", "abstract": "Comic – Introduction to networks: IP addresses (part 2) Publication date 27/05/2019 by Aryana Peze Tweet Share 0 +1 LinkedIn 0 Lire la BD en français Hello ! We previously saw ( here ) that any device connected to a network has an IP address. There are, however, two different types of IP addresses : private IPs and public IPs ! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure et opérations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-05-27"},
{"website": "Octo", "title": "\n                Putting an end to “Technical Debt”            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/putting-an-end-to-technical-debt/", "abstract": "Putting an end to “Technical Debt” Publication date 06/05/2019 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 The First Law of Technology Transfer: Long-range good tends to be sacrificed to short-range good. The Second Law of Technology Transfer: Short-range feasibility tends to be sacrificed to long-range perfection. Jerry Weinberg – Quality Software Management In this discussion, the term: heuristic refers to a method used in a given context, with no guaranteed outcome, that possibly contradicts other heuristics, and which helps reduce the time required to find a solution. The acceptance of such heuristic depends on the context of use, and not on an absolute standard. The term: state of the art ( sota ) refers to the set of heuristics that a person or a group uses at a given time in order to create the best possible solution to a problem only partially understood, in an uncertain situation, and using limited resources. (For more information on these terms cf. Billy Koen, Discussion of the method or this presentation. ) Why I am done with “Technical Debt” I would rather not use the expression “technical debt” in my work. I did so for a long time, but in the majority of cases where I currently use it or hear it used, this expression is inadequate, and even counterproductive. For me, it has many flaws: It is ambiguous : for some practitioners, it refers to a heuristic consisting in temporarily straying from a project’s sota in order to reach an intermediate goal. This is sometimes referred to as “tactical” technical debt. For many other practitioners, the expression refers to what constitutes a liability in a software artifact when it is deemed not conforming to the current minimal sota that is agreed on by our industry in general. This is sometimes referred to as “endemic” technical debt. It is this second meaning that seems to me an abuse of terms, and that I would rather not use in my work. It is negative : it presents the solution building activity as a process that has destroyed as much or more value as it has created, and its agents as careless (or cornered) borrowers who didn’t pay enough attention to the balance, that is to the consequences of their actions and decisions. It is exclusively financial : it implies, tangentially, that “quality” (another term that I wish to stop using) has a cost that can be known, and that despite our inability to reduce that cost, we can still manipulate it, like a quantity that we put in front of the (also fictitious) value created by the software, in an equation that would determine “developers’ productivity”. A simple metaphor at the start, it has been promoted to an engineering metric , and marketed as such. The fact that solutions exist on the market that can evaluate (in dollars) technical debt shows the immature tendency of our industry to transform simple metaphors into pseudo-industrial instruments. “Technical debt”, like “Test coverage” has to join in the waste basket of flawed concepts that, once instrumented, create at best an illusion of control. It creates guilt by pointing fingers at the financial deficit caused by the poor quality of a solution, which brings to the fore Quality as a virtue (Prudence, Honesty, Health), and Poor Quality as a vice (Negligence, Dishonesty, Anemia). It is inviting moral clichés that define the “professional” developer as one to declare “I refuse to deliver sh.. whatever the pressure, or the rules I am surrounded with”. It makes a hero of the software craftsman, while giving no perspective at all on the system that is creating the phenomenon being denounced. It sustains the archaic, inadequate separation between “technology” and “business”, functional and non-functional, by assigning to functional the invention of value, and to technical the responsibility for all the costs for the solution. Lastly, it does not contribute to solving the problem that we are trying to put our finger on. “Technical Debt Management” has become one more activity in the set of roles and activities associated with software development. It mainly consists of measuring the phenomenon and its consequences while not addressing its causes. Therefore, I wish to keep only this acceptation of the term “technical debt”: [Contracting Technical Debt] : heuristic by which one momentarily breaches the sota of a project in order to reach a high-priority, intermediate goal. And I want to rid my vocabulary of the term “technical debt”: Technical Debt: general state of a solution deemed not conforming to the sota that is commonly agreed on in the software industry. To assert that “this solution has technical debt”, without considering the initial sota chosen by the group in the context of its project, is predominantly and mistakenly taking one’s own sota for the initial sota of the given solution . We have to put an end to “Technical Debt” (in general) as we have to put an end to “Quality” (in general) . We can enumerate instead the qualities of any given solution in its context. These are the characteristics of the change that we seek to create when adopting a particular sota that is adapted to the context. But when I say “Quality”, I’m implicitly referring to a minimal standard sota, that I suggest is valid in all contexts, i.e an absolute standard. There is no absolute standard. There is no sota that we could denote by this word : Quality. At the Plaza Athénée restaurant in Paris, one can enjoy an excellent quality meal (for €380, beverage non included). And McDonalds, (where one can lunch for less than €8), has a quality management department. Here’s a quality software development project: the solution, created in only 200 days, has been tested and judged as sufficiently adapted to the problem at hand; the code and the documentation conform to the approved sota of the organization this software belongs to; defect prevention practices are at a level high enough to guarantee several releases per month without regression; the architecture is documented, and adapted to the ecosystem this software will be living in. And here’s another quality software development project: the solution, created in only 2 hours, is adapted to the problem at hand; the code is written in a scripting language that is easy to learn and documented; the script will be used only once; during the script execution, it will be possible to stop and resume operations in order to adjust some parameters; What we have here is two quality projects, A and B. Adapt the quality criteria from project A to project B and vice versa, and you will get two absurd projects that no customer would agree to pay for. An alternative I can hear you challenging me, in a calm, collected tone: Perfect. No more “Technical Debt”. What are you proposing? I propose, in situations where one wants to talk about technical debt, to replace this expression with the expression : conflicting heuristics . I propose to replace the phrase: Our solution incurs technical debt. With the phrase: Our solution relies on conflicting heuristics. This is for the short version. For a longer explanation: Evidently, the heuristics we had tacitly or explicitly agreed on until now, which constituted the state of the art that was adapted to the solution we were envisionning, are not followed anymore, or have been modified, and are, in one form or another, in conflict . The (temporary, incomplete, even fragile) agreement that was constituting the sota for our solution has thus to be reenacted, or the objectives of our solution have to be discussed anew. Therefore, the very first thing to do in order to improve the situation, is not to “manage technical debt” or to “pay back the debt” in this solution, but to precisely identify the distinct heuristics that are conflicting in our sota and to solve this conflict. Since it is aiming at a change in the way we consider a very common phenomenon, such an explanation will necessarily produce objections. Objection #1 : In our project, there never really was an “agreement on the sota”. The heuristics that you say made up the sota never were explicitly stated, not to mention examined, and certainly not validated by the team. Exactly. This is exactly the reason why we found ourselves observing that “this solution has technical debt”. Objection #2 : Conflict or not, it is still technical debt, since it is the responsibility of the technical side (and not of the functional side, business, users, management, coaches, etc.). Without the functional side, the business, the users, the management, our solution would not  have “technical debt”. It would just be an unusual, incidental, gratuitous object that some developers made because they could. There is “technical debt” because there is a conflict, and a conflict involves several parties. Objection #3 : To get into the detail of all the heuristics that contributed to our solution as it is and in order to examine them, amend them, and put them back into a coherent set: all of this takes time. Could we not just simply fix the problems? In order to fix problems, one has to resolve the conflicts. If several heuristics that are used to  create our solution are contradicting each other, this contradiction will persist and the state of our solution will deteriorate. We must therefore reconcile these heuristics, or change the objectives of the project. Objection #4 : Do you have at least one (or ten) examples of situations where a solution is in “debt” due to a conflict of heuristics? Yes (in my next article). Highlighting a conflict in heuristics in a project situation is easy, irrespective of this conflict being implicit or explicit. For instance, yesterday I tried until 8pm, to export data in order to create a “quick & dirty” diagram. I wanted to respond to the request of a customer who wants to get a clearer picture of its data with our software. I made the mistake of dealing with this request on my own instead of mentioning it during our standup meeting. I thought that coding this export in python and creating a graphviz diagram would take far less time than the time it would require for us to align on this task in an explicit manner, given the fact that I think I am the only one to master graphviz in the team. At least four of our team heuristics were in a contradiction (explicitly or implicitly) in this affair: [Extreme Customer Focus] : Propose to customers a unique experience in terms of responding to their needs while exceeding, if needed, the standard level of service. [Quick & Dirty] : creation through a series of expedients with the dominant objective of delivering a solution within a remarkably short period of time. [Consistent Code Review] : All the production code should be read by persons other than its authors. [Consistent Unit Testing] : All the production code includes unit tests ensuring that the code conforms to its authors intention. I made a trade off, thinking initially that a short script of twenty lines would suffice (the script is now 152 lines long), and that it was not production code, since this code was written outside the project’s code base. As if the code that makes it possible to ship an artefact to the customer was not production code. In a nutshell, this little script constitutes the beginning of what we could call “technical debt”, but that I would rather denote as : an artefact that was produced by means of conflicting heuristics. Objection #5 : the heuristics mentioned in your example don’t represent an absolute standard. Other teams, or other organizations could very well admit totally different practices and heuristics. There is no absolute standard. And if there was one, we would each make a relative interpretation of it. It doesn’t matter much, since what is important above all, is that in our state of the art, heuristics should be in harmony, not contradictinig each other. Although knowing very common practices is obviously useful (why would we isolate ourselves from all the know-how that permanently accumulates everywhere on the internet or in bookshops and library shelves?), it is better to own one’s state of the art, than to perfunctorily adopt some other place’s “standards” that are not adapted to our context, and to which we conform because we “have to”, and not because we have succesfully experimented with them. Objection #6 : It is hard to see how talking about conflicts can be a help in improving the situation. Communication within projects is already complex enough. The same way that a resource conflict doesn’t fatally end up in a battle, two conflicting ideas don’t necessarily create a conflict between people. A project that would unfold without any conflict of ideas is probably not worth doing. But a project in which conflicts are neither approached nor solved is not a healthy project, but a project in agony. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-05-06"},
{"website": "Octo", "title": "\n                Comic – Introduction to networks : IP addresses            ", "author": ["Aryana Peze"], "link": "https://blog.octo.com/en/comic-introduction-to-networks-ip-addresses/", "abstract": "Comic – Introduction to networks : IP addresses Publication date 03/05/2019 by Aryana Peze Tweet Share 0 +1 LinkedIn 0 Lire l’article en français Hello ! It is great to have a functional website – but it’s even better if people have access to it ! And in order to contact a server, we use IP addresses. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure et opérations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-05-03"},
{"website": "Octo", "title": "\n                Thoughts about Scaled Agile Frameworks and how to consider them            ", "author": ["Romain Vailleux"], "link": "https://blog.octo.com/en/choose-scaled-agile-framework/", "abstract": "Thoughts about Scaled Agile Frameworks and how to consider them Publication date 13/05/2019 by Romain Vailleux Tweet Share 0 +1 LinkedIn 0 “What if we scaled agile to the entire company…” This is a phrase heard more often than not in companies that becoming more mature at Agile. But what does this sentence mean exactly? What does the idea of ​​”scaling up” imply? Are there solutions that can be applied quickly? Quick overview: Did you say Scaled Agile Framework? What triggers an Agile transformation “at scale”? SWARMing, what is it? A strategy for using scaled agile frameworks ==> Read the entire article (free) <== Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-05-13"},
{"website": "Octo", "title": "\n                Test your infrastructure code with Terratest            ", "author": ["Nicolas Pascal", "Edouard Perret"], "link": "https://blog.octo.com/en/test-your-infrastructure-code-with-terratest/", "abstract": "Test your infrastructure code with Terratest Publication date 19/03/2019 by Nicolas Pascal , Edouard Perret Tweet Share 0 +1 LinkedIn 0 With the emergence of Infrastructure As Code , (Ansible, Puppet, Heat or Terraform), we’d like to take advantage of all the good practices brought by the Software Craftsmanship movement in order to guarantee our infrastructure’s code quality. Every professional developer knows that to ensure code quality you need tests. One of the resulting practice is TDD aka Test Driven Development. As a reminder, TDD consists in: begin by creating a test; verifying it’s failing; writing the code necessary to make the test succeed; relaunching the test and verifying it’s succeeding as well as the previous tests and finally refactoring the code before starting a new cycle. The 3 main TDD’s steps The advantage of this practice is to provide a short feedback loop and that way to detect bugs as soon as possible. It also makes it possible to meet requirements with minimal complexity and therefore to design better code. If today the tools allowing developers to do TDD have matured in most languages, this is not the case for Infra as Code tools. We can still read, in the very good post “ The Wizard ” that today, we could write Ansible code in TDD efficiently…. …but as far as provisioning tools like Terraform are concerned, it’s different. Manual test pains Terraform , built by HashiCorp , allows us to define infrastructure in a high level language and to deploy it in a cloud provider environment like Amazon Web Services or Google Cloud Platform. Today when we want to test code, we execute tests manually. We can’t do it locally, you can’t test a VPC installation on your machine using localhost . We are testing directly in our cloud environments. These can be several minutes long, often including many steps sometimes manual, which forces us either to wait or do regular context switching: therefore delaying the feedback loop. Then, we must validate that the generated infrastructure is matching our expectations. In order to do so, we’re using a web console or a command line. Sometimes, we connect directly to a machine to test the presence of a file. Then we delete this machines and restart again … What we would have done manually, maybe several times in order to correct our mistakes, Terratest helps us automating it. Terratest What is Terratest ? It’s a Go library that helps you create and automate tests for your Infra as Code written with Terraform, Packer for IaaS providers like Amazon, Google or for a Kubernetes cluster. Terratest is developed by Gruntwork , an American society, in partnership with HashiCorp who open-sourced more than 300 000 lines of code and which is using Terratest to maintain its codebase. Terratest is available on Github since April 2018, you can go here if you want to play with it. Why using Terratest ? There is many advantages to test your code with a library like Terratest, here is a non-exhaustive list: Test complex infrastructure behaviours End to end tests Infrastructure documentation No need for ops to maintain a permanent iso-prod infrastructure to test its modifications Fast feedback loop allowing efficient bug fixing Being able to launch swiftly and regularly these tests Ensure resilience when upgrading tools versions Being able to test cloud-init like scripts Validate deployed AMIs Moreover, Terratest provides many examples in its Git repository, easing the usage of the library. How does it work ? To write Terraform code following Test Driven Infrastructure principles, we’ll proceed in stages: We’ll start by writing the test using Go in *_test.go , like for example instance_test.go . We’ll be attentive to make true or false assertions in our tests. According to the TDD principle, we will first check that the test does not pass by executing the command “go test instance test_test.go” . Then we’ll write our Terraform code, describing our infrastructure, by trying to keep a modular structure of the project. We’ll relaunch Terratest, this time to deploy your infrastructure in your favourite IaaS. Be aware Terratest really builds the infrastructure. It makes a terraform apply , this can obviously involve costs. Terratest runs the tests. To validate the compliance of our infrastructure, the library might call HTTP endpoints, connect to machines via SSH, to execute commands, upload files, request Cloud Provider APIs and read Terraform outputs… Finally, the tests infrastructure will be destroyed by terraform destroy . Test results will be displayed in the console. Hands on Objective: To play with tool, we’ll test an instance initialization script by connecting directly to the machine to verify its content. Installation To discover the Terratest library you can clone its Git repository. The code repository contains both modules and many examples. The examples give a good idea of what the tool can do and are a good starting point. Terratest being a Go library, it is obviously necessary to have it installed on your machine. To install the Terratest library modules it is preferable to use a dependency manager such as dep . We can therefore install the module that will be used to test Terraform in this way: dep ensure -add github.com/gruntwork-io/terratest/modules/terraform Or like this with go get: go get github.com/gruntwork-io/terratest/modules/terraform It is advisable to describe all dependencies in the Gopkg.toml file, this also allows you to fix the version of the dependencies. Then make sure you have the necessary access to your cloud provider, in our case, we use AWS and load the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. We’ll also make sure we have a private key/public key pair to connect to the created machines and have our public key in AWS . For this example, we named the key “terratest_key” Finally, we’ll create a project with empty files structured in this way: Project Structure Dependencies To start, we use the well named “test” Go test package and import the modules we need into the instance_test.go file: package test import (\r\n   \"testing\"\r\n   \"fmt\"\r\n   \"time\"\r\n   \"github.com/stretchr/testify/assert\"\r\n   \"github.com/gruntwork-io/terratest/modules/terraform\"\r\n   \"github.com/gruntwork-io/terratest/modules/aws\"\r\n   \"github.com/gruntwork-io/terratest/modules/ssh\"\r\n   \"github.com/gruntwork-io/terratest/modules/retry\"\r\n) First test : The SSH Key Then, we declare our first test function, which must be named like this: func TestXxx(*testing.T) to be able use it with the go test command. We start by testing that a machine is well created and with an SSH key. func TestInstanceSshKey(t *testing.T) {} Now that we have written the “configuration” part, it’s time for action. We want to initialize our Terraform working directory and apply our code (terraform init + terraform apply) . It is the InitAndApply method that will launch this creation. We also want to destroy all our machines at the end of the tests (terraform destroy) . The keyword defer allows you to add the Destroy method to the list of actions to be performed when returning the function. defer terraform.Destroy(t, terraformOptions)\r\nterraform.InitAndApply(t, terraformOptions) And finally we fetch the terraform output in which we will find the name of the SSH key of the instance. This key will allow us to connect with Terratest to the machine. We will therefore make a first assertion so that this key can be defined. instanceSshKey := terraform.Output(t, terraformOptions, \"instance_key\")\r\nassert.Equal(t, \"terratest_key\", instanceSshKey) Here is our first complete test function: func TestInstanceSshKey(t *testing.T) {\r\n    terraformOptions := configureTerraformOptions(t)\r\n    defer terraform.Destroy(t, terraformOptions)\r\n    terraform.InitAndApply(t, terraformOptions)\r\n    instanceSshKey := terraform.Output(t, terraformOptions, \"instance_key\")\r\n    assert.Equal(t, \"terratest_key\", instanceSshKey)\r\n} Run the test The following command: go test instance_test.go … runs our test and will reward you with a joyful : FAIL: TestInstanceIp (0.27s) Not being very verbose by default, we’ll rerun the command with the -v option to get more details. With a quick glance at the logs, we quickly understand that terratest didn’t create anything, it’s normal, we haven’t implemented anything yet. On the other hand, we notice in the logs that it has initialized the working directory (terraform init) and we now find Terraform .tfstate files. Implementation We now want to use Terraform to build our EC2 Instance. In the main.tf file we declare the following: resource \"aws_instance\" \"example\" {\r\n    ami           = \"ami-5026902d\"\r\n    instance_type = \"t2.micro\"\r\n    key_name = \"terratest_key\"\r\n} This simple code snippet describes a centos 7 instance (describe by the ‘ami” property), t2.micro . And configures the instance SSH key, this key already exists in AWS, we have created and copied it during the installation). Let’s now add in the output.tf file, in which we indicate the output of the code, the instance key name: output \"instance_key\" {\r\n    value = \"${aws_instance.example.key_name}\"\r\n} New attempt We restart the tests with the -v option which will allow us to have a more complete report. The logs show the  Terraform initialization step (init), then the application (apply). We also see the requested output : the instance key name. Then we observe the machine destruction. And finally, the relief: --- PASS: TestInstanceSshKey (73.80s)\r\nPASS\r\nok      command-line-arguments  73.812s There is a cache so if you run the same command twice without any changes in the Go code the answer is instantaneous but the tests are not replayed. We can still force the execution by changing the following environment variable: GOCACHE=off. No refactoring to do, our code is very simple. If we connect to the console, we see that our instance has already been destroyed. The test lasted a little over a minute and above all without any intervention on our part. On the other hand, we haven’t tested anything interesting, except that Terraform was doing its job well and we’re getting an output from it. Second test: The public IP Let’s now write our second test. Our final goal is to connect to a machine to check the presence of a file. We must therefore ensure that the instance is publicly accessible . Let’s start by coding the test. To improve modularity, we will write the test in an independent function. This allows us to run each test independently of the others, but reduces the readability of the output logs. On the other hand, it requires the creation and destruction of a new instance for each test, a very time-consuming operation. Let’s create the new test function: TestInstanceIp. The structure of our instance_test.go file now looks like this: func configureTerraformOptions(t *testing.T) *terraform.Options {...}\r\nfunc TestInstanceSshKey(t *testing.T) {...}\r\nfunc TestInstanceIp(t *testing.T) {...} We want to make sure that our instance has a public IP. And we would like to use the Terratest aws module for this purpose. As in the first test, let’s add the methods to create and destroy our small architecture: terraformOptions := configureTerraformOptions(t)\r\ndefer terraform.Destroy(t, terraformOptions)\r\nterraform.InitAndApply(t, terraformOptions) Let’s make sure the test is red first. To obtain the IP with AWS, we need the instance ID. The ID is an output parameter of Terraform, let’s test and implement its recovery, identical to that of the SSH key. We obtain the following assertion: instanceID := terraform.Output(t, terraformOptions, \"instance_id\")\r\ninstanceIPFromInstance := aws.GetPublicIpOfEc2Instance(t, instanceID, awsRegion)\r\nassert.Equal(t, “fake_ip”, instanceIPFromInstance) Let’s verify our test is red: (There is a lot of logs between the 2 results) --- PASS: TestInstanceSshKey (45.02s)\r\n=== RUN   TestInstanceIp\r\n---\r\n--- FAIL: TestInstanceIp (52.22s)\r\ninstance_test.go:50:\r\nError Trace: instance_test.go:50\r\nError: Not equal:\r\nexpected: \"fake_ip\"\r\nactual : \"35.180.230.122\" Indeed, the IP of the machine is not “fake_ip” . Our test fails, we can rely on it. Implementation We then realize that by default, AWS instances are created with a public IP; we will check that the IP returned by the Terraform output is identical to that of Terratest. Let’s add the following output in output.tf: output \"instance_id\" {\r\n    value = \"${aws_instance.example.id}\"\r\n}\r\noutput \"instance_public_ip\" {\r\n    value = \"${aws_instance.example.public_ip}\"\r\n} Then let’s modify our test function TestInstanceIp() in order to compare the two values. La fonction entière : func TestInstanceIp(t *testing.T) {\r\n    terraformOptions := configureTerraformOptions(t)\r\n    defer terraform.Destroy(t, terraformOptions)\r\n    terraform.InitAndApply(t, terraformOptions)\r\n    instanceIP := terraform.Output(t, terraformOptions, \"instance_public_ip\")\r\n    instanceID := terraform.Output(t, terraformOptions, \"instance_id\")\r\n    instanceIPFromInstance := aws.GetPublicIpOfEc2Instance(t, instanceID, awsRegion)\r\n    assert.Equal(t, instanceIP, instanceIPFromInstance)\r\n} We restart the tests: --- PASS: TestInstanceIp (79.52s)\r\nPASS\r\nok      command-line-arguments  124.562s Victory, it’s green ! Okay it’s green but the details are not very explicit. Third test : File Content We have verified that the instance is created with our SSH key and a public IP address. We will use these tests now to test the writing to a file by a script launched at the machine initialization. We want to check that the file “/tmp/salut” contains the string “Hello World”. We use the ssh package and the CheckSshCommandE() function to execute the “cat /tmp/salvation” command on the machine and compare it with the character string. Which gives us the assertion: expectedText := \"Hello, World\"\r\ncommand := fmt.Sprintf(\"cat /tmp/salut\") // Command executed on target machine\r\nactualText, err := ssh.CheckSshCommandE(t, publicHost, command)\r\nassert.Equal(t, expectedText, actualText) Now we have to tell Terratest how to connect to the machine in question, to get its address as the output. We use the default user ‘ec2-user’ and our agent’s SSH key. publicIP := terraform.Output(t, terraformOptions, \"instance_public_ip\")\r\npublicHost := ssh.Host{ Hostname:  publicIP, SshUserName: \"ec2-user\", SshAgent: true, } So we ask for 30 tries, 1 every 5 seconds , ignoring the exceptions in order to be able to continue. Finally, since it can take up to a few minutes for the instance to start, we need to make sure that Terratest will try to connect the machine several times before declaring a failure. maxRetries := 30\r\ntimeBetweenRetries := 5 * time.Second\r\ndescription := fmt.Sprintf(\"SSH to public host %s\", publicInstanceDNS)\r\nretry.DoWithRetry(t, description, maxRetries, timeBetweenRetries, func() (string, error) {\r\n    actualText, err := ssh.CheckSshCommandE(t, publicHost, command)\r\n    assert.Equal(t, expectedText, actualText)\r\n    return \"\", err\r\n}) Here is the TestFileContent() test function : func TestFileContent(t *testing.T) {\r\n    terraformOptions := configureTerraformOptions(t)\r\n    terraform.InitAndApply(t, terraformOptions)\r\n    defer terraform.Destroy(t, terraformOptions)\r\n    publicIP := terraform.Output(t, terraformOptions, \"instance_public_ip\")\r\n    publicHost := ssh.Host{\r\n        Hostname:  publicIP,\r\n        SshUserName: \"ec2-user\",\r\n        SshAgent: true,\r\n    }\r\n    maxRetries := 30\r\n    timeBetweenRetries := 5 * time.Second\r\n    description := fmt.Sprintf(\"SSH to public host %s\", publicIP)\r\n    expectedText := \"Hello, World\"\r\n    command := fmt.Sprintf(\"cat /tmp/salut\")\r\n    retry.DoWithRetry(t, description, maxRetries, timeBetweenRetries, func() (string, error) {\r\n        actualText, err := ssh.CheckSshCommandE(t, publicHost, command)\r\n        assert.Equal(t, expectedText, actualText)\r\n        return \"\", err\r\n    })\r\n} It’s quite tedious and requires programming knowledge in Go. On the other hand, we achieve automatically what we would do manually ( cat on the file). What did we forget ? We’re running the test : go test -v instance_test.go -run TestFileContent Running command cat /tmp/salut on ec2-user@35.180.190.131:22\r\n\"returned an error: dial tcp 35.180.190.131:22: i/o timeout. Sleeping for 5s and will try again.\" Oops , the instance port is not open … We implement and assign to our instance a security group allowing the SSH connection to the machine from any IP in the main.tf file : resource \"aws_security_group\" \"ssh\" {\r\n    ingress {\r\n        from_port = \"22\"\r\n        to_port   = \"22\"\r\n        protocol  = \"tcp\"\r\n        cidr_blocks = [\"0.0.0.0/0\"]\r\n    }\r\n} Let’s retry : go test -v instance_test.go -run TestFileContent New error : Running command cat /tmp/salut on ec2-user@35.180.190.131:22\r\n\"returned an error: Process exited with status 1. Sleeping for 5s and will try again.\" The message is not very meaningful, but we know that we did not create the file. So we finish our main.tf by adding a script that writes in the file /tmp/salut at the creation of the machine : resource \"aws_instance\" \"example\" {\r\n    ami           = \"ami-5026902d\"\r\n    instance_type = \"t2.micro\"\r\n    key_name = \"terratest_key\"\r\n    vpc_security_group_ids = [\"${aws_security_group.ssh.id}]\"\r\n    user_data = <<-EOF\r\n    #!/bin/bash\r\n    echo 'Hello, World!' > /tmp/salut\r\n    EOF\r\n} “ Voilà !” : Running command cat /tmp/salut on ec2-user@35.180.208.120\r\n--- PASS: TestFileContent (0.61s)\r\nPASS\r\nok      command-line-arguments  0.623s We implemented and tested writing to a file with an initialization script. In this exercise, the script is not tested in its entirety (we could still check the permissions etc…), but this already gives an idea of what we can accomplish with this tool. To go Further These non-exhaustive tests allowed us to see some aspects of Terratest. By digging a little into the repository, we found examples of more complex tests to validate the behavior of your architecture over time, such as a deployment without service interruption. It also describes how to separate tests into steps using environment variables to avoid the systematic creation and destruction of instances between tests. In addition, Terraform generates a tfstate , a .json file that describes the state of the infrastructure, we can use it to run a test several times without rebuilding or deconstructing the instances. Also in this repository, mechanisms will be found to randomize the regions in which the infrastructures are created to ensure that creation is possible in all. We will also find out how to make the names of the machines random to avoid conflicts. It can be interesting to integrate Terratest into a continuous integration platform, and to play tests at each update of the infrastructure code. Building environments only for the duration of the tests is more economical than having dedicated environments all the time. Finally, the same repo includes test examples and modules covering other AWS services such as S3, ARDS, CloudWatch, IAM and VPCs. Not to mention that Terratest covers other tools: Packer, GCP and K8. Bonus : Cloud Nuke A risk identified by Gruntwork is to have resources not destroyed after failed test series and therefore to have stagnant instances among those that are really used. This represented a significant cost for them: ~85%. They have therefore developed a tool, Cloud Nuke , that regularly cleans the environment of these instances, launch configurations, load-balancers and lost EIPs . Everything that was created more than an hour ago is considered lost, which is longer than the execution of all tests. Their test environment uses an AWS account independent of others to avoid the risk of destroying machines in other environments. Reservations Unlike other infra as code testing tools such as kitchen or molecule, the library does not abstract the logic of creating, testing or destroying its environment. It follows that it is up to the ops to implement certain mechanisms such as retry in its tests. Once again: be careful, the cloud environment in which Terratest builds the test infrastructures must be partitioned and separated from other environments. It would be a shame to destroy production machines by trying to test your infra … Third, for Terratest to interact with the cloud services to be tested, it must have the appropriate rights. This potentially implies having to manage a new user or powerful role and give it the accreditations. The test output report is either too concise: a FAIL/PASS line in non-verbose mode, or too verbose, displaying all the details of creating and destroying terraforms that overwhelm the information. The library is not very well provided on the less “traditional” AWS services but which are, in its defence, extremely numerous. What about the others in all this? There are other infrastructure testing tools, such as kitchen-terraform , a set of test-kitchen plugins written in Ruby, the very young ruby rspec-terraform too, or the Terraform testing framework . Terratest focuses on the functional aspect of the overall infrastructure rather than the individual properties of these components. The library focuses on automating tasks that validate a behavior rather than observing it. For example, we would rather make real http calls and analyze the return code than check that the httpd service is running on the server. Conclusion We have seen that it is possible, although complicated, to create tests written in Go to guarantee the properties of an infrastructure produced by Terraform code. Terratest is fulfilling its promise of generations of ephemeral environments and test automation. It guarantees that at the end of the execution the machines will be destroyed. It also allows a large number of parameters to be tested on AWS EC2 instances. We can see that the tool deserves to be easier to use, to be enriched on its target services, to be more readable on reports and to allow better use at scale. We can think that the TDD will quickly mature and that the arrival of these new tools will democratize this practice and that soon the provisioning tools will be easily testable. With these libraries interacting in imperative languages on infrastructure, we can even think that the infrastructure code is moving towards such a paradigm. Sources https://github.com/gruntwork-io/terratest https://blog.gruntwork.io/open-sourcing-terratest-a-swiss-army-knife-for-testing-infrastructure-code-5d883336fcd5 https://blog.gruntwork.io/cloud-nuke-how-we-reduced-our-aws-bill-by-85-f3aced4e5876 https://blog.octo.com/tdi-ou-test-driven-infrastructure/ (in French) Tweet Share 0 +1 LinkedIn 0 This entry was posted in Infrastructure et opérations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Test your infrastructure code with Terratest” Divya Singh 08/09/2020 à 22:54 Hi team,\r\nPls can you provide your complete code repo link, as I want to check  configureTerraformOptions. because it is really helpful.\r\n\r\nRegards\r\nDivya Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-03-19"},
{"website": "Octo", "title": "\n                What you need to consider when publish a Slack app using GDrive’s API            ", "author": ["William Ong", "William Thomson"], "link": "https://blog.octo.com/en/what-you-need-to-consider-when-publish-a-slack-app-using-gdrives-api/", "abstract": "What you need to consider when publish a Slack app using GDrive’s API Publication date 24/01/2019 by William Ong , William Thomson Tweet Share 0 +1 LinkedIn 0 Nowadays many companies or teams use Slack to communicate and share content. Slack is very flexible and opens up possibilities to developers, as it provides webhooks (incoming, outgoing), bots, and event subscriptions. Your Slack reflects the way that you work. In our company (Octo Australia), we play a lot with Slack webhooks to facilitate our office management (for example to check if someone is at the office, open the building door …) or to create fun tools and notifications (please water the plants, a connected ‘wheel of mis fortune’). Our most recent idea was to create our own bot to save our pictures automatically into our drive because we share a lot of photos of our events and want to save them for later. And we also thought it could be great if we could share it with the world in the Slack app store! Steps Here’s the architecture of our app: We built our internal bot in Golang using Serverless framework to quickly have our product online. Serverless helps us to setup easily our AWS infrastructure and deploy it. Then in order to transform our internal product into a Slack application that is ready to be published, a number of steps need to be followed: Configure your Slack App and be compliant with Slack policy Configure Gdrive App Save tokens from Slack and Gdrive Start by Slack configuration You can easily create an application with Slack via their interface ( https://api.slack.com/apps ) and it should be managed under your slack team. One thing you need to do is to create a second app for the production version because there’s no way for the moment to use the same app and connected to 2 different environments, even so, Slack currently provides you with 2 sets of credentials (for dev and prod) Enable features Now you have to select carefully which permissions you need for your app to grant access to your users’ Slack workspace. These permissions are given by a token using OAuth flow. You can only add the scopes that you are currently using (Slack’s security policy). The thing is if you need a new scope the users will have to reinstall your app. Scopes you can access As Slack use OAuth 2 to generate tokens you need to declare your redirect URLs, which is the endpoint for your server and which receives the code to be able to process. You can add as many as you want and this works well with your local server. Depending on the Slack features you selected, you may to add configuration, for instance in our case we ask for an event subscription and so we need to set an URL (to the server) for Slack to call it when specifics event occurs. Once this done you can generate your Slack button to be used for installation through a website or Slack app store. As a new product, you have to build a landing page and we use an AWS S3 bucket to store our content. The last step concerning Slack is to be compliant with policies about the usage of users data, right on images, support for the users, and so on, which is a long checklist but it’s not a really big deal (https://api.slack.com/docs/slack-apps-checklist). Slack provide you with a lot of tutorials to help you to create your app and moreover, their teams are very reactive to give feedback after submitting. Google Drive For the next part, we need to use Google Drive’s API and for that, we must configure Google OAuth. Because our application requires server to server communication and an authorisation grant (we upload to Google Drive on the users behalf) it’s necessary to obtain a token, which is a credential we can use to perform predefined tasks (uploading). In the Google Cloud console , under APIs and Services, Credentials, we can add an OAuth screen and download the credentials we will need when making a request to the API for our clients credentials. In order to set this up we also need provide a few things, including authorised domains (eg photostashbot.com) and before launching it needs to be approved. Application The essence is sending a request to the API with our application credentials, from a registered domain and with a preconfigured redirect URL as a kind of callback endpoint which will receive the token. Once the token is granted it can be stored in DynamoDB, along with a ‘Refresh Token’. This is necessary to generate new tokens from Google. Each access token has a limited lifespan, meaning rotating the token in use for accessing the account is mandatory. This is different to the Slack API which does not use refresh tokens. Some of the documentation for working with Google Drive was also difficult to find. For example, if a user uninstalls the application from Slack we also want to remove the credentials from our database for Slack and Google, but in the case of Google, we also need to remove the apps access to the users Google account by revoking the token. In the end it is a simple API call to https://accounts.google.com/o/oauth2/revoke?token= but the documentation specifying this was hard to find. Environments management (local/dev/production) Working with a serverless architecture makes it a little bit more difficult to set up environments. While Serverless framework definitely does make it easy to deploy infrastructure to AWS, there are some specific challenges when working in local and dev environments. In order to test locally, we use SAM and specify configuration in a yml file. But this has limitations because in some case we need to actions with ‘live’ interaction with our dev slack api endpoint. For example it is not easily possible to test the OAuth flow (sign in with Slack, add to Slack) locally. In this case we must rely on deploying to AWS from a local laptop. The issue with this however, is that if you deploy to dev you will overwrite the current dev deployment, and in many cases this is necessary. So it can add complexity as team members work on the project synchronously. The overall flow of the software factory is facilitated by CircleCI. Configuration for endpoints, database names and redirect URIs are stored in a set of config files while all sensitive secrets for accessing Slack or Google are accessed via environment variables which are provided by CircleCI during the build and deploy process. Conclusion The most complex development part was to deal with Slack and Gdrive credentials to save and secure them, how to deal with it and how it works especially Gdrive. When you create a public application there is a lot of things you need to take care like legal usage or images, the condition of use etc. It was a lot of fun developing a small project that can be useful for others to use. Want to try it out? Here’s our Slack app: https://photostashbot.com/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged google drive , Slack . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-01-24"},
{"website": "Octo", "title": "\n                OCTO’s vision on the Service Mesh: radiography of the Service Mesh            ", "author": ["David Alia", "David Alia"], "link": "https://blog.octo.com/en/octos-vision-on-the-service-mesh-radiography-of-the-service-mesh/", "abstract": "OCTO’s vision on the Service Mesh: radiography of the Service Mesh Publication date 18/01/2019 by David Alia , David Alia Tweet Share 0 +1 LinkedIn 0 This year, the Service Meshs are of all conferences: istio , linkerd , kubeflix , even zuul ?… In a first article , we positioned the Service Mesh and its stakes in the ecosystem of microservices. We will now propose a radiography of these. Our definition of Service Mesh is as follows: The Service Mesh refers to a platform responsible for ensuring the security, routing and traceability of communications between microservice applications deployed dynamically in containers. Different solutions have emerged to meet these needs: At the infrastructure level, container orchestration platforms such as Kubernetes are expanding with solutions such as Istio to provide an infrastructure platform At the API level, solutions like Kong are expanding to become more dynamic and gain in traceability At software level, software frameworks such as HAPI or Spring offer plugins and libraries that provide very similar functionalities Multiplying solutions is, in our view, the wrong approach. But are all these different solutions equivalent? Insights will be given here to compare the features of these implementations. This will help choose or compose the best Service Mesh solution according to your needs and your legacy. In our first article, we listed two fundamental issues for current applications: modularity and scalability. Here we will describe a framework to list the features required to meet these challenges. For that, we have mainly reused 12-factor apps patterns to characterise these features independently of their implementation. We have positioned these different patterns along 2 axes: The level of technical specificity of each of these patterns with four levels: functional, applicative, technical and infrastructure The type of problem to tackle: services, routing / integration and transverse security We will quote here some solutions for illustration purposes, without being exhaustive on the features for each of them. We will discuss the functional coverage of these solutions in the following articles. Security If we consider a microservice exposing an API, security can be based on the following elements: Developer portal: it allows developers to register with the API and obtain all the information required to make its first authenticated call. OAuth2 : propagates a user’s rights (all its permissions) between several applications OIDC : Open ID Connect extends OAuth2 feature with authentication capability Point to Point HTTPS: HTTP over TLS, or simply HTTPS, secures connections. Frequently, TLS protection were stopped at a central reverse proxy. From now on, security can be achieved not only up to the cluster entry-point, but up to each container entry-point Port Binding: segregates traffic up to the container level, each container only getting the traffic routed to it For example, Linkerd and Istio offer a feature from TLS to the final container in a transparent way for applications. Istio can provide RBAC authorisation functionality, but will delegate all authentication. Conversely, a tool à-la Kong will be better equipped to integrate with an OAuth2 and OIDC infrastructure. Services In a digital ecosystem, we look for modular, scalable services. Deployment is automated or continuous. An application can be divided into a large number of microservices. Data driven development is gradually being implemented with the collection of metrics on a large scale. Needs are also driven by the possibilities of the different platforms. Here is a non-exhaustive but as representative as possible list of the state of the art of microservices: Feature flipping : Continuous deployment of new features is not always desired by users who may need time before the official roll-out. Training may be necessary or simply promotion must be done on the new feature. Feature Flipping enables or disables a feature. This allows to decouple production roll-out and deployment which is fundamental to allow continuous deployment. API Documentation : ability to provide all API information to make it more intuitive for a client developer. Developer Portal and Analytics : user identification is a prerequisite for collecting statistics. These metrics can range from SEO on the API development portal to drills down into methods response times as part of performance management. Centralised logging : in a context of ephemeral containers, logs must be extracted and correlated to each other to provide a global view of the application for error management and diagnosis. Distributed tracing : beyond logs, the traceability of exchanges between different microservices is an essential functionality to understand routing and diagnose possible latency problems. One codebase & environment configuration : in order to follow the version of an application from end to end, all these elements must be versioned. On the one hand a basic code that remains identical between environments, and on the other hand an environment configuration managed by an infrastructure solution as code (container or other orchestrator). This configuration will also be versioned and used for deployment in each environment. Disposability, idempotence & self-admin tasks : ability to handle all start/stop tasks (e.g. data initialisation, deduplication of messages that are already processed) by the microservice itself. This is an essential feature of an application for it to run on a dynamic platform such as a container orchestrator. Indeed, at any time the instance can be destroyed or created again. The microservice needs to configure itself to be ready to process requests. Bulkhead : like the watertight housing of a boat’s hull, an application’s ability to continue to respond in degraded mode in the event of a serious error on one of its components or one of its dependencies. Circuit breaker : like the physical circuit breaker, a proxy implementing a state machine that will disable calls to a dependency in case of too many errors. This pattern allows not to overload a microservice due to a problem in another downstream microservice. Container : packaging of a process that isolates the filesystem and IT resources from the execution of this process. Docker is the most well-known type of container. A container is used to standardise deployment. Container orchestrators can thus process all microservices in an industrialised way. Scaleout through stateless process : ability to distribute processing over several processes that do not share any states. This is an essential feature of the application so that it is scalable and can leverage the capacity of an on-demand infrastructure such as a cloud or container orchestrator. Placement strategy : functionality at the heart of container orchestrators that allows a container to be executed on the most appropriate physical node based on a number of constraints. Istio and Linkerd will provide advanced distributed tracing features. But they will have to rely on other tools for centralisation of metrics or identification. Some application features such as Circuit Breaker can also be supported by these tools. But for a totally transparent behaviour for the user, these generic tools will always be limited. For example, an elegant graceful degradation strategy cannot be implemented by a generic business-agnostic tool. Communication – routing The finer the modularisation granularity becomes, the more complex the communication between different microservices. The following features are gradually becoming essential to manage this complexity: Consumer Driven Contact (CDC) : If we use Sam Newman’s definition in “Building Microservices”: “ With the Consumer Driven Contract, we define the expectations of a consumer on a service […]. Its expectations are captured in the form of tests, which are then executed on the producer service. As a target, these CDCs must be executed in the ongoing integration of the service, so as to ensure that the service is never deployed if it does not respect one of these contracts. “ API Management Portal : This extension of the developer portal allows to configure APIs and configure query routing. It can also manage API documentation. Tolerant Reader : The principle is to ensure that the consumer of any external system is tolerant to the evolution of the data schema that is provided to it in read mode. This consists in providing palliative solutions in the event of unexpected value, absence of value or not respecting the expected format. Web browsers, for example, are relatively tolerant of deviations from the HTML standard. Orchestration : SOA’s traditional approach consists of having a central component (often an ESB) to orchestrate all exchanges between services. The orchestration approach consists in delegating this responsibility to the various microservices and thus decentralising it. Request Collapsing & Request Queuing : ability to delay or group several requests to make only one physical request to the target service. This pattern optimises scalable routing. Canary Release : ability to release a new version of the application to a subset of selected clients. This allows the new version to be validated before it is widely deployed. Read more about it in our article about Zero Downtime Deployment . Load balancing : ability to redirect a network stream to multiple components transparently to the caller. This makes it possible to distribute the load or to compensate for the failure of one of the components. Software Defined Network (SDN) : completely virtual network environment where the routing part itself (data plan) is separated from the routing plan part (control plan) which defines the routing rules globally. Istio and Linkerd provide very advanced functionalities in terms of dynamic routing and traffic management. They are also very well integrated with the SDN functionality of container orchestrators. Their differentiation is based precisely on an SDN-like architecture (the data plan in a container is attached to the application container, such as a sidecar is attached to a motorcycle). Service Mesh applies this concept to application routing. Their choice to have a transparent approach towards applications will necessarily limit them on the API and application part. API management solutions will be very well equipped for API management and load balancing, but they will require more development to follow the dynamic topology of a container orchestrator. Through this framework, we were able to show that the Service Mesh provides very useful functionalities to meet the challenges of modularity and scalability. But the tool by itself may not be sufficient or it may not be necessary in your context. Like any tool, it is essential to identify your needs beforehand. Choosing it because it is on everyone’s lips today will only add technical debt to your architecture. Twisting a tool to meet a new need for which it was not designed will have the same type of adverse consequence. We hope this article will help you make an informed choice. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-01-18"},
{"website": "Octo", "title": "\n                On Operations, DevOps and soft skills            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/on-operations-devops-and-soft-skills/", "abstract": "On Operations, DevOps and soft skills Publication date 17/01/2019 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Let’s talk about communication for a bit. One of the most interesting roles I’ve had to fulfil the last couple of years has been the “ Operations guy working as a part of a Development team ”. This is a fascinating situation to find yourself in. Allow me to elaborate. Historically, Operations teams have been isolated from Development teams. Two separate organizational entities . The reasons for this were manyfold: Operations people were a scarce resource, they needed to accommodate vast amounts of work for numerous Development teams (server provisioning for team A, middleware configuration for team B, application deployment for team C), it made sense for management to group people into teams using their skillset as sorting criteria… you name it. In the end, most interactions between Development teams and Operation teams happened through ticketing systems. Effective communication These workflows created and fueled most of the communication issues within organizations, and by doing so, created gargantuan bottlenecks on the development/deployment pipelines. Applications and services took months or even years to ship into production. This, needless to say, was frustrating for everyone involved in the process. It also had a huge impact on Time to Market. No one was happy. While these situations still occur nowadays, the incorporation of Agile approaches into Software Development is becoming increasingly common, and its impact on Operations is clearly visible from an organisational point of view. It is not rare, for example,  to see Feature Teams within an organization . These are cross-functional , which means that they tend to incorporate many different profiles into their ranks. From conception, design, and implementation, up to deployment, product owners and designers will be working hand in hand with developers. This most likely means that an OPS will also be a part of the team . How is that a game changer? When working as an OPS in a Development, Feature or Product team, most of your responsibilities will shift , whether you realize it or not. It will not be about taking team X’s artifact and deploying it on servers A, B and C anymore. It is your team now , and therefore, your artifact and your servers . You will most likely have to deal with a lot of things you are not used to dealing with. This is a good thing. Embrace it. It is an opportunity. If you do things right, you will be able to: Use and apply your knowledge regarding technical architecture and systems. This may concern not only the application itself, but also every service or platform involved. Facilitate discussions and answer questions regarding your areas of expertise. Once again, this not only includes the application, but also the components around it. Show people what you do everyday, and how you do it. What’s the point of your work? Is it actually necessary? Isn’t it something that needs to be done only at the beginning of an application’s lifetime? Improve the team’s dynamics: help them grow, and help them care. Don’t hold back: from good development practices applied to Operations, to workshops on existing processes. Learn, to a vast extent. Under this light, the role of an OPS inside a development team is virtually Tech Leading, focusing on operational aspects. In short, it’s mostly about soft skills (even though technical skills are still required), about the ability to express ideas, vulgarize subjects, solve issues, analyze and solve problems, and share knowledge with your team. That’s a tremendous change. And the basic ingredient for all of these is kindness. On technical architecture When working on an organizational setup like this, there is a high chance that you will be the most technical person on the team. When I say technical, I mean with the biggest background on Linux/Windows systems, middleware, networks and virtualisation. If not, that’s great news . It means that you will have people with whom you will be able to discuss all of these subjects. Someone who will be able to challenge you, or remind you what’s important when you’re having tunnel vision. In any case, you will most likely have to propose solutions to many different issues. At some point you will have to deploy your application and its dependencies somewhere. That’s where your knowledge on the subject comes in. Web servers, application servers, reverse proxies, caching systems, databases, failover, high availability, disaster recovery… These are all things you will have to analyze. Should you use nginx or Apache? PostgreSQL or MySQL? The answer is always the same: it all depends on your needs . Try to analyze what you need before proposing a solution. Leverage your experience when doing so. Be cautious: this does not mean that you need to make all of these decisions all by yourself . There are trade-offs for every single choice you will make. These trade-offs will not only impact the functioning of the application itself, but also its development. This means that their opinion on the matter is paramount. Your role is to explain the different options to the people that are concerned by the choice. Remember, consensus is key. Embrace challenge too. For good ideas, you need human interaction, healthy conflicts, argument and debate. When doing so, remember to be kind . Don’t impose your opinions on everyone else. Be constructive. And this is not specific for this section. There should be an acronym for this, RTBK. It should be used way more often than RTFM . This also applies to Continuous Integration and Continuous Deployment. What is the simplest workflow in order to take the application’s source code, shape it into the actual application and make it accessible to users? This has tremendous value: it will allow you to automate time-consuming, repetitive tasks, creating a safe automated pipeline, which will in turn allow everyone to concentrate on delivering value. A piece of advice: start small, and work your way up to something that suits your needs. “Simple is better” should be your mantra. Once again, apply the knowledge acquired from previous experiences when doing so. Improve, based on your experiences On facilitating interactions More often than not, people on your team will have questions regarding your field of expertise. Discussions will be held on subjects you know in depth. A lot of terms will be thrown around: availability, redundancy, stress testing, building and deployment. Once again, if they know everything there is to know about these subjects, that’s a good thing too . Either way, you might want to step in and catalyze the discussion. Individuals and interactions over processes and tools Before doing so, remember to be kind . It is a key aspect in human interaction, and it will encourage participation, collaboration, and innovation. First off, explain the meaning of every concept being discussed to everyone. It is crucial for every single person on the team to share the same language in order to have effective interactions . When having discussions about the “bastion”, one person may be talking about the web interface of a Cloud provider, whereas another one might be talking about a Linux server. Be clear with your communication. More specifically, work on your vulgarization. I can’t stress this enough. The ability to express complex concepts in a simple fashion is priceless. It’s one of the most valuable skills you can learn. They don’t necessarily need to know every single detail on the subject. It is not the same, saying “there was a problem with the application’s configuration” than “the application crashed because we forgot to set the heap’s maximum size”. A fundamental part of the vulgarization exercise is to be capable of discerning the appropriate level of technical depth for each person. Sometimes, these discussions will start to consume the time allocated for other purposes: stand up meetings, retrospectives, backlog grooming or others. While it’s good to be able to facilitate these discussions, it is also important to find the right instances to do so. If they do not exist, you can propose new ones yourself. Or even better, just make yourself available in order to discuss these subjects. Individuals and interactions over processes and tools. On sharing A large amount of people see Operations as an impenetrable affair. It is safe to say that it is a hard topic to grasp. This is mainly due to the huge amount of subjects it covers. Operations entails development, systems administration, networks and security, just to name a few. This may sound intimidating to most newcomers. Nevertheless, this does not mean that they are not interested . They are often just scared of the unknown . Once people see that you are approachable, they will start asking questions. Even if they don’t, you should ask them if there are things that they want to learn. If things go right, you’ll get the opportunity to share your knowledge. Before even thinking about doing so: RTBK . If you don’t know what this means, you’re skipping important parts of this article. Don’t. Go back, and take the time to read them. There are two fundamental reasons to share your knowledge. First, mentoring people is one of the most interesting and rewarding things you can do . Motivated people have tremendous potential , and if they are willing to learn and are interested in what you do, you can catalyze their growth to a great extent . Sometimes it only takes a little push for someone to discover a great deal of capabilities . You just need to know how to give the right push, in the right direction. Second, it allows you to spread the knowledge . If you’re the only person working on these subjects, you will most likely become a single point of failure . If something happens to what you built when you are not available to fix it, the whole system goes down. It is reasonable to want to share that responsibility with someone . Do not expect them to become fully autonomous, or a plug in replacement for you right away, they are already doing something else as a full-time job. Still, having enough knowledge to be able to debug common issues is already great. Fullstack DevOps engineer The most efficient form of knowledge sharing is pair programming. It is fundamental that everything that is going on is completely understood. You can’t expect people to pick up Infrastructure as Code without having any knowledge on Linux systems, for example. This is not necessarily an issue, as you can teach them as you go. Make sure they understand what’s going on, and do it often. Ask them to reformulate what you just said: drawing things works great for this purpose. This is probably going to be a good exercise for you as well. Explaining a concept to someone forces you to structure it differently in your head, and it will allow you to know if you fully understand it too. If you do not, you can look up the answer together. This is not a problem. More on this in the next section. Later on, you can give them tasks you would normally work on by yourself. When doing this, the most important part is to be able to select the right amount of work, with the right complexity. It must be challenging enough for them to feel excited, and not hard enough to make them frustrated. Aim for balance. You can also use other formats in order to teach large groups of people at the same time. Mob programming works great too, or code katas, if you have interesting subjects you can share. You can teach them how to code their own wrappers using Test-Driven Development , how to test their infrastructure code , or how to leverage monitoring or reporting tools in order to understand what’s going on within your system. On improving Last but not least, you should contribute to the continuous improvement of the team. When doing so, remember to be kind . Make sure everyone on the team knows how important this is. It is a must-have quality when trying to improve as a whole, when proposing enhancements and giving feedback. Change your formulations, switch from “you made a mistake” to “we made a mistake”, it will help you stay away from finger pointing. Responsibilities should be collective. It’s hard to be specific on this topic, since areas of improvement will vary from team to team. I’ll give it a try though. At first, most people will refer to you as the DevOps of the team. Explain them that DevOps is not a role, but a culture of collaboration and communication. It should be a goal you should all aim for. Fail , and fail fast, too. Champion innovation, testing new ideas, validating them, and cope with the fact that sometimes they don’t work. It’s a good thing, as long as you know when to seek an alternative, and you get to learn something. Work on quality . Start doing code reviews, for example, if you are not doing them already, and include infrastructure code as well. They are most important when trying to see what people are doing, to correct or improve certain practices, and to share the code. Have them read your code too. At first they will be scared, and they will tell you that they don’t want to because they won’t have any remarks or contributions. This is not true. They will read it, and understand how it works. If they don’t, they can come over and ask for clarifications. No single points of failure. Measure everything . Preach the importance of monitoring and observability. Let them know how to work on code instrumentation for it to be easily observable. Show them the benefits of having structured, clear logs, and being able to query the platform to have immediate answers on what is going on, at any time. They’ll be onboard as soon as you show them the advantages. Lead by example . Motivate them, code with them, show them that you can do the same things you do in standard development when you’re doing Operations. Show them Clean Code, Test-Driven Development and refactoring, applied to infrastructure code. Learn from errors and mistakes. If you have a production incident, analyze the root cause so that you can learn from it, and prevent it in the future. Asking yourself five times why something happened is a great way of finding root causes . You will often see that what you thought was a technical issue actually has an organizational or design root cause. Say “I don’t know”, when you don’t know. You are not meant to know everything. Not knowing is not an issue. It is impossible to know everything. It is a good thing to say it. Be honest. People will trust you more, and start doing it themselves too. “I don’t know”. Say it. There’s no way I’m remembering all those things You don’t have to. It all comes down to six basic principles: Remember to be kind. Use your power for the greater good. Help people, explain things to them, resolve issues. Share what you know. Improve the team itself. Remember to be kind. And most importantly, have fun. If it’s not fun, it’s probably not worth it. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-01-17"},
{"website": "Octo", "title": "\n                Keep your infrastructure keys safe with Vault            ", "author": ["Benoît Gastinne", "Erwan Alliaume"], "link": "https://blog.octo.com/en/gardez-les-cles-de-votre-infrastructure-a-labri-avec-vault/", "abstract": "Keep your infrastructure keys safe with Vault Publication date 27/12/2018 by Benoît Gastinne , Erwan Alliaume Tweet Share 0 +1 LinkedIn 0 We have already presented Hashicorp Vault on this blog, how it is working in PKI mode (FR) , its integration with AWS (FR) and with Kubernetes (FR) . This time we will focus on another Vault goodness: how to use Vault to manage your SSH accesses in your infrastructure .To do that, we are going to use in the article the SSH Secret Engine provided by Vault.. Version used : 1.0.0 Preamble: How do you do without Vault? The problem of managing accesses to a Linux infrastructure is not new. The classic and cost-effective solution that could be used today would be composed of the following elements: A user with SSH keys on his workstation that allow him to access the infrastructure A controlled and secure entry point to the network, most often a SSH Bastion or a VPN From this entry point, direct access to the servers On those servers: An OpenSSH service, well configured, secure and audited rights per user key, managed through files authorized_keys An Infrastructure as Code tool like Ansible in pull mode or Puppet in charge of maintaining authorized_keys up to date This would give us something like this: This solution has several advantages: it uses components that are already used in your infra, nothing new bastion is an excellent pattern to access your infrastructure the configuration management of your servers is very useful beyond the management of authorized keys it does not require additional expertise, this is a consequence of the previous point the use of authorized keys offers a relatively fine grained control over what the user can do once logged in a well configured and auditable (by sending connection logs out of the machine) OpenSSH server, already provides a satisfactory level of security for many uses Unfortunately, this kind of solution can become complex to maintain, especially when you have to manage accesses for a large number of people on a large number of servers: grouping machines and make different roles for accesses can become necessary this logic must then be implemented and maintained in the infrastructure code Moreover, changes to the infrastructure code must be controlled with great finesse (to allow users to assign certain roles and not others, for example) Some more elaborate solutions exist to address this complexity, such as the open-source solution Teleport from Gravitational which also provides an interesting feature for audit. However, we can go further while continuing to use the standard features of OpenSSH and limiting the number of infrastructure components with the features offered by Vault! Vault as the heart of your authentication With a Vault based solution, the client will need to obtain access from the Vault server API before connecting to other servers of your infrastructure. Now the solution looks like this: To obtain this access, the client will call the API of the Vault SSH Secret Engine . The SSH Secret Engine respects the “philosophy” of Vault: it provides limited access, within a fine grained perimeter/li> it provides time-limited access Accesses provided to the user are “on-demand” and “temporary”. So, how do I ensure that the accesses provided by Vault to the client are accepted by the servers? Excellent question, and the answer is very simple: it depends! There are actually two types of accesses that Vault can provide, each with its own server-side validation mode: One Time Password (OTP) access type SSH certificate access type (and a third and deprecated type that we won’t describe here) In the following examples, we will assume that we are communicating with a Vault server that has been previously unsealed with an access token which has enough rights to execute the action you want to do. It could be a admin token which has sufficiently broad modification rights on our Secret Engine . To test it by yourself, the root token should be good enough (but it’s a very bad practice to use it in production!). We will also consider that the SSH Secret Engine was activated on the path /ssh of the API (his path by default). In addition to the links of the Vault documentation, you can find information about these concepts in the article “ The PKI Vault on the grill (FR) ”. One of the objectives of using Vault in this article is to eliminate the need to update configuration changes in the repository of users and their access to the infra. For that, we will consider that the accounts are non-nominative on the servers (for example admin, web, read-only, …). This is a technical choice that is significant because what is gained in terms of server configuration ease is likely to be lost in terms of traceability of infrastructure usage. Indeed, it becomes more difficult and costly in terms of tools to identify who is at the origin of each action carried out with a non-nominative technical account. A solution to continue to use nominative accounts would be to use a LDAP directory as a user repository. Linux servers have a native integration that allows them to use an LDAP directory as a user account database. First solution: One Time Password access type In this mode, Vault generates one-time passwords with which users can connect to the target servers they requested the access. When attempting to connect to the server, the server will query Vault to verify the validity of this password. Configuring an access in OTP mode Once the SSH Secret Engine is activated, it is quite simple to set up access in OTP mode. We’re going to create what’s called a role in the Secret Engine , this role define a set of access rights that a person can/may request. An OTP role can be created like this: $ vault write ssh/roles/otp_web - <<'EOH'\r\n{\r\n   \"key_type\": \"otp\",\r\n   \"default_user\": \"web\",\r\n   \"allowed_users\": \"web\",\r\n   \"cidr_list\": \"10.1.42.0/24\"\r\n}\r\nEOH We just created an “otp_web” role which will allow to get a one time password from whom can use it ( \"key_type\": \"otp\" ), to connect to any servers of the subnet “10.1.42.0/24” ( \"cidr_list\": \"10.1.42.0/24\" ) and only by using the “web” user ( \"default_user\": \"web\" and \"allowed_users\": \"web\" ). Once this role is created, the user will be able to request a password for the server, for example 10.1.42.123 (which is therefore part of the subnet 10.1.42.0/24) with the following API call: $ vault write ssh/creds/otp_web ip='10.1.42.123' In the SSH Secret Engine role of the OTP type on the path “ssh/roles/[name of the role]” will make the path available “ssh/creds/[name of the role]” on which the user will be able to obtain single-use login passwords (within the limit of what the role configuration is allowing). Checking the password on the server Now that Vault is able to generate single-use passwords, all that remains is for the servers to be able to check the validity of the passwords presented to them. To do this, you can use the small utility vault-ssh-helper . The tool is a self-supporting Go binary that can be downloaded here . This binary will be called through the authentication mechanism PAM to check the passwords of users who are trying to log in. Once called, he will check the password with Vault, which will confirm if it is valid or not. If the same password is subsequently submitted to Vault for verification, Vault will refuse it, thus ensuring that each password is used only once. The README.md file of the GitHub project gives all the necessary information to set up the tool, it describes the configuration (very simple) of the tool, how to configure  PAM to call vault-ssh-helper when checking passwords for incoming SSH connections and how to configure the SSH server to ensure that they use PAM to verify the passwords they receive for their incoming connections. First impressions This option is interesting in many ways: It allows to control very precisely the users and servers on which a connection is possible (using allowed_users and cidr_list parameters) It ensures that the access provided by Vault are used only once It offers very good auditability of server access, as each verification request made by a server will be logged in the Vault audit system However, it has a major drawback that is even mentioned in the documentation: it requires a call to Vault for each password check. This presents a risk of spoofing the Vault server that could allow unauthorized access to the servers. In addition to this significant disadvantage, we can mention some other minor ones: the need to use a third-party tool to insert into the PAM authentication process adds complexity to the basic software foundation that all servers in the infrastructure must have the use of passwords rather than SSH keys to connect makes this system a little more difficult to support with some automated SSH accesses the inability to control with Vault what the user can do once logged in (e.g., allowing port forwarding) Second solution: SSH certificates access type In this mode, Vault will sign the user’s SSH public key with his private key, this signature will have a short validity period of a few tens of minutes maximum. When this user presents his signed key to the SSH server, the SSH server will verify that the signature matches the Vault public key that has previously been installed and indicated as a trusted CA ( Certificate Authority ). You meant “TLS certificate”, right? Well, no, it’s a SSH certificate. Indeed, the asymmetric SSH key system is based on the same cryptographic primitive as TLS: the RSA encryption . Other systems start to become more popular (whether on the SSH or TLS side) but RSA remains the reference. It is also the only encryption supported by SSH Secret Engine . Note that the notion of TLS certificates is a small misuse of language, they are X.509 certificates which are used in the context of the TLS protocol. The use of these certificates is also essentially the same as in TLS: the public key of an SSH server can be replaced by a certificate signed by a CA that users who connect trusts. This makes it possible to trust a server on which you connect for the first time, exactly like what happens on the Internet with HTTPS (i.e. HTTP in TLS). This mechanism then replaces SSH’s traditional Trust On First Use mechanism based on trust given to the server at the first connection, with its usual anxious message “The authenticity of host ‘…” can’t be established”. the public key of an SSH client can be replaced by a certificate signed by a CA that SSH servers trust. Thus, by providing this certificate and proving that it has the private key associated with its public key, the client can show the server that it has been authorized to connect, this client certificate mechanism also exists for HTTPS, it is for example one of the possible authentication means on the Vault API (described here ). In the case of SSH, this mechanism replaces the server-side requirement for a priori knowledge of the client’s public key through the authorized_keys file. The major difference between TLS and SSH certificates is the system of certification chain which is not existing in SSH. For more details on SSH certificates, this page describe its format. In summary, this format must contain a validity period for the certificate and principals for which it is valid (usernames in client certificate mode and hostnames in server certificate mode). In addition to these mandatory elements, there are optional fields that could allow you to restrict the use of the certificate ( critical options ) or on the contrary to increase their authorizations ( extensions ), these two types of additional fields are described at the end of the document in a section with the same name. The optional restrictions that are available are: force execution of a command, which makes it possible to provide a certificate allowing only the execution of a very specific command on the server concerned restriction of source IPs allowed to connect. For example, certificates can be restricted so that they can only be used from the bastion The additional rights that are available at the moment (and therefore disabled by default) are as follows: the authorization of the forwarding of the port of SSH agent of X11 (for remote visualization of graphic applications) the execution of the “~/.ssh/rc” file of the login account the PTY allocation (i. e. having an interactive shell) Add the “permit-pty“ extension will be needed to allow the user to use an interactive shell on the servers. These optional fields correspond roughly to the options available in the files authorized_keys (described here ), with some missing options such as the ability to fine-tune which ports can be redirected. Configuring an access with the Certificate Authority mode This time we will have to set up a CA type role . But before you can do that, you have to initialize this CA, i.e. create your SSH key pair. The public key can be retrieved and installed on the server and the private key will remain in Vault without any way to extract it. $ vault write ssh/config/ca generate_signing_key=true Once this command is executed, Vault will have a private signature key that it will never disclose and a public key that can be retrieved without authentication, for example with the following cURL command: $ curl 'https://vault.internal:8200/v1/ssh/public_key' Now that our CA is initialized, we can create our roles and start using them to sign our keys. A role of type CA can be created as follows: $ vault write ssh/roles/ca_web - <<'EOH'\r\n{\r\n    \"key_type\": \"ca\",\r\n    \"allow_user_certificates\": true,\r\n    \"default_user\": \"web\",\r\n    \"allowed_users\": \"web\",\r\n    \"ttl\": \"5m\",\r\n    \"max_ttl\": \"5m\"\r\n}\r\nEOH We just created a “ca_web” role that will allow people with access to it to sign their SSH public key ( \"key_type\": \"ca\" ) to get a client certificate ( \"allow_user_certificates\": true ) with a default lifetime of maximum 5 minutes ( \"ttl\": \"5m\" and \"max_ttl\": \"5m\" ), that will allow to connect using the “web” user only ( \"default_user\": \"web\" and \"allowed_users\": \"web\" ) to all machines that trust the CA of this SSH Secret Engine . Here we have limited the lifetime of the certificate and the authorized user (which will be indicated in the valid principals of the certificate). On the other hand, our role as it is, still gives the user complete freedom regarding the extensions he can request in his certificate. We can lock our role by adding additional parameters, first we add a constraint on the source IP (the IP of our bastion, 10.0.0.0.1): $ vault write ssh/roles/ca_web - <<'EOH'\r\n{\r\n    \"key_type\": \"ca\",\r\n    \"allow_user_certificates\": true,\r\n    \"default_user\": \"web\",\r\n    \"allowed_users\": \"web\",\r\n    \"ttl\": \"5m\",\r\n    \"max_ttl\": \"5m\", \"allowed_critical_options\": \" \", \"default_critical_options\": { \"source-address\": \"10.0.0.1\" } }\r\nEOH These additional settings tell Vault not to allow users to manage the critical options of their certificate ( \"allowed_critical_options\":\" \" ) and to systematically insert a critical options which must be connected from the bastion ( \"source-address\": \"10.0.0.1\" ). We will add parameters that allow the user to get a PTY, but nothing else, no port forwarding in particular: $ vault write ssh/roles/ca_web - <<'EOH'\r\n{\r\n    \"key_type\": \"ca\",\r\n    \"allow_user_certificates\": true,\r\n    \"default_user\": \"web\",\r\n    \"allowed_users\": \"web\",\r\n    \"ttl\": \"5m\",\r\n    \"max_ttl\": \"5m\",\r\n    \"allowed_critical_options\": \" \",\r\n    \"default_critical_options\": {\r\n        \"source-address\": \"10.0.0.1\"\r\n    } \"allowed_extensions\": \"permit-pty\", \"default_extensions\": { \"permit-pty\": \"\" } }\r\nEOH These additional parameters tell Vault not to allow users to ask for other extensions than permit-pty , the one which allow to allocate a PTY, ( \"allowed_extensions\": \"permit-pty\" ) and to insert this default extension permit-pty ( \"permit-pty\": \"\" ). Once this role is setuped, a path “ssh/sign/[name of the role]” become available via the API. This path allows you to have your public key signed by Vault, within the limits of what is possible with the configuration of the role , for example : $ vault write ssh/sign/ca_web public_key=@\"$HOME/.ssh/id_rsa.pub\" The answer will contain a “signed_key” field corresponding to the SSH certificate file which will be stored in an “id_rsa-signed.pub” file for the next examples. This certificate file can be inspected with the following ssh-keygen command: $ ssh-keygen -L -f id_rsa-signed.pub In this command, the -L option tells ssh-keygen that we want to inspect an SSH certificate and the -f option indicates the file to be inspected. Which will give us something like after execution on the console: id_rsa-signed.pub:\r\n        Type: ssh-rsa-cert-v01@openssh.com user certificate\r\n        Public key: RSA-CERT SHA256:7yCwFqI21OEANvCtlAiZNyDMsSv2lV1EEaSnbQF4BD0\r\n        Signing CA: RSA SHA256:HitN+Wc0hSKFBIQ0+Heczw4OFkBlrTLheuL7FVOCRn4\r\n        Key ID: \"vault-root-ef20b016a2...\"\r\n        Serial: 10734498250688175174\r\n        Valid: from 2018-12-12T22:12:06 to 2018-12-12T22:17:36\r\n        Principals:\r\n                web\r\n        Critical Options:\r\n                source-address 10.0.0.1\r\n        Extensions:\r\n                permit-pty This shows that all the default certificate settings are present: 5-minute lifetime authorized login user : “web” (only value in “Principals”) source address imposed on 10.0.0.1 authorized PTY allocation Server-side implementation All that remains now is to set up the server to verify that the certificate provided to it comes from Vault. On the server side, you simply need to reconfigure the OpenSSH service so that it trusts the public key of the Vault CA and, any other form of authentication, i. e. passwords and authorized keys , can also be disabled. If we consider that the public key of our CA Vault is in a file “/etc/ssh/user_ca.pub” it is enough to add the following line in the SSH server configuration (the file “/etc/ssh/sshd_config” most often) : TrustedUserCAKeys /etc/ssh/user_ca.pub You can then disable password authentication: PasswordAuthentication no\r\nChallengeResponseAuthentication no and also authorized keys : AuthorizedKeysFile /dev/null After a restart of the service, our server will be ready to accept any certificate issued by our CA. Connect with a certificate To use this certificate when connecting, simply add it to an “-i” parameter of our SSH client: $ ssh -i id_rsa-signed.pub -i ~/.ssh/id_rsa web@server.internal Or wth this SSH configuration in (~/.ssh/config) CertificateFile id_rsa-signed.pub First impressions This mode of operation also has advantages: it does not require any exchange between our server and Vault, only between the bastion and Vault. This reduces the attack surface on the latter it allows to control finely what the user can do or not do once connected thanks to critical options and extensions it uses on the client and on the server the already existing OpenSSH functionalities On the other hand, despite its elegance, this solution has some disadvantages. During the certificate’s validity period, it can be used on an arbitrary number of servers that trust the Vault SSH CA as many times as the user wishes. And these connections do not give rise to any log outside the logs of the system itself, which it is therefore important to keep and centralize outside the servers. To mitigate the very large number of machines that can be accessed with a certificate, we can do: Set up different CAs for different server sets. This is done by activating the SSH Secret Engine several times on different paths which will have different CA keys Set up separate user names for each machine or group of machines. Thus if a role only allows to connect with the user “web.webserver” which only exists on web servers, he will then only have access to web servers. Some policies to link user and role To set up the last step of our access control, all that remains is to associate each of the users of our Vault with the access rights they have within the infrastructure. As we have seen, a role allows you to define a set of rights on the infrastructure servers that the user can request from Vault in the form of a password or SSH certificate. Giving access rights on the infrastructure to a user consists in giving him access to particular roles in the SSH Secret Engine . This access control to roles is done by creating policies , which will be assigned to our users when they authenticate with the Vault API. The principle of a policy in Vault is to restrict the API resources that the user can access and the interactions he can have with these resources. For example, the “web_access” policy can be created by the following command: $ vault policy write web_access - <<'EOH'\r\n\r\npath \"ssh/sign/ca_web\" {\r\ncapabilities = [\"update\"]\r\n}\r\n\r\npath \"ssh/creds/otp_web\" {\r\ncapabilities = [\"update\"]\r\n}\r\nEOH Allows the user to use the role “otp_web” OTP type and the role “ca_web” CA type. But to access the other resources of the Vault API it will need additional policies . Take Away In this article, we have seen how to give Vault the responsibility of controlling SSH connections to our infrastructure. This access management mode allows: to have to do only an initial configuration of each server, reducing the maintenance load on the infrastructure code to control users and their rights over the infrastructure centrally and securely through the Vault API to take advantage of the possibilities offered by the Vault API in terms of interconnection with existing user repositories (the different Auth Methods ) and fine permission management All of this simply requires the installation of a binary and a reconfiguration of PAM in OTP mode or a reconfiguration of the OpenSSH server in CA mode. This avoids us to separate ourselves from the usual server access tools: SSH and its OpenSSH reference implementation. Reminder of the advantages and disadvantages of the two operating modes of the SSH Secret Engine : OTP mode CA mode Good points fine control of target servers truly single-use access user connections reported in the Vault audit log log Good points no access between servers and Vault control of actions for the user with certificate settings no additional software to install The worst need for network access between all servers and Vault risk of spoofing on password verification with Vault third party tool to install use of passwords, less easy to automate lack of control over possible actions once connected The worst difficult to fine-tune control of target servers lack of visibility on the use of the SSH certificate in the infra The main cost of this solution is the addition of a Vault server to the infrastructure and everything that this new component entails in terms of the need for security, high availability, audit and access control. This is a significant cost because your Vault server will become a prime target within your infrastructure, but it will at least have the advantage of using software designed for security. In addition, some OpenSSH limitations persist, such as the impossibility of limiting the lifetime of an active SSH connection. This means that you can, for example, remain connected with an expired SSH certificate as long as the initial authentication took place while it was valid. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure et opérations , Sécurité . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 3 commentaires sur “Keep your infrastructure keys safe with Vault” pablo 18/01/2019 à 14:00 Great article with a perfect in-depth explanation of all mechanisms. If you are interested to use the last method, with ssh certificates, there is \"signmykey\" an opensource ssh tool to automatically manage ssh certificates based on Vault: https://github.com/signmykeyio/signmykey Cristian Iaroi 12/08/2019 à 21:30 Can you please give me an example of how you would go about this particular point: \"difficult to fine-tune control of target servers\" ? In other words with the CA approach how can you create a mechanism of role/server or other to allow an ssh connection to a set of servers but not to others using the same key. Benoît Gastinne 07/10/2019 à 19:45 I would say that the two pieces of information given by an SSH client certificate that can easily be used to decide wether to authorize a connection or not are: the signing CA and the user.\n\nWith that in mind, you could represent a role with a specific mount of the SSH backend, this instance will have a specific CA signing key. You can then selectively authorize (or not) the key of each backend (i.e. each role) on the servers. For a limited number of roles this should be manageable.\n\nThe other way would be to create different \"roles\" in the same mount with each role allowing only a specific user. You can then authorize and give rights to each role on the servers through this user. This solution will require some automated unix user management on the servers but should provide more flexibility.\n\nFor both of these \"simple\" solutions, a change in authorization for a role will require reconfiguring affected servers to be effective, which is not ideal. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-12-27"},
{"website": "Octo", "title": "\n                Better understanding of Android accessibility for blind people            ", "author": ["Thierry Lee"], "link": "https://blog.octo.com/en/better-understanding-of-android-accessibility-for-blind-people/", "abstract": "Better understanding of Android accessibility for blind people Publication date 09/01/2019 by Thierry Lee Tweet Share 0 +1 LinkedIn 0 Making your app accessible is making sure that people with disabilities can use your app without any extra help from another person : you wouldn’t want to be forced to ask for help every time you use an app, do you ? Despite being an obscure topic, most articles I read about accessibility tend to follow a common pattern : they enumerate technical solutions focusing on “how” to use them. For each solution, a brief description of “what” problems people with disabilities may encounter is given… As someone who worked with them for 4 years, I am already aware of these problems, thus I quickly understand what those articles refer to. But that might not be the case for most readers, often developers. That is why in this article, I will pinpoint these problems, giving concrete examples about blindness issues. Hopefully, it will give you a better understanding of why accessibility is important, and how simple it is to solve some issues when you directly work with actual users, here blind people. TalkBack Before going further, I have to explain how blind people use their phone. They use a tool named TalkBack. It is a screen reader that greatly changes the way you use your phone. For those of you who already know about TalkBack, you can skip to the next paragraph. For the others, I’ll try to explain : it allows you to “explore” elements on the screen touching it with your finger. You will get an audio feedback on the visual element your finger is on and it will “select” this element. Once an element is “selected”, to actually “click” on it you will have to “double-click” anywhere on the screen. But that’s how WE (developers) use TalkBack, not how blind people actually do… Blind people navigate through elements in reading order. From left to right, then top to bottom They use a swipe left-to-right to “select” next element, right-to-left to “select” previous one. Images speak louder than words, so here is a video that explains it better (starts at 2m05s, you can stop at 3m25s) : https://youtu.be/YJSWYLZD8EI?t=125 Now that you know how TalkBack works, let’s get started ! An example that could be you! Bob is a developer who made an app that is supposedly so simple that his grandmother Alice could use it… but never actually tried. It is an app listing puns, with quick actions on each line : like, comment and share. There is also a floating button to submit your own pun. It looks like this : I must admit, Bob is terrible at finding puns… There is also a detailed screen for each pun to give explanations in case you didn’t understand it. This detailed screen also includes the same actions as listed earlier : like, comment and share. Before publishing his app, he decides to let Alice test it, just in case… but Bob learns that Alice became blind since the last time he saw her. He decides to proceed anyway, that would make an even better test ! Content description After few minutes of testing, Alice calls Bob. Alice : Hello Bob, I really appreciate that pun about “Line 2”, I would like to send it to one of my friends. How can I do that ? Bob : It’s simple, just hit the “Share” button. You know, the one with three dots that forms a “less than” symbol… Alice : They all “look” the same to me *giggles* . Is it the first, second or third button after the text ? Bob : Uh… right sorry. It’s the third one Alice : Alright, thank you. You should add a description to your buttons ! Indeed, from Alice’s point of non- view, all three buttons look the same, as if you had this screen : TalkBack can’t read images, and neither can you ! Therefore, it will describe the app’s quick actions as “button”. But it can read its textual description … if there is one, which is usually referenced as : contentDescription . As for how to set them up, I’ll let Google explain : contentDescription’s documentation . Making navigation smooth Minutes later, Alice strikes again. Alice : Your app is great, but if I just want to read a lot of puns, I have to  “scroll” a lot. Any way to avoid that ? Bob : What do you mean ? Alice : Ideally, it would be great if I could skip the “Like”, “Comment” and “Share” buttons on the list, because I can do it when I select * a pun anyway Bob : Uh ok, I’ll see what I can do.. * Meaning, accessing the pun’s details screen To better understand what Alice means, let’s see : On the left, what the screen looks like from Alice’s perspective On the right, how TalkBack outlines the screen Each framed element can be selected using TalkBack. The number next to it is the number of swipes (referred as “scrolls” by Alice) to do in order to reach and select outlined element. So in this example, to access “Line 3”, Alice has to swipe 10 times. For “Line 10”, it would be 38 !! Bob does not want to hide these quick actions : it’s a key feature of his app. However, they can be “hidden” from TalkBack . These actions can be triggered in the detailed pun’s screen anyway. With this little tweak, TalkBack navigation on this screen would look like this : Thus, this makes “Line 3” reachable with only 4 swipes instead of 10. For “Line 10”, it is 11 swipes instead of 38. To make views “hidden” from TalkBack, you have to declare them as “not important for accessibility” using an Android property called importantForAccessibility : documentation here . Reading order problems Now that Alice can read a ton a puns with ease, she has ideas she wants to submit but can’t find any way to do so on the app. So she calls Bob again… Alice : Hi again, there are a lot of puns on your app, did you find everything by yourself ? Bob : No, of course not, users can submit their own ! You can use the floating button on the bottom right of the screen. Alice : Oh, is it after the list ? Bob : Yes. Alice : Now I understand why I couldn’t find it… Let’s take a look from TalkBack’s point of view to see what is off : Problems are : You have to “scroll” through all elements of the list in order to reach elements below it The list can have a huge (or worse, infinite) amount of elements It means that Alice will have to do a nearly infinite number of swipes to reach the “submit” button… Other blind people would probably never know that the “submit” feature even exists ! Changing your UI to resolve this issue would be very ugly. A better way to do so is to change TalkBack’s reading order. Let’s say we want our “submit” button to be reached before the list of puns, TalkBack’s navigation order would look like this : Once again, this can simply be done using a single Android property : accessibilityTraversalAfter or accessibilityTraversalBefore . In that case, all you have to do is declare that the “submit” button must be “traversed” before the list. As usual, the link for more details : documentation here . Conclusion Making an application hoping that it would be easy to use, even by blind people, will likely not happen. In this example, they will only be able to use about 20% of Bob’s app : read puns laboriously, … and that’s all. Yet, as you can see, it takes little to no effort to make it to a 100% !… there is still some room for improvement though. However, it is difficult to identify what will not work… One good way to do so is putting yourself in the user’s shoes, to find “what” problems they might bump into. Better : you can ask an actual user for feedbacks. Here I have only listed 3 of them, that’s far from exhaustive ! I hope this article will inspire you to make your apps more accessible ! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-01-09"},
{"website": "Octo", "title": "\n                Cache me if you can – 1            ", "author": ["Léo Jacquemin"], "link": "https://blog.octo.com/en/cache-me-if-you-can-1/", "abstract": "Cache me if you can – 1 Publication date 17/12/2018 by Léo Jacquemin Tweet Share 0 +1 LinkedIn 0 Introduction – scope of the article This series of articles deals with caching in the context of HTTP. When properly done, caching can increase the performance of your application by an order of magnitude. On the contrary, when overlooked or completely ignored, it can lead to some really unwanted side effects caused by misbehaving proxy servers that, in the absence of clear caching instructions, decide to cache anyway and serve stale resources. Before we get to the tactical details of how caching works, it helps to understand the context and the landscape of the problem we are up against. For this reason, the first part of this series covers where caching should happen and why we need it. Then, once we’re all convinced of the importance of HTTP caching, we’ll deep dive into How it works . For this, we’ll have to take a closer look at the Cache-Control , Etag , If-Modified-Since , If-None-Match , Last-Modified , and Expires headers, as well as the infamous Vary header. Furthermore, we will also put each web resource to the test, to understand which policy to apply, and when. After reading this series, you should have gained some theoretical and practical insights that will allow you to start caching any web resources you want and leverage the power of all the proxy servers, friendly or hostile, that are widely deployed on the Internet. Without further ado, let us start with an overview of key considerations to keep in mind when dealing with HTTP caching, and to a lesser extent, with web performance in general. Caching everywhere Browsers Caching is a very popular technique. The idea is indeed pretty appealing: no matter how long the I/O request, how CPU-intensive the computation, or any other programming task, it is always the same: storing the result somewhere and retrieving it as it is, for its further application. Taking the example of browser’s HTTP cache that all browsers implement, web resources are stored on the user’s filesystem. Hence further requests that will access these same resources will have them delivered instantly. No network request, no client/server round-trips, no database access, and so on. Can you think of any performance enhancement that would yield better results than no latency at all and complete server offloading? That’s simply not possible. One might think that this situation is too ideal and impractical. If it were true, how come most pages don’t load that fast? One reason for this is because, even though all web resources are cacheable, they should not be cached the same way. HTML files for example, which are the first to be downloaded and that contain references to other assets, are notoriously dangerous to cache, so they’re unlikely to find their way to browsers caches except for a few minutes at most, as we shall see in a moment. But another possible explanation, one that we find more likely based on our experience, is that caching policies are often completely left out for web servers to decide. Setting a flag on a web server’s configuration file to automatically activate ETag generation and Last-Modified headers is not time-consuming and can give decent results. For some time at least. Until one realizes that the feature doesn’t work as expected or even worse, that users are starting to be served stale content due to unknown reasons. Besides, these web servers won’t do much good in terms of caching your API. Most of them generate cache headers based on files metadata, which can have subtle consequences . But with API requests, there is no file to read metadata from. Hence when they see a resource that is dynamically generated, all they can do is watch and forward the request. Granted, as efficient as it can be, caching in the browser is not as easy it as looks. Major browsers implement different layers of cache, of which the HTTP one we’re talking about is only a piece. And in case you’re wondering, they can interact in some ways that are not always as predictable as we would like them to be. Besides, caching in the browser is notoriously dangerous, because developers lack the ability to invalidate resources at will. Doing so would involve allowing a web server to push information to every client that interacted with it, without clients initiating a connection, which is not possible in a client/server architecture. Furthermore, from the cacher’s point of view, as powerful as it is, the browser has other flaws. It uses some doubtful heuristics when no caching instructions are explicitly present, all the more reason for us to help him know exactly what to do. But even if we were to do it, users have the ability to flush their cache anyway or disable it. Not the almighty caching tool that one could ask for Christmas, after all. So let us take the browser out of the equation for now and ask ourselves: without it, is HTTP caching still relevant? Is it implemented at other places? As it turns out, the browser is just one piece of the caching chain. And if browsers were all of a sudden to stop caching everything, rest assured: CDNs got us covered. CDN Content Delivery Networks (CDN) are the unified champions of the HTTP caching world. Most of them have installed tons of servers – Akamai has ~ 240 000 –  all geographically around the globe in order to serve our content closely to our end users. These companies have accumulated decades of experience on web performance. Most of the people who write the specs or the software that power the specs usually work in these companies, which is a bit of an indication that they know what they’re doing. Let us take a quick tour of why they are so important and how they work. First and foremost, is it crucial to understand that all of these servers are HTTP servers. In HTTP terminology, they are proxy servers which means that they speak HTTP. They do not encapsulate our requests into another shady proprietary application protocol. They just use these proxies, most of which are even free or open source! As a direct consequence, any knowledge of HTTP caching is immediately actionable to leverage the infrastructure they put at our disposal. In addition to billions of browsers, we now have thousands of servers strategically placed by specialized companies waiting for us to instruct them how to cache our content for maximum efficiency. Furthermore, depending on what your priorities are, that’s not even the best feature. Most modern CDNs advertise the ability to programmatically purge resources out of the CDN’s network instantly. Let us say this again: programmatically and instantly . As far as HTTP caching is concerned, the two hard problems in computer sciences might just have been reduced down to one! Caching anything and invalidating instantly whenever we want. From a developer keen on web performance point of view, it can hardly get any better. A word of caution: CDNs are not to be blindly trusted based on that. We’ve already experienced some slight differences between what was marketed on the brochure, and what we had in production, where other cache busting techniques had to be used to ensure the cache was actually cleared. Before starting to cache all of your resources forever carelessly, experimenting on a small sample first might be a good idea. However, if it’s not here today, it will eventually land consistently everywhere, making our lives much easier. Another aspect to keep in mind is that it is in every CDN’s best interest that developers see them as effective and powerful tools. As a consequence, they’ll often do their best to comply to the specification. Also, they all provide some web interface that makes it a breeze to give caching rules that will either override or play nice with upstreaming caching directives coming from origin servers. The ability to configure your caching policy outside of your codebase happens to have two interesting consequences. First, it means that people other than developers can have control on this which can be seen as a strength or a weakness based on your perspective. The ability to have someone other than a software developer fine-grain caching settings at the CDN’s level on critical occasions might come in handy in some situations. But perhaps even more importantly, it means that the performance part of your application, or at least a large part of it, can be completely factored out at the infrastructure level. All developers that have experienced performance problems at some point know this: it is rarely something you can mutualize in a single file called performance and is often best planned ahead , just like detailed application-level logging. But that’s not necessarily the case with caching. Provided your caching headers at smartly set and your CDN well configured, you could write poorly optimized server code (although, please don’t) and still have the vast majority of your users be served content in less than 300 milliseconds, by reusing cached versions that are still perfectly fresh. On the flip side, as one might expect, setting up and maintaining such a network of servers is both expensive and complex. As a result, although some of them have free plans that already allow for some serious performance boost in rather wide geographic areas, they remain paid solutions. If your intention is to cache millions of resources, be prepared to pay several thousand of dollars. This is where private proxy caches come into play. Private proxy The third and last player of the http caching game is simply the same softwares many of the CDNs we just talked about are made of.  Do the names Varnish , Squid , Traffic Server , or even Nginx ring a bell? Well, they certainly should! Given what we just said about the unmatched performance of web browser caches, and the case we just argued in favor of CDNs, one might legitimately asks: why bother setting up these in front of my origin servers when CDNs can do much more, and browsers are closer to my end users? Well, this third and last solution in the HTTP caching landscape also comes with its fair share of advantages. As a matter of fact, we’ll argue that this should often be the first solution to look for. Let us examine a few bonus points of the most popular solutions. First, these solution are free and open source, which can be seen as a double edge sword. How many of such software that were once praised by the community suddenly stopped being maintained by their core committers due to a  lack of interest, sponsoring, or both? The fear of seeing a project’s main dependency (web framework, ui library…) going to the softwares graveyard is a real concern. Although when assessing this risk, one must always consider the maturity of the technology, how long it’s been around, which big company is using or supporting it – they usually do both – and how effective it is at solving a particular problem. Lucky for us, the proxies we’re talking about score pretty high on all levels. Another aspect simply comes from the performance gain. As mentioned previously, these softwares are what CDNs are made of. This has two consequences. First, it massively decreases the chance of termination of their usage, because CDNs whole infrastructure relies on it. This kind of stability is greater than when a company is just using a library as part of a larger system. In this case, the software is the system. Second, any hard gained knowledge about their installation, configuration and maintenance will directly be transferable the day you decide to switch or complement your caching infrastructure with a CDN, since they are the same servers! In the software development world, where everything changes so fast, this is always good news. It’s the same reason why learning HTTP caching is a good bet, because it’s relevant in many different places. And will likely stay that way for decades, we shall see why in the end of this article. Browsers, edge servers, proxy servers… that’s a lot of caching intermediaries. Thinking about all these caches at the same time can be a little overwhelming and hard to picture. Luckily for us as we mentioned previously, all these caches speak HTTP and comply to the same specification. As proxy servers, they all act transparently both for clients and for servers. Origin servers communicate with the proxy as if it were the client, and end users browsers communicate with the proxy as if it were the server. This holds true even between proxy servers. As such, we can model the reality by considering that all caching infrastructures are equivalent to one with a single caching proxy in place. This is best described by the following picture: We’ll use this simplification in the rest of this series of articles. This abstraction is helpful to visualize the mechanisms at play, but it comes with certain limitations. Putting two proxies one after the other can have subtle consequences . So far, we have covered a lot of ground without giving away anything about the detailed interactions between a client and a caching server. HTTP caching is a complex subject. Before moving on to the real technicalities in part 2 , there is one last important aspect that needs to be explored. Caching for the future Have you ever had to wait a few seconds to interact with a web page, or to see anything meaningful on the screen? Probably. That’s actually not unlikely at all . Everyone has experienced the slow internet at some point in his life, even at home with a fiber connectivity. Past UX research actually has got some scary metrics about this. Metrics that have been the same for more than 40 years, by the way, making them unlikely to change anytime soon. Their guidelines are expressed in terms of hundreds of milliseconds , whereas we are used to browse the Internet and wait several seconds. So why do we believe that HTTP caching is relevant today and will almost certainly remain so for the years to come? It all comes down to this basic question: w hy is the web so slow? This question is undoubtedly a difficult one, and a rigorous examination would take us far too deep and lose our primary focus, which is HTTP caching. However, if we don’t have any idea on what makes a web page slow, how can we be sure that caching, despite all of its virtues, is the right tool for the job? From its conception, the web has certainly changed a lot. The days where web pages consisted of simple HTML files containing mostly text and hyperlinks are long gone. These days, many websites are labeled as web applications, as their look and feel resembles the one of desktop apps. But despite all the improvements and innovations that have been made over the years, one thing has always remained the same: it’s always begun with the downloading of an HTML file. As it gradually downloads the HTML, the browser discovers all the other resources that combined will result in all the client’s side code that gets parsed and ultimately, executed. In case of a typical SPA for instance, the flow of requests goes on. Upon execution, the application starts downloading data from the server, typically serialized as JSON these days, in order to render the UI. Each URL inside the JSON payload will, once referenced into the code (it doesn’t even have to be added to the DOM), triggers another download so that it can be displayed on the screen. Sequence diagram of a typical SPA loading This model of execution, where all the bytes needed for the application to do its job are scattered among different places and must be downloaded every time is quite remarkable. Arguably, it’s what makes the web so unique in the software development’s world. But there is a catch. Indeed as of today, the typical web application requires 75 requests and weighs 1,5 Mb,  meaning that browsers must initiate a lot of requests of ~20kB each. To put it another way, it means that a typical web application is made of lots of short-lived connections. And here is the catch: this is the exact opposite of what TCP is optimized for. The anatomy of a web request In theory, all of these 75 requests should go through these steps: DNS resolution TCP handshake SSL handshake Data downloading constrained by TCP flow and congestion control Let us walk through each of them and draw a counter-intuitive consequence from it. DNS resolution is the process of converting a human readable hostname such as example.com into an IP address. Although the DNS protocol is based on UDP instead of TCP, the journey to getting a hostname IP address can be really long , involving multiple DNS servers. And it’s not uncommon that these DNS resolutions take between 50 to 250 milliseconds. Then, each request must initiate a TCP connection. HTTP has always needed a reliable transport protocol to work. If the ASCII bytes representing every HTTP request were to be delivered out of order, a status line such as “ GET /home.html HTTP/1.1” would become “ GTE /mohe HTTP/1.1” and the request wouldn’t make much sense. In order to guarantee delivery order, TCP marks each byte of applications data with a unique identifier called a sequence number (SYN). The problem is that this number must be chosen randomly for security reasons . Therefore, if a client is asking for a resource, it cannot do so without signaling to the server its initial sequence number (ISN). Then it must wait before the server acknowledges good reception of this segment, before being able to send application data. Well, unless the request is secure, which it probably is since 80% of HTTP requests these days are actually secure HTTPS requests. These are normal HTTP requests, except that they are encrypted in order to guarantee (at least, up to this day) confidentiality, integrity, and authenticity. To accomplish that, the client and servers must now agree on a TLS protocol version, select cryptographic functions, authenticate each other by exchanging and validating x509 certificates… for no less than ~10 protocol messages that can be packed into a minimum of 2 TCP exchanges, and as many round trips. Once the connection is setup and secure, then TCP can finally start sending segments carrying our application data, such as our HTTP request. Unfortunately for us, TCP prevents us from sending all our data at once in one batch of multiple segments. This restriction is a necessary evil, so that we don’t accidentally cause a buffer overflow on the receiver. When an application that initiated a connection asks the underlying TCP socket for data, TCP will refuse to give any chunk that would be incomplete or made of unordered smaller chunks. Hence, the need for a buffer. The way it works is that TCP sends N segments in the first batch, and, if all segments were received by the server, will send twice as many segments (2N) in the next batch, and so on, leading to an exponential growth. This mechanism is commonly known as the slow-start algorithm and is one of the two only possible mode in which TCP operates, along with congestion avoidance. With all these steps in mind, let us make a simple calculation to realize something important. At the beginnings of the web, the N parameter (known as the congestion window in TCP’s terms) was equal to 1. With such a window and an average resource of 20kB, we can determine how many round trips are necessary for an average request to be fully transmitted. Indeed, under normal circumstances, the maximum segment size ( MSS ) is 1460 bytes. That’s equivalent to 20 000 / 1460 = 14 TCP segments. When dispatched according to the exponential scheme we just described, this is equivalent to 4 round-trips to the server. 75 requests that all require 8 round-trips each result in a total of 600 rounds trips to the server. A typical RTT between Europe and the US is 50ms, which gives us the amount of time that information spend flowing on the Internet when we request a typical web application: 30 seconds. And it could get worse. The Amazon homepage for instance, a typical MPA , currently weighs 6.3Mb and requires 339 requests. That would translate into a salient page loading time of 2 minutes and 15 seconds. As an exercise, try do the same for the Facebook Messenger homepage, a typical SPA . How to interpret this number? This would be the actual page load if every single resource had to be downloaded sequentially, TCP initial congestion was down to its minimum value of 1, and if DNS requests, TCP and SSL handshakes had to be done all over again every time. The web would be a much different place for sure. Fortunately, many improvements have been made over the years. DNS resolutions are cached at different places, TLS handshakes results are reused. TCP connections were allowed to be persisted between multiple requests, avoiding the cost of both connection setup and slow-start on each request. TCP’s initial congestion windows was lifted up twice, from 1 to 4 and more recently, to 10 . Browsers started to open up parallel connections (6) as well as some really advanced strategies to accelerate page load times. Some proposals tried to break free from the connection setup, although none of them are widely implemented. Some CDN even acquired patented algorithms to tune some of TCP aspects such as congestion avoidance, always for the same reason: speed up delivery. What consequence can we draw from all these round-trips between browser and origin servers? That bandwidth stopped being the bottleneck many years ago . This is somewhat counter-intuitive to most of us, because bandwidth has embodied the browsing speed for years. After all, it has exactly the dimension of a speed, bits by unit of time, and it is the only thing ISPs advertise when trying to lure us into becoming their customers. Besides, browsing on 3G is clearly slower than on 4G. But that is because the threshold at which bandwidth stops being the bottleneck is 5 Mb/s, and 3G maxes out at 2 Mb/s in ideal conditions! And if wireless technologies such as Wi-Fi or 5G are indeed slower than their wired counterpart of some sort, it is also because in wireless systems, packet drops caused by interferences are commonplace thereby making latency much higher and volatile. The latest version of the HTTP protocol, HTTP/2, codenamed H2, was a continuation of the SPDY protocol which itself was initially designed following this very observation: bandwidth doesn’t matter much anymore. Latency does. But latency is fundamentally a function of two things: the speed of light in optic fiber, and the distance between clients and servers. Active research to increase the speed of light in optic fiber has already been conducted, but it only got us so far. In most deployments, light already travels at 60% of its maximum theoretical limit. But even if we were to reach 99%, that would only have a significant impact on your website if it’s already loading in a few seconds at most. If it’s loading in more than 5 seconds, even though the performance increase would be noticeable, it would still feel slow. Therefore we are left with one obvious choice: reducing the distance. And the only way to accomplish that is by leveraging browsers and content delivery networks with HTTP caching. Conclusion Along this article, we have argued that HTTP caching is one if not the most effective way to improve the performance of your application. And as many studies keep pointing out, page load time is an important subject that can be directly translated into user satisfaction and, ultimately, profitability. We have seen that caching can happen pretty much everywhere, from the browser, to CDN, to private proxy servers sitting just in front of your origin servers. But also that, unlike many performance decisions, it can be completely externalized outside the main codebase, which is both valuable and convenient. Finally, we did a little analysis of the anatomy of a modern web application, to understand why latency is the new bottleneck, making caching relevant today and for years to come, even with the steady deployment of H2 . In the next article of this series, we will deep dive into the How. In a way, this first part was merely a warm-up! We’ll learn how all of this actually works: resource freshness, revalidation, representations, cache-control headers… and much more! Stay tuned! To go further: Ilya Grigorik High Performance Browser Applications (a must read): https://hpbn.co/ Mike Belshe paper that served as a basis for the SPDY protocol: https://docs.google.com/a/chromium.org/viewer?a=v&pid=sites&srcid=Y2hyb21pdW0ub3JnfGRldnxneDoxMzcyOWI1N2I4YzI3NzE2 Active CDN blogs with tons of great articles: https://www.fastly.com/blog https://blogs.akamai.com/web-performance/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Cache me if you can – 1” David Okwii 29/08/2019 à 15:25 Wow, this is in-depth take on HTTP caching. Very well delivered. Thanks a lot. Léo Jacquemin 13/09/2019 à 16:31 Thank you David for your kind feedback. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-12-17"},
{"website": "Octo", "title": "\n                Industrial document classification with Deep Learning            ", "author": ["Alaa Bakhti", "Aurélien Gervasi", "Fabien Arcellier"], "link": "https://blog.octo.com/en/industrial-document-classification-with-deep-learning/", "abstract": "Industrial document classification with Deep Learning Publication date 07/01/2019 by Alaa Bakhti , Aurélien Gervasi , Fabien Arcellier Tweet Share 0 +1 LinkedIn 0 Knowledge is a goldmine for companies. It comes in different shapes and forms: mainly documents (presentation slides and documentation) that allow businesses to share information with their customers and staff. The way companies harness this knowledge is central to their ability to develop their business successfully. One of the common ways to ease the access to this document base is to use search engines based on textual data. At OCTO, we have decided to use optical character recognition ( OCR ) solutions to extract this data, since they have offered a strategy with a good coverage compatible with any PDF, even the scanned ones. The problem with this approach is that the extracted text usually has a poor quality with spelling mistakes and non preserved formatting. This bad quality limits the level of the search engine results: this level was acceptable when the number of documents was lower, but is no more compatible with the increased number of documents we experience today. As a result, OCR based search engines no longer allow companies to exploit their knowledge base in a coherent and productive way. We believe that those limits can be overcome when we take advantage of the documents’ visual features along their textual data since nowadays documents are usually visually augmented with images and charts to illustrate ideas. In this article, our work focuses on OCTO’s knowledge base with the classification of its presentation slides: a type of documents designed for visual presentations. Dataset OCTO’s knowledge base gathers more than 1,5 million slides. It is daily fed with new documents that consultants create to illustrate ideas for our clients. To prepare the dataset, we started by interviewing some of our consultants to identify classes with high value to the company . Each class corresponds to a distinct typology of slide that has a real usefulness to our core business . We considered that each slide can be associated with only one class. After that, we started labeling the dataset documents. However, because the manual labeling can be very tedious, we developed a web application to speed up the process. But one problem remained: how to recognize the slides that do not belong to any of the identified classes? One idea we had was to group the remaining classes in a new class « Other » . The final dataset is composed of 6 different classes with 500 samples each (3000 slides in total). For each class, we used 400 samples to train our model, 50 samples for validation and the remaining 50 samples to test its accuracy. What model should we use ? In Deep Learning, there are many types of networks to choose from: multi-layer perceptron (MLP), convolutional neural network (CNN) , recurrent neural network (RNN) , etc. Many of these networks are flexible enough to be used with different types of data (text, image, etc.). However, each of them has some characteristics that helps it get the best accuracy when used with the appropriate type of data in the appropriate context. In our case, we chose to use a CNN model because it is highly suited for image classification tasks. In fact, this type of network uses some interesting operations like Convolutions to detect low (edges, textures, etc) and high level (objects, etc) patterns in images. Let’s stimulate some neurons To evaluate our model, we used the accuracy metric along with the confusion matrix (this metric gave us a more detailed information about the model classification performance on each class). We started by training a simple CNN architecture ( MobileNet v1_1.0_224 ) but we quickly realized that we were limited by the size of our training data. After all, 3000 samples were not enough for the model to learn to correctly classify our dataset. In fact, our model had more than 4 million parameters that needed to be optimized. Yet, since they were randomly initialized, it requires a huge amount of images (tens of thousands) to find the proper weights that learn useful and sufficiently abstract features. To increase the size of our dataset, one of the possible techniques is to use data augmentation (cropping, zooming or flipping images). However, because our documents are mainly presentation slides, they always have the same form. So this technique will not generate new valid samples and will only make it difficult for the model to learn. At this stage, since we can not increase the size of our dataset, we choose to simplify the training of our model by transferring knowledge from another model (base model) that has already been trained on a related domain : image classification. This technique is called Transfer Learning . «Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned .» – Transfer Learning, Lisa Torrey and Jude Shavlik, 2009 With this technique our model can benefit from the knowledge of the base model by starting training from its already learned features and using our dataset to fine-tune them for our new classification goal. «In transfer learning, we first train a base network on a base dataset and task, and then we repurpose the learned features, or transfer them, to a second target network to be trained on a target dataset and task. This process will tend to work if the features are general, meaning suitable to both base and target tasks, instead of specific to the base task.» – How transferable are features in deep neural networks?, Jason Yosinski et al, 2014 We choose to transfer knowledge from a pre-trained MobileNet network: a CNN model trained on ImageNet dataset (1,2 million images) . We added to this base model a simple model (top model) consisting of 2 dense layers. After training our top model, we reached an accuracy of 98%. In fact, our model correctly classifies all the Project reference card , Mini biography and Architecture diagram documents but still makes some errors when it comes to Customer reference card , Chapter and Other documents. For example, it confuses some of the Customer reference card slides with Mini biography , Project reference card and Other slides. Why Transfer Learning significantly improved our model accuracy? We were curious about the impact of Transfer Learning on our model accuracy. So we investigated its effect on the high level features of the base and top model. We started by computing the representation of our dataset images using MobileNet and our top model respectively. Then we created a 2D representation of their bottleneck features (the last activation maps before the fully-connected layers) by reducing their dimension from 1024 to 2 using T-SNE algorithm. Finally, we plotted the slides in the 2D space. MobileNet groups slides based on their visual similarity and not their belonging to a class or another. Our top model identifies the visual discriminators that separate one class from another. Simply put, the top model specializes in our dataset by separating documents that are visually similar if they do not belong to the same class and gathers documents that belong to the same class even if they are visually different . For example, before fine-tuning Chapter and Content classes were grouped together ( Content belongs to the class Other ) and after fine-tuning they were seperated. Do we have the perfect model? Even with a limited amount of images, we managed to make a model classifying nearly all documents correctly (98 % accuracy on the testing set). Nevertheless, our model still have some limits. Model limits Confusion between classes (visual features of a class incorporated into another) When a document belonging to class A includes some illustrations that contain visual features of class B, the model will struggle to identify its class. Indeed, if we had enough samples in the training dataset illustrating this situation, the model would not fall into this trap. Different classes with the same visual features Our model relies only on visual features of documents to classify them. Therefore, if we have some classes with the same visual features, our model can not determine the visual discriminators that separates them for the simple reason that they do not exist. Poor visual features When a class does not have any specific visual characteristics or have fewer of them, our model can not learn to classify it. For example, if we train it to classify semantic classes ( Blockchain, IoT , etc), it will not succeed because this type of classes relies mostly on textual features and not visual ones. Not all visual features are covered in the training dataset When some of the visual features of a class are not present / covered in the training set. In this example, the 2 slides belong to the Mini biography class. The model correctly classifies the 1st but not the 2nd because it belongs to the new template which is not sufficiently covered in the training set. Possible improvements We believe that these limits can be overcome when we expand the model feature resources by taking advantage of the textual data of documents along with their visual properties. In fact, each class has textual features in the form of keywords. For example, the class Customer reference card always includes the keywords: context, demand, request, approach, etc.. Moreover, our improved solution can be composed of 3 models: CNN model for visual features. NLP model for textual properties. Meta-model that uses the CNN or NLP model to classify the documents. As a result, for each class, the meta-model will learn to privilege the appropriate features whether they are visual, textual or the combination of both of them . For instance, if we have a class « Blockchain » , the proposed model may give 80% importance to the textual properties and only 20% for visual features. To wrap up We managed to create a deep learning model that classifies industrial documents (slides) perfectly for classes with a strong visual identity. However, for this handcrafted successful proof of concept to not just be confined to a research lab, it should be deployed into production. Nevertheless, this step needs some reflection because the industrialization of such AI powered systems comes with a number of hurdles if we don’t follow the good practices of implementation and integration . If you want to learn more about the industrialization of Artificial Intelligence products, check out this presentation of our Big Data Analytics team where they share their knowledge about the organisational, methodological and technical patterns to adopt for the creation of practical and sustainable AI products. To go further CNN: Andrej Karpathy blog Classification d’images: les réseaux de neurones convolutifs en toute simplicité Transfer Learning: Andrej Karpathy blog A Gentle Introduction to Transfer Learning for Deep Learning – Jason Brownlee Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Big Data , Data Science and tagged bigdata , convolutional neural network , Deep Learning , document classification , knowledge management , machine learning , transfer learning . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Industrial document classification with Deep Learning” Mohammed Furqan Rahamath 02/06/2020 à 09:08 Thank you for the article. It was an interesting read. \r\n\r\nI am a graduate student in Computer Science at Northeastern University, Boston, US. I am currently working on a project which requires presentation slides classification in a professional setting and this article is a big help in getting an initial direction. \r\n\r\nHave you guys made the code public? If so, please share the details. I would love to fork the repo and work on top of the existing work. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2019-01-07"},
{"website": "Octo", "title": "\n                I am a Developer: why should I use Docker ?            ", "author": ["Aryana Peze", "Quentin Cattez"], "link": "https://blog.octo.com/en/i-am-a-developer-why-should-i-use-docker/", "abstract": "I am a Developer: why should I use Docker ? Publication date 07/11/2018 by Aryana Peze , Quentin Cattez Tweet Share 0 +1 LinkedIn 0 Lire l’article en français I have noticed that containers are increasingly popular, and that the IT community nowadays is particularly enthusiastic about Docker. Colleagues keep trying to convince me that containers will make my daily tasks quicker and simpler. But how? First of all, what are the differences between a Virtual Machine (VM) and a container? I now work on Docker. This is the tool that democratized containerization and the most popular one. Many open source tools are based on it, such as Molecule (used to test Ansible code), or Kubernetes (used for container orchestration). There are two main concepts to grasp to understand how Docker works: images and containers. An image is a snapshot of your application, installed with all its dependencies and ready to run. This image is then used to build containers, “boxes” in which the code runs. The container can be used and then easily destroyed. You can try comparing it to object oriented programming: an image would be a Class and the container would be an Instance of this Class. On the whole, the container principles are quite simple: one process per container a container is immutable a container is disposable An iso-production dev environment “A bug? It’s not my fault, it works on my computer!” Whether Developer or part of the Operations staff, everyone has heard the infamous saying “it works on my computer”. The problem is generally due to differences in the configuration files (problems concerning versions, rights…) which are hard to debug. The first step on the way to environment standardization has been achieved through the use of virtual machines. Unfortunately, the configurations and updates need to be repeated on each new machine or environment. If my applications are stateless, configured through environment variables, and respect the 12factor standards, my containers will all be the same and have the same environment variables. This means that if my tests are validated for one container, it will be the case for all the containers that were built from that same image. Indeed, using containers does not exempt you from writing tests! Therefore, the new tested image will be used in production, and I can be sure that it will behave the way I want it to. Similarly, if I encounter a bug in production, I simply need to build a container based on that image with the production configuration to be able to reproduce it. Note that even if you do not use containers in production, you can still use containers to simplify the application’s development. Nevertheless, you must keep in mind that it you wish to start learning how to use Docker, it will take some time. It is for you to decide if your priority is to work on a simpler architecture or to work on an environment that matches the CI/production as early as possible. A simplification of some of my daily tasks “It makes onboarding so much quicker!” Today is my first day on the project. Fortunately, my team uses Docker! I only need to install Docker, start a container based on the latest version of the code, and I am ready to go! I am now sure that I am working with the same tools and the same versions of these tools as the rest of the team, whatever OS I use. This also greatly simplifies upgrades, as everyone is sure that the whole team is up to date. “I do not wish to pollute my computer with all the tools I download for my various projects.” As a developer, I get to work on various projects, and therefore accumulate different tools. I now face a problem: I have many versions of the same tool installed on my computer. For instance, I have two versions of PostgreSQL. When I switch projects, I need to deactivate one version and activate the other. I lose precious time doing that, and I am never sure I have uninstalled everything correctly if I decide to delete one of them. Using a containerized database solves my problem: it is easy to install and to use, and I am certain I have correctly deleted everything when I destroy my container. Borrowing your neighbour’s tools Let’s take an example: my computer is a Mac, but I would like to use a very useful Linux tool: ipcalc. Before learning about containers, I would have resigned myself to finding a similar Mac tool, but now, I can use ipcalc directly without any trouble. “I do not want to reinvent the wheel” One of the advantages of containers is that you have off-the-shelf solutions. It is indeed unnecessary to code a new solution from scratch to solve a problem that has already been addressed. There are many places – called “registries” – where you can find images from which you can build containers. As I use Docker, I went onto https://hub.docker.com/ to discover all the existing images and how to use them. Most images are open source, so anyone can  contribute to improve them. Some indicators can help quickly select the best image you need: the number of stars (given by the users), the image’s download count, the date of the last updates, and when it was last downloaded. Regardless of these indicators, make sure to always check what is inside the image as it could contain malware ( backdoored docker images ). But what if you do not want to publish your images publicly on docker? No problem! You can create your own registry based on a Docker image (for example: https://docs.docker.com/registry/ ) and then deploy your images there. “POCs don’t frighten me anymore!” I can work on my projects without fearing to download useless packages when I need to compile a file, or to download a tool that conflicts with those I already have on my computer. I just need to use a container and destroy it afterwards and my computer will be as good as new. I am also more prone to do POCs or to test new tools, as I can install them in a container that I can delete as soon as I am done with it. It is very quick – creating or destroying a container takes only a few seconds – and it enables me to simulate the exact environment on which I wish to work. I can also easily simulate calls to a database or to an API. If it does not work, it takes me only a few seconds to destroy my container and then recreate it! My feedback loop is considerably reduced “Testing my app is so complicated!” On most of my projects, I have an application, and it calls an API and a database. I would like to be able to test all of these interactions: it is possible to do so on my computer, but much more complicated when it comes to continuous integration. As seen before, Docker enables my to quickly pop up a database. In the same way, if I have access to the API’s code, I can create its image and containerize it. I can thus generate a test API in mere seconds, and use it in my continuous integration. If I do not have access to the API, I can always mock it and use the mock in the same way. Docker thus allows me to quickly and simply recreate my project’s topology. “It takes so much time to test my app!” Until now, in order to test my code in an environment that matches production, I asked the client to provide me with a VM. This could take a few weeks. What a waste of time! And if ever I made a mistake in my configuration and wished to start over, I had to go through the process all over again – and pay additional fees. A container enables me to do so without needing the client’s VM, and to confirm that my app and its dependencies communicate correctly. Keep in mind that a VM is longer to build and destroy than a container, and it blocks a fixed amount of the host machine’s resources when it is launched, whereas a container uses only the resources it needs. Conclusion Advantages As a developer, using containers can make my life easier: It is easier to share the same environment between developers. The dependencies are the same, whatever the environment (I am sure that my colleagues and I have the same versions of the tools we use). Docker is cross-platform: OS differences between devs are no longer an issue. I work on an isolated environment. I have access to off-the-shelf solutions thanks to the open source images available online. I can do POCs quickly thanks to Docker images without polluting my computer. Containers are quicker to build and destroy than VMs. It accelerates the feedback loop and therefore the deployment in production. I forces me to apply development best practices: 12factor app To consider Careful not to rush headlong into Docker: Not every application can be containerized: they must respect the 12 factor app rules. Docker offers many functionalities you must experiment before creating complex topologies. Docker can be less pertinent on smaller projects Docker introduces an extra complexity: a bug can be caused by my configuration, my Docker topology, or by my app. Containers are based on my computer’s OS’ Kernel: If I use specific functionalities from my Kernel, my image might not be compatible with another machine (ex: images built for Windows cannot be used on Linux and reciprocally). A containerized process can exploit a weakness in the Kernel and impact all the other processes in the host machine. Keep in mind that Docker is only one tool amongst many others: RKT (CoreOS), or even Windows Containers (Microsoft) for exemple. It is up to you to chose yours! Sources https://www.contino.io/insights/beyond-docker-other-types-of-containers https://www.aquasec.com/wiki/display/containers/Docker+Alternatives+-+Rkt%2C+LXD%2C+OpenVZ%2C+Linux+VServer%2C+Windows+Containers ) https://www.lemagit.fr/conseil/Conteneurs-VS-VM-les-entreprises-pronent-la-mixite https://blog.wescale.fr/2017/01/23/introduction-a-rkt/ https://docs.docker.com/engine/reference/commandline/run/ https://docs.docker.com/registry/ https://12factor.net/ https://www.lemagit.fr/conseil/Comprendre-les-enjeux-des-containers-as-a-service https://deliciousbrains.com/vagrant-docker-wordpress-development/ https://www.bleepingcomputer.com/news/security/17-backdoored-docker-images-removed-from-docker-hub/ https://blog.octo.com/applications-node-js-a-12-facteurs-partie-1-une-base-de-code-saine/ https://github.com/benoit74/docker-ipcalc Play time! https://training.play-with-docker.com/ https://docs.docker.com/get-started/ https://hub.docker.com/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-11-07"},
{"website": "Octo", "title": "\n                Hexagonal Architecture: three principles and an implementation example            ", "author": ["Erwan Alliaume", "Sébastien Roccaserra"], "link": "https://blog.octo.com/en/hexagonal-architecture-three-principles-and-an-implementation-example/", "abstract": "Hexagonal Architecture: three principles and an implementation example Publication date 15/10/2018 by Erwan Alliaume , Sébastien Roccaserra Tweet Share 0 +1 LinkedIn 0 Documented in 2005 by Alistair Cockburn , Hexagonal Architecture is a software architecture that has many advantages and has seen renewed interest since 2015. The original intent of Hexagonal Architecture is: Allow an application to equally be driven by users, programs, automated test or batch scripts, and to be developed and tested in isolation from its eventual run-time devices and databases. To explore the benefits of piloting an application by automated tests , or developing and testing in isolation from the database , we recommend that you read this series of blog post on the test pyramid we published recently: the test pyramid by practice . The promise is quite attractive, and it has another beneficial effect: it allows to isolate the core business of an application and automatically test its behaviour independently of everything else. This could be the reason why this architecture has caught the eye of Domain-Driven Design (DDD) practitioners. But be careful, DDD and hexagonal architecture are two quite distinct notions which can reinforce each other but which are not necessarily used together. But this is a topic for another time! Finally, this architecture is not very complicated to set up. It is based on a few simple rules and principles. Let’s explore these principles to see what they mean in practice. Principles of Hexagonal Architecture Detail: How is the code organized inside and outside? Detail: At the Runtime Detail: Dependencies inversion on the right Detail: Why an Interface on the left? Testing in Hexagonal Architecture To go further ahead References Principles of Hexagonal Architecture The hexagonal architecture is based on three principles and techniques: Explicitly separate User-Side, Business Logic, and Server-Side Dependencies are going from User-Side and Server-Side to the Business Logic We isolate the boundaries by using Ports and Adapters Vocabulary note: Throughout the rest of the article, the words User-Side , Business Logic and Server-Side will be used. These words come from the original article, and they are defined in the section below. Principle: Separate User-Side , Business Logic and Server-Side The first principle is to explicitly separate the code into three large formalized areas. On the left, the User-Side This is the side through which the user or external programs will interact with the application . It contains the code that allows these interactions. Typically, your user interface code, your HTTP routes for an API, your JSON serializations to programs that consume your application are here. This is the side where we find the actors who drive the Business Logic . Note: Alistair Cockburn also calls it the Left Side. The Business Logic , in the center This is the part that we want to isolate from both left and right sides. It contains all the code that concerns and implements business logic. The business vocabulary and the pure business logic , which relates to the concrete problem that solves your application, everything that makes it rich and specific is at the center. Ideally, a domain expert who does not know how to code could read a piece of code in this part and point you to an inconsistency (true story, these are things that could happen to you!). Note: Alistair Cockburn also calls it the Center . On the right, the Server-Side This is where we’ll find what your application needs, what it drives to work. It contains essential infrastructure details such as the code that interacts with your database, makes calls to the file system, or code that handles HTTP calls to other applications on which you depend for example. This is the side where we find the actors who are managed by the Business Logic . Note: Alistair Cockburn also calls it the Right Side . The following principles will allow to put into practice this logical separation between User-Side , Business Logic and Server-Side . Why is that important? A first important feature of this separation is that it separates problems . At any time, you can choose to focus on a single logic, almost independently of the other two: user-side logic, business logic, or server-side logic. They are easier to understand without mixing them, and the constraints of each logic have less impact on the others. Another characteristic is that we put business logic at the forefront of our code . It can be isolated in a directory or module to make it explicit to all developers. It can be defined, refined and tested without taking on the cognitive load of the rest of the program. This is important because, in the end, it is the developers’ understanding of the business that goes into production. And finally, in terms of automated tests (as we will see below), we will succeed in testing with a reasonable effort: The whole Business Logic individually, Integration between User-Side and Business Logic independently from the Server-Side Integration between Business Logic and Server-Side independently on the User-Side Illustration: a small example of an application To illustrate these principles more concretely, we will use the small example used during the “Alistair in the Hexagon” event, proposed in 2017 by Thomas Pierrain ( @tpierrain ) and Alistair Cockburn ( @TotherAlistair ) himself. Note: you will find the videos and the event code at the end of the article. The purpose of this small application is to provide a command line program that writes poems into the standard output of the console. Example of the expected output of this application: $ ./printPoem\nHere is some poem:\nI want to sleep\nSwat the files\nSoftly, please.\n-- Masaoka Shiki (1867 - 1902)\nType enter to exit... To correctly illustrate the three zones ( User-Side , Business Logic , Server-Side ), this application will search poems in an external system: a file. We could also connect this application to a database, the principles would be identical. In this context, how can we apply this first principle, namely the separation into three zones? How to distribute on the left (what drives), in the center (the core business), and on the right (what is driven)? The User-Side From the user’s point of view, the program is presented as a console application. So the concept of console will be on the left, on the User-Side . It is through the console that the user will drive the domain. The Server-Side Technically, in our case, the poems are stored in a file. This notion of file will be found on the right, on the Server-Side . The business will make the request of its poems by piloting this right side, concretely implemented by a PoetryLibraryFileAdapter. Here, as mentioned above, we can easily exchange our source of poems (a file, a database, a web service…). The actual implementation of the source as a file is therefore a technical detail (also called a technical implementation detail). The Business Logic Our core business in this case, what has value for the user, is the notion of reading poems . We can materialize this notion in the code with a PoetryReader class for example User-Side → Business Logic interactions From a business point of view, it doesn’t matter whether the request comes from a console application or another, it’s a technical detail that we want to be able to abstract. This is precisely one of the initial intentions: “to be driven as well by a user as by tests”. There is therefore no concept of console in the Business Logic . What our application does allow, however, from the user’s point of view (= the service it provides) is to ask for poems . It is this notion that we will find in the Business Logic (materialized by IRequestVerses) and that will allow the User-Side to interact with the Business Logic . Business Logic → Server-Side interactions Similarly, from the Business Logic point of view, it doesn’t matter whether the poems come from a file or a database, we want to be able to test our application independently of external systems. No notion of file in the Business Logic . To operate, the domain still needs to get the poems. We find this notion of obtaining poems in the Buisiness Logic in the form of the IObtainPoems interface. It is this notion of obtaining poems that will allow the domain to interact with the Server-Side . Note: From here, when you read the diagrams, you can start to observe the arrows that show the relationships between the classes. A solid arrow represents a call or composition interaction. And an arrow without filling represents an inheritance relationship (as in UML). But no need to analyze everything right away, we’ll explore it in detail later. Note: the names IRequestVerses and IObtainPoems represent many interfaces, we will talk about them in a principle to follow. For the anecdote, the convention of starting an interface name with an “i” is no longer in fashion but Thomas Pierrain reads interface names as sentences in the first person singular. IRequestVerses reads: I request verses for example. I like this idea. Principle: dependencies go inside This is an essential principle for achieving the objective. We have already begun to see this in the previous principle. Principle: Dependencies go to the Business Logic The program can be controlled both by the console and by tests, there is no concept of a console in the Business Logic . The Business Logic does not depend on the User-Side side, it is the User-Side side that depends on the Business Logic . The User-Side (ConsoleAdapter) depends on the notion of poem request, IRequestVerses (which defines a generic “poem request” mechanism on the part of the user). Similarly, the program can be tested independently of its external systems, the Business Logic does not depend on the Server-Side , it is the opposite. The Server-Side depends on the Business Logic , through the notion of obtaining poems, IObtainPoems. Technically a class on the Server-Side side will inherit the interface defined in the Business Logic and implement it, we will see it in detail below to talk about dependency inversion. Inside and outside If we see dependency relationships (<<depends on…>>) as arrows, then this principle defines the central Business Logic as inside, and everything else as outside (see figure). We regularly find these notions of inside and outside when we discuss hexagonal architecture. It can even be the fundamental point to remember and transmit: dependencies go inside . In other words, everything depends on the Business Logic , the Business Logic does not depend on anything . Alistair Cockburn insists on this demarcation between inside and outside, which is more structuring than the difference between User-Side and Server-Side to solve the initial problem. Principle: boundaries are isolated with interfaces To summarize, the user-side code drives the business code through an interface (here IRequestVerses) defined in the business code. And the business code drives the server-side through an interface also defined in the business code (IObtainPoems). These interfaces act as explicit insulators between inside and outside . A Metaphor: Ports & Adapters The hexagonal architecture uses the metaphor of ports and adapters to represent the interactions between inside and outside . The image is that the Business Logic defines ports , on which all kinds of adapters can be interchangeably connected if they follow the specification defined by the port . For example, we can imagine a port of the Business Logic on which we will connect either a hard-coded data source during a unit test, or a real database in an integration test. Just code the corresponding implementations and adapters on the Server-Side , the Business Logic is not impacted by this change. These interfaces defined by the business code, which isolate and allow interactions with the outside world are the ports of the Ports & Adapters metaphor. Note: as mentioned previously, ports are defined by the business , so they are inside . On the other hand, Adapters represent the external code make the glue between the port and the rest of the user-side code or server-side code. Here, the adapters are ConsoleAdapter and PoetryLibraryFileAdapter respectively. These adapters are outside . Another Metaphor: the Hexagone Another metaphor that gave its name to this architecture is the hexagon, as we see on the previous figure. Why a hexagon? The main reason is that it is an easy-to-draw shape that leaves room to represent multiple ports and adapters on the diagram. And it turns out that even if the hexagon is quite anecdotal in the end, the expression Hexagonal Architecture is more popular than Ports & Adapters Pattern. Probably because it sounds better? The theoretical part is over, there are no other principles: for everything else we are totally free. Detail: How is the code organized inside and outside? Apart from the principles seen above, we are totally free to organize the code within each zone exactly as we want. Concerning the business code , the inside, a good idea is to choose to organize its modules (or directories) according to the business logic . One organization to avoid is to group classes by types. For example the “ports” directory, or the “repositories” directory (if you use this pattern), or the “services” directory. Think 100% business in your business code , including for the organization of your modules or directories! The ideal case is to be able to open a directory or a business logic module and immediately understand the business problems that your program solves ; rather than seeing only “repositories”, “services”, or other “managers” directories. See also on this topic: https://medium.com/@msandin/strategies-for-organizing-code-2c9d690b6f33 https://martinfowler.com/bliki/PresentationDomainDataLayering.html Detail: At the Runtime How exactly do you instantiate all this to satisfy runtime dependencies? If you are using a dependency injection framework, you may not need to ask yourself this question. But I think that to understand the hexagonal architecture, it’s interesting to see what happens when the application starts . And to do this, do not use dependency injection framework at least for the time of this article. For example, here is how we will write the entry point of the application if we instantiate everything by hand: class Program\n{\n    static void Main(string[] args)\n    {\n        // 1. Instantiate right-side adapter (\"go outside the hexagon\")\n        IObtainPoems fileAdapter = new PoetryLibraryFileAdapter(@\".\\Peoms.txt\");\n\n        // 2. Instantiate the hexagon\n        IRequestVerses poetryReader = new PoetryReader(fileAdapter);\n\n        // 3. Instantiate the left-side adapter (\"I want ask/to go inside\")\n        var consoleAdapter = new ConsoleAdapter(poetryReader);\n\n        System.Console.WriteLine(\"Here is some...\");\n        consoleAdapter.Ask();\n\n        System.Console.WriteLine(\"Type enter to exit...\");\n        System.Console.ReadLine();\n    }\n} The instantiation order is typically from right to left: First we instantiate the Server-Side , here the fileAdapter which will read the file. We instantiate the Business Logic class that will be driven by the application, the poetryReader in which we inject the fileAdapter by injection into the constructor. Install the User-Side , the consoleAdapter that will drive the poetryReader and write to the console. Here the poetryReader is injected into the consoleAdapter by injection into the constructor. We said the inside shouldn’t depend on the outside. So why do we inject the fileAdapter, which is code from the Server-Side , into the poetryReader which is code from the Business Logic ? We can do this because, by looking at the schemas and code, in addition to being a PoetryLibraryFileAdapter ( Server-Side ), the fileAdapter is also an instance of IObtainPoems by inheritance. In practice, the PoetryReader does not depend on PoetryLibraryFileAdapter but on IObtainPoems, which is well defined in the Business Logic code. You can check it by looking at the signature of its constructor. public PoetryReader(IObtainPoems poetryLibrary)\n{\n    this.poetryLibrary = poetryLibrary;\n} PoetryLibraryFileAdapter and PoetryReader are weakly coupled. Detail: Dependencies inversion on the right The fact that the fileAdapter depends on the business for its definition (dependency by inheritance here), but that at the runtime the poetryReader can control in practice an instance of fileAdapter is a classic case of dependency inversion . Indeed, without the IObtainPoems interface, the business code would depend on the server-side code for its definition, which we want to avoid: The interface allows to reverse the direction of this dependency: In addition to making the business independent of external systems, this interface on the right allows to satisfy the famous D of SOLID , or Dependency Inversion Principle . This principle says: High level modules should not depend on low level modules. Both must depend on abstractions. Abstractions should not depend on details. The details must depend on the abstractions. If we did not have the interface, we would have a high-level module (the Business Logic ) that would depend on a low-level module (the Server-Side ). Note: for interactions between the left side and business code, dependency is naturally in the right direction. This difference in the implementation of interactions is related to the difference between User-Side / Business Logic and Business Logic / Server-Side relationships. Reminder: the User-Side drives the Business Logic , and the Server-Side is driven by the Business Logic . Detail: Why an Interface on the left? Since the dependencies between User-Side and Business Logic are already in the right direction, the role of the IRequestVerses interface is not to reverse dependencies. However, it still has an interest: that of explicitly limiting the coupling surface between the User-Side code and the Business Logic code. In practice, the PoetryReader class can have other methods than those of the IRequestVerses interface. It is important that the ConsoleAdapter is not aware of this. And it is aligned with another SOLID principle, Interface Segregation Principle . Clients should not be forced to depend on methods they do not use. But once you have understood the intent, if a port to the left side has only one method, and its implementation has only one method as in our example, is the interface really necessary? In a dynamic language that will work by duck typing in the end? We can answer with a question: what does your team think about this? Is the isolation objective clear to everyone, no need for an interface to even trigger a conversation? It’s up to you to decide altogether. Testing in Hexagonal Architecture An important benefit of this software architecture is that it facilitates test automation, which is part of its original intent. How to replace some code from the User-Side? In the general case, the role of the left code can be directly played by the test framework. Indeed, the test code can directly drive the business logic code. Note: The figure illustrates an integration test because the right part is not replaced. It can also be replaced, see below. How to replace some code of the Server-Side ? The code on the right must be driven by the business. In general, if you want to write a unit test, you replace it with a mock or any other form of test double depending on what you want to test. Target reached! Allow an application to be driven by users , programs, automated tests or batch scripts, and to be developed and tested in isolation from its possible execution systems and databases. Be careful! This does not prevent you from testing your User-Side and Server-Side code, any code deserves to be tested. On this subject, I refer you again to the series The pyramid of tests by practice . And indeed, by combining what we replace or not, we see that with this architecture we can test what we wanted: The whole Business Logic individually, Integration between User-Side and Business Logic, independently from the Server-Side Integration between Business Logic and Server-Side, independently on the User-Side To go further ahead Talk about it as a team, who already knows how to do it at home? Go ahead, experiment in real life, on your code. A small personal project for example, or a small project with your team. What is easy for you, what is difficult? Here are some additional questions you may have during implementation: A port can have only one method, or group several methods. What makes sense in your case? Even when it follows the dependency principles well, the code is not necessarily separated into three explicit modules or directories or packages or namespaces. As in Thomas Pierrain’s code, I have seen several times the Business Logic code in a “Domain” directory, and both the User-Side and Server-Side code in an “Infrastructure” directory. In his example, the inside code is located in the HexagonalThis.Domain namespace, and the outside code in the HexagonalThis.Infra namespace. Quick reminder: there is no silver bullet . The hexagonal architecture is a good compromise between complexity and power, and it is also a very good way to discover the subjects we have addressed. But it is only one solution among others. For simple cases, it may be too complicated, and for complicated cases, it may be too simple. And there are other software architectures worth exploring. For example, Clean Architecture goes further in formalisation and insulation (with a zest of extra SOLID). Or in a different but compatible axis, CQRS makes it possible to better separate readings and writings. References The videos of the event “Alistair in the Hexagone” are here . The code from this event is on Thomas Pierrain’s github . You can also read these good articles on this topic: http://alistair.cockburn.us/hexagonal-architecture http://wiki.c2.com/?HexagonalArchitecture/ https://martinfowler.com/bliki/PresentationDomainDataLayering.html https://fr.slideshare.net/ThomasPierrain/coder-sans-peur-du-changement-avec-la-meme-pas-mal-hexagonal-architecture http://tpierrain.blogspot.fr/2016/04/hexagonal-layers.html http://alistair.cockburn.us/Configurable+Dependency http://blog.cleancoder.com/uncle-bob/2016/01/04/ALittleArchitecture.html Finally, thanks to Thomas Pierrain for allowing me to reuse his sample code, and thanks for the suggestions and proofreadings by: Etienne Girot, Jérôme Van Der Linden, Jennifer Pelisson, Abel André, Nelson Da Costa, Simon Renoult, Florian Cherel Enoh, Mathieu Laurent, Mickael Wegerich, Bertrand Le Foulgoc, Marc Bojoly, Jasmine Lebert, Benoît Beraud, Jonathan Duberville and Eric Favre. Update note: in a first version of this article, we used the words Application , Domain and Infrastructure in place of User-Side , Business Logic and Server-Side . We went back to the original words because this substitution was sometimes ambiguous and not helpful. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 12 commentaires sur “Hexagonal Architecture: three principles and an implementation example” Juan 23/10/2018 à 21:01 Good article, congratulations. Just three thoughts that come to my mind:\r\n\r\n(1) I think that Ports must be distinguished, because an adapter depends just on a port, so not structuring the hexagon by type isn't 100% true.\r\n\r\n(2) I think that an interface in the driver side is necessary, because although it has just one implementation (the hexagon) it provides information hiding, and the S principle of SOLID. If you don't create an interface of the implementation, the adapter would depend on the whole hexagon, not just the port, and you couldn't apply the Configurable Dependency Pattern on the driver side.\r\n\r\n(3) I think your understanding is similar to mine, I have written an article ( https://softwarecampament.wordpress.com ) but I don't mix in it DDD stuff. I will do it apart in another article to make clear that you can do hexagonal architecture without DDD. I've seen you advise from that too, but then you mix it too.\r\n\r\nRegards,\r\nJuan. Juan 27/10/2018 à 06:24 Hello, in my opinion application layer of DDD is inside the hexagon. Application services interfaces are driver ports. And it is the use case boundary. The console adapter in the drawing isn't the application of DDD. Console adapter, test adapter, or any other driver adapter fits into the UI layer of DDD. See chapter 4 and chapter 14 of Vaughn Vernon book Implementing DDD. Regards, Juan. Fernando Franzini 04/01/2019 à 18:41 Very cool, this architecture, I see as an evolution of DDD.\r\nAre there official books? Milos 05/01/2019 à 20:19 The first article which the right way explained principle and implementation. Thanks Antonio 12/01/2019 à 20:17 GOLD. But would the PoetryReader class be a use case? Chandan Kumar 10/03/2019 à 10:45 I agree that Application Depends on Domain but how Infrastructure(DB, Third party web services) are dependent on Domain. Looking at depiction above I see that Domain Depends on Infrastructure. Could someone help me understand my doubt. freak 19/04/2019 à 10:05 best article in this subject I've found, clear explanation. Good that you did not stick to hexagon image and try to explain things on it - in my opinion this way brings more questions than answers ; ) Denys 12/03/2020 à 11:42 Very useful article: simple and all vitamins at the right places. Muhammed Shakir 23/08/2020 à 08:33 Excellent article ! Appreciate the effort spent on putting up such a well articulated blog. Alexei Kaigorodov 05/10/2020 à 09:02 this architecture can be further refined:\r\n- the main application is also an actor\r\n- it can be split in smaller actors\r\n- preferable interfaces for ports are Publisher/Subscriber from reactive streams\r\n- other interfaces can be added with care. Avoid blocking calls.\r\n- actor can be implemented as a thread, as asynchronous procedure, or as as a coroutine, but this implementation detail is not visible from outside the actor\r\n\r\nI call this refined architecture \"Ports and Actors\". Or \"Actors with Ports\", I don't know which name is better. Ravi 26/10/2020 à 18:18 Very well explained. Awesome. Hari 06/11/2020 à 16:59 Its simply superb. Made me to understand about Hexagonal architecture very quick in time to start using it. Thank you once again. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-10-15"},
{"website": "Octo", "title": "\n                KotlinConf 2018 – Recap            ", "author": ["Maxime Bonnet", "Rémi Pradal"], "link": "https://blog.octo.com/en/kotlinconf-2018-recap/", "abstract": "KotlinConf 2018 – Recap Publication date 12/10/2018 by Maxime Bonnet , Rémi Pradal Tweet Share 0 +1 LinkedIn 0 On October 4th and 5th the 2nd Kotlin Conf took place. We were here to attend the conference, learning new Kotlin tricks, but above all, to feel the amazing Kotlin vibe. After a first edition in San Francisco, the Kotlin Conf set up this year in Amsterdam, in the beautiful Bars Van Beurlage venue (one of the conference room was even lit with stunning stained glasses). Here is a recap of the talks that impressed us the most, classified by the main themes we identified during these two days Kotlin as multi-platform language One of Andrey Breslav’s main point of emphasis during the opening keynote was that Kotlin is not limited to Android development. More than 40% of attendees were indeed using Kotlin for backend development, and more than 30% for web development. As mobile developers, we were curious to find how others used Kotlin, in multiplatform projects and backend applications. Ryan Harter gave a great talk and explained us how to begin Building Server Backends with Ktor . Ktor is an open source Kotlin server framework mainly developed by JetBrains. It uses a lot of the language’s loved features like extension functions, a type-safe builder to describe the application and is entirely based on coroutines. The Kotlin 1.3 release comes with Ktor 1.0 beta. We were amazed by how easy and intuitive it seemed to use the framework. In Architecting a Kotlin JVM and JS multiplatform project , Felipe Lima gave an overview of the main multiplatform principles. The most important concept is the use of a shared module for all platforms. Felipe encouraged us to share as many code as possible between platforms, until platform specific APIs are needed. You can then bridge the common and platform specific modules with the expect/actual mechanism. However, we were a bit disappointed that he only told us about a project he did in his spare time, a NES emulator, and not a project he worked on at AirBnb. Given their past experience with React Native, we were eager to hear about what he had to say about their experience with dealing with multi-platform in a professional context. Plus, his personal project was not done yet, and he had yet to work on the JS side. Multiplatform projects are still in an experimental state, meaning some breaking changes may occur when hitting 1.4 or later versions of Kotlin, but it’s one of our main point of attention, as we feel it will shape our work in the future. We’ll make sure to keep an eye on it. “Unexpected” use cases of Kotlin As mobile developers, we were impatient to discover some unexpected use cases of the Kotlin language that could differ from what we do on a daily basis. The Kotlin Conf 2018 filled our expectations by offering talks on some “unusual” topics (at least for us). One was Paul Merlin’s talk Type-safe build logic with Gradle Kotlin DSL . In 2016, Gradle announced its will to add the possibility to write build scripts in Kotlin instead of Groovy. The goal is to take benefit from the statically typed characteristic of Kotlin to achieve faster builds, a better integration with the IDE and checks at “compile time” instead of run time. We made some attempts to use the Gradle Kotlin DSL since and we experienced many performance and IDE related issues. Therefore, we were impatient to see the improvements brought by 1.0-RC version. In his talk, Paul showed us step by step how to migrate an existing Groovy script to a Kotlin one. If the intention is praiseworthy, we think it would have been more relevant to show us how to write a script from scratch to put the light on Kotlin and not be in the “shadow” of Groovy. Moreover there was some unfortunate malfunction in the Kotlin script refactoring presented (the well-known “demo effect”) which slightly tarnished the demonstration. Another issue conceded by Paul is today’s Kotlin gradle script slowness compared with its Groovy version. To sum up, our opinion is that, even if promising, the Gradle Kotlin DSL is not mature enough to be used in our projects. Another possible usage for Kotlin is the ability to perform some data science analysis thanks to a Kotlin stack. Indeed, in his talk Building Data Science Workflows with Kotlin , Holger Brandl demonstrated how he solved a Kaggle competition (the famous taxi fare prediction ) thanks to a Kotlin stack. He started admitting that the Kotlin statically typed attribute was a disadvantage compared to the “classic” data science language, Python and R, when it comes to write some “quick” exploratory code. He used the following stack: Sparklin & KShell for data processing and interactive shell Krangl a data manipulation library developed by the speaker himself allowing data operations (including data import from various file format) thanks to a data scientist friendly api. Kravis , also developed by Holger, a wrapper around ggplot2 used to display graphical results Jupyter kotlin : a portage of the well known notebook commonly used for formatting data science experiments. Even if most of these tools are in an early stage of their development and brought a slight “messy feeling” to us , it is really exciting to see that there is already, in Kotlin, a usable stack of tools to perform these kind of tasks. Coroutines When announcing Kotlin 1.3 features during the opening keynote, Andrey Breslav happily started with coroutines reaching 1.0. Meaning we can finally drop the experimental package from our imports. We’ve been using coroutines in production for some months now, and the experimental experience has been really easy to deal with. It fits with the mindset Breslav explained in his keynote: we do not want any legacy in the language code, but we want to have comfortable upgrades. Besides the keynote, they were several talks on the subject. In Android Suspenders , Chris Banes went back explaining why the changes in the 0.26 version were needed: before that, launched coroutines were unscoped and it was really easy to leak them (that sadly happened to us). He then went ahead and explained how he used coroutines with the Architecture Components View Models. In the ever longing question “RxJava vs Coroutines”, Banes joined the opinion of others (Christina Lee & Jake Wharton for example): If you only need Rx to do basic threading, coroutines will be simpler and better suited. If you want to do trickier things, Rx operators can be a daunting task to learn whereas there are fewer, more traditional operators that come with coroutines. Plus, it’s quite easy to write custom ones. In order to represent things closer to Rx streams, we could use channels. But something that was discussed often, by C. Banes in his talk and C. Lee and J.Wharton during the ending panel, was the inability of channels to represent cold streams . For now, they are closer to Rx’s Subjects than Observables. After those two days, everyone appeared really excited about coroutines hitting 1.0. And we were quite comforted on our position, that coroutines are well suited for our needs as Android developers. Use Kotlin’s power to your benefit One recurring topic was to use the language’s many features to gain readability, safety, and ease of development. It was inspiring to see what kind of issues developers faced, and how they used the same tool we’ve been using for more than a year now to solve them. Romain Guy told us in Kotlin for graphics how he used the language to do 3D programming. That means that all he’s doing is … maths. In this case, functional programming works really well. We do not need to abstract maths with OOP, and all that’s needed are some top level functions, some operators overriding (for matrices and vectors for instance) and you’ve got yourself quite a readable, concise graphics code. Christina Lee, in her talk Sealed Classes are great Representing State: the Kotlin Edition wanted to make us aware that types matter. The big takeaway is that usual types like Int or String are unbounded, and represent an infinity of values. You have to ask yourself: is it what I really want to represent? If not, a great way to bound your types and make an opinionated choice is to use Kotlin sealed classes. We’ve been using them to represent view models and errors / successful results for a while now and this talk comforted us in our vision of Kotlin as a language that enforces good programming practices. Romain Guy also mentioned some 1.3 features that will help have greater control over which types we use, with the incoming UInt s and inline classes . From the opening keynote, one of the buzzwords of this Kotlin Conf was meta programming . If you’re wondering what it is, Amanda Hinchman gave a quick definition in her talk Kotlin: The Next Frontier for Modern (Meta)Programming : It’s a program that treats programs as data. It can read, analyse or transform other programs. Traditionally, languages like C++ use macros to tackle this, but the Kotlin team chose not to follow this path, and use annotation processing and compiler plugins instead. Amanda Hinchman explained how more traditional AOP (Aspect Oriented Programming) based on annotation processing, like in Java, is still a viable solution. But a more interesting one is to use Kotlin’s type-safe builders to create your own DSLs. On the other hand, Writing Your First Kotlin Compiler Plugin by Kevin Most was an advanced talk on the subject. You may have used a compiler plugin without knowing it if you’ve used the Kotlin Android extensions. They are similar to annotation processors, but instead of generating Kotlin or java code, they directly generate bytecode. As we already said: this is a very advanced topic, and you should think twice before trying to develop a plugin. Both talks gave the impression that metaprogramming is a very powerful tool, but it targets very specific use cases and you should not dive into it just because of all the hype around it. Some tips grabbed from the talks Here are some general purpose tips that we learned over the two days. In no particular order: You can annotate an extension function with @Receiver:MyAnnotation , to state that this extension only applies on types annotated with @MyAnnotation . – Romain Guy It’s super-easy to wrap callback in coroutines to write synchronous-type programming, with suspendCoroutines . – Chris Banes Use sealed classes everywhere! – Christina Lee Use Arrow’s optics to update immutable nested data classes – Raúl Raja Martínez Use sequences instead of collections when more than one processing step is needed – Florina Muntenescu Conclusion To conclude, we were quite impressed by the dynamism of the Kotlin community, which seems committed not to be confined to today’s Kotlin main use case: the mobile development (Andrey Breslav quoted a survey in which it appeared that around 60% of Kotlin developers are mobile developers). We are more than ever convinced that its pragmatic aspect, along with built-in features and community dynamism make Kotlin a perfect choice for many types of project. As a final word, we regret the medium quality of the conf mobile app (no autoscroll to current time slot, loss of favorite talks in the middle of the day). We hope that next year, the app will be a better showcase of this great language capabilities! You can see the records of the different talks on the Kotlin Conf YouTube playlist Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-10-12"},
{"website": "Octo", "title": "\n                Authorisation for AWS S3 static website            ", "author": ["Ilya Trofimov"], "link": "https://blog.octo.com/en/authorisation-for-aws-s3-static-website/", "abstract": "Authorisation for AWS S3 static website Publication date 13/09/2018 by Ilya Trofimov Tweet Share 0 +1 LinkedIn 0 Hi, Today I’m going to tell you a story about a feature we wanted to implement to secure a website by adding authorisation to it. TL;DR: Authorization: plan your authorization flow in advance, with all involved calls and pages Lambda@Edge: keep html content out of the lambda code keep lambda code as simple as possible it could be tricky to find your lambda logs in CloudWatch Check out the code at https://github.com/ilya-v-trofimov/s3-site-auth Intro It might seem to be a trivial task, but the website happened to be a static website on AWS S3. And it turns out that there is no out of the box mechanism in AWS to add authorization to S3 static website So, we started to think about a solution. And we’d been thinking for 3 days and 3 nights and finally came up with the plan. Idea was simple: Start with something very basic, like single HTML page on S3 bucket Gradually add complexity, including authorization functionality Apply solution to the whole website with js/css/images CloudFront + Lambda + S3 Keeping that in mind, we have set about implementing the first step. One of the requirements we had to consider was to keep the solution serverless, with pay-as-you-go paradigm: this would allow to run a website almost for free, since low traffic was anticipated and there was no point in running whole EC2 instance for that. We scratched heads and drew this: Basically, a User accesses the AWS CloudFront distribution URL via his browser. CloudFront (AWS CDN solution) allows to run Lambda@Edge against every HTTP request to particular resource. If request is authorized, CloudFront serves S3 Website data. If not, 4xx error is sent back to User. This is oversimplified explanation, but we’ll dig into details together soon. Authorization Since all users of the website are supposed to have a corporate Google account, we have decided to use Google’s OAuth 2.0 service for the website user authentication. Google provides some developer’s documentation on that here , however one should be familiar with OAuth 2.0 authorization options in order to choose proper help topic. In our case, we need two parts: Client- and Server-side functionality. Process flow looks as follows, index.html is used as an example: Client side This is a HTML page to be displayed in User’s browser. This should 1. render a Google’s Sign-in button 2. let User sign-in to his corporate Google account 3. and obtain OAuth token, which then to be sent in Authorization header to a server along with HTML page request. This is more or less covered in https://developers.google.com/api-client-library/javascript/samples/samples#authorizing-and-making-authorized-requests and some api description is available here: https://developers.google.com/identity/sign-in/web/reference As you may have noticed on authorization flow diagram there are two resources mentioned: index.html and login.html . Well, in fact, there is another one: main.html Basically, main.html takes over Sign in/out controls from login page wraps index.html in an iframe. This approach is taken to be able to obtain Auth token in one page (login) and then use it when requesting another one (index): So, once authorization token is obtained, we use XmlHttpRequest object to send a GET request for index.html, containing our sensitive content. We also attach authorization header with token to the request. Some code snippets and configuration required for this to be implemented is described in details in https://developers.google.com/identity/protocols/OAuth2UserAgent In addition, please check out full code of the main.html page on github: https://github.com/ilya-v-trofimov/s3-site-auth/blob/master/html_templates/main.html Server side This part is no more complex than a client one, we only need to extract authorization token from corresponding header of request validate token with Google send back response based on validation outcome The code is wrapped into Lambda, which is attached to a CloudFront behaviour responsible for index.html resource. Code for the Lambda@Edge could be found here: https://github.com/ilya-v-trofimov/s3-site-auth/blob/master/main.js Frankly speaking, when we were planning to implement Lambda@Edge, things seem to be straightforward. HOWEVER! When we start to implement, deploy and test the code, we faced different issues on every single step. In the end, we came up with the following best practices for Lambda@Edge: The role, associated with Lambda@Edge must contain the following statements in Trust Relationship tab: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": [ \"lambda.amazonaws.com\", \"edgelambda.amazonaws.com\" ] }, \"Action\": \"sts:AssumeRole\" } ] } This will allow a code to be run by Lambda@Edge In case of ‘Viewer-Request’ CloudFront’s behaviour type, you would want to forward a request when Authentication header is valid. In this scenario use a request from event to return it in a callback function. Remove authorization header preliminary: Try to avoid generated responses, especially with html body attached, as It may produce errors in Lambda@Edge and consequently in CloudFront behaviour if html is not formatted properly cosmetic changes to html code will require Lambda@Edge redeployment, which take 15-20 minutes! Put your html to S3 and serve it from there. Use Lambda@Edge only to route the flow or adjust requests (e.g. adding/removing headers) In case of generated responses for 4xx or 3xx codes, use simple responses without a body: Keep a code as simple as possible in Lambda@Edge: again, it takes 15-20 minutes to redeploy your CloudFront distribution along with lambda Logging for Lambda@Edge Once your website and authentication harness is deployed, you would probably want to test it. And I bet your solution would not work right from the start. And most probably, it’s the Lambda@Edge logs which you would look for first. So, what do you need to do to see those logs? Well, it’s not that straightforward. First of all, you wouldn’t see the logs for Lambda@Edge in Monitoring section of your Lambda. What you need to do is to: in AWS Console, switch to the region where the user accesses CloudFront location. That’s a bit tricky, because sometimes you need to guess which location your user accesses. For example: If CloudFront distribution is configured to ‘ Use All Edge Locations’ and your user in Sydney, then you would need to switch to Sydney region (ap-southeast-2) However, if CloudFront distribution is configured to ‘ Use Only U.S., Canada and Europe ’ and you user in Sydney, then you would find the logs for your Lambda@Edge in Oregon (us-west-2) Navigate to CloudWatch service, select Logs section Find your logs under log group named ‘/aws/lambda/us-east-1.<name_of_your_lambda>’ S3 bucket permissions Ok, once you debugged your app and everything runs smoothly, it’s time to cut off the public access to your S3 bucket and leave only one trail to your site – via CloudFront distribution url with Authorisation in place. It’s described in details in https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#private-content-granting-permissions-to-oai However, we want to make it simple and for that we use automatic feature of CloudFront: Navigate to your CloudFront distribution Go to Origins tab and edit your website’s origin Restrict Bucket Access -> Yes Origin Access Identity -> Use an Existing Identity Your Identities -> Choose an existing one. If not exists, choose Create a new one in Origin Access Identity parameter Grant Read Permissions on Bucket -> Yes, Update Bucket Policy After the permissions are set, verify that you can access your site via CloudFront and can’t via S3 static site link Domain name and certificates One more thing to add: If you want to attach a custom domain name to your CloudFront distribution, you need to Create a CNAME for your domain name in AWS Route53 or in another domain names provider of your choice Navigate to a Cloudfront distribution and add a CNAME created to an Alternate Domain Names (CNAMEs) list Also, if you use https, you would need a certificate for your custom domain to be attached to CloudFront distribution in order to avoid ‘invalid certificate’ warning in the browser. Just use SSL Certificate field in your CloudFront distribution for that. Honourable mentions: One of the ideas we considered for this task was a combination of API Gateway with Lambda for authorisation: This approach works well when you have a single resource request flow. And we even managed to implement that. But when it comes to the whole website with multiple resource types (html, css, images, js), you might have hard times serving those varieties. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Authorisation for AWS S3 static website” Kotty 11/06/2020 à 22:36 Thanks, here is also a simple automatic solution https://tracklify.com/blog/simple-basic-auth-in-aws-cloudfront-with-serverless Gari 16/07/2020 à 16:54 I am trying to get this solution working. However it only works for first request to the website by user. if the same user subsquently clicks another page then user gets error as auth token in lost.\r\n\r\nI am trying to serve help files genereated by content publisher software.\r\n\r\nAny ideas ? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-09-13"},
{"website": "Octo", "title": "\n                The Test Pyramid in Practice  5/5            ", "author": ["Lyman GILLISPIE", "Jérôme Van Der Linden"], "link": "https://blog.octo.com/en/the-test-pyramid-in-practice-5-5/", "abstract": "The Test Pyramid in Practice  5/5 Publication date 11/10/2018 by Lyman GILLISPIE , Jérôme Van Der Linden Tweet Share 0 +1 LinkedIn 0 In prior articles ( 1 , 2 , 3 , 4 ), we’ve implemented isolated tests which offer us precise and reliable feedback – and are more or less fast depending on whether we’re running unit tests or component tests which need to load a Spring context. But these tests have their limits, precisely because they’re isolated. In this article we’ll deal with tests that are even higher in the pyramid: integration and end-to-end tests. This article originally appeared on our French Language Blog on 29/06/2018 . Integration tests What to test? Despite the tests we’ve put in place, there are still scenarios that can slip through the cracks. Indeed, so far we have isolated all our tests, but what will happen when we connect everything together? Are we sure that our journey-booking component correctly calls the connection-lookup? Is the database properly configured and accessible? The objective of integration tests is to verify that the “ wiring ” is done correctly, and that our component communicates with its dependencies, no more and no less. We won’t test different business cases (i.e. input parameters, behaviours, etc.), because this has already been done in the unit tests. The following diagram summarizes the integration tests we will implement: Implementation At the moment, the ease of writing tests is inversely proportional to the complexity of their execution. We’ll remain in the JUnit / Spring environment and the objective is to validate integration, not the business logic. We could almost have just call the method without asserting anything other than that it successfully called the external component. Database Without any specific configuration, the Repository test allows us to validate the database connection which we configured in application.yml. The test is therefore very simple ( gitlab ) : The real complexity lays in the data insertion scripts, which have to be maintained as the data model evolves and be kept as fast as possible. Here they are implemented by way of the @Sql annotation. By the same token, the cleanup script is critical to the ability to replay the test as many times as necessary and avoid bleeding state into other tests which will then fail and require investigation time. HTTP client Testing the Lookup service connection is even more straightforward ( gitlab link ): What about running the tests?The tests are easy to write, but we need an entire environment to run them. And with more micro-services and links between them comes more complexity in standing up an environment. Docker can be our friend when addressing this issue, but we won’t go into the details here. Our code on gitlab contains the necessary maven, Dockerfile, and docker-compose.yml to start a postgresql database and the two microservices. There’s an important point to keep in mind: integration tests are not executed by the standard maven build cycle, instead they’re in a separate project. We’ll need to set up a build pipeline (on gitlab, or jenkins or some other CI tool) to start the services before we can run these tests. See the sample project’s readme for more details. End-to-end testing What to test? We finally reach the top of the pyramid, with the end-to-end tests mentioned earlier. As their name suggests, the purpose of these tests is to validate the entire application chain. They aim to validate the integration between of all components. In the diagram below, we see that we’ll test our two services as well as the database and the connection to the Open API transport: Like an integration test, the objective of an end-to-end test is to validate the “wiring”, not the business rules. End-to-end testing usually involves GUI testing when it comes to web or mobile application, or testing programmatic interfaces like REST APIs as in our case. Implementation Our component’s entry point helps us determine how to test it. When testing a web interface, we’ll likely use Selenium or something equivalent (i.e. Protractor on Angular). When testing an API, we recommend the rest-assured framework, which is what we’ll use for our example ( gitlab link ): After configuring the API’s url, rest-assured allows us to use BDD syntax to request different endpoints. It’s thus possible to configure the request headers, parameters, body, etc. using given , to execute it with when , and finally to check the response’s return code, headers, body, etc. via then .  Using json path allows us to check the contents of a json object. As with integration testing, the runtime environment must be in place before these tests can be run. We therefore put end-to-end testing alongside integration testing in a dedicated project. These are then executed at the same time by the build pipeline after spinning up an environment or deploying the services on existing servers. Other tests As initially described, the pyramid didn’t mention other types of tests. But we’d be remiss not to mention a few other types of tests that are important to implement and automate as much as possible. Security testing We haven’t implemented any application security in our example, but know that Spring and the other libraries used provide the necessities (authentication and permission handling) for the component, integration and end-to-end testing that we’ve developed. Moreover, OWASP is an endless source of free information and best practices for web security (i.e. TOP 10 2017 ). They provide a comprehensive guide to testing and a list of tools to help automate security testing. Performance testing Performance testing often happens very late in the development cycle, which goes against the principle of rapid feedback. However, it is possible to automate some of these tests, run them regularly in the build pipeline, and get feedback on the application’s performance evolution. Tools such as Gatling or JMeter are our the tools of choice for continuously testing performance. Acceptance testing Acceptance tests, or functional tests, are the tests that validate the application from the end-user’s point of view, and are often called User Acceptance Tests (UAT). We’re divided on these types of tests: they’re generally in the same format as end-to-end tests (typically GUI tests) which we add a layer of BDD ( cucumber ) to add clarity for the business and testers. For all the reasons explained above, we therefore wish to limit them, but at the same time the business would like all of the rules, use cases, etc. to be validated. Our advice it advise to limit yourself to some smoke tests, which allow us to traverse the entire application via the famous happy path . Once again, business rules will be validated lower down the pyramid. This involves building a relationship of trust between developers and the business regarding the developers’ ability to test application functionally and ensure non-regression through tests already developed. Conclusion It’s time to conclude this series of articles. In the end, we have 26 unit tests, 15 component tests (+ 1 contract test that could likely replace a component test), 2 integration tests and 2 end-to-end tests. We thus respect the testing and our component is secured. To summarize, and if you had to remember only a few points, remember these: Favor tests that provide accurate, fast and reliable feedback . Invest heavily in unit tests , which are cheap to write and provide the best feedback on your business code. Don’t neglect component tests that validate configuration and glue. If you’re building web services / API / micro services, no matter what name you give them, seriously consider contract testing to ensure backwards compatibility. Limit the scope of more complex integration and end-to-end tests to validating the plumbing between your application and the rest of the system. Automate all of these tests and run them as often as possible on a continuous integration server. Do not duplicate tests at each level of the pyramid: each type of test has its own objective. It is a pyramid and not a cube, pyramid that I completed in the following diagram to synthesize this article. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-10-11"},
{"website": "Octo", "title": "\n                The Test Pyramid in Practice 4/5            ", "author": ["Lyman GILLISPIE", "Jérôme Van Der Linden"], "link": "https://blog.octo.com/en/the-test-pyramid-in-practice-4-5/", "abstract": "The Test Pyramid in Practice 4/5 Publication date 04/10/2018 by Lyman GILLISPIE , Jérôme Van Der Linden Tweet Share 0 +1 LinkedIn 0 In the last article we described component tests: tests which are half unit and half integration test, that allow us to validate both integration within our application (via dependency injection) and also with peripheral components. All of which while remaining sufficiently isolated, to limit friction during execution. Because this isolation works perfectly, our API client tests suffer from a major flaw: if and when the supplier changes the service signature we learn about it much too late. This is what contract tests attempt to avoid, and which we will address in this article. This article originally appeared on our French Language Blog on 28/06/2018. Contract tests What to test? In a microservice architecture, it’s likely that the services will communicate amongst themselves, and we’ll run into disaster if we don’t adopt certain practices, in particular autonomy: A shared nothing architecture in the technical implementation Team autonomy in the organisation Deployment autonomy as part of the process. It’s on this last point that contract tests help us. They’re an excellent complement to component tests when testing microservices. Indeed, contract tests make it possible to verify that the contract between producer and consumer is always the one defined between the two parties: On the producer side, we verify that we’re providing the service that consumers expect to receive (the implementation of the contract). If this isn’t the case it’s because our interface has changed, intentional or not. If it is unintentional, the test will fail and we avoid deploying a service that might cause our consumers to crash. If it’s intentional, then it’s necessary to communicate the change to our consumers and increase the service’s version. On the consumer side, we check that the contract hasn’t changed. If it has changed, we check with the producer to identify the change and adapt our code and the associated Component tests. We frequently hear about Consumer-Driven Contracts tests, a pattern that aims to construct contracts based on consumers’ needs rather than imposing one defined by the producer. The intention is to limit friction as the service evolves (i.e. upstream and downstream compatibility management, versions, etc. generally imposed by the producer), and to better understand how the service is used in reality (i.e. which endpoints and which values are useful). In our example, this is how contract tests will look: Implementation To implement our contract tests, we stay within the Spring ecosystem by using Spring Cloud Contract , there are other frameworks out there, Pact being one of the more popular. Let’s zoom in on these tests: Producer side Maven Configuration On the producer side we’ll need two things: A library to check that our controller is implementing the contract: A plugin which will generate tests from the contract and generate stubs to be used on the consumer side: The baseClassForTests is a parent class that will be used by all our generated tests, we’ll come back to it later. Contract definition Our first step is to create a contract describing the request and response (in yaml format or with a DSL such as groovy). The following example is simplistic, and it’s possible to do much more advanced things (including with regular expressions and other features, we encourage you to read the docs ). We’ll store this contract on the producer side in the directory src/test/resources/contracts, with the response body also stored in a file: At this point, if you run Maven the mechanism will come into place, at least in part. The test class will be generated (in target/generated-test-sources/contracts): But the test will fail. Contract tests’ base class To make the test pass, we need the basic class we referred to above to initialize a context: There’s a lot going on at this level: The primary objective is to provide what the Controller needs in order to answer the test, so we provide a MockMvc environment with RestAssuredMockMvc. We could settle on the last line, but in this case the Controller would use a real implementation of the Service which in turn uses a real client which would then call the real transport API. To avoid this and provide more isolation, we’ll do what we did earlier, and provide a MockBean that simulates the Service’s response by deserializing a json file, which is the same file used in the contract. It’s generally be necessary to create as many base classes as you have Controllers to simulate their behaviour. The maven configuration is somewhat different in this case. At this point the producer-side tests should pass, and if you’ve run mvn install or deploy, the jar containing the stubs should be published in the Maven repo, be it local or remote: Installing connection-lookup/target/connection-lookup-0.0.1-SNAPSHOT-stubs.jar to ~/.m2/repository/ch/octo/blog/connection-lookup/0.0.1-SNAPSHOT/connection-lookup-0.0.1-SNAPSHOT-stubs.jar Now let’s look at the consumer side: our journey-booking module. Consumer side Maven Configuration Here, we need to depend on the stub generated on the producer side: Moreover, we have to replace the wiremock dependency with the Spring Cloud dependency or we’ll have an exception. If you remember our client test, we used wiremock to simulate a server. Spring Cloud does the same but is based on the self-generated stub, which allows it to be aligned with the producer contract. The test The code is as follows ( gitlab link) : The test is similar to our component tests ( ConnectionLookupClientCompTest ), except that we no longer use wiremock directly. The @AutoConfigureStubRunner annotation configures the stub by fetching the Maven dependency specified by the ids attribute (in groupId:artifactId:version:classifier format) and exposing it on port 8090. By using “ + ” as the version number, we’ll always take the most recent version which makes it possible to verify that our test always passes despite the evolving producer. On the day the test fails, we’ll know the contract has changed, logically before the producer has put the new version into production. Contract test or component test? As we’ve just seen, the contract test is nothing other than a component test, but it also has the advantage of validating that the producer and consumer are aligned. Our recommendation is to always use contract testing, at least in a controlled environment (typically microservices within your company), but this doesn’t make sense when we’re using an open API (in our case the transport API), in this case we prefer component tests. In both cases, I consider these tests sufficiently important, and fast enough, to run to be integrated into a continuous integration pipeline, being run again and again with the objective of getting rapid feedback. Contract tests are often associated with the Consumer-Driven Contracts pattern, and are an excellent way to verify that service consumers and providers (whether via REST or via messaging) are aligned on a common, shared contract. They also have the advantage of running fairly quickly (because of the insulation provided by wiremock) and can therefore be integrated into the continuous integration pipeline. In the next article, we’ll discuss much less straightforward tests to run: integration and end-to-end tests. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-10-04"},
{"website": "Octo", "title": "\n                The Test Pyramid In Practice (3/5)            ", "author": ["Lyman GILLISPIE", "Jérôme Van Der Linden"], "link": "https://blog.octo.com/en/the-test-pyramid-in-practice-3-5/", "abstract": "The Test Pyramid In Practice (3/5) Publication date 27/09/2018 by Lyman GILLISPIE , Jérôme Van Der Linden Tweet Share 0 +1 LinkedIn 0 The previous article detailed the base of the pyramid: unit tests and their role in preventing regressions in our application. But they’re far from being sufficient, and we need to use other types of tests. In this article, we’ll cover component tests. This article originally appeared on our French Language Blog on 27/06/2018. Component tests Traveling up the pyramid, where we have more integration and less insulation, we reach component tests. Their purpose is to validate our component and its boundary, like integration tests, but to avoid integrating with external dependencies that may be offline or nonfunctional. Component tests live at the crossroads of unit tests — because of their isolation — and integration tests — because they focus on intra / inter component interaction. What to test? This type of test makes it possible to validate the Spring configuration (all the annotations, the application.properties/yml file, the injection of dependencies, etc.) and to ensure that all of the objects in our component integrate correctly. This also includes the interfaces where the component interacts with the outside world. Typically we’re looking to answer questions like: Does the Controller correctly expose our endpoints? Is our database configuration correct? We excluded the Repository from our unit tests, does it work? We also excluded the Client, is it annotated? Implementation Spring Controller While it’s true that we unit-tested the controller individually, we aren’t sure that the Spring configuration is correct. We’ll add tests to verify that the URLs are correct, the Service is well injected, the validation works, and the HTTP errors are managed. We’ll use the Spring MVC Test Framework (MockMvc for those in the know) which allows us to simulate incoming HTTP requests; the framework itself is based on mocks for the Servlet API. Below an excerpt from the test ( Gitlab link ): There are some important things to be aware of in this excerpt: The @WebMvcTest annotation is somewhat lighter-weight than @SpringBootTest: rather than completely auto-configure, it will only configure the MVC part. And, to further reduce scope, we specify the controller we want to load and test. This annotation allows us to use MockMvc, which will: Simulate HTTP calls to validate our URL mappings, Validate the input parameters (if you used the validation API) Check the response that the controller makes (status, body, headers, etc.). The @AutoConfigureJsonTesters annotation allows us to inject JsonTester utility classes to handle json, and, in our case, to serialize objects. Though an ObjectMapper would have been sufficient. We use the @MockBean annotation to create a Service mock and replace the same type of Bean in the Spring context; it allows us to be isolated like we were in our unit tests. Although it’s a mock, it still lets us verify that the Service is correctly injected into the Controller by Spring. These tests will seem very similar to unit tests, but they validate something quite different. In particular, notice that if we compare the two ( TU , TC ), we can see that the component tests validate the glue around the Controller (i.e. HTTP, deserialization, validation) materialized by the annotations, while the unit tests validate the “business logic”. Database Regardless of the environment, testing with a real database presents its own set of problems: schema versioning, sample datasets, cleanup, network latency, or unavailability of the database. To mitigate these, we can test using an in-memory database ( H2 ) in lieu of the Postgres database used by the application. This gives us more control over the schema and test data, and, as an added benefit, db access time is also much faster. Spring Boot provides a mechanism to auto-magically configure an in-memory database: the @DataJpaTest annotation. When associated with the right Maven dependency (H2, HSQL, Derby), Spring will configure a Datasource and an EntityManager, without you having to provide a single line of code or configuration. The objective here isn’t to test the Spring framework, but to test the database insertions. It’s also relevant to test custom methods (i.e findByFromAndTo) and those annotated by @Query. Note : Spring also provides data access utility classes, typically DataAccessUtils used below. Note 2 : While DataJpaTest is useful for Repository testing, don’t use it for everything. For example, unit test your Services by stubbing the Repositories, there’s no need for Spring or an in-memory database. The same is true for Controller tests. Be aware that Hibernate will be initialized with each test, which will take longer when you have more entities in your model, and keep in mind the value of speedy feedback. Excerpt from the code ( Gitlab link ): I can see the sceptics coming from here: “Sure, but H2 isn’t my database, the syntax is different. These tests have no value, they might work with H2 in test and crash in prod.” This isn’t completely false. Even if H2 is the generally recommended solution, it’s likely that you use features specific to your RDBMS (for optimization reasons or simply because it does not fully respect SQL standards). If this is the case then H2 won’t meet the need. A somewhat heavier option is to start a Docker container containing your test database during integration tests, this can be done automatically using the Testcontainers library. To do this, you will need the following dependencies: On the code side, and in order to demonstrate the two ways of doing this, we’ll use a profile (“testpg”) and an application-testpg.yml file containing the following configuration: At the code level ( Gitlab link ), we simply activate the profile in question. Apart from the annotations, the rest of the code is identical to the H2 test. When running the test, the framework will use Docker to retrieve a Postgres image and start it. You should see logs similar to this : 2018-06-03 12:44:15.992 INFO 71125 --- [main] ???? [postgres:9.6.8] : Creating container for image: postgres:9.6.8\r\n2018-06-03 12:44:16.153 INFO 71125 --- [main] ???? [postgres:9.6.8] : Starting container with ID: ea2d833e843ef8adc78da779e6b3c62b31e4c1fe8bf851ed82f1bae23f86d0c8\r\n2018-06-03 12:44:16.629 INFO 71125 --- [main] ???? [postgres:9.6.8] : Container postgres:9.6.8 is starting: ea2d833e843ef8adc78da779e6b3c62b31e4c1fe8bf851ed82f1bae23f86d0c8\r\n2018-06-03 12:44:21.361 INFO 71125 --- [main] ???? [postgres:9.6.8] : Container postgres:9.6.8 started Execution is a little slower – 600 ms once the image is downloaded, versus 400 ms for the test with H2 – but it’s still reasonable if using an in-memory database isn’t possible in your context. HTTP Client The problem is similar when testing the HTTP client: we don’t want to send requests to the real service, to avoid latency, unavailability, or other unwanted behaviour. To remedy this, we’ll use Wiremock to provide stubs for the HTTP calls. WireMock is configured through a JUnit rule, then used to create a stub for the URL we’re interested in. You can simulate all sorts of things: both the request and the response, and importantly the body of the response (here from a file located in src/main/resources/__files/ ). Below the code ( Gitlab link ): Note: If you only use the @SpringBootTest annotation without parameters, Spring will instantiate the entire context based on the @SpringBootApplication annotated class. It will auto-configure the database (datasource, entityManager, etc.), which, in the case of client testing, will be unacceptably slow. To avoid this, we precisely select the classes that interest us and, in addition, we choose the AutoConfiguration elements useful for the test. This may seem verbose and complicated, even superfluous for some tests, but when you have several tens or hundreds of tests, the time saved is counted in minutes. Unfortunately, the problem with this type of test is that it will continue to work the day the API changes interface, wiremock doing its doubling job perfectly. To remedy this we can implement integration tests, but there fall on the opposite side: the test will fail as soon as the API is not available. The contract tests allow us to answer this problem, and we’ll see how in the next article. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-09-27"},
{"website": "Octo", "title": "\n                The Test Pyramid In Practice 2/5            ", "author": ["Lyman GILLISPIE", "Jérôme Van Der Linden"], "link": "https://blog.octo.com/en/the-test-pyramid-in-practice-2-5/", "abstract": "The Test Pyramid In Practice 2/5 Publication date 20/09/2018 by Lyman GILLISPIE , Jérôme Van Der Linden Tweet Share 0 +1 LinkedIn 0 In the previous article , we discussed the theory of the Testing Pyramid — a testing strategy to ensure our application’s quality at a reasonable cost. Notable, we discussed the notion of feedback, and the importance of having fast, accurate, and reliable feedback. Unit tests typically address these criteria for a modest investment. Through this article we’ll develop a concrete example to explore the use of automated unit tests and try to answer some of our readers’ recurring questions. This article originally appeared on our French Language Blog on 26/06/2018. Application “The difference between theory and practice is that in theory there is no difference between theory and practice, but in practice there is one.” Jan Van de Snepscheut Let’s move to the practical part. To do this, and to complete our overview of tests, we will take the example of microservices. Of course this choice isn’t entirely random: microservices are intended to be as autonomous as possible (team, coupling, deployment, etc.) and this autonomy is enabled through testing: integration and end-to-end tests are not entirely appropriate if we want to continuously deploy our service independently of others. Example The following diagram succinctly describes the architecture of our example: We’ve decided to create a set of services to search and book trips by train, but rather than use the API from the France National Railway company, SNCF, we picked a Swiss Open API available on: https://transport.opendata.ch/ . The latter will provide us the routes and schedules. The Connections Lookup service is a facade over this API which allows decoupling from this external service. Our interest in this article is more educational, but we will come back to it. And finally the heart of the system, the Journey Booking service is in charge of searching for routes and recording them in a database. The endpoints are: GET /journeys/search?from=...&to=... allows searching for available routes, though not booked trips (this is the entry point for the Lookup service). GET /journeys gives the list of all reserved trips GET /journeys/{id} gives the trip whose id is passed in the query POST /journeys allows you to book a trip PUT /journeys/{id} allows you to change a trip reservation DELETE /journeys/{id} deletes the trip whose id is passed in the request The last 5 endpoints will interact with a database (in our case, Postgres). Our Booking microservice is structured as in the following diagram. It is very standard, the example being simple and the business logic minimal. It would be reasonable to do everything in the Controller, but for the sake of the example we’ll keep our Service layer and see where it leads us. From a technology point of view, we’ll use a standard: Spring and its ecosystem. There are many tools available to test Spring and it’s good to be clear on what to use and when. The complete project is available on gitlab . Unit tests We’ll start at the base of the pyramid with unit tests. A unit test aims to validate a single behaviour (i.e. a method or a subset of a method) resulting from a business use case in isolation from the rest of the world: other objects: instantiation, attributes, parameters, etc. other systems: a database, a web service, system time, etc. other tests: order of tests, test data Some will say it isn’t necessary to isolate everything. In Working effectively with Unit Tests Jay Fields introduces the notions of social tests and solitary tests. Personally, I’m in favor of isolating as much as possible to avoid any interference. For simplicity, our unit tests are independent of any external input/output, i.e. databases, file systems, networks, etc. To do this, we use what some call plugs, others stubs, mocks, or fakes — what’s called a Test Double in the literature. It’s an object that we completely control which will stand in for a dependency of our object under test. It allows us to validate different behaviors depending on values returned from the double — for example the happy path, edge and corner cases, and errors. While it’s possible to develop test doubles by hand, there are also many libraries available that simplify their implementation: Mockito , EasyMock or JMockit are the most known in the Java world. What to test? If we look at our previous schema, we would unit test each of the objects that make up our component: In truth, since the Client is implemented with the Feign library, there is no real code to test: Likewise for the Repository part, which is based on Spring Data and therefore has no code: We will come back to these two elements in our integration tests since our objective is not to test the underlying frameworks, which are already well tested elsewhere. So, we now have the following schema: Here is an excerpt from the Service ( Gitlab link ): And an excerpt of the Controller ( Gitlab link ): As we said before, the Controller is almost a simple utility layer, almost. Utility layers A common question that many readers ask us is “ Is it worth it to test an utility layer? “, which we answer with another question “is it worth it to have this utility layer?” . Often these layers are only to enforce a layered pattern, and have no purpose other than to be there “just in case”. In general, the practice of TDD (Test Driven Development) helps us to avoid this. Without going into the details of a practice that would be worth a complete article , TDD aims to specify expected behavior via a test before actually implementing it. So we first write a test and then the simplest possible code that allows the test to pass and therefore satisfy the specified behavior. This avoids over-design and “just in case” layers, and focuses on the simplest code that provides value quickly. In our example, though the controller seems to have little code, it still has two responsibilities: to expose Data Transfer Objects (DTOs) instead of entities and to expose the API via the use of annotations. The code (though minimal) will be tested individually and we will test the exposure (url mappings, error code management, etc.) in the component tests. Private methods Another recurring questions among our customers is “ is it necessary / how to test private methods? “. The extreme answer is “no”: If you do TDD, private methods only appear after the refactoring step ( red / green / refactor ) and are therefore indirectly tested through public methods. A more pragmatic answer is “no, but”: on legacy code, testing private methods can be a short-term way to put a test harness around a class before refactoring it (i.e. to reduce complexity, too much responsibility, too many dependencies etc.). Spring provides a utility class ( ReflectionUtils ) to simplify the writing of such tests. In the long term, after refactoring, these tests should be removed and replaced by public method tests. 100% coverage or nothing With tools such as Jacoco , Cobertura or Clover , it’s possible to determine how much of our code is reached / covered when running tests. Beyond this simple indicator, these tools allow us to see where the tests have passed and, especially, where they’ve failed. We can then check whether critical paths of our application are or are not tested. We should take care when relying on code coverage as an indicator, because it can be misleading: it’s certainly possible to execute 100% of the code without testing anything (by not asserting anything, for example). Don’t aim for 100%, instead begin by focusing on the critical parts of the application, and keep track of the your code’s coverage trend. Is it increasing? Decreasing? If you want to go further, it’s possible to apply mutation testing (also known as chaos-monkey testing), which modifies the business code, more or less randomly, and verifies that a test fails. If the tests continue to pass, it’s likely that t don’t effectively validate the code. The Pitest framework can automate this in Java. For example, the following report indicates that JourneyService (after removing all assertions) is entirely covered by tests, but these tests score rather poorly with respect to mutation coverage. Example of “incomplete” test: And the associated report: Implementation of unit tests We’ll use JUnit, AssertJ , and Mockito to implement our tests, notice that there’s no Spring at this level of the pyramid. Here’s an excerpt from our tests for the JourneyService ( Gitlab link ): Several things to note in this code: The test methods have explicit names . If a test fails, we’ll know very quickly what the source of the problem is. There’s no universal convention, but I advise that you adopt the following nomenclature, which is verbose but unambiguous: unitUnderTest_ Should ExpectedBehavior_ When InitialState We might not respect this naming convention, but test code must be as readable, if not more so, than the business code. So long as it is understandable, the test code documents what your application actually does better than any documentation. To aid readability, you can use the following standard structure in your test code: Preparation of the test environment and initialization of input data. Execution of the behavior you want to test (usually a method). Verification of the results and side effects. Personally, I use some comments from the Behavior Driven Development (BDD) syntax: given, when, then to structure the test. Others use the 3A rule: arrange, action, assertion . The key is to have a well structured and readable code. In the same vein, I use the org.mockito. BDDMockito class which adopts the BDD structure. So Mockito.when is replaced by BDDMockito.given and verify by then.Another important point in this example, Mockito is used both to provide a Stub (in the first two tests) and a Mock (in the third). Without going into details, the Stub is there to replace a dependency and validate that the tested system works. A Mock, on the other hand, allows us to check the interaction of the system under test with its dependencies. We can check that the dependency has been called with the expected parameters. We should pay attention to the use of Mocks in our tests. If we’re not careful the tests can become tightly coupled to our dependency’s implementation, which can quickly become a nightmare to maintain and understand. It should go without saying that these tests must should be run continuously within your build pipeline after every commit to detect regressions as soon as possible. Unit tests validate the business aspects of your application, i.e. business logic and algorithms. They are a security blanket for any code modification — i.e. adding features, refactoring, and bug fixes — and I can not stress enough that they are essential. They are necessary, but not sufficient. In the next article, we’ll discuss component tests, which augment the collection of tests that it is good to have in your toolkit. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-09-20"},
{"website": "Octo", "title": "\n                OCTO vision on Service Mesh : the challenges            ", "author": ["Marc Bojoly", "David Alia"], "link": "https://blog.octo.com/en/octo-vision-on-service-mesh-the-challenges/", "abstract": "OCTO vision on Service Mesh : the challenges Publication date 06/09/2018 by Marc Bojoly , David Alia Tweet Share 0 +1 LinkedIn 0 This year, the Service Meshes are of all conferences: istio , linkerd , kubeflix , even zuul ?…. Many tools are surfing on this wave. What exactly is this about? By comparing our different visions we have the feeling at each new tool of a déjà-vu or a superposition with the precedent ones. As if we were once told: in a world of microservices we need security, traceability, and intelligent routing. We have an HTTP reverse-proxy solution to meet these challenges – with the photo (one-wheeled wheelbarrow) at the top as an illustration. Then a few days later: in a container world where orchestration is more and more dynamic we need intelligent routing, traceability and security inside the orchestrator. And for that we have the solution that fits into our container orchestrator – by showing us the photo at the bottom (two-wheeled wheelbarrow). Let us try to see more clearly through this series of articles. The first will attempt to position the issues that lead to considering the Service Mesh in the ecosystem of microservices. The second will provide an X-ray of the functionalities offered by the tools claiming to be a Service Mesh and the related functionalities of the microservices and containers ecosystem. Then, we will discuss different solutions that implement these features, for example an application implementation in Java. Microservices, containers and platforms Today, the challenge is to design modular and scalable architectures to meet the stakes of the digital world. The principle of single responsibility has now become widely established as the state of the art to meet these challenges. It consists in breaking down an application into a set of components and services, each of which meets a specific need. Microservices are the most common implementation of this principle.  If we use Sam Newman’s definition in “ Building Microservices “, “microservices are small autonomous services that work together”. As a target, the developer focuses only on the implementation of a basic business functionality, with maximum freedom (choice of technology, weak coupling to other services, etc.) and possible responsibility (continuous deployment, you build it you run it ). Inseparably, DevOps principles have imposed themselves to provide developers with a foundation – a platform, we will come back to it – that offers freedom and responsibility right through to production. The implementation of this principle is based in particular on the infrastructure as code and on cloud solutions characterised by instantiation and payment on demand of various IT resources. Containers and container orchestrators are one of the most prominent solutions today to implement these principles. They package the applications – and therefore the services – in the form of stackable and immutable images, which makes them portable and relatively independent of the platform. As a developer, I will focus on writing business features. These will be exposed as an API, then packaged in containers and deployed in the cloud. But is that enough to make an application? No, because many other features are required to transform a set of software components into an application. How can they be coordinated to interact with each other? How to ensure security? How to make the API usable by front-end applications (JavaScript screens running in the browser)? How to monitor they are properly running? For a long time now, these transversal problems have been the domain of platforms . In English, a platform means a dock, something flat, homogeneous, in order to allow standardisation, to promote industrial activity. A port platform will thus be built around container standards and the Bill of Lading . Thus all the complexity of the transport is transparent for the sender and the recipient. Figuratively speaking, in the world of microservices and containers, the platform serves as a foundation to promote reuse and to free the developer from the implementation of non-differentiating transversal functionalities for the implemented business . What platforms are we talking about? In the world of the cloud, the term Platform As A Service appeared very early. These platforms – from which docker technology is indirectly derived – aim to transparently manage the infrastructure and deployment of an application.  In the application world, application servers have in the past been the platform offering software frameworks , security and exposure of these components. More recently, API management tools have provided reusable solutions for managing the API lifecycle and self-service security . Today, the increasingly evolving nature of microservice architectures makes the centralised vision of these different approaches obsolete. New platforms are therefore emerging to propose a foundation that embraces the decentralised principle of microservices. Where PaaS focuses on deployment, API Management on API, Service Mesh focuses on communication between microservices . Our definition will therefore be as follows: Different implementations exist as Thoughworks indicates in their radar . But the implementation of the best known solutions tend to hide other possible solutions. So is it simply an improvement of an existing tool like this two-wheeled wheelbarrow? Or is it a truly differentiating approach such as switching from axe to chainsaw that requires using the new tool in a completely different way? The next articles will try to give you the keys to compare Service Meshes to your context and your needs. Sources of illustrations: https://pixabay.com/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-09-06"},
{"website": "Octo", "title": "\n                The Test Pyramid In Practice (1/5)            ", "author": ["Lyman GILLISPIE", "Ali-Asghar Houssein", "Jérôme Van Der Linden"], "link": "https://blog.octo.com/en/the-test-pyramid-in-practice-1-5/", "abstract": "The Test Pyramid In Practice (1/5) Publication date 13/09/2018 by Lyman GILLISPIE , Ali-Asghar Houssein , Jérôme Van Der Linden Tweet Share 0 +1 LinkedIn 0 If you read this blog or our publications, you know how much testing is tied to software quality and, if I may say so, to software success. I insist on this point because all too often our customers treat tests as the fifth wheel when it comes to development. You know the consequences: an astronomical number of design anomalies, pernicious bugs in production and, worse still, software which ossifies little by little. This article is the first in a series, and will mostly address theory. In the following articles we will delve into practice, applied to Java and Spring. This article originally appeared on our French Language Blog on 25/06/2018 . Foreword Sometimes we decide to test and we manage to convince the “upper echelons” of the benefit and the need to take time (and therefore money) testing, but: “ We won’t make it with unit tests, our code is too complex/coupled/etc. “ “ Integration testing is complicated to set up, so we may as well deploy the entire application. “ “ The best thing would be to have tests that emulate to what our user does! And it’ll be easier to test. “ And then we deploy the heavy artillery: end-to-end tests, usually Selenium-type graphical interface tests: “ This’ll be great! We’ll validate the whole application in a few tests, and we’ll even be able to show the reports to the business and the testers if we put it in Cucumber or Serenity. They’re gonna love us ????.” But, after a few weeks or months we begin to realize that this may not have been the best idea: The tests are slow: “ 4 hours for 150 tests “, They fail intermittently: “ Why is it red? Probably Selenium, restart it to check… “, Even worse, standing up a complete testing environment with stable datasets is a headache at best and impossible at worst Then we invest even more because we say to ourselves that we aren’t missing much, and it’d be a shame to throw everything away now. And yes, things can be improved , but at what cost? And for how long? Overall, the “black box” testing strategy is neither the most effective nor the most cost-effective. I can see more than one grin forming as you reading this, but rest assured: you are not alone. Testing Strategy Before I tell you more about the Test Pyramid, let’s recall some criteria that are important to consider when thinking about a testing strategy. And, in order to resolve an all too common misunderstanding, in this article we’ll exclusively talk about automated tests. Feedback A test, whatever it is, has no other purpose than to give you the feedback: “Is my program doing what it is supposed to?” We can judge the quality of this feedback by three criteria: The accuracy of feedback . If a test fails, can we determine exactly which piece of code does not work? How long does it take for a developer to identify the piece of code that fails the test? The more granular a test is (at the method level), the more accurate the feedback will be. On the other hand, how do you know if a database access or a JavaScript error causes an end-to-end test to fail? Source : Culture Code, OCTO Technology Reliable feedback . The repeatability of a test is paramount. Can we trust a test whose results vary from one execution to another, without any apparent modification to the code, configuration or dependencies? Once again, end-to-end tests have an unfortunate tendency to explode for obscure and often uncontrollable reasons: network latency, a garbage collection event that slows down the JVM, a misbehaving browser, a deactivated account, a modified database schema etc.  LinkedIn engineers concluded that unstable tests are worse than no tests after calculating that they had a 13.4% chance of having a stable build with 200 tests and a 1% probability of failure on each test. “Stop calling your build flaky. Have you ever released to production when, say, your search functionality “sometimes works”?” Pavan Sudarshan, ThoughWorks Speed of feedback . Many of us had yet to be born when punch-cards were used for programming, the glorious, but thankfully past, era when it took hours or even days to know if a stack of cards “compiled” and then start over when it didn’t. It’s no longer acceptable to wait so long before knowing if our code compiles (it’s nearly instantaneous in the IDE), and the same is true for tests: the sooner you know if a test fails, the sooner you can identify the problem, and the cheaper it will be to fix : Source : Code complete, 2nd edition, by Steve McConnell [2] There’s even a virtuous cycle when the tests are fast: they’re easy to run [3] and build confidence in your code, prompting you to write even more. A good testing strategy therefore aims to maximize the number of tests that meet these three criteria: accuracy, speed, and reliability. Ease of creation and maintenance cost Knowing that you’re unlikely to have an infinite budget, your testing strategy will necessarily depend on it. You need to weigh the cost of different types of test against the value they provide, in other words evaluate their ROI. Unit tests are usually very simple to implement, provided you don’t wait until the code is crippled by technical debt. Because they operate in isolation, the environment is relatively simple to set up via stubs or mocks. Because they are very close to the code, it’s natural to maintain unit tests by refactoring. Integration tests are quite simple too (here we’re talking about integration tests within an application, not between applications). We overcome the major constraints imposed by end-to-end tests (i.e. the presentation layer and some dependencies). They’re also fairly close to the code which simplifies refactoring. However they cover a broader spectrum of code and as a result are more likely to be impacted by changes to the application code. End-to-end or GUI testing is more complex to implement because it requires a complete environment to be deployed. The presence of dependencies and datasets form a puzzle, complicated by instabilities due to environment reloads, changes by other teams, etc.These complications make this type of test very fragile and introduce maintenance costs in the form of build error analysis and the maintenance of the environment and data. The following table summarizes the main criteria for choosing the type of test to be used: From this perspective, we might say “Great! I only have to do unit tests”, but other types of tests exist for a good reason: unit tests can’t validate everything. The Testing Pyramid We now come to the famous Testing Pyramid, first described by Mike Cohn in his book Succeeding with Agile , which helps immensely when defining a testing strategy. We invest heavily in unit tests which provide a solid foundation for the pyramid by giving us fast and accurate feedback. Paired with continuous integration, unit tests provide a safeguard against regressions, which is essential if we want to control our product in the medium to long term. At the top of the pyramid, we reduce our end-to-end testing to a minimum: for example validating the component’s integration into the overall system, any homemade graphic components, and possibly some smoke tests as acceptance tests. And, in the middle of the pyramid, integration tests allow us to validate the component (internal integration) and its boundaries (external integration). The principle is again to favor internal component tests which are isolated from the rest of the system, over external tests which are more complex to implement: This wraps up our discussion of the theory. In the next article we’ll deal in depth more with the base of the pyramid: unit testing and putting it into practice on a Java / Spring project. http://www.ambysoft.com/essays/whyAgileWorksFeedback.html ↑ Code Complete, 2nd Edition, by Steve McConnell ↑ https://infinitest.github.io/ ↑ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-09-13"},
{"website": "Octo", "title": "\n                Cold start / Warm start with AWS Lambda            ", "author": ["Erwan Alliaume", "Benjamin Le Roux"], "link": "https://blog.octo.com/en/cold-start-warm-start-with-aws-lambda/", "abstract": "Cold start / Warm start with AWS Lambda Publication date 28/08/2018 by Erwan Alliaume , Benjamin Le Roux Tweet Share 0 +1 LinkedIn 0 The serverless brings many benefits for the deployment of web applications such as autoscaling, availability and having a very fine granularity on costs (billing per 100ms for AWS lambda). And of course the absence of server management (installations, patches,…). This article makes an inventory of the cold and warm start call metrics with AWS Lambda with different code implementations. Serverless is an ambiguous term that implies that there are no more servers: this is not the case! The term adapted could have been server[management]less but it would have been less attractive from a marketing point of view. In reality Serverless encompasses 2 different and overlapping notions: It is used to describe applications that incorporate many third-party services to manage backend logic and state management. These services are also known as Backend as a Service. It can also designate applications where server logic is coded by developers but, unlike traditional architectures, is executed in a stateless, ephemeral container which entirely managed by a third party. Cold start / warm call with AWS Lambda To understand what cold start is and how it impacts FaaS code, you need to start by looking at how Function as a Service works. This article will focus on AWS Lambda. Chris Munns’ talk at AWS re:invent 2017 gives a good introduction to this topic. Here are the main steps to execute a Lambda. First, you have to create and configure the Lambda: at this step, the code and configuration to execute a Lambda are simply stored somewhere on s3. Then, during the execution of the lambda, a container is mounted with the resources defined by the configuration and the code to be executed is loaded into memory. What we call “cold start” is the time spent to do this task of Lambda initialization. This diagram shows that part of the cold start process is managed by AWS (code loading, container creation). Because of that, the possibilities for a user to optimize the cold start are a bit limited. The initialization time of a Lambda represents a significant part of the total time. After a cold start, the Lambda will remain instantiated for a while (5 minutes) allowing any other call not to have to wait for this initialization to be done each time. Calls made during this period are called “warm call”, which means that the code is loaded into memory and ready to be executed when the Lambda is called one or several times. Cold start execution has a direct impact on the code execution time of an application. Is this impact significant? Are there ways to minimize it? Impact of cold start on a basic code To get an idea of what is the impact of a cold start on the execution time, we did a comparison with 2 really simple Lambdas just returning a “Hello world”: one written in Java, the other in NodeJS. The memory resources allocated for each Lambda were 128MB. Here are the results of this test: Here are some observations: First, in both technologies, the cold start increases the execution time of the Lambda. Note also that the language used has an impact on the execution time during a cold start. Generally, Java and C# are slower to initialize that Go, Python or Node. The container mounting part is optimized by AWS and does not appear on these graphs, which suggests that Java is much slower than Node to run a “Hello world”. The Java code to be executed is contained in a fat jar, this means that some time is consumed browsing the code in the jar and loading the JVM. On the other hand, as Yan Cui demonstrated in his article Comparing AWS Lambda Performance , the poor performance of Java and C# languages during the cold start should be readjusted with their good performance during warm calls. In addition, the size of the resources allocated to the Lambda has also an impact on the execution time. A Lambda could have different memory configuration that can vary from 128 to 3008 MB. On top of that, CPU resources are also to the choice of memory resources, for example, a 256MB Lambda will have 2 times more CPU than a 128MB Lambda. In a very synthetic way, by increasing resources, the execution time should decrease, but this is not so easy to check. In the simple example of the code returning a “hello world”, this is true for Java code but it is less obvious for Node code for which there is almost no gain between 512MB and 1024MB. To finish, costs also increase according to resources, which requires a study of the price/time/quality ratio of the service. AWS has put online a tool to estimate the cost of Lambda calls for you to be able to choose the good configuration according to your needs. Impact of cold start on an application Measuring a “hello world” code allowed us to get some metrics about the execution time and give us quick wins to try to optimize it. But let’s dig a bit more in this part, running our tests against a more elaborate code. The code used to make these new measurements is an application that takes in an HTTP request and then reads a database to return the read data from this database in response. The code is available here . Here is a diagram to visualize the application components and their interactions: This code has been implemented in two different languages: Java and Node, and with different frameworks: springboot , springcloudfunction , express , vanilla (no framework). What are the takeaways from those tests? Adding a framework to structure the code deployed in the Lambda increases the execution time. You can notice a small negative effect using interpreted languages like javascript: 120 ms for Node against 180 ms for Express. For languages like Java, this is another story. The cold start has more impact on Java code and this has repercussions on the use of a framework until it is very penalizing for the user with cold start times greater than 10 seconds. The comparison between different frameworks shows that using a serverless oriented framework like spring cloud function allows reducing the execution time compared to a web framework like springboot. About warm calls behaviour, the use of a framework has only a non-significant impact on execution time. Measurements show that on average a warm call is executed in 30 milliseconds with small variations of about 10 milliseconds. Switching from a simple code to a web application results in a significant increase in execution time. The question that may arise is where does this time go? Here are some additional measures to provide answers. By measuring the initialization time of SpringContext during a cold start and during a warm call, it appears that in the case where SpringContext is started and executed in an already initialized Lambda, the framework starts faster. It is therefore not the framework itself that penalizes the cold start, but rather the initialization of the JVM that is linked to the size of the jar deployed in the Lambda. This also explains the low impact of express in the case of a javascript application. Even if Spring is not slow to run, adding libraries (spring-boot-starter-web, spring-data-dynamo, etc.) makes everything longer to initialize. A possible optimization would be to clean the dependencies to keep only the essential, which is not so easy to do for a developer using spring boot. After viewing those metrics, a conclusion can be done: cold start should be avoided in an application intended for users, especially when the application is developed in a language such as Java. Is there a way to keep warm your lambdas Cold start is a drawback of implementing a serverless application since it induces a waiting time with the first call for the user. A workaround has therefore been implemented in serverless applications to prevent users from suffering too much from cold start, this technique can be called “keep warm” (or “keep alive” depending on the articles). The cold start is due to the fact that the Lambda has a fixed life of 5 minutes, which means that beyond 5 minutes the Lambda is no longer lit. Therefore, after 5 minutes the Lambda must be initialized again. The principle is simple, try to prevent the Lambda from being unloaded. For that, we could set up a cron which invokes this Lambda at regular intervals so that it remains used from the AWS perspective. This keep warm technique seems to answer the cold start problem, but some limitations linked to AWS Lambda must be taken into account. The first is that the Lambda will be reset every 4 hours even while doing keep warm. The second is that AWS autoscaling must be taken into account. Keep-warm and concurrent calls One of the advantages of developing serverless applications is its automatic autoscaling feature. This means that in case of concurrent calls, the load will be spread over several Lambdas instances. But each new Lambda initialized corresponds to a longer waiting time for the user because of the cold start. Here is a simple example that illustrates the case of concurrent calls (this example does not represent the actual operation of Lambdas autoscaling, but serves to understand its behaviour). One way to avoid cold starts in case of concurrent calls is to create a pool of Lambdas that are kept warm to be able to handle concurrent calls without the need to have a wait for a new instance of the Lambda to be created. The same example as above with a pool of Lambdas and keep warm gives a predictive response time. Making pools of Lambdas instances (1 lambda deployed several times) with keep warm mechanism allows to decrease the probabilities for a user to undergo a cold start during concurrent calls. But this technique has a disadvantage, if there are no or few concurrent calls, Lambdas are kept on while they do not perform any treatment. It is necessary to determine the optimal number of Lambda per pool in order not to keep Lambdas on unnecessarily. In addition, this at a cost, keep warm consisting of making calls on a Lambda at regular intervals, each call will be charged. In the case of a pool, the costs are multiplied by the number of Lambdas that constitute it. But it stays negligible comparing to the price of other infrastructures such as EC2 or Fargate containers. The following graph shows that for a small application (with less than 5 million requests per month) the use of Lambda remains advantageous. Don’t forget that pool of Lambda is not a feature provided by an AWS, you will need to put in place some tricks to be able to try to keep a pool warm without really knowing how many instances are deployed. Source: pricing AWS Serverless frameworks and monitoring tools The keep warm mechanism appeared as a bypass to keep Lambdas on by triggering events with another Lambda. This requires creating another Lambda that will be in charge of making these calls or creating a CloudWatch event for each Lambda to keep on. Since the beginning of this technique, the frameworks and tools around the serverless have proposed their own way to implement keep warm so that it requires little configuration and code to write to set it up. For example, thundra.io which is a monitoring tool for AWS Lambda allows to have metrics and tracing on deployed Lambdas. It also has a keep warm feature to limit the number of cold start in a serverless web application. Thundra is not the only tool/framework that allows implementing keep warm, it also exists with serverless and Zappa . As described in this article , keep warm is not intended to avoid all cold starts but to minimize its impact in the case of a web serverless application. Thundra’s approach to keep-warm requires playing on 2 parameters that are: the number of Lambda in the Lambda pool in keep-warm and the time lapse between calls to keep the functions on. But as said previously, you have no way to control very precisely the number of deployed instances of your Lambda, you just try to keep several instances warm without a good control on how they are really deployed. Source: https://theburningmonk.com/2018/01/im-afraid-youre-thinking-about-aws-lambda-cold-starts-all-wrong/ By default, Thundra will try to create a pool of 8 Lambdas and send an event every 5 minutes to try to keep the pool warm. To create a lambda pool and make sure to keep them on, the Lambda in charge of keeping warm must make calls to the target Lambdas. The Lambda code in the pool must handle keep warm requests differently. In this case, the code triggers a sleep so that the Lambda appears busy and the next request is sent to another lambda in the pool. By retrieving the id of each Lambda, it is possible to ensure that all Lambdas in the pool are kept on. With this configuration, on 5 concurrent calls of 5 requests, Thundra records 5 cold starts while with keep-warm the Lambdas do not suffer it. Here are the details of the invocations with and without cold start using this technique and Thundra tool. Finally, cold-start has an important impact on serverless applications for users. Cold-start can be minimized thanks to the keep warm technique but it still requires efforts from developers to optimize this technique. The tools increasingly integrate the implementation of keep-warm in an automated way. By writing this blog post, those solutions are more workaround than a real solution. Nothing is really mature around pools of lambdas. On the other hand, the ecosystem is growing very fast, there are new tools every day or so to help you working with Lambda. So we could expect more sustainable solutions in a very soon future. To conclude, and as a personal note, if you need a fixed size of instances of lambda as we just described, it might be more interesting to take a look at other runtimes especially if you already have many users. If you use Kubernetes for example (managed by cloud providers or not), you will be able to fine-tune the autoscaling mechanism to have a minimum/maximum of containers of a same deployed code, configure your CPU and memory resources… References: Articles: https://martinfowler.com/articles/serverless.html https://read.iopipe.com/understanding-aws-lambda-coldstarts-49350662ab9e https://theburningmonk.com/2017/06/aws-lambda-compare-coldstart-time-with-different-languages-memory-and-code-sizes/ https://medium.com/thundra/dealing-with-cold-starts-in-aws-lambda-a5e3aa8f532 https://read.acloud.guru/comparing-aws-lambda-performance-when-using-node-js-java-c-or-python-281bef2c740f https://blog.symphonia.io/the-occasional-chaos-of-aws-lambda-runtime-performance-880773620a7e Videos: Getting Started with Serverless Architecture Become A Serverless Black Belt Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 3 commentaires sur “Cold start / Warm start with AWS Lambda” Cédric 29/08/2018 à 09:45 Very interesting post with useful metrics, I've learned things.\r\nTo go a bit further on the Node.js side, I heard once that packaging and minifying your source code could also improve your start time. Never tried it though but I found some metrics here for example: https://medium.com/capital-one-developers/applying-minification-and-uglification-to-aws-lambda-functions-dbc7ad75241 (25 to 40% faster) Could be worth a look, and it probably also reduces AWS cost... maliniKB 20/04/2019 à 02:08 Excellent article. Thank you! PUNEET BABBAR 03/02/2021 à 03:10 Very well explained with great insights Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-08-28"},
{"website": "Octo", "title": "\n                Software Craftsmanship and OPS scripting: a love story            ", "author": ["Sebastian Caceres", "Tanguy Patte"], "link": "https://blog.octo.com/en/software-craftsmanship-and-ops-scripting-a-love-story/", "abstract": "Software Craftsmanship and OPS scripting: a love story Publication date 16/08/2018 by Sebastian Caceres , Tanguy Patte Tweet Share 0 +1 LinkedIn 0 Alright, story time fellas. TL;DR; We’re working with Terraform, layering and workspaces. This makes the standard `terraform workspace select x` pretty cumbersome and dangerous. We wrote a Ruby wrapper using Test-Driven Development. It allows us to have a tested, maintainable and efficient solution to the aforementioned issue. You can find the actual project here: https://github.com/sebiwi/terraform-wrapper If you keep reading, we’re going to talk about scripting, Test-Driven Development, Terraform and Ruby. In depth: Workspaces If you’ve ever worked with Terraform, you will know that it’s quite a particular tool. (Not so) recently, they started applying the concept of Workspaces. This is pretty interesting, as it helps you maintain many different infrastructure sets using the same code. Our first impression when we started using Workspaces was that the name itself (Workspaces) wasn’t clear enough. If you have the same feeling, please read the following paragraph from the Terraform documentation: In the 0.9 line of Terraform releases, this concept was known as “environment”. It was renamed in 0.10 based on feedback about confusion caused by the overloading of the word “environment” both within Terraform itself and within organizations that use Terraform. So that’s that (shrug). So why should we use Workspaces in real life? Let us take a look at some standard Terraform code. Whereas before you would have something like this: environments/ ├── dev │   ├── dev.tf │   ├── dev.tfvars │   └── variables.tf ├── prod │   ├── prod.tf │   ├── prod.tfvars │   └── variables.tf └── uat ├── uat.tf ├── uat.tfvars └── variables.tf Now, you will have something like this: terraform/ ├── infrastructure.tf ├── dev.tfvars ├── prod.tfvars ├── uat.tfvars └── variables.tf With all the necessary ` dev `, ` uat ` and ` prod ` Workspaces. You will still need to create the Workspaces using Terraform itself, like so: tf workspace new <workspace> You can list all your Workspaces using the `terraform workspace list` command: $ tf workspace list default dev prod * uat For each Workspace, Terraform will create a new tfstate file, adding the Workspace name as a suffix for the filename or the keyname if using a remote backend. If you don’t use Workspaces, you will still be using the `default` Workspace. This is phenomenal, as it allows us to reuse most of the code written for a single environment, and not to repeat it for every single one of them. In other words, it keeps our code DRY . We’re handling many different environments, so we’re using this feature heavily. Layering Have you ever heard about layering? It’s a Terraform code structuring pattern in which you create “layers”, based on logical stacks. For example, if you are working on a cloud provider, your first stack will probably be the network stack, since most IaaS resources depend on it. Then, you will probably create virtual machines that will run on the network layer. You could call this the compute layer, for example.  At the end you will have many layers in your scaffold, each one containing a specific logical group of resources, that depends completely or partially on the previous layer. How? Using references to the previous layer, using Data Sources . Once again, from the Terraform documentation: Data sources allow data to be fetched or computed for use elsewhere in Terraform configuration. Use of data sources allows a Terraform configuration to build on information defined outside of Terraform, or defined by another separate Terraform configuration. The use of layering has many advantages: Reduced coupling : all your code will not be placed in the same directory. It will be separated into different logical groups. This will help you and your team understand the code better (as you will know that each layer corresponds to an isolated group of resources of your infrastructure), and it will make it easier for everyone to contribute to it as well. For example, adding a new component to your infrastructure is simple: you verify if the component you want to add fits into one of the existing layers, and you add it to it. If it doesn’t, you create a new layer. This also reduces the complexity of the development workflow when many people are working on the same codebase: each person can work on a different layer on different features without destroying the other person’s resources due to the fact that the code that allows their creation is not on their respective branches. Dependency on the previous layer : since each layer depends on the previous layer, you will be able to validate both the code of the previous layer (since it needs to be valid before the creation of the stack that depends on it) and its state (since the infrastructure state needs to be valid in order for it to be used as a datasource for the next layer). Reduced execution time : since each Terraform execution targets less code, the refresh time of the real state of your infrastructure using your cloud provider’s APIs diminishes, which means that the execution of the code is faster for certain steps. Reduced execution perimeter : each Terraform execution becomes atomic to the layer you’re executing it into. This creates a relatively isolated blast perimeter if something goes wrong. Code-wise, we go from this: terraform ├── README.md ├── cloud_init.tpl ├── config.tf ├── core_storage_gitlab_registry.tf ├── network.tf ├── prod.tfvars ├── resource_group.tf ├── security.tf ├── storage_backup.tf ├── storage_registry.tf ├── test.tfvars ├── variables.tf ├── vm_backup_rotation.tf ├── vm_bastion.tf ├── vm_elastic.tf ├── vm_gitlab.tf ├── vm_gitlab_runner.tf ├── vm_jenkins.tf ├── waf_internal.tf ├── test.tfvars ├── dev.tfvars └── qa.tfvars To this: terraform ├── 00_resource_group ├── 01_dns ├── 02_tooling ├── 03_sandbox ├── 04_prod ├── 10_dns_record ├── prod.tfvars ├── qa.tfvars └── test.tfvars Each one of the numbered entries on the new directory structure represents a different directory, each of which might contain many different files. Issues Layering is thus quite useful, it allows you to decouple your code and to keep things simple. When combining this technique with Workspaces, it allows you to have structured code which you can apply to many different environments. Sweet! Note: from now on, we will start talking about environments. An environment is defined by a distinct Terraform Workspace. Don’t get confused! Nevertheless, it doesn’t come without a tradeoff in complexity. You really need to pay attention to the current workspace, on each layer. We can apply changes or destroy resources on a layer using the wrong workspace if we are not paying attention. In order for this to work, you need to select the right workspace in each directory. There is no easy way to be sure on which environment you are working when you go from a directory to another one. You can do a ` terraform workspace list ` each time you change your current directory, but this gets dull quite fast, and you will probably forget to do it eventually and screw things up. The other issue is that in order to apply changes you need to go through every single layer and launch Terraform, and Terraform doesn’t know about the existence of the different layers: each layer acts as an isolated Terraform execution context. Doing this is slow. It is tedious. It can get frustrating quite quickly. So… Solutions The first solution we tried was to display the current workspace in our prompt. This was fairly easy to do, since you can find the current workspace in the .terraform/environment file. Using some bash foo, we managed to display it as so: function terraform_env_name (){ if  [ -f .terraform/environment ] then TERRAFORM_ENV=`cat .terraform/environment` TERRAFORM_MESSAGE=”(${TERRAFORM_ENV})” else TERRAFORM_MESSAGE=”” fi echo -e  “$TERRAFORM_MESSAGE” } When you add this to your PS1 environment variable, your prompt will start looking somewhat like this: ~/terraform/00_resource_group (development) $ But most of the time, this is not enough. It helps you prevent some mistakes but it doesn’t prevent you from screwing things up. And it does nothing about the cumbersome aspect of going through every single layer of the infrastructure and doing ` terraform apply ` commands everywhere. You will probably just go from one directory to another and run your Terraform command from your bash history without checking the selected workspace. That is the reason why we wrote a wrapper. One of the purposes of the wrapper is to check that we are working on the right workspace on every single layer. So we started from what we would have loved to have . We imagined the right tool and then we built it. If we had an improved Terraform, called `tf`, and we would want to apply changes on every layer, on the development workspace, we would love to have something like this: tf development apply If the workspace does not match in any single one of the layers, tf will stop and tell you to select the right workspace before trying to apply your changes. Needless to say, it will also provide you with a way to choose the right workspace on every single layer, so you won’t have to go and do it manually on every directory. In our minds, this would work like this: tf workspace select <workspace> And we would also love to have a way of creating new workspaces easily. This should come in the form of: tf workspace new production With this somewhat flexible specification, we set our hands into coding! Implementation and Test-Driven Development So how did we do it? First, we had to choose a language. After several discussions, we decided to go with Ruby. Why? Because I had never done an operations script using it. Most of the time when I have a task like this one, I default to Python. So it was basically a “getting out of your comfort zone” thing, even though I had already done Ruby on Rails development. There are many other benefits of using Ruby. Handling arrays is extremely easy, and the test framework ecosystem is pretty evolved. Since we were using an interpreted language, doing Test-Driven Development was almost an obligation. It allowed us to be sure of what we were building step by step, and to have a certain level of confidence on what we were doing. Therefore, we completely built the wrapper using it. It’s pretty rad, because you can see in our test file how we progressively added features and our whole thinking process. We had to make some choices. The core functionality of our wrapper was the ability to go through a set of folders in a certain order and apply Terraform commands inside of each one of them. In order to do this, we had two choices: Mock every single directory changing task, knowing beforehand that we actually had to run assertions on the order of the visited directories, which adds complexity to the mocking procedure. Create mock directory structures that correspond to the cases that we’re trying to test. With this strategy, we would no longer be doing Unit Tests, but rather Integration Tests (which is fine, sometimes), as we would be using actual directories on the filesystem. First, we started our code using the first strategy, since it seemed cleaner and stricter. subject { wrapper.get_layers } before do allow(wrapper).to receive(:list_dirs).and_return(list_dirs) end context ‘when current directory is not a terraform dir’ do let(:result) { [’00_network’, ’01_vm’, ’02_dns’] } let(:list_dirs) { [’00_network/.terraform/’, ’01_vm/.terraform/’, ’02_dns/.terraform/’] } it { is_expected.to eq result } end context ‘when current directory is a terraform dir’ do let(:result) { [‘.’, ’00_network’, ’01_vm’, ’02_dns’] } let(:list_dirs) { [‘.terraform’, ’00_network/.terraform/’, ’01_vm/.terraform/’, ’02_dns/.terraform/’] } it { is_expected.to eq result } end Over time, we saw that the mock structures were becoming really complex, and that it did not allow us to easily understand which case (or directory structure) we were testing. Also, we needed to be able to test the order in which the directories were being listed, which wasn’t that easy using the first method. Therefore, we refactored our code and went for the second approach, and created different directory structures for different test scenarios. The directory structures can be seen in the GitHub repository itself, but they look like this: ▾ terraform_tests/ ▾ test_flat/ ▸ .terraform/ ▸ modules/ ▸ terraform.tfstate.d/ config.tf prod.tfvars ▾ test_flat_no_var_file/ ▸ .terraform/ ▸ terraform.tfstate.d/ config.tf ▾ test_layers/ ▸ 00_rg/ ▸ 01_network/ ▸ 02_vms/ ▸ modules/ dev.tfvars prod.tfvars Another interesting thing we did, in order to be able to test the order in which the different directories were swept through, was that we mocked the Terraform binary itself, like so: class Terraform def run params File.open(‘/tmp/terraform_mock_output’, ‘a’) do |file| file.puts(Dir.getwd) file.puts(params) end end end Needless to say, this mocked binary implementation was tested too. We used it to write the directory in which Terraform was launched and the parameters that were used when Terraform was called into a temporary file.  We then ran assertions on this file in order to test all our use cases. This solution allowed us to keep the code simple, and enabled easy scenario additions by simply creating the required directory structure in the `terraform_tests` directory. Maintainability for the win! Installation Just clone the project , and symlink the `tf.rb` file somewhere in your path, like so: ln -s <terraform_wrapper_directory>/tf.rb /usr/local/bin/tf Remember to use the full path to your terraform-wrapper’s directory. Final thoughts We had a lot of fun coding this thing. It allowed us to go back to classic development for a while, which is something we don’t have the opportunity of doing often enough, while working in Operations. People usually say that it is impossible to bring Software Craftsmanship practices to Operations. This is a common misconception, and we hope that people that work in the same domain as us start moving in the same direction as we do. Code is code! Most people would have written a Bash script. We’ve all done it. The thing is that Bash is a quirky language. It has a ton of non-usual behaviors, and it is rarely tested, which makes adding features to the script a really painful process. This is a problem for maintainability, and we created a critical tool for our workflow. We can’t afford to be blocked in its development. That’s why we choose not to do Bash anymore. We’re firm believers that we can do better, and that we must therefore do better. To do otherwise is to create limitations for ourselves. I hope you had fun reading this. We sure had fun writing it! See you next time! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations , Software Craftsmanship , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Software Craftsmanship and OPS scripting: a love story” Adi Chugh 02/01/2019 à 17:37 GITHUB link please? Sebastian Caceres 04/01/2019 à 10:23 Oh, sorry.\n\nIt's here => https://github.com/sebiwi/terraform-wrapper Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-08-16"},
{"website": "Octo", "title": "\n                Accelerate Innovations by Blending the Best Practices of Three Models of Innovation Diffusion  (Gartner, G.A. Moore , C. Christensen)            ", "author": ["Sylvain Fagnent"], "link": "https://blog.octo.com/en/accelerate-innovations-by-blending-the-best-practices-of-three-models-of-innovation-diffusion-gartner-g-a-moore-c-christensen/", "abstract": "Accelerate Innovations by Blending the Best Practices of Three Models of Innovation Diffusion  (Gartner, G.A. Moore , C. Christensen) Publication date 02/08/2018 by Sylvain Fagnent Tweet Share 0 +1 LinkedIn 0 Introduction When building innovative products we draw upon several models to help us follow the cycles and gauge products’ maturity levels (technological, market and use). We noticed that 3 complementary, superimposable models converge and together provide a coherence that makes the most of the various recommendations and best practices of each innovation strategy. The 3 diffusion of innovation models we’ve identified are: The Gartner model and “hype cycle” Geoffrey A. Moore’s Diffusion of Innovation model The “Innovating over the Horizon” model , popularized by Clayton Christensen Convergence of Diffusion of Innovation Models A model remains relevant until refuted by an experiment When we assist product teams in building their products (innovative or not), we draw upon several models to help us follow the cycles and gauge products’ maturity levels (technological, market, and use). As we like to say, all models are false but some are useful. I’d add that a model remains relevant and true as long as it hasn’t been refuted by a contradictory experiment . In other words, a model remains true until an experiment proves it false . From field work, people from organizations to universities have detected several recurring patterns and, through induction, have constructed a series of models. Seldom theoretical and limited in scope, these models are nonetheless quite effective since they are so close to empirical field observations. Draw an empirical model from field observation Source: https://www.lecolededesign.com/actualites/observation-des-usages-design-thinking-2795 The convergence of models confirms their solidity Among the different models we rely on when providing support or for diffusion of innovation, three are in heavy rotation: The Gartner model and “hype cycle” Geoffrey A. Moore’s Diffusion of Innovation model The “Innovating over the Horizon” model , popularized by Clayton Christensen; https://blog.octo.com/trouver-ou-vous-vous-sentez-le-mieux-pour-innover/ My colleagues and I had many conversations on how different models converge and had drawn a few conclusions on which elements factor heavily in model convergence. When we superimposed the models and supplemented the work with a well-garnished bibliography , our intuitions were proven to be right: the models did in fact converge. And that is a good thing—it means that by combining the 3 models we thought compatible, we were able to reinforce both their solidity and their relevance. Convergence of models Source: https://ifthenisnow.eu/nl/verhalen/de-wereld-van-de-luikse-kunstenaar-6-sophie-langohr By starting with different perspectives , we were able to clearly see that, once superimposed, the 3 models all converged together to tell the same story . Since pictures speak louder than words, here’s a visual aid to illustrate what happens during the superimposition. Convergence of Diffusion of Innovation Models Use the coherence of all three models to benefit from the best practices of each The coherence of the three models belies their sturdiness and complementarity. From where your innovation is on either model, draw on recommendations and best practices from the other two. A better perspective is the best way to grow your innovation. Another example: During the Trigger phase, imaginations can run wild when it comes to certain inventions. These “inventions” don’t yet qualify as innovations as they haven’t yet been disseminated in society. Tread carefully here, the terrain can be hazardous . Even once technological and/or adoption maturity is reached that doesn’t mean you’ve won the race. You have to continue developing the product either by extending it to other sectors and client types or by preparing a move to another team in the hopes of creating a cash machine . Conclusion As we said, the coherence of the 3 models shows their strength and complementarity (in terms of analysis perspective and, especially, in terms of innovation management and diffusion). In other words, you can rely on all 3 models at once to manage your innovation portfolio and draw on the best practices of all three . One caveat, however: The placement of your innovation is only dependent on expert opinion—yours, your team’s, or an external expert’s. It’s pretty safe to assume no scientific formula was used to ascertain the placement of your innovation on a set of regimented graphs or within a given horizon. You’re going to have to figure that out on your own. The best way to correctly position your innovation is to count on your own analysis and on the knowledge you’ve shared with the team managing the innovation. Model coherence helps improve innovation management by drawing on inspiring best practices. Src : https://www.mdcu-comics.fr/news-0023965-divers-lego-nous-presente-ses-brickheadz-dc-et-marvel.html & crédit : Warner Bros This might all seem theoretical to you at this stage but by reading the literature associated with each model (something we do regularly), you’ll find a wealth of best practices to apply to your work. Something a blog post, sadly, is much too short to accomplish! And keep in mind these models are empirical, from the field. The literature is teeming with concrete, applicable, and inspiring best practices. Lastly, these models should be used to help you arbitrate, project, or decide on how much time and money is worth investing in the project and are not an end on and of themselves. Going Further – OCTO & USI USI Talks – Unexpected Sources of Inspiration Innovation et sortie de crise – Marc Giget , USI 2013 (in French) The Myths of Innovation – Scott Berkun , USI 2014 Créer une proposition de valeur attrayante dans un business model innovant – Yves Pigneur , USI 2015 (in French) De quoi l’innovation fut-elle le nom ? – Vincent Bontems , USI 2016 (in French) 3 Keys to Innovation within the Enterprise – Ash Maurya, USI 2017 Extremely Inspiring Literature The Innovator’s Dilemma , Clayton Christensen Crossing the Chasm , Geoffrey A. Moore Running Lean , Ash Maurya Scaling Lean , Ash Maury Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology and tagged chiasm , christensen , crossing the chiasm , gartner , Growth hacking , innovation , Lean Startup , moore . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-08-02"},
{"website": "Octo", "title": "\n                Ansible Container: Chronicle of a Death Foretold            ", "author": ["Sebastian Caceres", "Adrien Besnard"], "link": "https://blog.octo.com/en/ansible-container-chronicle-of-a-death-foretold/", "abstract": "Ansible Container: Chronicle of a Death Foretold Publication date 20/06/2018 by Sebastian Caceres , Adrien Besnard Tweet Share 0 +1 LinkedIn 0 Alright, here’s what’s up: TL;DR We tried Ansible Container. We’d rather keep using Dockerfiles for image builds: creating a Docker image and provisioning servers with Ansible are two very different things. Different in terms of lifecycle, philosophy and workflow. So different, that in our opinion, they’re not compatible. Wanna know why? Read on. Disclaimer: While the current status of Ansible Container is not clear, it seems that during the writing of this article the tool has been deprecated: https://github.com/ansible/ansible-container/commit/2fa778a7c8d1699672314ac0b89c53554f435cb7 . After the limitations we noticed, we won’t say that we didn’t see that coming… Friendly reminders (because we’re friendly) Ansible Ansible is an automation software which allows you to do provisioning, configuration and deployment tasks. Everything is written in a neat, simple, easily readable YAML format, which is way better than old school Bash scripts. But it remains code, Infrastructure as Code. We won’t go deep into the details but we can say it has been a game-changer regarding automated configuration and deployment because of its simplicity: you only need SSH access to a server and that’s all. Every action is done from a centralized server (which can be your computer, but it’s usually a CI/CD server such as Jenkins) which connects to the other servers using SSH in order to execute actions (most of the time, that means running Python code). Simple, easy, efficient. One of the cool abstraction of Ansible are the Roles which are a way to describe how to setup an application without knowing the target host in advance, in a reusable way. Docker Docker is the de-facto container management tool. It allows you to build and manage images, from which you can create containers. You can use Docker to do this last part too if you want. Usually, when developing using a container-oriented workflow, you will use Docker in order to package your application. Sadly, the process of creating Docker images isn’t perfect. This, for example, is the sample Dockerfile used in order to create an Elasticsearch image: FROM openjdk:8-jre # grab gosu for easy step-down from root ENV GOSU_VERSION 1.10 RUN set -x \\ && wget -O /usr/local/bin/gosu \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)\" \\ && wget -O /usr/local/bin/gosu.asc \"https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc\" \\ && export GNUPGHOME=\"$(mktemp -d)\" \\ && gpg --keyserver ha.pool.sks-keyservers.net --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 \\ && gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu \\ && rm -rf \"$GNUPGHOME\" /usr/local/bin/gosu.asc \\ && chmod +x /usr/local/bin/gosu \\ && gosu nobody true RUN set -ex; \\ # https://artifacts.elastic.co/GPG-KEY-elasticsearch key='46095ACC8548582C1A2699A9D27D666CD88E42B4'; \\ export GNUPGHOME=\"$(mktemp -d)\"; \\ gpg --keyserver ha.pool.sks-keyservers.net --recv-keys \"$key\"; \\ gpg --export \"$key\" > /etc/apt/trusted.gpg.d/elastic.gpg; \\ rm -rf \"$GNUPGHOME\"; \\ apt-key list # https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html # https://www.elastic.co/guide/en/elasticsearch/reference/5.0/deb.html RUN set -x \\ && apt-get update && apt-get install -y --no-install-recommends apt-transport-https && rm -rf /var/lib/apt/lists/* \\ && echo 'deb https://artifacts.elastic.co/packages/5.x/apt stable main' > /etc/apt/sources.list.d/elasticsearch.list ENV ELASTICSEARCH_VERSION 5.6.8 ENV ELASTICSEARCH_DEB_VERSION 5.6.8 RUN set -x \\ \\ # don't allow the package to install its sysctl file (causes the install to fail) # Failed to write '262144' to '/proc/sys/vm/max_map_count': Read-only file system && dpkg-divert --rename /usr/lib/sysctl.d/elasticsearch.conf \\ \\ && apt-get update \\ && apt-get install -y --no-install-recommends \"elasticsearch=$ELASTICSEARCH_DEB_VERSION\" \\ && rm -rf /var/lib/apt/lists/* ENV PATH /usr/share/elasticsearch/bin:$PATH WORKDIR /usr/share/elasticsearch RUN set -ex \\ && for path in \\ ./data \\ ./logs \\ ./config \\ ./config/scripts \\ ; do \\ mkdir -p \"$path\"; \\ chown -R elasticsearch:elasticsearch \"$path\"; \\ done COPY config ./config VOLUME /usr/share/elasticsearch/data COPY docker-entrypoint.sh / EXPOSE 9200 9300 ENTRYPOINT [\"/docker-entrypoint.sh\"] CMD [\"elasticsearch\"] It makes you want to die, doesn’t it? It’s complex, verbose and hard to interpret. What are the logic steps involved in the construction of this image? If you spend some (a lot) of time reading this Dockerfile in order to understand what it does, you will realize that it starts from an openjdk image, it installs gosu , and then it does a lot of things in order to install Elasticsearch. Ansible Container Now imagine this: you work with your infrastructure in the cloud, and you configure and deploy everything using Ansible. You have a huge galaxy of roles that allow you to do many things related to your application (like installing Java and Zookeeper, to name a few). You want to transform your workflow in order to start using a container-based approach, but you don’t want to lose all your Ansible resources. What if instead of doing all that, you do this: --- - host: container roles: - gosu - elasticsearch Wouldn’t that be great? Wouldn’t it? Yes it would. So we started looking around and we realized that there’s a tool called Ansible Container . From its homepage: WHY NOT BUILD YOUR CONTAINERS WITH ANSIBLE PLAYBOOKS? NOW YOU CAN. Ansible Container represents an end to the command && command && command (and so on) syntax you’ve been struggling with to build containers. Sounds pretty good, doesn’t it? Let’s take a look into it. Let’s play! Overview In order to play with Ansible Container, we’re going to deploy our own simple Scala application which uses ZooKeeper in order to manage its state. Our application Description Our application is dead simple: First, it reads its configuration from the /etc/archiver.conf file ; For each cycle on configured frequency, it looks for files under the configured directory. If it finds any, it compresses them and then it deletes the original files. The fun part is that you can launch as many instances of the application as you want, and only one of them will do the compression/deletion tasks. That’s what Zookeeper is used for. In order to make the application communicate with Zookeeper, we use Apache Curator , which is a Zookeeper library. It provides many utilities we can use in order to provide distributed lock policies and leader election. You can see the code of our application here : https://gitlab.octo.com/abesnard/ansible-container-zookeeper-article/tree/provision-virtual-machines/archiver . We wanted to create an application that needs Zookeeper because it seemed like a coherent test case provided the assumptions stated before: we have an Ansible-managed workflow, on virtual machines, and we’re moving towards a container-based approach. The Ansible Role The Ansible Role is fairly simple: We create a user which will be used in order to run our application ; We copy the JAR ; We create the configuration file using some Ansible variables ; And finally we run everything using systemd . --- - name: create system user user: name: \"archiver\" system: yes home: \"/usr/lib/archiver\" - name: create directories file: path: \"{{ item }}\" owner: \"archiver\" group: \"archiver\" state: directory with_items: - \"/usr/lib/archiver\" - name: copy JAR file copy: src: \"archiver.jar\" dest: \"/usr/lib/archiver/archiver.jar\" owner: \"archiver\" group: \"archiver\" register: copy_jar_file - name: create config file copy: content: | archiver { input-folder-path = \"{{ archiver_input_folder_path }}\" output-folder-path = \"{{ archiver_output_folder_path }}\" tick-period = {{ archiver_tick_period_in_seconds }} seconds } zookeeper.servers = [ {{ archiver_zookeeper_servers | map('quote') | join(', ') }} ] lock.timeout = {{ archiver_lock_timeout_in_seconds }} seconds dest: \"/etc/archiver.conf\" owner: \"archiver\" group: \"archiver\" register: create_config_file - name: create systemd service copy: content: | [Unit] Description=Archiver [Service] owner=archiver Group=archiver ExecStart=/usr/bin/java -jar \"/usr/lib/archiver/archiver.jar\" dest: \"/etc/systemd/system/archiver.service\" register: create_systemd_service - name: reload systemd systemd: daemon_reload: yes when: create_systemd_service.changed - name: start systemd service systemd: name: \"archiver.service\" enabled: yes state: started - name: restart systemd service systemd: name: \"archiver.service\" enabled: yes state: restarted when: create_systemd_service.changed or create_config_file.changed or copy_jar_file.changed ZooKeeper Description In a few words: ZooKeeper is a distributed and resilient key-value store. It also provides some abstractions, making it a great solution in order to have a distributed lock… which is exactly what we want because we only want one instance of our Archiver application to run compression/deletion tasks at a time. The Ansible role Because we do not want to reinvent the wheel, we’re going to use an existing Ansible role from the Ansible Galaxy in order to provision ZooKeeper on our machines : https://galaxy.ansible.com/AnsibleShipyard/ansible-zookeeper/ . It’s a little bit off-topic, but Ansible Galaxy is a great tool which you can use to organize and centralize your roles. More information here: https://galaxy.ansible.com/ ! End to end Our playbook looks quite simple : --- - name: install ZooKeeper become: yes become_method: sudo hosts: zookeeper roles: - role: \"AnsibleShipyard.ansible-zookeeper\" zookeeper_hosts: \"{{ groups['zookeeper'] | map('extract', hostvars, ['ansible_' + iface, 'ipv4', 'address']) | list }}\" zookeeper_version: 3.4.12 - name: install Archiver hosts: archiver become: yes become_method: sudo roles: - role: \"archiver\" archiver_zookeeper_servers: \"{{ groups['zookeeper'] | map('extract', hostvars, ['ansible_' + iface, 'ipv4', 'address']) | list }}\" archiver_input_folder_path: \"/shared/to-archive\" archiver_output_folder_path: \"/shared/archived\" archiver_tick_period_in_seconds: 10 archiver_lock_timeout_in_seconds: 1 As you can see, it works great on our two Vagrant boxes (it takes some time because of the downloads of Java and ZooKeeper, but if you skip to the end… Everything goes fine!) : The Use-Case First Try As we said before, a perfect use-case of Ansible Container is to leverage on all the roles which have already been created in order to Dockerize existing applications. So let’s do it: we’re going to containerize ZooKeeper and our Archiver application… We first write a container.yml file like this: --- version: \"2\" settings: conductor: base: ubuntu:xenial project_name: ansible-container-blog services: zookeeper: from: ubuntu:xenial roles: - role: \"AnsibleShipyard.ansible-zookeeper\" zookeeper_version: 3.4.12 archiver: from: ubuntu:xenial roles: - role: \"archiver\" archiver_zookeeper_servers: [ \"zookeeper\" ] archiver_input_folder_path: \"/shared/to-archive\" archiver_output_folder_path: \"/shared/archived\" archiver_tick_period_in_seconds: 10 archiver_lock_timeout_in_seconds: 1 depends_on: - zookeeper links: - zookeeper volumes: - /tmp/shared:/shared Let’s try it, and see what’s going on! It does not work, because we need to add some stuff in our role in order to make it Docker compliant: Instead of having systemd to start a daemon, we need to provide a Docker CMD . If we take a step back, it’s easy to say that most of your roles are not ready to be used as-is in a container using Ansible Container: multiple tasks are relevant only in the context of classic servers ( systemd services, firewall, mounts, etc.). It’s due to the fact that Ansible is more that a tool to install applications: it goes way beyond this simple use case… which is not relevant in a Docker context. Note that Ansible is aware of this problematic and that’s why the notion of Container-Enabled Role has been created: https://docs.ansible.com/ansible-container/roles/galaxy.html . So when you use Ansible Galaxy, be aware of that! Second Try Fine… We’re going to make our role compliant: First, add when: ansible_env.ANSIBLE_CONTAINER is not defined in the step which are not relevant in a container context (so in our case, everything which is related to systemd ) ; Then add a meta/container.yml file in our Ansible role in order to tell Ansible Container what is the actual CMD which needs to be run : --- from: \"ubuntu:xenial\" user: \"archiver\" command: [ \"java\", \"-jar\", \"/usr/lib/archiver/archiver.jar\" ] And here we go again! It’s alive! You can even see the Docker images which have been created by Ansible Container (everything is properly named and tagged, which is pretty nice) and the Conductor images (which are used by Ansible Container under the hood to provision the containers): Just for you to know: we lied to you. We actually didn’t use the AnsibleShipyard.ansible-zookeeper role as-is: we had to patch it for this particular reason in order to continue our trial of Ansible Container: https://gitlab.octo.com/abesnard/ansible-container-zookeeper-article/blob/build-containers-after-changes/ansible/roles/AnsibleShipyard.ansible-zookeeper.patch . Ansible Container even allows us to launch everything it created by creating an Ansible playbook which actually starts the containers… Well, let’s do it! Pretty neat, right? Reusage As-Is We just saw that with a few minor modifications of our role, we’re now capable of using Ansible Container in order to produce Docker images instead of actually deploying our role to some virtual machines. Before going straight ahead to a Kubernetes cluster, we want to be sure that we can reuse our newly created Docker image using Docker Compose. Easy, let’s define our docker-compose.yml file: --- version: \"3\" services: zookeeper: image: \"zookeeper:latest\" ports: - 2181:2181 archiver: image: \"ansible-container-blog-archiver\" depends_on: - \"zookeeper\" volumes: - \"/tmp/shared:/shared:rw\" You can see that instead of leveraging of the ansible-container-blog-zookeeper image, we’re going to use the official Docker image of ZooKeeper: it’s totally possible because of the plug’n’play spirit of these images. And run start Docker Compose: Everything is fine! Update configuration And now, what I want to do is to change the frequency of the file scan. Well… we can’t actually do this without building the images using Ansible Container again. And for us, this is the main issue of Ansible Container: an Ansible role is fundamentally different from a Dockerfile . Build Time V.S. Run Time And here we are… using our role (which heavily leverages Ansible’s template module), we created a Docker image that is designed in order to work only in the context of the architecture described in the container.yml file: the Docker image we created cannot be used in a plug’n’play fashion. You cannot easily reuse a Docker image which have been created with Ansible Container. The cause of this is written in the /etc/archiver.conf file, all the value are hardcoded in the Docker image: archiver { input-folder-path = \"/shared/to-archive\" output-folder-path = \"/shared/archived\" tick-period = 10 seconds } zookeeper.servers = [ zookeeper ] lock.timeout = 1 seconds The only way to update theses properties is to change the container.yml file like this and rebuild everything… And we do think that it’s not a good practice in a Docker context: we usually use environment variables to set or override properties. Wrap Up Limitations Ansible Container is not a bad tool and we actually think that it’s a good idea: writing a Dockerfile is often a pain, and leveraging on Ansible could have been a good idea. But provisioning with Ansible heavily rely on the template module which set the properties during the provisioning, but do not permit to modify them afterward. There is no such thing as Run Time with Ansible. We didn’t cover that in this article, but Ansible Container is also bad at caching stuff, which is a shame because it’s one of the best things that the different Layers boughts by Docker provide… Alternatives There are multiple ways to bypass this limitation: The best way is to update your application in order to make it aware of the environment variables (this kind of behavior is native with multiple configuration framework, like Spring : https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html or Typesafe Config : https://github.com/lightbend/config ) ; Otherwise, before starting your application in your entrypoint.sh file, you can use: The envsubst program which will allow you to replace reference to any environment variable by its value when called: https://linux.die.net/man/1/envsubst The confd program which is also a tool to fill a template using values, except this time the values can come from other sources like Consul, etc.: https://github.com/kelseyhightower/confd . Please also note that if your Dockerfile is a mess because of the multiple scripts it embeds, you still can put all the logic in a clean separate python file… maybe this simple solution can also be a good start. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Ansible Container: Chronicle of a Death Foretold” WGM 26/06/2018 à 18:25 \"But provisioning with Ansible heavily rely on the template module which set the properties during the provisioning, but do not permit to modify them afterward. There is no such thing as Run Time with Ansible.\"\r\n\r\nWe've solved this problem by leveraging kubernetes configmaps. Basically our ansible-container run creates \"clean\" containers with no config, then we have a second ansible process that runs through the repo locally and creates and applies environment specific configmaps from our ansible templates, which are then mounted in the k8s deployment.\r\n\r\nI suppose the downside of this is that our containers are now reliant on k8s and would require some tweaking to run natively in docker, but we have no need to run them natively in docker so we're fine with it. Nabheet Sandhu 16/11/2018 à 05:59 Looks like someone else took ownership of the ansible-container github project and it is not deprecated anymore. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-06-20"},
{"website": "Octo", "title": "\n                My 3 leadership learnings from learning to code            ", "author": ["Ngoc Quê-Minh Vo"], "link": "https://blog.octo.com/en/my-3-leadership-learnings-from-learning-to-code/", "abstract": "My 3 leadership learnings from learning to code Publication date 13/06/2018 by Ngoc Quê-Minh Vo Tweet Share 0 +1 LinkedIn 0 [Or what I did learn from javascript 101, even though I am not a software engineer] When I was young(er), I’ve started my career in multinational corporations. One of the key things which struck me at that time was that, even though each of this company was quite hierarchical, every manager must, to evolve in their career, have made a compulsory training on the field. In other words, whether you are a finance or marketing director, you have to regularly play the role of a sales person in the shops, visit the factories or the place where the product come from to know where your product come from and really understand the value chain which at the end delivers value to your customer. Then, nowadays, in such a global competitive environment and a world which change really quickly with the technological evolution, why wouldn’t any manager do a compulsory training “on the field” by putting themselves in the shoes of a software engineer? According to the saying “eat your own dog food” , I’ve myself experienced the CodeSchool JavaScript classes , from scratch and here’s what I learn. NB: I’m not sponsored at all, this is not a commercial post, just my feedbacks on my personal experience. 1. About focus Photo by Dmitry Ratushny on Unsplash What a software engineer can look like From the outside, your software engineer looks like a teenager with the special combo jean/sneakers/headphones who doesn’t want to leave his·her eyes from the screen and sometimes, they don’t even take the time to leave their headset when you pass by to say hello to you… How irritating! Myself in their shoes *In the open space, besides my colleagues commercial executives and the partners.* I’m trying to solve my first JavaScript exercises. First thing first, I’m reading the wording of the exercise and I’m trying to understand what the aim of the exercise is. Since I’m doing a training for beginners, I can easily guess which function or which JavaScript notion I’m asked to use. Though, trying to understand to which functional needs the exercise apply is far more difficult. In the meantime, 50 cm from me, my colleagues are having strategic discussions regarding to the next big commercial leads in the pipe and how the company will deal with this. There’s also another colleague just in front of me who’s asking me every 30 seconds the English version of technical words because he’s working on a commercial proposal… Whereas I’m quite ok with multitasking when I have to shape up a Powerpoint presentation, here I’m really in difficulty. I lost a quite substantial amount of time looking for what I did wrong in my exercise: I forgot a semi-colon somewhere… Beginners’ classic fault, but a simple one mistake which could have been spotted if I were more focused. So what ? In my example, I just made a typo in my code… and that’s because I’m a beginner, please don’t take this the wrong way. Coding is about solving complex problems. Like writing, you have to think about the structure, maybe write some stuff, and then rewrite it. If someone is interrupting you every time before you finish a sentence, I guess you will lost motivation or even the track of what you were supposed to write. As a C-Level, aren’t you going to solve complex problems too? By having an executive assistant, aren’t you trying to cut yourself from constant interruption ? At the end, that’s just about common sense, and as a C-Level-to-be: I guess I won’t need to spend my money on these kinds of offers to realize the impact of multitasking and focus. NB: As I said, I’m not sponsored at all, though, this place seems quite nice. I would endeavor to be exemplary on deep working session, especially because of the workforce of the future will be (and already is) feed with social medias, stories and so on… 2. About productivity First impressions (or things I often hear in my working daily life) People from your company has insisted on having a baby foot in the cafeteria, maybe a ping-pong table also. It’s quite trendy and it’s a nice stuff to show your interviewees. But WTF, to you, your software engineers are spending their time playing, taking huuuuuuge breaks and you really don’t understand how you can still get a ROI from them at the price you pay them. Photo by Jordan Whitfield on Unsplash Lessons learned from coding: hard work vs. deep work Coding in javascript was totally new to me and at the beginning, even coding just few lines of codes to develop basics functions was really hard. I have sometimes spent a whole afternoon all by myself on an exercise without managing to solve it at the end. At the end of the day, I was tired looking for answers on forums and not really motivated. Working harder (meaning here: doing extra hours) wouldn’t allow me to solve the issue. Best case who could happen: just copy/paste the answer of the exercise with no value-added for me in terms of improving my skills. Though, in real life, I would have written something quick & dirty which works but not good over the long term (if the code I had written was use in the real life). How did I solve my issues (after turning the problem upside down all by myself): mainly by asking for the help of a more experienced person. Sometimes I answered myself the question just by asking it to someone else. Fun fact: I even learn this was called the rubber duck debugging and this name is so weird that I will remember it. The other time, the more experienced person explained to me how to solve it. Most of the time, it just took him·her a few minutes, and once I understand it I really got the notion which I was trying to learn. My vows As a C-Level to be, I will truly focus on the output of my teams and not their working hours. I will keep in mind that to my teams, having breaks is part of their work days the same way they will have sharing times (meetings or whatever). I would give them my trust and clear conditions to be autonomous rather than choosing a command and control leadership style. Oh, and by the way, in my company we have a babyfoot and a flipper. At my client place, a PS4 and a pool table. 3. About the working tools What you software engineer can look like Same as point 1 above. You have actually no idea of what your employee is doing all day long (when he·she’s not playing ping pong or baby foot), except from the back, it looks like this. Photo by Fabian Grohs on Unsplash Plus, he·she dares asking you for extras such as giant screens or more powerful computers which costs a substantial part of their salary. What I’ve understood by putting myself in their shoes OMG coding could be soooooo tiring ! hehe More seriously, my best epiphany on this topic concerns the crucial importance of the compulsory training on the field for the managers in my previous companies in order to understand where the products come from and understand the whole value chain which at the end delivers value to the customers. Putting yourself in the other shoes tends to make you understand more the pain, the constraints and the needs of the people who are really producing on the field. While it’s so obvious for anybody who hire a painter for a home renovation that he·she has to get quality brushes and painting in order to get the job well done, it is not for most of the C-level who “order” a brand new app to a software engineer. And why’s that ? Because he·she has no clue of what the job is. So what ? You probably think the painter example is a bad faith one. To give another one: have you already seen a trading room ? Do you know how much a Bloomberg terminal cost ? At this point, I guess 1 or 2 extra screens for 1 trader is not really an extra cost. Let’s go back to our example. We talked of ROI previously. At the price your software engineers are paid, would you rather invest €1,000 more on each computers you give them or let them take a coffee every time they have to test something ? While I have 100% the mindset of being rational, always seeking the ROI, calculating the costs and the gain in each steps of my decision. I also take into account the fact that there are hidden costs and gains that you can’t guess if you’re not on the field . For example, the cost of making your teams wait every time they have to test something. But also the hidden gain of having a great employee experience by provide them great working tools. As an illustration, my current employee asked me before my arrival which working tools I needed to among a small catalog (PC or Mac with 2 different configuration for each of them, mouse, bags…). Modeling is great, but common sense would also add that: don’t forget you are managing people , not (yet ?) some machines which write code. My invitation Here, my invitation for you to try to learn how to code is not to masterize the skill, but to put yourself in other shoes and find what it can bring for you at your level, whether you’re a partner of a multinational companies or a program manager. You can find herebelow more details on my JavaScript roadtrip but all I can say is that most of the insights I shared with you were obvious to me just after 5 to 6 hours of learnings . For my part, I love working with the IT teams and Operations (marketing, finance, HR…). This experience didn’t make me want to be a software engineer (like some HR colleagues who ended to be software developers). I would just go to play with some functions on Scratch with my nephew and niece. Though, as an agile coach, it helps me on daily basis in my struggle to simplify the organization of my clients by scaling agile methodologies. Extra advices Photo by Fab Lentz on Unsplash The Challenge in itself: don’t only choose what you want to learn but also the way you want to learn Before starting the challenge, I’ve read the Beginner’s Guide to Web Development (approximately 40 pages) which explains technically how a website works. Then, I’ve passed the following CodeSchool classes: – HTML and CSS – JavaScript roadtrip level 1, 2, 3 Why did I choose this one? I had the choice between 3 technical certifications (decided by my company’s catalog): Java, Amazon Web Services, and JavaScript. The Java and Amazon Web Services certification were the ones from the editors. As I said previously, my goal was not to change my career path, but to enrich my technical general knowledge. Also, I was more attracted to learn about web front language in order to directly see the result of what I was coding. Considering the fact I’m a consultant, the CodeSchool classes for the flexibility offer by e-learning methods were the best to fit my working lifestyle: you can do it whenever you want it, pause the video, accelerate it… Keep in mind that coding is not something inaccessible Anybody with regular brain capacities is able to learn how to code, there’s no need to sacralize it (even though it can really be a fascinating job) or to think it’s a trap. It is not because you are traumatized by your mathematics lessons that you would not be able to learn algorithms or get the logic of any language. Actually, I often felt also I was learning a real foreign language with grammar rules and syntax. Just as much as learning anything new, you’ll learn coding making connections with was you already know, whatever this is. Manage your time over the long run and be clear with your management It takes a lot of time! and except if you can manage to do it in group for a whole day every week, you’ll have to make a trade-off between your current operations (i.e.: to contribute to make direct money for your company) and this challenge. One of my colleague manage during his mission to do the “Google rule”: a whole day per week during 3 to 4 months. As far as I’m concerned, I’ve picked another solution, more in line with my working constraints (actually the ones of my clients ;)): I did it by chapters, when the activities level was low (mainly school and bank holidays as I work in France). Because most of us do not work at Google and get 20% of their working time allowed to personal projects, be careful and make sure with your managers and the HR you don’t put your evaluation and annual bonus in jeopardy. To go further: https://www.codeschool.com/ [In French] Décoder les développeurs, Enquête sur une profession à l’avant-garde, 2017, 180p. https://www.editions-eyrolles.com/Livre/9782212567397/decoder-les-developpeurs [In French] « Les leaders de demain doivent penser comme des codeurs ! », Interview d’Aurélie Jean sur le blog de la conférence USI, 31 mai 2017 https://blog.usievents.com/interview-aurelie-jean-les-leaders-de-demain-doivent-penser-comme-des-codeurs-pour-innover/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Change Management , Consulting Chronicles , Culture . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-06-13"},
{"website": "Octo", "title": "\n                The Wizard: Scenarios            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/the-wizard-scenarios/", "abstract": "The Wizard: Scenarios Publication date 07/06/2018 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 This is another episode of the “The Wizard” series. If you haven’t read the previous episodes, you can find them here and here . You just don’t know how to do it. You’ve been thinking about it for a while too. It’s an important issue. The crafting of the spell is the most important part. The journey is usually more important than the destination itself. But you don’t feel like you are doing it right. The purpose is simple, yet it has a duality of character. You want to enable the transmission of information between all realms into a single library of knowledge. A task you’ve already accomplished many times in the past. — # filebeat/tasks/main.yml – name: install Filebeat package: name: filebeat={{ elk_version }} state: present update_cache: true notify: restart Filebeat – name: copy configuration file template: src: filebeat.yml.j2 dest: /etc/filebeat/filebeat.yml owner: root group: root mode: 0600 notify: restart Filebeat – name: start Filebeat service: name: filebeat state: started enabled: true You have crafted this spell properly, following the same principles you have taught many people across the years, the same ones you’ve used for your own . The same ones that Merlin taught you the first time he showed you how to create a playground to refine your creations, thousand of years ago. import os import testinfra.utils.ansible_runner testinfra_hosts = testinfra.utils.ansible_runner.AnsibleRunner( os.environ[‘MOLECULE_INVENTORY_FILE’]).get_hosts(‘all’) def test_filebeat_is_installed(host): filebeat = host.package(‘filebeat’) assert filebeat.is_installed def test_filebeat_is_running(host): filebeat = host.service(‘filebeat’) assert filebeat.is_running def test_filebeat_is_enabled(host): filebeat = host.service(‘filebeat’) assert filebeat.is_enabled def test_filebeat_configuration_exists(host): filebeat = host.file(‘/etc/filebeat/filebeat.yml’) assert filebeat.exists Nevertheless, you want to add an extension to the spell, depending on whether you want to receive information about all the portals you’ve spawned into this world or not. Visualisation is key. — # filebeat/tasks/main.yml … – name: include module tasks import_tasks: nginx_module.yml when: activate_filebeat_nginx_module — # filebeat/tasks/nginx_module.yml – name: copy nginx module file template: src: nginx.yml.j2 dest: /etc/filebeat/modules.d/nginx.yml owner: root group: root mode: 0644 register: nginx_module notify: restart Filebeat – name: setup filebeat shell: filebeat setup -e && touch /tmp/filebeat_configured when: nginx_module.changed tags: – skip_ansible_lint You know how to craft the general part of the spell , but not the extension. If only… N appears out of thin air. Hey! What are you up to? Fighting with an Ansible role. Can I help? Sure. I’m trying to conditionally add some tasks to my role, but only if I want the nginx logs to be forwarded to my Elasticsearch cluster. I use the nginx Filebeat module , which also happens to create pretty cool dashboards. Great. Just use an include_tasks conditioned by a boolean variable defined somewhere. You could name it `activate_filebeat_nginx_module`, for example. He’s good. Yeah, that’s what I did. Then again, my problem isn’t that part, but rather testing the conditional part. I don’t activate the module by default. The `activate_filebeat_nginx_module` is set to false in the role’s defaults. So my Molecule test only tests the default case. Yes. That’s because that’s the default scenario. Just create another scenario. You’re confused. What? N pulls up his sleeves and steps forward. Remember how you taught me how to create a role with tests using Molecule? The principle is rather simple. You type `molecule init role –role-name filebeat` and Molecule creates all the resources you’re going to need in order to test your role. That includes the test files and all the other playbooks you may need: the prepare playbook, the side_effects playbook… Indeed. I remember. The thing is that Molecule creates a `default` scenario in order to do that. That’s why all your resources are under the `molecule/default` directory on your role. N waves his hands around in the air. Light appears everywhere, and you can see the structure of your current spell: filebeat/ ├── README.md ├── defaults │   └── main.yml ├── handlers │   └── main.yml ├── meta │   └── main.yml ├── molecule │   └── default └── tasks ├── main.yml └── nginx_module.yml You’re right. If you examine the playbook on the default directory, you will see that there are no variables specified for the role. N waves his hands once again in the air and the light and the images change: — # filebeat/molecule/default/playbook.yml – name: Converge hosts: all roles: – role: filebeat This is because you are actually testing the `default` scenario: the one where you only use the default variables that are defined in the `default` directory of your role. I see. You have seen this before, but you’ve never really understood the abstractions or their meaning, even if you’ve already been doing this for a while. You feel on the verge of enlightenment. So you just need to create a new scenario. N claps his hands, and energy starts flowing through him as a new component of the enclosure you used in order to craft your test appears. `molecule init scenario –scenario-name activate_filebeat_nginx_module –role-name filebeat` –> Initializing new scenario activate_filebeat_nginx_module… Initialized scenario in filebeat/molecule/activate_filebeat_nginx_module successfully. molecule ├── activate_filebeat_nginx_module │   ├── Dockerfile.j2 │   ├── INSTALL.rst │   ├── create.yml │   ├── destroy.yml │   ├── molecule.yml │   ├── playbook.yml │   ├── prepare.yml │   └── tests └── default ├── Dockerfile.j2 ├── INSTALL.rst ├── create.yml ├── destroy.yml ├── molecule.yml ├── playbook.yml ├── prepare.yml └── tests You both walk to the new part of the playground. In this new directory, you can specify everything you need in order to properly test your new scenario. What do you need first? I need to specify that I want to include the conditional part of my playbook. Where would you do that? I’d use the group_vars at the playbook level. That’s good. Then again, there are no group_vars here. You only have a small playbook that specifies everything you do in order to test your role. So I’ll do it in the playbook.yml file. Correct. Isn’t that a bit dirty? I’ve always thought that you’re not supposed to use variables on a playbook, but rather put them somewhere else. That’s a style thing, but yes, I would say that generally it is a good practice. Nevertheless, it is precisely what you need right now. As I just said, your playbook defines every parameter used to test your role. It’s easy to see how are you testing your role just by looking at it. You know precisely which variables are defined and how. Even if the name of the scenario should be explicit enough already in order to understand it. Try it. You mold the spell accordingly. — # filebeat/molecule/activate_filebeat_nginx_module/playbook.yml – name: Converge hosts: all roles: – role: filebeat activate_filebeat_nginx_module: true Good. Now you can test everything you wanted. Just remember that in order to test every scenario, you need to use `molecule test –all`. This will execute every scenario, which is probably what you want. If you want to speed up the red/green/refactor loop, you can use the `create`, `converge` and `verify` actions like you taught me before, but you need to specify the scenario name when executing every action, like this: `molecule create –scenario-name activate_filebeat_nginx_module` You are impressed with N. It’s hard to believe that you were the one teaching him things once. Maybe he’s been the one teaching you all from the start. That’s precisely what I needed. I’m glad I could be of service. Do you need any more help? No, I’m all set. I just… You’re having a déjà-vu. N is outside your field of vision. You slowly turn your head around… N is still there. What? Nothing. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Software Craftsmanship , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-06-07"},
{"website": "Octo", "title": "\n                How to do TDD on Auth0 scripts            ", "author": ["Laurent Dutheil"], "link": "https://blog.octo.com/en/how-to-do-tdd-on-auth0-scripts/", "abstract": "How to do TDD on Auth0 scripts Publication date 01/06/2018 by Laurent Dutheil Tweet Share 0 +1 LinkedIn 0 Auth0 is a SaaS platform that offers a set of tools to implement Authentication and Authorization for your web, mobile and legacy applications. This platform provides a great developer experience with a good documentation and innovative features, such as rules , JavaScript functions that are executed when a user authenticates to your application. GitHub deployment extension The GitHub Deployments extension allows you to deploy rules and database connection scripts from GitHub to Auth0. You can configure a GitHub repository, keep all your rules and database connection scripts there, and deploy them automatically to Auth0 each time you push to your repository. There are also extensions for GitLab or Bitbucket . Problem with unit tests I need TDD to express the behaviour of my scripts (article in french). To write Unit Tests, you have to modularize your script to be able to import the function you want to test in your test. But, with the extensions convention and this serverless platform, the scripts can only contain a single function. Auth0 offers several testing strategies . You can find how to do Integration Tests, performance tests or how to mock Auth0. Nevertheless, none of them are intended for Unit Testing. Solution to do TDD My first thought was to read the file I want to test in a String, then execute an eval with it. But I discover this article that explains you should use Function instead of eval. The bonus of this method is that you can create a closure (article in french) which facilitate the injection of mocked dependencies. So the solution is to add these lines to your tests : const mockedDependency = require('some_dependency');\r\n\r\nconst fs = require('fs');\r\nconst scriptSource = fs.readFileSync('<path_of_the_source_script>', 'utf8');  // 1.\r\nconst testableScript = new Function('one_dependency', 'return ' + scriptSource + ';'); // 2.\r\nconst functionToUseInTests = testableScript(mockedDependancy); // 3. These lines : read the script through the file system inject it in a new function with its dependencies (aka a closure) evaluate the closure with the mocked dependencies So now you can TDD \\o/ With an example on Github If you want more details you can go to this repository and pay attention to these two commits that show the addition of two behaviours in two steps in accordance with the TDD principles. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “How to do TDD on Auth0 scripts” Amirault 17/11/2020 à 17:21 Thx a lot ! It work so nice ! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-06-01"},
{"website": "Octo", "title": "\n                Confluent.io: Part 3 – STREAM PROCESSING            ", "author": ["Arthur Baudry"], "link": "https://blog.octo.com/en/confluent-io-part-3-stream-processing/", "abstract": "Confluent.io: Part 3 – STREAM PROCESSING Publication date 07/05/2018 by Arthur Baudry Tweet Share 0 +1 LinkedIn 0 This article is part of a series designed to demonstrate the setup and use of the Confluent Platform. In this series, our goal is to build an end to end data processing pipeline with Confluent. Disclaimer: While knowledge of Kafka internals is not required to understand this series, it can sometimes help clear out some parts of the articles. In the previous articles, we set up two topics, one to publish the input data coming from PostgreSQL and another one to push the data from to AWS S3. But we have yet to connect the two topics together and it is exactly what we will do in this article. To that end, we are going to use Kafka Streams, do a bit of processing and expose the data for what could be a real-time web UI. Our pipeline will then look more like this KAFKA STREAMS Kafka Streams is a set of lightweight stream processing libraries enabling application design without worrying about the infrastructure behind it. Using Kafka Stream, your application will be deployed with your Kafka brokers making it highly scalable, elastic, distributed and fault tolerant. Because Kafka Stream is tightly integrated with Kafka, the parallelism of your application will be determined by the number of partitions for your Kafka topics (more specifically the number of partitions of the input stream topic). Kafka Stream processes event in real time (no micro-batching like Spark Streaming) and allows you handle the late arrival of data seamlessly. By default, every Kafka Stream application is stateless which allows you to scale it automatically using your favourite process management tools (Kubernetes, Mesos, YARN) but we’ll see there is some way you can actually manage a state with Kafka Stream. In this article, we are going to use this feature to maintain a state which will be exposed for querying through RPC calls. Kafka streams defines the following logical components: Stream: Unbounded, continuously updated dataset, composed of stream partition Stream partition: a part of the stream of data. Processor topology: a graph of stream and stream processor which describes how the streams are to be processed. For those familiar with Spark, it resembles a logical plan. Stream processor: a step to be performed on a stream (map, filter, etc…). There is two special stream processor in a topology, the source processor and the sink processor (read/write data). Two APIs are available, the Kafka Stream DSL or the low-level Processor API. Strea m task: an instance of the processor topology attached to a partition of the input topic and stream processor’s steps on that subset only. Strea m thread: can run one or more stream task, is used to scale out the application, if the number of stream threads is greater than the number of partition, some instances will stay idle. If a streaming task fails, an idle instance will take its place . Record: A key-value pair, streams are composed of records Tables: used to maintain a state inside a Kafka Stream application Streams and tables are used extensively within Kafka Stream. It introduces the concept of stream-table duality where we view a stream as a changelog of a table continuously updating and a table as a snapshot of a stream. At API level, these components are modelled like this: KStream: a stream of record, can be viewed as an INSERT in a table KTable: a changelog stream of data, can be viewed as an UPSERT (INSERT/UPDATE) in a table. A KTable represent a state in the application locally. Under the hood, the state is stored in a local RocksDB instance which is an in-memory key-value store. A changelog topic is also created alongside each RocksDB to ensure fault tolerance. GlobalKTable: A KTable representing the state of the application globally. It can be used to do more efficient join or to broadcast variables to all instances of your application. Downsides of this feature include network traffic and broker load increase. At least once processing is guaranteed because the offset is not committed every time a record is read. Thus, it is possible that while recovering from a failure, a record is read twice. Exactly once semantics (or EOS) is available with Kafka 0.11.0 but is limited to specific use cases when ingesting data although it works with Kafka Streams. It works using compacted topics but that could be the object of a whole other article by itself. Now that we’ve established the basics, let’s start coding our Kafka streams application … BUILDING A BRIDGE Note: the code for the server is available at https://github.com/ArthurBaudry/kafka-stream-basket If you followed our previous article, you should be able to stream data in a Kafka topic with Kafka Connect, now let’s create a Kafka Streams application that is going to process it. First, let’s start a Maven project in your favourite IDE and add the dependencies from the pom.xml . We’ll reuse the classes SpecificAvroSerde, SpecificAvroSerializer and SpecificAvroDeserializer from the schema registry. Those classes are wrapping KafkaAvroSerializer and KafkaAvroDeserializer which extend AbstractKafkaAvroSerializer and AbstractKafkaAvroDeserializer which in turn are directly using Avro libraries to serialise and deserialise data. We’ll tell Kafka Streams how to deserialise the data published in confluent-in-prices by adding the SpecificAvroSerde as the way of deserialising the values on the <key, value> messages on this topic settings.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\r\nsettings.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, SpecificAvroSerde.class);\r\nsettings.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, SCHEMA_REGISTRY_URL); The Schema Registry URL is important because it will be used to get the Avro schema and deserialize the data into a POJO. Once this is done, we will then be able to manipulate typed objects through the KafkaStream API rather than parsing messages ourselves. Note: There are two ways to serialize/deserialize with Avro, the one we are using which generates POJO and a generic one which only allows GenericRecord object manipulation and is, therefore, less convenient (no type-safety). This second method is using the embedded schemas in Avro data and therefore is not dependent on the Schema Registry. To automatically generate POJO from Avro schema files (*.avsc) for development purposes, you need to add the kafka-registry-maven-plugin and avro-maven-plugin to your pom.xml: <plugin>\r\n\t<groupId>io.confluent</groupId>\r\n\t<artifactId>kafka-schema-registry-maven-plugin</artifactId>\r\n\t<version>3.2.1</version>\r\n\t<configuration>\r\n\t\t<schemaRegistryUrls>\r\n\t\t\t<param>http://<ec2-ip>:8081</param>\r\n\t\t</schemaRegistryUrls>\r\n\t\t<outputDirectory>src/main/avro</outputDirectory>\r\n\t\t<subjectPatterns>\r\n\t\t\t<param>^confluent-in-[a-z0-9]+-(key|value)$</param>\r\n\t\t</subjectPatterns>\r\n\t\t<subjects>\r\n\t\t\t<confluent-in-prices-value>src/main/avro/confluent-in-prices-value.avsc</confluent-in-prices-value>\r\n\t\t</subjects>\r\n\t</configuration>\r\n</plugin>\r\n<plugin>\r\n\t<groupId>org.apache.avro</groupId>\r\n\t<artifactId>avro-maven-plugin</artifactId>\r\n\t<version>1.7.7</version>\r\n\t<executions>\r\n\t\t<execution>\r\n\t\t\t<phase>generate-sources</phase>\r\n\t\t\t<goals>\r\n\t\t\t\t<goal>schema</goal>\r\n\t\t\t</goals>\r\n\t\t\t<configuration>\r\n\t\t\t\t<sourceDirectory>src/main/avro</sourceDirectory>\r\n\t\t\t\t<outputDirectory>${project.build.directory}/generated-sources/avro</outputDirectory>\r\n\t\t\t</configuration>\r\n\t\t</execution>\r\n\t</executions>\r\n</plugin> The schemaRegistryUris parameters should point to the public IP of your instance. Make sure port 8081 is actually open. The registry plugin download maven goal will allow you to download schemas from the schema registry and therefore be up to date on the latest schema version used to produce data. mvn schema-registry:download You should see a file created in src/main/avro containing the schema we used to produce data to the confluent-in-prices topic {\r\n\t\"type\" : \"record\",\r\n\t\"name\" : \"prices\",\r\n\t\"fields\" : [ {\r\n\t\t\"name\" : \"id\",\r\n\t\t\"type\" : \"int\"\r\n\t}, {\r\n\t\t\"name\" : \"item\",\r\n\t\t\"type\" : \"string\"\r\n\t}, {\r\n\t\t\"name\" : \"price\",\r\n\t\t\"type\" : \"float\"\r\n\t} ],\r\n\t\"connect.name\" : \"prices\"\r\n} Note: If you chose to change this schema as a reader you could use the test-compatibility goal and check if your schema described in <subjects></subjects> is compatible with the one stored in the Schema Registry. Note 2: You can also choose to register a new schema from your application. Once you actually got the avsc file from the Schema Registry you can use the avro maven plugin schema goal to generate POJO classes. mvn avro:schema Check out the generated class called prices under target/generated-sources/avro. We can directly use this class in our code. Note: Since this class is auto-generated during the Maven project compile or package phase, there is no point in maintaining it inside src/, any modification made would be erased every time you would package your application. Once you got your class, here price, we can deserialize any incoming avro formatted message from the input topic to a price object and manipulate it as is using the KafkaStream API. Here’s the code base to actually build a Kafka Streams application using our objects Nothing complicated here, we have a configuration to define our serde and our services like the Schema Registry and our broker. We then create a stream from the topic confluent-in-prices and we push this data back to the confluent-out-prices topic without any processing. Transformation/aggregation have yet to be implemented. If we were to run the application now, we would have a simple pass through where messages are taken from one topic and pushed to the next. Note: Depending on how your incoming data is formatted you might be able to save a few CPU cycles by connecting Kafka Streams directly to you Kafka Connect connector. More info at https://www.confluent.io/blog/hello-world-kafka-connect-kafka-streams/. KAFKA STREAMS OPERATIONS One of the big advantages of Confluent, as we’ve seen before, is to allow the duality between tables and streams, as a result, we will use a KTable as a snapshot of a stream and provide, through interactive queries, a visualisation layer of our table state. Interactive queries are a useful feature of Kafka Stream that allows other applications including other Kafka Stream applications to tap into state(s) of a particular application (i.e. tables). Because states are usually spread across different state stores (especially if your application is deployed across multiple brokers), you need to gather states from all state stores in an application to get a full snapshot. State stores in Kaka Streams are powered by RocksdB and come in two flavours depending on how you construct them, key-value stores and window stores. The main difference between the two is that a particular key can have multiple values associated depending on the window while in a key-value store, that same key will only be there once and its value continuously updated over time. Interactive queries are read-only, the processor topology which created it is the only one to have a write access to the state store.  You will never be able to write data from another application into a Kafka Stream state store. Let’s insert this piece of code in our program to create a state store Here we are processing our prices object, aggregating by item names and adding the prices of the same items together to get a total per item. The aggregate operation is actually creating a KTable maintaining a state of the total per item adding the price of incoming items to the existing ones. Note: When running the application which is going to scale out to many nodes, it’s important to remember that every time you create a state store by using a KTable you are actually creating an instance of a state store on each of the nodes where your application runs. Now that we have a persisted state for our items, we need to be able to query the aggregates. QUERY ENDPOINT There are two ways to query state stores, locally to get the state to a local store or remotely to get the full state of your application, possibly to expose it to other application. The first method requires less coordination but does not correspond what we want to achieve. We’ll use the first one because we only have one store. The second one is heavier and requires setting up an RPC endpoint which we will do anyway to simulate how we would query state stores over the network from another application. But we won’t implement the discovery part necessary to get the full state of an application over multiple state stores. Kafka Streams will discover each RPC endpoint through a property called application.server which has to match with the port you are exposing your endpoint on, here we use port 9090. Let’s design our RPC endpoint first. We have chosen to use Thrift which is a scalable lightweight cross-language service development stack. It uses a compact and fast data serialization as well as its own binary protocol to send the data over the network. It has been designed by Facebook and used by several Big Data technologies. It performs code generation on a thrift file describing the functions and data types a service i.e. the endpoint should expose. First of all, install Thrift 0.9.3 not 0.10.0 using this documentation Note: If you encounter a problem with openssl on Mac, follow this link We are going to add the following properties to our pom.xml to perform code generation with the maven thrift plugin <plugin>\r\n\t<groupId>org.apache.thrift.tools</groupId>\r\n\t<artifactId>maven-thrift-plugin</artifactId>\r\n\t<version>0.1.11</version>\r\n\t<configuration>\r\n\t\t<thriftExecutable>/usr/local/bin/thrift</thriftExecutable>\r\n\t</configuration>\r\n\t<executions>\r\n\t\t<execution>\r\n\t\t\t<id>thrift-sources</id>\r\n\t\t\t<phase>generate-sources</phase>\r\n\t\t\t<goals>\r\n\t\t\t\t<goal>compile</goal>\r\n\t\t\t</goals>\r\n\t\t</execution>\r\n\t\t<execution>\r\n\t\t\t<id>thrift-test-sources</id>\r\n\t\t\t<phase>generate-test-sources</phase>\r\n\t\t\t<goals>\r\n\t\t\t\t<goal>testCompile</goal>\r\n\t\t\t</goals>\r\n\t\t</execution>\r\n\t</executions>\r\n</plugin> Create a file named prices.thrift namespace java prices\r\n\r\ntypedef i64 long // We can use typedef to get pretty names for the types we are using\r\n\r\nservice KakfaStateService\r\n{\r\n\tmap<string,long> getAll(1:string store)\r\n} We declare one method that is going to be exposed by our endpoint for anyone to query. This getAll method is going to return the total prices per item. Run the thrift code generation using the plugin mvn thrift:compile A service class should have been generated under target/generated-sources/thrift/prices. We need to implement the actual code of the getAll method in a handler class using the generated interface. Our implementation is using the Kafka Streams API. Next, implement the state server class that will boot the thrift server and expose our endpoint i.e. our getAll method. Add the property application.server to your Kafka Streams configuration and make it point to the same port you’ve configured for your Thrift server. settings.put(StreamsConfig.APPLICATION_SERVER_CONFIG, \"localhost:\" + String.valueOf(KafkaStateServer.APPLICATION_SERVER_PORT)); We also need to add the line that will start the server in our Kafka Streams application KafkaStateServer.run(streams); Next, let’s package our application mvn package Send the jar file to the server and run the application java -jar yourjar.jar You should see you Kafka Streams application start. Now we need a client to actually query our aggregates. Note: If you were to publish an updated version of your Kafka Streams application and there are already some old instances running, Kafka Stream would reassign tasks from the existing instances to the updated one. The same happens when shutting down an instance of the application, all the tasks running in this instance will be migrated to other available instances if any. CLIENT Note: The code for the client is available at https://github.com/ArthurBaudry/kafka-stream-basket-client Let’s design our client. The whole point of using Thrift is to be able from the same thrift file to generate multiple clients or servers in a different language. We’ll stay in the same language and generate a Java client in the same exact way we’ve done for the server. Create a new project and use the same thrift file called prices.thrift to generate your thrift service class. This time, we’ll use the client part to query our remote state store. Make sure port 9090 is open on your instance. Our getAll method returns a map of total and their total price so our code will simply display the content of the map. Let’s run our code from our IDE. The client should connect to the thrift server without any trouble. Now let’s go back to our RDS instance and insert more values inside. Let’s make sure we insert the same item several times to witness the results. psql -h <rds-end-point-url> -U <user> -d <database>\r\nINSERT INTO PRICES VALUES ('laptop', 2000);\r\nINSERT INTO PRICES VALUES ('laptop', 2000);\r\nINSERT INTO PRICES VALUES ('laptop', 2000);\r\nKey is laptop and value is 6000 We inserted three times the same item and my client display the total for this item. Congratulations! You now have an end to end pipeline which carries data from a database to S3 for archiving or further processing while exposing real-time data to third parties application. Sources: Images: https://www.confluent.io/blog/hello-world-kafka-connect-kafka-streams/, https://docs.confluent.io/3.1.0/streams/concepts.html Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data and tagged confluent , kafka , kafka streaming , Streaming , Thrift . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-05-07"},
{"website": "Octo", "title": "\n                Confluent.io – Part 2: BUILD A STREAMING PIPELINE            ", "author": ["Arthur Baudry"], "link": "https://blog.octo.com/en/confluent-io-part-2-build-a-streaming-pipeline/", "abstract": "Confluent.io – Part 2: BUILD A STREAMING PIPELINE Publication date 01/05/2018 by Arthur Baudry Tweet Share 0 +1 LinkedIn 0 This article is part of a series designed to demonstrate the setup and use of the Confluent Platform. In this series, our goal is to build an end to end data processing pipeline with Confluent. Disclaimer: While knowledge of Kafka internals is not required to understand this series, it can sometimes help clear out some parts of the articles. BASICS If you have gone through every step from our previous article, you should have a Kafka broker running along with Zookeeper and Control Center. Now, we are going to implement a basic end to end data processing pipeline where the input data will come from Postgresql database (data source) and the output data will be dumped to AWS S3. Kafka Connect is used to perform simple extraction from other systems to Kafka or from Kafka to other systems. When it comes to streaming data from a database to Kafka, it usually requires hours of development from someone who’s able to understand both databases systems (e.g. how to activate CDC on a database for example) and the way Kafka works (e.g. how to produce to a Kafka topic). Kafka Connect is meant to simplify the process by taking care of those bits. Kafka Connect is made out of four components: Connectors: they define where and how to copy data to/from Kafka. Tasks: for each connector, a number of tasks can be run in parallel, tasks actively use the connector definition and actually copy the data. Depending on the size or frequency of the incoming stream of data you may need to run multiple tasks. In that case, you’ll need to specify a topic to store the tasks state. The number of tasks defaults to 1 which means no concurrency. Workers: they are JVM processes executing the tasks (along with the connectors) you defined. Workers can work in distributed mode, spreading the load of tasks across workers and taking care of rebalancing tasks when a worker fails. Converters: used to translate data to Kafka Connect internal data format upon sending or receiving the data. Because converters can be reused across connectors they are not part of the connectors implementation. For example, when streaming data from a database system to Kafka, using the Avro connector (recommended) would transform data from Kafka Connect internal data format to Avro when producing to Kafka. Under the hood what Kafka Connect is wrapping Java implementation of Kafka producers and consumers. Now that we’ve cleared that up, let’s configure the necessary parts to make Kafka Connect work. SETUP Setup topics Firstly, we have to create two Kafka topics, one where the incoming data from the database will be published and another one to publish the data which will then be written to AWS S3. We will insert a processing part in between the two topics later on. Let’s keep it simple and deal with scalability later, we will set only one replica and one partition for those two topics. kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic confluent-in-prices\r\nkafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic confluent-out-prices Run kafka-topics --list --zookeeper localhost:2181 You should see your two topics, plus lots of other topics used by Confluent to manage the platform. Setup Postgresql Next, our purpose is to work our way to an end to end application. Usually, people tend to start off by pushing data incoming from SQL relational databases to Kafka so we are going to do just that by using a Postgresql database. We could have used another database system but the Confluent supported connectors list does include PostgreSQL. Let’s start an AWS RDS instance running PostgreSQL, choose the free-tier eligible instance, that way the cost will stay low. During setup always go for simplest configuration, we don’t need much. Take note of your database name as well as your username and password. You’ll also have to make sure the security groups attached to the RDS instance are allowing your Confluent AWS instance to access it. Setup schema registry We’ll be working with the Avro data format for messages published to Kafka. Avro is a data format which enables fast and compact SerDe, but not only, more on Avro here. Using the Avro data format usually means the schema is embedded within the data which is fine for big data workload where a big schema still makes up a relatively small part of your file compared to the actual data. Things are different when using Kafka because a big schema embedded with each message sent to the brokers could impact performance drastically. To avoid this effect a schema registry is available to store Avro schemas and serve them to producers and consumers using this data format. When an Avro schema is registered, it is assigned a schema id which is in turn added to each message sent to brokers for connectors to retrieve the schema used to read/write the data from the schema registry. The schema will not be retrieved for each message as this would be constraining for the schema registry but is cached instead. The Avro data format coupled to the schema registry is a great way to maintain a contract between the data producer and the data consumer while allowing for schema evolution without breaking the consuming applications. When you feel the need to make your schema evolve (in our case imagine the database schema changes), you only need to make sure the new schema is compatible (BACKWARD, FORWARD, FULL) with the old schema. By following this contract, you ensure the consumer can still read the incoming data using the old schema i.e. depending on the type of compatibility chosen. The schema registry enforces compatibility at its level when you attempt to register a new schema or if you modify the input table in your database while using Kafka Connect. To enable Avro support we have to start an instance of the schema registry schema-registry-start /etc/schema-registry/schema-registry.properties & Setup Kafka Connect worker Copy the template for Kafka Connect properties to a temporary location and activate Interceptors. cp /etc/schema-registry/connect-avro-standalone.properties \\\r\n/tmp/worker.properties\r\n\r\ncat <<EOF >> /tmp/worker.properties\r\nrest.host.name=0.0.0.0\r\nrest.port=8083\r\nconsumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor\r\nproducer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor\r\nEOF We set the rest.host.name and rest.port to access worker information, configuration and management. We also set the interceptor classes for the producer and consumer. Interceptors are used to examine and potentially modify messages passing by (e.g in Flume). In Confluent’s case, it is used to send metrics to a specific Kafka topic which Control Center reads from and use to display and monitor usage metrics. If you need to visualise metrics about your clients (broadly meaning producers, consumers and streams), they have to implement the interceptor provided by Confluent. This particular interceptor will send metrics to Control Center every 15 seconds. You will be able to visualise things like latency (consumed_timestamp – message_timestamp), throughput, message count or checksums. Note: The metrics published by the interceptors go to a topic configured by confluent.monitoring.interceptor.topic, it will default to _confluent.monitoring. Note 2: If you have multiple Kafka clusters, it’s possible to make the interceptors of all clusters push inside this topic which is attached to a particular cluster. Obviously, you don’t want to have one Control Center for each cluster. Inside this properties file, you can find offset.storage.file.filename specifies where to store offsets. In distributed mode, this would be posted to a Kafka topic. We’ll use that feature later on. The following properties (already uncommented) enable Avro format as the one being used to transfer data from Kafka Connect format to Kafka. key.converter=io.confluent.connect.avro.AvroConverter\r\nkey.converter.schema.registry.url=http://localhost:8081\r\nvalue.converter=io.confluent.connect.avro.AvroConverter\r\nvalue.converter.schema.registry.url=http://localhost:8081 The schema of the database table will be registered automatically to the schema Registry by the connector and then used for each message sent to a Kafka topic. For that reason, we need to provide the schema registry endpoint to the worker. Finally, we need to make sure our worker knows our broker bootstrap.servers=localhost:9092 Note: Avro schema registered to the schema registry is built using metadata from the database engine. Note 2: When the schema of your table evolves, a new schema will be registered, however by default new schemas have to be backward compatible. To change this property, use avro.compatibility.level or use the REST API. Note 3: You can develop your own connector Setup input connector Finally, let’s set up the connector by creating its properties file, we’ll name it postgresql-connector.properties and put it in /tmp,add the following properties in the file name=connect-postgresql\r\nconnector.class=io.confluent.connect.jdbc.JdbcSourceConnector\r\ntasks.max=1\r\nconnection.url=jdbc:postgresql://<rds-endpoint>:5432/<database>?user=<username>&password=<password>\r\nmode=incrementing\r\nincrementing.column.name=id\r\ntopic.prefix=confluent-in- The connector class is the name of the class for the Kafka Connect JDBC connector. The number of tasks carrying data imports is set by tasks.max which defaults to 1. It’s worth mentioning that Kafka Connect does not support more than one task to connect to the same database table. The connection.url should be set to the connection string of your RDS instance. The properties mode and incrementing.column.name allows triggering an import every time the column id is incremented. The property topic.prefix is used to send over data contained in a table in PostgreSQL to a topic starting with the property’s value. Check that the database connector’s JAR (i.e. PostgreSQL) is available on the system in /usr/share/java/kafka-connect-jdbc/ Note: If you wish to add a new database connector, you can simply add it to the CLASSPATH. Note 2: it is possible to separate classpath on workers for connectors that would share different versions of the same third-party dependencies. On your AWS instance, install postgresql client sudo yum install postgresql-server Try to connect to your instance, <database> is “postgres” if you did not specify a database name when setting up RDS psql -h <rds-end-point-url> -U <user> -d <database> You’ll be prompted for your password. Now create a table named prices like this CREATE DATABASE <database>;\r\n\r\n\\c <database>\r\nCREATE TABLE PRICES(ID SERIAL PRIMARY KEY, ITEM VARCHAR(100) NOT NULL, PRICE REAL NOT NULL);\r\nINSERT INTO PRICES (ITEM, PRICE) VALUES ('laptop', 2000); CONNECTING THE PIPES In the previous part we assembled our worker and connector configurations, now let’s start Kafka Connect using them connect-standalone /tmp/worker.properties /tmp/postgresql-connector.properties After starting the worker and the connector you can use the schema registry REST API to query or update its properties. Run the following request, you will get the schema registered for your table in the schema registry curl -X GET http://localhost:8081/subjects/confluent-in-prices-value/versions/1\r\n{\r\n  \"subject\":\"confluent-in-prices-value\",\r\n  \"version\":1,\r\n  \"id\":62,\r\n  \"schema\":\"\r\n  {\r\n    \\\"type\\\":\\\"record\\\",\r\n    \\\"name\\\":\\\"items1\\\",\r\n    \\\"fields\\\":\r\n    [\r\n      {\r\n        \\\"name\\\":\\\"id\\\",\r\n        \\\"type\\\":\\\"int\\\"\r\n      },\r\n      {\r\n        \\\"name\\\":\\\"item\\\",\r\n        \\\"type\\\":\\\"string\\\"\r\n      },\r\n      {\r\n        \\\"name\\\":\\\"price\\\",\r\n        \\\"type\\\":\\\"float\\\"\r\n      }\r\n    ],\r\n    \\\"connect.name\\\":\\\"prices\\\"\r\n  }\"\r\n} This is the schema which has been automatically created from your table metadata in the schema registry. This schema will be used to serialise each message streamed to the confluent-in-price topic. It will be updated whenever the table changes its structure. Let’s also test out the REST API for workers information and configuration e.g. to get a list of active connectors for a worker (you might need to open port 8083 in your AWS security group) curl localhost:8083/connectors\r\n[\"connect-postgresql\"] Let’s run an Avro consumer in the terminal: kafka-avro-console-consumer --new-consumer --bootstrap-server localhost:9092 --topic confluent-in-prices --from-beginning\r\n{\"id\":1,\"item\":\"pendule\",\"price\":100.0} You should see the row you just inserted in your table. Keep the consumer running on one terminal and insert more data in your table. The data should flow from the database to Kafka in the same way. You have successfully setup a stream of data from Postgresql to Kafka. Take a look at your metrics within Control Center, you should see the message count incremented for your newly created topic. You can also see your Kafka Connect configuration in Kafka Connect > Sources Note: You could also have set all this up within Control Center UI. Now that we have created a flow of data from a database to Kafka, we need to configure the Kafka to AWS S3 flow. FROM KAFKA TO S3 We are going to use Kafka Connect again to dump our data in the confluent-out-prices topic to AWS S3. There could be several use cases for that, like keeping a data history, perform batch analysis/machine learning jobs on this data using Spark and Spark MLlib which all support Avro or index data in Elasticsearch for researching purposes. Since we are trying to get data from Kafka to S3, we’ll use a sink connector packaged with Confluent. But first, you’ll need to create a bucket in S3 to dump your files. Let’s create a file named confluent-s3.properties and add the following properties name=s3-sink\r\nconnector.class=io.confluent.connect.s3.S3SinkConnector\r\ntasks.max=1\r\ntopics=confluent-out-prices\r\ns3.region=ap-southeast-2\r\ns3.bucket.name=confluent-demo\r\ns3.part.size=5242880\r\nflush.size=3\r\nstorage.class=io.confluent.connect.s3.storage.S3Storage\r\nformat.class=io.confluent.connect.s3.format.avro.AvroFormat\r\nschema.generator.class=io.confluent.connect.storage.hive.schema.DefaultSchemaGenerator\r\npartitioner.class=io.confluent.connect.storage.partitioner.DefaultPartitioner\r\nschema.compatibility=NONE There’s a number of things to set here: topics should be the name of your output topic s3.region should indicate the AWS region in which your bucket is located. s3.bucket.name is the name of your bucket. flush.size determines the number of records after which a file will be committed to S3, set this to a small value to see the effect right away. You might not want to do that in a real-life production environment as writing to S3 can be slow. You also need to configure AWS_ACCESS_KEY and AWS_SECRET_KEY environment variables and have correct access rights set for these access credentials so that the connector is able to write to your S3 bucket. export AWS_ACCESS_KEY=*******\r\nexport AWS_SECRET_KEY=******* Note: You can also use AWS IAM roles. We are going to reuse the same worker.properties we used for the source connector. If your database to Kafka connector is still running, stop it and then run connect-standalone /tmp/worker.properties /tmp/postgresql-connector.properties /tmp/confluent-s3.properties Both connectors will be managed by the same worker. Now try and produce data to confluent-out-prices topic kafka-avro-console-producer --broker-list localhost:9092 --topic confluent-out-prices --property value.schema='{\"type\":\"record\",\"name\":\"prices\",\"fields\":[{\"name\":\"id\",\"type\":\"int\"},{\"name\":\"item\",\"type\":\"string\"},{\"name\":\"price\",\"type\":\"float\"}]}'\r\n{\"id\":1,\"item\":\"laptop\",\"price\":2000}\r\n{\"id\":2,\"item\":\"tv\",\"price\":1000}\r\n{\"id\":3,\"item\":\"ps4\",\"price\":500} If everything is running you should see files under topics/confluent-out-prices/partition=0/s3_topic+0+0000000000.avro with the offset difference in each filename matching exactly the flush.size property. To visualise the data, you can download and deserialize it by downloading the avro-tools jar and running java -jar avro-tools-1.7.7.jar tojson <s3_topic>+0+0000000000.avro\r\n{\"id\":1,\"item\":\"laptop\",\"price\":2000}\r\n{\"id\":2,\"item\":\"tv\",\"price\":1000}\r\n{\"id\":3,\"item\":\"ps4\",\"price\":500} Now we have both a connector publishing incoming data to the confluent-in-prices topic and a connector publishing data from the confluent-out-prices topic to AWS S3. In the next part, we will bring everything together and add processing to the mix as well as data exposition. Next part > Sources: Images: https://www.confluent.io/blog/announcing-kafka-connect-building-large-scale-low-latency-data-pipelines/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data and tagged confluent , Event , kafka , kafka connect , Streaming . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Confluent.io – Part 2: BUILD A STREAMING PIPELINE” Rupesh Malladi 12/03/2019 à 14:43 Hi Arthur -\r\n\r\nThanks for publishing this article. Can you explain the use cases as to how pulling data from database can be a streaming use case? What advantages does Kafka bring for pulling data from database and pushing it to S3? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-05-01"},
{"website": "Octo", "title": "\n                Confluent.io – Part 1: INTRODUCTION & SETUP            ", "author": ["Arthur Baudry"], "link": "https://blog.octo.com/en/confluent-io-part-1-introduction-setup/", "abstract": "Confluent.io – Part 1: INTRODUCTION & SETUP Publication date 20/04/2018 by Arthur Baudry Tweet Share 0 +1 LinkedIn 0 This article is part of a series designed to demonstrate the setup and use of the Confluent Platform. In this series, our goal is to build an end to end data processing pipeline with Confluent. Disclaimer: While knowledge of Kafka internals is not required to understand this series, it can sometimes help clear out some parts of the articles. INTRODUCTION Let’s begin with these two questions: what is the Confluent Platform and why use it? What? The Confluent Platform is a data streaming platform built around Apache Kafka, it offers an enterprise-ready solution to ingest and process data in real-time. While many of the core tools are open source and available freely, Confluent adds more components to help with data and platform management. Why? As depicted by Jay Krepps (Confluent CEO and Kafka Co-founder) in his article series dated from 2015 (part 1 and part 2), most companies information systems architecture look like this: A number of different systems or services interacting with each other with various characteristics and operational needs. In this organisation, you would have a hard time to get a single unified view of the data available throughout the company. Moreover, it seems difficult to have a practical means to extract value from the data, like a business report for instance. The first response to this increasing complexity came with Hadoop and data lakes, the first step towards data storage unification. Every single system producing data was supposed to feed a distributed storage like S3 or HDFS (part of Hadoop ecosystem) for further batch processing. It actually solves part of the problem by placing a central brick where data analysts, data scientists and other data-related professions can connect and get their raw material i.e data from. It acts as a single source of truth as long as everyone within a company is willing to pour their data in. Hadoop is good at archiving and batch processing but doesn’t do so good when it comes to low latency data processing. It’s the reason why it’s usually coupled with some low latency streaming processing framework which will archive its output onto a distributed file system/object store. While this is a suitable approach, it remains difficult to manage and operate. Confluent created its streaming platform with this in mind, keeping the part of the single source of truth but adding a way to access and process the data asynchronously and in real-time. With all systems connected to the streaming platform, the previous information system architecture would transform into this: In this architecture, each legacy system would stream its data in real time to Kafka basically acting as a gigantic buffer serving applications data from all around the information system. Each tool would take its source from within Kafka ensuring both a single source of truth and a low latency access to the data. Of course, much like a data lake, it does not relieve you from operational needs but rather concentrates it on one platform which is easier to manage. Recently released, Confluent Cloud could help you cut your operational costs even further. Now let’s jump to the implementation. CONFLUENT PLATFORM SETUP Requirements: we assume you have good knowledge about Kafka architecture and its internals if not, it would help to start by reading the documentation. First of all, here is a short introduction of the tools we are going to install and use: The famous Kafka: a distributed and reliable message broker. Confluent platform has been built around Kafka by people previously working at LinkedIn. Kafka Connect: get data from/to other systems to/from Kafka. In Confluent, a table is viewed as a stream of changes. It uses CDC (Change Data Capture) to capture the logs (e.g an update will generate a line of a log) from a database and push them into Kafka. Kafka Streams: a lightweight distributed processing framework introducing new paradigms and tightly integrated with Kafka during execution. Confluent Schema Registry: an in-memory store allowing to persist and retrieve data schemas to publish and consume messages efficient to/from Kafka. Kafka Clients: libraries available in several languages to programmatically produce and consume to/from Kafka. Kafka REST Proxy: REST API to interact with Kafka clusters, mostly used to produce and consume to/from Kafka. Not covered in the scope of this series. Kafka Security: adds an extra layer of security to Kafka services providing authentication, authorization and encryption. Not covered in the scope of this series. We’ll use AWS extensively along the way, the cost will be relatively low if you remember to stop your instance when you are not using it. You’ll need access to EC2 (to install Confluent platform), RDS (used as a data source system) and S3 (object storage). Of course, you can choose not to use AWS but it is more convenient i.e. we do not provide setup instructions for databases. The following instructions are best suited for RedHat and CentOS, please follow the installation instructions if you plan on using another OS. Spin up an EC2 RedHat 7 instance with 8GB RAM or more, at least 10GB disk and set up security groups rules so only you can access it through SSH at first. SSH into the instance (username might change depending on the OS you chose) ssh -i <path-to-keypair> ec2-user@<public-ip> Install the JDK 8 sudo yum install java-1.8.0-openjdk-devel Add Confluent’s public key sudo rpm --import http://packages.confluent.io/rpm/3.2/archive.key Create a file named confluent.repo in /etc/yum.repos.d and add [Confluent.dist]\r\nname=Confluent repository (dist)\r\nbaseurl=http://packages.confluent.io/rpm/3.2/6\r\ngpgcheck=1\r\ngpgkey=http://packages.confluent.io/rpm/3.2/archive.key\r\nenabled=1\r\n\r\n[Confluent]\r\nname=Confluent repository\r\nbaseurl=http://packages.confluent.io/rpm/3.2\r\ngpgcheck=1\r\ngpgkey=http://packages.confluent.io/rpm/3.2/archive.key\r\nenabled=1 Clean your dependencies sudo yum clean all Install the platform sudo yum install confluent-platform-2.11 At the end of the installation, you’ll find scripts to start and stop Confluent services in /usr/bin, configuration files in /etc/<package> and jars in /usr/share/java/<package> To ease control and visualisation of the components we are going to start, let’s start by running the Confluent Control Center. The Control Center is a web UI and is part of the enterprise edition of Confluent, it is designed to manage and monitor services. It uses specific Kafka topics to publish metrics later displayed in the UI which give you the ability to trigger alerts on metrics thresholds. Since Control Center uses Kafka, we need at least one Kafka broker up and running so we’ll need to start Zookeeper first zookeeper-server-start /etc/kafka/zookeeper.properties & Then edit /etc/kafka/server.properties and uncomment the following lines metric.reporters=io.confluent.metrics.reporter.ConfluentMetricsReporter\r\nconfluent.metrics.reporter.bootstrap.servers=localhost:9092\r\nconfluent.metrics.reporter.zookeeper.connect=localhost:2181\r\nconfluent.metrics.reporter.topic.replicas=1 Those properties set the Confluent Metric Reporter which collects and publishes metrics on Kafka clusters to its own topic named _confluent-metrics by default. A full list of properties and their description is available here. Start a Kafka broker kafka-server-start /etc/kafka/server.properties & A Kafka broker is now running. Copy and modify Control Center properties setting the partition and replication numbers for the Control Center topics as well as for the monitoring topic where metrics about other topics are stored. An exhaustive list of properties can be found here. cp /etc/confluent-control-center/control-center.properties /tmp/control-center.properties\r\n\r\ncat <<EOF >> /tmp/control-center.properties\r\nconfluent.controlcenter.internal.topics.partitions=1\r\nconfluent.controlcenter.internal.topics.replication=1\r\nconfluent.controlcenter.command.topic.replication=1\r\nconfluent.monitoring.interceptor.topic.partitions=1\r\nconfluent.monitoring.interceptor.topic.replication=1\r\nEOF Uncomment the following line to access Control Center from your laptop. confluent.controlcenter.rest.listeners=http://0.0.0.0:9021 Finally, start Control Center control-center-start /tmp/control-center.properties & Set up a Custom TCP rule for port 9021 in the AWS Security group attached to your EC2 instance and start browsing at http://<aws-public-ip>:9021 Take a look at the Control Center and try to make sense of the metrics and features available to you. All set! You now have one Zookeeper node, one Kafka broker and Control centre running. You should see 1 broker available and a bunch of metrics about it in the bottom part of the screen in Control Center. As you can see, it is easy to bootstrap a partly monitored platform in just a few commands. In the next part of this series, we will start setup up a data processing pipeline. Next part > Sources: Images: https://www.confluent.io/product/confluent-platform/ Installation: https://docs.confluent.io/current/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data and tagged confluent , kafka , Streaming , tutorial . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-04-20"},
{"website": "Octo", "title": "\n                JenkinsX – new Kubernetes dream? Part 2 [Technical!]            ", "author": ["Mykyta Shulhin", "Ilya Trofimov"], "link": "https://blog.octo.com/en/jenkinsx-new-kubernetes-dream-part-2-technical/", "abstract": "JenkinsX – new Kubernetes dream? Part 2 [Technical!] Publication date 19/04/2018 by Mykyta Shulhin , Ilya Trofimov Tweet Share 0 +1 LinkedIn 0 Alright, stranger! As we pass through the stage of basic Jenkins X overview in our first part , we are ready to go to infinity and go beyond the general application of Kubernetes environment. In this article, we are going to step further into Jenkins X world, by going through a step-by-step setup process and creating quickstart project. Furthermore, you will get an overview of what is going on in the background, detailed description of Git flow, release management and magic behind deployment automation. Please, be advised, that following tutorial is based on local Minikube Kubernetes cluster for simplicity and clearer understanding. Considering a tough journey is ahead, I will give you a basic set of definitions to solve most of the problems. Try to remember all of them! Word by word. I’m not joking! Term Definition Helm Kubernetes Package Manager Tiller Helm server side in Kubernetes Helm packages Pod A group of one or more containers which share storage, network and contain Node (minion) Worker machine which runs Pods Minikube Single-node Kubernetes cluster NGROK / Localtunnel Utility to expose localhost to the web chart Application definition for Kubernetes Chartmuseum Helm Chart repository Nexus Artifact repository manager Docker registry Docker image repository MongoDB NoSQL database Ingress Allows external users and client applications access to HTTP services What will be required for successful completion of this mission: MacOS with brew package manager ( https://brew.sh gold http://jenkins-x.io/getting-started/install/ for other platforms ) VirtualBox, VMware Fusion, or HyperKit (recommended) for virtualization purposes. Command-line tool for Kubernetes management – Kubectl. Single Kubernetes cluster Minikube . Huge Swiss Army knife. Wait … Wrong tutorial. No need for it. Jenkins X installation In this part, we will explain how to install Jenkins X (JX) on the local Minikube Kubernetes cluster. Once everything is set-up, we can catch the wild JX with a help of brew: brew install jx Once JX has been successfully captured, we can interact with it via CLI tool. Our first step to initialize JX pipeline is to create a cluster based on Minikube. To do so, we need to invoke: jx create cluster minikube -local-cloud-environment = true The previous command will trigger setup and configuration qui will prompt for :: Amount of RAM allocated to the VM minikube (by default 4096 MB) Number of CPUs allocated to the VM minikube (by default 3) VM driver (it is recommended to use hyperkit instead of xhyve, since it will be deprecated soon) As a part of the setup, Jenkins will install Tiller and Helm. During the next stage, the JX install will prompt for Git credentials for CI / CD. After that, the installer will create Pods for: Chartmuseum Docker registry Jenkins MongoDB Nexus repository The default admin password will be displayed, which is used by all services. In case you clear the terminal without capturing the password, it is possible to retrieve it in BASE64 format and decode it. Run command to display secret for a particular JX service : kubectl get secret jenkins --namespace = jx -o yaml After that, staging and production repository environments will be created locally and pushed to specified Git. Each has the following structure: . ├── Jenkinsfile (Jenkins buildscript) ├── LICENSE ├── Makefile (Helm package builder script) ├── README.md └── approx ├── Chart.yaml (Wrapper for Chart application) ├── requirements.yaml (Track file version) ├── templates └── values.yaml (Ingress exposure configuration) So, what has happened after installation? We got a complete set of solutions with different services . All services are hosted on Minikube. In addition to the standard default , kube-public and kube-system namespaces, JenkinsX created jx-production , jx-staging, and jx (development) environments. To get a list of all available environments we can run: jx get env It is worth to note, that only staging and production environments contain a Git repository address, which point to a remote. Development environment is created only for a context of development tools. Our team asked the question to lead JX developers and got reply: It could be a good idea to be a “dev” environment to “developer tools” in order to avoid confusion, but such a decision should be made by community :) Bear in mind that when Kubernetes is running on a local minikube cluster, Jenkins is unable to receive webhooks calls from a remote repository (such as GitHub) since the local address will not be reachable from outside the network unless configuring some port forwarding. A simple solution is to expose local address via services such as NGROK or LocalTunnel. To check all Services running on Kubernetes cluster run: kubectl get svc Result would look like: name Type Cluster-IP External-IP Port (s) Age heapster CLUSTERIP 10.98.140.95 <None> 8082 / TCP 5d jenkins CLUSTERIP 10109220237 <None> 8080 / TCP 5d Jenkins agent CLUSTERIP 10.96.101.173 <None> 50000 / TCP 5d Jenkins-x-chartmuseum CLUSTERIP 10.109.214.28 <None> 8080 / TCP 5d Jenkins-x-docker-registry CLUSTERIP 10.106.57.184 <None> 5000 / TCP 5d Jenkins-x-mongodb CLUSTERIP 10102247177 <None> 27017 / TCP 5d Jenkins-x-monocular-api CLUSTERIP 10106221194 <None> 80 / TCP 5d Jenkins-x-monocular-prerender CLUSTERIP 10.97.208.46 <None> 80 / TCP 5d Jenkins-x-monocular-ui CLUSTERIP 10.104.74.160 <None> 80 / TCP 5d nexus CLUSTERIP 10108159155 <None> 80 / TCP 5d Figure 1. Kubernetes services Some of the services are exposed to outside networks via Ingress out of the box. To check detailed Service configuration run: kubectl describe svc <SERVICE_NAME> . Here is sample output for ‘nexus’ service: Figure 2. Nexus pod Service configuration (exposed) In particular cases, ‘nexus’ service exposes a public URL which can be accessible from outside of the cluster. When the service is not exposed, it can be accessed only within the cluster itself (example of ‘jenkins-x-mongodb’): Figure 3. MongoDB Service configuration (not exposed) Congratulations! You successfully installed JX on your local Minikube cluster! Stay tuned, next step will be to create and promote our first application with a help of Jenkins X. Meanwhile, here is your trophy: Jenkins X create quickstart project One of the best options to get started with Jenkins X is to create a project from a provided template. JX provides a selection of quickstart projects for Node JS, Rust and GoLang amongst others . jx create quickstart will launch installation process which will ask for language selection, set up a git repository and push the project, which in addition to standard code source will contain Docker, Jenkins file and compilation Makefile. All quickstart projects are cloned from https://github.com/jenkins-x-quickstarts repository, provided by Jenkins. JX project advertises the opportunity to create custom quickstart projects and which can be submitted via pull request to be included in samples. Once quickstart project is created, it is registered within Chartmuseum via Monocular: Figure 4. Monocular with quickstart NodeJS application To ensure our application image is pushed to Docker registry, we can send a request curl http://docker-registry.jx.192.168.64.4.nip.io/v2/_catalog?n= 1 which gives a first list of Docker images: { “repositories”: [ “jenkinsx/node-http” ] } Figure 5. Response from Docker registry By default, the created application doesn’t belong to any of environments (Even though expected to reside within development context). By definition, the development environment only contains services which were created by Jenkins X. To make application visible within staging or production environment, it should be promoted . Jenkins X promotion workflow Let’s assume we have our newly-created Node JS application ready. And now we want to add some feature (display particular text on the main index.html page). Firstly, we create a branch from master called feature-6 , and modify a single file with our new change. For a better feeling of how build and testing is performed by JX, we will also include a failing on purpose test on this branch. Once we finished, changes are committed and the branch is pushed to the remote Git server (GitHub in our scenario). Since our local Jenkins server is web-hooked to remote GitHub, branch pipeline is automatically triggered locally. But wait! Since our branch name doesn’t satisfy PR-* convention, it is only pushed on the server without being built or tested: As result of the particular configuration, Jenkins shows success on latest push on feature-6 branch (even though one test has been set on purpose to fail): Next step is to create Pull Request to merge our feature branch to master. It can be done from GitHub UI in repository dashboard: Once pull request is created, JenkinsX automatically creates PR-* branch, and builds/runs all tests before the merge. Since our test was purposefully failing, GitHub was unable to merge pull request created by JX: And following description, Jenkins provides an overview of failure: After the test has been fixed, JenkinsX refreshes PR-* branch with a new version of feature branch and runs build/test process to ensure PR can be merged: In addition, to test and build check, JX also creates a preview environment which can be accessed from GitHub pull request comments section: Yes! We can check our application straight on preview environment before merge request is approved! But what happens after accepting merge request? Well, that is where JX shows its most powerful features with automatic versioning, artefact management and environment management. There are two main stages in approved PR pipeline: Build Release stage: Jenkins runs tests, creates Docker image and pushes it to local Docker registry. Promote to Environments stage: Jenkins creates release information in repository including archived source code and pull request history. It also creates a Chart and pushes it to Chartmuseum, which will be accessible after from Monocular to be pulled via Helm. It is worth to mention, that Chartmuseum will contain all versions of Charts pushed to it. In addition to described stages, Jenkins also includes versioning information to requirement.yaml in staging environment repository and makes application to be accessible from stage environment URL : But what about resources? Resource-wise , Jenkins requires decently-powerful hardware to be run on. Considering default RAM for VM is 4Gb and minimum 3 CPU, it is impossible to create JX cluster on a free AWS EC2 Micro instance (for example). As a load comparison, here is a snapshot of Kubernetes allocated resources dashboard before any worker execution: Figure 6. Kubernetes allocated resources dashboard (idle) From provided snapshot it is clear that all CPU and RAM limits are within allocated capacity. In order to understand resource allocation load during building process, we submit a pull request which invokes Node JS worker. As a result, we are getting dramatic changes in our dashboard: Figure 7. Kubernetes allocated resources dashboard (during build process) Both CPU and Memory allocation resources limits have been increased (by 5.33% in CPU and 13.98% in RAM). Pods allocation remained on almost the same level with unnoticeable change in less than 1%. Considering only one quickstart application has been run through the pipeline, situation would change while processing couple of different applications simultaneously. ????????????Congratulations! application made it through entire pipeline! Freshly built, tested and deployed to repository – it is versioned, and chart can be pulled using helm. Here is your trophy! You deserved it. Conclusion So, once we discovered the wild Jenkins X, let’s see what is behind its magic? JX provides a no-hassle, automated management of your application which has microservices [b] [c] architecture. Establishing such a pipeline manually, connecting all services together and managing releases/artefacts would be quite complicated. Features we liked: Automated promotion to environments Artefact management and deployment Live preview environment of application Great set of out-of-the-box tools Complimentary automated commit messages on build results Having a tool that is capable to take care of CI/CD processes in Kubernetes environment is a  big deal, considering scarcity of open source competitors. From personal experience, developers are dedicating a lot of time to improve Jenkins X and reply on any issue almost instantly. Growing popularity of Kubernetes-based applications requires a reliable CI/CD solution, and Jenkins X does it very well. There is no doubt this solution will become an official part of Jenkins family and take a major role in establishing Kubernetes pipeline management , which allows Jenkins to compete against other solutions that propose working pipeline out-of-the-box. JX is also a perfect product for those companies which value on-premise aspect, which means having a full control over pipeline without depending on vendor or subscription. Authors downunder: Ilya Trofimov and Nick Shulhin ❤ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-04-19"},
{"website": "Octo", "title": "\n                JenkinsX – new Kubernetes dream? Part 1            ", "author": ["Mykyta Shulhin", "Ilya Trofimov"], "link": "https://blog.octo.com/en/jenkinsx-new-kubernetes-dream-part-1/", "abstract": "JenkinsX – new Kubernetes dream? Part 1 Publication date 17/04/2018 by Mykyta Shulhin , Ilya Trofimov Tweet Share 0 +1 LinkedIn 0 Hi there! Today we’re going to share about Jenkins X . I think every developer knows what Jenkins is. Or at least has heard of it once. But what is ‘X’? Basically, Jenkins X is not just a CI/CD tool to run your builds and deployments, it is an attempt to automate the whole development process end to end for containerised applications based on Docker and Kubernetes. It is obviously open source, as all best applications. This sounds very promising at a high level. So, let’s see what’s behind the glam cover of Jenkins X! TL;DR: Here is our takeaway  from the Jenkins X project: Though project has been made public just recently, it appears to be mature enough to make use of it in Production systems. Although migration of existing CI/CD pipelines into Jenkins X might be found difficult due to existing custom processes, new projects may benefit from using Jenkins X a lot, as it saves significant amount of effort by setting up basic GitOps flow out of the box. Customisation of the Jenkins X system shouldn’t be a problem as it utilizes already familiar tools like Jenkins, Make, Docker, Helm Strengths of Jenkins X: Entry level for DevOps/GitOps implementation lowers significantly due to E2E automation of GitOps processes Strong concept: GitOps processes as a foundation of the project Good toolset out of the box, which is already configured and works (k8s, Jenkins , Docker registry , Chartmuseum , Monokular , Nexus ) Quickstarts make a creation of new apps an easy ride Ability to customise the pipelines and their templates Not that good points of Jenkins X: Jenkins X is another framework to learn. It requires understanding of its concepts, though still requires understanding of Kubernetes, Helm , Docker concepts It is only the early beginning of the project, still a lot of things to implement and improve Documentation is not always comprehensive and only provides basic information. That entails lots of questions and makes it sometimes difficult to understand all the caveats. Only a few quickstarts are available and it requires to raise an issue on Jenkins X Github to create new ones. Our opinion on Jenkins X is that it goes into right direction, addressing the pain points and streamlining implementation of GitOps principles. So it’s effectively DevOps for everyone, providing basic, standardised implementation, though there is still a room for customization. Most probably, we’ll see more radical implementations of similar tools in future, making PaaS implementations even easier, but less customizable. However, this is the common technological trend: deliver things simpler, faster, cheaper . Ok, if you want to get to know Jenkins X closer, let’s jump into details! The problem The main problem, which Jenkins X (‘JX’ hereafter) is going to address is formulated by its creators as ‘ make it simple for developers to work to DevOps principles and best practices ’. Some of that are: Frequent deployments Low Mean Time to Recover CI/CD Configuration as Code Automated Release Management In plain English, that means JX takes a challenge to automate a creation of development environments and facilitate development process. So, once JX is installed and set up, it does: create a Git repo for a new application create a pipeline configuration in Jenkins for a new application and connect it with a Git repo automate the DevOps processes (like builds, artefacts and containers creation and publishing, deployments) based on Git operations (branching, commits, PR creating, PR merging) Jenkins X building blocks: In order to achieve the goal and solve the problem set, JX makes use of 4 main building blocks: Jenkins X – a standalone platform (not just an extension of Jenkins), that coordinates all the components below. Provides an abstraction layer which facilitates the communication and management of those system components. The abstractions of Jenkins X utilise the terms of DevOps (e.g. application, environment, promotion) explicitly, thus converging terminologies of different technologies. Comes with a CLI to manage the platform. Git – stores all the code and configurations, including environments setup. Serves as a source of truth for everything. Jenkins X manages remote Git repositories and follows the GitOps principles (explained further in the article) Kubernetes – manages containers at scale, providing work environments for all developing and administrative applications. The resources inside k8s are managed with help of Helm and organized in Helm Charts for convenience. Jenkins X designed to install all the modules on k8s cluster. It also can create a k8s cluster itself with the following cloud providers supported: AWS GKE Azure Minikube OpenShift (planned) EKS (planned) Jenkins – open source CI/CD solution. Jenkins X make use of Jenkins to create and run CI/CD pipelines. Jenkins X abstractions: As different components of the system utilise different vocabulary, Jenkins X tries to span across all domains with its own abstraction layer. And the most important entities of that one include the following items: Environment – is a container where Applications are deployed to. By default, ‘ staging’ and ‘ production’ environments are created during JX installation to k8s Application – is a representation of an application being developed. There is much more entities than that in JX, but the ones above are essential for understanding the concept of JX. Jenkins X components relationship Now, that we know the main building blocks, let’s try to put them together. Next picture shows how parts of JX system are connected in static: Figure 1. Components of the system As you can see, associations between different entities of the components are pretty straight forward: JX abstractions are mapped 1:1 to Git, k8s and Helm objects. To get more understanding of how everything works, It’s worth to consider each of the components in more details: Kubernetes and Helm Kubernetes is the foundation of a development infrastructure. Its ecosystem covers most of the software development and operational needs in modern IT world. Distributed cluster manages Docker containers and provides features like auto-scaling, self-healing, secrets and configuration management, automatic rollout/rollback and others. For more about Kubernetes, please check out following articles: https://blog.octo.com/en/how-does-it-work-kubernetes-episode-1-kubernetes-general-architecture/ https://blog.octo.com/en/the-twelve-factors-kubernetes/ Kubernetes hosts all services deployed by JX, including administrative ones (Jenkins, Chartmuseum, Monocular etc). Deployment of the services (or applications) is coordinated via Helm. Helm’s Charts allow sharing of application templates and makes versioning easy. Helm also takes care of upgrade and rollback cases, which makes it quite useful. Each Helm Chart is a folder, that contains the following files: Chart.yaml : contains Chart’s metadata like name, version, description etc requirements.yaml : describes Chart’s dependencies on other charts. Each dependency consists of chart’s name, alias, version and repository of that dependency. templates/ directory: contains template files written in Go template language . Templates describe what containers to use, their replication factors, services descriptions etc values.yaml : contains default values for templates As part of JX installation, following services are deployed to facilitate Helm Charts managements: Chartmuseum: Helm Charts repository, helps to manage charts via rest api Monocular: web-based UI to Helm Charts repository Jenkins Jenkins instance is deployed to Kubernetes cluster as part of the installation of JX. It runs in “master/agent” distributed mode. That means there is always a ‘master’ process running, serving requests to Jenkins console and distributing the load to multiple ‘agents’. This allows to process multiple pipeline runs in parallel and evenly distribute the load, create/delete agent nodes based on demand. Master and each of the agents are deployed as separate Pods in Kubernetes, this can be easily monitored via kubectl CLI or a dashboard. Jenkins pipelines are typically described in a configuration file called Jenkinsfile . This file contains set of stages to execute for each pipeline. It’s normally checked into VCS along with an application code. Git In order to fulfil GitOps requirements, Jx uses Git to store two types of data: Application repo : this is the code of particular applications itself, something that developers work on. Each application is associated with one Git repository. Apart from the code, such repos also include: Jenkinsfile : describes Jenkins pipeline configuration for the app Dockerfile : contains Docker  image building instructions for an application charts/ directory : contains a set of charts for an application: Chart config for app version . Used when app version is created (first time promoted to any environment). Also includes Makefile to describe how to build a version Chart config for preview . Used only when Git PR is created for feature branch. Facilitates a process of review and merge approval for a PR. Also includes Makefile to describe how to build a preview Environments repo : these contain a set of configuration files, which describe environments metadata. Each environment is associated with one Git repo. Also includes: Jenkinsfile : contains Jenkins pipeline configuration for the environment. In conjunction with Makefile describes steps on how to (re)build the environment Makefile : contains environment building commands. In current implementation, it completely relies on Helm capabilities, in particular ‘ helm upgrade ’ command env/ directory: contains configuration of environment contents. That means, what applications of which versions should be deployed there. So, JX is connecting all these pieces together with its smartly weaved fabric of automation scripts and configurations. And the pattern of that fabric is known as GitOps . GitOps What is GitOps? This is a set of principles for managing software and infrastructure based on Git: Git is considered to be a Source of Truth for everything, from code to environments setup. Any operational changes, including environments config updates, are done via Pull Requests A declarative configuration of infrastructure is implied Any divergence from Git version is detected by diff tools and convergence mechanism is triggered In case if rollback is needed or disaster happens, the state of the system can easily be pulled from Git As you can see, there is a direct association between JX Environments, Git Repos and k8s Namespaces. Actually, JX environment is nothing more than just an abstraction, which keeps configuration in specific Git repo and deploys to the corresponding k8s namespace. Applications (in our case it’s myGreatApp ) are another abstraction in JX. For each of the applications, corresponding Git repo is created. Different versions of apps can be deployed (promoted) to different environments in k8s, but only through changing of the configuration of particular environment in Git. Jenkins X CLI Now, we’re coming to the main element of JX, the tool which does all the magic, glueing all building blocks together and providing an entry point to a system management and orchestration. It’s a JX CLI interface and it’s written in Go. The JX CLI is utilized by end users to manage resources (apps, environments, urls etc) as well as in Jenkins pipelines created by JX. Some of the most important commands are provided below: jx install – installs JX on k8s cluster jx create – creates JX resources and associated underlying services (e.g. k8s namespaces, pods) jx import – imports a project (code) into JX. It then creates all required objects (e.g. Git repo, Helm Charts, JX objects) jx preview – creates a temporary preview environment for an application version jx promote – promotes an application version to a specified environment Jenkins X Flow As a next step, let’s consider the lifecycle of the application development, from the initiation and all the way to the Production deployment. At a high level, it’s presented on the diagram below: The picture above shows, that JX covers not only operational processes based on Git, but also provides the ways of initiating an application, as well as promoting to Production, thus covering e2e cycle. Let’s have a look at how the JX processes correlate to Git actions. For simplicity, let’s use only master and one feature branch. The table details the steps of the flow. Figure 2. Git workflow events Jenkins X installation Alright, so once we’ve covered got some overview of JX, let’s see what happens when we install it! But before that, the main prerequisite for JX installation is to have Kubernetes cluster (or Minikube – simplified single-node Kubernetes cluster) installed, as well as command-line Kubernetes CLI – Kubectl and lastly – VM environment if we choose to go with Minikube (either VirtualBox, VMware Fusion, or HyperKit). There is a lot of things happening with our Kubernetes cluster in background of the installation. Below is a diagram showing the process of what JX does: Figure 3. JenkinsX moved functionality Once JX is installed, it initialises Pods with the following applications in jx namespace of k8s: Chartmuseum (Open-source Helm Chart repository) Docker registry (Docker image storage) Jenkins (Automation server) MongoDB (NoSQL Database) Nexus repository (Artefact storage) This jx namespace of k8s is associated with the Dev environment in JX. Those host all utility tools which are used for development, but are not being developed themselves. Apart from Pod installation, JX creates staging and production environments that hold versions of applications being developed. These environments configurations are pushed to remote Git repos and webhook is created to link to Jenkins installation in k8s. It is worth to mention that JX exposes IPs of some Pods via Ingress. Which means you can access all the utility tools like Docker Registry, Monocular or Nexus from an external to k8s network. Jenkins X: create new application JX supports the creation of a Quickstart application based on Node JS, Spring boot, GoLang or Rust (as of now) . This is basically a creation of application structure from template. When quickstart is created, JX initializes a local repository, unpacks code template, adds the Jenkinsfile template, Helm charts, Dockerfile and Makefile from so called ‘draft’, and then pushes the new repo to the provided remote Git. Figure 4. Jenkins X quickstart installation and first pipeline run As a part of quickstart installation, JX initializes the first pipeline as following: Create an app image, push it to Docker registry Create local Jenkins repository and push it to remote Git Create Application chart and publish it to Chartmuseum (could be checked on Monocular) Considering the type of application, if any artefacts are created (such as Spring boot app), they will be pushed to Nexus when a new version of app is created/promoted. Jenkins X community Even though JX positions itself as a sub project of the Jenkins Foundation, it has a very vibrant and responsive community. During our initial product exploration, we opened issues on GitHub page regarding JX functionality. Surprisingly, major contributors resolved our issues within a couple of hours with immediate actions. We are very pleased with such a fast feedback. Figure 5. Example of GitHub issue discussion Conclusion Jenkins X spans quite a number of technologies and though it is intended to make application development lifecycle easier, it still requires an understanding of many technical concepts. Apart from the complexity of the toolset, Jenkins X appears to be a promising project which can level up a DevOps experience on Kubernetes and reduce initial implementation costs. Its Quickstarts and configurable drafts can fulfil the customisation needs of picky DevOps engineers allowing (with some additional effort) to go beyond basic GitOps flow and standard application templates. The well articulated roadmap shows the width of intentions and instills confidence in the ability of Jenkins X to cover a variety of technical cases. It would be nice to see more detailed documentation on main principles and architecture of the system, as well as technical details on some CLI commands. There is also very limited information available on making custom extensions like Quickstarts and drafts. Table of definitions name Description Git The most popular VCS Jenkins An open source CI / CD automation platform Jenkins X A subproject of Jenkins, automates CI/CD processes in Kubernetes based systems Kubernetes a.k.a. k8s, open source, distributed management system of containerized applications Minikube A tool to run Kubernetes locally GitOps A set of principles for managing software and infrastructure based on Git Docker A containers management platform Docker registry A repository to store and distribute Docker containers Nexus An artifact repository Helm A package manager for Kubernetes Helm Chart A collection of configuration files that describe a related set of Kubernetes resources. E.g. chart, describing an application stack Chartmuseum An open-source Helm Chart Repository Monocular An open-source web-based UI for Helm Chart management Quickstart Pre-made Jenkins X applications templates a developer can start a project from, instead of starting from scratch Authors downunder: Ilya Trofimov and Nick Shulhin Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “JenkinsX – new Kubernetes dream? Part 1” Jet 13/05/2020 à 13:20 Very useful article, however, few things are outdated. Have you republished the article with considering `jx boot` Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-04-17"},
{"website": "Octo", "title": "\n                The upstream Kubernetes            ", "author": ["Yassine Tijani", "Eric Favre"], "link": "https://blog.octo.com/en/the-upstream-kubernetes/", "abstract": "The upstream Kubernetes Publication date 16/04/2018 by Yassine Tijani , Eric Favre Tweet Share 0 +1 LinkedIn 0 Kubernetes has quickly become the de facto standard for container orchestration. If the basics are now well understood, the new “upstream” features are much less, even though they make the product richer and able to address some very specific use cases. This article is a review of these new features, as well as the upcoming ones that come with the 1.10 release. A prior knowledge of kubernetes is a prerequisite. Disclaimer: some of these features are alpha, meaning that some backward incompatible changes may be released with the upcoming versions. Always refer to the release-notes when upgrading. Kubernetes and node management Taints and tolerations are Kubernetes features that ensure Pods are assigned to appropriate nodes. A taint is composed of a key or a key/value pair (the value is optional), and an effect. Three effects are available: A taint with the NoExecute effect indicates that no more Pods should be assigned to the node and that it should be drained out. This enables the creation of eviction policies based on taints. A taint with the NoSchedule effect indicates that no more Pods should be assigned to the node. A taint with the PreferNoSchedule effect indicates that Pods should be assigned to this node only as a last resort. In some cases, the cluster administrator needs to tolerate taints for some reasons. A toleration is a Pod’s property. It’s composed of a tolerated taint’s key, an optional value, a toleration delay (infinite by default), and an operator. There are 2 types of operators: The Exist operator indicates that the check was done on the key and effect The Equal operator performs the same checks as the Exist operator, and also checks that the optional value is the same as on the taint The taints and tolerations introduce new behaviours in Kubernetes, such as tainting node with their NodeCondition . This allows to tolerate some states and to assign Pods even though the node is tainted with networkUnavailable , so as to diagnose or prepare a node deployment. Kubernetes makes kube-scheduler a critical component The scheduler is getting richer and richer, starting with the priority & preemption APIs that begin to move towards beta support. Indeed the critical pods will integrate directly with the priority API instead of relying on rescheduler and taints. Preemption will become more complete in the next version, managing Pods preemptions to assign DaemonSet when resources forbid it. To get there, the assignment will no longer be handled by the DaemonSet controller but directly by the kube-scheduler. This makes the kube-scheduler a critical component when starting a cluster with tools that deploy masters as DaemonSet, as early as the next version. The preemption management when several schedulers are in use will happen in version 1.11. Performance wise, the version 1.10 will embed the Predicates-ordering design. Short reminder, the scheduler algorithm operates a suite of predicates, a set of functions, to determine if a pod can be hosted on a node. The eligible nodes are then prioritised by a set of prioritisation functions to elect the best fitted node. The scheduler will define the predicates execution order, so as to execute first the higher restriction and less complex predicates. This will optimise the execution time, and if a predicate fails, the remainder of predicates will not be executed. This last part will be detailed further in a following article, to present the work done by Googler Bobby Salamat about it. Towards more modularity Kubernetes is becoming more modular and easier to extend with external contributions, such as CNI (Container Network Interface) and CRI (Container Runtime Interface) currently. It now becomes important to split Kubernetes’ core when it come to: Integration with cloud providers thanks to CCM (Cloud Controller Manager) Integration to storage management thanks to CSI (Container Storage Interface) Kubernetes addresses the Cloud with the Cloud Controller Manager Ever since Kubernetes 1.8, the Cloud Controller Manager makes possible for any cloud provider to integrate with Kubernetes. This is a great improvement as they don’t have to contribute directly into Kubernetes code. Thus, the release pace is given by the provider and not by the Kubernetes community, which improves velocity and the variety of features proposed by the cloud providers. Some providers are already developing their own implementation: Rancher , Oracle , Alibaba , DigitalOcean or Cloudify . This works by relying on a plugin mechanism, as any cloud provider implementing cloudprovider.Interface and registering to Kubernetes can be linked to the CCM binary. In the next releases, every cloud providers (including providers already supported by kubernetes/kubernetes ) will implement this interface outside of the Kubernetes repository, making the project more modular. For instance, here is the roadmap discussed by the Azure community. Kubernetes launches CSI (Container Storage Interface) Launched in version 1.9 in alpha, CSI is a spec that allows any provider implementing the specification to provide a driver that allows Kubernetes to manage storage. CSI will go alpha in version 1.10. The pain point addressed by CSI is twofold. Like CCM, it enables externalization of storage drivers and makes it possible for providers to define their release pace. Secondly, CSI resolves the hard installation issue of the FlexVolume plugins. If you want to write your own driver, see here how to deploy it. Kubernetes v1beta1.Event to the rescue Events have always been an issue on Kubernetes, semantic wise and performance wise. Currently, the whole semantic is embedded in a message, which hardens considerably the information extraction and the events requesting. Performance wise, there is room for improvement. For now, Kubernetes has a deduplication process to remove identical entries, which decreases the memory footprint of etcd, Kubernetes’ distributed key-value store. However, the number of requests to etcd remains an issue. The version 1.10 will introduce the new event processing logic which should release this pressure. The principle is simple, event deduplication and events updates will happen periodically, which will greatly decrease the number of api-server calls  for writing requests to etcd. This part will be further detailed in a following article, to present the related work by Googler Marek Grabowski and Heptio’s Timothy St. Clair . Kubernetes  HPA – Horizontal Pod Autoscaler Kubernetes will make it possible to scale your Pods depending on custom metrics. This feature is now beta. An aggregation layer makes it possible to extend the api-server beyond the native APIs provided by Kubernetes. It does so by adding 2 APIs: Resources Metrics API: collect metrics called resources, namely CPU and memory. The nominal implementation is Metrics-Server , that makes Heapster redundant. Custom Metrics API: collect custom metrics, which facilitates the use tools such as Prometheus to manage auto-scaling. An adapter polls Prometheus metrics and makes them available as auto-scaling thresholds. There are various use cases, and this allows to address each of their specific needs. This part will be detailed in a next article. Kubernetes federation The need for deploying multiple clusters becomes more and more mainstream. Kube-fed offers to manage that across on-premise sites, various cloud providers or on different regions of a same hosting service, which enables a considerable flexibility. Resources are relatively mature , and several features were added for federation: The HPA support is on its way, with the same principle as for a cluster, except that the processing is done on another level: the federation one ensures auto-scaling on multiple clusters. The private image registries for federation is planned for version 1.10. It will be possible to choose on what node to deploy the federation-controller thanks to the nodeSelector. Conclusion The Kubernetes eco-system is reaching a satisfying maturity level, and keeps improving to address the many needs of its users. The possibility to extend Kubernetes will offer the users a way to address some very specific use cases. The next articles will detail more thoroughly some of the features introduced in this article such as the HPA principles, the event management, etc. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations and tagged container , DevOps , Kubernetes , orchestration , upstream . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-04-16"},
{"website": "Octo", "title": "\n                BDD (Behavior Driven Development) – The essential in one drawing!            ", "author": ["Jennifer Pelisson"], "link": "https://blog.octo.com/en/bdd/", "abstract": "BDD (Behavior Driven Development) – The essential in one drawing! Publication date 11/04/2018 by Jennifer Pelisson Tweet Share 0 +1 LinkedIn 0 Today, I propose you a summary, in poster format, of the french podcast: « Café Craft – Behavior Driven Development avec Cédric Rup » Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile and tagged 3 amigos , agile , BDD , Behavior Driven Development , Best Practices . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-04-11"},
{"website": "Octo", "title": "\n                Woody Zuill about Mob Programming – Interview            ", "author": ["Romain Vailleux"], "link": "https://blog.octo.com/en/woody-zuill-about-mob-programming-interview/", "abstract": "Woody Zuill about Mob Programming – Interview Publication date 05/04/2018 by Romain Vailleux Tweet Share 0 +1 LinkedIn 0 Interview with Woody Zuill who will be teaching a Masterclass Mob programming with Octo Academy on May 28th. [Original version – traduction en Français ici ] You have practiced Mob Programming and trained development teams in doing Mob Programming for several years. What brought you to this way of working? Woody Zuill – Mob Programming emerged from a team that was focused on learning to use an Agile approach to software development. Among other things, the team wanted to improve their ability to work together well.  Over a six month period we had been studying and practicing Clean Code techniques by using a Coding Dojo style that is sometime called a “Randori”.  In this practice we work on a coding exercise and each team member takes a turn at the keyboard (the driver) while another team member guides them (the navigator).  Every four minutes we would switch out the Driver and Navigator. We follow a simple guideline:  For an idea to go from someone’s head into the computer it must go through someone else’s hands (credit for this guideline goes to Llewellyn Falco). We liked this way of practice, and were becoming good at the different techniques needed to make this work well such as expressing our ideas, listening, and following the guidance of the navigator. One day a team member asked the rest of the team to look at some difficult code and perhaps help figure out how to proceed, and we just naturally started working on the code together using the same techniques we had been learning in our Coding Dojo. During this session we all noticed we were learning a lot, getting a lot done, and having fun doing it, so we decided to continue working this way the next day – and we haven’t stopped since. We have literally worked this way everyday over the last six years. What are the most observable benefits when working as a Mob Programming team? Woody Zuill – There are a lot of good things happening, but the most obvious is that we are able to work through almost any coding task or story very quickly with everyone was working together.  We are rarely blocked from going forward due to unanswered questions or missing skills or knowledge – so we find we get more work done. We also noticed the quality of our work increased dramatically because we have the whole team reviewing the code, and suggesting improvements as we do the work. Additionally Mob Programming provides a rapid learning environment where we learn from each other and all grow our abilities day long. What are the main obstacles you encounter when a team converts to Mob Programming? And how do you overcome them? Woody Zuill – To be able to work on a Mob Programming team it’s important to learn how to work well with others. Most developers have been practicing working alone for their entire career, so it can take a lot of effort to learn the skills needed for close, continuous teamwork .  We need to become good at listening – and this is difficult for some of us. At the same time we need to develop our ability to communicate and share our ideas while being willing to try other team members ideas. Lack of these and related skills can be a problem for a team. Paying attention to how we interact with each other, and working at improving our listening and teamwork skills takes time. We’ve used a number of techniques, and perhaps the most helpful has been to follow the guideline that when we are the Driver (the person at the keyboard) we simply follow the instructions of the Navigator .  Another technique we use is to be willing to try each other’s ideas. We learn from doing, rather than merely discussing our ideas. If you wanted to make all the development teams of a company to work in Mob Programming, what would you do ? Woody Zuill – It is important to me that this way of working be entirely voluntary – so I would avoid dictating that all the teams should work this way.  Mob Programming is not a “silver bullet”, and not everyone is suited to working this way. If we can get good at it then it is another option along with solo and pair programming. For those interested I suggest they gather together regularly and frequently to practice the techniques and skills needed. Practicing first on a few code exercises allows us to learn how to work together without the pressure of work . During these sessions we focus on improving our listening and teamwork skills. After a few practice sessions it might be helpful to work on some simple bug fixes or small stories to get used to the flow of working this way.  After each session it is helpful to review how things went, and look for ways to improve. You want to participate in the masterclass ? Information and registration on the OCTO Academy website Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile , Methodology , Software Craftsmanship , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Woody Zuill about Mob Programming – Interview” uzzal 10/04/2018 à 22:25 i want to interview with him. he is very talented guy Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-04-05"},
{"website": "Octo", "title": "\n                User Stories – The essential in one drawing!            ", "author": ["Jennifer Pelisson"], "link": "https://blog.octo.com/en/user-stories/", "abstract": "User Stories – The essential in one drawing! Publication date 30/03/2018 by Jennifer Pelisson Tweet Share 0 +1 LinkedIn 0 Today, I propose you a summary, in poster format, of the french podcast: « Café Craft – Les User Stories avec Nicolas Fournier » Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile and tagged agile , delivery , development , user story . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-03-30"},
{"website": "Octo", "title": "\n                AWS Summit Sydney 2018            ", "author": ["Nicolas Pascal"], "link": "https://blog.octo.com/en/aws-summit-sydney-2018/", "abstract": "AWS Summit Sydney 2018 Publication date 14/04/2018 by Nicolas Pascal Tweet Share 0 +1 LinkedIn 0 Hi, I’m here to talk about the last day of Sydney AWS Summit Sydney 2018! It was my first attendance ever at an event of this type, I have to say it’s quite a different vibe than your usual meetup! The lights, the loud music, they really know how to stage a show. Who is the big star of the year??? It’s Machine Learning!!! Containers management is coming close behind. AWS is pushing its vision to create a seamless customer experience where the machines adapt themselves to the human and not the reverse. Their plan of action is to: Reduce the complexity for developers Bring ML capabilities closer, in the Edge to provide fast real-time predictions One of the big announcement is AWS SageMaker, which provides you tools and predefined algorithms that allows you to quickly build and deploy Jupyter notebooks. With the transfer learning capability, you can only train the last classification part, if needed. It is only available in Virginia so far. If you want to go further and have more customisation you can use AWS Deep Learn AMIs. Another is the release of a very exciting device Deep Lens which posses built-in ML capabilities powered by a specially designed Intel chip to provide fast real-time predictions. Intel also presented a very cool neural network computing stick called Movidius, yes you can have these capabilities in a device that fits in your hand. The day where you needed to reserve NASA-like computing power is way gone … The live demos, showcasing real-time detection of human movement but also of objects were quite impressive. Everyone went wild when they threw inflatable hot dogs and unicorns into the audience, yes you heard me right, for the demonstration. The speed of the categorisation considering the number of “objects” to consider in the audience (persons, smartphones, chairs …) was impressive. When zooming with the camera, the experience was really seamless. They had some accuracy problems though, it kept detecting a tie for the presenter that just had a slightly open shirt. To finish with the ML part, you may want to dive in: AWS greenglass ML that provides ML in the Edge Darknet Open source Neural network C library that enables real-time prediction optimization called YOLO: You only look once! Amazon Mechanical Turk to speed the tedious work of labeling your data Still following the principle of creating a better user experience, AWS launches Sumerian (in public preview): to easily create AR/VR/3D experiences in your browser. It works with ARKit and ARCore and is powered by AR.js. Another star of the conference is containers! After being surprisingly absent for a long time from the K8S battlefield, AWS is making a comeback with EKS and now Fargate. With this new service release, you can now leverage the serverless approach to manage your cluster of containers using ECS or EKS. You can find the code used in the live demo to deploy microservices using Fargate here: https://github.com/aws-samples/aws-microservices-deploy-options On the front end space, you might have heard of GraphQL , a language that allows you to query only the data that you need, created by Facebook. AWS AppSync is a GraphQL engine using Apollo under the hood (the teams collaborate closely together). It provides built-in integration for many AWS datasources. The live demo showcased a Progressive Web App using AppSync ( https://github.com/aws-samples/aws-mobile-appsync-chat-starter-angular ) Conclusion: AWS continues to be a key cloud player by reinforcing its presence both in the Machine Learning and the Containerization space. I really liked the focus of making things simpler like abstracting complex ML notions or providing more serverless services. I agree that ML can provide a greater experience for customers and help provide taking better decisions or provide more value out of the huge amount data that we have nowadays. However, it’s not always totally accurate, I wouldn’t create a service that is accessible only through voice/image recognition or take critical decisions solely based on algorithm decisions, even if they have a good accuracy. The AWS summit is a great experience to get up to date with the latest AWS features and IT trends. However it is at times a bit too superficial and commercial for my taste, and focus more on the how of things than the why. For example, it’d have been great to dive a bit more into why some part of the Amazon warehouse has still human presence than showing a video of robots moving stacks. Tweet Share 0 +1 LinkedIn 0 This entry was posted in News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-04-14"},
{"website": "Octo", "title": "\n                Google Home understands your pet!            ", "author": ["Christophe Durand", "Vincent Guigui"], "link": "https://blog.octo.com/en/google-home-understands-your-pet/", "abstract": "Google Home understands your pet! Publication date 01/04/2018 by Christophe Durand , Vincent Guigui Tweet Share 0 +1 LinkedIn 0 What’s up pussy cat ? Developing a conversational agent for Google Home is complex. But the hard part is to test it. Especially if you work in an open space full of facetious colleagues. OCTO’s BYOP (Bring Your Own Pet) policy does not help: sounds from pets are also detected by Google Home devices. To our astonishment we’ve found that this detection is very sophisticated: Google Home seems to have acquired abilities to understand the language of animals! Dog or Chat Bot? For example here is  the Json returned by a Google Home device when Oriane’s cat mewed during a test sequence: Intrigued, we tried with Fonzy, the dog of our CEO. This time, the Json contained the following sequence: WTF?! We’ve activated our contacts at Google to find out. They confirmed the information: since April 2017, Google Speech uses Youtube as the main source of learning. Given the massive number of videos of cats and dogs hosted on Youtube, Google’s Machine Learning algorithms have automatically learned to classify animal sounds . Google is collaborating with the startup No more woof and plans to extend the capabilities of the Google Translate service to promote inter-species communication and allow your dogs and cats to order food in your absence. Access to this service is still currently secure: the voice command “OK Google” remains essential before Rex can place its order and eat its own dog food without you. Reverse-Engineering Rex Investigations with a Meuh box and a duck from OCTO Google refused to further comment and disclose the list of animals supported.  We had to proceed by trial & error in order to find out which animal sounds work and what type of information is detected and returned. Investigations are still under way using an original semi-automated functional testing system developed in collaboration with our partner CloudPets . Here are the first results: Attribute Description Example of collected data rawtext Sound phonetic transcription “wooaaf ooaf”, “miaew”, “mooo”, “onk” locale Species and local dialect “dog-fr”, “cat-fr”, “cow-fr”, “camel-fr” textValue Value transcribed in the default language of the Google Home device “Ouaf ouaf”, “Miaou”, “Meuh”, “Onk” size (Animal?) Size “small”, “medium”, “large”, “huge” mood Emotional state (?) “cool”, “passive”, “agressive” Splash! According to our observations, it doesn’t work with all animals yet. For example, understanding the intentions of the OCTO duck is limited. The Mooh box works better. The goldfish from our office’s reception was not recognized when we stuck a Google Home Mini to the bowl side. We thought it was due to the thickness of the glass and plunged the device into the water. For some unknown reason, it shorted before the first bubbles were recognized. Stimulated by this new challenge, our teams will continue the experiments and keep you informed of the evolution of voice recognition for fish. To be continued… Tweet Share 0 +1 LinkedIn 0 This entry was posted in Digitalisation , IoT and tagged Bot , Google , Voice assistant . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-04-01"},
{"website": "Octo", "title": "\n                Retrospectives – The essential in one drawing!            ", "author": ["Jennifer Pelisson"], "link": "https://blog.octo.com/en/poster-retrospectives/", "abstract": "Retrospectives – The essential in one drawing! Publication date 21/03/2018 by Jennifer Pelisson Tweet Share 0 +1 LinkedIn 0 Today, I propose you a summary, in poster format, of the french podcast: « Café Craft – Back to basics: les rétrospectives avec Élise Derouault » Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile and tagged agile , continuous improvement , Rétrospective , ritual . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-03-21"},
{"website": "Octo", "title": "\n                The Wizard: Side effects            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/the-wizard-side-effects/", "abstract": "The Wizard: Side effects Publication date 21/02/2018 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 See the previous episode here . He has questions. Nevertheless, he has not yet arrived. And you know you have answers. You interrupt your summoning, and sit down. You can continue in another moment. Everything happens for a reason. You just hope the reason is good. N appears right next to you. Hey, what’s up? I’m setting up the auto-scaling configuration of the new project. Nothing huge. Cool. I need some help. He has progressed immensely since the last time you saw him. People are starting to ask him for help now. You can’t help but feel pride when you see what he has become. Yeah sure, show me. Remember that nginx role you helped me create ? What about it? It’s flawed. Interesting. He did realize after all. Is it, now? Yes. I usually run my Ansible playbooks every night, in order to ensure that all my infrastructure and services are at a certain state. At least the ones in charge of server and middleware configuration. That’s good stuff, you think. That’s good stuff. It is. It works as long as my playbooks are idempotent. Are your playbooks idempotent? Yes, except for the ones I use for application deployment. What’s your issue then? My nginx role. The service stopped unexpectedly, and my role wouldn’t restart it. He realized the hard way. That is probably a good thing. It is the best way to learn. Okay. Let’s take a look. You summon the enclosure you both worked on on that occasion. You need it in order to properly analyze the spell. You check the structure of it: # tasks/main.yml --- - name: Install epel-release yum: name: epel-release state: present - name: Install nginx yum: name: nginx state: present notify: Start nginx It all comes back to you. You remember noticing the little issue with this proposal . It was a time bomb. But then again, he found it. It’s precisely what you wanted. Well, you’re only starting your service when installing nginx. Yes. So if your service is stopped, and you launch your playbook again, nothing will change when nginx is already installed. Yes. So this whole testing thing… I don’t think it’s a good idea. I refactored my code, the tests passed, and the service is not running in production because of a side effect. Patience is key. Did you test that side effect? Well… no. How could I? It’s a side effect. Write the code that would cause your side effect, and then see if your actual code is able to handle that scenario. He considers the idea for half a second. So I’m supposed to put code which makes my components fail inside of my role? That doesn’t make any sense, does it? You don’t put it inside your role. You put it alongside your code. Watch. You reflect for a second, structuring your thoughts, trying to imagine exactly what you need. You analyze the dimensions of the enclosure. Depth, width and height. It won’t suffice for what you have in mind. Once you figure it out, you close your eyes, and expand the enclosure in order to create a new dimension, invisible to the naked eye. Inside this new pocket within time and space, you place the side effect of the spell, just as he described it: # molecule/default/side_effect.yml --- - name: Stop nginx hosts: all tasks: - name: Stop nginx service: name: nginx state: stopped This will probably suffice, you say to yourself. Would you say this code is capable of recreating the side effect you described earlier? Yes, I would. Great, let’s use it now. You concentrate as you modify the structure and logic of the enclosure in order to use the new dimension: # molecule/default/molecule.yml scenario: name: default test_sequence: - destroy - syntax - lint - create - converge - side_effect - converge - idempotence - verify So we’re converging twice? Yes. You need to test whether your code is able to correct the state of your component after a side effect. Yes, indeed. Now, launch your test. N claps his hands, and as he separates them, a stream of bright red light appears between them: =================================== FAILURES =================================== __________________ test_nginx_is_running[ansible://instance] ___________________ host = <testinfra.host.Host object at 0x108390290> def test_nginx_is_running(host): # Given nginx = host.service('nginx') # Then >       assert nginx.is_running E       assert False E        +  where False = <service nginx>.is_running tests/test_default.py:20: AssertionError He looks stunned for a while. So we weren’t testing the whole way, were we? No, we weren’t. I mean, we were not testing for all of the edge cases we could have had. You can’t possibly test for all of them. There’s way too many. This was an interesting one though. I see. Well, you’re in red again. Go to green. N modifies the spell structure. # tasks/main.yml --- # tasks file for nginx-tdd - name: Install epel-release yum: name: epel-release state: present - name: Install nginx yum: name: nginx state: present - name: Start nginx service: name: nginx state: started enabled: true It looks good. And now let me test… N does exactly the same thing he did before. Only this time, bright green light emanates from his palms: ============================= test session starts ============================== platform darwin -- Python 2.7.11, pytest-3.3.2, py-1.5.2, pluggy-0.6.0 rootdir: /Users/sebiwi/stuff/ocac/skool-ops/craftmanship-ops/ansible-tdd/ansible-nginx-tdd/molecule/default, inifile: plugins: testinfra-1.7.1 collected 3 items tests/test_default.py ...                                                [100%] =========================== 3 passed in 7.78 seconds =========================== Verifier completed successfully. N looks at you. That was simple. Was it? I mean, I could have made my tests better from the beginning. One bug in production is just a thing you didn’t test. It seems like overkill for a simple nginx installation though. He is right. But he needs to see the bigger picture. It probably is. But now you know that you’re really testing interesting scenarios, the ones you really want. Besides, think about it. The possibilities are endless. What do you mean? You only have a single node component here, but you can use the same principle in order to test incredibly complex things. Such as? High availability on distributed components, leader election, node failover… you name it. You can simulate a disaster on a side effect and see how your code will react to it. He reflects. He’s starting to get it. Can I help you with anything else? Your attention already shifted back to the battlefield. What now? No, I think that will do. I’ll probably go add this to my other playbooks. You’re right, the possibilities are… You’re no longer there again. N looks around in confusion. How does he do that ? Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-02-21"},
{"website": "Octo", "title": "\n                The semicircle (episode 11 — Boxes and Arrows)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-11-boxes-and-arrows/", "abstract": "The semicircle (episode 11 — Boxes and Arrows) Publication date 09/02/2018 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 You’re running late, and not just a little late. You throw your bag under the table and boot up your PC. “Hi! Sorry I’m so late! You say hello to Jeremy, Audrey, and Farid. Audrey says:  “Maria came by. She was looking for you.” “Did she say what for?” Hmmm – ‘I’m waiting for an action plan. I don’t see anything coming. I’m worried’ “OK” Not OK. This action plan that Maria is waiting for has been spinning in your head for over a week now.  You’ve collected some random ideas picked up from here and there – as if throwing them haphazardly into a soup. They’ll soften but won’t produce anything edible. The PC lights up warning you of the impending updates. Sheesh, go ahead, do it. You get up and head towards the kitchen. You pass by Audrey’s office saying, “It’s not my day.” “It seems like its not your week. You in trouble?” “Routine stuff.” The kitchen area is oddly deserted, quite inexplicable at this time of day. Well, explicable after all: the coffee display shows in small red characters: “WAIT MAINTENANCE O_o”. Maybe you can make some tea. It might wake you up just as well as coffee. You start boiling some water. You catch up with news on your social network. Scroll. Scroll. Scroll. You get back to your workstation holding the paper cup which is both too hot and too small. The PC has just completed its upgrade, and now it is restarting. You feel like crying. Why don’t we tell a good joke instead? “Jeremy, do you know how we know that there is software in a can opener?” “The can opener downloads an update? You shared that one with us last week.” Maria walks into the office abruptly. “Hello again. Anybody got any instant coffee?” You anticipate her next request “I’ll see you in 10 minutes, Maria.” “That won’t work. Come see me at 2:00.” You open the bug tracker. number: 4243        \t\tdate: 08/31/2017 status: en cours    \t\ttype: bug severity: critical    \t\tsubmitter: C. COURDEL summary: budgeted partial report has incorrect result: I submit the budgeted year, check partial report, check no bypass option, submit it, wrong amount, I expect 34320 I get 53896. See attached document. Assigned to: _ Resolution: _ number: 4244        \t \tdate: 08/31/2017 status: in progress    \t\ttype: bug severity : critical     \tsubmitter: C. COURDEL summary : we’re almost there with the budgeted partial report description: I submit the budgeted year, check partial report, check no bypass option, submit it, wrong amount, I expect 34320 I get 34000. Same dataset as 4243. Assigned to: _ Resolution: _ “Couldn’t see the forest because of the trees.” “Huh?” says Jeremy. “Bug 4240.” “Could you give me a little more context?” “Bug 4240 was masking bugs 4243 and 4244.” “I don’t follow you.” “4240 is a Null Pointer Exception. We fixed it, put it into acceptance : that allowed Claudia to find 4243 and 4244. In fact, I think that what’s happening in 4243 is having an impact on the outcome that is described in 4244.” “The first bug must have corrupted the database. That’s why I hate databases. Databases are a huge collection of global variables.” “Tough luck. This is where our customers keep their valuable data.” “You don’t say.” You open the IDE and dive into the code again, coming across bug 4243, the one that probably masks bug 4244. Remember, this is not your day. It’s always the same mess, but it’s not like you can find your way way back. You would need detailed notes. Like some sketches or photographs at a crime scene, taking care not to touch anything. “Where are you?” You hear your teammate’s voice; he’s here in the building, for sure. There, at the back of the room (maybe) there is a door, which leads to another room. He’s over there. You’re screaming: “Over here. I’m stuck. I’m stuck. I can’t move any further.” Sitting on dusty ground. Stuck. You don’t know how you got here anymore. Nor what you were looking for. Oh wait, yes… of course: the light. “No better off” The sound of your voices is faint, diminished by the numerous objects that (probably) clutter the space. Two voices in the dark. Two control officers visiting the housing of a tenant with a hoarding syndrome. All this accumulated disorder. You think: If the only mistakes we have the courage to correct are syntax errors, this will be the result.  Welcome to pervasive disorder. Just put that down there. There’s still a bit of space left. “How long do you think it’ll take to organize all this?” Three seconds of muffled silence. “The question is more likely: How quickly can we get the hell out of here?” Silence. Creaking. Inanimate disorder. Paralyzed inspectors, status: disabled. Blip. Window at the bottom right: Maria: I prefer 2:30, after my lunch. A ray of light penetrated the room for one or two seconds, piercing the thick stream of dust suspended there. What do we do with the Action Plan? Two thirty leaves you very little time to revisit and rework this plan.  On the other hand, it leaves you all the rest of the morning plus the lunch break to find some excuse to ask Maria for a little more time. Okay. ToF: Oleg, you want to help me? Between noon and 2. I’ll come over and bring some sandwiches! Oleg: Sure. I will. Don’t bother with the sandwiches, we have food here. 12h30. At about 1 p.m., you find Oleg sitting and working at one of the tables in the multipurpose space that also serves as a cafeteria. Oleg and you study the famous action plan. It fits on an index card: Correction of current tickets: \t\t80 days Code review on fixes: \t\t\t10 days Code improvement (technical debt):  30 days Develop version 4.5:\t\t\t\t120 days While sipping your very first coffee of the day, you explain: “We cut the effort to develop new stories by half, i. e. the scope of version 4.5, and we use the extra time to fix all bugs, and to clean up the code as much as possible. We keep the current budget: four people for three months.” “How are you going to convince your management that this is the right thing to do?” “That’s where I am stalling, Oleg.” “What happens if you keep up with your current momentum, following your usual process?” “The first priority is to fix all the defects, and the rest of the time would be spent on stories. But doing that puts us at risk of having an increasing number of defects, since nothing is being done to reduce the contributing “defect factors.” “What are you calling ‘defect factors’? That isn’t in your plan.” “It is the level of complexity and technical debt in the code.” Oleg is looking at you in complete silence. You wonder whether he’s waiting for more explanation from you, thinking about what you just said, or something else outside his control has rattled him. You choose the first hypothesis and add: “Do you agree with me that the dirtier the code, the more likely it is that we’ll insert new defects?” “Maybe.” “Software rots, as they say…” “Software is indestructible. As long as medium is still usable, the software remains absolutely intact, as it was produced by its authors.” “So how do you explain that software becomes more and more expensive to maintain?” “Its ‘value in use’ decreases. To maintain or increase it, you have to modify the software. If modifying the software is easy, then maintenance costs remain low. “Agreed.” “How many stories are planned in your new version?” “39” “How many stories, in the previous version?” “52” “And in the previous version?” “60. That’s what we were just saying: every new quarter, we produce fewer stories; as the cost of maintenance increases. “What contributes to the maintenance cost?” You’re looking at Oleg. This conversation seems to be going in circles. You offer: “Shall we get a second cup of coffee?” “Of course!” Oleg’s beating around the bush because he can’t really help you. Oleg’s giving you a course on software development economics. Oleg acts as devil’s advocate to test your plan and help you improve it. Option 3. While ordering a double espresso macchiato, you list the reasons: “the complexity of the domain, the technical complexity of the solution, the turnover, and of course the conditions under which the program is modified” “For example?” “For example, if I change three lines of code at random, making sure that it compiles, and if I deliver that, I’m pretty sure it’ll create a bug, and therefore make maintenance costs go up in the end.” “So the maintenance process itself is a cost factor.” “You got it.” “What has changed?” “What?” “You produce fewer and fewer stories each quarter, it seems. What varies among the four cost factors?” “I guess the complexity of the domain has increased a little bit. Technical complexity has also increased due to the technical debt.” “There is no such thing as technical debt.” You hold bite your tongue. Oleg’s a pain in the ass, actually. Oleg likes to provoke. Oleg is thinking for himself. Oleg continues: “Everyone talks about technical debt, but debt is when you owe something to someone else, usually a bank. Developers owe nobody anything. On the contrary, they are paid to do what they do.” “It’s a metaphor. They are borrowing time from themselves. As in saving time now that they’ll have to pay back later.” “Not if they go to another project, or if they get fired.” “They are decreasing the maintainability of the code. It’s like they’re depreciating the value of a company’s assets, so to speak.” “In the company where we are right now, if someone found out that you were damaging some equipment, they would stop you right away. So, your supervisor lets you wreak havoc with the maintainability of the code?” “I would say she even encourages me.” “So maintainability of the code isn’t considered an asset.” “Not for her, not for the management, anyway.” “So not for you.” “Are you suggesting that we should let the maintainability of the code decline?” “Not suggesting a thing.  Just pretty sure you won’t be able to convince your management that they have assets to maintain that they didn’t know about until now, and that it’s because of a deterioration in these assets that they’re going to be late, or that the quality will deteriorate.” Back to square one. You are all sitting at the big table again. The room’s almost empty. Oleg smiles. He’s asks: “What do you observe about defects?” “For one, in each new release, there are more defects than in the previous one.” “So you produce stories, and defects.” “Sort of.” Oleg gets up, goes to rummage around one of the shelves at the back of the room and comes back with an A3 sheet and markers. “So you spend time coding. This produces: 1) functionality 2) defects, which cause 3) incidents. The more you develop, the more defects there are.” ￼ “When you have incidents, you look for the defects and correct them. So you spend time correcting, which reduces the number of defects.” ￼ “The time spent coding and the time spent correcting cannot increase simultaneously: you have to choose. That’s a good thing, because when you make fixes, you reduce the number of defects directly and indirectly because you code less.” “Not necessarily: there are cases where the correction of a defect creates an additional defect.” “Maybe, but we should try to keep it simple…” “But not simplistic…” “This is the danger with models, but let’s continue. If you don’t rectify defects as quickly as you produce them, the time spent coding stories will decrease, to the benefit of the time spent correcting defects. How can this situation be remedied?” “You can test the code, which in your schema means reducing the number of defects before they generate incidents.” “Alright. Note that testing takes time, and therefore reduces the number of stories you can produce. Again, you have to choose between the different activities. ￼ “What else?” “We could just as well prevent defects, as you say, by reviewing the code systematically. Or by setting up mob programming.” “OK.” ￼ “Yes, ummm no. I retract what I just said. We tried the reviews, it takes way too much time, without any results.  You know how our tentative attempt at mob programming went, right?” “OK. Let’s just say that as of right now tests and reviews are not very effective” ￼ So your ability to reduce defects is affected by the effectiveness of tests and reviews. If these activities were effective, they would help reduce the number of defects as quickly as you produce them. But they are not effective, so if you applied them, they would take too long.” “There you go.” “But by improving tests and reviews – for example, through training – you could become more effective.” “Most likely.” ￼ “So you have these metrics: number of stories done, number of defects, number of incidents.” “Yes.” “And you must divide your time between: 1) Coding new functionality 2) Correcting defects found 3) Testing for defects 4) Conducting reviews to prevent defects 5) Training to improve your tests 6) Training to improve your code reviews Good luck!” You run to catch up with the bus, the A3 sheet folded in your pocket. But that’s not a plan, that’s a model. And with lots of scribbling for that matter! You’ll just have to go find out what Maria thinks. (to be continued) Previous episodes : 1 — If the code could speak 2 — See/Advance 3 — Communication Breakdown 4 — Driver/Navigator 5 — Brown Bag Lunch 6 — Takeaway Tips 7 — Crisis/Opportunity 8 — The Fifth Floor 9 — What is to be done 10 — Either … Or … Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-02-09"},
{"website": "Octo", "title": "\n                The semicircle (episode 10 — Either … or …)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-10-either-or/", "abstract": "The semicircle (episode 10 — Either … or …) Publication date 02/02/2018 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 The Eye-Brain Law: To a certain extent, mental power can compensate for observational weakness. The Brain-Eye Law: To a certain extent, observational power can compensate for mental weakness. Jerry Weinberg “I received your email. I get what you want to do. But, what’s the goal?” Comfortably secured in a state-of-the-art ergonomic seat, and protected by a natural fortification of edible, combustible, destructible, fungible, flexible, securely-fixed, decay-prone, sensitive and transmissible elements, Mazare observes both you and Jeremy. Jeremy answers, “We would like to analyze the lead time for bug resolution using queries that the bug tracker interface doesn’t offer out of the box.” “Is it a S.R.?” “An S.R.?” “Special request. There’s a form to fill out. This gets you access to the services in those situations where you want to look at data that doesn’t belong to you.” Jeremy raises his hand saying: “Stop right there, in this case, it’s our data. “In the bug tracker tables? I don’t think so, no.” “This is our application. These are our bugs!” Mazare hesitates. You return your attention to the inventory-taking of the visible part of his office, which his first question had interrupted. Mazare scribbles a password on a repositionable sticky note and while handing it to Jeremy, says: “Point taken. Go take a seat at back of the room.” You go sit next to Jeremy. “It was easy.” “Mazare is a friend. I’m doing him a favor on monitoring.” In less time than it would take to even put a dent on the subject of monitoring, Jeremy has queried two tables, retrieved the answers in a spreadsheet format, placed the files on a shared drive, disconnected the session, ripped up the password and put the chairs back in place. “We have what we need.” Thirty minutes and two joins later, Jeremy’s spreadsheet displays the information you were looking for. id   | issue                  | date_1     | date_2     | d1-d2 0313 | Data transfer broken...| 2015/11/11 | 2016/01/04 | 54 0321 | Pb Dialogue box \"cho...| 2015/11/13 | 2015/11/16 |  3 0476 | Calc err res.S1 v S2...| 2015/11/16 | 2015/11/20 |  4 0477 | S2 result err. w/o S1  | 2015/11/16 | 2015/11/20 |  4 0480 | \"unknown vers\" in mo...| 2015/11/18 | 2016/02/03 | 77 ... ... 1397 | incorrect msg after ...| 2017/04/06 | 2017/05/02 | 26 You ask, “If you select all the version 4.3 tickets, that’s to say the version prior to the one we have in production right now…” “So all tickets from November 2015 to May 2016.” “Yes. What’s the average time between entering a bug and resolving it?” Jeremy adds a formula and the spreadsheet displays: 9.390625 Jeremy adds another formula and says, “With 65 tickets in 181 days, we consistently have between 3 and 4 tickets being worked at a time. What are you getting at?” You don’t know. You’ve never been very comfortable interpreting numbers. You ask: “What about version 4.4?” “So from May 2016 to October 2016… ” 8.753433 “What about the current version?” “You mean the version under development?” “Yes. It’s been four and a half months, something like that.” “123 days, with 51 tickets. Not all tickets are resolved, however.” “What is the average resolution time?” “10.235475, that’s between 4 and 5 tickets in process at a time.” You’re thinking out loud. “On the version under development, not only will there probably be more tickets in total, but the resolution time will be longer. So….” “So…?” “So the team will end up with more tasks than time to complete these tasks. A bottleneck.” “You’re making an erroneous assumption: the lead time for resolution is the time elapsed, not the time worked.” “What difference does it make? There are approximately 210 working days per year. Multiply all figures by a factor of 210/365. What difference does it make?” “That’s not what I mean. You don’t spend 10 days on each ticket. The ticket stays in the pipe, but that doesn’t mean we’re working on it all the time.” “I repeat, what difference does that make?” This conversation is creating something of a déjà-vu impression, an aftertaste, a mixture of uncertainty and feverish naïve questioning. The banter of suckers who don’t have enough data but who have already bet too much at this table, and who nevertheless try hard to guess what is actually going on. “Hello…?” “Sorry. Yes. What difference does it make? These ten days are the usual rhythm at which we do things. Whether the ticket waits 2 days out of 10 or 8 out of 10, it comes down to the same cause: it’s because of everything we do in addition to the ticket resolution: development, meetings, etc.” “For all we know, bug fixes won’t take as long this time.” “And why would bugs take less time to fix this time?” Audrey and Farid, who had come for the 10 o’clock break, are standing behind you looking at the same numbers. Audrey intervenes. “If we each put our heart into it, there has to be a way!” “What exactly do you mean: put one’s heart into it?” “Make an effort!” “I’m going to assume you mean by that, do more, i. e. waste less time. Where do you think we could save time?” “I don’t know!” “Because that’s what this bug tracker request hasn’t been able to tell us, thus far.” Jeremy insists. “I insist: we are not certain that it will take us as much time in this version as in the previous one.” “Oh, really? What has improved in the way we work since the prior version?” “Well, we already know a little bit more about the system and the business.” “But we have a little more code, and things got a little more complicated in the data model.” Farid intervenes calmly. “What we need to know is where the delays come from.” Audrey says: “Okay, but how do we find out?” No one knows. Instead of making random assertions, you remain quiet. I’ve made enough bets at this table. Jeremy closes the spreadsheet, launches his IDE and knowingly concludes: “Either we take on fewer stories, or we take on fewer bugs.” Audrey’s heads for the coffee shop. “Wow, what great ideas you have.” Farid brings his chair closer to Jeremy and asks him, “By the way, do you have a few minutes for a code review? I fixed the 4237, but I would like to make sure it holds up.” “OK. Remind me which module is it in?” They’re inspecting the code. You are watching them distractedly. You think: you just need to know how to deal with the code in this project. To know which doors to open and which doors not to open, which obstacles to get out of your way, and what not to touch, under any pretext. And that’s how we would survive in this code base: by staying just on the surface, like skaters on a lake, who would have perfect knowledge of areas where the ice is too thin. Making at most some cosmetic changes here and there, but without putting anything at risk. You refocus on the conversation at hand. Farid explains, “I have changed almost nothing, just this variable that becomes a field, and…” “Stop. There’s a bug here.” “Are you sure?” “You take the value of the field before calling checkCompleteAmount , so you don’t use the method that splits the key in half.” “Huh?” “You are aware that this field is used as a primary key, right?” “Maybe…” “For sure. And so your key value will conflict with an already existing period value.” “What? I don’t know what you mean.” “OK. Let me give you an example: the key value is the day concatenated to the month.” “OK.” “And we have, say, the 1st of November which merges with the 11th of January, because ‘1’ glued to ’11’, produces the same value as ’11’ glued to ‘1.’” “Holy cow!” “Yes, holy cow. With this code, you are adding budgets from different periods.” You interrupt. “How much time did we just save?” Jeremy and Farid look at you confounded. You continue. “We just identified a bug, didn’t we?” “Yes, answered Jeremy, so what?” “Assume that the review you just did didn’t take place, how much time would have elapsed before this bug appeared?” “Appear where?” “That’s my question.” Farid’s tries to answer. “We don’t deploy to the acceptance environment until Friday. By the time Charlene runs the tests again…” Jeremy intervenes, “What tests? I doubt if the current test suite contains any test case that would produce this bug.” “Then it could only be detected by some customer. In November, for example.” You’re counting on your fingers. “After the ticket is first entered: the time to reproduce the bug would be, say, 4 hours.” “At least.” “Understanding what’s going on, changing the code: 1 hour.” “Running the tests…” “Let’s add an hour. Removing the old code and delivery of the new code: 1 hour.” “You forget that you also potentially need to repair the customer’s data. The last time that happened, it took us half a day.” “We’re already at 10 hours. We just saved 10 hours.” Farid approves. “Fair enough. We should be doing code reviews more often.” Jeremy intervenes. “Why do reviews more often? Reviews are a time sink.” “If a review like the one we’ve just done can save us something like 10 hours, I’ll be glad to invest a little time.” “But it isn’t true that each review will reveal the same kind of bug!” “No, but with what we’re going to find on average, we could be winning.” “I don’t see how!” You jump right in: “Can’t you see how an hour of double proofreading just saved us 10 hours of work? If you can detect a defect instead of letting it slip into the production release and having to correct it later…” “The reason I saw this bug is that we had a similar incident two years ago. It’s a coincidence. Doesn’t mean all code reviews are going to be as successful.” You rub your eyes like as if to chase away a building migraine headache. You’re back at the game table. In the middle of the clutter. Players are allowed to make any comments they deem relevant, they can draw any conclusions they want. There is never enough information or else there is too much variation. And the code is rotting. It is becoming less and less clear. But the players keep betting, making guesses, and betting again. They get into debt, and it is always the (complexity) bank that wins in the end. You go back to your seat, affirming, more for yourself than for the rest of the team, “It might be worth it.” (to be continued) Previous episodes : 1 — If the code could speak 2 — See/Advance 3 — Communication Breakdown 4 — Driver/Navigator 5 — Brown Bag Lunch 6 — Takeaway Tips 7 — Crisis/Opportunity 8 — The Fifth Floor 9 — What is to be done Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-02-02"},
{"website": "Octo", "title": "\n                The Twelve-Factors Kubernetes            ", "author": ["Etienne Coutaud", "Eric Favre"], "link": "https://blog.octo.com/en/the-twelve-factors-kubernetes/", "abstract": "The Twelve-Factors Kubernetes Publication date 05/02/2018 by Etienne Coutaud , Eric Favre Tweet Share 0 +1 LinkedIn 0 “Kubernetes is the Linux of the cloud” This quote by Kelsey Hightower during the Kubecon 2017 in Austin emphasize the rise of Kubernetes among modern cloud infrastructures. This rise is partly driven by the developers community, but also by the web giants such as Google, Amazon, Alibaba or Red Hat who have invested a lot on this technology, and keep contributing to its improvement and smoothening its integration to their respective ecosystems. EKS for AWS, GKE for Google and AKS for Azure are good illustrations of that. This article lists the 12 basic rules and good practices to know about when starting using Kubernetes optimally. This list interests anyone, developer or sysadmin, who uses K8s daily. I. 1 Pod = 1 or n containers The naming is important to make sure everyone is on the same page about what’s what. In the Kubernetes world, a Pod is the smallest computing unit deployable on a cluster. It’s made of one or more containers. The containers within a Pod share the same IP, the same storage and are co-located on the same node of the cluster. To go further: https://kubernetes.io/docs/concepts/workloads/pods/pod/ II. Labels everywhere Most Kubernetes resources can be labelled: the Pods , the Nodes , the Namespaces etc. This labelling is done injecting a key-value pair into the metadata. Labelling components allows for 2 things: A technical use, as many inter-dependant resources use labels to identify one another. For instance, I can label part of my Nodes “zone: front” because these nodes are likely to host web applications. I then assign a affinity to my frontend Pods so that they get hosted by the nodes labelled “zone: front”. An organisational use: assign labels allow to easily identify resources and request them efficiently. For instance, to retrieve all nodes in the front zone, i can run: $> kubectl get nodes -l zone=front To go further: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ III. Infrastructure as Code and versioning All Kubernetes resources can be written as a YAML or JSON files. The resource creation from a file is done with the command line: $> kubectl apply -f MYFILE.{yaml,json} The apply command does a smart diff, so it only creates the resource if it wasn’t already there, updates it if the file was changed, and does nothing otherwise. The use of files allows to track, version and reproduce the complete system at any time. It is therefore a commonly adopted practice to to version the K8s resource description files with the same rigor as for the code. To go further: https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#kubectl-apply IV. A Service to expose Pods never communicate directly with one another, they got through a Service , because Pods are volatile and short-lived across the cluster. During some maintenance operations, some Pods may migrate from one node to another. These same Pods may also reboot, scale out, or even be destroyed, when upgrading for instance. In each of these cases, the Pod IP changes, as well as its name. The Service is a Kubernetes resource located in front of the Pods and allowing for some ports of the Pods on the network. Services have a fixed name and a fixed dedicated IP. Thus you can access your Pods whatever their IPs or names. The matching between Services and Pods relies on labels. When the Service matches several Pods , it load-balances the traffic with a round-robin algorithm. To go further: https://kubernetes.io/docs/tutorials/kubernetes-basics/expose-intro/ V. ConfigMap and secret to configure ConfigMaps and Secrets are Kubernetes resources allowing to manage the Pods configuration. The configuration is described as a set of key-values. These configurations are then injected into the Pods as environment variables or as configuration files mounted on the the containers. The use of these resources allows to decouple the Pods description from any configuration. Whether they are written in YAML or JSON, the configurations are versionable (except for Secrets which hold sensitive information). To go further : https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/ VI. Limit and Request to control the resources utilization In the many Pods configuration options, it is possible to define the resources used and usable by the Pod (CPU and memory): requests, this configuration is applicable to CPU or memory. It defines the minimum resources the Pod will need to run. These values are used by the scheduler at the time of the node allocation. It also enables auto-scaling.  The target CPU utilisation is based on the requested CPU and the Pods autoscaler – which is also a Kubernetes resource – will automatically scale up or down the number of Pods to reach it. limits , just like request , this configuration is applicable to CPU and memory. It defines the maximum amount of resources usable by the Pod . Defining these parameters prevents a failing Pod to compromise the whole cluster by consuming all the resources. If the Kubernetes cluster administrator defined resource quotas for the Namespaces , defining requests and limits becomes mandatory, or the Pod won’t be scheduled. When these values aren’t defined, the administrator may also define default values in a K8s resource named LimitRange . To go further: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ VII. Think of the Pods lifecycle It is possible to deploy a Pod by describing its configuration in a  YAML/JSON file and injecting it into K8s with the kubectl client. Be careful, this method does not benefit from the Pods resilience offered by K8s by design. If the Pod crashes, it won’t be automatically replaced. It is recommended to use a Deployment . This K8s object allows to write a Pod along with its configuration but it also hides the resilience complexity. Indeed, the Deployment will generate a ReplicaSet . The only goal of the ReplicaSet is to make sure that the number of running Pods matches the desired number of Pods . It also provides the abilityto scale Pods at will. The Deployment allows to configure the deployment strategies. It is for instance possible to define a rolling-update strategy in case of a new version of a Pod’s container. The following command allows to start Pods (for instance, a Nginx): $> kubectl run nginx image=nginx --replicas=2 This command will generate a Deployment with a Pod running the Nginx container. The same Deployment will also generate the ReplicaSet which will ensure 2 Pods are running at any time. To go further: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ VIII. LivenessProbe to monitor As we saw, the ReplicaSet allows to ensure the number of running Pods matches the number of desired Pods . It restarts any failing Pods . It is also possible to configure the Pods resiliency on the functional level. For that there is the LivenessProbe option. It provides the abilityto automatically restart the Pod if the condition is not verified. Just like LivenessProbe monitors the status of an application, the ReadinessProbe monitors when an application is available after reboot. This is useful for an application that runs tasks before it actually starts (eg. data injection). To go further: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ IX. Latest is not a version K8s is a container orchestrator and the name of the container to deploy is specified in the Pod configuration. The naming of an image is composed as such: <Registry name>/<Container name>:<tag or version> It is a common practice to increment the image version just like you increment the version of a code base, but also to assign the tag “latest” to the last build image. Configuring a Pod to deploy an image with the tag “latest” is not a good practice for several reasons: No control over the deployed version, and some possible side effects related to the new versions of the image components. Latest may be buggy. It is possible to configure the Pods to select an images “pull” strategy. It’s the “ ImagePullPolicy ” option which can have 3 values: IfNotPresent : Pull image only if it isn’t locally available on the node Always : Always pull image Never : Never pull image IfnotPresent is the default option, so if we use an image with the tag “latest”, Kubernetes will fetch the “latest” version image from at the first deployment. Then as it will be locally present on the node for subsequent deployments, it won’t download the  image again from the registry, even if a new “latest” version image was pushed. To go further: https://kubernetes.io/docs/concepts/configuration/overview/#container-images X. Pods are stateless Pods are short lived and volatile, they can be moved to other nodes in case of maintenance, deployments or reboot. They can also – and that’s a big perk of K8s like systems – scale on demand. The inbound flow to Pods is load-balanced by the Service in front of the Pods . That’s why applications hosted on K8s must use a third party provider to store data. For instance an e-commerce website storing session information as files within a container (let’s say a purchase cart) will lose data when the Pod scales or restarts. The solution to address this issue vary depending use cases. For instance, a key-value storage service (redis, memcache) can be considered to store session data. For a file hosting application, an object storage solution such as AWS S3 will be favored. To go further: https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/ XI. Distributed Storage Volumes As we have seen, your applications should be stateless. You may need however to deploy stateful components requiring a storage layer such as a database. Kubernetes provides the abilityto mount volumes within the Pods . It then becomes possible to mount a volume provided by AWS, Azure of Google’s storage services. The storage is then external of your cluster and remains attached to the Pod in case of a redeployment to a different node. It is also possible to mount a volume between the node where the Pod is deployed and the Pod itself. But this solution should not be considered. Indeed, your Pod and its volume attached to the host may be migrated and lose all its data. To go further: https://kubernetes.io/docs/concepts/storage/volumes/ XII. My applications are 12 factors apps The application code that will eventually be deployed to a Kubernetes cluster has to respect a set of rules. The 12 factors apps are a set of advice/good practices created by Heroku . Heroku is a PaaS provider hosting applications as containers, and these principles are a way to best operate code meant to be containerized. The main recommendations are: Code versioning (GIT) Providing a health check URL Stateless application Environment variable based configuration Log output to standard or error output Degraded mode management Graceful start/stop To go further: https://blog.octo.com/applications-node-js-a-12-facteurs-partie-1-une-base-de-code-saine/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations and tagged DevOps , Docker , Kubernetes . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-02-05"},
{"website": "Octo", "title": "\n                Edge Computing : learn to delegate            ", "author": ["Jordan Afonso"], "link": "https://blog.octo.com/en/edge-computing-learn-to-delegate/", "abstract": "Edge Computing : learn to delegate Publication date 30/01/2018 by Jordan Afonso Tweet Share 0 +1 LinkedIn 0 While the interest for the Internet of Things by companies is no longer to be proven , this area continues to give a hard time to experts, as for security and architecture. Indeed, the multiplication of data sources brings a reflection on the architecture of networks. As Satya Nadella (Microsoft CEO) said on stage in 2017: “When I joined Microsoft in 1992, all Internet traffic was 100 gigabytes a day. Today is 17.5 million times that amount … per second! “And we are only at the beginning of this exponential increase. This is how debates emerge on the interest of setting up an architecture where the data will be rather exploited on the Cloud (or more generally in a centralized way) or as close as possible to the end nodes of the network. In a Cloud Centric architecture, the computing capabilities of remote virtual machines are maximized to provide the end user with new services. However, when data sources start to become extremely numerous (IoT devices, smartphones, third-party applications, etc.) the network tends to congestion and the low latency required for IoT applications will no longer be respected. We therefore observe the emergence of Edge computing. According to HPE , Edge Computing is a practice of processing data near the edge of your network, where the data is generated, not in a centralized data warehouse. We can then wonder if the Cloud and its enormous computing power will disappear in favor of lighter servers that will only redistribute data to end users. Let’s take two sufficiently revealing examples to highlight the differences of these architectures: Devices located on all the street lamps of the city of New York (~ 250 000). Imagine that these devices are equipped with a light sensor and a motion sensor. 4K surveillance cameras in a city like London (~ 500,000). Imagine then that we would like to store the video stream only when the camera detects motion. We would also hope that when the camera recognizes the face of a person wanted by the police, an alert is raised to allow to geolocate the person. NB: Yes, it seems that there are twice as many cameras in London than streetlights in New York … The architectures The centralized architecture in the Cloud In this architecture, all the nodes of the network will speak live and at regular intervals with the central node located in the Cloud. In the first example, the street lights will transmit to the Cloud, every second, the ambient brightness and if they noticed a movement. The cloud can then for example extract from the raw data of each street lights, the value of the brightness. Finally, by comparing this data with the thresholds it knows, it can transmit to the relevant streetlights the ignition control. However, the strength of the Cloud lies in the fact that it is quite simple to go further. Let’s say the New York City Hall realizes that turning on streetlights (even with LEDs) when the ambient light is dimming is not enough to save energy. Since the devices on the street lamps have motion sensors, it is sufficient to extract the motion signal from the raw data in order to light the street lamps only when a movement has been detected. This avoids lighting an area where there is no one. This improvement is quite simple to implement because the motion data is already sent to the Cloud which has only to extract and exploit it. However, given the number of devices and events (250,000 / s), it is quite difficult to believe that the ignition control resulting from the processing of each of these raw data is instantaneous. During a short period of time, the raw data of each device must traverse the network, be filtered, stored and analyzed, and then an ignition command must be sent or not. At this scale, there will necessarily be a latency not insignificant. In the second example, the 4K video stream of 500,000 surveillance cameras must be sent continuously to the cloud. We realize that neither the network nor the infrastructure can support this burden. In practice, we use gateways that will serve as buffers and then will in turn transmit the data they have stored. Once the data arrives on the cloud, they can be analyzed finely to determine for each camera if a movement has been detected. If so, it remains to record the flow as well as determine if a person sought by the police has been detected and transmit his position. The Cloud is therefore able to perform a fairly complex processing on large data and if by chance, the London mayor no longer wanted to rely on a movement detected in the image to record the flow but on schedules, this change would be enough simple to implement. In this case, it is even more obvious that the latency will be extremely important compared to the travel time of the data and its processing which will be the result of quite complex algorithms (Computer Vision, Machine Learning …). Distributed architecture (Edge computing) Here, the Cloud receives only the results of the calculation steps performed by the network ends and can in turn perform other calculations or simply serve as a relay to end users. Delegating work to devices on the periphery of the network makes it possible to reduce the size of the data to be sent and thus to decongest the communication channels in order to reduce the latency. This also helps to reduce the load on the server. The job of a device maker is to find the balance between the battery life, the computing power of the device and communications. Today, we are able to equip more and more computing power because these components are less and less energy consumers and prices become affordable. In the first example, the street lights could for example locally store the brightness thresholds from which they would themselves decide to turn on or off. We thus see here a first constraint of the Edge Computing architecture. Indeed, imagine that for one reason or another, we wanted to change the brightness thresholds. The solution is to set up a centralized device management system with the ability to send data to devices. This system must be located in the cloud and could also allow to configure different thresholds for each lamppost if desired. The treatment is no longer done remotely, but locally. Only the configuration and business rules are periodically deployed from the cloud. Let’s go a little further. We can decide to delegate the motion detection to the device located on the streetlamp which could then decide alone to turn it on or off and that will only have to transmit its state (only when it changes) to the centralized management system. This adds an additional security constraint. In the cloud-centric architecture, the device sends raw (potentially encrypted) data on a regular basis, which is then analyzed before commands descend through secure communications. If the device is corrupted, it will simply stop receiving its commands and become unusable. A simple intervention will then put it back into its normal operating state. In the case where the device has calculation capabilities, decides itself when it lights up or not and only sends its state when it changes, if it is corrupted, the attacker can then decide to implement a totally different behavior without this being noticed by the centralized management system. Of course, when the only hardware available is an LED to turn on or off the consequences of an attack are not dramatic but let’s move to our second example. City of London cameras can, in this distributed architecture, detect a movement in their field of vision and start the transmission to the Cloud that can automatically record the flow. They can also make face recognition in the image and thus raise an alert automatically when a face corresponds to that of an individual they have in memory. We realize here the interest of this kind of architecture because the cameras are no longer obliged to stream content 4K continuously but only occasionally. The Cloud is not even obliged to process the image to recognize someone because the cameras take care of it. However, it is here that the notion of security becomes important. If a camera that only transmits its streaming video is corrupted, the attacker will have access “only” to this stream that he can exploit himself. In the case where the camera is more autonomous, its takeover could remove a face from the base, add more, not allow the Cloud to detect its malfunction because it no longer transmits continuously. These constraints are not blocking and quite simple workarounds exist, but we must be aware that adding “smart” nodes in a network can increase its sensitivity. Conclusion Then try to extract recommendations from these observations. If the amount of connected devices increases as expected , the first factor to regulate will be network congestion (and of course security). Edge computing is an extremely effective solution for dealing with this problem. The components that make the calculations are increasingly affordable (ex: ATtiny, 8-bit processor 20MHz <3 €) and less and less energy consumers. It is therefore possible to deport certain calculations to the ends of the network to unclog it by sending only the most relevant data. However, this trend does not eliminate the interest of the Cloud which must now dedicate its computing power and its high availability to device management interfaces and services requiring a very high computing capacity (registration of devices, sending of command, configuration management, predictive maintenance, …). In the case of Machine Learning, the learning phase is done on the cloud from large databases of knowledge but the real-time processing based on model generated remains local. There is also a need to be more rigorous about security in this kind of architecture because there are more flaws in a network that has more and more intelligent nodes. When the raw data is sent to the cloud, it is quite simple to implement new services by filtering this or that data. When delegating processing to the devices, the Cloud receives only the result of this treatment and then you have to keep your hands on the devices to make a change (update, two-way communications, …). Another solution is to use gateways that serve as a connectivity aggregator and translate IoT protocols to more standard protocols or to encrypt or decrypt data. In this case, these gateways may also perform calculation and processing steps to reduce the amount of data to be processed by the IoT objects. These gateways are called the Fog Computing layer. In short : Edge computing helps to unclog networks and thus limit failures. Lowering the load on the central servers also reduces latency. In the event of a network failure or unavailability of the centralized system, the devices may continue to operate autonomously until they return to normal. We are talking here about the resilience of architecture. Data that ultimately reaches the cloud is not very sensitive and encrypted. When we distribute the resources in an information system, we multiply the attack vectors and we increase its vulnerability to attacks. A powerful device management system must be in place to manage their status and update remotely. Embedded firmwares must be able (if necessary) to communicate with each other without the intermediary of a smarter flat pass that would serve to synchronize and archive these exchanges. The ideal architecture would therefore be: More and more autonomous and intelligent devices without any impact on battery consumption A centralized system with a high computing power which is exploited for the implementation of very complex algorithms and for the remote management of the equipment park A sufficiently robust and fast network to provide reliable communications without latency that would impact the end service If you want to make the most of the opportunities offered by IoT, learn to delegate. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , IoT and tagged architecture , Edge Computing , IOT , Network Architecture , security . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-01-30"},
{"website": "Octo", "title": "\n                The semicircle (episode 9 — What is to be done ?)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-9-what-is-to-be-done/", "abstract": "The semicircle (episode 9 — What is to be done ?) Publication date 26/01/2018 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 Since practical computation demands that implicit assumptions be brought out into the open, it is no coincidence that computer programmers are attracted to an approach devoted to studying how people make assumptions. — Gerald Weinberg. number: 4240\tdate: 08/29/2017 status: in progress type: bug severity: serious\tsubmitter: C. COURDEL nature: partial budget carry forward does not work description: I skip the budgeted year, perform a partial carryover, option: without exceeding, I submit the form, nothing happens! Owner :  _ Resolution status: _ You’ve just opened ticket 4240 and you’re reading through the description, mumbling. We’ll have to call Claudia. You wonder if it would be worth trying to reproduce the bug before calling her, just to know a little more about it (and then you’ll feel less stupid when you have to ask her for clarification) . You have no idea what happened. You find yourself again as if in a dream in front of the massive bleak building on this deserted street. Five floors. All windows are shuttered with brick. Rain runs along the cracks in the mortar that was covering this beige brick wall. It’s both depressing and worrying at the same time. You have to go back inside. Login, password. You sigh so loudly that Jeremy hears you. “You okay?” “No, not at all.” “Are you on ticket 4240? For all we know Claudia Courdel might not have the right dataset. That’s why it works for us but not for her.” “Oh my, how brilliant you are…” Three seconds of silence lasting two minutes. “If that’s how you want to take it…” “I dunno, we could actually look before deciding…” “I’m just hypothesizing…” “No. You’re giving your opinion. A hypothesis would be if you said: I would like to demonstrate that the defect occurs in Claudia’s environment but not in ours. I will start by comparing the two relevant tables in both databases. If they are  identical, I would look at the recently imported files, and then…” “You’re mincing words.” Pause. I never touched any of the code in question. And, in any case, it’s not my fault. But right now, I just need to be right. So I’m getting angry with the only person who can help me on this ticket. Jeremy focuses on his screen. He looks tense. You get up, moving closer to Jeremy and you say to him, “I was angry. And I was saying whatever. I’m wrong, I take back what I said, it was bad, sorry.” “No problem.” Silence. You take mental note of things you’ve got to do today. Or at least before Wednesday. Are you really going to start on this ticket? I thought you had to draw up an action plan to put an end to these quality problems, along with an estimate for its implementation. It’s urgent! And there is also the redesign of the budget module, which everyone agreed to work on together, remember? Through the window you can see the usual traffic congestion on the avenue at 8.30 am. “I need a coffee. Time for yours too?” “I’m working on something…” “Can I bring you something?” “No thanks.” On the way to the kitchen area, you think back to the dream that you were having when the alarm went off. You realize that if you write the dreams down just after you wake up, you remember them lots better. It takes place in a bowling alley. It’s a game between two teams and it’s not going well. You’re on one of the teams, the one that loses. You’ve done nothing but gutters since the beginning of the game. You stand in front of the track. A feeling of embarrassment. You’re looking for a face sipping a stale beer. One of the players – you don’t know who he is, his face is blurred – is sick of your mediocrity – to a point where he throws his bowling ball into the scoring monitor screaming ‘Now we’re going to see which team is the best!’ And you wake up. Of course, the kitchen area is a mess. The sink is overflowing with all the coffee cups from yesterday afternoon. You wash some of them. You dry them. At your service. You calculate. Let’s see, 51 tickets, at the rate of… at the rate of how many hours per ticket? I don’t have a clue. Perhaps between 2 and 4 hours per ticket? Not to mention the time to redeploy, not to mention the meetings in between… But no, some tickets don’t even take 10 minutes. You make yourself a coffee, then get up – leaving the space to a particularly noisy threesome that just arrived. You go back to your office. You’re blindly relying on coffee to get some mental clarity. You open your notebook and start a list. Never seen a morning this slow. You’re calculating. You ask Jeremy: “Jeremy, how long does it take to resolve a ticket?” “It depends on the ticket.” Obviously. You find a better question. “To resolve a ticket, how many different things does it depend on?” “Don’t understand.” “What are the factors that affect the time it takes to resolve a ticket?” Farid, who is starting his PC says. “Good question. There are quite a few, but the most important thing is: is the ticket a bug or an enhancement.” Jeremy adds, “Is it a calculation problem, a cosmetic problem, or something in the data…” “Can we reproduce it easily?” “Does the person in charge of the ticket know the relevant part of the code?” “Yes, that should always be the case.” “It should be, but it isn’t necessarily the case.” “If you own the ticket: you should know the code. Otherwise you risk making things worse.” “Ideally, yes.” “Ideally, there would be no tickets.” “It would be easier to estimate that way.” “So who is taking care of it is one factor.” “Not who is taking care of it, but: does the person who is taking care of it know the code.” “The time elapsed since the ticket was logged.” “The quality of the code where the problem is found to be.” You write down all these factors in your notebook. You add, saying loudly, “Whether or not the code has tests in the area exhibiting the problem.” You suddenly lose yourself in muddy thinking. The cup of coffee isn’t working. You continue, “How do we estimate the ticket turn around time here, Jeremy?” “You’ve been here long enough now, you should know that: we use the guess-o-meter.” “Ah.” “Right, well – actually we don’t ask ourselves how long it will take.” “But I’ve got to ask this question.” Farid asks, “Why?” “Maria is asking me how we can be sure that no new quality problems will show up in the next release. Or in subsequent ones.” “And how are you going to do it?” “Right now, I have no idea. But it would be useful if we could brainstorm it together.” Jeremy, without moving his eyes from the screen. “And wham!…. Another meeting.” We’re tied. Audrey is first to enter the meeting room. Ghastly neon. Pastry crumbs all over the table. “Thank you for the empty coffee mugs and crumbs. Geez!” You bring up the topic in a neutral tone. “OK. Maria is asking us to solve the quality problems before the next release, which takes place in exactly three months, and this is her number one priority. She needs to know what solution we are considering and what it will cost.” Jeremy asks, “What does she mean by quality problems? I have an idea, but I would like to know your point of view.” You connect to the PC in the room, and look for the bug tracker icon while explaining. “There are currently 51 tickets pending or in the process of being resolved. This is more than the total number of tickets we had on the prior prod release, and we’re only 60% finished with this new release.” Farid continues, “Again, not everything is a bug. We should triage everything before diving into solutions.” “Of course. In fact I wonder if it’s worth resolving  all of these issues.” “What?” asks Audrey. “I think it’ll take more than three months to fix them, even if we spend 80% of our time on them.” “If you intend to deliver a buggy release and get out of the demo alive, you still have some learning to do around here…” “That’s one way of looking at it. But no one should be held to an impossible goal.” “So what’s your view, would you be in favor of leaving some bugs in the product?” “Audrey, I imagine you noticed that some tickets appeared because of some new defect caused by a ‘fix’?” “Yes. That doesn’t happen a lot. What are you proposing we should do?” The atmosphere is tense. Your internal barometer reads: “Knots in the stomach. Tense, with possibility of rants.” You shouldn’t have had this third coffee. It may not be a coffee issue. You answer the question. “I propose that we find preventive measures in order to improve our process, and that we present them to Maria, so that she can validate our strategy and help us to put it into action.” “Correcting the bugs isn’t preventive?” “Right, it isn’t. You wait for the bugs to appear, you analyze them, and then you correct them. Where is the prevention in that?” “Ok. If we did a little more testing…” Jeremy interrupts Audrey. “No, I have to stop you there. Even for a trivial program, such as a simple calculator – if you want to have complete coverage, i.e. to execute all the possible paths in the code, at least once, by combining all the possible entries, it would take you an almost infinite amount of time.” “Almost infinite, that’s meaningless.” “What I mean is that we can’t test everything.” “So you now mean let’s not test anything.” “Not nothing! We have 26% coverage! But if you want to go have fun testing everything, it’s your choice.” “We have 26%, so 74% of the code is never verified. But we can’t test everything, so let’s not worry about it.” You sense that this isn’t the first time that Audrey and Jeremy are raising their voices with each other. Audrey adds hastily, “As if you were saying: biking will never go as fast as the train. So let’s walk.” “Whatever!” You reread your notes: Bugs – enhancements Tests – almost infinite – 26% – nothing – everything In this imposing building, enduring under the rain, there is a large room on the fourth floor that was probably used to store equipment. An indescribable sense of disorder permeates. This room has one peculiarity. On one of the windows, the plank boards used to block access aren’t well mounted. If you could crawl up there, you might be able to use a crowbar or even a long enough screwdriver to dislodge one of them. You would then notice that it’s actually dark outside too. What’s the point? Farid tries to tame the debate by changing the subject. “Whether we test or not, as soon as we write software, we have bugs, that’s life.” Audrey fires back, “Screw up a demo, or even get fired, that’s life too. We could try to not get fired. If we can accomplish that, I’m in.” “We’re not going to get fired.” “That’s not the point. The point is: are we declaring defeat in the face of the bugs or are we looking for a better way of working?” You finally got to connect to the bug tracker on the PC, and open the request that you had concocted last week. You ask the team to pay attention while you point out the table up on the screen. “Here is a summary of all open and resolved bugs on the previous release to production, by ticket type.” type: - bug\t        | 23 - enhancement\t| 17 - admin\t        | 2 - documentation\t| 0 - n.a\t        | 9 Jeremy comments, “I didn’t know that ‘documentation’ was a type of bug.” You think about your last conversation with Oleg. You say, “Not all incidents occur because of a defect in the code. A defect in the documentation can cause an incident.” “I see.” You continue, “We are missing some crucial information in this bug tracker: the time spent to fix a bug.” Jeremy says: “Easy. It’s the time elapsed between the moment when the bug is assigned to an owner and the moment when its status changes to solved.” “Yes but that data isn’t filled in. There’s a creation date, but there’s no date for when the bug is assigned or for when the ticket is resolved.” “Just read the history table for the ticket modifications.” “How do we do that?” “Right here, right now? It’ll take a little time…” “When exactly could we do that?” “You can’t access the history table in normal mode. You need access rights to those tables. It’s Mazare, from prod, who showed me that.” “When can we go see him?” “Almost anytime. Except Friday.” “Why not Friday?” “On Friday, Mazare is super busy. He’s even having lunch in front of his screen.” “Busy doing what?” “Checking that no one is putting anything in prod.” Audrey gets up. Everybody rises. Audrey mumbles as she leaves. “Geez. This company… some days I…” (to be continued) Previous episodes : 1 — If the code could speak 2 — See/Advance 3 — Communication Breakdown 4 — Driver/Navigator 5 — Brown Bag Lunch 6 — Takeaway Tips 7 — Crisis/Opportunity 8 — The Fifth Floor Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-01-26"},
{"website": "Octo", "title": "\n                How to hack Spark to do some data lineage            ", "author": ["Tristan Le Guillou", "Yacine Benabderrahmane"], "link": "https://blog.octo.com/en/how-to-hack-spark-to-do-some-data-lineage/", "abstract": "How to hack Spark to do some data lineage Publication date 12/02/2018 by Tristan Le Guillou , Yacine Benabderrahmane Tweet Share 0 +1 LinkedIn 0 This article explores some key aspects of data lineage at the data set level, indicating how  to hack the Spark engine to achieve this. Data lineage, or data tracking, is generally defined as a type of data lifecycle that includes data origins and data movement over time. It can also describe transformations applied to the data as it passes through various processes. Data lineage can help analyse how information is used and track key information that serves a particular purpose. Mastering the lifecycle of data monitoring has become a key issue for many IT systems using Big Data technologies and other distributed architectures. If in some cases this function can be facilitated by the presence of native tools on the platform and dedicated to data lineage (like Hadoop environments), however, the possibilities of the latter do not always meet all functional needs and fine granularity of lineage and functionally acceptable calculation times. To take the example of two Open Source stacks that can perform data lineage in the Hadoop constellation, Apache Falcon and Apache Atlas , we realise the limits when it comes to being flexible in front of the diversity of functional needs and efficiency on perimeters with drastically different sizes. Thus, Atlas, which is maturing (v0.8.1), performs global data lineage by scanning internal metadata of the stacks ( HDFS , Hive Metastore, HBase , Kafka ,…). The granularity of the lineage is high, at the level of the data path, but without integrating all the information of the data lifecycle, such as the version. A dataset can keep indefinitely the same URI but its version can evolve over time, which is not managed by Atlas. It is mainly used to perform static lineage, which answers questions such as :”From which table is such other table an output ?”. Falcon, also maturing (v0.10), serves mainly as a meta-orchestrator, which can generate data pipeline lineage information. The grain is natively limited to the data pipeline. If the need is to precisely trace the life cycle of the data, e.g to the dataset’s version, then answering the question: “Which data set and version another dataset is issued from?” becomes a very difficult architectural use case. This is referred to as dynamic data lineage , where tracking follows the lifecycle of the data as processing pipelines evolve in terms of functions and perimeter. This article aims to present an effective way of answering this last question. This solution is based on the use of Apache Spark as a processing engine, in particular by exploiting the Logical Execution Plan tree. However, it remains open by design to be used by other treatment processes. A little bit of theory The main theoretical challenge of data lineage is to answer the question “which data has been used by which process to produce other data”. Although the tracked object is the data itself, it is its metadata that will be used to establish this lineage. The concept usually adopted to solve this type of problem is based on the formulation of relationships between entities in the form of triples semantics . This concept is one of the pillars of the RDF standard (Resource Description Framework), a family of metadata model specifications used in particular to describe the links between resources of Semantic Web . It can also be found in many models such as the models which describe graphs. A triple semantics is a set of three entities that transcribe the relationship between them. It takes the form: subject –> predicate –> object . Applied to data lineage, it can be presented in two modes: The forward lineage , indicated for propagation analyses, uses a predicate that links data to those produced by a process using it. The backward lineage , more appropriate for tracking, uses a predicate that links data to those used to produce it through a process. If we transpose these principles to the data processing in order to carry out a lineage with the granularity of the data set and its version, the triple semantics can be formulated as follows:”the data set B version vB was produced by the process X version vX using the data set A version vA”. This is a backward lineage , the subject being the data set produced (B in vB version), the predicate (or the verb) being “was produced by the X process version vX using” and the object Each product data set is connected to the source data sets via a process (and vice versa). This approach results in the construction of a cyclical graph oriented or DAG (that is, not containing a closed circuit), allowing to trace from each data set / version to all the data sets used for its generation. It is interesting to note that in this case (especially at this granularity level), only the I/O of the processes are taken into account, it is not necessary to exploit the data manipulated internally. As seen above, this level of granularity (data set / version) also requires data dynamic lineage so that the implemented mechanism can follow the evolution of the versions without changing model or code. One of the major difficulties that can be encountered is the management of the version of data set. A multitude of cases can exist and lead to a different strategy, depending on the use case and the architecture adopted. As an illustration of possible strategies, let’s take an example where the version of each data set corresponds functionally to the data collected/processed during a given day and is materialized by a simple increment. It is then possible to adopt two different data addressing strategies, corresponding to two architectural choices: The source data sets of different versions are addressed by version : this is the case in particular when the version of the data set to be used by a process is known at launch in a direct or indirect way (provided by a Scheduler operating precise rules of deduction of the version) The source data sets of different versions are addressed indiscriminately : this case is more complicated to manage. Without a strategy of transmitting version information to the generated data sets, this ends up in a situation where the generated data sets are going to come from all versions of source data sets (see figure below). A solution based on the following principles can be adopted: The version of the data sets generated by a process is, during its execution, calculated according to the versions of the of the source datasets. It is then identical, by version of of the source datasets, for all generated datasets In the source group of a process, a single data set, which will be called “ reference data set ” for the process, must be chosen to port the “reference”version . It is the latter that is propagated as a single version of the generated data sets. In Case I, the source data sets A and B are addressed without reference to the version. The generated data set C is then dependent on all versions of the data sets A and B. We return to a static lineage situation. In Case II, the source data sets A and B are addressed by version, the vC version of the generated data set C is then calculated according to vA and vB. In our case, we designate a reference data set, e. g. A, and a simple calculation function: vC = vA. Treatment of a typical case The following proposal is the result of work carried out in a client’s IS, within the framework of a PoC, to provide a solution presenting: Minimum footprint for existing and future applications (development effort, maintainability, refactoring, overhead processing,…) Remaining compatible with an existing kernel running Spark, without taking risks by integrating exogenous stacks. Highly available, resilient, scalable, evolutive, reactive, decoupled, and CQRS compliant architecture of the overall architecture Functionally, the customer’s main motivation was to respond to a legal constraint to allow a full audit of the data origin at the data set level. Operationally, it is a question of obtaining perfect control of the impact perimeter of an action on a data set stamped as critical to ensure a re-generation in all circumstances. Simplification Not all of these topics can be addressed in this article. I propose a simplified vision of the problem, focused on the problem of lineage while keeping in mind the constraints mentioned above. The entire demo source code used is available here . Thus, in this very simplified use case: Data sets are read on HDFS and persisted on the same file system. The data sets clients-vX and products-vY are used by a first Spark driver ( BusinessDriver_One ) to produce a composition of this data materialized in the data set namesAndProducts and persisted in the container of the same name. Here, X and Y denote the version of the data set. A second driver Business_Driver_Two is responsible for generating from this last data set an aggregated view forming the data set productSummary . The drivers are Spark batchs that are potentially released several times a day. Let’s choose the second release propagation strategy mentioned above for our example. The content of the data sets can also change over time, as new versions of the recordings can be written. In this case, it makes sense to include the version of the data set in the record data model, as follows in the case of the data set clients-v15 : {\"name\":\"Bike\":\"surname\":\"Mike\",\"product-id\":\"B12\",\"quantity\":\"1\",\"version\":\"15\"}\r\n{ \"name\":\"Poll\":\"Poll\",\"surname\":\"Kevin\":\"product-id\":\"R23\",\"quantity\":\"2\",\"version\":\"15\" } < It will therefore be necessary to adapt the lineage and persistence mechanism to take into account the possible values of this version. For example, in our case, we chose to use the version as one of the downstream partitioning keys for input files (i. e., namesAndProducts and productSummary ) to allow data purge operations based on the data set version. Datasets lifecycle also means unambiguous identification in the IS of any process instance. This implies that static identification of a process (by the driver class name for a very simplistic example) is not enough. Each instance, i. e. Each effective launch of each driver will have to be uniquely identified. In this example, this instance will be identified by an Run Id compounded with the driver identifier. Our example is then described in the following diagram: In Run Id 12 , the files clients-v15. json and products-v3. json are processed by BusinessDriver_One and then persist in namesAndProducts. json . In the Run Id 13 a new client version is integrated. The Run Id 47 of BusinessDriver_Two intervenes after these two runs to produce an aggregated view of namesAndProducts in productSummary. json . Spark as a local provider of metadata for lineage In order to generate lineage information dynamically, the idea is to exploit certain features of the Spark processing engine. Indeed, in one of the phases of preparation of the execution of the treatments in a distributed way, this engine builds in particular a DAG of the transformations carried out which it formalizes in a logical execution plan , which makes it a lineage of the transformations. Since these transformations use input data sets and generate output data sets, we just have to find a way to use this DAG to meet our need to trace the data sets. In Spark-based industrial processing applications, such as those developed for the Data Hub, the most widely used data containers are the RDD . Since Spark version 2.0, the use of DataFrame and DataSet is growing with the maturity of Spark SQL. In either case, operating the DAG will be done differently. Use of RDD On an RDD instance, there is a function toDebugString () that provides a String describing the transformation DAG to obtain this RDD: (2) ShuffledRDD[29] at reduceByKey at BusinessDriver_One. scala: 46[]\r\n +- (2) UnionRDD[28] at union at BusinessDriver_One. scala: 46[)\r\n    | MapPartitionsRDD[13] at map at BusinessDriver_One. scala: 20[)\r\n    | MapPartitionsRDD[7] at rdd at AppCore. scala: 52[)\r\n    | MapPartitionsRDD[6] at rdd at AppCore. scala: 52[)\r\n    | MapPartitionsRDD[5] at rdd at AppCore. scala: 52[)\r\n    FileScanRDD[4] at rdd at AppCore. scala: 52[)\r\n    | MapPartitionsRDD[27] at map at BusinessDriver_One. scala: 22[)\r\n    | MapPartitionsRDD[21] at rdd at AppCore. scala: 52[)\r\n    | MapPartitionsRDD[20] at rdd at AppCore. scala: 52[)\r\n    MapPartitionsRDD[19] at rdd at AppCore. scala: 52[)\r\n    FileScanRDD[18] at rdd at AppCore. scala: 52[) This function is intended for debugging and does not provide structured information. I have reproduced the exact logic of Spark 2 to provide, through a RDD parameter, the lineage tree of these treatments. This gives for the same RDD (see function >RDDLineageExtractor. lineageTree () ): (2) ShuffledRDD[29] at reduceByKey at BusinessDriver_One. scala: 46[]\r\n UnionRDD[28] at union at BusinessDriver_One. scala: 46[)\r\n   MapPartitionsRDD[13] at map at BusinessDriver_One. scala: 20[)\r\n     MapPartitionsRDD[7] at rdd at AppCore. scala: 52[])\r\n       MapPartitionsRDD[6] at rdd at AppCore. scala: 52[])\r\n         MapPartitionsRDD[5] at rdd at AppCore. scala: 52[])\r\n           FileScanRDD[4] at rdd at AppCore. scala: 52[)\r\n   MapPartitionsRDD[27] at map at BusinessDriver_One. scala: 22[)\r\n     MapPartitionsRDD[21] at rdd at AppCore. scala: 52[])\r\n       MapPartitionsRDD[20] at rdd at AppCore. scala: 52[])\r\n         MapPartitionsRDD[19] at rdd at AppCore. scala: 52[])\r\n           FileScanRDD[18] at rdd at AppCore. scala: 52[) This function can then be used in a centralized code, such as the generic kernel of applications (here symbolized by the class AppCore ), to be invoked for each action (typically an I/O) of any executed driver. The lineage information, including the processing tree, is then generated by invoking the same function (here lineSparkAction() ) when reading a JSON, just after building the corresponding RDD and writing an RDD, just before its persistence: def writeJson (rdd: RDD[_ < Row], outputFilePath: String): Unit = {\r\n    lineSparkAction (rdd, outputFilePath, IoType. WRITE)\r\n    val schema = rdd. take (1)(0). schema.\r\n    spark. createDataFrame (rdd. asInstanceOf[RDD[Row]], schema). write. json (outputFilePath)\r\n  }\r\n  def readJson (filePath: String): RDD[Row] = {\r\n    val readRdd = spark. read. read. json (filePath). rdd\r\n    lineSparkAction (readRdd, filePath, IoType. READ)\r\n    readRdd\r\n  } The lineage function has three responsibilities: to generate the processing tree by the function lineageTree() , to generate the meta data of the data set (whether it is input or output, everything depends on the direction of the I/O) by analyzing the RDD and the target container and, finally, to produce the lineage messages by exploiting all these data. In our case, and for the sake of simplicity, these messages are also persisted on HDFS. However, one can imagine many other patterns beyond the scope of this article. private def lineSparkAction (rdd: RDD[_ < Row], outputFilePath: String, ioType: IoType. Value) = {\r\n  val rddLineageTree: TreeNode[String] = RDDLineageExtractor. lineageTree (rdd)\r\n  val datasetMetadata: (String, String) = getDatasetMetadata (rdd, outputFilePath)\r\n  val processInformation: String = processId\r\n  produceLineageMessage (rddLineageTree, datasetMetadata, processInformation, ioType)\r\n} Use of DataFrame (or DataSet) When using Spark SQL DataFrame (or DataSet), the task is greatly simplified. Since Spark version 2, processing lineage information is provided directly from the DataFrame API. For a given DataFrame, the inputFiles () method returns a table of the data sources used to generate this DataFrame. Therefore, it is sufficient to intercept only the outputs to generate the lineage information. The immediate impact is that fewer lineage messages are generated. def readJsonDF (filePath): DataFrame = {\r\n    spark. read. json (filePath)\r\n  }\r\n  def writeJson (df: DataFrame, path: String, lineData: Boolean = true, saveMode: SaveMode = SaveMode = SaveMode Overwrite): Unit = {\r\n    if (lineData) lineSparkAction (df, path, IoType. WRITE)\r\n    df. write. mode (saveMode = saveMode). json (path)\r\n  } Keep in mind: “Depending on the source relations, this may not find all input files” . The designer/developer must verify that the various data sources used are well traced. Contents of lineage information The generated lineage messages must contain the information needed to build in detail the semantic triples needed to build the data set lineage graph. A sufficient but not exhaustive list of information to be included is: Univocal process definition Full name of the driver class Run Id Application domain identifier (e. g. group-id maven) Application module identifier (e. g. application-id maven) The version of the application Univocal definition of the data sets Identifier (e. g. table name) URI (e. g. full URL on HDFS) version (the version of the data set with the target granularity) I/O type (read or write) I/O type initial lifecycle status Springs and wells ( sink ) lineage In the case of an I/O in writing: The “destination” of the treatment generating the lineage message (so-called well or sink ), corresponding to the root of the lineage tree The “origins” of the processing generating the lineage message (sources), corresponding to the leaves of the tree In the case of a read I/O Sources only (leaves of tree) Sources only The lifecycle status being precisely a meta-data that will evolve with the functional lifecycle of the data set (e. g. go from a “given” state to “critical data”). In our case, the identification of the process in the lineage messages has been simplified to its strict minimum. Thus, only the class name and the Run Id run are used. The rest of the information to be identified in the method lineSparkAction () is not a technical challenge. +----------------+------+--------------------------------------------+---------+-----------------------------------------------------------------+------------------------------------------------------------------------------------------+-------+------------------------------------------------------------------------------------+-------+\r\n|datasetName     |ioType|producer                                    |reference|sinks                                                            |sources                                                                                   |status |uri                                                                                 |version|\r\n+----------------+------+--------------------------------------------+---------+-----------------------------------------------------------------+------------------------------------------------------------------------------------------+-------+------------------------------------------------------------------------------------+-------+\r\n|namesAndProducts|WRITE |com.octo.spark.lineage.BusinessDriver_One$13|false    |(1) MapPartitionsRDD[29] at map at BusinessDriver_One.scala:52 []|FileScanRDD[4] at rdd at AppCore.scala:45 [],FileScanRDD[16] at rdd at AppCore.scala:45 []|Initial|data-lineage-spark-demo/src/main/resources/examples/generated2/namesAndProducts.json|16     |\r\n|namesAndProducts|WRITE |com.octo.spark.lineage.BusinessDriver_One$12|false    |(1) MapPartitionsRDD[29] at map at BusinessDriver_One.scala:52 []|FileScanRDD[4] at rdd at AppCore.scala:45 [],FileScanRDD[16] at rdd at AppCore.scala:45 []|Initial|data-lineage-spark-demo/src/main/resources/examples/generated2/namesAndProducts.json|15     |\r\n|productSummary  |WRITE |com.octo.spark.lineage.BusinessDriver_Two$47|false    |(1) MapPartitionsRDD[13] at map at BusinessDriver_Two.scala:28 []|FileScanRDD[4] at rdd at AppCore.scala:50 []                                              |Initial|data-lineage-spark-demo/src/main/resources/examples/generated2/productSummary.json  |16     |\r\n|namesAndProducts|READ  |com.octo.spark.lineage.BusinessDriver_Two$47|true     |                                                                 |FileScanRDD[4] at rdd at AppCore.scala:50 []                                              |Initial|data-lineage-spark-demo/src/main/resources/examples/generated2/namesAndProducts.json|16     |\r\n|products-v3     |READ  |com.octo.spark.lineage.BusinessDriver_One$12|false    |                                                                 |FileScanRDD[16] at rdd at AppCore.scala:45 []                                             |Initial|data-lineage-spark-demo/src/main/resources/examples/raw/products-v3.json            |3      |\r\n|products-v3     |READ  |com.octo.spark.lineage.BusinessDriver_One$13|false    |                                                                 |FileScanRDD[16] at rdd at AppCore.scala:45 []                                             |Initial|data-lineage-spark-demo/src/main/resources/examples/raw/products-v3.json            |3      |\r\n|clients-v15     |READ  |com.octo.spark.lineage.BusinessDriver_One$12|true     |                                                                 |FileScanRDD[4] at rdd at AppCore.scala:45 []                                              |Initial|data-lineage-spark-demo/src/main/resources/examples/raw/clients-v15.json            |15     |\r\n|clients-v16     |READ  |com.octo.spark.lineage.BusinessDriver_One$13|true     |                                                                 |FileScanRDD[4] at rdd at AppCore.scala:45 []                                              |Initial|data-lineage-spark-demo/src/main/resources/examples/raw/clients-v16.json            |16     |\r\n+----------------+------+--------------------------------------------+---------+-----------------------------------------------------------------+------------------------------------------------------------------------------------------+-------+------------------------------------------------------------------------------------+-------+ One quickly realizes the incomplete side of messages: many messages (those in reading) do not include “ sinks “. In this case, in order to build the lineage graph, it is necessary to carry out a consolidation step. It consists of linking source nodes to well nodes by analyzing the graphs built from messages. Once this consolidation step is completed, and once the graph has been constructed (class LineageGraphGenerator ), we obtain the following lineage for our example: Node[0]--->[Dataset: namesAndProducts, Version: 16] has been produced by:\r\n [com. octo. spark. lineage. BusinessDriver_One$13] using:[Dataset: products-v3, Version: 3]\r\n [com. octo. spark. lineage. BusinessDriver_One$13] using:[Dataset: clients-v16, Version: 16]\r\nNode[1]--->[Dataset: namesAndProducts, Version: 15] has been produced by:\r\n [com. octo. spark. lineage. BusinessDriver_One$12] using:[Dataset: products-v3, Version: 3]\r\n [com. octo. spark. lineage. BusinessDriver_One$12] using:[Dataset: clients-v15, Version: 15]\r\nNode[2]---->[Dataset: productSummary, Version: 16] has been produced by:\r\n [com. octo. spark. lineage. BusinessDriver_Two$47] using:[Dataset: namesAndProducts, Version: 16] Presented graphically (by truncating class names for readability), this corresponds to the following: Exploitation of the lineage graph This lineage graph can then be used to manage the data lifecycle at the data set level. For example, suppose that only the data set productSummary in version 16 should be retained in the long term because it is considered as a critical data by the business. In parallel, we want to delete in a purge process the older versions of the data sets clients and products . To what extent can this deletion be achieved without jeopardizing the possibility of recovery (as in the case of a PRA ) of the data productSummary ? The answer to this question is to carry out an impact analysis on the lineage graph. This analysis is based on the identification of connected components (connected components) by implementing the algorithm of the same name using a graphically oriented database engine. The engine used in our case (always in the class LineageGraphGenerator ) being Neo4j , via a very simplistic integration for the needs of the article. This algorithm then provides a subgraph that allows you to know all the connected data sets / source versions with productSummary in version 16, i. e. clients v16 and products v3. This is where the attribute “ status ” added to the data model of the lineage messages comes in. By default, all nodes are assigned the “Test”value. If a data set is declared “critical” by the business line, a specific department will have to update the corresponding node in the graphical database accordingly. The same service will request this database to get the connected data sets and tag them as “critical”. The purge service will have to systematically check the status of a data set / version and not delete it if its status is “critical”. The subject is vast and involves profound impacts, both on the architectural level, development management and data governance. In this article, however, we have cleared some of the key aspects of data lineage at the data set level, namely the following: the possibility – for the moment – to implement in-house the solution if the tracking granularity is relatively fine the theory on which to base the thinking the possibility of using Spark’s interesting features to integrate this logic in the central parts of processing applications how they can fit into the code with the treatment of a typical case version management strategy for data sets the structure of the lineage message structure the exploitation of these messages to build the lineage graph an exploitation track of the graph Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Big Data . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 3 commentaires sur “How to hack Spark to do some data lineage” Harshali 14/09/2018 à 12:55 Wow! That was something interesting to try with spark. I'll do it today itself. Thanks for the article. \r\n\r\nCheers! Daniel 01/11/2018 à 07:57 Hi, I'm very curious about your reproduced function ( >RDDLineageExtractor. lineageTree ()) but the link require me to logging, and I can't logging with my gitlab account. Is there any other way to take a look at the function? Thanks a lot! Frank 04/11/2018 à 00:27 Hi there,\r\nvery interesting read! It seems like I need to login to your gitlab instance with an @octo.com mail address (which I don't have, obviously :-) to see the code. Is it possible to change that? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-02-12"},
{"website": "Octo", "title": "\n                The semicircle (episode 8 — The Fifth Floor)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-8-the-fifth-floor/", "abstract": "The semicircle (episode 8 — The Fifth Floor) Publication date 19/01/2018 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 The treatment of error as a source of valuable information is precisely what distinguishes the feedback (error-controlled) system from its less capable predecessors. Jerry Weinberg @OlegTxl Direct Message Hi Oleg, can you spare me an hour of your time? ok, around 6pm? ping me (…) ping pong Thanks Oleg. I would like some advice about mob programming for my project at work OK I was given three months to turn the situation around on the app I’m working on situation ==? we have to deliver a major release in 3 months, but we’re having a lot of problems related to quality. My mgr wants me to improve quality. what kind of problems? a bit of everything, but mostly design and regressions, but not everything is a bug, I think users are trying slip new stories in while we’re not looking. how many people are working on this app? 4. My mgr is also Product Owner. Do you think we need to hire another dev? if you want to spend the next 3 months training a new recruit, do that. Well, yeah, I’m with you on that, it’s futile. I want to propose mob prog, but what if it doesn’t work? what options are you allowed to try? it’s up to me to define – within the bounds of reasonableness. What I would like is to find a way to have fewer bugs can we talk to each other live? Or by tel.? I’ll call you “What’s your definition of ‘bug’ exactly?” “Good question! Some unexpected task that just slips into your work…” “That’s not clear…” “Let’s say we have incidents in production, linked to errors. Is that less vague?” “So an ‘incident’ is a problem that could be about the application or the user or the system, or possibly the documentation?” “What you mean is the lack of documentation. But yeah, that’s pretty much it.” “And what is an error?” “An error is when there is a defect in the program, or the data. But sometimes we are sent defects that are not really defects.” “And what difference does that make?” “I don’t understand your question.” “Okay, no worries. Can you tell incidents from defects in your follow-up analysis?” “No we have everything in the same tool. We only create incidents. Why do you ask?” “So I can understand what you do with the information.” “What information?” “The information embedded in the incidents and defects.” “I don’t follow you.? “Incidents and defects are a source of information about your product and your process, right?” “I agree, but exactly what are you proposing we do with the incident info?” “I would say there are 3 things you gotta do with the incidents. First: manage the incident and find the defect that is ‘likely’ the cause of the incident (duh).” “Of course.” “Second: create a system to better detect defects, for example, tests.” “For example.” “And third: create a system to catch the defects, which really means to prevent them. Because the best defect management strategy is to not create them in the first place.” “Good on paper, maybe.” “I know.” “Are you saying that mob programming is a defect prevention practice?” “Why not? We could think of it that way. Why would we get in groups to produce code that we could program on our own, if it wasn’t to find defects more quickly?” “Can you help me do this now?” “Dude, I don’t have time!” “Too bad.” “Yes.” “Ok, what would be your best idea?” “I already gave you my best idea.” “Huh?” “1: handle the incidents, 2: detect the defects, 3: prevent the defects.” “Hey but that’s a bit abstract for guidance.” “Where are you with these three activities at work?” “I guess we do know how to manage the incidents. Tests: we could improve a lot. As for prevention we have no strategy. There’s work to be done.” “I suggest we do a little role-play. In this role-play, imagine that you are a very young developer just out of school.” “OK.” “You’ve just been hired into a company that seems to be working well. You have joined a team of three, and you have been entrusted with the maintenance of an existing program that does complicated calculations for one of the business areas.” “OK. Sweet.” “You are on the first floor. Floor 1.” “Floor 1? Okay.” “Sylvia, a user of the said program just had an ugly surprise: she gets an incorrect result on one of the calculations: 70,000 instead of 140,000. A doubling error. What are going you do about it?” “I make a copy of the data and I try to reproduce the defect in my environment.” “Bravo. You are now on floor 2.” “That was easy.” “You found the origin of the problem: it’s a typo in a variable name. The author of the code had written ‘prefit’ instead of “profit”. As it turns out, the program is written in Awk , a language in which variables don’t need to be declared. When Awk finds a variable it doesn’t know, it creates it and initializes it to zero. It’s convenient because it allows you to write very concise programs. But it’s also a problem, because in the case of a typo, your program continues with a zero value without reporting any error.” “I see.” “What are going you do about it?” “I will rewrite this in a better language!” “You don’t have weeks. We must act immediately. What are going you do about it?” “I am going to correct the problem, and deliver a new version. It was just a typo.” “Good. You are now on floor 3.” “Perfect.” “Perfect, not so much: Sylvia comes to you to ask you if there is a way to avoid these miscalculations in the future, because it makes her look bad.” “Ouch.” “What are going you do about it?” “I will do some testing of the app before each new delivery?” “Bravo. Here you are on the fourth floor! Now you are running systematic tests on your program before delivering it. You’re still far from testing it thoroughly, and anyway you know it’s not possible, but you are discovering some interesting things: – One of the results was not displayed right, the standard format for displaying thousands was not used. – Another miscalculation, related to another typo: you confused two variables (those names were quite close, mind you). – A logic error: in a somewhat special case, the algorithm doesn’t complete the calculation, because it goes into an infinite loop. – And finally, by running your program on a very large file, you saw that the execution time goes from 4.32 seconds for 100 lines, to 1 hour 20 minutes for 10,000 lines.” “Wow.” “What are going you do about it?” “Uh. I’ll look for another job?” “Really?” “I will correct all these problems, and redeploy.” “Ok good. You’re still on floor 4. Sylvia reported these problems to Harold, her manager. This is normal, since Harold’s results depend on your program, and she’s accountable for that issue.” “Argh…” “So you’re invited (or rather summoned) by Harold. Harold asks you: so far we are quite satisfied, but in the future would there be a way to respond a bit faster to our requests? A week of acceptance testing for a program a few lines long is hard to swallow.” “Hmmm.” “What are you going do about it?” “I’ll say to him: Walk a mile in my worn shoes!” “Seriously, what are going you do about it?” “I’ll ask my teammates if they can help me by reviewing the code with me.” “Very good. Now you are on floor 5. You’re organizing systematic reviews of your program. It takes a little time, but there are almost no defects in production. Plus there are some collateral benefits: – Your team now knows enough about the code to help you change it; as it is a complex domain, this helps you a lot; – you have a coding standard, which is improving with each review; – out of a total of 5 reviews you have already found: – 2 other typos in variable names (yep); – 3 rather subtle logical errors; – a dozen improvements to the code formatting; – a new way to automatically generate test data.” “Life is beautiful!” “Yes. Harold summons you again.” “Oh?” “He says to you: ‘Now that everything works like clockwork, we wondered if you could free up some of your time for another small program, but it’ll have to be quick, well executed, with a process a little more lightweight than your usual process, ok?'” “Errr, uh…” “What are going you do about it?” “I’ll tell him no.” “You can’t really say no to Harold. What are going you do about it?” “I’ll try to show him that it’s better to follow the new process.” “Very good. How are you going to do that?” “I imagine that it’ll be enough to compare the results I had on the 1st floor with those I get on the the 5th floor.” “What results? What interests Harold is the numbers.” “I would say, for each floor, I show: – the number of defects found in production; – the time spent to prevent defects; – the time spent correcting defects; And after comparing the floors, he decides to let me apply my process rather than his own.” “Right. And you stay on the fifth instead of coming back to the first floor.” “I see.” “The real question is: how fast do you want to go from the first floor to the fifth floor?” “Exactly, is it even feasible in my situation? In three months? I have my doubts.” “You asked me for my best idea; that’s my best idea.” “I want to invite you for a beer as a way to thank you.” “Thanks, maybe some other day, I don’t really have time.” “By the way: what’s happening on the 6th floor?” “On the 6th floor, you become manager.” “And what does a manager do?” “You help your teams to climb up, floor by floor.” (to be continued) Previous episodes : 1 — If the code could speak 2 — See/Advance 3 — Communication Breakdown 4 — Driver/Navigator 5 — Brown Bag Lunch 6 — Takeaway Tips 7 — Crisis/Opportunity Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-01-19"},
{"website": "Octo", "title": "\n                Walkthrough: Watch your Ansible deployments in Grafana !            ", "author": ["Rémi Rey"], "link": "https://blog.octo.com/en/walkthrough-watch-your-ansible-deployments-in-grafana/", "abstract": "Walkthrough: Watch your Ansible deployments in Grafana ! Publication date 15/01/2018 by Rémi Rey Tweet Share 0 +1 LinkedIn 0 A few months ago our friends Guillaume Lefevre and Etienne Coutaud made an appearance at the 2017 KubeCon to present their work on a monitoring solution based on Prometheus and Grafana. (The video can be found here ) A bit over a month later, we faced a problem in production, the platform started to behave abnormally. The graphs showed bursts in metrics which was unusual and some people were wondering if a deployment could have changed something in the configuration and triggered the problem. Even if we had a lot of probes in the different stacks of the solution, it appeared that a critical operation such as a deployment was totally invisible in the Grafana dashboard. We could only see the impact on the graph, but there was no mention of an operation. Luckily, Grafana has an awesome feature that would answer perfectly our need: the annotations. Grafana annotations What are annotations you say ? A picture is worth a thousand words: Credit: grafana.org From the Grafana online documentation : Annotations provide a way to mark points on the graph with rich events. When you hover over an annotation you can get event description and event tags. The text field can include links to other systems with more detail. As all our deployments are performed through Ansible , the idea is to have Ansible create an annotation in Grafana everytime a playbook is executed. The Pushgateway is not an event store We started to look at how we could push events to Prometheus through the Push Gateway and display them as annotations in Grafana, but as soon as we went to the PushGateway readme we found a message which seemed addressed to us: The Pushgateway is not an event store . While you can use Prometheus as a data source for Grafana annotations , tracking something like release events has to happen with some event-logging framework. With that immediate stop on our idea, we delayed the topic as it wasn’t in the project’s critical path, but we kept it in mind as a “nice to have” feature. After some time, I found myself browsing the Grafana documentation, and found out that Grafana 4.6+ now comes with an HTTP API allowing to store the annotation natively, meaning no need to store anything in Prometheus. As soon as we got the news, we came back on the topic and started thinking about how we should implement the solution. Annotations from Ansible How can we interact with this brand new API from Ansible ? We identified 3 possibilities: Solution 1: Ansible URI module in the playbooks Since we are dealing with a HTTP API, the first solution coming to our mind was to use the Ansible URI module that would allow us to send the appropriate HTTP request to the Grafana endpoint. But it also means that we have to update all our playbooks and find a way to avoid breaking their idempotency, so it may look trivial but it can result in several lines of yaml. Solution 2: Dedicated Ansible module We thought about writing an Ansible module, mostly to try to handle the idempotency problem in the module and keep the yaml short. Solution 3: Dedicated Ansible callback An Ansible callback would allow to send the annotations without even modifying the playbooks, because it is code that is always executed by Ansible, no matter what you do in the playbooks. But the code is executed only at specific stages of the playbook run. Since our objective was to have all deployments (whatever the playbook) being visible in the Grafana dashboards, a callback seemed to be the good option as everything becomes traceable as soon as the callback is enabled. We also have all our playbooks idempotent and we did not want to lose that or spend too much time trying to keep it while adding the grafana annotations through a module use. I won’t explain how the Ansible callback works, or how we implemented it, but the code is available on Github (and was submitted to the Ansible repository and is awaiting review while I’m writing these lines). Choose the proper representation You may have noticed that we have 2 kinds of annotations available in Grafana: The regular annotation A simple bar with information available on hover. The annotation has only one time indication. Regular annotation In our use case it fits the notification of playbook start or failure. The region annotation Aims at representing a period on the Graph. The annotation has two time indication, one for the event start time, the second for the event end time. The event information are available on hover anywhere between the start/stop times. Region annotation In our use case it fits the representation of the playbook execution period (and duration). Install/Enable the callback From the Ansible documentation : You can activate a custom callback by either dropping it into a callback_plugins directory adjacent to your play, inside a role, or by putting it in one of the callback directory sources configured in ansible.cfg . Plugins are loaded in alphanumeric order. For example, a plugin implemented in a file named 1_first.py would run before a plugin file named 2_second.py. Most callbacks shipped with Ansible are disabled by default and need to be whitelisted in your ansible.cfg file in order to function. For example: #callback_whitelist = timer, mail, profile_roles In our case, the Grafana callback is not shipped with Ansible, so we will have to create the “callback_plugins” directory near our playbooks: $ cd <your_playbook_dir>\r\n$ mkdir callback_plugins Then copy the callback source into the directory: $ cd callback_plugins\r\n$ wget https://raw.githubusercontent.com/rrey/ansible-callback-grafana-annotations/master/callback_plugins/grafana_annotations.py Finally, enable the plugin in the ansible.cfg by adding it in the variable “callback_whitelist”: [...]\r\ncallback_whitelist = grafana_annotations Note: You may have to create the ansible.cfg as it is not created by default when you install Ansible. Different location are supported, see the Ansible documentation . Once the callback is enabled, you’ll have to set some environment variables to provide the callback the required information. Here is the different parameters you can define through the environment available: GRAFANA_SERVER : The Grafana server address GRAFANA_PORT : The Grafana server listen port. GRAFANA_SECURE : Boolean (default 0). If set to 1, you define that HTTPS protocol should be use to talk to the API. GRAFANA_API_TOKEN : The Grafana API Token to be used for the authentication. With the environment variables set, you can run your playbooks without changing anything in the Ansible command call. Demo time! Let’s do a quick demo! First let’s start a Grafana instance through docker: $ docker pull grafana/grafana\r\n$ docker run -d --name=grafana -p 3000:3000 grafana/grafana You now have Grafana reachable on http://127.0.0.1:3000 , the default credentials are admin/admin Create a dashboard Once logged in, create a Dashboard with a Graph panel and save it. For this demo, we don’t need to create a datasource and have data available. When you create a Graph panel, Grafana displays a graph with random data. If you refresh the dashboard you will see that the data completely changes. Again, it is not important for the demo as we simply want to see the annotation displayed at the proper time. Go to the dashboard settings and go to the “annotations” sub-menu: By default, a dashboard has a “built-in” annotation query that only displays the annotation and alerts of the dashboard.(You can scope an annotation to a dashboard by specifying its id). Looking closer at this built-in query definition, we can see that the configuration specifies that the query filters by “Dashboard”. The built-in query for Annotations & Alerts In our case, we have an instance of Grafana and Prometheus per environment, so there is no risk of publishing annotations that are totally unrelated to the environment. So the callback will not publish the annotations on a specific dashboard, they will be global. Since the built-in query will only display the annotations explicitly scoped to the dashboard we need to add a new query that will be able to display our global annotations. Click the “New” Button under the built-in query and configure the following query: Our new annotation query We define a query performed in Grafana’s native store, and the query filters the annotations by tags. It means that our query will look for annotations tagged with the value “ansible”, which is one of the tags defined by our callback for all annotations. The callback also define a tag with the playbook name and 3rd tag among the following possibilities: ansible_event_start: set on the playbook start annotation. ansible_event_failure: set on the playbook failure annotation. ansible_report: set on the playbook stat annotation. The annotations tagged with this value will be region annotation that will cover all the playbook execution period. For the demo, selecting the “ansible” tag will allow to display all the annotations with only one query. Create the API Token for Ansible If you have the dashboard and a Graph panel inside it, let’s now create an API Token for Ansible. You can do it from Grafana UI but you can use the API like me: $ curl -XPOST 127.0.0.1:3000/api/auth/keys --user \"admin:admin\" --data '{\"name\": \"ansible-callback\", \"role\": \"Editor\"}' -H \"Content-Type: application/json\"\r\n{\"name\":\"ansible-callback\",\"key\":\"eyJrIjoiZ2RNc2NPWXZmNE5IZmxjb1hHOGJTNk5YSjJqWXdmbVYiLCJuIjoiYW5zaWJsZS1jYWxsYmFjayIsImlkIjoxfQ==\"} The token value is returned in the server’s response. Write it somewhere, you can not get it twice, you will have to recreate it if you loose it. Setup the environment The configuration parameters are provided to the callback through environment variables. Export the following variables: $ export GRAFANA_SERVER=127.0.0.1\r\n$ export GRAFANA_PORT=3000\r\n$ export GRAFANA_API_TOKEN=eyJrIjoiZ2RNc2NPWXZmNE5IZmxjb1hHOGJTNk5YSjJqWXdmbVYiLCJuIjoiYW5zaWJsZS1jYWxsYmFjayIsImlkIjoxfQ== Note: be sure to replace the token value by your own. Since Grafana is reachable through HTTP in the container, we can leave GRAFANA_SECURE to its default value. That is why I am not exporting it. Now let’s write a dummy playbook that will not do much: test.yml: - hosts: localhost\r\n  connection: local\r\n  tasks:\r\n    - debug:\r\n      msg: “Hello world” We just want to trigger the callback, so what the playbook does is not really important here. Let’s run it: Now that everything is ready, let’s run the playbook: $ ansible-playbook test.yml [WARNING]: Host file not found: /etc/ansible/hosts [WARNING]: provided hosts list is empty, only localhost is available PLAY [localhost] *************************************************************************************************************************************************************************************\r\n\r\nTASK [Gathering Facts] ******************************************************************************************************************************************************************************* ok: [localhost] TASK [debug] ***************************************************************************************************************************************************************************************** ok: [localhost] => { \"msg\": \"“Hello world”\" } PLAY RECAP ******************************************************************************************************************************************************************************************* localhost : ok=2 changed=0    unreachable=0    failed=0 Check your dashboard and enjoy the fancy annotations! Annotations in details The playbook start annotation This annotation aims at notifying of a playbook execution. Since a region annotation needs a start time and an end time, it is not possible to create the region before the end of the playbook. We can still have something visible on the Grafana dashboard by having this simple annotation so that anyone watching the dashboard can see something is going on before it ended. The playbook report annotation The annotation allows to see the playbook run period through a region annotation. Now if you see huge peaks/falls in a panel, you immediately see that it is related to a deployment, you don’t question yourself about the possible root cause of the problem to realize that a deployment was performed. Awesome right ? Howdy! That’s nice! Now we just have to configure our Jenkins (or any kind of CI/CD tool) pipeline/jobs to define the environment variable before executing the playbooks, and we’ll have all the deployments being nicely displayed in the Grafana Dashboards. That’s it. If you have questions, comments or remarks, don’t hesitate to comment! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Walkthrough: Watch your Ansible deployments in Grafana !” L_Mar 23/04/2019 à 17:53 Hello ! First, thank you for the really nice tutorial , i find it really helpful and interesting \r\nWell, I followed the tutorial step by step (Working on grafana and ansible only no Prometheus involved or installed )but when trying to run the playbook to trigger the callback i keep getting this error :\r\n\"ERROR! No setting was provided for required configuration plugin_type: callback plugin: grafana_annotations setting: grafana_url\"\r\nit's my first time working with grafana so i really don't know what configuration is missing or if i need to install Prometheus even if I don't explicitly need it.\r\nI would really appreciate your help ! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-01-15"},
{"website": "Octo", "title": "\n                Octo Australia 2017: a retrospection            ", "author": ["Eric Favre"], "link": "https://blog.octo.com/en/octo-australia-2017-a-retrospection/", "abstract": "Octo Australia 2017: a retrospection Publication date 07/01/2018 by Eric Favre Tweet Share 0 +1 LinkedIn 0 Hi all, Happy new year! 2017 is now over, and it’s time for us to contemplate everything that happened in Octo Technology Australia and appraise our success and failures. Hidden Tech Challenge Late 2017, we launched a geolocated quiz game in Sydney, and it was a lot of fun. Not only did we build the product from the idea to the delivery, but we also handled the communication, animation and follow-up of the game. This design was displayed everywhere in Sydney We had a retrospective on this initiative and it revealed we were all happy with the technologies involved (Vuejs, AWS lambda, serverless, circleci, S3…), the friendly and self-disciplined team dynamic and the results (more players than we hoped for). The changes we’d like to see for next time is to be more data-driven, to dedicate more office time to it, and to keep trying to communicate more effectively. This was mostly a success, we are very proud of what we did and how we did it. Celebration time in a French restaurant 4 Coding nights We’ve also had coding nights, a geeky time at the office where we try to build something fun and operational within an evening. Preferably not just a web/mobile app. We had 4 of them: Ping pong game referee with Amazon Alexa, AWS lambda, React and an LCD screen plugged to a raspberry pi. This night also ignited 2 more Alexa projects, a BOF timer and Houston . Office presence detector with a magnetic reed switch, a Raspberry Pi and Slack Image recognition with an Android application, AWS lambda, AWS Recognition Rock-paper-scissors application with LeapMotion, React, Websockets, Nodejs, Express, EC2, AS3 … This one was a very special one, since we had a bunch of people from Accenture Australia with us and a lot of cheese (fondu party)! These nights are always a good deal of fun and learning, and they are a good opportunity to work altogether. What? There is some fondue? Why didn’t you say so earlier?! We’ll definitely do that again, and we already have plenty of ideas to implement. 10 Meetup presentations We did a few meetups presentations, including: MAPE Regressions Kata Bowling in Haskell Machine Learning using biased data Alexa Houston Elm hands-on Making the right product the right way Frontend tests with Vuejs Hidden Tech Challenge We take pride of having in depth knowledge about these versatile subjects. Next year we plan to talk even more than that. BOFs & BBLs All year long we’ve had our BOF (Birds Of a Feather), a monthly ritual where some of us present technical subject they feel like presenting (new technology, return on experience, side project…). Every time almost everyone was attending. There were over 40 different subjects presented by each Octo (2 or 3 times each). We also invited 2 external speakers to our BOF. Here are our top 6 favorite topics: Big Data (6) Software Craftsmanship (6) Frontend (5) REX (5) OPS (4) Machine learning (3) Later this year, we also rebooted the BBLs (Brown Bag Lunch), and it was good. We talked about blockchain, why getters and setters are evil, database update management and JWT security flaws. We’ll try to keep it up in 2018. HR This year we are happy to have 7 newcomers joining us. Welcome onboard all! A few OCTOs visited us from France too, and it was great to have them here! They can come back anytime :) Unfortunately we still have a hard time finding all the senior consultants we wish we had. Another aspect we need to work on is to become a true English speaking company. We are mostly French (14 French for 3 non French), and switching back to French in private conversations is often the easy way out. This needs to be properly addressed to avoid any discomfort among English speaking employees. Overall, managing recruitment is still a challenge and while the HR team is doing its best, we feel like we should be able to do better. We had a brainstorm on how to improve that with all OCTOz during the last plenary, and many ideas were raised. We now need to evaluate their feasibility and activate them. Woctoz During Winter we had an amazing Woctoz (Weekend Octo) in the Snowy Mountains. It was fun to ski in Australia, and the weather was mostly perfect. A picture is worth a thousand words: If you look closely you can see some people in the middle We don’t know where the next Woctoz will take place, but this one will be hard to top. Christmas party For Christmas we had a plenary in the morning. It was a good opportunity to assess our performance, business wise and on an other aspects too. something funny was just said Then we had a great Christmas boat trip in Sydney’s harbour. We had an onboard BBQ, some went for a swim and champagne was flowing… This was preceded by a party at the CEO’s the week before. Of course there was also champagne, swimming and BBQ :) This gif is mesmerizing Techshare Ali and Nicolas P. are organizing the first Octoz meetup soberly named Tech Share. We share! About Tech! It will be a recurring meet-up where each event will have a different thematics. The first one is about Data, is partnered by Confluent, hosted by ING and will feature a presentation on Spark by Arthur and Nicolas G. And good news, it’s still time to join! This is a very excitinhg initiative and we’ll do our best for 2018 to see the Tech Share thrive. Cédric’s ping pong victory One of our clients organised a 3 weeks ping pong doubles tournament. As our most talented ping pong player (comment if you disagree), Cédric Nicoloso took part in the tournament, and with his also talented partner they ended up winning the tournament. This was good fun. That’s about it for 2017, see you next year for 2018’s retrospection! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Culture , News and tagged australia , OCTO , retrospection . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Octo Australia 2017: a retrospection” Nick 08/01/2018 à 09:50 Proud to be OCTOZ!!! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-01-07"},
{"website": "Octo", "title": "\n                The semicircle (episode 7 — Crisis / Opportunity)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-7-crisis-opportunity/", "abstract": "The semicircle (episode 7 — Crisis / Opportunity) Publication date 12/01/2018 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 Managers are not confronted with problems that are independent of each other, but with dynamic situations that consist of complex systems of changing problems that interact with each other. I call such situations messes. Problems are extracted from messes by analysis. Managers do not solve problems, they manage messes. Russell Ackoff A chat window opens at the bottom right. Maria: come to see me when you can for 15 minutes today pls. You save your work, answering: I’m coming. You grab something to write with. Maria invites you to take a seat at the small round table filling the small space she was allocated, closes the door and sits down across from you. “While you were meeting yesterday, three new tickets appeared. Did you see them?” “Yes. It’s from the last delivery, the one from Friday.” “4236: ‘When submitting the form, I get an error message in red, mandatory field. All my fields are filled in. Can not validate the form.'” “We’ve looked at that. I can explain but it may take time.” “4237: ‘The Contracts pane doesn’t open automatically after validating the Address pane in update mode.'” “There I’m clueless. Probably the flow is confusing.” “4238: ‘The sales report contains blanks in place of the total amounts, this problem has been reported and corrected in the previous version, but it appears again in this version.'” You are dumbfounded. Maria continues. “These tickets are putting the project behind. What shall I say at the next Steering Committee Meeting?” “You could tell them that we have a structural problem with this part of the application code and that we will have to invest a little time in its design before we can return to normal…” “That’s not what they want to hear.” Breathing. You are tempted to answer, but you feel that the discussion is already complicated enough. No need to add missing interlocutors or imaginary answers. Maria continues, looking you straight in the eyes. “I hired you to solve problems, not to create new ones. I thought your approach to problem solving would get us out of trouble, not push us in even deeper.” Breathing. This is an interesting conversation to debug. With patience and meticulousness. Remember. Patience and meticulousness. “My approach is to avoid creating new defects as much as possible when I update the code, but that does not mean that I master all the code.” “I get that.” Breathing. What would make this conversation more effective would be me not feeling so threatened. What would work would be me feeling confident in the possibility of extracting this bug, reproducing it, diagnosing it, and correcting it once and for all. If only she knew how much this has nothing to do with who is touching the code or which principle of problem solving is being used. This code base is a random labyrinth, plunged into darkness. Some parts are flooded. Each rescue attempt causes further landslides. There is nothing to do, only to pick up debris here and there, and move them a little further away. You say, “It’s not so much a question of who changes the code, or which approach to adopt. It’s more a question of having maneuverability.” “What do you mean by that?” “Take global warming, for example.” “Not sure that is a good example.” “OK. These three new tickets: I hear you would like to get rid of these problems. If only we could make these problems disappear.” “That’s what I’m asking you to do.” “These tickets are not problems to solve, they are symptoms.” “So what?” “What you’re asking me is to suppress the symptoms more effectively. But doing that will not change the root cause.” “What is the root cause?” “Would you mind if I make another analogy?” “No.” “Good. Think of a guy who has major health problems. He is overweight. His heart is failing. He’s had a heart attack. His doctor is performing the checkup. He explains to his patient that during the last twenty years, he hasn’t had a very healthy lifestyle: food too rich, far too little exercise, and most importantly, a pack of cigarettes a day!” “OK…” “The doctor prescribes: ‘Stop smoking. Start being a bit more active. Not too much, don’t fool yourself. A healthy diet. Alcohol out of the question.’ And the person answers: ‘Yes doctor, but not right now. I just need you to put me back in shape because tonight is the corporate anniversary and we’re getting ready for a hell of a party.'” “That’s ridiculous.” “What I mean is that the problems within this application won’t be solved with a few bug fixes. The time to remedy this problem is proportional to the time it took for the problem to appear, namely several years.” “Well, that remains to be proven!” You don’t know exactly whether Maria is feeling more angry, impatient or anxious. But you can’t find the courage to ask her. You continue. “We’re talking about two hundred and fifty thousand lines of code.” “It must be possible to produce code that doesn’t contain defects! Can’t you at least explain to your teammates what needs to be done?” “The world would be very different if all that was needed was to explain how to fix a problem.” “Spare me the philosophy. I’m talking about bugs in production, seen by beta testing customers! There must be something we can do for heaven’s sake!” Breathing. It’s as if you had just invited your manager to make a foray into the obscure mazes of the code. Like the others, she bumps forcefully into the invisible obstacles, but she interprets this experience very differently. She was imagining a simple functional space, a well thought out architecture. She reasons in terms of plans, projects, results. She didn’t expect to find so many floors in ruins, no light, no elevator, no stairs, nothing to hang on to. You continue. “It can be helpful to explore 4236, if you have a little time.” “I have exactly eight minutes.” You open your notebook and you sketch a form. “We reproduced the bug, and we did a complete post mortem. When the user enters data in the form, one of the options, when checked, brings up two new fields. If they are present on the screen, these fields are mandatory. The user begins to enter one of the two fields and then changes the option again. Both fields disappear. The user submits the form, which triggers the validation logic on the fields. The controller sees that one of the two fields is entered and deduces that the other field should also be entered. ‘Mandatory field’ appears at the top of the page. If the field in question was visible, it would also appear in red. The user is forced to enter a field he doesn’t see.” “That’s idiotic! How did we get into such a situation?” “In the previous version, we didn’t have an option to check. Instead we had three fields, say A, B and C for short. B and C appeared depending on whether A was filled in or not. For the controller, the rule was: When the field A is filled in, the fields B and C must also be filled in. Field A has been replaced by a check box. The controller was then modified: the rule  became: when the field B is filled in, the field C must be filled in as well. But suddenly the check option is no longer used in the controller, which explains that the controller can incorrectly prevent a legitimate entry. ” “I see. It’s a mistake of logic.” “Exactly.” “What kept you from discovering this logical mistake?” “We retested the entire form, in both cases: option checked, unchecked. But not in the case where the option has been checked, one field out of two filled, then the option is unchecked. We can’t test everything …” “I understand that we can’t test everything. You don’t understand my question: What prevented you from discovering this logical error before the tests?” You are left speechless. Could Maria actually think that you read the code of every single update before every release? “We don’t exactly have the time to review all the code, Maria. This isn’t the corporate strategy.” “You won’t get away with blaming that on the company strategy!” Maria seems furious now. It’s as if you had just told her: this ruined barrack, in which you’re blind and stumble every two steps, this isn’t just a question of execution. It was perhaps part of the plan: “We’re going to build it here, and then we’ll put a second floor above it. As for the central pillars we’ll deal with it later, for the moment there’s no budget. Hurry up!” Breathing. There is no point in being cynical or getting defensive. “I take it back. I’m not talking about strategy, I’m talking about the process.” “What’s wrong with our process?” “In our process there is no place – not even a single place, none – for prevention.” “Prevention?” “You ask me if I could have noticed this error before the tests. My answer is: No.” “I see.” “To continue with a medical analogy, it’s like being in a hospital, wondering why so many people get sick, and trying to treat them one patient after the other, while being completely ignorant of the rules. Basic medicine such as the presence of microbes around us and the practice of washing hands.” “Well, I’m counting on you to change that.” “With whose budget?” “I don’t have a budget. I have to ask for one. I need an estimate with results forecast. Over three months.” Once again you are left speechless. You are still in the middle of the debris without light. Your manager is already on the next level. What are you waiting for? “You got it. I want to explore this mob programming technique…” “No more exploration. You find me a solution!” “I’m not sure I have a solution at this point…” “You find me a solution that I can sell to the Steering Group.” “I’ll see what I can do.” (to be continued) Previous episodes : 1 — If the code could speak 2 — See/Advance 3 — Communication Breakdown 4 — Driver/Navigator 5 — Brown Bag Lunch 6 — Takeaway Tips Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-01-12"},
{"website": "Octo", "title": "\n                The semicircle (episode 6 — Takeaway Tips)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-6-takeaway-tips/", "abstract": "The semicircle (episode 6 — Takeaway Tips) Publication date 05/01/2018 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 It’s all talk until the code runs Ward Cunningham After the waiter finishes combining four small tables, the group sits down and orders food. They start talking about programming. Someone brings up the meaning of the number 42. You start with your main topic. “So yesterday afternoon, I did a mob session with my team in the conference room.” Oleg, who often proposes to facilitate Mob Programming, answers. “OK! How did that go? “Not very well, actually.” You caught the group’s attention. “I’ll spare you the endless debates about class and method names. You have to bear in mind that our application is not really what could be called a trophy winner.” “Do you mean it’s a bunch of mumbo jumbo?” “Not exactly, but it lacks consistency for sure since this code base is actually a merging of several systems.” “Oh yeah, those are the worst.” The person who seems to be the most senior, Victor, asks, “What is the worst variable name that you’ve come across in your career?” Everyone hesitates. Victor exclaims, “Foobar2!”. Laughters. “That one you frame and hang in the building lobby.” “No, you leave a little comment in the code: slash, slash we aren’t at the circus.” “And then, someone comments on your comment: slash, slash, I’d like to see how you would name it!” “Slash, slash, damn it, change this variable name and stop this rotten thread!” “Slash, slash, look who’s talking.” “Slash, slash, Talk is cheap.” You pick up where you left off. “Well, it wasn’t quite like that, but not far off. Heck, if the only issue was naming, we would be fine.” “Say more,” says Oleg. “Discussions about the variable names were nothing compared to discussions about the design.” “Oh.” “Twenty-five minutes on the subject of dependency injection and constructors, that’s a lot for four people.” “Indeed.” “As my project manager says, ‘you are burning dollars.'” “And where did you get to?” “What do you mean?” “With respect to dependency injection…” “Regarding dependency injection, nowhere. It didn’t matter so much, since the code we were discussing is never executed anyway.” “OK! So that stopped the discussion, then,” someone says. “You’re not going to believe it, but in fact, no.” Oleg asks, “How did you know that the code you were discussing was never triggered?” You say, “It’s very simple: we put in a break, we launched the application and it didn’t stop at the break.” Another participant blurts out, “Breakpoint Driven Development!” You begin to feel uncomfortable with the tone of this conversation. Or it’s fatigue. You conclude, “In short. It didn’t work out as I hoped. If you have suggestions, I’m interested.” Oleg intervenes, “I noticed that in general, code discussions are much more effective when the code has tests. Did your code have any tests?” “No, unfortunately. I mean, we test the application before delivering it, but that’s probably not the type of test you’re referring to.” “Any test is better than nothing at all. But preferably automated, repeatable tests, which run independently of external resources.” “So no, the code doesn’t have this type of test for the moment. The interest I had in working with everyone was to think about how we could start writing tests…” Victor asks, “Oleg, why are code discussions more efficient when the code has tests?” “Because with tests, you can protect your code against possible regression defects, while without tests, you are at constant risk of introducing them.” “OK, but how does that make the discussion more effective?” “I’m getting to it. Without tests, you can’t improve the design, or else you’ll be putting the application at risk. You may want to change the design, but you know that this change won’t result in operational code right away. Not without a considerable amount of downstream testing.” “For sure.” “So you find yourself with what you think is a good idea but you can’t show it to your colleagues immediately. You feel forced either to convince them or to drop the idea.” Someone adds, “Whereas if you could simply refactor the code while relying on the tests, the code would speak for you.” “Exactly,” says Oleg. “Since the tests are done downstream of the design and since they are also rather expensive, one tends to wait until a bunch of ideas have accumulated before deciding whether to make the change or not. And it’s not effective.” “With tests, you can write your improvement right away. If your idea breaks the tests, or if it’s not convincing after all, you can undo it very quickly.” “Which is to say that when it has tests, the code can participate in the discussion. Without testing, the code does not participate in the conversation.” “Exactly. The conversation becomes quite abstract. That’s what makes tests so useful when discussing design.” “That is to say during a refactoring.” “Yes.” You nod showing that you agree with what’s being said, “I see. But it’s not going to be easy. In order to test the code, we should refactor it. But refactoring code that has no tests is too risky. We don’t know what we are getting into and how long it will take.” Oleg agrees. Then he adds, “Speaking of time, your team should try time-boxing what you do during the session. It’s a good way to check that we are not going for something impossible, or having a sterile debate, or shaving a yak.” “Do you mean: decide in advance how much time we should take for each task?” “It’s more like: limit the time spent by the Driver on the keyboard, by setting a rotation every 5 minutes for example. And also rotate the roles, so as to change the point of view and most importantly to bias the team toward action.” “I’m not sure everyone would accept this way of working. Then again, this is our first try, I can’t really be the judge.” One of the participants remarks, “Developers who have worked alone on their code for years or even decades would be hard put to form a Mob Programming team overnight, let alone an excellent one. If you want this approach to work, you have to give it a little time.” Oleg adds, “And it absolutely must be facilitated. Without facilitation, it is difficult for a group to think together. Some speak too much, others not enough…” “Some are more comfortable speaking in a group. Others wait politely before speaking up even thought their ideas might be superior. Sometimes they give up altogether.” “Some are willing to talk but they lack confidence that their ideas will be heard. These are the late speakers.” “Then there are those who portend design issues and intervene to prevent the solution from being put in place. These are the early speakers.” “Which causes others to insist more, and to counter-resist.” “That happens a lot with us. So there is no way to start with something simple. There are objections right away, and we excel at building a bunch of stove pipes.” “Suddenly you’re building the bridge before seeing river.” “Some have side-bar conversations, they are disengaged. They criticize past choices without really offering alternatives. Or they take care of something else on their own screen while making snarky jokes.” The discussion is wrapping up. The faces are gradually fading. Your energy is decreasing. “In short,” concludes Oleg, “if you want your team to self-organize, you’ll have to help it along.” “I have the impression that Mob Programming works best with teams where everyone gets along well with each other.” “I would say it’s exactly the opposite,” Oleg answers. “Namely, that in my team we get along well because we code a lot together.” It’s getting late. You head to the nearest metro. You postpone the decision on whether to try this way of working with the team one more time. Too many variables. (to be continued) Previous episodes : 1 — If the code could speak 2 — See/Advance 3 — Communication Breakdown 4 — Driver/Navigator 5 — Brown Bag Lunch Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-01-05"},
{"website": "Octo", "title": "\n                The semicircle  (episode 5 — Brownbag Lunch)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-5-brownbag-lunch/", "abstract": "The semicircle  (episode 5 — Brownbag Lunch) Publication date 29/12/2017 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 Jeremy arrives first, he settles in near the projector and takes a sandwich out of his brown paper lunch bag. “So if I understand correctly, you propose that we spend the lunch break on the XXL code.” “Yes. But just experimenting, mind you. It’s a trial.” “Don’t bother justifying, I am joking.” Farid and Audrey now show up and take a seat. Farid asks, “Could you remind us what you mean by Mob Programming? I forgot.” “Rather than trying to explain it, I suggest we try if you want to?” “Okay said Audrey. The question is: on what code?” The projector is now on. You search the source directories of the XXL project, saying, “We need an in-progress story, I think.” “Take number 106.” You click on a yellow rectangle and a pop-up appears. As a user, I want to be able to allocate the budget over any period. Jeremy points out, “It’s unclear.” Audrey explains: “Today the budgets are calculated on a semester basis. The user enters an amount, and the program splits the amount into months by dividing by six. What’s needed is to be able to enter any number of months, and the amount would be spread over all those months.” “A number of months and also a starting month, you mean?” “It isn’t specified.” “How should the program determine the starting month? Should it guess?” “Jeremy, you really should have come to the backlog grooming meeting.” “I had soccer duty.” “That isn’t funny.” Breathe. You say to yourself, “I should have started with the bowling kata.” You say out loud, “If we need to, we can always call Charlene.” “OK.” “Here’s how we’ll get organized: the person sitting in front of the keyboard is the driver. The other participants are navigators. The driver’s role is to implement what navigators tell him to implement. The driver can’t decide alone what code to write, he waits for instructions from the navigators. The driver is not allowed to think.” “OK! Can I be the driver?” asks Jeremy. “We’ll switch the driver every ten minutes, so that everyone gets a chance to try both roles.” “First dibs!” says Jeremy. He takes your spot in front of the keyboard. Audrey starts. “The first thing to do is look at the code of the SemesterBudgetManager class. Here. Go to the DefineBudget method. It takes the amount of the budget as an argument.” “How does it know which semester we’re using?” Farid asks. “It’s one of the class fields.” Sitting in the back of the meeting room, you stare at the class code, but not without difficulty because Jeremy is moving the editor window around far too quickly. Farid says, “This code is hairy.” Audrey explains, “It was part of the business layer of the Gramma app that we inherited after the company bought out GM-Soft.” “I wouldn’t have written it like that.” Farid says. “Actually, I’m surprised you never got into this code.” “Maybe you are, but that’s the truth. I have plenty enough work with the front end, as a matter of fact.” Audrey says to Jeremy, “We just need to add a StartingMonth or BeginMonth field, something like that.” “Which one, StartingMonth or BeginMonth ?” asks Jeremy. “ BeginMonth doesn’t mean anything,” intervenes Farid. “You might as well be coding in Greek.” “It doesn’t change the problem,” answers Jeremy. “How will the class know that the starting month isn’t already in a budget?” “We only have to add a parameter to the constructor.” “Add a parameter to the constructor?” Your cell phone rings. Ten minutes have passed. You say, “I’ll take the keyboard. Jeremy, you join the navigators.” Jeremy resumes. “So, how will the class validate that the starting month isn’t already budgeted?” Audrey answers, “It isn’t described in the story. But I think that if you look in the CalendarBudgetManager class, there must be a method that takes care of that. ToF, open the CalendarBudgetManager class. It’s in the Calendar module.” You navigate in the subdirectories. You see this code base suddenly in a new way. It’s like a room full of rubble, dimly lit by a generator that a rescue team was able to bring by. You don’t see very far, but far enough to take an inventory together. You listen to Farid and Jeremy debate what step to take next. “You declare a new constructor, different from the default constructor, which will take the starting month as parameter.” “No way! There are rules for persistence. You can’t go tinkering with new constructors whenever you feel like it.” “It’s temporary. Just until we know how we can make this class work with the new story.” “Temporary, my ass. If you do that, we’re no longer following the original design. And besides, we’ll be undermining the Single Responsibility Principle.” “Remind me what that is?” He reminds him what it is. Eight minutes have passed, and you haven’t edited the code once. Audrey tries to reach Charlene for additional information, to no avail. Jeremy says, looking at the code bitterly, “That’s a completely half-assed solution, that idea of yours.” “It would at least be a step forward.” The alarm goes off. Farid takes the keyboard. You suggest, “Could we run the code now, putting a break and a print command to know what happens when the method is invoked?” “Ok, let’s.” Farid puts in the break and the print command. Then he runs the application. The application doesn’t start up because a setting in the acceptance environment is no longer there. You look for the setting. You accidentally launch a script twice, which has the effect of changing all the results of the previous calculations. Now the budget item chosen for the test can no longer be reset and put into the budget. After three rounds of drivers, you can finally start the budget calculation. The calculation is done, and the program returns to the menu. It didn’t go through the break point set by Farid. Jeremy laughs. “No kidding! The method is never called.” “Oh my, oh my…” “Well, it doesn’t surprise me, because I was thinking this code doesn’t look like the normal code in the business layer. It isn’t formatted the same way.” “For all we know, it’s dead code.” “If that is the case, let’s delete it.” Audrey intervenes. “Please don’t touch anything. I spent enough time in Maria’s office last week. I wouldn’t want to have to go to pains to explain to her in all manner of detail, why we have regression defects in the Budget module.” You consider the extent of the damage. This generator has limited autonomy. It is already one thirty. Jeremy offers a solution. “What we need is to review the model with the P.O. and completely redesign the module. To do it right, we would have to redo the design of the entire business layer in fact.” “As if we could embark on a redesign…” “So what about my story? What are we going to do?” You propose a round of feedback before ending the session. Farid says, “Mob programming is interesting, I think it can work, but you need a very clear agenda. And more precision in the user story, too.” Jeremy says, “The intention is good, but you have to be realistic. The current design of the app doesn’t lend itself to this way of working. And frankly, we should do a source code scan to detect dead code, and get rid of it once and for all.” Audrey says, “I was interested in the idea, especially after what I saw at the Dojo two weeks ago. But now my impression is that all we do is notice all the problems and take no action. And we don’t agree at all on possible solutions.” “We can agree,” says Farid, “It’s just that we need more time.” “I don’t agree,” says Jeremy. “If we don’t have the same fundamental design principles, we might as well work alone, as long as we stay in our separate areas.” “And what, integrate only in the end?” Farid asks. “That’ll be a gorgeous application.” “It depends on your perspective,” Jeremy replies. Audrey’s looking at you, while interrupting the discussion with a bold hand gesture. You say, “I’m a little disappointed. On the one hand, we are finally discussing the famous business layer, and I’ve been wanting to do that for a long time. On the other hand, given the time four of us need to run a simple bit of budget recalculation code…” “… and understand what’s going on!” “And understand what’s going on, I have doubts about the process.” Jeremy gets up, “Well, I am swamped. I have to go back to my technical story.” The group cleans up and puts the chairs back. Audrey is already in the hallway, on her way to her 2 p.m. meeting. She adds, “Turn off the lights when you leave!” (to be continued) Previous episodes : 1 — If the code could speak 2 — See/Advance 3 — Communication Breakdown 4 — Driver/Navigator Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-12-29"},
{"website": "Octo", "title": "\n                Clean Node – Part 1            ", "author": ["Simon Renoult"], "link": "https://blog.octo.com/en/clean-node-part-1/", "abstract": "Clean Node – Part 1 Publication date 05/03/2018 by Simon Renoult Tweet Share 0 +1 LinkedIn 0 I have been working with Node.js for almost 6 years now (started back in 2012 with 0.6.10). During these years, co-workers have been asking me the same question over and over again: “ What does your application look like? “. It is now time to answer this question (or at least try to) ! Coming from Java, Python or Ruby, the question might sound irrelevant since frameworks such as Spring, Django or Rails and patterns like layered architectures or clean architectures offered answers some time ago. However, if you have worked with Node.js, you might be able to relate. Indeed, the Node.js ecosystem has thrived these past years thanks to NPM , its package manager (among other things, of course). And even though NPM did a really good job at making small pieces of code available to all , the community has never really communicated on how should these pieces be orchestrated together . As a proof, take a look at https://github.com/sindresorhus/awesome-nodejs that gathers useful Node.js resources. This repository has 20k stars on GitHub and is an awesome list of cool and well-maintained libraries. Yet, I challenge you to find a single link discussing how can these pieces be assembled together in order to build a consistent and maintainable product. Funnily enough, answers to this question exist and have been discussed for years: SOLID principles, Clean Architecture, Hexagonal Architecture, Clean Code, eXtreme Programming, TDD, BDD, DDD, etc. All these techniques and principles help us design our code better . What I think is definitely lacking though is the application of these to the Node.js realm. This article is first of a three-part series discussing this exact topic and suggesting potential solutions. Each article will be using a test-first approach, either unit or end-to-end: This first article will set a common ground: quick and dirty version with end to end tests and discussing common mistakes, general opinions and misconceptions Second article will refactor the “quick and dirty” version with unit tests and a layered architecture Third article will refactor the code using the Clean Architecture and unit tests Ideas and implementations are 100% open to discussions. Comment, issues and PRs are very welcome: https://github.com/simonrenoult/nodejs-application-architecture Gears and features In order to get started, we need a few tools and materials we can play with. To keep things mainstream and easily applicable to your context, we’ll stick to widespread technologies: API built with the Express framework Relational database with SQLite Sequelize as our ORM Bunyan for logging Mocha and Chai as testing and assertion libraries We now need a use-case. Again, nothing fancy but some business rules to be realistic: Shopping API with products, orders and bills. Products Can be created and listed Have an identifier, a name, a price and a weight Products can be sorted by name, price or weight Orders Can be created and listed Have a status, a product list, a shipment amount, a total amount and a weight Orders status can be one of pending, paid or canceled Are offered 5% discount when the price exceeds 1000€ Shipment costs 25€ for every 50kg (50€ for 100kg, 75€ for 150kg, etc.) Bills Can be listed Have an amount and a creation date Are automatically generated when an order status is set to paid Down the rabbit hole… Quick and dirty Most Node.js tools and frameworks embrace the first rule of the UNIX philosophy: “do one thing and do it well”. What has been missing however is the translation of the cooperation rules. Which means that in Node.js, the look of your application architecture mostly boils down to your software engineering experience . To address this issue, we’ll start with a “quick and dirty” version in order to reveal recurrent design flaws Node.js developers can make. Please keep in mind that this code base is a combination of several common mistakes and might look silly to most of you. Let’s code! Application scaffold We’ll start with a server using Express (note: we separated the server starting instructions in bin/start to ease testing): const express = require(\"express\");\r\nconst app = express();\r\n\r\n// code will go here\r\n\r\nmodule.exports = app; We now add the body parser and the database connection which requires some refactoring in order to use async/await and a configuration file: const express = require(\"express\");\r\nconst bodyParser = require(\"body-parser\");\r\nconst Sequelize = require(\"sequelize\");\r\n\r\nconst env = process.env.NODE_ENV || \"development\";\r\nconst conf = require(path.resolve(__dirname, \"conf\", env));\r\n\r\nconst sequelize = new Sequelize(\r\n  conf.db.database,\r\n  conf.db.username,\r\n  conf.db.password,\r\n  conf.db.sequelize\r\n);\r\n\r\nmodule.exports = async () => {\r\n  const app = express();\r\n  await sequelize.sync();\r\n  app.use(bodyParser.json());\r\n\r\n  // code will go here\r\n\r\n  return app;\r\n} We’re now set to start the real work! A bit of methodology First I will start by choosing which set of features (that I call “epic”) I want my API to offer: product creation. In order to achieve this epic, I need to fulfil the following requirements: Success cases: API returns HTTP code 201 when it succeeds API returns the resource location in the header Location Error cases: API returns HTTP code 400 when product data is invalid API returns all the keys in error We want our product to be fully functionally tested. Why? Because writing specifications as automated tests is future-proof : it prevents code updates and refactoring to break any production behavior. This implies the translation of our specifications (listed above) into end-to-end tests. This practice is a derivative from ATDD where we start with product behavior specifications written as tests and only then implement these behaviors. This practice is also great to create discussions with product owners and business experts in order to tackle unexpected cases that might arise while discussing features. I say “derivative” because in this context there is no business expert. The first feature I like to start with error cases in order to get rid of the noise they create once we start thinking about the actual feature. Hence writing a test to check the HTTP code returned when product payload is invalid: describe(\"POST /products\", () => {\r\n  describe(\"When product data is invalid\", () => {\r\n    it(\"returns 400\", async () => {\r\n      const { statusCode } = await queryApi(\"POST\", \"/products\", { body: {} });\r\n      expect(statusCode).to.equal(400);\r\n    });\r\n  });\r\n}); The matching implementation: // API route to create a product\r\napp.post(\"/products\", async (req, res) => {\r\n  if (Object.keys(req.body).length === 0) {\r\n    return res.status(400).send();\r\n  }\r\n}); The second feature We now test that error keys can be found in the response body: describe(\"POST /products\", () => {\r\n  describe(\"When product data is invalid\", () => {\r\n    it(\"returns the keys in error\", async () => {\r\n      const { body, statusCode } = await queryApi(\"POST\", \"/products\", { body: {} });\r\n      const contextList = body.data.map(item => item.context.key);\r\n      expect(statusCode).to.equal(400);\r\n      expect(contextList).to.contain(\"name\", \"price\", \"weight\");\r\n    });\r\n  });\r\n}); The matching implementation: // Schemas and models\r\nconst productSchema = Joi.object().keys({\r\n  name: Joi.required(),\r\n  price: Joi.required(),\r\n  weight: Joi.required()\r\n});\r\n\r\n// …\r\n\r\n// API route to create a product\r\napp.post(\"/products\", async (req, res) => {\r\n  const { error } = Joi.validate(req.body, productSchema, { abortEarly: false });\r\n  if (error) {\r\n    const errorMessage = error.details.map(({ message, context }) => { message, context });\r\n    return res.status(400).send({ data: errorMessage });\r\n  }\r\n}); The third feature We can now check the success cases, starting with the server HTTP code response: describe(\"POST /products\", () => {\r\n  describe(\"When product data is valid\", () => {\r\n    it(\"returns 201\", async () => {\r\n      const product = { name: \"tshirt\", price: 20, weight: 0.1 };\r\n      const { statusCode } = await queryApi(\"POST\", \"/products\", {body: data});\r\n      expect(statusCode).to.equal(201);\r\n    });\r\n  });\r\n}); The matching implementation: // API route to create a product\r\napp.post(\"/products\", async (req, res) => {\r\n  const { error } = Joi.validate(req.body, productSchema, { abortEarly: false });\r\n\r\n  if (error) {\r\n    const errorMessage = error.details.map(({ message, context }) =>\r\n      Object.assign({ message, context })\r\n    );\r\n    return res.status(400).send({ data: errorMessage });\r\n  }\r\n\r\n  const product = await Product.create(req.body);\r\n  res.status(201).send();\r\n}); The fourth feature We will now test that the API returns the resource location: describe(\"POST /products\", () => {\r\n  describe(\"When product data is valid\", () => {\r\n    it(\"returns the product location\", async () => {\r\n      const data = { name: \"tshirt\", price: 20, weight: 0.1 };\r\n      const { headers } = await queryApi(\"POST\", \"/products\", {body: data});\r\n      expect(headers.location).to.match(/products\\/.+/);\r\n    });\r\n  });\r\n}); The matching implementation: // API route to create a product\r\napp.post(\"/products\", async (req, res) => {\r\n  const { error } = Joi.validate(req.body, productSchema, { abortEarly: false });\r\n  \r\n  if (error) {\r\n    const errorMessage = error.details.map(({ message, context }) =>\r\n      Object.assign({ message, context })\r\n    );\r\n    return res.status(400).send({ data: errorMessage });\r\n  }\r\n\r\n  const product = await Product.create(req.body);\r\n  res.set(\"Location\", `/products/${product.id}`);\r\n  res.status(201).send();\r\n}); A pattern emerges… Can you see where this practice leads us? We start adding blobs and blobs of code within the same anonymous function responsible of the product creation. And why would we do otherwise? It makes perfect sense from our functional test suite perspective. We have done what we were told. We could try to refactor but there is no incentive to go a different way we did and we have no idea which way to go. What’s cool? This code base has a few qualities which will help us refactor it: Relatively short: 200 lines Super low global complexity End to end tests Linter to maintain code style Variables are correctly named Magic strings and number are extracted What’s wrong? Where did we end up? We have added a few hundred lines of code with tests in total good faith. However, as the code grew, we kind of felt like something was going wrong, but what is it? Well, it mostly boils down to two major design flaws : Tight coupling Multiple responsibility See for yourself, the `index.js` file does almost everything: Server initialization Database connection Environment support Logging Route declaration HTTP deserialization Configuration logic Business logic Database bindings HTTP serialization What’s more, business and infrastructure logic depend on each other. For instance, in order to create an order, which is a real-life business use-case, we must extract information from the request body (infrastructure), then apply some format validation in order to work on consistent data (infrastructure), then calculate prices and discounts (business) and finally send the appropriate HTTP code (infrastructure). But why would we like our code to be different? What changes would that make? Dealing with the consequences Software development is the encounter of business and crafting skills. In order to achieve this marriage with the highest outcome possible: as many features in the shortest time possible, the current code is kind of a pain to deal with for many reasons we list below. Heavy cognitive load One has to keep many things in mind to comprehend how the code works at the very top level of our application. How does the API achieve an order creation? A product listing? This should be visible in one glance. As you can see, it is not. Also, the code lacks expressiveness. We do things but we never give these things a name. Instead, we have added comments that, while bringing meanings and explanations, also bring a lot of noise. These are all technical details that hide both developer and business intentions . Example: How does one know how to create an order? Well, looking at the code we can see this: // HTTP request payload validation\r\n// abortEarly is false in order to retrieve all the errors at once\r\nconst { error } = Joi.validate(req.body, orderSchema, {\r\n  abortEarly: false\r\n});\r\n\r\nif (error) { // Create the HTTP response error list\r\n  const errorMessage = error.details.map(({ message, context }) =>\r\n    Object.assign({ message, context })\r\n  );\r\n\r\n  return res.status(400).send({ data: errorMessage });\r\n}\r\n\r\n// Fetch the list of products based on the products provided in the order\r\nconst productList = await Product.findAll({\r\n  where: {\r\n    id: { [Op.in]: req.body.product_list.map(id => parseInt(id, 0)) }\r\n  }\r\n});\r\n\r\nif (productList.length === 0) {\r\n  return res.status(400).send({\r\n    data: [\r\n      { message: \"Unknown products\", context: { key: \"product_list\" } }\r\n    ]\r\n  });\r\n}\r\n\r\nconst productListData = productList.map(product => product.toJSON());\r\n\r\n// Compute the total weight order\r\nconst orderTotalWeight = productListData\r\n  .map(p => p.weight)\r\n  .reduce((prev, cur) => prev + cur, 0);\r\n\r\n// Compute the total price amount\r\nconst orderProductListPrice = productListData\r\n  .map(p => p.price)\r\n  .reduce((prev, cur) => prev + cur, 0);\r\n\r\n// Compute the shipment price amount\r\nconst SHIPMENT_PRICE_STEP = 25;\r\nconst SHIPMENT_WEIGHT_STEP = 10;\r\nconst orderShipmentPrice =\r\n  SHIPMENT_PRICE_STEP * Math.round(orderTotalWeight / SHIPMENT_WEIGHT_STEP);\r\n\r\n// Compute the discount\r\nlet totalAmount = orderProductListPrice + orderShipmentPrice;\r\nif (totalAmount > 1000) {\r\n  totalAmount = totalAmount * 0.95;\r\n}\r\n\r\nconst orderData = Object.assign(\r\n  {\r\n    total_amount: totalAmount,\r\n    shipment_amount: orderShipmentPrice,\r\n    total_weight: orderTotalWeight\r\n  },\r\n  { product_list: req.body.product_list }\r\n);\r\n\r\nconst order = await Order.create(orderData);\r\nres.set(\"Location\", `/orders/${order.id}`);\r\n\r\nres.status(201).send(); This anonymous function is 50 lines long and does many things. However, what creating an order really boils down to is (from both technical and business perspectives): Validate what comes in the API Send HTTP error when something goes wrong Retrieve the product list from the order Calculate the shipment price, total weight and amount Save the order in the database Return the appropriate HTTP code An appropriate code design should reflect these steps and hide the implementation details using higher levels of abstraction . By doing so, we are going to make tradeoffs, because that’s what design is: decrease the local complexity and increase the global one . Side-effects everywhere I started writing this application with functional tests first then implementing the requirements expressed in these tests. This is a great start in order to preserve API features across refactoring and code updates but it does not address the inner application design . As revealed by our current implementation, we designed the code with a single application layer , hitting all the infrastructure layers: HTTP and database. Doing a single (functional) test loop does not enforce any unit test driven development initiative. Such an approach would have helped us make the application design emerge off the unit test suite. No code reuse Our code base does not scale and is error-prone. The piece of code below contains the error mapping logic and is repeated twice (lines 85 and 126). Duplicating this code each time we want the appropriate error message means that modifications have to be made at several locations in our code, implying more occasions to forget things or make mistakes. const errorMessage = error.details.map(({ message, context }) =>\r\n  Object.assign({ message, context })\r\n); Functional tests take time to run Our functional test suite hits both network and database layers, this means four (de)serialization steps : HTTP to application Application to database Database to application Application to HTTP Plus the database response delay. If we consider that making a read operation on our database takes 50ms and a write operation takes 100ms, the test suite will take 22s to execute (as shown in the commit a877fc ), which is awfully long for a feedback loop. And that’s just on our 200 lines application. No way to test at the unit level What if the business comes to me and ask to change the discount logic? Currently, I have to update my functional tests which, as mentioned before, are already in charge of ensuring many steps of the application logic. Why should I bother with infrastructure details when all I want to ensure is the business logic my product owner asked me to implement? How do I get quick feedback on whether I broke an already existing rule? Well, at the moment, we literally can’t: our design does not allow it . Here is the code we would like to test: if (totalAmount > 1000) {\r\n  totalAmount = totalAmount * 0.95;\r\n} And this code is located between instructions doing completely different things! Above these are a few lines calculating the total amount and the shipment weight and prices, which are a completely unrelated set of business rules: const SHIPMENT_PRICE_STEP = 25;\r\nconst SHIPMENT_WEIGHT_STEP = 10;\r\n\r\nconst orderShipmentPrice =\r\n  SHIPMENT_PRICE_STEP * Math.round(orderTotalWeight / SHIPMENT_WEIGHT_STEP);\r\n\r\nlet totalAmount = orderProductListPrice + orderShipmentPrice; And below these lines is the creation of the client response body, which has more to do with infrastructure details than business logic: const orderData = Object.assign(\r\n  {\r\n    total_amount: totalAmount,\r\n    shipment_amount: orderShipmentPrice,\r\n    total_weight: orderTotalWeight\r\n  },\r\n  { product_list: req.body.product_list }\r\n); Generally speaking, we have no way to test specific parts of our application . This leads to inappropriate test practices made of longer feedback loop (because tests will take more time to execute) and a black box approach since our test practice does not allow us to drive our implementation and make it de facto modular. Misconceptions regarding software quality Linting is the level 0 of software quality Linting fills unanswered questions of the language, time consuming questions people love to debate: semicolons? Tabs or spaces? 80 columns? Etc. Don’t get me wrong though: linting is mandatory. It gets rid of all the unnecessary noise and helps developers focus on what really matters: developing product features while preserving software quality. Stick to a standard and never worry about it again. Functional testing has nothing to do with software design quality Whether it is unit, integration, functional or something else, testing is always a good practice. But different testing practices mean different costs and purposes. What functional testing ensures is non-regression and feature documentation. What it costs in terms of development time is up to your team experience and choices. What it brings to your software design is nothing. What’s next? OK, so where does this leave us? Well, we observed many flaws, most of them related to responsibility, readability, coupling and testing practices. In order to solve these issues, I am going to refactor the current code with two well-known approaches: TDD using unit tests Layered architecture What I am not saying though, is that these techniques will solve all of our problems. Rather, I bet it will offer an other perspective on how you can write, test and organize your Node.js code base. Hope you liked it and see you soon! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Software Craftsmanship and tagged automated tests , Best Practices , code quality , craftsmanship , development , JavaScript , nodejs , Testability . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 5 commentaires sur “Clean Node – Part 1” Raphael Boukara 09/03/2018 à 09:38 Hi, how to be notified on the second part?\r\nVery interesting! Thanks Piccosoft 06/04/2018 à 12:47 Your blog is very nice... Thanks for sharing your information... Pete 09/04/2018 à 17:19 Great start! Please hurry with 2nd and 3rd part! :) Tadeu 15/05/2018 à 22:43 Please, where is the second part? H 15/11/2018 à 12:45 Looking forward to next part Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2018-03-05"},
{"website": "Octo", "title": "\n                Kubernetes vs Swarm: Volumes!            ", "author": ["Sebastian Caceres", "Pierre-Yves Napoly"], "link": "https://blog.octo.com/en/kubernetes-vs-swarm-volumes/", "abstract": "Kubernetes vs Swarm: Volumes! Publication date 26/12/2017 by Sebastian Caceres , Pierre-Yves Napoly Tweet Share 0 +1 LinkedIn 0 Hey everybody! After reading both the Kubernetes and Docker “How does it work?” series, I guess you can’t wait to transform your old-school infrastructure and put all your applications inside containers. Indeed, containers are a great way to make your applications portable and easy to deploy. Nevertheless, there is a subject we have not discussed yet: data persistence. Before we start, I’d like to say that there are ways to handle data that are more cloud-oriented than volumes. These include managed relational database services, non-relational database services, and object storage services, all of which are easier to operate than volumes, since they harness most of the benefits of the cloud ecosystem. Volumes seem to match better with pets than with cattle and may be an obstacle to scalability unless you are using read only volumes (more on this later). In any case, be sure to regularly backup your data. A container is created from a fixed image, and all its changes are lost when re-instantiating the image into a new container. It is an ephemeral resource. How can we make sure that all the data in the container is persisted somewhere? You could snapshot the container into a new image, but there is a limit in the number of time you can do that. And what if your container dies? What if the machine that’s hosting the image dies? If you see yourself in one of the previous situations you should probably take a look at Volumes. What is a Volume? A volume can be defined in many ways. Kubernetes says that: “At its core, a volume is just a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used.” Whereas Docker says that: “Volumes are directories that are stored outside of the container’s filesystem and which hold reusable and shareable data that can persist even when containers are terminated. This data can be reused by the same service on redeployment, or shared with other services.” Even if Docker and Kubernetes use different words to define them, we can see that the two concepts are really similar and in both cases volumes serve the same purpose. Why would anyone use them? Basically, the two main reasons to use Volumes are data persistency and shared resources, or in other words, to be able to keep your data if your container dies, or to share data between multiple containers. These shared resources also include Secrets, which can be made available to containers/pods through Volumes. How are you supposed to do that though? The solution to this problem is not easy. Are Volumes binded to a single node? How are multiple containers located on different hosts supposed to use them then? Are they hosted on a single machine? Are they even hosted on a machine? Do you even Volume? The answer to this is basically different implementations for different needs. There are many types of Volumes (Types for Kubernetes, and Drivers for Docker), each one with its own advantages and drawbacks. Let’s take a look at some of them, shall we? Kubernetes Volumes Kubernetes came out with the notion of Volume as a resource first, then Docker followed. There are many Volume types. When I say many, I mean a lot . You’ve got local, node-hosted Volume types like emptyDir , hostPath, and local (duh). You also have Volume types that are hosted on Cloud IaaS platforms, such as gcePersistentDisk (GCE), awsElasticBlockStore (AWS), and AzureDiskVolume (Azure) . Some Volumes are even backed on traditional storage solutions, like nfs, iscsi, or fc (fibre channel). Others can be backed on modern, distributed file systems, like flocker, glusterfs and cephfs. There’s no way we can describe all of them in a single article. We just can’t. I’m sorry. We’ll do a couple of important ones though. The simplest kind of volume is the emptyDir Volume. These are created when a Pod is assigned to a node, and they exist for as long as the Pod keeps running on the node. If the Pod is removed from the node, the Volume is deleted. Then you have basic, but more complex kind of Volumes: the hostPath Volumes. These mount either a file or a directory from the node’s filesystem into the Pod itself. Yay, volumes! These are great and all, but they do not actually provide us with the persistence we’re looking for. The resources are either linked to the execution and lifetime of the Pod, or to the underlying host, which is precisely what we do not want. In order to have persistent Volumes with a different lifecycle than your pods, you will need to use a different volume type. We’re going to see how this works using the awsElasticBlockStore type based on the AWS EBS managed service. Basically, and as the name already hints, this type of Volume is backed against an AWS Elastic Block Storage volume. This is cool because the contents of the Volume will continue to exist, even if the Pod is removed: the EBS volume will just be unmounted. There are some requirements for these to work: all the cluster nodes must be EC2 instances, which need to be in the same availability zone as the EBS volume, and a volume might only be mounted by a single instance. Wanna see it? Hands to work! First, you will need a Kubernetes cluster running on AWS. There are many ways to achieve this. I’ll be using minikube for this one, but you can use kops if you wanna go for a big scale cluster. Then, you will need a working EBS volume, with and its ID. You can create that using the web console, or with the AWS CLI, like so: aws ec2 create-volume –availability-zone=eu-west-1b –size=5 –volume-type=gp2 Remember that the availability zone of the EBS volume must be the same as the one for the ECS instances hosting the cluster. Once you have that, you only need to create a Pod, and mount the required volume while specifying a mount point, like so: apiVersion: v1 kind: Pod metadata: name: mongo-ebs spec: containers: - image: mongo name: mongo-pod volumeMounts: - mountPath: /data/db name: mongo-volume volumes: - name: mongo-volume awsElasticBlockStore: volumeID: <Volume-ID> fsType: ext4 I’ll name this file mongo.yml. Then all you need to do is type the following command in order to create the pod: kubectl create -f mongo.yml This will create the pod with the associated AWS EBS volume mounted at the specified mount path. That was simple, right? Let’s see how we can achieve the same thing using Swarm. Swarm Volumes Swarm might not look as mature as Kubernetes: it only comes with one type of volume natively, which is a volume shared between the container and its Docker host. This might come in handy for testing a deployment locally, but it won’t do the job on a distributed cluster.  Between two deployments the container on which the volume is mounted might move from a cluster node to another. If this happens, it will lose the data that was on the precedent node and a new empty volume will be recreated on the new node. Ouch. One possible solution to palliate this problem is to associate placement constraints with the container so it always runs on the same node. You should never do this. It is a really nasty way to solve our problem and if your node crashes, well… you do the math. But yeah, you must have guessed it, there are many solutions other than local volume. Like for networks, Docker can use different drivers to handle its volumes even if it uses the local driver by default. There are two ways to install the drivers: Launch the driver as a system service, and configure the Docker host to use it. This is the way most drivers works for now. Install the driver as a Docker plugin, using the simple command: docker plugin install rexray/ebs Super simple right? Docker is gonna search for the plugin container on the Docker Hub and it’s going to download it right after. Sadly, very few plugins are available as of this moment.  This will most likely change in the near future. A great volume driver which can be installed as a Docker plugin is REX-Ray. It is compatible with multiple storage providers and it is agnostic of the provider it’s using, bringing better user experience to the balance. Let’s say we have a Swarm cluster and we need a Mongo database for an application running on the same Swarm cluster. We are going to launch a Mongo container that belongs to the same network as our application and map an EBS volume to it using REX-ray to ensure data persistency. To use EBS volumes, we need to provide credentials. This can be achieved in three different ways : by associating an AWS IAM role to the instance running Docker through a REX-ray configuration file through configuration variables while installing the plugin First we are going to install REX-Ray on our Swarm manager using a simple command: docker plugin install rexray/ebs EBS_ACCESSKEY=access_key EBS_SECRETKEY=secret_key Then we create a Docker volume using REX-ray: docker volume create –driver rexray/ebs –opt size=5 –opt volumetype=gp2 –name ebs_vol This is going to create a 5 gigabytes EBS volume of gp2 type gp2. All we have to do now is to launch a new Docker service using the MongoDB official image and map the volume to Mongo’s data directory: docker service create –network my_overlay_network –replicas 1 –mount type=volume,src=ebs_vol,target=/data/db –name mongodb mongo We now have a Mongo container accessible by all containers on my_overlay_network at mongodb:27017, which ensures data persistency if the service stops. Isn’t that great? From what I’ve seen so far, this is the best way to handle volumes with Swarm. Note that you’ll have to implement your own regular backup policy for the volume on the storage provider. But this will be outside of Docker Swarm scope. Wow, that was great! Yeah, it is indeed pretty cool. We got to see what is a volume, how to use it, and what should we use them for. We also know now how both Kubernetes and Swarm handle their persistent data. It is nice to see that both solutions are mature enough and capable of handling data using similar workflows. Even if the exact procedure used to mount and use volumes is not exactly the same for the two of them, you can see that the abstractions match in a certain way. Great minds think alike, don’t they? Anyway, TL;DR: Map volumes to you containers to ensure data persistency Use volumes on platform storage such as Amazon EBS or GCE Persistent Disk rather than local volumes Platform storage compatibility is native in Kubernetes and easily installable in Docker Be sure to backup your volume regularly If dealing with cattle, consider not using volume but replicated databases for better scalability That was fun, wasn’t it? See you on the next adventure! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Kubernetes vs Swarm: Volumes!” total_virtualization 28/07/2018 à 11:03 I have a two questions:\r\n1. Will performance will drop if using remote volumes like EBS and GCE Disk and how much?\r\n2. What does it mean \"dealing with cattle\"? Is it big database (warehouse) under high load? Wesley 18/05/2019 à 20:07 @total_virtualization\r\nI don't know for 1.\r\n\r\nBut \"dealing with cattle\" means treating all your servers as disposable not as something you look after. If something is wrong you just kill the node and you are back to where you started. \r\n\r\nYou can search for something like \"servers cattle not pets\" to find out more info.  As regards the EBS or GCE Disk performance I'd suggest that you look at the performance of the shared volumes of your individual hosting to get more details. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-12-26"},
{"website": "Octo", "title": "\n                The semicircle (episode 4 — Driver/Navigator)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-4-driver-navigator/", "abstract": "The semicircle (episode 4 — Driver/Navigator) Publication date 22/12/2017 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 You are late, but it is just getting started. You slip into your chair as quietly as you can. Hardly unnoticed, because the person who is standing near the projector calls you out, asks your name, and explains to you the purpose of tonight’s exercise. “We are doing a mob, using TDD, using the Bowling Score kata. That guy,” he points to the person who is sitting at the laptop, “he’s the Driver. The Driver writes the code, but he doesn’t think. You are the Navigators. You tell the Driver what he should code. We change Driver every five minutes.” “OK! Thank you. Sorry for being late.” There are 8 of you sitting in a semicircle facing the wall monitor. Two people have their laptop open on their laps. It seems that everyone knows each other. Quite young people, except for one guy who seems to be over fifty. Audrey had said she would come, but she must be late because of work. You smile. You are a little anxious to have left work right at six with all there’s to do there. Thank goodness this meetup is two blocks from the office. “We’re off!” “What are we going to do?” The driver is sitting at the keyboard looking at the others with a mocking attitude. Someone says: “Write a test.” “What test?” “Any one. Test that two plus two equals four. We want to see a test running.” “First make it fail. Write two plus three equals four.” The Driver obeys. Expected 4, but got 5. At the bottom of the screen, in the status section, the message bar turns red. “Now make it pass.” He makes the test pass. The bar is green. “And now?” The alarm sounds on the facilitator’s cell phone. Everyone moves one seat to the left, including you. You are three spots from the Driver’s seat, a bit worried, but you tell yourself that you still have time to get in the game. The person who just settled in at the keyboard is rubbing his hands and says: “So, I think we could start with the case of…” The facilitator interrupts him mimicking the sound of a TV Game show buzzer. “Oh you do? It’s for the navigators to decide.” Your neighbor to the left says calmly, “Let’s start with the trivial case where the player is only doing gutters. In that case, the score is zero.” While the driver is aligning dozens of zeros in a list and fumbles while finding the left square bracket on the keyboard, you rack your brains trying to recall the rules of bowling, a game you’ve only played three times in your life. That was ten years ago. The alarm sounds. Everyone shifts seats again. The new Driver finishes writing the test and makes it pass. “What’s next?” You go for it, suggesting, “Why not test what should happen if one of the throws is negative?” Someone at the other end of the semicircle answers. “It could happen, but should we do that scenario before the other ones?” Someone else adds “I don’t see how the machine that picks up the pins could communicate a number of pins smaller than zero.” You reply. “If it’s a faulty machine?” The discussion starts about the limitations of defensive programming. The facilitator raises his hand. “Hey!” Everyone stops. “The Driver has something to tell you!” The Driver smiles and says, “I’m bored.” Someone to your left suggests, “Line up twenty shots without strike or spare. For example two and six, ten times in a row. The score will be eighty.” The driver starts writing this test. Bell ringing, change of Driver. The following Driver passes the test. You recognize the pattern he is putting in place in the code. You make suggestions that are well received. Everyone seems to be perfectly in agreement with the principle of writing the test before the code that will pass this test, so you don’t propose another approach. Curiosity. It’s really less dark in this room than in the one you left behind your screen saver at work. It looks like all the obstacles are lifted in advance, made manifest, exposed in full light. Granted, the room is very small, even tiny, since the problem is particularly simple, and there are eight of us in this room. Six if we exclude the facilitator and the driver who don’t do the thinking. Someone says, “Can you write Gutter with two t’s?” “Ok.” “It doesn’t make a difference.” “We haven’t decided, but are we coding in English?” “We’re coding in Java!” “You understood me!” “OK for English.” “OK then correct English.” It’s your turn to be the Driver. You have no idea what should happen next. But that’s not the worst of it. You already have to get used to this keyboard and this environment, which are so different from what you have at work. Someone is proposing to deal with spares. You follow the group’s instructions. You don’t see exactly where this kata is going, but you’re doing pretty well. As soon as you look for a shortcut in the IDE, someone tells you what it is. That’s cool. On several occasions, the requests of the participants are so conflicting that you don’t understand anything the group wants, so the facilitator intervenes. We deal with the case of strikes. The design that the group chooses is surprising. You wouldn’t have coded the kata this way in the first place, but you think the approach is pretty consistent. And anyway, the last time you wrote a recursive function was several years ago. What is surprising about this room isn’t so much its now perfect clarity but the total absence of heterogeneous objects on the floor and around the pillars. Everything is in its place. This gives an impression of calm, like a positive vibe. You sense that new things are possible, unpleasant surprises are not. We can see the obstacles coming from a distance. The most fascinating thing is that it seems that the room has a perfectly adjustable architecture. Not long ago, the room was just a narrow corridor and now it’s much larger. Someone in the group pointed out a pattern, which everyone was guessing at but no one had found a way to express. “This bowling game is like a state machine, with some complicated states, but only two possible transitions.” And at that moment, the room changed shape, it became square, with three pillars in the middle and a triangular table. Whenever the group embroils itself in discussing what to do next, the facilitator steps in and suggests trying to make one or the other of the various hypotheses work in a time box rather than having endless discussions. The code is so well tested that it seems impossible to introduce an error without it being immediately apparent. You are looking for places where it could fail. On two occasions, you have a test written that should fail and show that there is a bug, but the test passes. We come to final edge case. There is a discussion about what the maximum score can be in Bowling. One of the participants opens his laptop and does a search. Someone says, “It’s complicated. Up until now, we didn’t need to count frames. Now we do.” The room just got a new addition: a whole new floor. It’s a bit of a shock. But we can still see very clearly. And the group seems perfectly confident that the structure can continue to improve. You realize now that the mobility of the elements in this room, this flawless flexibility, is mainly due to the fact that the room is permanently lit. The facilitator announces that the kata is over. They wrap up. The facilitator gives each person a chance to give feedback. “Not bad. New approach. I prefer it to the one I used before to solve this kata.” “We didn’t handle the case of input errors, finally!” “But it doesn’t matter…” “I appreciated the moment when everything came together after we discovered that the frame numbers must be taken into account …” “Yes. There are two steps where the design is twisted.” “We could do better …” “Do we need all these tests really?” “You’re asking if we need it ?!” “No refactoring without tests.” “Without tests, we can’t really reason about the code!” You ask to speak again. “Excuse me, I am not very familiar with TDD, but it seems to me that I reason all day about the code that I have in front of me. And it has no tests. At least no tests like the ones you write. Have you ever tried this approach, but without doing any tests?” The facilitator smiles. “We tried once or twice. It went in circles.” Someone suggests, “Note that mob debugging could be interesting too.” “Leave me out,” another person answers. “There wouldn’t be enough action.” You find yourself with the group in the street, looking for a place to have a drink. You decide to follow the group. You still have questions to ask the facilitator. (to be continued) Previous episodes : 1 — If the code could speak 2 — See/Advance 3 — Communication Breakdown Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-12-22"},
{"website": "Octo", "title": "\n                The semicircle (episode 3 — Communication Breakdown)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-3-communication-breakdown/", "abstract": "The semicircle (episode 3 — Communication Breakdown) Publication date 15/12/2017 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 The conclusion seems inescapable that at least with certain kinds of large programs, the continued adaption, modification, and correction of errors in them, is essentially dependent on a certain kind of knowledge possessed by a group of programmers who are closely and continuously connected with them. Peter Naur – Programming as Theory Building You look at that bit of code, the one where you have to add some new functionality and hear yourself saying out loud: “I would never have written it this way… How can one create such a convoluted design?” Jeremy, who wrote this code, is ten steps away. He’s currently working on some other part of the program, his headphones on. You could get up, interrupt him, ask him some questions, and give him some feedback about this design. But you tell yourself “What’s the use?” This will be at best yet another disagreement, highlighting again the chasm between you and Jeremy on topics of knowledge, preferences, and experience with software design. You remember that ten days ago he came to see you, announced he had decided to entirely rewrite the front end/back end communication module. “Oh really!?” “I don’t understand anything that has been done on in this module.” “And you want to rewrite it so you can understand it better?” “Exactly.” “…” “It will be simpler.” “You’ll do it at your own risk and peril.” “It’s ok with the PO. I wrote a technical story.” “OK by me, then. I am not in charge of coordinating this project.” You are thinking about this conversation again. You could have been more adamant. For example, mentioning to him that you’ve been here for ten months, but that it took six before being able to write one enhancement without having to ask four different people for help. You wonder if there’s a chance Jeremy will last another year on this project. The room you are walking in is filled with heaps and heaps of junk. Your entire job is to attempt to describe the obstacles in your way. It is hardly a reliable description since you can only see with your hands, knees, and occasionally your head. Correct descriptions require deep thought. And calm. How can one stay calm faced with a constant barrage of unidentifiable objects? But you think that by describing them you are saving time for your teammates since you are preparing a pathway for them. In a way. “I noticed that when the user clicks on the Admin menu to set authorizations, the service template isn’t the same as when I enter the authorization on the fly during a transaction.” “Do you mean the 2L transaction?” “Not the 2L, the 2LB.” “You don’t say. Welcome to the sh… Good luck!” “OK.” You could open your notebook to any page and label it: unexpected discovery number… You are walking around in a dark room, which is different from the dark room that Jeremy is currently exploring and nothing like any of the rooms that your colleagues are in. The objects are different not only in shape — they are inherently different. What’s the likelihood that we could produce an integrated, robust and coherent application given these conditions? Each exploring their own assigned space (are you even the same building?), with only a few tools to survive in the dark and a bandwidth so narrow that it is laughable. Jeremy doesn’t understand this whole thing about bandwidth. He says we’ve never had such powerful connectivity. “I am talking about the bandwidth between us, not the fiber network. It’s a metaphor.” “I don’t get your metaphors. Be clearer.” “See ?” Maria, the project’s Product Owner/Manager/Coordinator, quite agrees with that observation but says you have to just live with it. What counts is what the client sees. You ask yourself how the client could possibly see in this system a coherent set of items designed to work together. You only see disparate objects, an accumulation of scattered, crystallized particles, some fossilized, where successive technological waves have caused layering. You wonder what the clients would say about the amount of time each ticket takes after seeing a slice of the archeological history. Here’s where the budget goes; just look what it costs. Maybe they would tell us: Why is this meadow so barren when it has such rich soil? “I am not the one in charge of coordinating this project.” Blah blah blah. What a lame answer! Of course Maria will manage all the functional and technical requirements of the project, ensuring that everything lines up internally and externally, and she’ll manage to keep the schedule, and save money, and keep the team in a good mood, and grow the team. It’s guaranteed. It’s ten thirty. You go by to see if Audrey and Farid will take a coffee break with you. “It’s not my day, you say.” “What’s going on?” asks Audrey. “Communication breakdown.” Farid smiles and says, “Oh. When it’s like that, I just hunker down in my chair for the whole day and focus really hard on some very technical details.” “Speaking about communication,” says Audrey, “last Thursday I went to the programming Dojo.” “How was it?” “Not bad. One of the participants recommended trying out Mob Programming.” “Don’t know it.” “It’s like a regular Dojo, with the shared screen and all, with one small difference.” “What?” “The person at the keyboard waits for the participants to tell him what to code.” Jeremy, who just joined the conversation and is emptying an old teabag into the trash says, “Yuck. I would be waiting a long time.” Farid chuckles. “No joke, says Audrey, there’s even a principle in there for you who loves principles.” You say, “That’s intriguing. Tell me more.” “It’s the principle Driver/Navigator. I don’t remember the exact phrasing. But I wrote it down.” You return to your desks. Audrey opens her notebook which seems as full of notes as your own: lists, big exclamation marks, and WTFs. Here it is: For an idea to go from someone’s head into the computer, it must go through someone else’s hands. – Llewellyn Falco That’s what’s called the Driver/Navigator model. The driver is the one at the keyboard, the navigators are those who think, decide and design. Everyone is working on the same story, in front of the screen. Hmmmmm. Driver/Navigator Driver/Navigator … You get back to your piece of code. Obviously, that would change life in the dark labyrinths. You imagine the team in action. “Not there; there’s a whole class copied-pasted from the TransactionBL class. I saw that last week.” “Ok, get around it by renaming the class and only keep what’s needed.” “Who knows what’s needed?” “I know! Go to the Delivery module. I will show you.” “TransactionBL? Really? It’s about time we all agree on the naming conventions.” You make a mental note of all the possible pain points of working together in that way. Maria would immediately think of her budget issues and would come say it’s all fine to communicate about the code and all, but let’s not fantasize. Because Maria’s the one who labels our retrospectives “a huge waste of time”. Speaking of the devil, here’s Maria now entering the team room. She is heading straight towards you. “How is it going?” “So so. I don’t think I’ll be able to finish this ticket by Friday.” “That doesn’t work for me. I need to show something functional.” “I guess so…” “Do you think we should add another resource to the project?” “I wouldn’t do that if I were you… Adding resources to a late project…” “… makes it even later. I know. Brooks. What’s the issue with this incident? What would it take for this ticket to go a whole lot quicker?” “Hmmm. Revisiting the whole product design and starting from a more solid foundation, with a team standard?” “When you’ve landed back here on earth, answer my question please, says Maria smiling.” “OK, I pass. I have systemic issues.” “What does that mean?” “Like when you go to a global climate change conference and you drive your car. Get it?” “Not at all.” “In the short term, I could resolve this issue, but it would create more problems.” “Fix the issue. For the other stuff, we’ll see later.” Back to the code. Swimming up to your ears in code. You ponder what you could possibly do. You can’t rewrite the whole system from scratch, deciding on and applying principles that the whole team agrees on. That would take weeks and weeks of laborious, stormy, and unfruitful meetings. Stuck. Each in his own dark maze, bumping into stuff. The teamwork that would make this product great is impossible. You write this principle in your notebook: For an idea to go from someone’s head into the computer, it must go through someone else’s hands. You take note of two interesting impacts of working this way: You wouldn’t have any idea implemented without first having communicated it. You wouldn’t waste time discussing what won’t be implemented. You decide that you will go to the programming Dojo next Thursday. You go back to your screen. You plug in your earbuds and play Led Zeppelin, super loud. (to be continued) previous episodes: If the code could speak See/Advance Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-12-15"},
{"website": "Octo", "title": "\n                WebAssembly, an executable format for the web            ", "author": ["Arnaud Bétrémieux"], "link": "https://blog.octo.com/en/webassembly-an-executable-format-for-the-web/", "abstract": "WebAssembly, an executable format for the web Publication date 11/12/2017 by Arnaud Bétrémieux Tweet Share 0 +1 LinkedIn 0 With the latest Web browser updates, there is now a standard for high-performance code execution via the Web. WebAssembly is indeed now available on Edge, Safari, Chrome and Firefox. It promises a standard execution environment on all machines, regardless of hardware or OS. Some see in it the future platform for the deployment of universal applications — for mobiles, but not only. A platform that would enable performance very close to that of native applications. Others worry about the upcoming fragmentation of effort and methods for front-end development or lament the further erosion of the original values of the Web. WebAssembly is the result of an important coordination effort between the major web browser editors. It is a binary format designed to allow what could until now be done only via JavaScript, at least in the most common browsers: distribution of code via the web for execution in the browser. WebAssembly is designed to have better performance than JavaScript: the format is more compact than even compressed JavaScript, therefore takes less time to download and parse data is strongly typed, which facilitates automated code optimization compilation and optimization once on the target machine are faster because the code, having been pre-compiled before transfer is closer to machine language, and has been pre-optimized GWT, CoffeScript, TypeScript, Parenscript and the like, “languages” compiled to JavaScript, had already made of Javascript a kind of machine language for the Web. Mozilla had developed asm.js, a faster subset of JavaScript, destined to be the target of compilers like LLVM. Minification tools, in turn, mutilated JavaScript code to make it smaller, with the aim of saving download and parsing time. Google, with their NaCL project, allowed executing native code in a controlled environment within Chrome. WebAssembly is in a way the concerted fusion, the logical evolution of all this. Its execution performance should enable new applications, around everything that requires high computation power on the client side, like audio or video processing, 3D or encryption. For now, what the major browsers have implemented is what had been defined as the “MVP” for WebAssembly. This MVP provides: the binary encoding and an associated text representation the base semantics for the language a packaging and namespace system, in the form of so-called “modules” the Javascript interface , for now the only way to run WebAssembly. Functionality in this interface is accessible via global JavaScript Object “WebAssembly”. For the moment, the tools can only compile C, C++ or Rust. Theoretically, any language can now be compiled for the browser, the easier if it is compilable with LLVM, which can generate WebAssembly. That is unless the language relies on Garbage Collection, since this aspect, while planned, is not available in the MVP. Among other aspects in development are exceptions, threads, and direct DOM manipulation (which is currently only possible through JavaScript). The easiest way to play a bit with WebAssembly is to use WasmFiddle . It lets you write C code on one side, JS code on the other, and compile the C code to WebAssembly for running the whole thing, without leaving your browser. Writing a “Hello World” program with WebAssembly can feel a bit strange: WebAssembly does not currently support interacting directly with the DOM or with the JavaScript console. To write something, we have to go through the interface with JavaScript. Another hurdle is that WebAssembly has only 4 types: floating point and integer, 32 and 64 bits. There is no string type or array, which means that the “Hello World” text will need to go from WebAssembly to JavaScript in the form of a pointer, which will indicate an area in WebAssembly memory where JavaScript will read integers coresponding to character codes. A char * in our C program becomes an i32 buffer in WebAssembly. We send JavaScript a pointer to this buffer, which it will read to make a string. Simple ! Let’s start by writing our “Hello World” in C: char* hello() {\r\n  return \"Hello World !\";\r\n} We get the following WASM code, in which we can see the definition of our function, its “export” and the export of a memory propery that WasmFiddle automatically includes: (module\r\n  (table 0 anyfunc)\r\n  (memory $0 1)\r\n  (data (i32.const 16) \"Hello World !\\00\")\r\n  (export \"memory\" (memory $0))\r\n  (export \"hello\" (func $hello))\r\n  (func $hello (result i32)\r\n    (i32.const 16)\r\n  )\r\n) The Javascript code, which relies on WasmFiddle putting our compiled WebAssembly code in a “wasmCode” JavaScript array: function makeStringFromASCIICodes(memory, pointer) {\r\n  let s = \"\";\r\n  for (i = pointer; memory[i]!==0; i++) {\r\n    s += String.fromCharCode(memory[i]);\r\n  }\r\n  return s;\r\n}\r\n\r\nlet wasmModule = new WebAssembly.Instance(new WebAssembly.Module(wasmCode));\r\nlet memory = new Uint8Array(wasmModule.exports.memory.buffer);\r\nlet pointer = wasmModule.exports.hello();\r\nalert(makeStringFromASCIICodes(memory, pointer)); I have made the whole thing available as a WasmFiddle here: https://wasdk.github.io/WasmFiddle/?1aax07 What if we want to compile the same thing on our own machine ? The WebAssembly toolkit by DcodeIO is a good way to start. I found it much easier than direct use of the emscripten compiler as shown in the official tutorials. I write a hello.c file, containing the same code as earlier, but with an additional “include”, and an “export” keyword in front of the function definition. Both are conventions of dcodeIO’s WebAssembly tool: #include <webassembly.h>\r\n\r\nexport char* hello() {\r\n  return \"Hello World !\";\r\n} To compile the file: wa-compile -o hello.wasm hello.c I can then look at the corresponding assembly with wa-disassemble hello.wasm (module\r\n (type $0 (func (result i32)))\r\n (import \"env\" \"memory\" (memory $0 1))\r\n (table 0 anyfunc)\r\n (data (i32.const 4) \"0\\'\")\r\n (data (i32.const 16) \"Hello World !\")\r\n (export \"hello\" (func $0))\r\n (func $0 (type $0) (result i32)\r\n  (i32.const 16)\r\n )\r\n) To make things work in a browser, I can use the same JavaScript as before, but I need to reproduce what WasmFiddle did for me: include the compiled assembly code in a JavaScript array, and add memory initialization. Using the JS library bundled in the DcodeIO toolkit would have been easier, as it initializes memory and provides a few utilities, but doing it by hand is a good way to understand what is involved. To get the WebAssembly code in the form of a JavaScript array, I combined xxd and sed: xxd -c 10000 -p hello.wasm  | sed -e 's/\\w\\w/0x\\0, /g' <html>\r\n  <script type=\"text/javascript\">\r\n    var wasmCode = new Uint8Array([0x00, 0x61, 0x73, 0x6d, 0x01, 0x00, 0x00, 0x00, 0x01, 0x85, 0x80, 0x80, 0x80, 0x00, 0x01, 0x60, 0x00, 0x01, 0x7f, 0x02, 0x8f, 0x80, 0x80, 0x80, 0x00, 0x01, 0x03, 0x65, 0x6e, 0x76, 0x06, 0x6d, 0x65, 0x6d, 0x6f, 0x72, 0x79, 0x02, 0x00, 0x01, 0x03, 0x82, 0x80, 0x80, 0x80, 0x00, 0x01, 0x00, 0x04, 0x84, 0x80, 0x80, 0x80, 0x00, 0x01, 0x70, 0x00, 0x00, 0x07, 0x89, 0x80, 0x80, 0x80, 0x00, 0x01, 0x05, 0x68, 0x65, 0x6c, 0x6c, 0x6f, 0x00, 0x00, 0x09, 0x81, 0x80, 0x80, 0x80, 0x00, 0x00, 0x0a, 0x8a, 0x80, 0x80, 0x80, 0x00, 0x01, 0x84, 0x80, 0x80, 0x80, 0x00, 0x00, 0x41, 0x10, 0x0b, 0x0b, 0xa6, 0x80, 0x80, 0x80, 0x00, 0x03, 0x00, 0x41, 0x04, 0x0b, 0x04, 0x30, 0x27, 0x00, 0x00, 0x00, 0x41, 0x0c, 0x0b, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x41, 0x10, 0x0b, 0x0e, 0x48, 0x65, 0x6c, 0x6c, 0x6f, 0x20, 0x57, 0x6f, 0x72, 0x6c, 0x64, 0x20, 0x21, 0x00]);\r\n\r\n    function makeStringFromASCIICodes(memory, pointer) {\r\n      let s = \"\";\r\n      for (i = pointer; memory[i]!==0; i++) {\r\n        s += String.fromCharCode(memory[i]);\r\n      }\r\n      return s;\r\n    }\r\n\r\n    let wasmMemory = new WebAssembly.Memory({ initial: 1 });\r\n    let wasmModule = new WebAssembly.Instance(new WebAssembly.Module(wasmCode), {env: {memory: wasmMemory}});\r\n    let memory = new Uint8Array(wasmMemory.buffer);\r\n    let pointer = wasmModule.exports.hello();\r\n    alert(makeStringFromASCIICodes(memory, pointer));\r\n  </script>\r\n</html> Job done ! It was a bit more complicated that I imagined, but we got there. WebAssembly certainly has a lot of potential. But the promise of a universal execution environment for Web clients was not far from the main “raison d’être” for Java at its beginnings. I hope that WebAssembly does not herald the return of applets finally become “cool”, in one of the repetitions of history that are often seen in IT. Being the logical evolution of minification and use of Javascript or asm.js as compilation targets, WebAssembly unfortunately participates in the loss of openness on the Web: we are farther and farther from the Web of free information sharing, the Web where you could know how any site was made by looking at the source code directly in your browser, the Web where the eternal CRUD application was not a “fat client” in my browser. WebAssembly risks being used for things where one can wonder what gain it brings compared to native code, apart from control over code distribution. At a time when DRM has also reached universal support in browsers, I fear WebAssembly could be one of the blocks that allows trapping users more and more in an environment they have no control over. One of the blocks that enables further acceleration of the “cloudification” of software and data, that is to say of service as a software substitute, robbing users of the possibility of knowing what is done with their data, of verifying what code runs, of modifying that code, and of knowing that features available today will still be available tomorrow. On the other hand, if WebAssembly, in the same spirit as what HTML5 has started, can help replace some applications with “augmented” websites, I’ll be very happy. I hope we can collectively find the right balance, but I expect a hard battle. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-12-11"},
{"website": "Octo", "title": "\n                The semicircle  (episode 2 — See/Advance)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-2-see-advance/", "abstract": "The semicircle  (episode 2 — See/Advance) Publication date 08/12/2017 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 Continuation of prior episode: And yet… Could this possibly be the moment to at least pause and reflect? What if it was time for us to start changing the way we work? Let’s take this bug. In the blink of an eye, you found both the cause and the solution. You opened up the code, made the requisite change and compiled the build. In ten minutes, the application can be released for acceptance testing. But you’ll have to provide the customer with test-case data. And you’ll have to wait until he runs and greenlights the tests before releasing to production. You rack your brain. You project answers to the questions running through your head: “What if it doesn’t work?” “I’m sure it works.” “And what if it doesn’t work?” “It wouldn’t be our fault.” “If it doesn’t work, how long will it take to find out?” “Anywhere from several hours to several days.” You think some more. “Two possibilities: either you have fully fixed this program, or it has one or more new defects.” “If it’s fixed then we’re doing all this testing ritual for nothing.” “If it isn’t, then of course it’s just as well that we’re waiting.” You’re groping around in the darkness. There’s uncertainty. But one thing is certain: the room is huge – there’s basically an echo. A huge room cluttered with hundreds of indefinite objects, a crowded mess in which you can’t see a thing. You can only move hesitantly forward, running into whatever obstacles you encounter. This game you have loved playing since you were 15 (that is now running you ragged at thirty) has one rule all players recognize: ‘To see’ = to bump into an obstacle What would you rather do in this situation? Would you slow down and hold your hands out to find the obstacles, or would you confidently cross the room? Others might get down right away on all fours to avoid falling. Amateurs. Getting on all fours is like admitting defeat in advance. You walk upright. Blindly, but upright. You pretend to be able to get out of the room in one pass. You swear: It was the last bug!  You say: “There’s only this one very small problem to fix and we’ll be good.” But you’ve never played this game without running into some sort of obstacle. Every time you fall, and every time you quickly jump back up, dust yourself off, and rearrange the stuff around you figuring that it was likely in the shape of a block, a stack or at the very least a heap. There. That should do it. You would love to be able to go home now. You say to yourself “Tomorrow will be brighter.” But tomorrow, when you return to the room, it will still be just as dark. And maybe you’ll run into things again, odd-shaped things you don’t recognize. Logic bombs. As you fix one bug, you sow new ones. What could you do that would make a difference? You certainly aren’t able to turn on the lights. If you could, that would obviously fix most everything. If only someone would switch on the lights. You can almost hear the crowd sigh with relief: Ahhhhhh! You look out into the room: The first obstacle you were systematically hitting hadn’t been put away correctly. Once set in its place by the pillar that obstacle wouldn’t bother anyone, in fact. You now clearly see some of the obstacles that you had thought were inevitable, insurmountable. You realize you can clear them away in no time. The obstacle you had thought to be your last hurdle in fact scattered about the room in so many pieces you can hardly count them all. Your “last obstacle” was actually dozens of tripwires. And in this bright light, with this new clarity, you are on the verge of figuring out the elusive second rule, that very few players know: ‘Advancing’ = creating new obstacles But you can’t articulate anything around this second rule because you have neither the means (it would mean knowing how) nor the time (it would take months) to switch on  the lights again on this inscrutable project. Back to reality. What to do? Take a break. You go the corner kitchenette next to the foosball table make yourself some ramen soup. You wander back to your desk to slurp noodles and stare at the purple lines floating around the clock on your screen – 8:45 pm. You down some mango juice and pick up a fortune cookie. You open the wrapper and read your fortune: One learns from one’s mistakes… You will learn a lot today. Very funny. It makes you think of Jeremy’s joke yesterday. Jeremy, the skeptic who always makes fun of anything related to methodology, had opened his fortune cookie and read in a learned voice: “Every line of code is justified by a test that didn’t pass” which threw everyone into fits of laughter, even those who had never heard it before. Every line of code justified by a test that didn’t pass. What will they come up with next!? You log in to your system. Why not? You decide right now, as if it were a principle of survival in a strange dark environment, that obstacles you encounter once won’t get in your way again. You circle back, open the bugged version of the code. You create a test module. You write a test that fails. It fails because this bug is there. You take precious time writing the test. The code is not easily suitable to tests of this sort. That’s because the test targets a section of the code that should be executed independently of the rest. It seems ridiculous given this section is so tiny. Two lines. The code is tightly coupled with its external dependencies. The effort expended in creating this test is disproportional to the few lines of code that it will end up testing. All of this for just that. A mountain out of a mole hill. Frustration. Your vibrating phone interrupts your thought process. It’s Stephanie. “I heard your message. I ate. Are you coming home soon?” “Yes, soon. I’m exhausted. How are you?” “I tried to repair my bike. It’s the gear.” “Oh?” “It’s a mess. I tried to adjust it but I can’t move the gear without pedaling and when I’m sitting in the seat, I can’t see the gear up close. Plus it’s dark, and the light in the courtyard is too dim.” “Ahhh.” “So I turned it upside down onto the seat to pedal with my hands. But the chain doesn’t move correctly that way because the bike is upside down.” “…” “And you – how are you?” “Same.” “What time will you be home?” “I’ll head out in ten minutes.” You say to yourself: “What’s the use –  it’s impossible!” But think. In the time it took you to decouple this bit of code and write this test, were you really just going to wait around for the customer, doing nothing other than inserting more logic bombs in the code? You move forward. You break the dependencies. It’s not pretty. The code seems even more overelaborate than it did before this test. You succeed as best you can to isolate the culprit code. It isn’t spectacular, but at least your test runs without any calls to the database. It’s red. You fix the bug. The test turns green. Maybe you have just enough time to write another test. This part of the code is (in the dark) totally deprived of tests. For this part of the code, you don’t even know what the test would have to assert. You run the test, and the result tells you what the code is actually doing. You paste the result in the test, even though you have no idea whether this represents the truth. You tell yourself that at least you’ve figured out what the code is doing, and that you could come to rely on this test to ask the program manager more questions. That’s more reliable than throwing random hypotheses around in the dark, hoping not to bungle it up. You decide to stick with this principle for a while, writing a test for each new piece of functionality that you want to create or modify. It’s as if each time you poke around the dark room, you set down a little lamp to mark your spot. The light it casts may not be very bright, but it will shine on until the end of the project. You lock your computer and go home. (to be continued) Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-12-08"},
{"website": "Octo", "title": "\n                Vue.js unit test cases with vue-test-utils and Jest            ", "author": ["Clotilde Desquilbet", "Roman Quelen"], "link": "https://blog.octo.com/en/vue-js-unit-test-cases-with-vue-test-utils-and-jest/", "abstract": "Vue.js unit test cases with vue-test-utils and Jest Publication date 04/12/2017 by Clotilde Desquilbet , Roman Quelen Tweet Share 0 +1 LinkedIn 0 This article describes test contexts, from common to more complex ones, on a Vue.js stack. You may read TDD with Vue.js article as an introduction, which is mainly focused on methodology ( Test Driven Development ). This current article describes a wider range of test contexts based on Vue.js, using vue-test-utils helpers and the Jest testing framework. 1/ Jest Jest is a testing framework developed and used by Facebook to test JavaScript code. Its main goal is to simplify test configuration. It integrates most of the tools developers would need to test an application (test runner, assertions, mocks, DOM traversing,…) while having a better developer experience than usual. Still, Jest is not perfect, and we could summarise its benefits and drawbacks. Benefits: All in one: easy to configure (default configuration with jsdom, babel,…) Snapshot testing (cf testing strategy in part 3) Good developer experience (CLI) Parallel execution for tests Drawbacks: All in one: might do too much and give developers less choices Might have below average performances Can’t mock return value depending on parameter Jest is often associated with React because of its origin and because it works directly out of the box with it. However, we can still take advantage of its features for other libraries/frameworks, like Vue in this example. Since few days, Jest does not need much configuration as it is now included in the Vue webpack boilerplate . NB: We use a specific configuration file in our sample project, because this embedded configuration was not available at the time. 2/ Vue-test-utils Vue-test-utils is a Vue official tool that contains helpers to test Vue components . It is the equivalent of Enzyme from the React world. As such, it provides similar API with a few minor differences. It’s mainly used to instantiate a component while stubbing subcomponents (shallow), finding elements in template, trigger events and easily mock plugins. In the following examples, we will further detail the main helpers that we use in unit testing. In case you lack some helpers from vue-test-utils, you can still access to the Vue instance through the wrapper: “wrapper.vm” (see examples of event emission and async tests). 3/ Unit testing strategy A unit test is the smallest scale of test in development. It’s an automated test focused on a precise code portion by abstracting its dependencies. Its size allows for a quick feedback loop that eases TDD. With this definition, we can see that unit testing approach is subjective , but two criteria seem to be predominant: unit tests should be fast enough for TDD and isolated . Nowadays, all modern JS front-end frameworks or libraries (Vue, React, Ember, Angular, …) are based on components . That’s why we could say that the most atomic unit is a component, without their dependencies. Moreover, Vue components don’t need an actual browser to be rendered, so we can make fast assertions on their templates thanks to jsdom (used in Jest). With this in mind, we elaborated a unit test strategy based on our technical stack: Our unit test scale will be a component, with its template We will mock external dependencies (like axios ) thanks to Jest We will mock subcomponents thanks to vue-test-utils “shallow” We will test static elements of the template (class names, html structure, …) with Jest’s snapshots When possible, we will interact with the component as a User would by triggering events like click, thanks to vue-test-utils “trigger” When possible, we will make assertions on the template , thanks to vue-test-utils “find”. This unit test strategy allows us to be more focused on the component’s functional behaviour than its technical implementation. 4/ Examples In the following examples, “my-code-file.js” holds implementation code and “my-code-file.spec.js” holds test code. You can find all these different examples in the following repository . 4.1/ Common test cases 4.1.1/ Data initialization of a Vue component Here, we test the “message” data of a “Hello” Vue instance. By default, we want “msg” to contain “Welcome”. In the test, we use the “ shallow ” function of vue-test-utils. “ Shallow ” renders the component and stubs the subcomponents if any. It returns a Wrapper instance, which contains the mounted component and helpers from vue-test-utils. Hello.spec.js it('should contain default message', () => {\r\n // when\r\n const wrapper = shallow(Hello);\r\n\r\n // then\r\n const title = wrapper.find('h1');\r\n expect(title.text()).toContain('Welcome');\r\n}); Hello.vue <template>\r\n <div class=\"hello\">\r\n   <h1>{{ message }} {{ name }}</h1>\r\n </div>\r\n</template>\r\n\r\n<script>\r\n export default {\r\n   name: 'hello',\r\n   props: {\r\n     name: String\r\n   },\r\n   data () {\r\n     return {\r\n       message: 'Welcome'\r\n     }\r\n   }\r\n }\r\n</script> 4.1.2/ Data update of a Vue component We want to ensure that our template uses the “data” values of the Hello component, and not hard-coded values. Here, we test the “message” data of a “Hello” Vue instance after an update . We use “shallow” as in test of 4.1.1. “ setData ” method allows updating the data object. Hello.spec.js it('should update h1 title when message data is changed', () => {\r\n // given\r\n const wrapper = shallow(Hello);\r\n\r\n // when\r\n wrapper.setData({message: 'world'});\r\n\r\n // then\r\n const title = wrapper.find('h1');\r\n expect(title.text()).toContain('world');\r\n}); Hello.vue Same implementation as in test of 4.1.1. 4.2/ Static properties and snapshots Snapshot is a tool provided by Jest. We use it to detect UI changes of static elements. At first, it renders an HTML structure with the actual Vue template. Then, each time the test is executed, it will render the Vue template again. Finally, it will compare the two last rendered HTMLs, and expect them to be identical. This tool is used for regression testing , not in TDD. Hello.spec.js it('should match snapshot', () => {\r\n // when\r\n const wrapper = shallow(Hello);\r\n\r\n // then\r\n expect(wrapper.element).toBeDefined();\r\n expect(wrapper.element).toMatchSnapshot();\r\n}); Hello.vue Same implementation as in test of 4.1.1. Snapshot: Hello.spec.js.snap // Jest Snapshot v1, https://goo.gl/fbAQLP\r\n\r\nexports[`Hello.vue should match snapshot 1`] = `\r\n<div\r\n class=\"hello\"\r\n>\r\n <h1>\r\n   Welcome\r\n </h1>\r\n</div>\r\n`; Example of a failed snapshot test Here is an example of a failed snapshot, caused by the change of a CSS class in the template. 4.3/ Native events The following example shows how to test a native event . We render the Vue instance using “shallow” and we retrieve the event emitter using “find”. Here, we want to retrieve from the DOM the button we want to simulate a click on. Then, we fake the event emission by calling the “ trigger ” method on the event emitter. This method comes from vue-test-utils and takes the name of the event as an argument. You can take a look at the trigger documentation. ButtonCounter.spec.js it('should increment counter on click', () => {\r\n // given\r\n const wrapper = shallow(ButtonCounter);\r\n const button = wrapper.find('.test');\r\n\r\n // when\r\n button.trigger('click');\r\n\r\n // then\r\n expect(button.text()).toEqual('1');\r\n}); ButtonCounter.vue <template>\r\n <button @click=\"incrementCounter\" class=\"test\">{{ counter }}</button>\r\n</template>\r\n\r\n<script>\r\n export default {\r\n   name: 'buttonCounter',\r\n   data () {\r\n     return {\r\n       counter: 0\r\n     }\r\n   },\r\n   methods: {\r\n     incrementCounter: function () {\r\n       this.counter += 1;\r\n     }\r\n   }\r\n }\r\n</script> 4.4/ Custom events 4.4.1/ Emit a custom event This example shows how to test a custom event. At first, we render the Vue instance using “shallow”. Then, we simulate the action that will emit our custom event. Finally, we create a spy listener on the custom event we want to test. Here, “increment” is our custom event. It is supposed to be triggered by clicking on the “.test” button. The assertion will focus on the call of the listener . ButtonCounter.spec.js it('should emit an event increment on click', () => {\r\n // given\r\n const wrapper = shallow(ButtonCounter);\r\n const button = wrapper.find('.test');\r\n const spy = jest.fn();\r\n wrapper.vm.$on('increment', spy);\r\n\r\n // when\r\n button.trigger('click');\r\n\r\n // then\r\n expect(spy).toHaveBeenCalledTimes(1);\r\n}); ButtonCounter.vue <template>\r\n <button @click=\"incrementCounter\" class=\"test\">{{ counter }}</button>\r\n</template>\r\n\r\n<script>\r\n export default {\r\n   name: 'buttonCounter',\r\n   data () {\r\n     return {\r\n       counter: 0\r\n     }\r\n   },\r\n   methods: {\r\n     incrementCounter: function () {\r\n       this.counter += 1\r\n       this.$emit('increment')\r\n     }\r\n   }\r\n }\r\n</script> 4.4.2/ Handle a custom event The goal of this test is to check the reception of a subcomponent’s event by the parent component. The action of the test is to emit the event by the subcomponent, using the $emit instance method. We have to use $nextTick to wait for the DOM update caused by the “increment” event We use “async” and “await” because $nextTick returns a Promise. Finally, we can write the assertions on the updated DOM. You can take a look at the $emit documentation and $nextTick documentation. Counter.spec.js it('should increment counter on event increment from either buttons', async () => {\r\n // given\r\n const wrapper = shallow(Counter);\r\n const buttonCounters = wrapper.findAll(ButtonCounter);\r\n\r\n // when\r\n buttonCounters.at(0).vm.$emit('increment');\r\n buttonCounters.at(1).vm.$emit('increment');\r\n await wrapper.vm.$nextTick();\r\n\r\n // then\r\n const paragraph = wrapper.find('p');\r\n expect(paragraph.text()).toEqual('2');\r\n}); Counter.vue <template>\r\n <div class=\"test\">\r\n   <h1>counter</h1>\r\n   <p>{{ total }}</p>\r\n   <button-counter @increment=\"incrementTotal\"></button-counter>\r\n   <button-counter @increment=\"incrementTotal\"></button-counter>\r\n </div>\r\n</template>\r\n\r\n<script>\r\n import ButtonCounter from '../ButtonCounter/ButtonCounter'\r\n\r\n export default {\r\n   name: 'counter',\r\n   data () {\r\n     return {\r\n       total: 0,\r\n     };\r\n   },\r\n   methods: {\r\n     incrementTotal () {\r\n       this.total += 1;\r\n     },\r\n   },\r\n   components: {\r\n     ButtonCounter,\r\n   }\r\n }\r\n</script> 4.5/ Async Lifecycle Hook We want to show how to test instructions in a lifecycle step . In this test case, we will test instructions in the “ created ” hook. We use an external API ( http://www.mocky.io/v2/5a01affc300000da45fac0cf ) to get a number to initialize the counter and we use the axios dependency to perform HTTP requests. We can mock the external dependency using jest.mock(). It works by hoisting the mock to replace all calls to this dependency (‘axios’). Don’t forget the “mockClear” before each of your test to guarantee their independence. We have to use $nextTick because we can’t use the promise returned from axios in an async lifecycle hook . Notice that Vue auto-binds the component’s context in the “created” function. That is why we can access to the data counter in the “created” hook. You can take a look at the lifecycle documentation. ButtonCounter.spec.js import {shallow} from 'vue-test-utils';\r\nimport axios from 'axios';\r\n\r\nimport ButtonCounter from '../ButtonCounter';\r\n\r\njest.mock('axios', () => ({\r\n get: jest.fn(),\r\n}));\r\n\r\ndescribe('ButtonCounter.vue', () => {\r\n\r\n beforeEach(() => {\r\n   axios.get.mockClear();\r\n   axios.get.mockReturnValue(Promise.resolve({}));\r\n });\r\n\r\n it('should set counter to API value on component creation', async () => {\r\n  // given\r\n  const response = {\r\n    data: 11,\r\n  };\r\n  axios.get.mockReturnValue(Promise.resolve(response));\r\n\r\n  // when\r\n  const wrapper = shallow(ButtonCounter);\r\n  await wrapper.vm.$nextTick();\r\n\r\n  // then\r\n expect(axios.get).toHaveBeenCalledWith('http://www.mocky.io/v2/5a01affc300000da45fac0cf');\r\n  const button = wrapper.find('.test');\r\n  expect(button.text()).toEqual('11');\r\n });\r\n}); ButtonCounter.vue <template>\r\n <button @click=\"incrementCounter\" class=\"test\">{{ counter }}</button>\r\n</template>\r\n\r\n<script>\r\n import axios from 'axios'\r\n\r\n export default {\r\n   name: 'buttonCounter',\r\n   created () {\r\n     axios.get('http://www.mocky.io/v2/5a01affc300000da45fac0cf')\r\n       .then((response) => { this.counter = response.data })\r\n   },\r\n   data () {\r\n     return {\r\n       counter: 0\r\n     }\r\n   },\r\n   methods: {\r\n     incrementCounter: function () {\r\n       this.counter += 1;\r\n       this.$emit('increment');\r\n     }\r\n   }\r\n }\r\n</script> NB: for readability purpose, the API is directly requested from the component. This is a bad practice. In production code, we recommend to export this API call in another service (Single Responsibility Principle). Conclusion Now, you are more familiar with Vue.js, vue-test-utils and Jest. You can easily extrapolate from the given examples and official documentation for other test contexts. For instance, we chose to ignore “mount” helper, which allows integration tests by instantiating a component with its subcomponents. A component-based framework encourages to test through the DOM and user interactions since it strongly couples View and Controllers concepts of the MVC paradigm. That’s why these examples focus on the component’s functional behaviour rather than its technical implementation. This unit test strategy was designed by all the team members and fits our test definition and needs: fast to write fast to execute isolated (different than end to end tests) wider coverage than usual unit tests (add tests on template logic) Technical reminders: jest.mock() : mock external dependencies shallow() : mock subcomponents snapshot : test static elements trigger() : trigger user interactions wrapper.find() : find DOM element to make assertions Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Software Craftsmanship and tagged jest , snapshot , test , Test Driven Development , unit test , vue , vue-test-utils , vue.js , vuejs . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Vue.js unit test cases with vue-test-utils and Jest” vishwa 19/02/2018 à 08:15 how to test URL except for Button? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-12-04"},
{"website": "Octo", "title": "\n                The semicircle (episode 1 — If the code could speak)            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/the-semicircle-episode-1-if-the-code-could-speak/", "abstract": "The semicircle (episode 1 — If the code could speak) Publication date 01/12/2017 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 It starts with a sort of blissful ignorance. We write code oblivious to the consequences, unhindered by the necessity of receiving feedback about this code, and to the need for feedback on what we’ve done. We naively construct a tower with what we find here and there. When the tower shakes, we suddenly become extra careful and take that extra bit longer on each small portion of code, all the while buckling to pressure from our clients and managers. The tower crumbles; we rebuild hastily. There’s no time for quality assurance because there are too many issues to resolve that we didn’t anticipate, issues we didn’t anticipate because we have a weakness in our quality practices, a weakness we have because we don’t have time for quality assurance. We lose our calm and react irrationally, in a precipitous, disorganized way. Unable  to see what would be needed, or even observe what currently works well, we leap to imaginary false conclusions. We research whether better estimates could work and what better requirements would be, instead of looking into what’s actually slowing us down the most. We develop a sort of bad faith, which serves to explain everything. We are on the defensive. The whole time the code is slowly but surely sinking into entropy. Headlong rush to keep going. What would be needed to fix this problem? The more we wait, the more the cost of counter-measures increases and the more the work weighs on us. Want to see something that causes progressive disorganization? Watch the noodle of chargers and cords behind the desk that grows and tangles with each branch and merge. An utter mess. The problem only gets worse with time. We have no clue what could reverse the slide towards entropy. The problem then turns into a problem of results, credibility, and defensiveness, instead of a problem of quality. It becomes a debate, a conflict, a battle. Someone says: “If the code could talk, what would it say?” So we try to make the code talk, using tools that reveal the extent of the damage and calculate the technical debt. At least we’re doing something. Like at global gatherings on climate change, we reflect on the consequences when we don’t even know how to prevent the causes. And indeed, each cause is a tiny detail in the larger context. Each cause would require that we learn a new trick – the exact trick that would prevent future failures. The code of a program, such as it is presented in the books of the Sacred Art of Coding, is like a cathedral of logic. But for the newbie, newly recruited into this ‘TMA’ team, the code is a quagmire. It is a cacophony of perverse effects, of unexpected consequences in which the composite effect of each rashly made decision, of each addition done in haste, blindly, multiplies into a logical nuclear bomb. The program crashes –  a massive jumble of intertwined causes. The program fails on a test, which prevents another test, which would have revealed the presence of a gap between what was expected and what was produced; and that gap would have slowly but surely led to the detection of a logical error in the calculation code; and that error, had it been discussed between the testers and the developers, would have shown certain rules are not shared, due to differing interpretations because the two teams don’t see the product in the same way. At each step, there is a fuse. What do you think a professional team of landmine defusers would do? Exactly. It’s exactly the opposite of what a team of developers would do. No one takes the time to stand back far enough to see all the detail of what exists. Instead, a developer is busy repairing the program, while others are commenting on the situation, or doing busy work while waiting for a decision from management. By my logic, the response to an incident would go: ‘Let’s take the time to do rigorous root-cause analysis to find out what happened, without placing blame and without any rash decision-making, and let’s find what countermeasures we can take so that we can avoid similar incidents in the future.’ … my colleagues however, are  mystified by my logic: “That doesn’t look like something we could do without spending at least 4 hours!!” “It would be more worthwhile to…” “In any case…” “Yes, but…”‘ and then the manager interrupts because the client is furious on the other end of the line: “Fix it and re-deploy!” We live in complex times and we often hear that ‘software is eating the world’. Complexity eats the code too. This world, the one in which we take on, dismantle, take up again, and re-launch software projects surfs on all of that complexity. Recruiting ninja developers: multi-skilled, full-stack would be a plus, looking for a sense of adventure, if you have a team spirit and are a young geek, join us for this all-terrain project! The way things are going, we will always need ‘all-terrain’ teams. How can I achieve ‘all-terrain’ if I can’t even drive the jeep in the parking lot? I drive 20 miles, and wham! A dumb accident. We offer all kinds of excuses: ‘we should have…’, we list ‘best practices’, we make presentations and prescribe ‘transformations’. But we know perfectly well – since we would be incapable of restoring a Louis XVI armchair, performing a sonata, or dispensing care after a heart attack at the drop of a hat – we know perfectly well: It takes ten years to become an expert. That is to say that it takes time for basic practices to become habits, for knowledge to be sufficiently anchored to distinguish a professional from a passionate amateur. Suggesting a team change its habits in the middle of a project in crisis? At an accident scene, we certainly have more convincing arguments for a while. Change is hard enough even when things are going well, despite our conviction that there are good reasons for such change. Now is not the time to learn the basic practices. If the code could speak, what would it say? Let’s give it a voice: If you want to understand what I do, hang on. Here I say things like this, and there I say things like that. It’s just that way. I repeat myself so much that I could kill your attention, your interest, your enthusiasm the whole page down. It’s unpronounceable, twisted and complicated, but wait – there’s more! What do you mean: ‘does it work?’ Would you even be here if it didn’t work? Come on! You can show your disgust, but such is life. Just try to change this method. Good luck! I’ve cracked tougher nuts than you. For me to work correctly and be reasonably maintainable, you would have had to put in twenty times more work. Did you hear that? Twenty times! Forget your dreams of simplicity and efficiency. In the end, you are neither artist, scientist, thinker, nor engineer. You are an all-terrain ninja developer. We give in to just managing delays. We’ll fix the technical debt too. We’ll mend the relationship with the customer. We have to manage everything because nothing can be done independently. There is only trudging along, because there is no solution. There’s no way to make a late project such that it is no longer late. We know, but we will still go forward. Thirty six months of clean code that passes testing is already a highly delicate construct that requires daily attention (ask around), so just imagine 36 months of code written on the fly. We’ll be clearing landmines for years to come. It is, as I said before, a nuclear logic bomb. Yet… Could this possibly be the moment to at least pause and reflect? What if it was time for us to start changing the way we work? (to be continued) Tweet Share 0 +1 LinkedIn 0 This entry was posted in Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-12-01"},
{"website": "Octo", "title": "\n                The Wizard: Ansible, Molecule and Test Driven Development            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/the-wizard-ansible-molecule-and-test-driven-development/", "abstract": "The Wizard: Ansible, Molecule and Test Driven Development Publication date 20/11/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Magic has existed since the dawn of time. It has always been there, hidden in plain sight, making amazing things possible for those willing to open their eyes and harness its power. You can’t remember the first time you used it, and yet it feels like you’ve never existed without it. You’ve obliterated endless armies of enemies by virtue of spells and enchantments, and you’ve also constructed awe-inspiring marvels from the ground up. As a result, you’re renowned for your overwhelming powers in every single one of your incarnations through time. Everyone knows about the Wizard. This affinity with magic grants you the ability to see the real nature of things. Time for example, is not linear (as it was believed to be up until the late 21st century). Sometimes, it is hard to keep up with it… Suddenly you wake up. Or you come into consciousness. What year is it? So yeah, the output is still red, I really don’t know what else to do. What? You look to your left. N is sitting right there. He looks at you with mild concern in his eyes. Are you okay? Yeah, sorry. As I was saying, I don’t know why this isn’t working. You look at the spell he is trying to cast. His intention is clear: he wants to share the work of his clan with all the sentient entities in this reality by means of a portal. --- # task file for nginx - name: Install nginx yum: name: nginx state: present It is clear why he is failing. He is not taking into account the constraints of his clan’s environment. He must first gather the requirements of his spell, and he’s not looking at the right places. As a matter of fact, he is not looking at all. The answer is simple. Nevertheless… You reflect for a second, wondering if someone even taught N how to properly cast a spell. How are you testing your role? Hum… Are you testing your role? Yes, of course. How? I run my code on the target machine, and then I hope it doesn’t fail. You feel betrayed by the sound of his words. This is unacceptable. A whole stack of death spells appear in your head. Breathe, you repeat to yourself. In. Out. You keep going. Are you aware of the concept of Test-Driven Development? Yeah, I used to work that way all the time when I was doing dev with the team. So you’re familiar with it. Sure, L taught me. Ah, a familiar name. You like L. You fought together at Desaix, before he started mentoring his own apprentices on the arts of war. He must have taught N a thing or two. You wonder if he will be capable of surviving the trial. Why don’t you test your code that way instead? Because it is not Java, it’s Ansible. So? It’s not the same. Let’s start over. Do you mind if I grab the keyboard? Not at all. You harness your energy as you get ready to cast your first spell. You feel alive again. You’re in familiar grounds. Before you start working, you need an empty canvas, a playground to construct the structure and robustness of your magic. An isolated enclosure which will allow you to try and cast your spells, and which also follows the same rules as the target reality for the spell you’re trying to create. Have you ever heard of Molecule? …hum? It’s a Python tool. It allows you to test your Ansible code on many different levels, using local infrastructure. Local? Like a Virtual Machine? He’s fast. Time to make a choice. The Wanderer, or the Whale? The Whale is indeed faster. Not necessarily. You can use different providers. Vagrant and Docker are the most common if you want to work locally. We’re going to use Docker, since containers have a faster start-up time. Right on, Docker! The Whale it is. You concentrate and cast your first incantation. $ molecule init role --role-name nginx --driver-name docker You feel the energy flowing through. A whirlpool of blue water forms in the sky, and The Whale appears from inside of it. It comes swimming down through the air, looking at you both, and sings its song loudly. An empty enclosure materializes out of thin air, trapping you and N inside of it. The Whale disappears from the whirlpool where it came, right before the whirlpool itself vanishes into nothingness, as fast as it had appeared. --> Initializing new role nginx… nginx/ ├── README.md ├── defaults │   └── main.yml ├── handlers │   └── main.yml ├── meta │   └── main.yml ├── molecule │   └── default │       ├── Dockerfile.j2 │       ├── INSTALL.rst │       ├── create.yml │       ├── destroy.yml │       ├── molecule.yml │       ├── playbook.yml │       └── tests │           └── test_default.py ├── tasks │   └── main.yml └── vars └── main.yml You feel the need to test N. He’s looking at what you’ve produced, his eyebrows frowning. What do you think? It’s the standard role structure, isn’t it? – you ask, naively Yes, with the exception of the molecule directory with all of that stuff in it. Good. He realises there’s a difference. How can you easily explain the fact that, in order to control such an enclosure, you need intermediate spells to both create it and destroy it at will? You decide that it is not important right now, you’ll come back later to it. Don’t think about it yet. Let’s see if our test framework is working properly. You concentrate and you cast the spell on the enclosure/playground. $ molecule test --> Test matrix └── default ├── destroy ├── dependency ├── syntax ├── create ├── converge ├── idempotence ├── lint ├── side_effect ├── verify └── destroy N is impressed by the fireworks. He has so many questions. You feel overjoyed, but you give him a second so that he can get his ideas together. Test matrix? Default? And what are all of those things? When I talked about testing Ansible code on many different levels, this is what I meant. The default scenario is what gets executed when you type `molecule test` on your terminal, and it contains all of these actions: destroy, dependency, syntax, create, converge, idempotence… Yeah, I can read. Snarky. He wants you to know he’s not a child. He contemplates the enclosure’s structure. You wait for a while until he starts talking again. Are these… actions executed in order? Yes, unless you say otherwise. What do you mean? You decide now is the time to go down the rabbit hole. Since I typed `molecule test`, molecule executes the default test sequence, which corresponds to all of these actions. We can also redefine different sequences for different targets if the default test sequence does not satisfy our testing criteria. Different sequences? Like skipping all tests? Or executing only… let’s say… the linting? Sure. You raise your hand, and you snap your fingers. You hear warping sounds as the enclosure somehow shifts. Everything looks the same, yet everything feels different. # molecule.yml --- dependency: name: galaxy driver: name: docker lint: name: yamllint platforms: - name: instance image: centos:7 provisioner: name: ansible lint: name: ansible-lint scenario: name: default test_sequence: - lint verifier: name: testinfra lint: name: flake8 You take a deep breath, and you cast the spell again. $  molecule test --> Test matrix └── default └── lint --> Scenario: 'default' --> Action: 'lint' --> Executing Yamllint on files found in /private/tmp/test/nginx/... Lint completed successfully. --> Executing Flake8 on files found in /private/tmp/test/nginx/molecule/default/tests/... Lint completed successfully. --> Executing Ansible Lint on /private/tmp/test/nginx/molecule/default/playbook.yml... Lint completed successfully. Once you’re finished, you look back at N. He gets where you’re going. That was faster. Yes. I think we should add some actions back though. I agree. Which ones? Let me see the first test matrix we saw. He studies the playground spells thoroughly. Why is the destroy action executed first? Because the whole point is to be able to test that your role works in newly created infrastructure. Yeah, but it’s going to take longer if I need to create and destroy the container each time I launch my test, right? Good. He is connecting the dots, seeing outside of the box. You decide that you like N a little better now. Enough to keep the game going. Yes, but we will deal with that later. Just concentrate on the actions that we should execute in order to fully test our role. Okay, so then we need `destroy`… what is `dependency`? It allows you to pull dependencies from the galaxy, if you need them. Ah, let’s skip that then. What’s the difference between `syntax` and `lint`? The `syntax` action runs your playbook using the `–syntax-check` option native in Ansible, whereas `lint` checks your files using flake8, yamllint and ansible-lint. So `create` uses the create.yml file we just saw under the molecule directory inside the role right? Yes. It creates a Docker container suitable for Ansible use. And `destroy` uses the destroy.yml file. They’re both playbooks, so these actions are managed by Ansible. Eat your own dog food, alright. So `converge` executes my role on the container? That’s right. What about `side-effect` and `verify`? `side-effect` lets you produce situations in which you will be able to test more things, like HA failover, for example. I don’t think we’ll be needing that one for this situation. And `verify` is where all the magic happens. That’s where our unit tests get executed. Unit tests? What? You’ll see in a second. Alright. So I guess we’ll need `destroy`, `syntax`, `lint`, `create`, `converge`, `verify` and `destroy` again. No, wait. No destroy at the end. That’s why it is at the beginning. As you wish. You notice something is missing, but you decide to keep going to take advantage of the synergy. You will get back to it, eventually. You snap your fingers once again, altering the enclosure to match his words. # molecule.yml ... scenario: name: default test_sequence: - destroy - syntax - lint - create - converge - verify You cast the spell once again, using the modified reality of the enclosure. The fireworks appear one more time, this time slightly smaller. $  molecule test --> Test matrix └── default ├── destroy ├── syntax ├── lint ├── create ├── converge └── verify N seems happy. He understands how it works. You take advantage of his motivation to feed him more knowledge. This is going to act as our full test scenario. We’re not going to run all of that every single time we test though. Hum? Why not? What are we doing instead? Knowledge is power. Everyone knows that. If you’re able to acquire knowledge faster, you’re able to acquire power faster as well. Because we want our feedback loop to be as short and quick as possible. So we’re going to `create` our resources, `converge` them with our Ansible role, and `verify` that the results are what we expect. You can do this by typing `molecule create`, `molecule converge` and `molecule verify`. Then, if it all works, we’re going to run our whole test sequence. You let that sink in for a while, hoping that he doesn’t give up. You look outside the window. It’s raining. You like rain. At least on Earth. It’s better than on Venus, where it melts the skin off of your bones if you don’t cast a protection enchantment around you. That is if the temperature or the pressure don’t get you first. N doesn’t give up. He looks at you. Okay, I think I have it. Let’s create the container. You start summoning The Whale again, but you only hear its song this time. It is as loud as it was before. The enclosure disappears and reappears around you. $ molecule create --> Test matrix └── default └── create --> Scenario: 'default' --> Action: 'create' You look at N, and you think about the next steps. This is going to sound familiar to him. Let’s run our tests. They are located on the test_default.py file, under the molecule directory. You clap your hands once, loudly. As you separate them a stream of shiny green light emanates from them. $molecule verify --> Test matrix └── default └── verify --> Scenario: 'default' --> Action: 'verify' --> Executing Testinfra tests found in /private/tmp/test/nginx/molecule/default/tests/... Verifier completed successfully. Everything is okay, for now. Back to N. What are the three steps of Test-Driven Development? Red, green, refactor. L had done his job right. Excellent. What are you trying to accomplish? I want to expose my web app using nginx. What do you need for that? Well, I need to install nginx. Let’s go to red then. You spawn a glob of energy with your hands and use it to craft a rule within the enclosure’s reality: import os import testinfra.utils.ansible_runner testinfra_hosts = testinfra.utils.ansible_runner.AnsibleRunner( os.environ['MOLECULE_INVENTORY_FILE']).get_hosts('all') def test_nginx_is_installed(host): # Given nginx = host.package('nginx') # Then assert nginx.is_installed This feels familiar. It’s because it is. You clap your hands together, just as you did a minute ago. Nevertheless, red shiny light appears this time as you separate them. $ molecule verify =================================== FAILURES =================================== _________________ test_nginx_is_installed[ansible://instance] __________________ host = <testinfra.host.Host object at 0x10a9a7350> def test_nginx_is_installed(host): # Given nginx = host.package('nginx') # Then >       assert nginx.is_installed E       assert False E        +  where False = <package nginx>.is_installed tests/test_default.py:14: AssertionError See? That’s red. We’re good. What’s next? Green. So now we’ll add the task we need in order to install the server. You close your eyes and concentrate. The structure of the spell N had in the first place is recreated from the ground. --- # tasks file for nginx - name: Install nginx yum: name: nginx state: present Back to square one. It is now the same spell N had when he asked you for help. You cast the spell, knowing what will happen beforehand. $molecule converge --> Test matrix └── default ├── create └── converge --> Scenario: 'default' --> Action: 'create' Skipping, instances already created. --> Scenario: 'default' --> Action: 'converge' TASK [nginx : Install nginx] *************************************************** fatal: [instance]: FAILED! => {\"changed\": false, \"failed\": true, \"msg\": \"No package matching 'nginx' found available, installed or updated\", \"rc\": 126, \"results\": [\"No package matching 'nginx' found available, installed or updated\"]} No fireworks. N seems discouraged. You think about giving him a little push. He deserves it. It’s not about the magic itself, it’s about context. He’s not getting the sources for his portal from the right index. He needs the Epel codex. At least it’s easier to test now. Don’t be sad. The nginx package is contained within the epel-release repository. What? You could have told me that from the beginning, couldn’t you? We wouldn’t be talking about infrastructure testing if I had, would we? He smiles. He’s getting in the game, getting the feel of the mindset. He likes it. Let’s try it again. You modify the structure of the spell once again, specifying that you want your portal to be created following the instructions on the Epel codex. --- # tasks file for nginx - name: Install epel release yum: name: epel-release state: present - name: Install nginx yum: name: nginx state: present You cast the spell, with the new structure. $ molecule converge --> Test matrix └── default ├── create └── converge --> Scenario: 'default' --> Action: 'create' Skipping, instances already created. --> Scenario: 'default' --> Action: 'converge' … PLAY RECAP ********************************************************************* instance                   : ok=3    changed=2    unreachable=0    failed=0 The spell works, or at least it seems to work. The fireworks make the air vibrate within the enclosure. N seems happy. He doesn’t have the right reflexes yet. Can you blame him? It’s his first existence, and he’s just starting to play with magic. He won’t realize that his portal is not yet active, he’s just dazzled with the fireworks. It’s time for a new trial. Excellent! I guess we’re done! Yeah. Did you try it? Oh right, let’s see. Can I login to the container somehow? Yeah, `molecule login`. [root@instance /]# curl localhost curl: (7) Failed to connect to ::1: Cannot assign requested address Hm. But… ah, right! I never started the service. Be my guest, add the missing code needed to do it. He is about to modify the spell’s structure, when he suddenly stops. He looks at you. No. It’s not the right way. Can’t I test that first? He passed the test. He might make a good disciple. You start to understand why L started teaching in the first place. Sure, just add another test. Alright… def test_nginx_is_running(host): # Given nginx = host.service('nginx') # Then assert nginx.is_running I love writing tests in Python by the way. It’s way simpler than in Java. Don’t let L hear you say that. Try it. You know the Whale won’t let N get away with that, but you want to see him try. Failure is part of the learning process. It is almost more important than success. It’s the only way he can learn to get up and keep trying. $ molecule verify E       AssertionError: Unexpected exit code 1 for CommandResult(command=u'systemctl is-active nginx', exit_status=1, stdout=u'', stderr=u'Failed to get D-Bus connection: Operation not permitted') N claps his hands like you did before, but no light appears as he separated his hands. He looks as you, confused. Hm, this doesn’t look like the good kind of red. It is not an assertion error. It is not. Testinfra (your Python unit testing library) uses Systemd, Upstart or SysV in order to test if the services are running. Docker containers don’t have an init system. They do have Tini, but it doesn’t work the same way. So I need to change the image that I’m using. Is there a Centos image that includes Systemd? Yeah, it’s called centos/systemd. Go figure. Where can I modify that? On the molecule.yml file. You will need to specify the container’s command instruction in order to start Systemd, because Molecule overrides it by default. You will also need a privileged container, since Systemd requires CAP_SYS_ADMIN and non-privileged containers do not have that capability. Hum, I didn’t quite get all of that, but I think I’ll ask you about it again tomorrow. Let me redo the configuration. platforms: - name: instance image: centos/systemd privileged: True command: /usr/sbin/init The Whale will surely accept this offering. You know it will not reject your summoning anymore. N verifies the spell again. $ molecule test =================================== FAILURES =================================== ____________ test_nginx_is_running_and_enabled[ansible://instance] _____________ host = <testinfra.host.Host object at 0x110adff50> def test_nginx_is_running(host): # Given nginx = host.service('nginx') # Then >       assert nginx.is_running E       assert False E        +  where False = <service nginx>.is_running tests/test_default.py:20: AssertionError -- Docs: http://doc.pytest.org/en/latest/warnings.html ================ 1 failed, 1 passed, 1 warnings in 6.74 seconds ================ Done, we’re back to red. Now I’ll make the changes in order to start the service. You smile as you see N craft. He is not ready yet, but he is well under way. You feel proud of him. Alright, I’m done. What do you think? He shows you the structure of his spell. --- # tasks file for nginx - name: Install epel release yum: name: epel-release state: present - name: Install nginx yum: name: nginx state: present - name: Start nginx command: systemctl start nginx All of your previously found pride turns into disappointment. This is preposterous. It can’t be serious. He can’t be serious. He should understand that you can’t be spawning portals indefinitely. The results could be catastrophic. But then…you contemplate him. You realize he’s tired. Idempotence is a hard concept to grasp too. Benjamin Peirce taught it for over 50 years at Harvard. You remember the first time Merlin brought you to an enclosure like this one. Just the memory of it makes you feel goosebumps on the back of your neck. You decide that you’re going to let him continue, for education purposes only. He’ll realize the problem with his proposal. Why don’t you try it? He casts the spell himself. $ molecule converge --> Test matrix └── default ├── create └── converge --> Scenario: 'default' --> Action: 'create' Skipping, instances already created. --> Scenario: 'default' --> Action: 'converge' PLAY RECAP ********************************************************************* instance                   : ok=4    changed=1    unreachable=0    failed=0 Right after the fireworks, a green portal appears right in front of both of you. It seems to work. Great. I’ll verify now. $ molecule verify --> Test matrix └── default └── verify --> Scenario: 'default' --> Action: 'verify' --> Executing Testinfra tests found in /Users/sebiwi/stuff/test/nginx/molecule/default/tests/... ============================= test session starts ============================== ===================== 2 passed, 1 warnings in 6.70 seconds ===================== Verifier completed successfully. Green light everywhere. Perfect! Green! Or is it? You decide to give him a little push in the right direction. Try running the whole test suite. Alright $molecule test --> Test matrix └── default ├── destroy ├── syntax ├── lint ├── create ├── converge └── verify ---> Scenario: 'default' --> Action: 'lint' --> Executing Yamllint on files found in /Users/sebiwi/stuff/test/nginx/... Lint completed successfully. --> Executing Flake8 on files found in /Users/sebiwi/stuff/test/nginx/molecule/default/tests/... Lint completed successfully. --> Executing Ansible Lint on /Users/sebiwi/stuff/test/nginx/molecule/default/playbook.yml... [ANSIBLE0012] Commands should not change things if nothing needs doing /Users/sebiwi/stuff/test/nginx/tasks/main.yml:14 Task/Handler: Start nginx No fireworks, no lights. What? Oh, that’s just ansible-lint, it often fails for that sort of thing, I’ll just deactivate it for the task. You stare in disbelief as N “corrects” the structure of the spell. --- # tasks file for nginx - name: Install epel release yum: name: epel-release state: present - name: Install nginx yum: name: nginx state: present - name: Start nginx command: systemctl start nginx tags: - skip_ansible_lint He finishes the modifications and he casts the spell once again. $  molecule test --> Test matrix └── default ├── destroy ├── syntax ├── lint ├── create ├── converge └── verify All steps completed successfully. He looks at you proudly. You’re dismayed. He’s being naive and reckless. You realize that it is time to handle the situation. We’re green now. I think we’re finished. Not really. Previously, ansible-lint said that you’re changing things even if nothing needs to be done. Yeah, it’s because I was starting a service. Yes, but since you’re not using an Ansible module, you’re executing an action each time you run your playbook. I thought Ansible was idempotent. Not really. You can make idempotent code with Ansible. It doesn’t mean everything you do with it is going to be idempotent. Some things are not meant to be idempotent. Like what? Application deployments, for example. You’re deploying a new version of the application. It is meant to change things. I see. But this role should be idempotent. Certainly. You can add a new action to your default scenario test suite in order to test for idempotence. You close your eyes as you change the reality of the enclosure around you once again. scenario: name: default test_sequence: - destroy - syntax - lint - create - converge - idempotence - verify You cast the spell again. $  molecule test --> Test matrix └── default ├── destroy ├── syntax ├── lint ├── create ├── converge ├── idempotence └── verify --> Scenario: 'default' --> Action: 'idempotence' ERROR: Idempotence test failed because of the following tasks: * [instance] => nginx : Start nginx N looks at you. So we weren’t actually at green yet. We weren’t. Idempotence is a behaviour of our code. It also needs testing. I can fix it though. Show me. You hand over the control of the enclosure to N. He modifies the spell structure. --- # tasks file for nginx - name: Install epel release yum: name: epel-release state: present - name: Install nginx yum: name: nginx state: present - name: Start nginx service: name: nginx state: started You smile with relief. Much better. Try it. Right away. $  molecule test --> Test matrix └── default ├── destroy ├── syntax ├── lint ├── create ├── converge ├── idempotence └── verify All steps completed successfully. And now we’re green. Yes! Let’s continue with our TDD approach. Red, Green, and… ? Refactor! Do you see any possible improvements on this code? Well, you could use your “Start nginx” task as a handler, which is triggered by the installation of nginx, couldn’t you? Yes, I could. --- # tasks file for nginx - name: Install epel release yum: name: epel-release state: present - name: Install nginx yum: name: nginx state: present notify: Start nginx --- # handlers file for nginx - name: Start nginx service: name: nginx state: started $  molecule test --> Test matrix └── default ├── destroy ├── syntax ├── lint ├── create ├── converge ├── idempotence └── verify All steps completed successfully. N looks euphoric. He’s applying his well-earned war experience to a whole different situation, and it is working like a charm. There’s a slight catch within your proposal, but you decide that you will let him discover it by himself. Wow, that worked out great! What else? You could have only one “yum” task and iterate over the packages that you want to install. Good call. --- # tasks file for nginx - name: Install epel-release and nginx yum: name: \"{{ item }}\" state: present with_items: - epel-release - nginx notify: - Start nginx $ molecule test ... --> Scenario: 'default' --> Action: 'converge' PLAY [Converge] **************************************************************** TASK [Gathering Facts] ********************************************************* ok: [instance] TASK [nginx : Install epel-release and nginx] ********************************** failed: [instance] (item=[u'epel-release', u'nginx']) => {\"changed\": false, \"failed\": true, \"item\": [\"epel-release\", \"nginx\"], \"msg\": \"No package matching 'nginx' found available, installed or updated\", \"rc\": 126, \"results\": [\"No package matching 'nginx' found available, installed or updated\"]} PLAY RECAP ********************************************************************* instance                   : ok=1    changed=0    unreachable=0    failed=1 Hm. I guess I need to install epel-release before I can install nginx. It doesn’t work if I do both at the same time. True. I wouldn’t had realized that without the tests. I would have done my refactoring, and it would have worked, since epel-release would already be installed on my machine when trying to install it along with nginx in a single task. Indeed. Also, with the current code structure, you’re not only starting nginx after installing nginx, but also after installing epel-release. Right. I shouldn’t. That was a test, wasn’t it? Maybe. N transforms the spell structure back to its previous form. You feel relaxed. You always feel this way after creating something impressive. You feel the same way you felt when you structured the pyramids, only to a minor extent. This time, however, the actual creator is N. You only acted as a catalyzer. The knowledge was always inside of N, he just needed to hear the right questions. N looks tired, but he wants to keep going. We just went through a full Red/Green/Refactor cycle. Sweet! Do you want to do another one? Absolutely! What happens if your VM gets rebooted? Nothing, nginx will still be installed. Will it be running? Oh, no. It won’t. Yikes. You should enable it then. You know what to do. N creates a new rule within the enclosure’s reality. def test_nginx_is_enabled(host): # Given nginx = host.service('nginx') # Then assert nginx.is_enabled He claps his hands together. Red light appears everywhere. $ molecule verify --> Test matrix └── default └── verify --> Scenario: 'default' --> Action: 'verify' --> Executing Testinfra tests found in /Users/sebiwi/stuff/test/nginx/molecule/default/tests/… =================================== FAILURES =================================== __________________ test_nginx_is_enabled[ansible://instance] ___________________ host = <testinfra.host.Host object at 0x103b20fd0> def test_nginx_is_enabled(host): # Given nginx = host.service('nginx') # Then >       assert nginx.is_enabled E       assert False E        +  where False = <service nginx>.is_enabled tests/test_default.py:28: AssertionError -- Docs: http://doc.pytest.org/en/latest/warnings.html ================ 1 failed, 2 passed, 1 warnings in 7.29 seconds ================ Red. Go for Green. N modifies the structure of the spell to match the new reality constraints. He’s got a clear head now, he knows what is missing. --- # handlers file for nginx - name: Start nginx service: name: nginx state: started enabled: yes $  molecule test --> Test matrix └── default ├── destroy ├── syntax ├── lint ├── create ├── converge ├── idempotence └── verify All steps completed successfully. The green portal appears in front of you both. You know it is sustainable, you both tested it thoroughly. It’s a proper wizard spell. People would say it is one of your own. N looks at you and smiles. Green. Yes. Can you refactor? I don’t think so. It looks pretty clean to me. I agree. You do agree. N seems in charge. He needs more time to develop himself, to learn, to become comfortable with this whole new world. You won’t be as useful during that period as you were now. It’s time to go. You know how to continue, right? Yes, I think I can manage. This is pretty cool. Thanks. No problem kid. Just call me if you need more help. Will do. Hey, do you..? N looks at where you used to be 10 seconds ago. You are no longer there. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile , Infrastructure and Operations , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 3 commentaires sur “The Wizard: Ansible, Molecule and Test Driven Development” AICH 18/12/2017 à 10:15 L'article est intéressant, mais son format qui se veut ludique, rend en vérité difficile sa lecture, et donc sa compréhension.\r\n\r\nDommage. Bruce becker 11/05/2018 à 17:06 I feel the exact opposite of aich.\r\nthis is frikkin poetry. Romain Prévost 16/10/2019 à 14:33 I agree with you both, yes, at times it makes you think a bit harder than a \"classic\" tech blog post, but I find this helps me engaging more with the content and furthers my understanding. Good job! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-11-20"},
{"website": "Octo", "title": "\n                What’s up with Android? droidcon UK 2017            ", "author": ["Sandra Dupré-Pawlak", "Nicolas Telera"], "link": "https://blog.octo.com/en/whats-up-with-android-droidcon-uk-2017/", "abstract": "What’s up with Android? droidcon UK 2017 Publication date 09/11/2017 by Sandra Dupré-Pawlak , Nicolas Telera Tweet Share 0 +1 LinkedIn 0 October 26th and 27th, we had the occasion to attend droidcon in London. For both of us, it was our first steps in a conference of this scale and on our favorite domain. Our expectations were not the least: within our team, most of the topics of the moment seemed to have been explored back and forth, whether it is about architecture, asynchronism management libraries and methods or even Kotlin advantages. We hoped to discover news things, demystify obscure looking subjects and finally, come back with our arms filled with news topics to explore (and goodies). This year Google I/O put Kotlin and many of their researches about Machine Learning and Deep Learning (AlphaGo, TensorFlow, etc) forward. This year’s program clearly was influenced by these domains, which was a good thing for us. To this, add an important part about how does it works « under the hood », a good dose of applications development talks, all of this sprinkled with a bit of human, not forgetting the inevitable Android community figures. Under the Hood This year, one part of the talks was dedicated to the Android system internal functioning. Some gave us a global recall, in particular Effie Barak’s « Android Internals for Developers » that was about processes management, from their creation to their suppression by the system and the way they communicate. Some others a little bit more specific with an important part of views management. Britt Barak presented us the path from a high level object to a pixel displayed on our screen in the simply entitled « From View to Pixel » , going through the measuring of the view, its positioning and its drawing. All of this punctuated by the Android system assets modification through a resources overlay. That kind of talk feels so good. We spend our days, our months even our years developing awesome applications, implementing trendy architectures, migrating our code to Kotlin or trying to find one more way to manage asynchronism in our projects. We end up losing our curiosity about what’s going on being every line of code that we write. Knowing that the hexadecimals IDs that we sometimes see in Android Studio are actually formed with a package, a type and an entry identifiers or that the Android system in mainly managed by three managers (window, package and activity) launched by one unique service, stimulates that curiosity within us. We couldn’t miss these talks for anything and we were right. Of course, these last are not those that will enable us to come back to France with a lot of topics to explore. But strengthening our foundations doesn’t hurt! Android App Development Some talks were about topics that we know well within our team. Clean architecture and Kotlin are part of them. Igor Wojda’s « Why do we need Clean Architecture » reminds us about the main principles of this architecture and why it is time to stop using the MVP. His vision of the Clean Architecture is really similar to ours. About Kotlin, being in almost every talk, Kai Koening want to push Coroutines with « Kotlin Coroutine and Android sitting in a tree » . It is a test that mainly got our attention. This test creates a loop of one million iterations and opens a thread which add an element in a file in each of them. This program takes 1 minute to finish. He then writes the same program with coroutines and the program takes 5 seconds. A totally different scale that puts into question our use of the threads! In the « obscure » category, we find the ConstraintLayouts! For us, it was difficult to understand why use this layout when we can do everything with the good old LinearLayout and RelativeLayout! We were wrong, and we know now why. Britt Barak’s « From View to Pixel » had already told us about the difference in performance between these other layouts and the ConstraintLayout, Connie Reinholdsson with « Three key lessons when migrating to ConstraintLayout » gives us the keys to start using it. A good image comparison software, right-click « Convert to ConstraintLayout » and you can too. Chris Banes presents us « Becoming a Master Window Fitter » which proves that Android has already done all the work regarding our windows and that it is not a good thing to go modify all of its functioning, like changing the statusBar size. Ajit Singh ( « Online to Offline world of Mobile Apps » ) and Chris Le Roy ( « Hacking Android, a Hacker’s Narrative » ) changed our way of conceiving an application regarding the outside world. Thinking about places where the network is non-existent or being rigorous with the security of our applications are not always the first things we think about. Finally, Jeff Corcoran and his (quite funny) talk « So you made an app and nobody cares » gives us the keys so that our applications don’t fall into the anonymity of the PlayStore mass, while quoting himself. All of these tricks, advices and demonstrations tickled a curiosity that we thought we didn’t have on some topics. These talks erased our bias about the ConstraintLayout and opened our eyes on sustainability of what we produce. Big Stars in the Android Sky There no use trying to hide it, going to the droidcon makes you feel like a child excited by the idea to go sit in the main room listening to these people that you talk about almost everyday with your colleagues, during the four keynotes. It should be pointed out that the droidcon is a « ticketless » event, which means that, theoretically, you just show up carefree and your ID is sufficient to get your pass. Sounds good on paper. But actually, not really. Among the queue, people are called by the first letter fo their first name in order to redirect them toward the ticket offices. In the end, everybody is cheating, not cool. That’s why we ended up watching the opening keynote in the « overflow room ». But whatever, for the 10 years of Android, Chet Haase and Romain Guy presented us a retrospective of its evolution from its beginning. It feels like we soak with this story and become a little bit more complete developers. Then it was Lisa Wray’s time to close the first day. Many of us have already wanted to start creating a library. Lisa Wray gives us the keys to succeed, or at least stumble less often (let’s point out that these were the coolest slides that were given us to see). A quite similar talk was given to start the second day by Florina Muntenescu who gave us the good practices in the creation of an API. These two talks made us realize that when we have a need, thousands of others have it too. Whether it is about creating a library of an API, it leaves us with a more critical look about our environment. We want to find the use case that will enable us to start too, to create a community around our product and to make it evolve as a mini startup. What better way after half a day of intensive note-taking and some gnocchis that going to the first row of the main room to listen to Chet Haase and Romain Guy present us their new programming language supposed to replace the famous but already obsolete Kotlin? I’m talking, of course, about the new « Functional And Reactive Turing-complete language » ( F.A.R.T. ). An exceptional comedy talk with a crazy questions and answers part to which our two speakers took part with a confusing seriousness. To conclude these two days of droidcon, we went to the main room again to listen to Jake Wharton himself talk about bytecode. Ok, this can be scary and I have to admit that we had to hang on to keep with his talk. When we saw this tweet, we thought it was a joke: Well no, 339 slides of presentation to be precise, showing step by step some examples of bytecode. In the end of the droidcon, that hurts. Jake is not messing around. The goal of the talk was to teach us to read bytecode so that we are able to answer to questions like « What iteration variable type is the fastest? » of « How are the lambdas backported from Java 8 to Java 6/7? » . Actually, this was captivating. In the end of the talk, Jake made us the promise that by looking our bytecode every now and then, we would become better developers. Cool Intelligent Android: What’s up with Machine Learning? Two talks were about the famous but terrifying TensorFlow. It is Erik Hellman’s « My app is smarter than your app » and Qian Jin’s « Heat the neurons of your Smartphone  with Deep Learning » who tried to explain neuronal networks and predilection models to an Android developers audience. And that was not given! Despite the difficulty of the domain (who made him want to flip his table), Erik Hellman points out the necessity to start using it: user experience, automation… In addition to TensorFlow, he adds that we can play with Google’s CloudAI which already contains a lot of networks (recognition of images, videos and feelings in sentences etc). Qian Jin talked about the Magritte project: an application able to recognize fruits by filming them. She shows the evolution of its recognition model via Deep Learning and the ways to integrate it in an application. And let’s not forget it, our smartphones are increasingly powerful. With the new generation of DSP CPU, Deep Learning becomes accessible to the mobile. These talks were extremely important to us. After months looking at TensorFlow out of the corner of our eyes, here we are fully impressed by the possibility to make our applications even more intelligent. Intelligence is within range of code: let’s use it. Be a developer, Be Human When you go to a technical conference for the first time, you don’t expect talks about who we are and not what we do. But it was the case of these two talks, while totally different: « Master your career: Tips and Tricks to Rule your Future » by Jose Nieto and « Authentic Developer » by Anastasia Lopez. In the first talk, Jose Nieto talks about the importance of catching opportunities in our career. He had to choose between going on holidays or to the droidcon: he chose the last and the opportunities he had since then are worth the investment. We must say « Yes » to opportunities and have a positive attitude. His last advice is to target one or multiple objectives on 6 months (whether on a professional or personal level), and to check the results in the end. Where Jose Nieto talked about us, developers, Anastasia Lopez talked about us, humans. She talks about her experience, about the shame she experienced from it, about the image she has of herself. She talks about us, about this feeling that makes us never feel talented enough, funny enough, to be an imposter. She gives us 5 characteristics to help us find our authenticity: Natural, Sincere, Spontaneous, Open and Genuine. We have to work of those characteristics. She also encourages us to see the world through the eyes of others. You have the power to make the difference We have been fascinated by this last talk. The humanity and the vision of Anastasia Lopez fits the values that OCTO wishes to share. Talking about the shame that we can feel regarding our job and the way to work on it were really instructive. We came with great expectations, we go back with a lot of ideas. The feeling that we succeeded in finding topics to explore and to share with our team. Among all the talks that we went to, two of them left their mark of us. On the human theme, we learned a lot from Anastasia Lopez’s talk about authenticity. We would like to see that kind of talk more often and this message be heard by everyone. By listening to Anastasia, we realize that talking often is enough to unlock difficult situations. On the more technical side, Erik Hellman’s talk about applications intelligence lightened a mysterious subject and got our curiosity. Before the droidcon, we didn’t feel able to deal with that kind of domain. We are now convinced by the possibilities that offer to us and motivated as ever to explore them. During the Women Tech Makers event organized by the PAUG and the GDG in Paris, Marion Hayoun (AndroidMakers) shared with us the difficulty to find women speakers. The keynotes of this edition of the droidcon UK were done by as much men than women. On the whole event, almost 22% of the talks were presented by women speakers, pretty impressive compared to most of the other events. This subject is important to us. It reassures us and is encouraging for the future. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “What’s up with Android? droidcon UK 2017” Ted Horyczun 09/11/2017 à 19:59 Kotlin coroutines is a great tool for Android developers! It's a lot easier than async tasks and can go from the UI thread to the background thread in a snap. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-11-09"},
{"website": "Octo", "title": "\n                Ansible reporting with ARA: Ansible Run Analysis            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/ansible-reporting-with-ara-ansible-run-analysis/", "abstract": "Ansible reporting with ARA: Ansible Run Analysis Publication date 30/10/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 ̛I had a Slack conversation with some friends from work ( B , R and G ) the other day. It went somewhat like this: B : “Hey, have you guys ever heard of ARA ?” This was followed by 5 minutes of the deafening sound of contextualisation and Google searches by everyone on the channel. R : “Nope, but every piece of technology that could enable Ansible to catch up with PuppetDB/Board would be very much appreciated” G : “Reporting is cheating” (This statement makes way more sense in French) Me : “ It looks dope, it’s way better than scrolling through the output of a Jenkins job and you won’t be needing things like profilers anymore. I’ll try it and I’ll tell you if it sucks or not” So this is it. What is ARA? ARA is an acronym. It means Ansible Run Analysis . It provides you with a detailed analysis of your Ansible playbook runs, which includes: Tasks description, status and output Play description and duration Hosts involved Files executed Parameters used ARA Dasboard And a bunch of other cool things. We will take a look into that in a second. Yeah, really. How does it work? Basically ARA is an Ansible callback. It records everything related to a playbook’s execution into a database, which is SQLite by default. That’s it. As simple as possible . At least that’s the most important part. But also included with ARA is a CLI which allows you to query the aforementioned database interactively, a web interface coded in Flask which allows you to check out all of your data dynamically, and two Ansible modules: ara_record and ara_read . I’ll talk about these two later. How to install it? It depends on what you want to do with it, really. You can have a simple local database for testing purposes, or if you’re working on a project by yourself and you feel like recording everything you’re doing for further analysis. If that’s your use case, then the process is really simple. You just need to install the ARA dependencies: # RHEL yum - y install gcc python - devel libffi - devel openssl - devel redhat - rpm - config # Ubuntu apt - get - y install gcc python - dev libffi - dev libssl - dev And then install it using pip: pip install ara This will create a .ara directory at the user’s home, with an SQLite database inside of it, as well as a logfile. Then, in order to launch the dashboard, just use some `ara-manage` magic: ara-manage runserver This will launch the development server which will listen on port 9191 by default. You can specify different IP and port settings by using the -i and the -p options, respectively. After this you’re all set, and you can go and see all your beautiful dashboards on the web interface. Then again, you’ll probably want your dashboard to be accessible at all times if you’re working on a serious project. What I did was that I installed ARA in the same machine where I installed the Jenkins server, I created a systemd unit file in order to launch the dashboard as a service, and I exposed it using nginx. Simple. The ARA installation part remains the same for this case, unless you want to use MySQL or PostgreSQL instead of SQLite. I didn’t. Here’s a guide of the schemas you’ll need to create and the extra configuration variables you will need to add in that case . Your service file will look somewhat like this one: [Unit] Description=ara After=network.target [Service] User=jenkins Type=simple TimeoutStartSec=0 Restart=on-failure RestartSec=10 RemainAfterExit=yes ExecStart=/usr/bin/ara-manage runserver -h 0.0.0.0 -p 9191 [Install] WantedBy=multi-user.target You can change the IP and the port by modifying the ExecStart instruction on the Service namespace of the file. Note that I’m starting the process as the Jenkins user, since it’s the same user that will be running the Ansible playbooks. Moving on. For nginx, you get something like this: server { access_log /var/log/nginx/access_ara_ssl.log.json json; access_log /var/log/nginx/access_ara_ssl.log; error_log /var/log/nginx/error_ara_ssl.log; listen 443  ssl http2; ssl_certificate /etc/nginx/certificats/ara.mycert.crt; ssl_certificate_key /etc/nginx/certificats/ara.mykey.key; server_name ara.amazing.io; location / { proxy_pass http://jenkins-ip:9191 ; } } And that’s it. Your service will be accessible on the host. You can also use Apache if you want, or any software capable of working as a reverse proxy for that matter. Finally, you also need to tell Ansible to use the ARA callback. This needs to be done no matter which installation option you choose. It is fairly simple though. You just need to add the following lines to your ansible.cfg file: [defaults] # Callback callback_plugins = $ara_location/plugins/callbacks # ara_record and ara_read modules action_plugins = $ara_location/plugins/actions library = $ara_location/plugins/modules The next time an Ansible playbook is executed, the ARA callback will connect to the SQLite database located on the .ara directory of the user’s home directory. You can also configure these as environment variables. Just in case (shrugs). Playing with ARA Let’s talk about the Ansible modules and the CLI. The modules are called ara_record and ara_read. These allow you to do some cool things. The first one, ara_record, allows you to record data using a key/value format, just like you would do with an Ansible fact. In other words, something like this: - name: Record useless value ara_record: key: “all_your_base” value: “are belong to us” Which will translate into a record on the ARA database, or something like this on the dashboard: Record on the dashboard You can also record things using the ad-hoc command directly, like so: ansible localhost -m ara_record -a \"playbook=7ab3858f-5d17-48f1-bb94-19ccdec0c983 key=all_your_base value='are belong to us'\" You can obtain the playbook ID using the ARA CLI, with ` ara playbook list`. A cool thing about this is that you can actually record data once the playbook is already done, if you have the playbook’s id. The ara_read module, on the other hand, lets you read (duh) data that has previously been recorded on the same playbook run. This is interesting, because it enables you to use it across plays as long as you stay within the playbook’s scope. Thus, you can record a value and then use the same value on a different play, like an ID, an IP, the output of a command, or almost anything you can think of. Honestly, I don’t see any amazing use cases for this module. I think that if you’re using this as a key element in order to pass data between your plays, you’re basically structuring your playbooks/roles/plays the wrong way. But that’s just me. Finally, the web interface is quite cool since it allows you to filter information by hostname, keyword, or status. This is particularly useful when you’re trying to see the changed, or failed tasks of a playbook, or when you’re trying to see all the tasks linked to a certain role or middleware. Full text search Search by status Alright, let’s recap. It’s great because: It helps you troubleshooting your playbooks easily. You can search the information you want to find on the dashboard, instead of browsing through the extremely long output of a Jenkins job trying to figure out what went wrong It’s super easy to install it locally, and relatively easy to do it on a dedicated machine, exposed through a web server It is under active development by the RedHat team, which probably means that we will be seeing some new features soon You can use the CLI or query the database yourself if you don’t like dashboards that much. You can also build your own dashboard by querying the database if you don’t like Flask. You probably won’t do this. It’s not that great because: If you use it with a Continuous Integration server, such as Jenkins, you’ll need to check two different web interfaces in order to realize what is going on with your jobs. This is far from ideal. You can also generate static reports in HTML format, but you’ll also need a web-server in order to see them, which is a little bit annoying. There is no built-in database suppression purge politique. So far ARA has been running for two weeks, and we’ve got 122MB of data on a heavy-usage Ansible platform. This isn’t huge by any standards, but I’d still like to be able to delete old data automatically, without having to create a custom SQL query in order to delete the oldest playbook executions. Maybe this would go against the whole “simple is better” principle that seems to guide ARA development. Conclusion I really liked ARA. I think I’ll install it on every Ansible project I work on from now on. It’s a quick-win in any case. I’d love to see better integration with Continuous Integration servers, or a simpler workflow in order to work with them. Maybe I just haven’t found it yet. Big up to David for his work on ARA. Keep up the good work! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-10-30"},
{"website": "Octo", "title": "\n                A Responsive and Clean Android App with Kotlin Actors            ", "author": ["Maxime Bonnet"], "link": "https://blog.octo.com/en/a-responsive-and-clean-android-app-with-kotlin-actors/", "abstract": "A Responsive and Clean Android App with Kotlin Actors Publication date 23/10/2017 by Maxime Bonnet Tweet Share 0 +1 LinkedIn 0 One crucial point that each front-end developer must face when developing an application is handling the switch from UI to background threads. Indeed, we can all agree on the fact that your application shouldn’t freeze during long running operations. The Android SDK even strictly prevents us from calling an API from the Main Thread . There are many ways to tackle this issue, for instance with RxJava, with a ThreadPool ( check this out ) … This article aims at proposing yet another asynchronism solution, using Kotlin actors. We’ll also investigate what actors bring to the table that other solutions do not. Actors Why ? As said above, asynchronism is an issue we all have to face and multiple ways to approach it. As I don’t feel 100% satisfied by the solution I use daily, I wanted to investigate new possibilities. I first wanted to check Kotlin’s coroutines and decide if they were a viable solution. Simply put, coroutines can be seen as lightweight threads. To use them, you first need to add this to your build.gradle . implementation \" org .jetbrains .kotlinx :kotlinx-coroutines-android :0.18\" During my experiments, I found that they offer a clean, simple and concise way to switch from the Main Thread to the background : launch ( CommonPool ){ /* Background computation */ } And back to the Main Thread : launch ( UI ){ /* Update views */ } The launch coroutine builder creates and launches a new coroutine in the given Context (CommonPool & UI). It’s only one of the many builders that you can use with coroutines. One other commonly used builder is async that you can use to implement an async / await kind of asynchronism. I then stumbled upon the actor builder that allows to build an actor model in Kotlin. The actor model is quite popular in other languages like Erlang and Scala (with the Akka library ). It’s an higher level model that allows us to write complex concurrent systems. Simply put, actors are lightweight processes that you can send message to. Actors in Kotlin In Kotlin, the actors are available through the experimental couroutines package. The “lightweight process” is implemented as a coroutine, and you can send messages asynchronously in a Channel (I picture it as a mailbox). This is a basic actor code : private val gradeActor = actor<Int>(CommonPool) { for ( integer in channel) {\r\n            when ( integer ) { in 0..9 -> computeLowGrade() in 10..20 -> computeHighGrade() else -> computeIllegalGrade()\r\n            }\r\n        }\r\n    } The basic thing to know about actor is that they have a mailbox, and you can send them messages. Kotlin guarantees that the actor only treats one message at a time, and that they will be treated in the order that they’ve been received. The expression actor<Int>(CommonPool){...} returns an actor that takes Integers as messages. The CommonPool denotes that all code of this actor will be executed in the CommonPool context, meaning in background, well away from the Main Thread. Indeed, an actor is really a kind of coroutine that exposes a SendChannel that you can send message to. That’s why we can use an actor as a way to achieve asynchronism. Once in the body of the actor, you have access to its channel. In the example above, we go through the channel and for each message (represented as an Integer), decide what to do with it depending on its value. Note how Kotlin’s when expression is useful here. The Clean Architecture Let’s have a look at the Clean Architecture, in which I want to incorporate actors. Basics Although probably not as popular as some mobile architectures like MVP or MVVM, Uncle Bob’s Clean Architecture is often used on Android and iOS application development. Resulting from the application of many clean code principles such as SOLID and separation of concerns, this architecture perfectly fits with large projects and development teams. In larger projects, the heavy boilerplate code that comes with this architecture gives us a template that makes it easier to read code, debug, and integrate new features. In this article we’ll focus on this implementation of the Clean Architecture : Asynchronism Whichever solution you decide to use to handle asynchronism, one thing is for sure : we switch to a background thread out of the Display, and back to the Main Thread out of the Presenter. This way, only the Display’s code is executed on the Main Thread. As it doesn’t contain any logic, the user experience is as good as possible. All heavy computation is done in the background. Sample Application I’ll use a simple application to present my experiments with actors. This application will consist of a counter to which you can increment or decrement. Note that the Clean Architecture will look way overkill for this example. I did not want to make things more complicated than what they needed to be to present actors. Interfaces and basic implementation explanations Controller The Controller is called by the activity whenever the user interacts with the application. Here we handle the clicks on the two buttons. Implementation-wise, we do not handle any type of data, so there’s no type mapping needed in the data flow. interface CountController { fun plusOne ()\r\n    fun minusOne ()\r\n} Interactor The Interactor is called by the Controller. It handles the business logic of the app. This is where we’ll store the current counter value. For simplicity sake, we’ll focus on the “display loop” and forget about the repository part of the architecture. In a real implementation we would store the in-memory counter in a repository. interface CountInteractor { fun plusOne ()\r\n    fun minusOne ()\r\n} Presenter After changing the counter value, the Interactor wants to present it. We just need one method for this, and again, because it’s a very simple example, we do not need to map the data passed by the Interactor. interface CountPresenter { fun presentCount ( count: Int )\r\n} Display This method will simply be called by the Presenter to update the TextView value. interface CountDisplay { fun displayCount ( count: Int )\r\n} Clean Actors Now that we’ve seen the architecture that will be used for this example, let’s focus on fitting actors in the application. A Controller As stated earlier, we want to switch to a background thread straight out of the Display. We’ll try to fit actors in the Controller, and see how it goes. We first need to adapt our interface. That’s where the sealed classes play an important part. We can set up our actor so that he receives a certain class of messages : the sealed class. Then, we declare all kind of messages that can be received as subclasses of this sealed class. In this case, we’ve changed the CountController interface from : interface CountController { fun plusOne ()\r\n    fun minusOne ()\r\n} to sealed class CountAction class PlusAction : CountAction () class MinusAction : CountAction () interface CountController {\r\n    fun handle( countAction: CountAction): Boolean\r\n} We just need to communicate with the Controller actor, which gives us the full Controller implementation: class CountControllerImpl(private val interactor: CountInteractor) : CountController { private val actor = actor<CountAction>(CommonPool) { for (event in channel) { when (event) { is PlusAction -> interactor.plusOne()\r\n                is MinusAction -> interactor.minusOne()\r\n            }\r\n        }\r\n    }\r\n    \r\n    override fun handle(countAction: CountAction): Boolean { return actor.offer(countAction)\r\n    }\r\n} Unit Testing One of the Clean Architecture’s main focus is the testability of each component. That is why we do not want the Controller implementation to interfere with this. Let’s quickly see how you can test such a Controller : @RunWith (MockitoJUnitRunner:: class ) class CountControllerImplTest { @Mock private lateinit var interactor: CountInteractor private lateinit var controller: CountControllerImpl @Before fun setUp () {\r\n        controller = CountControllerImpl(interactor, Unconfined)\r\n    } @Test fun testPlusAction () { // When controller.handle(PlusAction()) // Then Mockito.verify(interactor).plusOne()\r\n    } @Test fun testMinusAction () { // When controller.handle(MinusAction()) // Then Mockito.verify(interactor).minusOne()\r\n    }\r\n} Those are basic tests using Mockito. Here I’m just verifying that the Interactor was called. They are kept simple, thanks to how the Controller is set up. Notice the second parameter Unconfined : I had to refactor the Controller constructor to define the actor coroutine’s context. The Unconfined value allows the actor to be executed in the same thread that the one in which he’s created. The actor’s code is now executed synchronously, which allows common testing. The Controller now looks like this: class CountControllerImpl(private val interactor: CountInteractor, context: CoroutineContext = CommonPool) : CountController { private val actor = actor<CountAction>(context) { for (event in channel) { when (event) { is PlusAction -> interactor.plusOne()\r\n                is MinusAction -> interactor.minusOne()\r\n            }\r\n        }\r\n    }\r\n\r\n    override fun handle(countAction: CountAction): Boolean { return actor.offer(countAction)\r\n    }\r\n} I have defined the second parameter with a default value, if omitted, the Controller’s actor will execute in background, thanks to CommonPool . The Activity Now, the Interactor and the Presenter do not have anything particular, the actors do not have any impact on their implementation. But one interesting thing to focus on is the activity, and particularly two things : how the Controller is called, and how the UI is updated. First, since we’ve changed the interface earlier, our button listeners now looks like this : override fun onCreate ( savedInstanceState: Bundle? ) { /* ... */ plusOneButton.setOnClickListener { countController.handle(PlusAction()) }\r\n        minusOneButton.setOnClickListener { countController.handle(MinusAction()) }\r\n    } Then, there’s one issue we’ve not yet tackled on the current implementation. We’ve switched from the Main Thread to the background in the Controller. But now, at the end of the loop, if we try to update a TextView content, an exception will be thrown because we’re trying to update a view from somewhere else than the UI Thread. We need a way to switch back to the UI Thread. override fun displayCount ( count: Int ) {\r\n        launch(UI) {\r\n            numberValue.text = count.toString()\r\n        }\r\n    } The launch(UI){...} block creates and launches a new coroutine in the UI context (remember the CommonPool context in which we launched the computation). This way, all code inside this block is executed on the Main Thread. Capacities One thing that actors offer that I haven’t yet discussed is a capacity. During its construction, we are able to describe how an actor handles its mailbox. Remember that the actor can only handle a message at a time. The basic predicate stating that the order in which the messages have been received will be the same as the one in which they’ll be executed will still be true. But we can change how we handle messages that arrive while the actor is busy computing the previous message. We have four options : First, and default one is the RendezVousChannel : private val actor = actor<CountAction>(CommonPool) { /*...*/ } The actor will not accept any message while being occupied. In our case, if we quickly click on a button twice, only the first click will be handled. Second, the ConflatedChannel : private val actor = actor<CountAction>(CommonPool, capacity = Channel.CONFLATED) { /*...*/ } While being busy, the actor will only remember the last message that it received. Third, the LinkedListChannel : private val actor = actor<CountAction>(CommonPool, capacity = Channel.UNLIMITED) { /*...*/ } Here, we set up the actor with an unlimited buffer (the channel is a linked list). This way, we store every message that are received during computation, and the actor will handle them all. Last, the ArrayChannel : private val actor = actor<CountAction>(CommonPool, capacity = 5 ) { /*...*/ } You may have guessed it already, in this last Channel implementation, the buffer has a limited size: only the 5 last messages will be handled. Conclusion Often, in this implementation of the Clean Architecture, the Controller does not play an important role. It often merely transforms data to a type comprehensible by the Interactor. That’s why I find it to be the best spot in the architecture to handle asynchronism. The actors do not add a lot of complexity to the class, and give an elegant way to tackle this issue. An extra benefit of this implementation is also the flexibility that is offered by the different capacities. You can really adapt the actor behaviour to your needs. Consider that since you may want to have different Controller behaviour on a same screen, you may need to decouple Displays and Controllers. This means multiple Controllers for a Display. You can check Uber’s Riblets Architecture for an example. If you want to check the application code, it’s available on my GitHub here . Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “A Responsive and Clean Android App with Kotlin Actors” Mobile Application Development Company 09/11/2017 à 11:58 Nice. I've been looking for something like this. Was actually in the middle of writing a small background processing library in Kotlin, similar to Rails SideKiq. I'll definitely take a look at this. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-10-23"},
{"website": "Octo", "title": "\n                Better network layer on iOS with synchronous calls            ", "author": ["Ahmed Mseddi"], "link": "https://blog.octo.com/en/better-network-layer-on-ios-with-synchronous-calls/", "abstract": "Better network layer on iOS with synchronous calls Publication date 17/10/2017 by Ahmed Mseddi Tweet Share 0 +1 LinkedIn 0 In most iOS apps, as app developers, we pay a very special attention to the user interface. For example, we try to perfectly implement the graphical design as proposed by the designer. But on the other hand, we pay very little attention to the other parts of the app such as the the data management or the network layer. In this article, I will explain why, during my last project, as we were trying to improve the unit tests of our network calls, we decided to switch our networking calls to synchronous ones and how this produced some desirable side effects. 1. Current networking situation For this article, I will take as an example a simple app displaying a list of posts. The user cam mark some of them as favorites. This app uses an MVP architecture: The Model is composed of two services: PostsService which gets the posts and UserService which can add a post to user favorites and get the user’s favorite posts ids. These two services use a networking library such as Alamofire for the network calls. The View is a passive view displaying the posts in a table view. A Presenter PostsListPresenter which contains the logic behind the view. 1.1. Services Usually, we have two strategies to write asynchronous services calling web endpoints. Either a simple completion callback containing the entity and the error, or two separate callbacks for success and failure: These two strategies work in the real world but have some flaws. For example for the first method it is unclear what should happen when both posts and error are nil. And for the second, we often are too lazy to implement both callbacks so we end up making the failure optional and we stop handling the errors properly. These flaws are not as severe as the real implementation which is often tightly coupled to a network library making them hard to unit test. 1.2. Unit tests In order to have control over the server response we have two options. Either implement a custom NSURLProtocol , register it to intercept the calls and override the responses or use an external library such as OHHTTPStubs . After that, we have to setup an XCTestExpectation to wait for the asynchronous call to respond. And finally we check that the received objects are equal to the expected ones. This gives us the following unit test: This test is not easy to read and not easy to understand for somebody joining the project. In addition if we ask new members to unit test their service call, they are likely not to be rejoiced and may even take it as some kind of hazing. To overcome this complexity, we started thinking how an easy to read unit test should look like. 1.3. Ideal target The complexity of the unit test is caused by the tight coupling to the networking library and the asynchronicity of the service. Thus we had to get rid of these two issues. The simplest unit test does some setup, executes a function and checks the output. So we thought that it would be nice if our unit test looked like this: As you may have noticed, to be able to write the test this way, we will have to make the service return the posts in a synchronous way. 2. Journey to better network calls 2.1. Regaining control To do this, we use the D ependency Inversion from the SOLID principles and introduce a new API protocol that we will be using in the services. The implementation of this protocol can use NSURLSession directly or any other available networking library. The APIRequest object contains all the necessary information to execute the request such as the path and the HTTP method for example. The APIResponse is composed of the received data or the error. 2.2. Implementing the API protocol The principal difficulty here is that all the tools we have at our disposal are asynchronous. So we had to find a way to make asynchronous functions synchronous. And we were lucky to find out that there was a simple way to do this with Apple Dispatch framework using the DispatchGroup class. Here is an example of the API implementation using Alamofire : 2.3. Unit Tests As we use the TDD ( T est D riven D evelopment) methodology, we start by writing the tests and then we write as little code as possible to make the tests pass. In our case we have two types of tests: the first ones will verify that we call the API with the correct APIRequest and the second type will verify the value returned by the service depending on the stubbed server response. To handle the two types of tests we start by writing a MockAPI implementing the API protocol. This mock will retain the request parameter so we could verify it and it returns a response by reading a local file: At this point we also have to think about how will we handle the errors. We chose to use a generic enum Result (similar to the Optional enum) having two cases: a generic success and a failure. With all these elements, we get the following test verifying that the service correctly parses the server response: This is much nicer to read and easier to understand. 2.4. Using the protocol in the services We start by injecting the protocol implementation in the service in the init method. Then in the getPostsList function, we build the request, use it in the API to get the response and finally parse it: 2.5. Using the services in the presenter This step is pretty straightforward but we have to make sure not to block the main thread. So we start by switching to a background thread before calling the different services and we go back to the main thread before updating the view: After this last step, everything should work as before. And in addition to that, our network layer is fully tested and regression-proof. Moving the asynchronicity management to the presenter will eventually complexify its unit tests. This is due to the architecture we choose as an example for this article. If the presenter becomes more complex and difficult to maintain, we could imagine migrating our project to a more decoupled architecture such as VIPER . This way, all the logic is contained in a synchronous interactor which is easy to unit test. And the presenter will only be responsible of calling the interactor and the asynchronicity management. 3. (Desirable) Side effects Developing all our network calls this way has produced some desirable side effects which are listed below. 3.1. Mock environment If the feature in the iOS app is ready before it is available in the backend, we can still show it to our product manager or client for validation purposes. Moreover the files that we used to stub the server’s responses can be used as a reference for the backend development. And we can update them after the backend is ready to check that everything is working as expected. 3.2. Better interface tests and up to date screenshots Having this stubbed API enables us to have better UI tests by making us able to test all the edge cases if we want to. It also makes it easier to have up to date app screenshots if we choose to automate them using fastlane/snaphot . 3.3. Integration tests The synchronous services make it easier to have integration tests that will call the app implementation of the API protocol. This will help us check that we are calling the backend and that it is responding as expected. If it is not possible to test all the endpoints, we can imagine having a critical scenario running regularly on an integration server and checking that there is no major issue. For example: 4. One more thing… Chris Lattner recently published a manifesto on github . His goal was to start a conversation about what features should the Swift language offer to handle concurrency. He proposed an approach consisting of introducing the async and await keywords. An asynchronous function will have the async keyword in its declaration and when we call it we add the await keyword to wait for the response. The publication of this manifesto reassured us about the direction we took when we were trying to improve our network calls and made us look forward to a better concurrency handling in the Swift language. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged automated tests , code quality , iOS , mobile , Networking , Swift , TDD , Testability . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Better network layer on iOS with synchronous calls” Oleg 06/12/2017 à 15:25 It is awesome article! I like it so much\r\nDo you have similar articles about how to organize app architecture? Leandro 09/05/2018 à 17:43 Great approach Ahmed! I arrived to a similar solution and I'm very happy with it. I add a link to this article in an article I wrote about this subject: https://medium.com/codika-solutions/keep-it-synchronous-s-id-a173cb53f77b. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-10-17"},
{"website": "Octo", "title": "\n                How does it work? Docker! Part 5: Get some work(ers) done!            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-docker-part-5-get-some-workers-done/", "abstract": "How does it work? Docker! Part 5: Get some work(ers) done! Publication date 19/10/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey there! TL;DR I hacked another thing together, this time in order to install a highly available Docker Swarm cluster on CoreOS (yeah, Container Linux), using Ansible. The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts. This is episode 5, regarding the actual implementation of the local cluster’s Worker nodes, using Vagrant, CoreOS and Ansible. If you want to try it: git clone https://github.com/sebiwi/docker-coreos.git cd docker-coreos make up You will need Ansible 2.2+, Docker, Vagrant and Molecule Where are my Workers man? Right, workers. Same procedure as before: main, configure and test. In order to configure the Worker node, we will first check if Swarm mode is already activated, like we did on the previous roles. If it is, we won’t do squat. If it isn’t, we will go fetch the token needed to join the cluster as a Worker node from the Leader node, and then join the cluster as a Worker node. You should know that the token used in order to join the cluster as a Worker node is different from the one user to join it as a non-Leader Manager node. So: --- - name: Check if Swarm Mode is already activated command: docker info register: docker_info changed_when: false - name: Recover Swarm Leader token shell: docker swarm join-token worker | grep token | cut -d ' ' -f 6 register: worker_token when: \"'Swarm: active' not in docker_info.stdout\" delegate_to: \"{{ groups['swarm-leader'][0] }}\" - name: Join Swarm Cluster as Worker command: docker swarm join --token {{ worker_token.stdout }} {{ hostvars[groups['swarm-leader'][0]]['ansible_env']['COREOS_PUBLIC_IPV4'] }} when: \"'Swarm: active' not in docker_info.stdout\" Once again, the delegate_to flag is needed in order to actually recover the token from the Leader node itself. Then, for the test part: --- - name: Check if node is Worker shell: docker node ls | grep {{ ansible_hostname }} register: docker_info changed_when: false delegate_to: \"{{ groups['swarm-leader'][0] }}\" - name: Fail if node is not Worker assert: that: - \"'Reachable' not in docker_info.stdout\" - \"'Leader' not in docker_info.stdout\" - \"'Active' in docker_info.stdout\" This is quite similar to the previous test, in the way that first, information is collected form the Leader node. Then, the assertions are slightly different: we will test that the node is not a Leader nor a non-Leader manager node, therefore asserting that it is a Worker node, and then we will test if the node is Active, since we want our Worker to work, by running containers. That’s why they’re Workers, right? Once we’re done with that, we’ll just add our newly created role to the swam.yml file: - name: Create Swarm Worker nodes hosts: swarm-worker roles: - role: configure/swarm-worker tags: [ swarm-worker ] Nothing special right here, just use the previously defined swarm-worker group and you’re all set. Did it work then? Sure it did. We tested manually. Oh right, we also did that molecule thingy at the beginning, didn’t we? Now, testing is something that should be done at every step of the way, but I’ll just show you now that everything we coded works, is idempotent and it’s syntax is valid. Use the test target from the Makefile: make test This will launch the whole molecule testing pipeline, first checking if the virtual machines are already created, and checking the validity of the playbook’s syntax as well: $ ansible  sebiwi  ~  stuff  docker-coreos   master  molecule test --> Destroying instances... --> Checking playbook's syntax… If it is, it will then proceed to create the instances: playbook: swarm.yml --> Creating instances... Bringing machine 'swarm-manager-01' up with 'virtualbox' provider... Bringing machine 'swarm-manager-02' up with 'virtualbox' provider... Bringing machine 'swarm-manager-03' up with 'virtualbox' provider... Bringing machine 'swarm-worker-01' up with 'virtualbox' provider... Bringing machine 'swarm-worker-02' up with 'virtualbox' provider... Bringing machine 'swarm-worker-03' up with 'virtualbox' provider... Once they are up, the playbook itself will be launched on the newly created infrastructure: --> Starting Ansible Run... PLAY [Bootstrap coreos hosts] ************************************************** TASK [bootstrap/ansible-bootstrap : Check if Python is installed] ************** fatal: [swarm-manager-01]: FAILED! => {\"changed\": false, \"failed\": true, \"rc\": 127, \"stderr\": \"Warning: Permanently added '[127.0.0.1]:2222' (ECDSA) to the list of known hosts.\\r\\nShared connection to 127.0.0.1 closed.\\r\\n\", \"stdout\": \"/bin/sh: /home/core/bin/python: No such file or directory\\r\\n\", \"stdout_lines\": [\"/bin/sh: /home/core/bin/python: No such file or directory\"]} ...ignoring If everything works fine, another idempotence test will be executed, which will just verify if there are any changes when the playbook is ran using the –dry-run option: PLAY RECAP ********************************************************************* swarm-manager-01           : ok=15   changed=5    unreachable=0    failed=0 swarm-manager-02           : ok=16   changed=6    unreachable=0    failed=0 swarm-manager-03           : ok=16   changed=6    unreachable=0    failed=0 swarm-worker-01            : ok=16   changed=6    unreachable=0    failed=0 swarm-worker-02            : ok=16   changed=6    unreachable=0    failed=0 swarm-worker-03            : ok=16   changed=6    unreachable=0    failed=0 --> Idempotence test in progress (can take a few minutes)... --> Starting Ansible Run... Idempotence test passed. Finally, ansible-lint is executed in order to verify the playbook style and usage of deprecated tasks/options, and then the infrastructure is destroyed: --> Executing ansible-lint... --> Destroying instances... ==> swarm-worker-03: Forcing shutdown of VM... ==> swarm-worker-03: Destroying VM and associated drives... ==> swarm-worker-02: Forcing shutdown of VM... ==> swarm-worker-02: Destroying VM and associated drives... ==> swarm-worker-01: Forcing shutdown of VM... ==> swarm-worker-01: Destroying VM and associated drives... ==> swarm-manager-03: Forcing shutdown of VM... ==> swarm-manager-03: Destroying VM and associated drives... ==> swarm-manager-02: Forcing shutdown of VM... ==> swarm-manager-02: Destroying VM and associated drives... ==> swarm-manager-01: Forcing shutdown of VM... ==> swarm-manager-01: Destroying VM and associated drives... Now, what I basically do when running these tests is that I run just the `molecule create` to create the infrastructure, and then I’ll just run `molecule converge` to test that my roles are working properly and `molecule idempotence` to verify that they are indeed idempotent. This helps reducing the duration of the feedback loop, which in turn helps me to develop faster. Just remember to launch the whole pipeline from time to time to check if your roles are able to correctly configure newly-created infrastructure. Let’s play! So if you followed all the steps correctly you should have a working Swarm cluster by now. Congratulations! Let’s see what it is capable of. First, the smallest schedulable unit of work in a Swarm cluster is not a container, but a service. Let us create one of those on the leader node: docker service create --replicas 1 --name redis --update-delay 10s redis:3.0.6 This says that we want to create a service, with one replica, with the name of redis, with a 10 second update delay, using the 3.0.6 version of the redis image. The update delay is the time between updates of tasks (containers) of a service. This means that the tasks will be updated one at the time, with a 10 second delay between them. You can then list your services using the `ls` command: core@swarm-manager-01 ~ $ docker service ls ID            NAME   REPLICAS  IMAGE        COMMAND 09j27f6ehaq6  redis  0/1       redis:3.0.6 And see information regarding the different tasks of the service using the `ps` command, with the service name: core@swarm-manager-01 ~ $ docker service ps redis ID                         NAME     IMAGE        NODE             DESIRED STATE  CURRENT STATE           ERROR 06cj3g824k8r0jjpoew0uip7z  redis.1  redis:3.0.6  swarm-worker-03  Running        Running 42 seconds ago You can see the image, the desired state, the current state and the node in which the container is running. You can also scale up/down your services, using the `scale` command: core@swarm-manager-01 ~ $ docker service scale redis=11 redis scaled to 11 This scales your nodes up to 11 . Sick! core@swarm-manager-01 ~ $ docker service ps redis ID                         NAME          IMAGE        NODE             DESIRED STATE  CURRENT STATE            ERROR 06cj3g824k8r0jjpoew0uip7z  redis.1       redis:3.0.6  swarm-worker-03  Running        Running 8 minutes ago 5wa862swszkvklchaug02powy  redis.2       redis:3.0.6  swarm-worker-02  Running        Running 25 seconds ago 67w0vlk9v7gh9h5qgwmsnjgya  redis.3       redis:3.0.6  swarm-worker-01  Running        Running 25 seconds ago 3ws3a9xwt1h4r962gg8htiun8  redis.4       redis:3.0.6  swarm-worker-03  Running ... You can use the same command with a different number in order to scale down (to 1, for exampe). Let’s try to update a service in order to see the rolling updates work. We’re going to go from redis version 3.0.6 to 3.0.7. Exciting, huh? For this, we will use the `update` command: docker service update --image redis:3.0.7 redis This will launch the rolling update process. It will take some time due to the update delay we set before. If you launch a `ps` command on the service, you should be able to see your containers updating: core@swarm-manager-01 ~ $ docker service ps redis ID                         NAME          IMAGE        NODE             DESIRED STATE  CURRENT STATE            ERROR ... e8lf3q9ic8674fba8a863ciwh  redis.5       redis:3.0.7  swarm-worker-02  Running        Running 22 seconds ago 4b7wmnhc6iqod481ge5njvw7o   \\_ redis.5   redis:3.0.6  swarm-worker-01  Shutdown       Shutdown 29 seconds ago ... 985dkagqrz8n40hke704an0pk  redis.10      redis:3.0.6  swarm-worker-01  Running        Running 3 minutes ago 0k4pn77hy4s6e3g778gohktnh  redis.11      redis:3.0.7  swarm-worker-01  Running        Running 3 seconds ago 4rn0r67hc8uscl0lq7kvx64t5   \\_ redis.11  redis:3.0.6  swarm-worker-01  Shutdown       Shutdown 11 seconds ago This should happens with every node eventually. You can see the service status if you use the `inspect` command on it: core@swarm-manager-01 ~ $ docker service inspect --pretty redis ID:             buye01j0ofdmt32lqplgvknic Name:           redis Mode:           Replicated Replicas:      11 Update status: State:         updating Started:       2 minutes ago Message:       update in progress Placement: UpdateConfig: Parallelism:   1 Delay:         10s On failure:    pause ContainerSpec: Image:         redis:3.0.7 Resources: Once it’s done, you should be able to see the `completed` state on the same inspection: Update status: State:         completed Started:       3 minutes ago Completed:     51 seconds ago Message:       update completed Afterwards, when you’re bored with it, you can delete it using the `rm` command: docker service rm redis What about the Routing Mesh? Let’s try to expose a port. We’ll launch an nginx service with two replicas, and then we’ll try to access it on the node with no container workload. This way, we will see if the request is routed all the way to the corresponding backend, even when the backend is not hosted on the accessed node. Just a little reminder: when you expose a service using the Routing Mesh, you map it to a certain port, and the every node in the cluster listens on that port and routes the request all the way to the containers. So: docker service create --name amazing-web-server --publish 8080:80 --replicas 2 nginx By doing this, we will map the 8080 port on all nodes to the 80 port inside the containers. Let us then see where our containers are running: core@swarm-manager-01 ~ $ docker service ps amazing-web-server ID                         NAME                  IMAGE  NODE             DESIRED STATE  CURRENT STATE                   ERROR 6viw0duiqjobqwlajs8flrbk1  amazing-web-server.1  nginx  swarm-worker-03  Running        Running less than a second ago 8vmfut5b34e04h84ojvyaeb30  amazing-web-server.2  nginx  swarm-worker-02  Running        Running less than a second ago We can see that they are running on swarm-worker-02 (10.0.0.122) and swarm-worker-03 (10.0.0.123). So, if we try to access 10.0.0.122:8080: This isn’t that amazing Cool, that works. What if we try to access swarm-worker-01 (10.0.0.121) though? This is pretty rad Now, you still need a reverse-proxy or a load-balancer in order to forward requests to the right Swarm node in order to access the right service, but still, the ease of use and effectiveness of the system is undeniable. What about node failover? Let us find out! Let us kill the Leader node first, to see what happens: vagrant destroy swarm-manager-01 --force No more Leader. Access the second manager node and see what’s going on: vagrant ssh swarm-manager-02 core@swarm-manager-02 ~ $ docker node ls ID                           HOSTNAME          STATUS   AVAILABILITY  MANAGER STATUS 1qmj079wp0cg5kys5ej8cs58i    swarm-worker-02   Ready    Active 3qeyfmoixwg7k64i6sw78gmms    swarm-worker-01   Ready    Active 3rgjfac5qau5rft2wpcpliaek *  swarm-manager-02  Ready    Drain         Leader 6ok8wzq137dxs7uow5xd3rjkd    swarm-manager-01  Unknown  Drain         Unreachable 9rarjje9gner4lwrcshymtszm    swarm-manager-03  Ready    Drain         Reachable a1lsaawsrcoohajmc1luon0mn    swarm-worker-03   Ready    Active So, swarm-manager-02 became the Leader. Sweet! Before, we saw that the nginx containers were running on swarm-worker-02 and swarm-worker-03. Now, we will destroy both nodes to see what happens: vagrant destroy swarm-worker-02 swarm-worker-03 –force If we check the service status: core@swarm-manager-02 ~ $ docker service ps amazing-web-server ID                         NAME                      IMAGE  NODE             DESIRED STATE  CURRENT STATE                ERROR 95u3ndb01onpx6ki5daohuwf1  amazing-web-server.1      nginx  swarm-worker-01  Running        Running about a minute ago 3zx3cua2eu1dgykphj8rsnuwd   \\_ amazing-web-server.1  nginx  swarm-worker-02  Shutdown       Running 7 minutes ago aff0ashfqbcjxbka0jz6sbril  amazing-web-server.2      nginx  swarm-worker-01  Running        Running about a minute ago 8va90yo2vn2uwvzjrc22d0pjv   \\_ amazing-web-server.2  nginx  swarm-worker-03  Shutdown       Running 7 minutes ago We can see that the containers running on swarm-worker-02 and swarm-worker-03 are in ‘Shutdown’ state, and that there are two new running containers on swarm-worker-01. What a time to be alive! Final thoughts This whole thing was fun. I (and hopefully you too) learned/noticed some things along the way: Swarm Standalone and Swarm Mode are two (very) different things. The latter is already integrated in the Docker Engine after v1.12. The Engine is divided into many different pieces as of today, including RunC and containerd, which are used to run the containers themselves and manage their lifecycle. The whole “let’s divide the Engine into little independent pieces” seems to have paid off, since every independent component is now evolving with its own lifecycle, and furthermore, it helps understanding the Engine as a whole. Docker uses a different container networking standard than Kubernetes, called CNM, with its own abstractions and resources. The Network Model, including CNM and the Network Routing Mesh seems easier to understand than the whole Kubernetes, even if it seemed harder to grasp before actually looking at it. I thought it would be hell at first, but it came out okay, kinda. Swarm works quite nicely as of now. It can handle the High Availability natively, and it doesn’t need an external key/value datastore like other solutions do. It is super easy to install. When I say super easy, I mean it’s super super easy. You should compare what we just did to what we had to do for the Kubernetes cluster , and you’ll see that these guys really worked their asses off in order to get the installation procedure right, nice and simple. I would have liked to do some other things as well: Explore the volume management further (or at all). It’s a super interesting subject, and I’m dying to check it out. I’d love to compare it to the way Kube handles them. I didn’t get to deploy complex applications or services on the cluster. No DABs and no stacks . This seemed interesting enough, but I didn’t have enough time to actually try it. I didn’t get to compare the usage of a different key/value datastore instead of the internal one, in terms of performance or ease of use. Run some benchmarks on leader election convergence time. I just know it works, I don’t know how fast it can be, and how does that change when your Manager group increases in size. I might do all of these things in the near future. Or not. Who knows. Anyway. I had a blast, I hope you did too! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “How does it work? Docker! Part 5: Get some work(ers) done!” Karthik Chandrappa 13/08/2018 à 11:33 Hi, Thank you for detailed explanation in 5 this part series. I liked explanations on Network and Route mesh. \r\nIt looks like orchestrators  war is heating up with Kubernetes leading and swarm close behind, in this case what is the future of other tools like Mesos and Hashicorp nomad. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-10-19"},
{"website": "Octo", "title": "\n                How does it work? Docker! Part 4: Control your Swarm!            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-docker-part-4-control-your-swarm/", "abstract": "How does it work? Docker! Part 4: Control your Swarm! Publication date 02/10/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey there! TL;DR I hacked another thing together, this time in order to install a highly available Docker Swarm cluster on CoreOS (yeah, Container Linux), using Ansible. The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts. This is episode 4, regarding the actual implementation of the local cluster’s Manager nodes, using Vagrant, CoreOS and Ansible. If you want to try it: git clone https://github.com/sebiwi/docker-coreos.git cd docker-coreos make up You will need Ansible 2.2+, Docker, Vagrant and Molecule Code, please! Yay, code! First of all, we will need the actual virtual machines where our cluster will run. We will create these using Vagrant. If you don’t know what Vagrant is, or why are we using it, check out this article . For this deployment, we will have three Manager nodes, and three worker nodes.  Why three Manager nodes you say? It may seem overkill, but in order to have High Availability you need to have an odd number of Manager nodes, otherwise, you will not get consensus from Raft. We will see this in action later on. We do not need an etcd cluster, since the key-value datastore is already included in the internals of the Docker Engine, and therefore, we will not include any machines for it. First of all, I’ll describe the amount of machines I want and their configurations as variables: # General cluster configuration\r\n$swarm_manager_instances = 3\r\n$swarm_manager_instance_memory = 2048\r\n$swarm_manager_instance_cpus = 1\r\n$swarm_worker_instances = 3\r\n$swarm_worker_instance_memory = 2048\r\n$swarm_worker_instance_cpus = 1 Afterwards, I’ll specify that I want to use the CoreOs (Container Linux) Vagrant box, from an URL: # Box management: CoreOS\r\nconfig.vm.box = \"coreos-stable\"\r\nconfig.vm.box_url = \"https://storage.googleapis.com/stable.release.core-os.net/amd64-usr/current/coreos_production_vagrant.json\" Just a little reminder, Container Linux is a lightweight Linux distribution that uses container to run applications. It ships with basic GNU utilities so you can do all your business, and some other interesting things, like kubelet , Docker , etcd and flannel . We’ll only be using Docker for this part. In short, CoreOS’s Container Linux is an OS specially designed to run containers, and we’re going to profit from that in our context. If you wanna know more about CoreOS, check this article . We’ll configure our Manager nodes with the variables we previously defined: # Swarm manager instances configuration\r\n  (1..$swarm_manager_instances).each do |i|\r\n    config.vm.define vm_name = \"swarm-manager-%02d\" % i do |master|\r\n      # Name\r\n      master.vm.hostname = vm_name\r\n\r\n      # RAM, CPU\r\n      master.vm.provider :virtualbox do |vb|\r\n        vb.gui = false\r\n        vb.memory = $swarm_manager_instance_memory\r\n        vb.cpus = $swarm_manager_instance_cpus\r\n      end\r\n\r\n      # IP\r\n      master.vm.network :private_network, ip: \"10.0.0.#{i+110}\"\r\n    end\r\n  end And then we’ll do the same thing with our Worker nodes: # Swarm worker instances configuration\r\n  (1..$swarm_worker_instances).each do |i|\r\n    config.vm.define vm_name = \"swarm-worker-%02d\" % i do |worker|\r\n      # Name\r\n      worker.vm.hostname = vm_name\r\n\r\n      # RAM, CPU\r\n      worker.vm.provider :virtualbox do |vb|\r\n        vb.gui = false\r\n        vb.memory = $swarm_worker_instance_memory\r\n        vb.cpus = $swarm_worker_instance_cpus\r\n      end\r\n\r\n      # IP\r\n      worker.vm.network :private_network, ip: \"10.0.0.#{i+120}\"\r\n    end\r\n  end Easy. If you wanna take a look at the Vagrantfile, it’s right here. Moving on. Next up, I’ll set up a Makefile with a Vagrant target, which will create all the virtual machines, and generate a configuration file with the SSH configuration needed to interact with them. Ill also add a clean target, which will destroy all the machines, and it will delete the SSH configuration file, if it exists. .PHONY: vagrant clean\r\n\r\nvagrant:\r\n\t@vagrant up\r\n\t@vagrant ssh-config > ssh.config\r\n\r\nclean:\r\n\t@vagrant destroy -f\r\n\t@[ ! -f ssh.config ] || rm ssh.config I’ll also put in a list of phony targets, since the targets previously defined are just actions and don’t really generate files. We’ve got the virtual machines, and the SSH configuration. For the Ansible part, we’ll start by creating an inventory with all our six nodes in it, separated by groups: [swarm-leader]\r\nswarm-manager-01\r\n\r\n[swarm-manager]\r\nswarm-manager-01\r\nswarm-manager-02\r\nswarm-manager-03\r\n\r\n[swarm-worker]\r\nswarm-worker-01\r\nswarm-worker-02\r\nswarm-worker-03 You will probably notice there is a `swarm-leader` group in the inventory, which contains a single host. Like I said in the first article, there might be many Managers in a cluster; Nevertheless, there is only one Leader at any given moment. We will use this group to launch specific actions for the Leader, common actions for all Manager nodes using the swarm-manager group, and actions destined for the non-Leader Manager nodes using the swarm-manager group, and subtracting the swarm-leader group from it. This may seem complex, but it is actually super easy, you will see. No IP configurations over here, we’ll just use the SSH configuration file we generated earlier. In order to do that, we have to specify it on our ansible.cfg file. No ansible.cfg file? Just create it: [defaults]\r\nansible_managed = Please do not modify this file directly as it is managed by Ansible and could be overwritten.\r\nretry_files_enabled = false\r\nremote_user = core\r\n\r\n[ssh_connection]\r\nssh_args = -F ssh.config We’ll also disable retry_files, and specify that we want to use the “core” user when connecting to the machines using SSH. I’ve already said this before, but CoreOS only ships with basic GNU utilities, which means no Python. And no Python means no Ansible, except for the raw module, the script module and the synchronize module. What we’re going to do is that we’re going to install a lightweight Python implementation called PyPy using only those modules, and then use that Python implementation in order to execute the rest of our playbook. Neat huh? We’ll use the same role we used for the Kubernetes provisioning project. If you want to read more about it, like the technical explanation behind it, you can find all the information here . So basically, we’ve got a role now under roles/bootstrap/ansible-bootstrap, which has 3 files under the tasks directory: main.yml, configure.yml and test.yml. The configure.yml file holds all the tasks necessary in order to install PyPy. The test.yml file verifies if Python is correctly installed by doing `python –version`. The main.yml file wraps these two files, adding the `test` tag to the test.yml part: # filename: roles/bootstrap/ansible-bootstrap/tasks/main.yml\r\n---\r\n\r\n- name: Install and configure PyPy\r\n  include: configure.yml\r\n\r\n- name: Test PyPy installation\r\n  include: test.yml\r\n  tags: [ test ]\r\n\r\n- name: Gather ansible facts\r\n  setup: I’ll follow this approach for each role on this project, so each role will have a smoketest, which will be enough to tell us if the component is correctly installed. This is pretty useful in order to test the already deployed infrastructure, as a conformance test, and check for deltas which might need to be corrected. Now that we have our first role, it’s time to create a playbook. Since we’ll be deploying a Swarm cluster, I’ll just name it swam.yml: ---\r\n\r\n- name: Bootstrap coreos hosts\r\n  hosts: all\r\n  gather_facts: false\r\n  roles:\r\n    - role: bootstrap/ansible-bootstrap\r\n      tags: [ ansible-bootstrap ] It’s quite straightforward so far, I’ll just launch the recently created role on each hosts, without gathering facts, since Python is not yet installed on the machines. Facts will be gathered at the end of the role though, as seen on the previous code snippet. Next up, tests. We’ll use molecule for the win. I spoke to you all about molecule on a previous article . It is basically a testing tool for Ansible code. It creates ephemeral infrastructure (either virtual machines or containers), tests your roles on it (not only the execution of the roles, but also the syntax and their idempotence), and then it destroys it. Since there are no CoreOS containers, and Virtualbox virtual machines through Vagrant being the target platform, I’ll just use the Vagrant driver. In order to test with molecule, I’m going to create a molecule.yml file, in which I’m going to define the Ansible files to use for the test, as well as the Vagrant machine’s specification and configuration. First, I’ll specify which Ansible configuration to use, and which playbook to run: ---\r\nansible:\r\n  config_file: ./ansible.cfg\r\n  playbook: swarm.yml Then, I’ll specify which Vagrant box to use for the virtual machines: platforms:\r\n  - name: coreOS\r\n    box: coreos-stable\r\n    config.vm.box_url: https://storage.googleapis.com/stable.release.core-os.net/amd64-usr/current/coreos_production_vagrant.json Then, what will my provider be, and how much physical resources will be used by each instance: providers:\r\n  - name: virtualbox\r\n    type: virtualbox\r\n    options:\r\n      memory: 2048\r\n      cpus: 1 After that, I’ll define the specifics of each instance, including both hostname and IP addresses: instances:\r\n  - name: swarm-manager-01\r\n    ansible_groups:\r\n      - swarm-leader\r\n      - swarm-manager\r\n    interfaces:\r\n      - network_name: private_network\r\n        type: static\r\n        ip: 10.0.0.101\r\n        auto_config: true\r\n    options:\r\n      append_platform_to_hostname: no\r\n  - name: swarm-manager-02\r\n    ansible_groups:\r\n      - swarm-manager\r\n    interfaces:\r\n      - network_name: private_network\r\n        type: static\r\n        ip: 10.0.0.102\r\n        auto_config: true\r\n    options:\r\n      append_platform_to_hostname: no\r\n  - name: swarm-manager-03\r\n    ansible_groups:\r\n      - swarm-manager\r\n    interfaces:\r\n      - network_name: private_network\r\n        type: static\r\n        ip: 10.0.0.103\r\n        auto_config: true\r\n    options:\r\n      append_platform_to_hostname: no\r\n  - name: swarm-worker-01\r\n    ansible_groups:\r\n      - swarm-worker\r\n    interfaces:\r\n      - network_name: private_network\r\n        type: static\r\n        ip: 10.0.0.121\r\n        auto_config: true\r\n    options:\r\n      append_platform_to_hostname: no\r\n... With that in place, I just need to run `molecule test` in order to test that my infrastructure is created and configured correctly. This is actually an oversimplification of everything that can be done using molecule, but since I already wrote about it on a previous article , just can just head there and read about it if you’re really interested. And with that, I also get to add two new (phony) Makefile target: smoketest:\r\n\t@ansible-playbook -i inventories/vagrant.ini swarm.yml --tags test\r\n\r\ntest:\r\n\t@molecule test The smoketest target allows me to run a conformance test on all the already deployed infrastructure, to check for deltas and see if something’s wrong whenever I want, and the test target allows me to test the code on fresh, newly created infrastructure, and to check for Ansible-specific good practices. Remember, this uses Molecule V1, so if you try to run it using Molecule V2 it will probably not work. Tests are set up thusly. Moving on. Manage: Lead and follow Next up, we need to setup the three Manager nodes. I’ll start by creating a swarm-leader role, under the configuration roles directory: roles/\r\n├── bootstrap\r\n│   └── ansible-bootstrap\r\n└── configure\r\n    └── swarm-leader\r\n         └── tasks\r\n             ├── configure.yml\r\n             ├── main.yml\r\n             └── test.yml And for this role, we’ll use the same task division strategy we used before in order to add our smoketest. First, the main.yml file, which is fairly simple, and only includes the other two yaml files, using a tag for the test file: — ---\r\n\r\n- name: Create Manager Leader\r\n  include: configure.yml\r\n\r\n- name: Test Manager Leader\r\n  include: test.yml\r\n  tags: [ test ] The configure file first checks if the cluster is already on Swarm mode. If it is, it doesn’t do anything else. If it isn’t, it creates the first Swarm node, creating thus the Swarm cluster, which will be joined by the subsequent nodes. It also disables scheduling on the Leader, making sure that the Leader does not handle any workload and that it concentrates its resources on leading the cluster: ---\r\n- name: Check if Swarm Mode is already activated\r\n  command: docker info\r\n  register: docker_info\r\n  changed_when: false\r\n\r\n- name: Create Swarm Manager Leader if it is not activated\r\n  command: docker swarm init --advertise-addr {{ hostvars[groups['swarm-leader'][0]]['ansible_env']['COREOS_PUBLIC_IPV4'] }}\r\n  when: \"'Swarm: active' not in docker_info.stdout\"\r\n\r\n- name: Disable Leader scheduling\r\n  command: docker node update --availability drain {{ groups['swarm-leader'][0] }}\r\n  when: \"'Swarm: active' not in docker_info.stdout and disable_leader_scheduling\" This last part is not actually necessary, specially for small clusters. Nevertheless it is usually a good practice, since the leader election process can be really intensive in terms of resource consumption. The `disable_leader_scheduling` variable is defined on the role’s defaults, and you can override it if you want your Leader to handle workloads. Fairly simple. Notice the `changed_when: false` parameter on the first command task. It is there because running  `docker info` will not change the state of the cluster, and it is therefore not a real action, just a way of collecting information. Next, for the smoketest, I’ll verify if the created Manager node is in fact a Leader (which it should be, since it was the first Manager node to be created), and whether its status is “Drain”, since the Leader node is not supposed to handle any workload: ---\r\n\r\n- name: Check if Manager node is Leader\r\n  shell: docker node ls | grep {{ ansible_hostname }}\r\n  register: docker_info\r\n  changed_when: false\r\n\r\n- name: Fail if Manager node is not Leader\r\n  assert:\r\n    that:\r\n      - \"'Leader' in docker_info.stdout\"\r\n      - \"'Active' in docker_info.stdout\" Now that the role is set, I’ll just add it to the swarm.yml playbook: - name: Create Swarm Leader node\r\n  hosts: swarm-leader\r\n  roles:\r\n    - role: configure/swarm-leader\r\n      tags: [ swarm-leader ] Using the host group we discussed earlier, and the proper tag in order to identify the action. And that’s it for the Manager Leader. We need some non-Manager Leader for that High Availability though! So we’ll just repeat the previous process, we’ll create a swarm-manager role up next, with the same structure of the previous role (main.yml, configure.yml, test.yml). I won’t show you the main.yml: it is basically the same one we saw before. The configure.yml file, on the other hand, checks if Swarm mode is activated on the node, the same way the Leader role does, but if it isn’t, it recovers the token needed to join the cluster as a Manager node from the Leader node, and joins the cluster with it. If Swarm mode is already activated, it does nothing: ---\r\n\r\n- name: Check if Swarm Mode is already activated\r\n  command: docker info\r\n  register: docker_info\r\n  changed_when: false\r\n\r\n- name: Recover Swarm Leader token\r\n  shell: docker swarm join-token manager | grep token | cut -d ' ' -f 6\r\n  register: leader_token\r\n  when: \"'Swarm: active' not in docker_info.stdout\"\r\n  delegate_to: \"{{ groups['swarm-leader'][0] }}\"\r\n\r\n- name: Join Swarm Cluster as Manager\r\n  command: docker swarm join --token {{ leader_token.stdout }} {{ hostvars[groups['swarm-leader'][0]]['ansible_env']['COREOS_PUBLIC_IPV4'] }}\r\n  when: \"'Swarm: active' not in docker_info.stdout\"\r\n\r\n- name: Disable Manager scheduling\r\n  command: docker node update --availability drain {{ ansible_hostname }}\r\n  when: \"'Swarm: active' not in docker_info.stdout and disable_manager_scheduling\" Notice the `delegate_to` option on the token recovery task. This needs to be done because the token must be recovered from the Leader node and the Leader node only. Scheduling is also disabled on these nodes, by default, because of the reason specified above on the Leader node. This time, the `disable_manager_scheduling` variable is also defined on the role’s defaults. You can override this variable if you want your Managers to handle workloads. The test file verifies different  things as well: ---\r\n\r\n- name: Check if node is Manager\r\n  shell: docker node ls | grep {{ ansible_hostname }}\r\n  register: docker_info\r\n  changed_when: false\r\n\r\n- name: Fail if node is not Manager\r\n  assert:\r\n    that:\r\n      - \"'Reachable' in docker_info.stdout\"\r\n      - \"'Drain' in docker_info.stdout\" It recovers the nodes information, and then it verifies that the node Manager type is `Reachable` rather than `Leader`, as it was for the Leader node. It also verifies that the nodes are “drained” since we don’t want them to run containers. Finally, once the role is ready, I’ll add it to the swarm.yml file: - name: Create Swarm Manager nodes\r\n  hosts: swarm-manager:!swarm-leader\r\n  roles:\r\n    - role: configure/swarm-manager\r\n      tags: [ swarm-manager ] Notice the `!` sign on the hosts part of the play. This specifies that we want to run the role on every node on the swarm-manager group, that isn’t in the swarm-leader group, thus preventing the Leader node to try to join the cluster as a non-Leader Manager. Sweet! Once we finish all this, we should have everything we need Manager-wise. Time to get some Workers running! I’ll probably talk to you about that on the next article though. Stay in touch! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-10-02"},
{"website": "Octo", "title": "\n                How does it work? Docker! Part 3: Load balancing, service discovery and security!            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-docker-part-3-load-balancing-service-discovery-and-security/", "abstract": "How does it work? Docker! Part 3: Load balancing, service discovery and security! Publication date 21/09/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey there! TL;DR I hacked another thing together, this time in order to install a highly available Docker Swarm cluster on CoreOS (yeah, Container Linux), using Ansible. The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts. This is episode 3, regarding Swarm Service Discovery, Load Balancing and Security. If you want to try it: git clone https://github.com/sebiwi/docker-coreos.git cd docker-coreos make up You will need Ansible 2.2+, Docker, Vagrant and Molecule Why Service Discovery? Right, Service Discovery. From a previous article : “ …What if your application needs to advertise its own IP address to a container that is hosted on another node? It doesn’t actually knows its real IP, since his local IP is getting translated into another IP and a port on the host machine. ” That’s why. The Docker Engine has an embedded DNS server within it, which is used by containers when Docker is not running in Swarm mode, and for tasks, when it is. It provides name resolution to all the containers that are on the host in bridge, overlay or MACVLAN networks. Each container forwards its queries to the Docker Engine, which in turn checks if the container or service is on the same network as the container that sent the request in the first place. If it is, it searches the IP (or virtual IP) address that matches a container, a task’s or a service’s name in its internal key-value store and returns it to the container that sent the request. Pretty cool, huh? As I said before, the Docker Engine will only return an IP address if the matching resource is within the same network as the container that generated the request. What is also cool about this is that Docker Hosts only store the DNS entries that belong to networks  in which the node has containers or tasks. This means that they will not store information that’s irrelevant to them practically, or that other containers do not need to know. Discover what’s up For example, in this image, there is a network called mynet. There are two services running on the network: myservice, and client. myservice has two tasks associated to it, whereas client only has one. Client then executes a curl request to myservice, and therefore, it also does a DNS request. The container built-in Resolver forwards the query to the Docker Engine’s DNS server. The request to myservice is then resolved to the 10.0.0.3 virtual IP. This information is then forwarded back to client. What if client requests something that’s not in the internal key-value store? For example, if client does a curl request for the sebiwi.github.io domain, the same flow will the triggered. The DNS query is forwarded by the resolver to the DNS server. Since the sebiwi.github.io name is not present in the key-value store (which in turn means that it is not a service within the network), the Docker Engine will forward the request to its default configured DNS server. This methodology seems quite logical and simple, but it is only possible due to the existence of a key-value store integrated with the Docker Engine. And Load Balancing? Yeah, there’s native load balancing too! There are basically two types of load balancing: internal and external. Internal is all about load balancing requests that are made from within the Swarm cluster (from other containers), whereas external load balancing targets ingress traffic that enters a cluster. Both of these functionalities are provided by the Docker Engine itself. Let’s talk about internal first. This feature is automatically enabled once a Service is created. So when a Service is created, it get a virtual IP address right away, on the Service’s network. As we said before in the Service Discovery part, when a Service is requested the resulting DNS query is forwarded to the Docker Engine, which in turn returns the IP of the service, a virtual IP. Traffic sent to that virtual IP is load balanced to all of the healthy containers of that service on the network. All the load balancing is done by Docker, since only one entry-point is given to the client (one IP). Things change slightly when doing external load balancing. First of all, the load balancing is not activated by default, but rather when you expose a service using the –publish flag at creation or update time. When this happens, every node in the cluster starts listening on the published port. This means that every node can respond to a request for the service mapped onto that port. What is really interesting is what happens when a node receives a request, but it does not have an instance of the container within it. Since Docker 1.12 (same version that integrated Swarm Mode to the Docker Engine), there is a feature called Routing Mesh, which uses IP Virtual Servers (ipvs) and iptables in order to load balance requests in layer 4. Basically, ipvs implements layer 4 load balancing functionalities on the Linux Kernel, which allows to redirect requests for TCP/UDP-based services to the real backends (containers in this case). In Swarm’s specific case, every node listens on the exposed port, and then forwards the request to the exposed service’s VIP, using a special overlay network called ingress. This overlay network is only used when transporting external traffic to the requested services. In this scope, the same internal load balancing strategies described in the previous section are used. Balance that load In this picture, a service is created with two replicas, on the appnet overlay network. We can see that the service is exposed on port 8000 on the three nodes. This is great, because traffic destined for app can be forwarded to any node. In this case, there is an external load balancer, that just happens to forward the request to the only node that does not have an instance of the service. This request is handled and forwarded by the IPVS on the third node, which redirects it to one of the actual containers on the cluster for that service, using the ingress network and therefore the aforementioned method of load balancing. Neat. Just for the record, when using Docker Enterprise Edition’s Universal Control Plane, this Routing Mesh is also capable of routing layer 7 traffic, by inspecting the HTTP header of requests, therefore operating at an application level. This is actually another feature of Docker Swarm, called HTTP Routing Mesh, or HRM, which allows each created service to be accessed through a DNS label. When using HRM, the normal Routing Mesh is used as well.  Every HTTP/1.1 TCP request contains a Host Header. At service creation time, the desired label must be specified. When a service is created using the io.github.sebiwi.ucp.mesh.http label, the HRM routes all requests with the Host field specified in the previously defined label to the VIP of the service. This enables direct access to the service under the form of a hostname, so theoretically it is the simplest way to expose your service to the internet. You won’t need an external load balancer in order to forward traffic to the right port at ingress. That’s pretty cool. Where’s the security though? Security is implemented by means of isolation and encryption. The isolation part works as follows: every network is segmented from each other to prevent all traffic between them. This provides actual layer 3 separation. The Docker Engine also manages host firewall rules which prevent access between different networks and which also manage ports for containers. Since all of this is managed by the Docker Engine itself, it changes dynamically according to tasks, services and networks that are created inside of the cluster. Traffic generated from inside containers to outside networks is allowed, and so are responses generated from this traffic. Ingress traffic is denied by default, and is only accepted through exposing service on ports, using the previously described methods. Let us talk about encryption. All the control plane traffic between nodes is secured through TLS. All managers and nodes have signed certificates in them, which are created automatically by Swarm and are rotated automatically as well. For data plane traffic, all traffic is encrypted using IPSec tunnels when leaving the source container, and it is decrypted once it arrives to the destination container. This guarantees security even when you do not fully control the underlying network infrastructure. No one will ever know In the picture, when container 1 sends traffic to container 2, everything is encrypted on the way out, and then it is decrypted once it arrives to host B, before entering the destination container. The Swarm leader periodically regenerates symmetric keys for IPSec, and it distributes them to all the cluster nodes. Docker Enterprise Edition’s Universal Control Plane also has advanced security options, like role based access control, for example. I won’t discuss these further, since I haven’t actually tried them myself. Anyways, that’s all for today. Next time, we will start coding stuff! Stick around, we’ll be right back! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 3 commentaires sur “How does it work? Docker! Part 3: Load balancing, service discovery and security!” Sk1f3r 28/01/2018 à 22:10 Thanks! Awesome article. Donald K 27/03/2018 à 14:37 Hi,\r\n \r\n           Would like to find out Docker Swarm uses IPVS in which mode?\r\n\r\n1)  IPIP. Encapsulates IP and is routable anywhere. The response skips the load balancer, so only the request goes through IPVS.\r\n2)  DNAT. Rewrites DST IP, uses the same L4 and behaves like a NAT, rewriting the ip packets and forwarding request and response traffic.\r\n3)  DSR. Rewrites DST MAC to forward at L2.\r\n\r\nThank you very much.\r\n\r\nDK quang vu 05/08/2019 à 12:34 I am having some problem with load balancing. how can I edit DNS Server, where to put it? Do I need to use a separate DNS server and in container I user choose DNS is DNS server I have created. Please answer me. Thanks! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-09-21"},
{"website": "Octo", "title": "\n                How does it work? Docker! Part 2: Swarm networking            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-docker-part-2-swarm-networking/", "abstract": "How does it work? Docker! Part 2: Swarm networking Publication date 04/09/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey there! TL;DR I hacked another thing together, this time in order to install a highly available Docker Swarm cluster on CoreOS (yeah, Container Linux), using Ansible. The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts. This is episode 2, regarding Swarm networking. If you want to try it: git clone https://github.com/sebiwi/docker-coreos.git cd docker-coreos make up You will need Ansible 2.2+, Docker, Vagrant and Molecule So what about networking then? Right, networking. From a previous article on Kubernetes Networking : “For each container that is created, a virtual ethernet device is attached to this bridge, which is then mapped to eth0 inside the container, with an ip within the aforementioned network range. Note that this will happen for each host that is running Docker, without any coordination between the hosts. Therefore, the network ranges might collide. Because of this, containers will only be able to communicate with containers that are connected to the same virtual bridge. In order to communicate with other containers on other hosts, they must rely on port-mapping. This means that you need to assign a port on the host machine to each container, and then somehow forward all traffic on that port to that container. What if your application needs to advertise its own IP address to a container that is hosted on another node? It doesn’t actually knows its real IP, since his local IP is getting translated into another IP and a port on the host machine. You can automate the port-mapping, but things start to get kinda complex when following this model.” That’s why Kubernetes chose simplicity and skipped the dynamic port-allocation deal. It just assumes that all containers can communicate with each other without Network Address Translation (NAT), that all containers can communicate with each node (and vice-versa), and that the IP that a container sees for itself is the same that the other containers see for it” Docker doesn’t do this. They just went nuts and decided to go crazy on the dynamic port-forwarding part. For this, they relied heavily on the existing Linux kernel’s networking stack. This is fairly cool, since the existing Linux networking features are pretty mature and robust already. You there? In order to provide its networking, Docker uses numerous Linux networking tools as building blocks to handle all of its forwarding, segmentation and management needs. Primarily, the most used tools are Linux bridges, network namespaces, virtual ethernet devices and iptables. A Linux bridge is a virtual implementation of a physical switch inside of the Linux kernel. It forwards traffic basing itself on MAC addresses, which are in turn discovered dynamically by inspecting traffic. A network namespace is an isolated network stack with its own collection of interfaces, routes and firewall rules. Network namespaces are used to provide isolation between processes, analog to regular namespaces They ensure that two containers, even if they are on the same host, won’t be able to communicate with each other unless explicitly configured to do so. Virtual ethernet devices or veth are interface that act as connections between two network namespaces. They have a single interface in each namespace. When a container is attached to a Docker Network, one end of the veth is placed inside the container under the name of ethx, and the other is attached to the Docker Network. Iptables is a package filtering system, which acts as a layer 3/4 firewall and provide packet marking, masquerading and dropping features. The native Docker Drivers use iptables in heavy amounts in order to do network segmentation, port mapping, mark traffic and load balance packets. Now that you know all that, let’s talk about models. Give me some CNM! I talked to you about the Container Network Interface (CNI) when talking about Kubernetes on a previous article . Docker uses a different standard, called the Container Network Model (CNM) which is implemented by Docker’s libnetwork . There are three main components of the CNM model: Sandboxes, Endpoints and Networks. The Sandbox contains the configuration of the container’s network stack, such as interface management, IP and MAC addresses, routing tables and DNS settings. A Sandbox may contain endpoints from multiple networks. The Endpoint connects a Sandbox to a Network. An Endpoint can belong to only one Network, and one Sandbox. It gives connectivity to the services that are exposed in a Network by a container. The Network is a collection of Endpoints that are able to talk to each other. A Network consists of many endpoints. Each one of these components has an associated CNM object on libnetwork and a couple of other abstractions that allow the whole thing to work together nicely. The NetworkController object exposes an API entrypoint to libnetwork, which users (like the Docker Engine) use in order to allocate and manage Networks. It also binds a specific driver to a network. The Driver object , not directly visible to the user, makes Networks work in the end. It is configured through the NetworkController. There are both native Drivers (such as Bridge, Host, None, Overlay, and MACVLAN) and remote (from plugin providers) that can be used for different situations depending on your needs. Basically the Driver owns a network and handles all of its management. The Network object is an implementation of the Network component. It is created using the NetworkController. The corresponding Driver object will be notified upon its creation, or modification. The Driver will then connect Endpoints that belong to the same Network, and isolate those who belong to different ones. This provided connectivity can span many hosts, therefore, the Network object has a global scope within a cluster. The Endpoint object is basically a Service Endpoint. A Network object provides an API to create and manage Endpoints. It can only be attached to one Network. It provides connectivity from and to Services provided by other containers in the Network. They are global to the cluster as well, since they represent a Service rather than a particular container. The Sandbox object , much like the component described above, represents the configuration of the container’s network stack, such as interface management, IP and MAC addresses, routing tables and DNS settings. It is created when a user requests an Endpoint creation on a Network. When this happens, the Driver in charge of the Network allocates the necessary network resources, such as an IP address, and passes that information, labeled as SandboxInfo back to libnetwork, which will in turn use the specific OS tools to create the network configuration on the container that correspond to the previously mentioned Sandbox. A Sandbox object may have multiple Endpoints, and therefore, may be connected to multiple Networks. Its scope is local, since it is associated to a particular container on a given host. As I said earlier, there are two basic type of Drivers: native and remote. Native Drivers don’t require any extra modules and are included in the Docker Engine by default. Native Drivers include: Host, Bridge, Overlay, MACVLAN and None. When using the Host driver , the container uses the Host’s network stack, without any namespace separation, and while sharing all of the host’s interfaces. The Bridge driver creates a Docker-managed Linux bridge on the Docker host. By default, all containers created on the same bridge can talk to each other. The Overlay driver creates an overlay network that may span over multiple Docker hosts. It uses both local Linux bridges and VXLAN to overlay inter-container communication over physical networks. The MACVLAN driver uses the MACVLAN bridge mode to establish connections between container interfaces and parent host interfaces. They can be used to assign IP addresses that are routable on physical networks to containers. The None driver gives a container its own network stack and namespace, without any interfaces. Therefore, it stays isolated from every other Network, and even its own host’s network stack. Remote drivers are created either by vendors or the community. The Remote Drivers that are compatible with CNM are contiv , weave , calico (which we used on our Kubernetes deployment!) and kuryr . I won’t be talking about these since we will not be using them. Different networks drivers have different scopes. We will talk about overlay networks, since they hold a “swarm” scope, which means that they have the same Network ID through the cluster, and which is what we wanted to explain in the first place. Overlay Networks! How come you don’t need a key-value datastore, you say? It’s all because of Docker’s Network Control Plane. It manages the state of Docker Networks within a Swarm cluster, while also propagating control-plane data. It uses a gossip protocol to propagate all the aforementioned information. It is scoped by Network, which is quite cool since it dramatically reduces the amount of updates a host receives. Control that cluster It is built upon many components that work together in order to achieve fast convergence, even in large scale clusters and networks. Messages are passed in a peer-to-peer fashion, expanding the information in each exchange to an even larger group of nodes. Both the intervals of transmission and the size of the peering groups are fixed, which helps keeping the network usage in check. Network failures are detected using hello messages which helps to rule out both link congestion and false node failures. Full state syncs are done often, in order to achieve consistency faster and fix network partitions. Also, topology-aware algorithms are used in order to optimise peering groups using relative latency as criteria. Overlay networks rely heavily on this Network Control Plane. It uses standard VXLAN to encapsulate container traffic and send it to other containers. Basically, VXLAN is an encapsulation format that wraps Layer 2 segments with an IP/UDP header, and then send it over Layer 3 networks. In this case, the Layer 2 frames come from a container. This “underlay” header provides transportation between hosts on the underlay network, while the overlay is the stateless VXLAN tunnel, that exists as point-to-multipoint connections between each host participating in the overlay network. Like this For example, in the previous diagram: c1 is sending c2 packets through a shared overlay network. What happens is that c1 does a DNS lookup for c2. They are both on the same overlay network, so the Docker Engine local DNS service resolves c2 to 10.0.0.3, its overlay address. C1 generates an L2 frame destined for the MAC address of c2. The overlay network driver then encapsulates the frame with a VXLAN header, with the physical address of host-B, which he knows by knowing the state and location of every VXLAN tunnel endpoint through the control plane. The packet is then sent and routed as a normal packet using the physical network. Finally, the packet is received by host-B, decapsulated by the overlay network driver, and passed on to c2. This whole process seriously resembles the SDN protocol implemented by flannel , described on the Kubernetes networking part of its “How does it work” series, with flanneld replacing the overlay network driver. Neat, huh? Great minds think alike. What about the driver itself? It automates all the VXLAN configuration needed for an overlay network. When creating the network, the Docker Engine creates the necessary infrastructure on each host. A Linux bridge is created for each overlay network, with its associated VXLAN interfaces. The overlay is only instantiated on hosts when a container that belongs to the network is scheduled on the host, preventing unnecessary spreading of the overlay networks when they are not needed. When a container is created, at least two network interfaces are created inside of it: one that connects it to the overlay bridge, and the other to the docker_gwbridge. The overlay bridge is the ingress/egress point to the overlay network that the VXLAN encapsulates. It also extends the overlay across all the hosts that participate in this particular overlay. There is one per overlay subnet on each host, with the same name as the overlay network. The docker_gwbridge is the egress bridge for all the traffic that leaves the cluster. There is only one docker_gwbridge per host. Container-to-container traffic flows do not go through this bridge. As I said before, overlay networks span multiple Docker hosts. They can be either managed by a Swarm cluster on Swarm Mode, or without it. Nevertheless, when you’re not using Swarm Mode, you will need a valid key-value store service ( etcd , Consul or zookeeper ) in order for the network to work properly. This mode of operation is not encouraged by Docker, and it might even be deprecated in the future (which further discredits the whole “we’re not deprecating Swarm Standalone” argument). Cool stuff That’s all for the networking part. I’ll talk to you about service discovery, load-balancing and security on the next one. Don’t go anywhere! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “How does it work? Docker! Part 2: Swarm networking” Sk1f3r 28/01/2018 à 16:42 Excellent series of articles.\r\nThank you! Subhasis Mohapatra 05/11/2018 à 10:19 Good one. Thank you. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-09-04"},
{"website": "Octo", "title": "\n                Android library development best practices guide            ", "author": ["Rémi Pradal"], "link": "https://blog.octo.com/en/android-library-development-best-practices-guide/", "abstract": "Android library development best practices guide Publication date 13/09/2017 by Rémi Pradal Tweet Share 0 +1 LinkedIn 0 As Android developers, we are used to having to integrate many libraries in our applications. It can be from quite small .jar files to huge .aar archives which embed multiple screens. Sometimes the integration of these libraries goes smoothly but in some cases, it can be quite painful and lead to the addition of some “hacks” into the app in order to integrate it properly. In some other cases, it can also lead to time-consuming exchanges between the person who integrates the library and the one who developed it. In this article, I will try to give Android library developers a few tips to avoid common pitfalls you might fall into. It can be of great benefit to you, If you are developing an open source library : people will tend to not use your work if they find it hard to integrate in their own project. If you are developing a private library in a business context, you and the team which will have to integrate the library will lose a lot of time (and money) if the integration is too complicated. I will not develop in this article some good practice aspects which are “obvious” and are generally followed by any serious Android library developer : clean versioning, good documentation, regular maintenance … Provide a convenient way to retrieve the library file Probably one of the most annoying things when it comes to integrate a new library in a project : discovering it is only possible to integrate the library thanks to a manually downloadable .jar/.aar that the developer has to put in the /libs folder of his project. Here are a few reasons (list not exhaustive!) why, as a library developer, you should avoid delivering libraries by sending directly the artifacts: Your library versioning will be lost : the user will have no other way to know the library version he is using than renaming it. It is obviously error prone. Large files are not meant to be committed in the versioning control system (VCS) of an app. It will increase the overall code base size. Version bumps are painful for the integrator. Indeed to perform the version upgrade the developer will have to check on a pre-established channel if there is a new version, download it and put the new file in the project. It induces friction so the integrating developer will upgrade your library version less often.. That being said, you now understand why it is necessary that you expose your binary files in a way that avoid all the disadvantages listed above : a binary repository manager (often improperly named “maven repository”). Having to remind such a thing might sound commonplace but in my experience, for private libraries, I had to face many situations where the binaries were sent directly by emails! If you are exposing your library to everyone, the simplest way would be to host your library on a repository like maven central , jCenter or even jitpack which let you upload artifacts easily, for free and are repositories which are usually already used in any Android application. If you are developing a library which will be diffused internally then you will have to upload your artifacts on a private repository and provide repository address and credentials to the developers who will have to integrate your library on their project. In the case where you have the infrastructure to do so, you can install a binary repository manager on a server you own and administer it by yourself. If you do not want to bother with maintaining it, SaaS private binary repository manager exists which will provide excellent service for a reasonable price. You can find different repository systems on this quite complete page . You can compare their different characteristics and whether a SaaS version exists or not. Think about how your library will integrate with Proguard Proguard is a tool widely used but often misunderstood. Ignoring how your library will work when Proguard is activated in the integrating application might lead to some compilation error or even crashes at runtime. To understand properly this point, let’s make a quick recap of how Proguard works when a library is integrated into a project using Proguard features: The library binary ( .aar/.jar ) is generated and then exposed to integrators. During the app compilation, the output byte code is generated, containing the app bytecode and all the libraries bytecode indistinguishably. If Proguard is enabled, it will apply the Proguard rules of the app over the app bytecode AND the libraries bytecode indistinctly. A first solution to make sure that your library will work properly when integrated in an app is to provide a set of Proguard rules that the integrator will have to put in its own set of proguard rules. You can easily verify that these rules are appropriate by creating a sample application integrating your lib and applying the “standard” Proguard rule . An easier solution (and more convenient for the developer integrating your library) is to take advantage of the consumerProguardFiles property. This property, as stated in the documentation , allows you to specify a set of Proguard rules which will be merged to the integrating app set of Proguard rules. This is similar to the first solution, except that the integrating developer do not have to manually add the rules (therefore there is no risk that he forgets to do so). Keep in mind that the rules you set in this file will then be applied to the entire app + libraries bytecode, so put very specific rules to prevent impacting bytecode not related to your library. In some cases, for instance when you do not want anyone to be able to easily read your library bytecode, you want to apply a first Proguard pass before exposing your library binaries. In that case, you can use the “regular” proguardFiles property. Of course you will have to take great care of the rules you add in this file (for instance you do not want your public APIs to be obfuscated). If you do so then your library bytecode is subject to two Proguard passes : a first time by you when you generate the library binary and a second time after being integrated in the app. This example of proguard files shows what can be the appropriate rules when you want to process your library before exposing it. The gradle snippet below is an example of a proguard configuration of an .aar library with a Proguard pass before exposing the binary : Be sparing about the libraries you include in your own new library This point is self-explanatory. If you include too much libraries in your own library the developer who will integrate it in his project might face some issues : If you add many libraries (particularly libraries whose size is big such as guava) the size of the generated app will increase. This is particularly true if the integrating app has not properly set up with Proguard. In that case even if your library uses a single method of another library the whole bytecode of this transitive library will be embedded in the final apk. Embedding multiple libraries increases the risk of having dependency conflict. If the library you embed in your own is also used by the project integrating yours (or if it is used by another library integrated as well) with a different version, then it might lead to dependency conflict. I will not explain what are the solutions to face these kind of problems as it could be an entire article alone. If this kind of problem occurs, the developer integrating your library will lose some time fixing it. Prevent resource names conflicts It is common when you develop an Android library to have the need of defining your own resource (string, integer, theme…). If this is done the wrong way, it can lead to some issues when the library is integrated. Indeed, what happen if the resource name that you use is redefined elsewhere in application? Before giving some insights on how is it possible to tackle this issue, let’s make a quick recap of Android’s resource conflict merging policy : If there is a conflict between a library resource and the app resource, the project will simply do not compile at all. In that case, the developer will have to rename its resource to make sure there is no more conflict. This is obviously problematic because a library integration should not have this kind of impact on the application code base. If there is a conflict name between two libraries integrated in the app, then the resource which will be eventually included in the apk is the one belonging to the library defined first in the gradle file. This is even more problematic that the previous case because the developer might miss the fact that a resource is being overloaded and then it can lead to some unexpected behaviors. The solution to this problem is to make sure that all the resources you define have a unique name. The recommended way is to use a scheme which will ensure that the resource name will probably be unique. The android gradle plugin provides a convenient way of having this kind of scheme : the resourcePrefix parameter . If you set a string to this parameter you will be “forced” to have all your library’s resource names be prefixed with the parameter you gave. This is a good way to keep in mind to use a unique pattern to avoid name conflicts. Meanwhile, do note that you will still be able to generate your .aar if the resource names do not comply with the prefix you set : this safeguard only warns you in Android Studio if you do not follow the rule you set. The following gradle file extract shows how to define resourcePrefix : This screenshot is an example of the kind warning you have if your resource name does not comply with the rule you set : Do not make your APIs too invasive When you design the way your library will be used it is important to keep in mind that your library will be integrated with many others into a project. So solutions that might seem elegant or are more convenient for you can, in a concrete complex project, be at best constraining and in the worst case make the integration simply impossible. A typical example of invasive library is a library, in order to be used, which requires that the Android Application object is inherited from a particular BaseApplication class of the library. This is quite a bad choice because in the case where we have to integrate another library who has made the same design choice, then we are stuck. Maybe this design is the one that leads to the fewer code change in the integrating project but it is too invasive. In more general terms, favor a design which allows integrating developers to isolate the different calls of your library. Conclusion In this article we have seen some points that you should keep in mind when developing your library. This will prevent you from having to modify your binaries after their integration in applications. Of course, this list is not exhaustive at all. You can find other tips and tricks for developing an Android library on the related Android Developer page and great advices for designing your API here . Last but not least, do not forget that the most important thing is probably to maintain an easy way to communicate with the people using your library : doing so you will contribute to increase the “Developer experience” of your library. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged android , Best Practices . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Android library development best practices guide” Abhijeet Gupta 17/09/2017 à 08:35 Which is best in sharing .jar or .aar and why? Rémi Pradal 06/10/2017 à 17:01 Hello,\r\nIt depends whether your library makes sense outside the Android context (i.e it contains only java algorithm) or not. If yes you should make a jar. If no, go for an aar file as you will be able to embed useful things compared to a jar file (such as a consumer proguard file for instance). Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-09-13"},
{"website": "Octo", "title": "\n                What Redis deployment do you need ?            ", "author": ["Thomas Wickham"], "link": "https://blog.octo.com/en/what-redis-deployment-do-you-need/", "abstract": "What Redis deployment do you need ? Publication date 18/08/2017 by Thomas Wickham Tweet Share 0 +1 LinkedIn 0 Redis is an in-memory database that I really love. It’s one of the rare technologies that make both devs and ops happy. For those who don’t know Redis already, here is a small introduction . There are four main topologies of Redis, and each one has and uses different and incompatible features. Therefore, you need to understand all the trade-offs before choosing one. Here we go: Redis Standalone The old classic. One big bag of RAM. Scale vertically, easy as pie, no availability, no resilience. Pros: This is the most basic setup you can think of. Cons: No resilience Scale only vertically (using bigger hardware for bigger workloads) Remediations: As the Redis protocol is simple, you can use an external solution, a proxy like Twemproxy , to do the replication to other nodes (for resilience) and shard the keys yourself (for horizontal scalability) Use the built-in replicated Redis or Redis-Sentinel or Redis-Cluster Redis replicated There is a master and there are replicas. The master pushes data to replicas. Replicas don’t talk between themselves. They are Read-Only and will tell you so if you try to do a SET on them (or any Write operation for what it’s worth). Pros: Really easy to setup You always have a hot snapshot of your data It’s an easy Disaster Recovery Plan (DRP) with Master/Stand-By Cons: Write performance is bounded by the master In order to achieve resilience, you need manual operations (changing the master Redis manually and restarting the clients) The replicas are not used to their full potential The Redis client needs to know which Redis to ask for which operation (or you can just ask the master every time) Remediations: Smart clients (eg: Lettuce ) able to ask by themselves the right Redis for the right operation You can setup a sharding with an external solution like Twemproxy for horizontal scalability This setup is so easy, you have no excuse to deploy in production a Redis instance without replication. It’s a cheap way to keep your service running when things have gone awry, with just a bit of manual operations. If you are a little to medium-sized organization and it’s your first Redis deployment, this may be the best trade-off for you. Redis Sentinel Ok, a bit of history: Antirez , the Redis author, started to work on a concept for Redis clustering some years ago. It turns out, distributed systems are complex and the right tradeoff is hard to reach. There are two problems with standalone Redis: Resilience, and Scaling. Ok, how about we only solve one? Redis-sentinel is the solution to resilience, and it’s quite brilliant. It’s a monitoring system on a Redis replicated system (so, a master and N replicas) which aims to answer two questions: Who is the manager in charge in this cluster? (the current master) Oh damn! We lost contact with the current master, who will take its place? (actually, it also takes care of reconfiguring the Redis instances on the fly so that the newly promoted master actually knows it can accept Write operations) But, hey, and if we loose the Sentinel? We want resilience after all, so the Sentinels should also be resilient! That’s why the Sentinels should always be used as a cluster. They have a built-in election system to track who is the master Sentinel (which can elect the master Redis when the current master is down). As it’s a quorum system, you need 3 nodes to support losing 1 (you need the majority of the cluster to be up for a successful election). Recap: you have 3 nodes with Redis-Sentinel in a cluster, and 2 or more Redis in replicated mode. You are smart, so you most likely have 3 machines, each hosting both a Redis instance and its Sentinel. This is my preferred topology. You need to deploy your sentinels first, and then you deploy your Redis instances, they register, and it works. Pros: Redis-Sentinel is builtin in the Redis binary so it’s easy to setup Good trade-off in complexity Very stable. One of the few no-hassle distributed systems Automatic resilience It doesn’t eat your RAM or CPU Cons: Still a big step up compared to Redis or even Redis replicated alone The client MUST support Redis-Sentinel. Half the magic is in the client Not trivial to upgrade a “Twemproxy + Replica” cluster to a Redis-Sentinel cluster, as you need to orchestrate the upgrade with your consumers Doesn’t solve scale issues You may need to configure your firewall to open the flow between the Sentinels Remediations: You can soften the operational complexity and resource usage by monitoring multiple redis clusters with 1 sentinel cluster. I am currently doing just that: dozen of api-dedicated mini-Redis clusters (100Mo LRU cache) monitored by 1 Sentinel cluster You can soften the client migration path to the Redis-Sentinel protocol with a little trick: you can have old clients that only query the master Redis and manually update the target Redis when Sentinels elect a new Redis master. It’s painful, but feasible. This setup is less easy to put in place. Don’t do it if you are not ready to pay the price. Sadly,  Twemproxy is not easier so if you need automated Resilience, this is still your best bet. Redis Cluster This is the Big Gun. The One we waited for so long. It aims to help large deployments, when you need both resilience AND scaling (up to 1000 nodes). It’s a multi-master architecture: the data is partitioned (sharded) into 16k buckets, each bucket with an assigned master in the cluster, and typically replicated twice. It’s the same design as Kafka or CouchBase. When you SET mykey myvalue, to a Redis-Cluster node: the hash of mykey is computed, this gives us the bucket number if the current Redis node is the master of this bucket, it accepts the operation with OK if it’s not the master it answers MOVED with a destination node, then you must connect to this node, repeat your operation, and wait for the OK to complete your SET There is a catch, though. A node is either a master node (it owns a subset of the 16k buckets, monitors other nodes of the cluster, and votes for a new master if one fails), or a replica node (exactly like a Redis replicated, but specific for exactly one master). Thus, for a reliable cluster, you will need at least 6 nodes . 3 or more master nodes and 3 or more replicas nodes. You need to have the same number of replicas per master if you want to have homogeneous resiliency. In practice, if you have a Write-heavy workload, keep 2 replicas per master and increase the master count, and if you have a Read-heavy workload increase the replicas per master and use a smart client that can load-balance between replicas ( like lettuce ). Pros: Incredible documentation, see here . The Redis author, Antirez, is great at explaining his work It scales, it heals, it works It’s bundled in the redis binary Cons: You need at least 6 nodes It works in a full-mesh fashion . Expect lots and lots of east-west traffic (intra-datacenter) Few clients support Redis Cluster. Check explicit support for your programming language It is definitely not compatible with the other Redis configurations. Neither standalone, nor replicated, nor sentinel Redis You won’t be able to upgrade from another typology, nor downgrade to another. It’s a one-way ticket Remediations: None Wrapping it up When it’s so easy to have replicas, it’s really a must have for any production deployment. Don’t deploy Redis standalone. Redis Sentinel is simple and a very good trade-off when you need resilience Clients can’t change easily between typologies so take your time before deploying one So there you have it. You may have variations on these deployments but the meat will stay the same. I hope your future Redis deployment will at least be replicated! Thanks to Sebastian Caceres, Florent Jaby, and Simon Denel for the review. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-08-18"},
{"website": "Octo", "title": "\n                API designs for low-connectivity            ", "author": ["Florent Jaby"], "link": "https://blog.octo.com/en/api-designs-for-low-connectivity/", "abstract": "API designs for low-connectivity Publication date 11/08/2017 by Florent Jaby Tweet Share 0 +1 LinkedIn 0 When designing Web APIs, we are -sometimes without our knowledge- effectively designing distributed systems . Think about it and count all the actors you can find in your typical web application (API-first of course): browsers, phones, watches, a set of databases, scalable web servers, workers, etc. That’s excluding other applications with which you most probably want to integrate. However some of these actors exist beyond a notoriously bad layer of network: phones. You may find that mobile connectivity has improved on your iPhone but it’s still far from a regular broadband connection or even an antique modem. Also, your users may not have access to this kind of luxury. In this article, we’ll see how a sound API design can help you mitigate the risks of dealing with users on low-connectivity. Let’s get one thing out of the way first: I only titled my article this way to lure you in. Low Connectivity is not an edge case . As a matter of fact, the famous fallacies of distributed computing list “ The network is reliable ” as their first misconception. You should in fact consider that networks will fail you, whichever networks you are considering. Network failures in the context of an API For Web APIs, there are two types of failure that we can target (considering timeouts and hard failures are similar). These are most problematic when the client wishes to create or mutate some resources. On mobile networks, this is most often caused by dropped packets resulting in the timeout of the connection. Keep in mind that, from the client’s perspective, these two types of errors are impossible to tell apart. Failure on request In this first case, the request never reaches the server. Repeating the request, hoping for the network not to fail again, can solve the problem quite easily. In fact, this should be automated. Obviously, this does not work if the network never heals (is your computer even plugged in?). Failure on response In this second case, the failure happens when the server has processed the request and has replied successfully. However, the response never reaches the client for which it will be impossible to know that the request did in fact work. In this case, retrying the same request might result in a conflict or otherwise unprocessable request for the server. Since the client has no way of knowing if the request even reached the server in the first place, our mitigation strategy must not depend on whether or not the request has actually been processed. In real life I will use a simple use case as an example for the rest of this article. Consider a delivery system where delivery people are assigned a set of flower bouquets to deliver at different locations. The bouquets haven’t been ordered by the person receiving them so we need to inform the buyer when the bouquets have correctlybeen delivered via email or something. Let’s iterate on this use-case and see what designs we can come up with, while keeping in mind that our network is unreliable. Idea 1: Posting to an action to trigger a transition This seems straightforward enough, let’s try this first one. We imagine our bouquet orders as state-machine objects. These state can transition like this: (ordered) → (scheduled) → (delivered) . We can use the POST verb on a related URI to trigger the transition. Whoops! In this scenario, the semantics of state machines don’t play well with network failures. The transition from (delivered) to (delivered) is not legal and it causes an error. Although, you may not want to be this strict in your state machine implementation, the concept your are exposing through your API remains the same. It is unclear what should happen when someone is retrying a request for the bouquet order to change state. As a general rule of thumb, you should avoid resources with an action in their URI. They reveal an API based on procedures rather than resources and it causes problems in distributed systems. POST requests have the same problem, their specification only states that it is the only one to mutate another resource. POST is basically an HTTP wildcard that you should only use when all else fails. Idea 2: Patching the status of the resource Alright, let’s not overthink the status of our bouquet orders. Let’s just say they have a “status” field and we PATCH it when we need. This one seems to work. But in reality there are two problems with it. First, PATCH is the second least specified HTTP Verb (after POST) and only states that you are mutating the resource at a given URI. There’s no way to tell how the body { status: “delivered” } must affect the resource. Should it remove all other fields? Should it only change this one? Should it add to an array? Should it replace another field? The semantics of patching something imply knowledge of the current state of the resource. In case of a retry, there’s no way to be sure we still have a correct knowledge of this state. PATCH has the same problems as POST regarding retries: they are not idempotent. Idempotence means repeating this action n times does not change the outcome, n being equal or greater than 1, obviously . Second, we’ve uncovered another can of worms: concurrent access. If you consider all your actors, you might find yourself in a case where several of them want to PATCH your resource at the same time with different data. Here is one. In this case the order was scheduled to be delivered but Client A has no idea and is set up to retry (several times if necessary) until it has a response. Meanwhile Client B has received the list of bouquets to deliver and has delivered the one we’re interested in. The delayed PATCH for the “scheduled” status then succeeds. Please note that at the end we have an inconsistent state AND no errors, which is rather unfortunate. In order to address this problem, we’d usually try and have the bouquet order behave as state machines but we’ve already tried that… Let’s explore elsewhere. Idea 3: Putting to a flag OK, if everyone is so hyped about idempotence on the Internet, maybe we should try one of the idempotent HTTP verbs. In this case, we imagined a resource /isDelivered related to our bouquet order which acts as a simple flag. Our system only cares if the resource exists or not, so what we PUT to it doesn’t really matter. This seems to work. We can have the same sort of related resources for when the bouquet is ordered, scheduled or anything. No matter how many concurrent actors are putting to /isDelivered or /isScheduled and how many times they have to try before success, in the end we have consistency. This works well because you are in fact always adding information, never mutating or removing. This is a key design pattern for resilient systems. You might have heard of append-only databases for example. This design is a fair one and can get you pretty far. I can see one thing creeping up though. We might want to have additional data regarding the delivery of the bouquet: the date and time of the delivery, the exact location, a signature or a selfie to send back to the buyer (♥‿♥). You could imagine having other related resources like /deliveryDate (which can serve the purpose of /isDelivered by the way), /deliveryLocation and /deliveryImage . However, it can quickly become impractical and also somewhat incoherent. If your PUT /deliveryDate works fine but PUT /deliveryImage fails, some other actors might not get what they expect. Let’s see if we can do something about that. Idea 4: Creating a related resource This time, we’re doing it right. We want all information related to a delivery contained in a single resource which references our bouquet order, the creation of which is done as a single request so it’s never partially there. I know this, it’s a CRUD system! We’ve seen this a million times with CRUD systems and we should be able to get it to work. The creation of a resource is just a usual POST on a /deliveries resource which creates it and then we get the URL of that delivery as the Location header in the response. Oh no! We created two resources when we only wanted to create one. As a side-note, I can tell you that this very problem happened to me when I tried to wire money while on the train. I set up the transfer to quite a large amount, checked everything and hit “send”. About a minute later, I had an error saying something went wrong and I had to retry. I hit it again and it worked fine this time… But in fact, I had just wired twice as much as I intended and only found out days later when I could not pay for anything anymore. Needless to say, it was not very amusing. Considering this, you might want to forbid creating the same resource twice, like in the following diagram. This time, we have our unicity taken care of. But for the client retrying a request, the second time the response is an error, while in actuality everything went rather fine. Why should anyone have to code around and worry about “normal errors”? Chances are these two words made you cringe already. What we’d like is, for a second request for the creation of a delivery resource, the API replaces the resource or otherwise replies with a success code. You can implement it with this design, but there is a way to make this behaviour more intuitive. Let’s check out this idempotent business once again. Idea 5: Putting a related resource This is the last design I will present as it includes, in my opinion, all the expectations and behaviour we need for our use-case. We define a related resource /delivery which contains everything there is to know about the delivery of a given bouquet. The trick here is that there can be only one delivery for a bouquet and the URI implements this requirement. Since the client knows in advance for a given bouquet what is going to be the URI of the related delivery, it can simply PUT the complete resource there. In this last scenario, we get: A repeatable request which always yields the same outcome All information pertaining to a delivery bundled within a single resource A URI which makes it clear how many deliveries there can be for a bouquet A reproducible pattern for /schedule , which addresses the problem with concurrent access that we saw earlier between scheduling and delivering since they are now explicit separate resources Because the semantics of PUT make it idempotent, we can trust that retrying the exact same request will result in the same outcome. Retrying requests safely makes it easy not to worry too much about requests failing due to a fragile network. In turn, not worrying makes you healthier! Conclusion As you have seen, depending on your API design, it may be more difficult to recover from a network failure ( that will happen ). Here is a list of things we can gather from these scenarios: Retries are to be expected for any resource on your API, because the network is unreliable Create resources where it makes sense (even if it does not equate one-to-one to a model in your persistence layer) Idempotent requests like GET, PUT and DELETE play well with cache and retries, their semantics make it easy to reason about redundant actions You should prefer idempotent HTTP verbs whenever possible Repetition and redundancy are the foundations of robust distributed systems Next time you are thinking about new API endpoints, ask yourself: “But what if the network fails?” You’d better be prepared for it as it will inevitably happen! Now tell us. How would you design your API for this use-case ? Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-08-11"},
{"website": "Octo", "title": "\n                How does it work? Docker! Part 1: Swarm general architecture            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-docker-part-1-swarm-general-architecture/", "abstract": "How does it work? Docker! Part 1: Swarm general architecture Publication date 08/08/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey there! TL;DR I hacked another thing together, this time in order to install a highly available Docker Swarm cluster on CoreOS (yeah, Container Linux), using Ansible. If you want to try it: git clone https://github.com/sebiwi/docker-coreos.git cd docker-coreos make up You will need Ansible 2.2+, Docker, Vagrant and Molecule Why? Well, I did the same thing with Kubernetes a while ago , because I wanted to test all the different new Kubernetes features and have a local cluster to play with. I wanted to do and have the same thing with Docker in Swarm mode. I’m also planning on comparing both of them eventually. So I needed the whole prime matter first: installation procedures, general architecture, networking model, concepts and abstractions and so on. Finally, I thought it would be nice to explain how every Swarm component fit together, so everyone can understand what’s under the hood. I keep getting questions like “what is a Kubernetes and where can I buy one?” at work, and if it is “better than a Docker”. You won’t find the answer to the last question in this article, but at least you will (hopefully) understand how Swarm works. You can make up your own mind afterwards. Especially after reading both articles. Okay, you got me. What is a Swarm? Docker Swarm is Docker’s container orchestration and clustering tool. It allows you to deploy and orchestrate containers on a large number of hosts, while enabling you to do some other cool things in the way. There is usually mild confusion when talking about Swarm, which is relatively normal since the name has been used to refer to different things over the years. First, there was Docker Swarm, currently known as Docker Swarm Standalone. It basically allowed you to turn a pool of Docker hosts into a single, large, virtual Docker host. In this scenario, a discovery service or a key-value data store like Consul , etcd or Zookeeper was needed in order to obtain high availability on the manager nodes (I will discuss this point further later on in the series). Nowadays, and since Docker v1.12.0-rc1, there is something called Swarm Mode , which is included by default in the Docker Engine. Swarm Mode allows you to manage natively a cluster of Docker Engines. It is highly integrated with another toolkit developed by Docker, called Swarmkit , which removes the need of using a key-value data store for service discovery like you needed to do when using Swarm Mode Standalone: it is already included in Swarm Mode. Swarm me up So there’s two of them. Theoretically, Docker has no plans of deprecating Docker Swarm Standalone. From the Docker Swarm’s GitHub page: “Docker does not currently have a plan to deprecate Docker Swarm. The Docker API is backward compatible so Docker Swarm will continue to work with future Docker Engine versions.” Nevertheless, from the Docker Swarm’s overview page: “You are viewing docs for legacy standalone Swarm. These topics describe standalone Docker Swarm. In Docker 1.12 and higher, Swarm mode is integrated with Docker Engine. Most users should use integrated Swarm mode … Standalone Docker Swarm is not integrated into the Docker Engine API and CLI commands.” Which means that even if they’re still maintaining Swarm’s Standalone, Swarm Mode is where their money is. Therefore, it’s what we’re going to be using for this project. Let us discuss architecture first. Docker architecture Before talking about Swarm, I’d like to talk about Docker itself. I’ve mentioned the Docker Engine quite a few times without actually describing what it really is. It is basically a client-server application with three major components: a command-line interface (CLI), a REST API and a server. The server is a daemon process called dockerd. It listens for Docker API requests and manages all Docker resources, such as images, networks, containers and volumes. It can also communicate with other daemons to manage Docker services (more on this later) The REST API is served by the Docker Engine, and it allows clients to talk to the daemon and control every aspect of it. It can be accessed with any HTTP client, but if you want to stay “official” there are many standard SDKs on many languages, and there is also the standard CLI The command-line interface is the primary and most frequently used way of communicating with the server Not your regular engine Basically, you use the CLI to talk and interact with the Docker daemon, through the REST API. The daemon then creates all the necessary resources such as containers, networks and volumes. The client and the server may coexist on the same machine, or they may also be on different hosts. Docker is composed of many different things at an engine-level too. It is Docker’s plan to separate and release all of its infrastructure plumbing software , and they’re doing a great job so far. As of Docker 1.12, the Engine is decomposed and built upon two different tools: runC and containerd . First, runC is a CLI tool for running containers. The Docker Engine used to do that before. First by using LXC , and then with libcontainer . Nowadays, libcontainer is still used, but only by runC. Another cool thing about this is that runC is able to run Open Container Initiative ( OCI ) containers. OCI is a standardisation initiative which specify a common interface for containers images and container engines, effectively enabling any container build with a any OCI-compatible tool to run on any OCI-compatible engine. In other words, these are containers that abide by a de jure container standard. What is de jure, you say? Check it: De facto is a Latin phrase that means in fact (literally by or from fact) in the sense of “in practice but not necessarily ordained by law” or “in practice or actuality, but not officially established”, as opposed to de jure . So basically, “De facto” is used, whereas “De jure” comes from a widely accepted standard or law. Cool, huh? This is great, since it will theoretically allow containers created with one engine to be run on a different engine. Therefore its name, Open Container Initiative.At the time of writing this article, the main OCI-compatible tools are Docker, and Rocket (Rkt). containerd is a daemon that uses runC to manage containers. It exposes a CRUD container interface using gRPC, with a very simple API. This way, all the container lifecycle actions (like starting, stopping, pausing or destroying containers) are delegated by the Docker Engine to containerd which uses runC to execute all its container-related actions. The Engine still manages images, volumes, networks and builds, but containers are now containerd’s territory. Like a layer cake There’s also another component called containerd-shim which sits between containerd and runC, acts as the container’s process parent and allow the runtime (runC) to exit after starting the containers. This whole thing is pretty funny, because the Docker Engine is not able to run containers by itself anymore: it delegates all these tasks to runC and containerd (and containerd-shim, somehow). That’s about it for the Docker architecture. Let’s talk about Swarm now. Swarm architecture and concepts A Swarm is created by one or many Docker Engines, which uses swarmkit for all the cluster-management and orchestration features. You can enable Swarm mode by either creating a new Swarm or joining an existing Swarm. When using Swarm, you don’t launch single containers, but rather services. But before talking about that, I need to talk about nodes. A node is just a Docker Engine that is a member of the Swarm. There are two types of nodes: Managers and Worker nodes. A Manager node receives a service definition and then it dispatches tasks to Worker nodes accordingly. They also do the orchestration and cluster management functions required to maintain the desired state of the swarm. There may be many Manager nodes on a Swarm, but there is only one leader, which is elected by all the other Manager nodes using the Raft algorithm and which performs all the orchestration tasks. Worker nodes receive and execute tasks from Manager nodes. By default, Manager nodes are also Worker nodes, but they can be configured to not accept any workload ( just like real-life managers ) therefore becoming acting as Manager-only nodes. There is also an agent on every Worker node, which reports on the state of its tasks to the Manager ( so basically, middle-management ). That way, the Manager can maintain the desired state of the cluster. A service is the definition of one (or many) tasks to be executed on Worker nodes . When creating a service, you need to specify which container image to use, and which commands to execute inside the containers. A service may be global, or replicated. When the service is global, it will run on every available node once. When it’s replicated, the Manager distributes the given number of tasks on the nodes based on the desired scale number. This number may also be one. A task is just a container and the commands to be run inside of the container. It is the standard scheduling unit of Swarm. Once a task is assigned to a node, it can’t be moved to another node. It either runs on the selected node, or fails. So that’s about it for this first chapter. On the next episode, I’ll talk to you about Docker (and Swarm’s) networking model. Stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-08-08"},
{"website": "Octo", "title": "\n                “eXtreme Quotation”: Agile Planning on steroids            ", "author": ["Nicolas Guignard", "Guillaume Duquesnay", "Jonathan Scher"], "link": "https://blog.octo.com/en/extreme-quotation-agile-planning-on-steroids/", "abstract": "“eXtreme Quotation”: Agile Planning on steroids Publication date 07/08/2017 by Nicolas Guignard , Guillaume Duquesnay , Jonathan Scher Tweet Share 0 +1 LinkedIn 0 OCTO Technology has been using this methodology for quite some time both in France and in Australia. The following is the story of the 1st time we applied it. As a team, we estimated about 90 user stories in 20 minutes. Basically, the whole story map. We supposed that this technique would make things quicker and less painful and it really turned out that way Since then, this experience has been run in France and Australia with several teams and we obtained similar results. Let’s detail how we run this Agile planning to help you understand why it works so well. Extreme Quotation : agile planning “cards on desk” Duration 1h max. But plan to wrap it up in less time Participants Developers, product owner, business analysts Product manager Agile facilitator Equipment A room 2 tables User stories from the Story Map printed on post-it or cards Planning poker cards Colourful scotch (chatterton for instance) We will illustrate some of these steps with pictures we took when we ran the agile planning in a project in Sydney. If you have to animate here are the different steps : Prepare the room Put one table in the centre without chairs; you want the members of the team to be able to walk without getting in each other’s way. This table will be the Evaluation table. Use the scotch to split the table in 6 or 7 columns (or pie slices, depending on the table’s shape). On top of the columns put the planning poker cards in increasing order (Fibonnacci until 100 and the “?” card). Place one or two calibrating user stories on the table that the team has chosen beforehand. Put the second table close to the Evaluation table. But not close to the point it is getting in members way to move freely. This is the User story table. Put all the user stories to estimate on this table. Start To introduce the principle of this agile planning to your team you can start like this: «Here is the program/activity/workshop of the day: place the user stories in the corresponding columns based on their complexity to fully test them. Basically, if I give you the application and assure you that the feature works, how many tests do you need to verify the feature and how complex they are? Here are some pieces of advice : evaluate relatively and move the stories by comparing them; first put down the stories as you feel them; Start by categorizing, you will move them again after, we want a quick answer first of all. Even if you think the answer is wrong; we will improve on it iteratively in the next step; if you have any doubts, move the story to the upper column; if you don’t know how to evaluate a user story put it on the “?” column; the product manager and the product owner don’t estimate but they are here to answer your questions. For those who have business knowledge, be open and encourage the team members to question your choices; let’s roll! » Sort user stories for a first time Stay in the back as a facilitator and do not give specific instructions. The group understands that everybody is moving the stories in the meantime. Let the team work. Answer to team’s questions, repeat instructions if it is necessary but try not to participate to the debate as much as you can. If it is taking time, encourage the team to place the cards the as quickly as possible : “We want to have as many estimations as possible in the least amount of time we can”. Observe how the group behaves. This phase should take around 10 minutes. Shake the result Once the user stories are placed and nobody is moving them anymore, ask questions to the team to make everybody think a bit more. Do not ask several questions at a time, only one at a time and observe if this causes adjustments. In case it happens, wait until it stops again. Example of questions you can ask : Did everybody look at all user stories ? Do you think there still are some stories that are placed at least two columns from where they should be ? Do you think the stories’ estimations are consistent between each others ? Have you understood all the user stories ? Do you think we have finished ? Do you think we can start the next iteration based on these estimations ? etc The aim of these questions is to question the reasoning and make the estimations increase; ask questions until the user stories don’t move from one column to another. When the team members don’t move the post-its / cards it means they are reasonably happy about the estimations, it’s over. Before shaking : Discussion about the stories where there doubts : Clean up It is not over yet. Ask the team to write down on each user stories card the number of the column where it is placed. Then put everything back where it is supposed to be : tables, chairs, throw away chatterton, etc etc. Experience return If you have tried this style of agile planning, we are interested in your experiences. Feel free to drop us a comment or to ask us anything about this method. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-08-07"},
{"website": "Octo", "title": "\n                Agile transformation with Henrik Kniberg @OCTO Technology            ", "author": ["Sergey Larionov"], "link": "https://blog.octo.com/en/agile-transformation-with-henrik-kniberg-octo-technology/", "abstract": "Agile transformation with Henrik Kniberg @OCTO Technology Publication date 26/07/2017 by Sergey Larionov Tweet Share 0 +1 LinkedIn 0 Henrik Kniberg is Agile/Lean coach at Crisp in Stockholm, speaker at international conferences and author of popular books about XP, Scrum, Kanban and Lean from the Trenches. Working primarily with Spotify and LEGO, he enjoys helping companies succeed with both the technical and human sides of software development. After his presentation at USI , Henrik had accepted our invitation to answer some questions from our clients in their transformation journey towards Agile. This event is facilitated by Sergey Larionov, Agile Coach at OCTO Technology, and made in a breakfast format, where clients can come for 2 hours before starting their day of work. The event was filmed and full video can be found on this link (user/password : hkniberg/the_nextmorningafter_ USI). How did you get to Agile? “Entirely by accident…” :) Looking for a way to run Henrik’s own organizations as an entrepreneur and manager, and searching for diverse sources of inspiration, he discovers some frameworks such as Scrum or eXtreme Programming and by implementing them step-by-step learns that it is way better than waterfall. Being a consultant, he started helping clients to apply these methods as well. He started to write about what he learnt, generated more consultant engagement and it became viral. What brought you to Spofity? Spotify is the most known example of Henrik’s work on Agile culture and Product Ownership . One day, after delivering recurrent talk, what he use to do between consultant engagements, Henrik got a call from Spotify, a small start-up at that time build by a group of students from a Swedish university, who participated on his talk. They were trying to apply Scrum and had a “growth pain” and asked Henrik to come and help dealing with that. What about Agile journey? The journey is quite different depending on where you come from. For big organization with many people Agile means simplifying, allowing teams autonomy, removing waste and overloaded structure. For startups, it is the opposite, Agile serves to add structure where there is a significant growth with too much entrepreneurial chaos, although not becoming bureaucratic. When there are few teams it is easy, when it comes to 6-9 teams, the books don’t give you answers anymore to questions such as synchronization between teams or how to organize people into teams, etc. Applied experience in different contexts, use case study and try some patterns may bring you these answers. How did you get into LEGO? LEGO is Henrik’s second flagship project. They got inspired by his work at Spotify, notably by Scaling Agile , and asked him to create an environment where people are happy and doing a good job. Agile might help to do it. LEGO was using SAFe framework and they needed help. Why LEGO have decided to implement SAFe? The Product department has already been Agile, but other departments such as Sales, HR, Marketing, used a lot of Waterfall method, which worked well, until one day they understood that the world is moving too fast and they had too many failures. Since then, they have gone crazy with innovation by allowing teams full autonomy. LEGO was running out of money and almost went bankrupt because of loosing control of everything. They made up changes and became super focused as a company by aligning everything that everybody does with the high level goals of the company. Meanwhile, there was a disconnect between portfolio and budgeting systems based on 2 years cycle and software development teams trying to be very fast. They had a middle management layer running in meetings all the day, updating spreadsheets and sending emails. Over several years they figured out that it doesn’t make sense, not effective nor motivating. Until one day middle management took trainings about Agile and SAFe and got inspired. They were not sure if it would work, but maybe it’s worth trying. And they started from experiment. Experiments are a lot easier to sell than organizational change. LEGO asked Henrik, who is framework neutral, helping them trying these experiments. They tried to put 120 people together and do sprint planning with single backlog, introduce cross-functional teams, etc. and it kind of worked. Then they focused on removing waste in this new functioning. Starting from 80% couple of years ago, nowadays LEGO is using only about 20% of SAFe. In conclusion, SAFe is a framework based on Agile & Lean principles, it is just too detailed. At scaling there is no “one-size-fits-all”, but there are some patterns. All frameworks are saying the same thing: use Agile principles, work with teams, do the planning together if you have dependencies, plan synchronisation meetings where all the teams come together for an integrated demo where everyone can learn, then make changes. “All the frameworks are saying the same thing, it is just a different level of detail”. Look at frameworks and case studies, steal the best ideas, but avoid the trap believing that a framework is going to answer all your questions. How to switch from command & control to the Agile mindset? Those who behave in command & control mode are used to top-down way of working, but once they tried another approach they would switch the mindset, because often their behaviour wasn’t intrinsic, but the result of the system they are living inside. When the system changes, behaviour changes. Of course for those people who love control and get scary by uncertainty, they may not fit into Agile. It doesn’t mean that they don’t fit for the organization, but they don’t fit for the part of the organization where there is uncertainty. For repeatable actions with minor changes and low complexity you probably don’t need Agile. But for uncertain, changing and complex world, Agile will definitely help you. Although command & control people would not be happy in this new working mode, would not deliver a good work and will resist. Better to find for them another environment. Starting a transformation, don’t judge people. Just go, but be observant. For people, who do not understand, help them to understand, listen to them, but if you notice that someone is clearly unhappy help him to find a place where his skills would fit better. How to deal with top management who wants 100% control, but also convinced by need of Agility to welcome continuous rapid change? Start by asking, “What do you want to control?”, “Why do you need Agile?”, “How did the last project work out?”, “Did it work out as planned?”, “Are you happy with the results?”. If the previous project worked perfectly, you don’t need Agile. But often all you have in the waterfall is illusion of control. We can control the budget, number of teams and the deadline, but not what exactly the product is going to look like. It doesn’t make sense to lock the product, instead focus on the desired outcome and what problem we want to solve. Mostly you will find out that Agile gives a better control. Every sprint is a real-time control point to look at something that runs and works, to see what failed (after one sprint instead of at the end of the project), and make changes. Agile increases the control and removes the illusion of control. How do you go to maintenance mode after product has been developed? What do we do with an Agile team when the main functional scope has been finished? In waterfall mode once the project team has done their project, it goes to a low-cost maintenance team. Giving the project from Agile team to maintenance team is not a good idea, as it is a risky and costly action. No matter how much project team documented, the most knowledge is in the heads of developers. It takes time to understand the product, why it was designed this way, writing documents, spending time on explaining. A better solution would be to move the Agile team on another product while delivering value and keeping maintenance of a previous one. One of common patterns is, once the main functional scope is delivered, the team just starts working on a new product B, and fix what is needed from previous product A. Whether adding it when needed in a current sprint, whether plan a specific sprint for maintenance, which would impact the A product fixing reactivity, but would let the team focus on product B and make it move faster. Basic Agile architecture is stable teams, stable backlogs, just decide which team pulls from where. Another pattern is a common single backlog, where teams just pull from it to make sure the highest value is delivered by following the development. Gather teams, introduce them the context, explain the priorities and let them choose their own team backlog items. If there are a large number of people you may need a Chief Product Owner to prioritize between different Product Owners and take decisions on common priority. Only you can decide which pattern makes sense in your context. If you are not sure, then just try, and if it doesn’t work, try something else. How can we do continuous delivery in multiple teams when we have dependencies? One of very powerful patterns is integration cadence. It is a moment, every week @Spotify or every month @LEGO, when everyone, who works on the same product, gets together. It creates intrinsic motivation to collaborate in preparing demos and space to fail for innovation. It also helps to solve a typical Agile problem of abused autonomy by introducing a hard constraint, where no matter what, the team has to show the results. In addition, it forces them to communicate with dependent teams on potential delays and take decision on planned scope. How to deal with dependencies between Agile and waterfall teams? It is exactly the same situation at LEGO where Digital solutions is an Agile team and Corporate IT is a waterfall one. A pattern that helped here was to visualize dependencies and invite teams to see it, let the people speak to each other. It happens every month at LEGO during Planning meeting (see also Program Increment at SAFe) where teams collaborating in optimizing planning taking in account mutual dependencies. Visualize, optimize, remove the bottleneck and organize yourself to minimize dependencies. Here is the presentation shown during the event. Do you have a transformation template to standardize the working process? If your way of working is pre-defined, what happens with continuous improvement? In the case when the team wants to make an improvement they would have to pass by phase of negotiation with others or do it secretly. Any of these cases generates frustration. Standardization is greatly overrated. Some people may become rebellious and hide the way they work. It breaks trust and goes against Agile, as it is about “Individuals and Interactions Over Processes and Tools” (one of Agile values). When people ask for standardization leading to the common way of work, there is usually a reason for that. First of all, ask why they want it and what need do they want to satisfy. Usually the need is valid, but solution is not good. For instance, their need is to go faster and standardization is the proposed answer. Let’s think about other options to go faster. By making teams work, whatever way they want, they continuously improve and in consequence move fast. Another example is to reveal problems. Within the team, the retrospective or any other tool of continuous improvement can help to visualize impediments and let team members find most appropriate solutions. In dependent teams make them discuss together and collectively find a way to solve problems. The standardization could happen if it answers to a real need. In case of generic problems, for instance, when different teams are using various versioning control system or none, that causes absence of a simple way to integrate the whole product, multiple teams are working on, and to test it in a continuous way. Bring them together, do root cause analysis and choose a system that they all hate the least. Let the problem drive the standardization and keep it on the minimum level to not slow down the rate of innovation. How do you deal with the feeling of loosing the ability to do everything in start-up, which is growing and multiplying specialisations? Scrum organization says “we are the team” and within that team, we are cross-functional, so we have a designer, back/front developer, and other roles demanded to deliver end-to-end functional scope. People still have a specialization or main competency, but in Scrum there is a permission to step out of your specialty to help others, as well as common goal, demonstration and mutual learning. Sources Henrik’s Blog Article “Agile is about taming complexity” Visual support produced by Nicolas Kalmanowitz, Agile Coach at OCTO Technology: Tweet Share 0 +1 LinkedIn 0 This entry was posted in Agile , Change Management , Methodology , News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-07-26"},
{"website": "Octo", "title": "\n                End to end testing from the trenches with Protractor            ", "author": ["Alexandre Masselot", "Jérémy Gobet", "Jérôme Van Der Linden"], "link": "https://blog.octo.com/en/end-to-end-testing-from-the-trenches-with-protractor/", "abstract": "End to end testing from the trenches with Protractor Publication date 06/09/2017 by Alexandre Masselot , Jérémy Gobet , Jérôme Van Der Linden Tweet Share 0 +1 LinkedIn 0 Standing on the top of the test pyramid , web end-to-end tests are aimed to automate user acceptance scenarii through a browser window. In practice, those tests are often either disregarded by the development team or, on the contrary, used as the single line of defense to catch all problems in a web application development. If none of those extremes are a likable target, end-to-end testing certainly has a role to play in a sustainable quality development. However, they drag with them a bad reputation among developers: unstable, slow, costly, unmaintainable. In this article, we will try to be more pragmatic and enthusiastic about the topic. We will step above the basic introduction to Protractor (the most common testing framework in the Angular world) and dive into practical insights faced in real life situations, from the technical, the methodology and the human perspectives. What is end-to-end testing? For Techopedia , “end-to-end testing is a technique used to test whether the flow of an application right from start to finish is behaving as expected” . For a web application, it commonly refers to automatizing a user centric flow of events (clicking and interacting across the application) and ensuring the correctness of the outcomes down the path. Those tests can be undertaken manually but the advantage of automation is to save brain power and ensure faster feedback loops to the development teams, hence shorter Time-To-Repair intervals. By opposition to unit or integration tests, they are aimed at running on a fully built version of the application and are decorrelated from low level implementation details. Figure 1: the test pyramid , popularized by Mike Cohn , shows various levels of tests, from ubiquitous low level unit tests, directly targeted towards the implementation up to rarest automated end-to-end, or GUI, tests and even manual sessions. The pyramid reflect the numbers of tests to be typically written on each layer, as well of the implementation cost. Protractor is a very popular framework for end-to-end web application testing. In practice, a typical test takes the form of ( source ): describe('angularjs homepage todo list',() => { it('should add a todo',() => { browser.get('https://angularjs.org'); element(by.model('todoList.todoText')).sendKeys('first test'); element(by.css('[value=\"add\"]')).click(); var todoList = element.all(by.repeater('todo in todoList.todos')); expect(todoList.count()).toEqual(3); expect(todoList.get(2).getText()).toEqual('write first protractor test'); todoList.get(2).element(by.css('input')).click(); var completedAmount = element.all(by.css('.done-true')); expect(completedAmount.count()).toEqual(2); }); }); What end-to-end testing is not? As Mike Wacker puts it in the Google’s testing team blog , in theory, everyone loves end-to-end tests: developers, because they don’t write them; business, because they reflect the real user experience; testers, because they act at the user high level and reveal “visible” bugs. A common pitfall is therefore to rely upon those tests to catch most of the application bugs, soon turning over the pyramid (figure 2). Figure 2: the inverted test pyramid (left). As yummy as the “ice cream” anti-pattern might looks, it soon reveals to be indigestible. However, any boat can be turned around (right): Wassel Alazhar posted a tale on how a team could revert the pyramid back on its feet (values are in percent): we see how the relative number of end-to-end tests decrease across time, mainly due to the surge of unit tests, but also the removal of end-to-end ones. A real life situation The experience shared in this article comes from various missions we have undertaken, as OCTO Technology consultants. Even though these patterns are recurring, most of the practical points described in the following sections come from one customer situation, and we shall spend a couple of paragraphs to brush the context. The environment there can be described as mature on an agile scale. The management levels are engaged, the team under scrutiny is a small dozen, collocated, mixing developers, ops engineers and a Product Owner (figure 3). The team follows a ScrumBan approach and information flows continuously amongst the party. To complete the picture, they steadily deliver value on its single product. In our story, a couple of developers have been called to engage with a diverging stack of user stories lacking end-to-end tests and increasing quality issues from the front end user perspective. Figure 3: The setup in which work the team developing the main application under study within this article. The application at hand As pictured in the figure below, the application at stake is of a medium complexity. However, it encompasses several components that make it interesting from the functional testing point of view. Far beyond being only a data driven web application, developed upon a classic Java/AngularJS stack, its ecosystem relies on external components: a proprietary web platform ( ServiceNow , used by the backend analysts), REST services, two factors authentication (2FA), emails, QR codes. The data volume and the richness of the underlying architecture also offer challenges in term of response time and volume. Many aspects of this application are of course covered by different types of tests, but we shall now focus on the browser oriented end-to-end tests implementation. End-to-end testing technical flaws Let’s address the most  common arguments faced by end-to-end testing. Although those criticism are often based on actual observations, there are some practical approaches to reduce the burden. e2e tests are hardly maintainable – Criticism: “Web pages structure continuously evolves, thus an element defined at one position will certainly move around” – Answer: “use a robust way to describe element positions” With protractor, an element can be located by id ( by.id ), css ( by.css ), xpath ( by.xpath ), button ( by.buttonText ) or way more other options, often coupled to the underlying AngularJS implementation. The common pattern is to inspect an element via a browser developer tool, copy its description anchor and paste it into an appropriate locator for action to take place ( e.g. check the content of a field, click on a button, enter a value in a textfield etc.). Although this method produces the correct behavior, the path returned by the developer tool is often intricate: element(by.xpath(’//*[@id=\"whatever\"]/div/div[7]/p[1]/button[2]’)) . The other extreme might be too simplistic: element(by.xpath(’//*[contains(text(), “submit”’)) . It is more robust to use a minimalist combination of protractor locator functions, better describing the intention. One shall use an id , if available: element(by.id(‘whatever’)) Selectors can also be nested, e.g.: element(by.id(‘whatever’)) .element(by.css(‘div.tool-bar’)) .element(by.buttonText(‘submit’)) Your fellow developers will certainly be thankful for the extra readability and sustainability. – C: “XPaths and location descriptors are all over the test files and any refactoring is a nightmare” – A: “use page objects to have a single location to describe your page” Instead of spreading locators all across your test files, use page objects (cf. ThoughtWorks and Martin Fowler posts). A web page, or part of a page, is associated with a business oriented object. Therefore, instead of having: element(by.id(‘whatever)) .element(by.css(‘div.tool-bar’)) .element(by.buttonText(‘submit’)) .click() One can write: const page = new MyPage(); page.submitToolbar(); And having the submitToolbar() function deported once for all into the MyPage object definition. Serenity , another framework on top of Selenium (another well-known GUI test framework), provides such a feature with PageObject : @DefaultUrl(\"http://localhost/whatever\") public class WhateverPage extends PageObject { @FindBy(id=”submit”) WebElementFacade submitButton; public void submitToolBarForm() { submitButton.click(); } } It is then easy to open and manipulate this page, from anywhere: WhateverPage page = switchToPage(WhateverPage.class); page.open(); page.submitToolBarForm(); Although writing tests is like writing any piece of code, one shall not forget a couple of good reflexes: use naming convention, like getXXXButton() , getYYYTextfield() or findAllZZZTableCells() ; write a simple spec upfront to assess that your page object is up to date with the actual tested page. Having the corresponding test fail early will make analysis easier. – C: “Tests output are unreadable: how can I know the reason why a test did fail?” – A: “Use tailored matchers and meaningful messages” Matcher functions compare an evaluated value with an expected one, making a test fail if the condition is not met. For examples: expect(myArray.length).toBe(42); expect(email).not.toBeUndefined(); A first improvement is rarely used: it consists in adding an extra comment parameter, that will provide more context in case the condition does not pass. For example, the values actually stored in myArray : expect(myArray.length).toBe(42, ‘myArray should contain 42 values but contains: ‘ + myArray); Jasmine is the underlying testing framework. It provide default matchers, but those are limited. Thankfully, the  library can be extended with the versatile jasmine-matcher library. The following example will fail with the appropriate message whether myArray is not an array or not of the asserted size: expect(myArray).toBeArrayOfSize(42); Many more matchers are available, even though you may need one which is not present there. – A’: “Create your own custom matchers” For example, you want to compare array of time strings, but you want them to match within a 5 seconds window. If the comparison has to be done once you could write a suite of assertions: check the returned variable is an array; check the sizes of both arrays are the same; check the return variable contains a string in a meaningful time format; check one by one the values to see if the delta in seconds is lower than 5. However, your actual business test becomes intricate, and hardly reusable. Fortunately enough, it is possible to create your own matcher, leveraging lodash and moment.js libraries: jasmine.addMatchers({ toBeEqualTimesWithin5Sec: () => { return { compare: function(actual, expected){ //do we have an array? if(! _.isArray(actual)){ return {pass: false, message: ‘is not an array: ’ + actual); } } //Are arrays the same size? if(actual.length !== expected.length){ return {pass: false, message: ‘sizes are not coherent. Actual:’ + actual.length + ‘ vs expected: ‘ + expected.length }; } //Does the array contain time strings? //Are both arrays times close enough? … //finally return true return {pass:true, message: ‘dates are aligned’}; } }); And your test will simply become: expect(myArray).toBeEqualTimesWithin5Sec([‘12:34:55’, ‘15:42:00’]); Finally, a matcher should come with it own test suite to assert its completeness as well as to document its behavior. And never forget: “Always code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. Code for readability.” (John Wood, 1991) e2e tests are slow Frontend tests are notoriously slow. The causes are both based on the actual browser rendering and library performance. It is not uncommon to see real life situations diverge, with full suites taking up to 9 hours to run! However, the burden can be reduced. As tempting as an “improvement” idea may sound, one shall always remember that any optimisation effort shall be guided by upfront measures. – C: “Searching elements in the DOM using xpath is very slow” – A: “Use id attribute. Finding by id is much faster” Browsing the DOM seeking for a particular element is slower than using css selectors, itself slower than retrieving an element with its id. Here are the three ways to achieve the same action: element(by.xpath(’//*[@id=\"bar\"]’)) // based on evaluate element(by.css(‘#bar’)) // based on querySelector element(by.id(bar)) // based on getElementById But the last one is nearly 60% much faster than css selector and 100% faster than xpath! You can test by yourself here . Furthermore, id tends to be more stable in time than style selectors or xpath, which can change according to business goodwill. And if you’re not convinced yet, while getElementById is generally over optimized on all browsers, xpath is not always well implemented, leading to different behaviours on different browsers. So always prefer the id over the other solutions. – C: “Tests are invaded by browser.sleep(nnn) statements, slowing down any test suite” – A: “La programmation sans sleep!” (“Programming without sleep”, but that sounds better in french…) browser.sleep(3000) will make the browser hold for 3 seconds. Such calls are often made to wait for an element to be visible. Although the solution seems appealing, it soon clutters the whole code, as the timings set into the sleep functions try to capture the worst case scenario. Using JavaScript promises is the default way to execute a piece of asynchronous code and return the result (or an error) as soon as it is completed. Protractor makes a comprehensive use of the feature. For example, element(by.id(‘my-id’)) will not return the element itself, but a promise to be completed. If we have to wait for a dynamic element to be displayed, we can write a utility function that will return a promise with the element, waiting first the element to be present, then visible. const waitForVisibleByLocator = (locator, delay) => browser.wait(() => element(locator).isPresent(), delay); .then(() => browser.wait(() => element(locator).isDisplayed(), delay)) .then(() => element(locator)); And the function being called in a test: waitForVisibleByLocator(by.id(‘my-id’)) .then((el)=> // assertions); – C: “My tests require to swap among a list of logged in users, but each login step slow down the whole execution. What can I do?” – A: “Use one browser instance per user.” Some suites require logging in back and forth between different users. If the login process is not smooth ( e.g. your in house application commonly take up to 15 seconds to login), most of the time is spent on the login page. Although speeding up the login process can be a sound action, it might also be hard, depending on legacy systems and of little end user value ( “my users login once a day on the application and stay in for the full working day” ). To tackle the issue from the testing side, it is for example possible to attach on browser per user name, which shall either exist and be logged in at once, or not yet be alive and thus need to be created. For example: browsers[user.name] =  browser.forkNewDriverInstance(); – C: “I have many specs files and Google tells me protractor can execute them in parallel. Why isn’t that so simple?” – A: “That’s a route, but beware of dependencies and concurrency issues.” By default, it is possible to configure protractor to parallelize tests. For example, to spread a suite over 4 browser instances, one can configure: { shardTestFiles: true, maxInstances: 4, browserName: chrome } However, a perfectly decoupled world is not always reached and several issues can raise: design without dependencies can be hard (imagine testing email change, password edition and 2FA token reset in parallel for a single user); on the login, some 2FA token for a given user can be used in concurrent sessions; scale up is hardly linear, and will at best be limited by the time of the longest suite. Of course, all theses issues can be addressed by a careful applicative architecture, extensive use of mocking or manual decoupling of the full test set into functionally independent suite ran concurrently. – C: “Are there other ways to speed up the overall e2e testing process for Continuous Integration?” – A: “Definitely yes, but tradeoffs are unavoidable” Yes and there are no limits to your imagination. However, beside the points mentioned above, there are a few common way to decrease the feedback loop latency for a low cost: as tempting as it might look to measure testing quality by the number of tests, try to limit them: don’t assert the same mechanism in different test; Use different test suites (smoke, perf, total etc.) with lower functional coverage but with shorter running time: a fast suite can be ran continuously, while the more demanding one are executed only twice a day. e2e tests are flaky – C: “My tests are flaky and hardly reproducible. WTF?” – A: “Sort out root causes and use protractor-flake ” The topics is not new, and as Simon Steward stated in his 2009 blog post , “It’s very seldom because you’re uncovered a bug in the test framework.” If having incremental tests (each action is companioned with an assertion) certainly helps to nail down the problem origin, the root cause can lie in external services or other unmastered dependencies. The idea of running the failed tests a second time is not the most appealing, from an intellectual point of view, but it might solve those type of problems. Protractor-flake is a tool that will conveniently relaunch the failing suites only: protractor-flake --max-attempts=3 --color=magenta -- --suite=all NB: protractor-flake needs the terminal reporter to be activated, dumping the stack trace to work. – C: “How can I grasp a global picture of which tests are repetitively failing?” – A: “Use Jenkins test-results-analyzer plugin” End-to-end tests are not instantaneous to be run, and the whole suite is typically executed once or a couple of times a day on a Jenkins server. It can be handy to compare through time which tests have succeeded or not. The popular test-results-analyzer plugins addresses this issue, by offering a time based comparison, as shown in the figure 4. One can immediately see that two tests have been failing over the last three executions, and both their names refer to “calendar”. This information, joined with the fact that they were executed at the beginning of a month, naturally points out the cause. F igure 4: the test-results-analyzer Jenkins report. For each suite and the last 10 execution, the success/fail status is displayed, together with an approximation of the running times. – C: “My application should be tested on multiple browser, but it appears not to work as easily as in the book. WTF?” – A: “Head up soon to multi browser” Via the multiCapabilities option, protractor offers the possibility to test multiple browsers. As it looks so trivial, the feature is often disregarded for faster test execution ( “it’s trivial, we’ll activate it later, when we’ll be in production” ). As trivial as it may seem, slight difference exists between browsers ( e.g. the TAB character might be sent after filling a text field in some firefox version). The situation can be expensive to solve if the multi browser capability is add only once hundreds of test have been written and some of them failing repetitively for unexplained reason. The solution is of course to activate the feature early in the development process, even though it will not be called upon at every test execution. And so much more can go South with e2e testing! – C: “Can I deal with two factor authentication with google token?” – A: “Yes, to a certain extent” Two factors authentication (2FA), with a time/seed based token associated to a classic password has become a de facto standard. The second factor is initialized at once with a random seed (typically exchanged via an email and a QR code) and usually generated for login by a smartphone. With these channels, the login process has therefore become more complex to hack, but also to automatically test. However, if the second factor is for example based on the classic time-based one-time password algorithm , such as the one used by Google, libraries are publicly available with an implementation. – C: “Some processes, like profile change or password reset, require email interactions. Is it possible to include them in the tests?” – A: “External interactions can take place, although it comes with a price.” A real system comes with emails exchanges, QR code images etc. Thanks to libraries available on npmjs.org , it is possible to scan a mailbox, or decipher a QR with an equivalent to a cell phone scanner. However, such connectors come with a price at development time, at execution time (think of allowing delays for emails to be delivered), and stability risks (the more dependencies on external systems, the more failures points.) – C: “How can I interact with a third party’s web application, hardly written with e2e testing in mind?” – A: “Write an independant (tested) library to interact with, via a REST API or browser navigation.” In one of the underlying example to this article, the custom application is strongly dependant on the service management enterprise software ServiceNow . To fully test the in house web application, some activities must be handled through this third party web application. If no REST API is available, one can also use protractor driver to navigate and simulate analyst’s workflows. The goal is not to test it thoroughly, but to have the minimum required interactions. An independent library is then to be written to interact with it. As the third party solution can be upgraded and changed without prior notice, some tests are also to be written to harness the stability of your library. The methodology perspective Until now, we have covered vast lands of technical challenges to overcome the end-to-end testing of a complex solution. If we believe that we can push the frontier of testing, it is also important to take into account design and methodology. Testing shall not be done in an isolated box, it shall be conducted hand in hand with the development process. This journey is a matter of design choices, team organization and methodology. Design for testing “A software system can best be designed if the testing is interlaced with the designing instead of being used after the design.” A.J. Perlis NATO Software Engineering Conference, 1968 – C: “As a test developer, I face so many different manners to locate elements on a web page. How could we build a more consistent experience?” – A: “Define guidelines, improve then and include e2e testing early on the functionality development.” We have seen in the previous sections how little browser manipulation are out of reach of Protractor. But being able to do any thing is not a reason for willingly doing anything. The test source code must be coherent, modular, light, readable and sustainable. Coding guidelines shall take into account the testing perspective, writing for example id attributes in camel case, or applying a predictable logic for naming classes. Based on observations, JavaScript code lack the culture found, for example, in Java. Tools, such as jslint , can be used in continuous integration to ensure that some rules are followed by the team. However, communication among developers and a minimalist documentation remain the most efficient channel. – C: “Sometimes tests are hard to write on a feature and the user story exceeds its initial estimation.” – A: “Guess what? Embed the testing perspective in the estimation!” As obvious as this point may appear, tests are scarcely evaluated during a User Story estimation ( Tres amigos or spring planning ). A simple question shall be ask when planning: “do we foresee any particular challenge to test this feature? How much do we think is can cost?” This question can stimulate a discussion on the story from an original perspective and shake design choices.. – C: “Our designer followed some business requirements and have built pretty custom UI widget for option selection or a phone number input field. How can I test those non standard widgets?” – A: “Don’t reinvent the wheel and share the testing costs with stakeholders.” “Let’s imagine a super cool new widget to enter a phone number. Based on the country code, it will automatically format the text string, following any fashion one can enter her number. What a cool way to enter a phone number we are going to invent!” Fueled by imagination and positive energy, new widgets are drawn on the board, that will “revolution their user’s experience”. Designer are hired to shape those widgets, often outside the development team. The latter will be then in charge to embed the revolutionary components into the application, test them… and fix them. And then, rules reveals to be more complex than first thought, bugs appear, execution is slow. Unexpected behaviors happen when copy/pasting or typing to fast…  And a simple phone number input field has burned 6 weeks of work, to be seen once in the whole registration process. Beside a poor evaluation of cost/value, we see with this example how the testing has to be part of the initial widget design and not be relegated to an afterthought. And how the “Keep It Simple, Stupid” principle be grounded by a testing perspective. Shortening the feedback loop – C: “Even with all the above principles in action, how can we make the e2e suite run faster?” – A: “The fastest test is the one which is not executed.” Throughout this article, we have see many hints and tips to make web e2e tests run faster. Without reducing the actual testing coverage, there are a couple of major directions where huge savings can be claimed: beware of the ice cream pattern (fig. 2)! Features shall be tested by unit and integration suites wherever possible, as they cost less to write, easier to refactor and can be order of magnitude faster to run. WIth the same idea, do not use e2e tests to assess underlying REST API. Don’t Repeat Yourself! Once more, a software craftsman motto can be applied to writing tests. Although it is tempting to copy/paste test suite files, only to make a few partial changes, it will prove to be costly in the longer term. A more insidious pattern is to test the same component, derived through multiple pages. Although these two advices might seem obvious, experience shows how thes anti-patterns are repeated again and again in multiple projects. – C: “What shall we test?” – A: “Use a comb coverage” Obviously not all workflows, with all their detours and particularities, can be explored by functional tests. They would become too expensive to build, run and maintain. The “comb pattern”  strategy consists in: Aiming for the largest low depth testing , where for example all pages are at least assessed for their main component presence. Diving deeper for the most business critical web site areas ( e.g. the web registration process, where customer shall not be lost at any cost) and try to cover at least once every meta-component or workflow types. If a bug appears in acceptance or production, the opportunity of an e2e test shall be assessed (the feature was not as trivial as the developers first thought). Team Organization Before After Figure 5: an organization segregated by responsibility (up) is a reason to have slow feedback loop and ultimately long TTM. If Dev/Ops proximity is well accepted nowadays, testing culture shall also be shared among the team (down). The last topic of this article, but maybe the most important one: shaping the team to make testing be part of the software development cycle, from idea to release. The boundary between development and operation has been blurred by the DevOps terms (or if it has not been blurred in your IT department, at least the buzz  has certainly made its way through). Sadly enough, a wall still exists between “developers” and “testers” in many organisations, where the later only to access to the level of the former. “You build it, you run it!” said Amazon CTO Werner Vogels. And what about “you build it, you test it”? In a small team delivering a working software through short iterations, the developer work is by essence versatile. If no one contest that unit testing is done on par with feature implementation, by the same person (or pair), why the end-to-end testing should be pushed out of the cycle? Why should it be sent to junior developer or outsourced to the other side of the world? We believe that e2e test development are to be a crucial part of the agile team, with any skilled enough member taking her share to the effort. The full testing pyramid then becomes a team responsibility, bugs are more seldomly filed as “defects” and the final product quality is to be pursued as a whole. Heading Further Alexander Tarlinder, through his book “ Developer Testing: Building Quality into Software ” or  a podcast interview on IEEE software engineering radio “ Agile Testing: A Practical Guide for Testers and Agile Teams ” by Lisa Crispin and Janet Gregory Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “End to end testing from the trenches with Protractor” gagannarang 01/06/2018 à 13:09 Thank you for sharing such valuable information , This is going to help me a lot and thanks for explaining and giving the actual data for testing part.Hope you will keep on posting to update us.Actually i like your post your way of explaining is very simple and effective. javix 05/09/2018 à 09:44 Thanks for the interesting article. However, I have a question about that. \r\nIn my test I changed all (by.css) to (by.id) on a trial basis. To measure the time of each protractor step I use the 'protractor-timeline-plugin'. \r\nBefore the conversion stood there: \r\n\"command\": {\r\n    \"name_\": \"findChildElements\",\r\n    \"parameters_\": {\r\n        \"using\": \"css selector\",\r\n        \"value\": \"news-form-input [formcontrolname = \\\" name \\ \"]\"\r\n    }\r\n}\r\n\r\nAfter the changeover:\r\n\"command\": {\r\n    \"name_\": \"findElements\",\r\n    \"parameters_\": {\r\n        \"using\": \"css selector\",\r\n        \"value\": \"* [id = \\\" name \\ \"]\"\r\n    }\r\n}\r\nI miss the output \"using\": \"css selector\". Although I use the id selector.\r\n\r\nI have to say that the number of protractor steps has been reduced from about 900 to about 600.\r\nBecause I'm calling element by.id (id) instead of parentElement.element (by.css (...)). This saves the previous search for the parentElement. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-09-06"},
{"website": "Octo", "title": "\n                API, Zero Downtime Deployment and SQL Migration: Theory and Case Study            ", "author": ["Julien Kirch", "Benjamin LACHS"], "link": "https://blog.octo.com/en/versionning-dapi-zero-downtime-deployment-et-migration-sql-theorie-et-cas-pratique/", "abstract": "API, Zero Downtime Deployment and SQL Migration: Theory and Case Study Publication date 28/07/2017 by Julien Kirch , Benjamin LACHS Tweet Share 0 +1 LinkedIn 0 To demystify Zero Downtime Deployment In the patterns associated with the web giants , the Zero Downtime Deployment (ZDD) shares a characteristic with auto-scaling: it is talked about even more because it is implemented so little. The ZDD is victim of a vicious circle: it looks very complex because it is not very practiced, and as it is little practiced, it is thought to be very complex. To get out of this impasse, this article presents a practical case, code with support. The goal is not to make everyone ZDD, because it will add more work and complexity, but provide you a clear vision of how to achieve it. This will allow you to decide whether or not it is worth doing. Our example case Our example approaches the one described in the first article . An application exposed via a REST API. Initially this application manages people, each person having zero or one address. This version is exposed on the prefix /v1 using a unique resource type /person . The next version of the application will associate multiple addresses to a person, and will be exposed on the /v2 prefix using two resources, /person and /address . The two versions This application highlights its high availability, so it is essential that all operations are carried out without interruption of service. The API is public, so we do not control the use of it. It is therefore impossible to make a toggle from /v1 to /v2 at one time. The two versions will therefore have to work together to allow the migration of all users. This period may be very long depending on the case. Customers consume APIs via intermediaries, so it is possible that during this period they use both versions /v1 and /v2 . In the rest of the article, /v1 and /v2 correspond to the versions of the services, and v1 and v2 correspond to the two ways in which the data will be stored in the database. The strategy How to manage the migration ? It is possible to do this without a migration script in the traditional sense: you expose services in /v2 , and when called for data still in /v1 format, you migrate the data on the fly. This allows data to be migrated gradually as service users call the APIs /v2 . This is the approach that is often taken with NoSQL databases. Unfortunately, by doing so, the migration may never end, or else it may take a very long time (if you purge older data). During this time, you must maintain the additional code to support this case. The other approach is to use a script. This ensures that migration takes place quickly. This is the same type of script you use for your usual migrations, except that it must take into account the fact that it runs at the same time as the code. Thus, all operations that create locks for more than a few milliseconds should not be used, and you must ensure that you do not create deadlocks. Migration must be done in a transactional way, to avoid inconsistencies in the data. In case of problems, the script must also be able to be interrupted and restarted without disrupting the execution of the program. In the article it is this approach that we will use. When to migrate ? Without ZDD, the migration process is conventionally the following : Close the service Stop the application Make a backup of the data, in order to be able to restore it in case of problems Migrate the data Deploy the new version of the application Start the new version of the application Verify that everything works Open the service In ZDD, there is no longer any question of closing the service: the migration of data takes place “hot”. There are two possible ways to do this, depending on whether the migration takes place before or after the services are opened /v2 : The two ways to migrate Number of the stage Code version Migration after the opening of /v2 Migration before the opening of /v2 1 1 Application serving /v1 with a data model v1 2 1 Change the BDD schema to allow storing the necessary data to /v2 3 2 Deploy a new version of the exposed application /v1 and /v2 on the v1 or v2 data model Deploy a new version of the exposed application /v1 on the v1 or v2 data model 4 2 Migrate data to v2 without disruption, taking into account code /v1 and /v2 Migrate data to v2 without disruption, considering the code that serves /v1 5 3 Deploy a new version of the exposed application /v1 and /v2 on the v2 data model 6 3 Clean BDD Schema of Artifacts v1 7 4 Deploy a new version of the manager /v2 application on the data model v2 The first approach opens services /v2 faster because there is no need to wait for the data migration. The second approach is simpler : The exposed version of the application that works with the data models v1 and v2 only exposes the services /v1 , so you do not need to worry about when a service /v2 call accesses data v1 ; During data migration, services /v2 are not yet exposed, this means fewer data access patterns to consider when designating a migration that avoids data inconsistencies and deadlocks. Unless your migration process is extremely long, the second approach is preferred, and this is the one that will be used in the rest of the article. /v1 and /v2 are in a boat … Migrations of open APIs pose two business problems and a technical problem. How to migrate data ? The first problem, also valid for closed APIs, is how to migrate the data from /v1 to /v2 . I do not speak from a technical point of view but from a business point of view: the semantics changes between the two versions, so we must determine how to transform the data from /v1 to /v2 in a logical way, and in a way that does not surprise the users of the API. In our case the solution is immediate : /v1 has at most only one address, and /v2 can have several addresses, the address of /v1 thus becomes one of the addresses of /v2 . How to handle backward compatibility ? The other problem is how to interpret data /v1 in /v2 . If the API is open, your users can call your services /v1 while the data is already in the /v2 model. It is often more complicated than the first car as the evolutions, the API tend to become richer. Accessing richer data from the /v2 through the narrower prism of the API /v1 can be a real headache. If this is the only way that this transition happens, it is sometimes necessary to adapt the design of the API /v2 . It is a balance to be found between the transition facility, possible restrictions to add for IPA callers, and the time to invest. How to respond quickly and well ? The technical problem is to achieve the various services, including compatibility, while making sure to always have consistent data and without penalizing performance (too much). If, between the two versions, the data is no longer structured in the same way, the compatibility management can request to cross the data of several tables. So in our example, in v1 the addresses are stored in the person table while in v2 they are in a separate adress table. During the compatibility period, calls to v1 that update the person’s name and address change the two tables in a transactional manner to prevent a v1 reading that would occur at the same time from returning inconsistent data . Additionally, you have to be able to do this without having to put too many locks in the database, because otherwise it will make data accesses too rude.. The best strategy is to focus on an approach that you have mastered well and that gives acceptable results rather than a more efficient or faster but more complex solution. In all cases, tests are absolutely essential. To serve both versions of the API, you can use a single application or choose to separate your code into two applications, one per service version. This question is not relevant for the question of the ZDD, we choose not to consider it here. In our example, we chose to have only one application. … and ZDD joins them on board Without ZDD the situation is clear: we stop the application, the data is migrated, and we restart the application in the new version. So there is a before and an after. With ZDD the migration takes place while the services are available, so there is an intermediate situation. During this period, the data can still be stored in /v1 format or migrated to /v2 . It is then necessary to determine in which state the data is: in order to know which code is to be called, it is necessary to know whether the data has been migrated or not. In addition, the piece of code in charge of this will be run very often, so it must be very effective. In case of difficulty, the solution that should work in all cases is to add to the tables involved a number indicating the “schema version” of the corresponding data, and which will be incremented when the data is migrated. In this case the verification operation is very simple and fast. The column addition operation is then done in advance of phase, which increases the work necessary for the migration. If you choose to migrate data after opening /v2 , the case where an api /v2 is called while the data is still stored in v1 format is added. It is then necessary to migrate the data hot, in a transactional manner by limiting the induced slowdown. To summarize, there are four situations : Call /v1 Call/v2 Data stored in format v1 Reply as before (Only if migration after opening /v2 ) Migrate hot data Data stored in format v2 Compatibility v1 Reply with the new semantics Well open /v2 , and properly decomission /v1 When you open /v2 for the first time, pay attention to how the toggle to the new version is made. Before making the new endpoints accessible, make sure that all servers use the latest version of the application. Otherwise, if you call a /v1 when the corresponding data has been migrated to v2 the code will not be able to read it correctly and may crash or return false information. Another problem is the way you have implemented the data changes when you call an API /v1 . The first case is to save the data in v2 format, but this means that the previous versions of the application will not be able to read it again. The easiest solution is to use feature flipping to switch the code. Otherwise, your code must detect in which format the data is stored, and re-save it in the same format: a piece of data in v1 remains in v1 , and a piece of data v2 remains in v2 . The feature flipping is avoided, but in exchange the code is more complex. In order to decommission /v1 it is enough to make the endpoints inaccessible, the deletion of the code can be done later. About locks and schema changes As we have seen, the ZDD relies heavily on the use of the database, and in particular its concurrent access features. If your business requirements are simple, you are using an ORM, and you have automated performance tests, this is an area you do not often need to consider. If you do not do it right, it’s easy to block the database, return errors (in case of deadlock), or inconsistent results. Our advice is to document your upstream or even to make POCs to avoid having to redo a design because your database does not work as you imagined. Do not trust memories or rumours: read in detail the documentation for the version of the tool you are using, and most of all ensure that you test ! If you have never dug into these topics or are rusty, the first migration will surely require a lot of work, and will give you a few cold sweats when you run it. But tell yourself that all the subsequent operations will deal with the same concepts, and will therefore go much better. It’s not just the REST in life REST has two features that make it an ideal candidate for the ZDD : Exposing several versions of services is a standard practice ; The calls are supposed to be stateless. If your services are exposed in any other way, you will need to be interested in these topics. Sessions, like all cache types, may require special attention if the data they contain is subject to a change in version structure. Back to our example We assume that the data model directly follows the resources to be exposed. The address is initially a field of the person table, and is migrated to a separate address table. The evolution of the schema We do not use a specific column to store the “schema version” of the objects. Instead we will check in the database to see how the data is stored: if the person table contains an address, it is because it is in version v1 , otherwise it is necessary to check the existence of an address in the dedicated table . This avoids adding to the SQL schema, but increases the number of queries executed. Steps to follow for migration : Initial version: the address is in the address column of the person table, the code only works in this way. Adding the new table address in the database, at this stage the code does not yet know this table. Deploying the code that provides the api /v1 and that is compatible with both ways of storing the address. Running the migration script. Deploying the code that provides api /v1 and /v2 and which is compatible with the new way to store the address, the address column of the person table is no longer used by the code.. Removing the address column of the table person table. The ZDD adds code versions and migrations of intermediate schemas. In an environment where deployments are not automated, this means an increase in workload and therefore the risk of error. So it’s better to equip and have a reliable delivery pipeline before you get started. Detailed analysis Compatibility of services In our example the problem of compatibility is the following: once a person migrated, it can have several addresses. What do you do when you retrieve that same person through the API /v1 ? Here there is no obvious answer: there is no notion of preferred address, or last address used that would provide a way of discriminating the different possibilities. Since the response affects the behaviour of the API, it is a decision to be made by those skilled in the art. The solution chosen here is to return an address from the one in the list. It is not perfect, but it can be acceptable according to the use made of it: it is up to the persons skilled in the art to decide. Transactionality To solve the question of transactionality, we chose the simplest solution: put a lock on the corresponding entries of the person table. If all the operations follow the same principle, this lock acts as a mutex by making sure that the calls are executed one after the other: when an operation poses a risk, it begins by asking for access to this lock, and for this she must wait her turn. Example with a call to PUT /v1/people/127 while the corresponding person is stored in format v2 but has not yet an address. Example without lock : Execution thread 1 Execution thread 2 PUT /v1/people/127/addresses PUT /v1/people/127/addresses BEGIN BEGIN SELECT * from person where id = 127 to retrieve the person, verifies that there is no address and that the other fields are not to be modified SELECT * from person where id = 127 to retrieve the person, verifies that there is no address and that the other fields are not to be modified SELECT * from address where id_person = 127 to retrieve an address to be updated, does not find it and deduces therefore that it is necessary to insert one SELECT * from address where id_person = 127 to retrieve an address to be updated, does not find it and deduces therefore that it is necessary to insert one INSERT INTO address … to insert the address INSERT INTO address … to insert the address commit commit Result: the person ends up with two addresses ! Example with lock : Fil d’exécution 1 Fil d’exécution 2 PUT /v1/people/127/addresses PUT /v1/people/127/addresses BEGIN BEGIN SELECT address from person where id = 127 FOR UPDATE to retrieve the person, verifies that there is no address and that the other fields are not to be modified and locks the line SELECT * from address where id_person = 127 to retrieve an address to be updated, does not find it and deduces therefore that it is necessary to insert one INSERT INTO address … to insert the address commit which releases the lock on person SELECT address from person where id = 127 FOR UPDATE to retrieve the person, verifies that there is no address and that the other fields are not to be modified and locks the line, waiting for the lock on person to be available SELECT id, address FROM address WHERE id_person = 127 retrieves address SELECT * from address where id_person = 127 to retrieve an address to be updated, finds the address inserted by the other thread UPDATE address set address = … where id = 4758 updates address commit which releases the lock on person Result: a single address. The SQL Migration Script The migration script moves the data in blocks of person to address . In our example, once the code is switched to the new version, all data is written in v2 format, whether it is creatives or changes.. Migration is therefore irreversible, we know that it is enough to migrate all the data once for the work to be done. It starts by retrieving the highest id of the person . As the script is launched after the new version is deployed, all people created after that time are created with an address stored in address . This means that the script can stop at this value. The script iterates in groups of person from 0 to the id that it has just recovered. The step of the iteration is to be determined experimentally: a larger step makes it possible to make fewer requests thus to decrease the total time of the migration, to the detriment of the unit time of each iteration, and thus the time when the locks exist in base. It starts a transaction. It selects the id of people who have an address, and locks them. It inserts the corresponding data in address with an INSERT … SELECT …` . It clears the address field of these entries in the person table . It validates the transaction, releasing the data. If the script is stopped, the already migrated data is not lost, and restarting the script does not cause any problems, since the migrated data is not reprocessed. Steps to follow Initial version providing the API /v1 and where the address is stored in the address column of the person table. Added in base of the address table, not yet used by the code. Creating a table does not normally have any impact on the database, but it must be verified. Provides the API /v1 , stores the address in the address table and knows how to read it in both places. During a reading in /v1 on a data item /v1 the data is not migrated to v2 to keep the code simpler. Migrates addresses to the address table. Provides the API /v1 and /v2 , and only reads it in v2 format, removing the address column from the person table of the code, then the column is always in base. Delete the address column of the person table. In some databases, deleting a column triggers the rewriting of the entire table and therefore can not be done in ZDD. We are therefore satisfied with a logical deletion, for example by adding an underscore in front of its name, and by “recycling” it when a new column is needed. The implementation The implementation is on GitHub . The code is open source under MIT license, so you can use it.. Each step of the migration is in a separate module, it allows to easily examine what happens without having to handle git. The code is in Java and references the Dropwizard library. The database is PostgreSQL, access is via Hibernate, and SQL migrations utilize Liquibase . Some highlights : In step 3, the person’s DAO with the methods to set locks to avoid inconsistencies. In step 4, the migration script . Since this is a script and not a single query, it is in the form of a Java class called since Liquibase . In step 6 it is possible to remove column address because PostgreSQL simply makes it invisible, and retrieves the space later . To conclude Doing ZDD is not magic: it takes work and rigor. If you can do without, so much better for you, but if you need it, you should now have a little more precise idea of ​​what it represents. Remember that the example developed here is a simple case: use it to get an idea of ​​what to do, and not as a guide to measuring effort. The first migration will surely be a bit of a challenge, but the next ones will be easier. In any case, do not forget to test, test, and still test ! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Stratégie SI and tagged SQL , ZDD . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-07-28"},
{"website": "Octo", "title": "\n                Discovering Flynn            ", "author": ["Thibaut Gery"], "link": "https://blog.octo.com/en/discovering-flynn/", "abstract": "Discovering Flynn Publication date 15/07/2017 by Thibaut Gery Tweet Share 0 +1 LinkedIn 0 Flynn is a Platform as a Service that allows to deploy and scale applications easily. It is based on containers, it is open-source, and its pitch is: Throw away the duct tape. The first impression There are many ways to install the platform , using vagrant, or directly on a server. I chose to install it on AWS. The set-up is simple: through a custom web interface, you configure a CloudFormation template that is run on AWS: create the EC2 instances with the correct security groups and settings install Flynn on the instances setup the DNS record redirect a subdomain from the Flynn team (olxl.flynnhub.com in my case) create a hosted zone on route53 with all the setup for high availability create alarms on endpoints From the Getting started , within 2 minutes, I could deploy an application that was previously on Heroku. Needless to say, I was thrilled to create a PaaS platform able to scale and deploy applications in less than 30 minutes (mostly waiting for CloudFormation). Everything seems automagic, but is it dark magic or pure genius? Let’s find out… Can it live up to the hype? How can we deploy an application? First of all, it is necessary to develop the application following the 12-factor app g u i d e l i n e s. There are two ways of deploying an application after declaring it in Flynn. Create an Application on Flynn After cloning the repository https://github.com/ThibautGery/todomvc-mongodb , you can configure your application using the CLI: Using the heroku buildpack git push flynn When using git, the code is pushed to a remote git repository. The buildpack will detect the application’s language and download its dependencies before running the server using the Procfile. You can also use the GUI using the Github integration to deploy clicking on buttons. Using Docker The repository must contain a Dockerfile in order to build the Docker image docker build . -t my_image flynn -a my_app docker push my_image Flynn scale app=1 Using Docker, the image is pushed to a remote Docker registry. Then, the image will be run as a container after you configure the scale option. But first you need to configure Docker on your CLI: Databases Flynn offers little or no configuration possibilities on the application. Rancher , for example, offers some kind of configuration with a config file (similar to docker-compose). Flynn doesn’t, but it packages four ready-to-use databases: MySQL, PostgreSQL, MongoDb (not sure it qualifies as a “Database”…) and Redis. But there are some restrictions: Flynn’s databases are currently designed with staging, testing, development, and small-scale production workloads in mind. They are not currently suitable for storing large amounts of data. From: flynn.io/docs/databases The developers don’t give any information about their understanding of “ small-scale workloads “, thus you should run load tests to verify the stability of the platform in your context. Using Flynn There are four tools to use the Flynn platform: the developers will be using the GUI, the CLI, the API. The Ops have their own CLI to perform tasks on the cluster. The developers can create the application, scale it, and remove it. They can manage their databases (MySQL, PostgreSQL, Redis, Mongodb), their HTML routes, their SSL certificates, their jobs and environment variables. Everything can be done using the GUI (great when starting) or using the CLI or the API (great for automation). The Administrators can manage the cluster and the nodes (promoting or demoting them…). For example, they can list the nodes: $ flynn-host list ID         ADDR             RAFT STATUS ip1002237  10.0.2.237:1113  peer ip1001170  10.0.1.170:1113  peer (leader) ip1006123  10.0.6.123:1113  peer High availability The best way to check the availability of a system is to run a benchmark. The setup follows this configuration: Used a cluster of 3 nodes deployed on AWS t2.medium instances I deployed the todo App https://github.com/ThibautGery/todomvc-mongodb with MongoDB using Docker I deployed the application using three containers (scale = 3) Used 3 A records from todo-docker.vgry.flynnhub.com pointing to the 3 cluster’s nodes Created a Gatling test available here Run a control assay on the cluster, the results can be found here Run a test and 3 min after starting, brutally stop one cluster node , the results can be found here There were 8 users that had the following error: j ConnectException: Connection refused: todo-docker.vgry.flynnhub.com/52.62.245.207:80 The DNS on Route53 records uses health checks and as soon the error was detected the DNS was changed. By the time the DNS was propagated, the faulty server was not accessible to the public. The test shows that the current setup provides High Availability (HA) but there is a delay since it relies on DNS propagation. Depending on the use case, it might be sufficient. If you need a more aggressive HA, you can use a system in front to do it, like AWS Elastic Load Balancer or HAproxy . A few notes: This test didn’t take into account the database, therefore before deploying in production, run additional tests The sample application is really bad when it comes to web perf, please Concatenate, Compress and Cache. Behind the hood The platform is built in Go and has several components and everything is detailed in their documentation : flynn-host is the main daemon it is responsible for running the container of the host using libcontainer , a docker submodule. Every other internal or external services are running in containers. Discoverd: custom service discovery component flannel : virtual network developed by CoreOS postgresSQL : database to store the internal state of the cluster controller: custom component providing the HTTP interface for communicating with the scheduler scheduler: custom component responsible for starting stop and scale the different application in the cluster Router: custom component routing the HTTP/HTTPS request to the correct internal API or application Buildpack : using Heroku component to create application Log aggregator: custom component to centralize logs using TCP. Do it yourself features The missing features are: There is no account management, just a token to connect to Flynn, you cannot restrict the access to different applications. There is an active open ticket but it has been pending since November 2013… There is no centralized log, there is a component called logaggregator but there is no documentation about it. The hacky solution could be: using the CLI  the data could be streamed to an external system. This solution might not scale. The better solution would be using a log aggregator like logstash or fluentd and let the administrator configure the tool to send the log to their favorite system like elasticsearch , S3 … In my opinion, this is the main blocker of using the platform in production. There is no integration with the common supervision platforms (like New Relic or Datadog ) for the cluster itself, you will need to install the agent yourself which is not a problem. ( More info ) Conclusion Flynn is a promising platform because of its ease of use however it is still a young framework and it is lacking some key features needed for enterprise deployment. Flynn is not the silver bullet, here are some good reasons not to use it: If you deploy one application, no need for a platform, just use infra-as-code tools such as Ansible, Puppet or Chef. If you plan a heavy load on your databases, you can’t use Flynn’s ones If you are a big organisation and you want more granular access control However, in the meantime, if you need to deploy several simple applications like an MVP to test ideas, the platform will help you to reduce your time to market. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Discovering Flynn” Khalil 17/09/2017 à 11:11 Thanks for writing this detailed brief. I'm currently thinking about trying out Flynn as a means to reduce the cost of hosting different apps on Digital Ocean.\r\n\r\nQuick question: I gather that Flynn is useful for simpler deploys (e.g. git push) and for scaling apps (thanks to Docker); do think that Flynn can be used to host multiple different apps on a single cluster. Concretely, can I use Flynn host 9 different web apps on a cluster of 3 machines? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-07-15"},
{"website": "Octo", "title": "\n                IoT and Security: an impossible union?            ", "author": ["Jordan Afonso"], "link": "https://blog.octo.com/en/iot-and-security-an-impossible-union/", "abstract": "IoT and Security: an impossible union? Publication date 22/06/2017 by Jordan Afonso Tweet Share 0 +1 LinkedIn 0 One of the most crucial questions to ask about the Internet of Things today is: does the IoT have a future despite its inherent lack of security? Studies show that despite the obvious acknowledgment that this ecosystem is full of security breaches that can endanger its functioning, companies’ interest in IoT does not go away. Indeed, in a pragmatic dynamic, organizations opt for the undeniable contribution that these technologies bring on their business and their methods of work compared to the dangers generally accepted as a fatality. A study by the institute 451 Research shows that 71% of organizations use data with the Internet of the Things and wish to increase their investments in this area for the next 12 months. However, cyber security remains the main obstacle to the emergence of new uses and a total adoption of these technologies. This raises a question: can we really consider the security breaches found to be a fatality? Spoiler Alert: The answer is no! We will try to demonstrate that in this article. The wrong example The events described below are based on real facts. Any resemblance to cases and people you have already met is totally normal. Let’s introduce Bobby. Bobby is a fairly wealthy forty years old man who owns a sublime house very envied. That is why he wants to have a video surveillance system. Another crucial detail, Bobby is a handyman deep down. So he went to his favorite e-commerce website and ordered half a dozen “easy installation” IP cameras. The installation is actually really simple. It could either connect his cameras via Ethernet to a home hub or via  WiFi. As he had a fiber network that delivered a broadband WiFi, he chose the wireless configuration. The rest of the procedure is perfectly described in the documentation which Bobby naturally hurried to read. When the cameras are switched on for the first time, they broadcast a WiFi network to which the user needs to connect. Then, with a very clean web interface, the user needs to put the username and password of his local network to get the public IP of the cameras in return to access it from anywhere. Once they are connected to the local WiFi, as indicated in the documentation, Bobby can connect to his cameras by entering the default login (“root”) and the default password (“root”). Reassured by his installation, he now feels safe. What Bobby has just done is actually much more dangerous than leaving the sublime house unattended. It has just created a huge gap on his local network by allowing anyone to connect to it. Today, there are robots capable of scanning all the IPv4 addresses of the world in less than 24 hours and trying to connect to them with widely used login / password (“root”, “admin”, “password”, “12345 “,” Qwerty “…). Bobby only followed an incomplete documentation that did not strongly suggested (or even encouraged) to change his login details. In addition to providing access to the video stream, Bobby’s connected objects will be potentially used by the robots to carry out attacks of which he will be recognized as responsible. Furthermore, all the connected elements in Bobby’s local network are now in danger. The strength of a chain depends on the strength of its weakest element and here, a very sensitive element has just been added. But Bobby’s story is not over. He has just bought a gorgeous German car with a 4-ring logo. To open his car, he has nothing to do. It is enough to be near its vehicle with his electronic key in his pocket in order to open the car automatically (or just a button to press). This constructor had chosen not to encrypt the data of the key which allowed the opening. An interception of this RFID signal is sufficient to obtain the opening code which is found to be identical on millions of vehicles. A malicious person can thus recover this digital key and thus open and start more than 100 million vehicles in the world. On top of it, a vehicle theft without breaking traces is not covered by insurance. The German manufacturer then chose to implement a HiTag2 encryption algorithm specifically designed for NXP Interception module based on Arduino microcontrollers which were present in those vehicles. This algorithm, which dates from the late 1990s, takes today only 10 minutes to be bypassed with equipment costing less than 30 euros. Especially for all vehicles produced until 2016, the manufacturer’s recommendation is to disable the hands-free mode of these keys … Want to know if your vehicle has been hacked? If the vehicle refuses to open (except in case of battery problem) when the key button is pressed, the signal has already been intercepted and used. Indeed, in order to couple the new algorithms with a simple technique, the manufacturers have added a requests counter for opening controls in the vehicles microcontrollers. If this counter is not synchronized with the number of presses stored on the key, the opening command will not work. Security in the IoT The lack of security in the IoT is only a consequence of the business impact on the projects. The ever-shorter Time To Market and the willingness to “do IoT” by unscrupulous actors causes precipitation and mistakes that are easy to avoid. There are special characteristics to this field that can make the security point of view a little less affordable than simple prototyping. With the multiplication of DIY (Do It Yourself) tutorials, lower prices on hardware (microcontrollers, sensors, telecommunication modules, cloud offer…) and their simplified programming interfaces it has become much more easier to create your own small communicating object. However, from a more industrial perspective, it is necessary to master more complex concepts to counter the specific faults of the connected objects. There are 3 types of vulnerabilities: Hardware vulnerabilities Software vulnerabilities Communication flaws Let us go beyond these generalities to glimpse the light at the end of the tunnel. Hardware vulnerabilities One of the hardware flaws that may seem trivial but which is actually the cause of many troubles is access to the debug interface. It is often necessary for a developer to retrieve information through these interfaces to track the progress of a program over time. The problem lies in the possibility of transferring sensitive information such as identifiers or frames of unencrypted data. The most sensitive objects are those that may need to be connected to a PC via USB. Some interfaces such as the Android Debug Bridge (ADB) even allow you to run commands on the object itself. It would be enough to disable this interface during the production of the objects so that it is impossible to access them. JTAG connected to a Samsung smartphone A tool well known by electronic technicians can be the source of a total loss of control over connected objects. The JTAG (Joint Test Action Group) is a hardware interface allowing initially to test continuity on the circuit board roads (absence of short circuits). This tool is used to give access to all the input pins of the integrated electronic components. The objective is to perform black box tests on logic components, ensuring that the output signals comply with selected inputs. However, the strength of this tool is not limited to testing. It can also be used as a BDM (Background Debug Module) to emulate an embedded system and retrieve its internal signals. The use of these signals can allow to reconstitute sensitive data. It is even possible to reprogram the flash memory of objects with this tool and to implement its own code with a completely different behavior. This technique requires a particular connection that is not to be made available in the production version of the object in question. In both cases, exploitation of the faults requires direct access to the hardware. A good practice consists in realizing a good packaging making it as difficult as possible to access the hardware, especially if the object does not need to be charged up. This must be combined with good management of the electromagnetic environment. It is important to know the environment in which the object will evolve to take countermeasures. An increasingly popular concept in the latest generation of microcontrollers makes it possible to isolate the OS from the object and the application part. The concept of Safezone or Trustzone makes it possible to isolate physically the sensitive code which exploits the material in its whole and the application code. These codes will run on different parts of the hardware and the alteration of the application code will have very little impact on the overall object’s security. ARM TrustedZone architecture Software vulnerabilities In the embedded electronics world, the memory size is often reduced (a few KB) and this will cause quite a lot of worries. For example, the programmer must often manipulate the memory to the order of magnitude of the register. Particular attention must be paid to memory overflow. As in programming on traditional architectures, these implementation errors can cause behaviors that are difficult to predict and it is one of the first software flaws that hackers are trying to exploit. We have already mentioned the importance of changing the default login details that are often not encrypted on the device or in the telecommunications frames. It is a classic error that would be solved by asking the user to change his login details or he will no longer be able to use his material. Another hardware constraint will have an impact on the software implementation. The microcontrollers are generally provided with a low computing power which is often adjusted according to the target application. This low computing power limits the possible cryptographic operations. The libraries required to use proven encryption algorithms are often too large for objects, which implies the emergence of new standards. Hash operations, symmetric or asymmetric encryption, are however possible using libraries of reasonable sizes but very difficult to interoperate. There are also a large number of open source projects in this direction whose reliability is difficult to assess. Cryptocape: encryption dedicated component The solution in vogue is to integrate cryptographic functions into the silicon of the microcontrollers or thanks to dedicate components. This allows not to reduce the available size with bulky libraries (especially if one device uses several libraries) but the drawback is that it definitively fixes the algorithms used. It is hard to imagine a technician unsoldering all the cryptographic components and replacing them with others in case of faults discovered on an algorithm. The solution that seems best is the cryptography on the elliptic curves (implemented in the libuecc for example). These operations on discrete logarithms are simpler to perform than on integer similarities (RSA), but the resulting encryption is much more complex to break. It is estimated that a 200-bit key based on elliptic encryption is safer than a 1024 bit RSA key. The last software fault that happens to be very dangerous is the ability or not to update the software of a connected object. If a default is found, you need to be able to patch the fault found and not to leave an isolated infected device. To do this, you must implement the update’s signature and ensure that you have enough memory space. Signing helps to prevent a malicious person from using the update mechanism to take control of the object. Communication flaws This is the last step with concrete risks. Authentication is the first problem encountered regarding the huge quantity of devices. Methods like OAuth 2.0 or OpenID Connect 1.0 can be used to solve this problem, but for now they can only be used in HTTP. However, HTTP is not ideal for all use cases (such as device-to-device communications) and we often prefer CoAP or MQTT. The next challenge will be to implement effective authentication methods for these protocols. The (very) great diversity of communication protocols makes interoperability very complex and requires a fairly long process of appropriation. Data integrity must also be considered when performing checksum or redundancy operations that are not necessarily native to certain protocols. The checksum consists of performing an operation on the data whose result is sent with the original data. On reception, the same function is applied to the data and it is checked that the results are the same. The great success of LPWANs (Sigfox, LoRa, NB-IOT, Qowisio, etc.) also poses some problems. These low consumption, low-flow but very large-scale networks are experiencing an impressive growth (nearly 300 million euros of funds raised for Sigfox). These protocols are practical for certain use cases (Telemetry, heartbeat, …) but we always need to think about the counterparts. The trend is to position oneself on one of these technologies without really knowing the limits. Sigfox is, for example, a network for transmitting a maximum of 140 messages per day of 12 bytes of payload. Sigfox does not provide any method for encrypting messages on the network layer. It is up to the developer to implement encryption steps during the telecommunication. Download messages to objects are very limited (only 4 8-byte messages). We must therefore forget the updates via this network. It will also be complex to transmit encryption keys by this means. LoRa is bi-directional but not simultaneous. The payload is about 242 bytes so this is also difficult to use for updates. However, the data are natively encrypted. These networks are designed to transmit messages over long distances (up to 40 km) and have a very good indoor penetration. They therefore have their advantages but complicate certain security measures. Conclusion The last element in the IoT ecosystem, Cloud platforms, has not been addressed because they represent the nodes of the system on which special attention has been given to security. If they are not managed by the organizations themselves, they are managed by web giants (Amazon, IBM, Microsoft, …) which impose fairly strict security standards. Safety in IoT is only a matter of good practices and the flaws noticed do not sound the death knell of the domain. It is enough not to rush to a young market just to say that “We are in!”. Some issues are still being resolved but this is not so different from the classic internet. Some initiatives such as the Allseen Alliance or the Open Connectivity Foundation are even attempting to unify IoT communications protocols to define a single standard that can be understood by all elements of the ecosystem. The consequences of bad security are of course more dramatic than the fate of our dear Bobby. The privacy of your data is no longer guaranteed. These may be used for commercial purposes without your knowledge. Your local network, though well protected, finds itself exposed to all attacks. These attacks can exploit your hardware for malicious purposes. For example, you can read what Dyn has recently experienced. In addition, you are responsible for the items you own, whether you are an organization or an individual. So take into account the danger and trust the actors who take care of the good practices. Tweet Share 0 +1 LinkedIn 0 This entry was posted in IoT and tagged Best Practices , Connected Objects , Internet of Things , IOT , security . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “IoT and Security: an impossible union?” niral 09/07/2017 à 07:53 Nicely written and comprehensive post. Thank You! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-06-22"},
{"website": "Octo", "title": "\n                TDD with Vue.js            ", "author": ["Pierre Trollé", "Eric Favre"], "link": "https://blog.octo.com/en/tdd-with-vue-js/", "abstract": "TDD with Vue.js Publication date 14/06/2017 by Pierre Trollé , Eric Favre Tweet Share 0 +1 LinkedIn 0 Introduction: We’ve all heard of Vue.js, the last trendy JavaScript framework. Some have already played with it. But beyond the POC, it’s time to gear up for real life Vue.js projects. And what’s better than some TDD with Vue.js to achieve that? The point of this article is to share the basics to develop with Vue.js using TDD. To do so, we will use a very simple component based on Vue CLI default template. We will add to it a few WebFront basic features: two buttons to increment or add a random value (from 1 to 6) to a counter. You can find more information about Vue.js on its well designed documentation: https://vuejs.org/v2/guide/ . Consume without moderation. Git repository: If you get lost, every commit related to each step of this article can be found here: https://github.com/trollepierre/my_project_vue/commits/master Bootstrap: Like most front end frameworks, Vue.js come with a good CLI (Command Line Interface). It has most tools we’ll need et we’ll follow each step of its process: To install it: npm install –global vue-cli To create a new project, we will use webpack: vue init webpack name_of_my_project This command comes with several options. We advise to keep the default ones: – runtime build and compiler – vue-router – eslint – standard configuration (the AirBnb configuration is not advised to beginners) – Karma + Mocha, for tests – However, e2e tests (end-to-end) will not be covered by this article. Here you go, your new project is ready to be started: cd name_of_my_project npm install npm run dev That’s it, the environment is ready and a web page is waiting for you on http://localhost:8080 : Related commit: https://github.com/trollepierre/my_project_vue/commit/dd5d7c4 The Home Page generated by the Vue CLI And here is the folders tree generated by the CLI: Folders tree generated by the Vue CLI A test is already written. It’s a template test which is not very atomic. Indeed, it is asserting that after the view is built, the $el element, which defines the HTML, does contain a h1 tag with a text defined by a {{ msg }} variable. Hello.spec.js import Vue from 'vue'\r\nimport Hello from '@/components/Hello'\r\n\r\ndescribe('Hello.vue', () => {\r\n  it('should render correct contents', () => {\r\n    const Constructor = Vue.extend(Hello)\r\n    const vm = new Constructor().$mount()\r\n    expect(vm.$el.querySelector('.hello h1').textContent)\r\n      .to.equal('Welcome to Your Vue.js App')\r\n  })\r\n}) 0) Work environment Whatever IDE you use, you should install the Vue.js plugins. They are functional and provide useful development helpers. For this article, I chose IntelliJ with the Vue.js plugin. Foreword To simplify the writing of the tests, I advise to make the vm variable (for Vue mounted: vm is the Vue instance after construction and with a loaded template, refer to the lifecycle diagram ) a global one and to define a beforeEach method preceding all assertions. We can thus access the “Vue mounted” vm built for each assertion. Hello.spec.js let vm\r\n\r\nbeforeEach(function () {\r\n  const Constructor = Vue.extend(Hello)\r\n  vm = new Constructor().$mount()\r\n}) 1) Testing data First, we will take an atomic approach and unit test the msg variable value. To do so, we will use the vm. $data .variable_name accessor. Hello.spec.js it('should check that msg is Welcome to Your Vue.js App', () => {\r\n  expect(vm.$data.msg).to.equal('Welcome to Your Vue.js App')\r\n}) This test directly checks the variable value. This allows to only have one test to update should the initial value change. Hello.spec.js it('should render my msg inside a h1', () => {\r\n  expect(vm.$el.querySelector('.hello h1').textContent)\r\n    .to.equal(vm.$data.msg)\r\n}) Another solution is to directly provide the variable to the test constructor. Hello.spec.js it('should render correct contents', () => {\r\n  const data = {\r\n    data: {\r\n      msg: 'plop'\r\n    }\r\n  }\r\n  const Constructor = Vue.extend(Hello)\r\n  vm = new Constructor(data).$mount()\r\n  expect(vm.$el.querySelector('.hello h1').textContent)\r\n    .to.equal('plop')\r\n}) Related commit: https://github.com/trollepierre/my_project_vue/commit/bb5fa77 We can now easily add new variables, such as a counter whose initial value should default to 0: Hello.spec.js it('should create a counter with zero value', () => {\r\n  expect(vm.$data.counter).to.equal(0)\r\n}) To make this test go green, we can simply add a counter to our data: Hello.vue data () {\r\n  return {\r\n    msg: 'Welcome to Your Vue.js App',\r\n    counter: 0\r\n  }\r\n} Related commit: https://github.com/trollepierre/my_project_vue/commit/6d46beb We want to add this counter to the template: ⇒ The related test will have to check that the counter value does match the content of a div with a counter class Hello.spec.js it('should render counter with counter data value', () => {\r\n  // Given\r\n  const data = {\r\n    data: {\r\n      counter: 48\r\n    }\r\n  }\r\n  const Constructor = Vue.extend(Hello)\r\n\r\n  // When\r\n  vm = new Constructor(data).$mount()\r\n\r\n  // Then\r\n  expect(vm.$el.querySelector('.hello div.counter').textContent)\r\n    .to.equal('48')\r\n}) ⇒ We run the tests and check that they are red for the right reason. ⇒ We write the simplest code to make it green. Hello.vue <div class=\"counter\">{{ counter }}</div> ⇒ We run the tests again, and they go green! ⇒ Let’s see if we can do some refactoring ⇒ We can move to the next assertion Related commit https://github.com/trollepierre/my_project_vue/commit/a7b267a 2) Test the name To test our Vue name, we can use the $options accessor. $options returns all options necessary to the instantiation of the vue, such as its name, its methods, its directives or its components. Hello.spec.js it('should check the name of my vue', () => {\r\n  expect(vm.$options.name).to.equal('hello')\r\n}) Related commit: https://github.com/trollepierre/my_project_vue/commit/6c80506 3) Create a component a) Add a test on the name We now want to add a component: a special button which the user has to click: ClickMeButton. We first create the matching test class: ClickMeButton.spec.js (similarly to Hello.spec.js) and we test the name: ClickMeButton.spec.js import Vue from 'vue'\r\nimport ClickMeButton from '@/components/ClickMeButton'\r\n\r\ndescribe('ClickMeButton.vue', () => {\r\n  let vm\r\n\r\n  beforeEach(function () {\r\n    const Constructor = Vue.extend(ClickMeButton)\r\n    vm = new Constructor().$mount()\r\n  })\r\n\r\n  it('should check the name of my vue', () => {\r\n    expect(vm.$options.name).to.equal('clickMeButton')\r\n  })\r\n}) We can now create the component: ClickMeButton.vue <template>\r\n  <div class=\"clickMeButton\"></div>\r\n</template>\r\n\r\n<script>\r\nexport default {\r\n  name: 'clickMeButton'\r\n}\r\n</script> Related commit: https://github.com/trollepierre/my_project_vue/commit/c774799 b) Add a test on the template elements We want to add a button in the template: ClickMeButton.spec.js it('should render button with text Click Me Button', () => {\r\n  expect(vm.$el.querySelector('.clickMeButton button').textContent)\r\n    .to.equal('Click Me Button')\r\n}) The code to write in the ClickMeButton.vue is very simple: ClickMeButton.vue <template>\r\n  <div class=\"clickMeButton\">\r\n    <button>Click Me Button</button>\r\n  </div>\r\n</template>\r\n\r\n<script>\r\nexport default {\r\n  name: 'clickMeButton'\r\n}\r\n</script> Related commit: https://github.com/trollepierre/my_project_vue/commit/94c6a11 c) Add a test on the components props Each Vue.js component has its own scope. This means you can’t access the data of a parent component from its subcomponents. That’s why the parent components call their subcomponents passing them ‘props’ (short for ‘properties’). We want to choose the text displayed by the button for each use of the ClickMeButton component. To do that, we use a props: In the test, we update the beforeEach so that a propsData is passed on for each component creation. ClickMeButton.spec.js beforeEach(function () {\r\n  const config = {\r\n    propsData: {\r\n      message: 'Click Me Button'\r\n    }\r\n  }\r\n  const Constructor = Vue.extend(ClickMeButton)\r\n  vm = new Constructor(config).$mount()\r\n}) Thus, we can update the ClickMeButton.vue file and remove the hardcoded string ‘Click Me Button’ ClickMeButton.vue <template>\r\n  <div class=\"clickMeButton\">\r\n    <button>{{ message }}</button>\r\n  </div>\r\n</template>\r\n\r\n<script>\r\nexport default {\r\n  name: 'clickMeButton',\r\n  props: ['message']\r\n}\r\n</script> Related commit: https://github.com/trollepierre/my_project_vue/commit/c1731f7 4) Test for the presence of a subcomponent Firstly, we import the ClickMeButton into the spec class: Hello.spec.js import ClickMeButton from '@/components/ClickMeButton' Then we assert that it is present after the class instantiation: Hello.spec.js it('should include a clickMeButton', () => {\r\n  const clickMeButton = vm.$options.components.ClickMeButton\r\n  expect(clickMeButton).to.contain(ClickMeButton)\r\n}) We can update the code by importing the component into the script section, and by adding the component name to the components to export to the template. Finally, we add the component name using kebab-case into the template (Vue will change the names of the components from PascalCase to kebab-case). Hello.vue <template>\r\n  <div class=\"hello\">\r\n      <click-me-button></click-me-button>\r\n  </div>\r\n</template>\r\n\r\n<script>\r\nimport ClickMeButton from '../components/ClickMeButton'\r\nexport default {\r\n  name: 'hello',\r\n  components: {\r\n    ClickMeButton\r\n  }\r\n}\r\n</script> Related commit: https://github.com/trollepierre/my_project_vue/commit/2789185 5) Test the props passed to a component We want to change the message returned by the ClickMeButton. To do that, Hello.vue must define the message property anytime the ClickMeButton is used. This is done using props. Here is how to assert that the ClickMeButton was called with the proper prop message: Hello.spec.js it('should define a message to put inside the clickMeButton', () => { \r\n  expect(vm.$options.components.ClickMeButton.props).to.haveOwnProperty('message')\r\n}) To have this test go green, the propsData message must be added to the click-me-button tag in the template. Hello.vue <click-me-button message=\"Increment counter\"></click-me-button> And to assert the content of the button text, passed on through the message variable: Hello.spec.js it('should verify textContent of the Click Me Button', () => {\r\n  expect(vm.$el.querySelector('.clickMeButton button').textContent)\r\n  .to. equal ('Increment counter')\r\n }) Related commit: https://github.com/trollepierre/my_project_vue/commit/1c20a6f 6) Test a method It’s very easy to access the methods and test them. You can directly call the method on the vm object we defined in the tests. Hello.spec.js describe('incrementCounter', function () {\r\n  it('should increment the counter to 1', () => { // When vm.incrementCounter() // Then expect(vm.$data.counter).to. equal (1)\r\n  })\r\n}) We can now create the method: Hello.vue methods: {\r\n  incrementCounter: function () {\r\n    this.counter += 1\r\n  }\r\n} Related commit: https://github.com/trollepierre/my_project_vue/commit/4f2136d Testing methods is fairly easy, and that’s good because that’s what we’ll want to do most! We also want to add a new onButtonClick method to the ClickMeButton component, so that a ‘buttonHasBeenClicked’ event is emitted. The test is simple: we call the new method from the vue once it’s mounted: vm.onButtonClick() The test is just a spy with Sinon.js ClickMeButton.spec.js describe('onButtonClick', function () {\r\n  it('should emit click ', () => { // Given sinon.spy(vm, '$emit') // When vm.onButtonClick() // Then expect ( vm . $emit ). to . have . been . calledWith ( ' buttonHasBeenClicked ' )\r\n  })\r\n}) We can now implement the method: ClickMeButton.vue <script>\r\n  export default {\r\n    name: 'clickMeButton',\r\n    props: ['message'],\r\n    methods: {\r\n      onButtonClick: function () {\r\n        this.$emit('buttonHasBeenClicked')\r\n      }\r\n    }\r\n }\r\n </script> Related commit: https://github.com/trollepierre/my_project_vue/commit/31ed02d 7) Bind the method to the template The method is tested but it hasn’t been added to the website yet. To do that, we will add it to the template and link it to a button. We want to check that the method was called. ClickMeButton.spec.js it('should emit an event when button is clicked', () => { // given sinon.spy(vm, '$emit')\r\n  const button = vm.$el.querySelector('button') // when button. click () // then expect ( vm . $emit ). to . have . been . calledWith ( ' buttonHasBeenClicked ' )\r\n}) We can now add the related piece of code: ClickMeButton.vue <template>\r\n  <div class=\"clickMeButton\">\r\n  <button v-on:click=\"onButtonClick\">{{ message }}</button>\r\n  </div>\r\n</template> Related commit: https://github.com/trollepierre/my_project_vue/commit/a12fa30 8) Test the event transmission from a subcomponent to its parent The point is now to increment a counter each time the button is clicked. One way to do it is to trigger the ‘incrementCounter’ method anytime the ClickMeButton component emits the ‘buttonHasBeenClicked’ event. As this article was written I hadn’t found out how to stub an event emitted by a subcomponent. The workaround is to use an integration test, simulating the click on the previously created button, and checking that the counter variable was properly incremented. Hello.spec.js it('should increment counter when button from ClickMeButton is clicked', () => { // given let button = vm.$el.querySelector('.clickMeButton button') // when button. click () // then expect(vm.$data.counter).to. equal (1)\r\n}) And the related code, asserted by the test: Hello.vue <click-me-button message=\"Increment counter\"\r\n\r\nv-on:buttonHasBeenClicked=\"incrementCounter\"></click-me-button> Related commit: https://github.com/trollepierre/my_project_vue/commit/a3c89ac We can now display the result with the command: npm run dev The result: For each click on the ClickMeButton, the counter displayed on screen is incremented 9) Call an external API: the use of http Vue-resource To call an external API, we will need vue-resource: npm install vue-resource – – save To use VueResource in each Vue object of the production code, we insert it into the src/main.js file, as well as in the Hello.spec.js file. src/main.js et Hello.spec.js import VueResource from 'vue-resource'\r\nVue. use (VueResource) To write the next tests, we will need stubs and promises. That’s why we’ll use sinon-stub-promise and its karma integration karma-sinon-stub-promise npm install sinon-stub-promise – – save-dev npm install karma-sinon-stub-promise – – save-dev Add sinon-stub-promise to the karma configuration file: test/unit/karma.conf.js frameworks: ['mocha', 'sinon-stub-promise','sinon-chai', 'phantomjs-shim'] The rollthedice API returns for every call the result of a die roll. We will use this URL: http://setgetgo.com/rollthedice/get.php It is time to write the next test. First, we need to stub the call to a URL (Vue.http.get) thanks to a Sinon.js promise. It is necessary to define this stub before the object is constructed. Then we can construct the “Vue mounted”and define the call to the method that we’ll name incrementFromTheDice. The assertion checks the URL called by the API. After the test, don’t forget to restore the state with the Vue.http.get.restore method. Otherwise some conflict might appear with the other promise tests, and that would be very bad! Hello.spec.js describe('incrementFromTheDice()', () => {\r\n  it('should call api to get the dice number', () => { // given sinon.stub(Vue.http, 'get').returnsPromise() // construct vue const Constructor = Vue. extend (Hello)\r\n    const vm = new Constructor().$mount() // when vm.incrementFromTheDice() // then expect(Vue.http.get).to.have.been.calledWith('http://setgetgo.com/rollthedice/get.php') // after Vue.http.get.restore()\r\n  })\r\n}) The related production code: Hello.vue incrementFromTheDice: function () {\r\n  this.$http.get('http://setgetgo.com/rollthedice/get.php')\r\n} Related commit: https://github.com/trollepierre/my_project_vue/commit/08b4bcd We can add the die roll result to the variable displayed by the counter by returning a result via the promise: Hello.spec.js it('should call increment counter from API answer', () => { // given const promiseCall = sinon.stub(Vue.http, 'get').returnsPromise()\r\n  promiseCall.resolves({ body: '5' }) // construct vue const Constructor = Vue. extend (Hello)\r\n  const vm = new Constructor({ data: { counter: 6 } }).$mount() // when vm.incrementFromTheDice() // then expect(vm.$data.counter).to. equal (11) // after Vue.http.get.restore()\r\n}) The parseInt() function is necessary to convert to an integer the string returned by the API. Then we add the retrieved value to the counter: Hello.vue incrementFromTheDice: function () {\r\n  this.$http.get('http://setgetgo.com/rollthedice/get.php')\r\n    .then((response) => {\r\n      this.counter += parseInt (response.body, 10)\r\n    })\r\n} Related commit: https://github.com/trollepierre/my_project_vue/commit/dac8dd2 We also need to test the case when the API returns and error. In that case, we want to reset the counter to 0. Hello.spec.js it('should reinit counter when api rejects error', () => { // given const promiseCall = sinon.stub(Vue.http, 'get').returnsPromise()\r\n  promiseCall.rejects() // construct vue const Constructor = Vue. extend (Hello)\r\n  const vm = new Constructor({ data: { counter: 6 } }).$mount() // when vm.incrementFromTheDice() // then expect(vm.$data.counter).to. equal (0) // after Vue.http.get.restore()\r\n}) The production code becomes: Hello.vue incrementFromTheDice: function () {\r\n  this.$http.get('http://setgetgo.com/rollthedice/get.php')\r\n    .then((response) => {\r\n      this.counter += parseInt (response.body)\r\n    }, () => { console .log('La Base semble être KO !')\r\n      this.counter = 0\r\n    })\r\n} Related commit: https://github.com/trollepierre/my_project_vue/commit/fd38826 Finally, to visualise the results, all is left to do is adding the button to the page: Hello.spec.js it('should incrementFromTheDice when button roll-the-dice is clicked', () => { // given let button = vm.$el.querySelector('button.roll-the-dice')\r\n  const promiseCall = sinon.stub(Vue.http, 'get').returnsPromise()\r\n  promiseCall.resolves({ body: '5' }) // when button. click () // then expect(vm.$data.counter).to. equal (5) // after Vue.http.get.restore()\r\n}) And we add the button to cast the die: Hello.vue <button class=\"roll-the-dice\" v-on:click=\"incrementFromTheDice\">ROLL THE DIE</button> Related commit: https://github.com/trollepierre/my_project_vue/commit/b5912ad Conclusion And here’s a page rather simple to realise! It contains 2 buttons to increment or add a die roll to a counter, which is obviously very trivial. But the point of this article is to write a code that thoroughly follows the TDD principles, and that’s properly covered by a fine grain test harness. In this article, we took time to detail the main types of unit tests that the Vue.js framework lets us do. We wanted to emphasise that TDD is a valid option with Vue.js, and the that these tests are within anybody’s reach. In a following article, we may detail how to test the created() method on the Vue creation and the directives. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Methodology and tagged development , driven , TDD , test , vue , vue.js , vuejs . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-06-14"},
{"website": "Octo", "title": "\n                From Mobile developer to WEB Front developer (2nd part)            ", "author": ["Guillaume Lagorce"], "link": "https://blog.octo.com/en/from-mobile-developer-to-web-front-developer-2nd-part/", "abstract": "From Mobile developer to WEB Front developer (2nd part) Publication date 29/05/2017 by Guillaume Lagorce Tweet Share 0 +1 LinkedIn 0 As I explained in the first part available here , I recently switched from Mobile development to WEB front end development. Last month has been full of learnings, let me describe them. Angular: quickstart for the win As my first web mission will be an Angular application, I focused my attention on this framework. The official quickstart from Angular website ( https://angular.io/docs/ts/latest/quickstart.html ) is a very good starting point. I really enjoyed building step by step a concrete application. This application is a simple dashboard of top rated heroes. Selecting a hero, you can access detailed information, and even edit the hero’s name. The tutorial is a 7-steps guide. At the beginning of each step, a link to the app in its final state is available. This is very motivating since you already have a clear view of what you’re building before getting through the technical points of each step. Tour of heroes: the application built during Angular quickstart ( live example ) The tutorial shows the various concepts of Angular: modules and components, routing, data binding, HTTP services. Each step offers the right balance between theoretical explanations of Angular concepts and a practical walkthrough to apply these concepts inside the Tour of Heroes. The quickstart is full of links to the official documentation, thus it gives the ability to go deeper into the topics of your choice. Following these links, I could see that the official documentation is clear and gives a good insight of the key concepts. At the end of the tutorial, I felt comfortable enough with Angular to start an app from scratch. Despite all these good points, I still missed the lack of explanation of the Angular CLI tool. Unit testing was also not present at all and as a Software Craftsman, it is always a huge frustration not be able to write tests before writing the implementation (more information here ). SFEIR School Angular 200 free training During my search of Angular learning resources, I had the surprise to see on my twitter feed that SFEIR was giving a two-days Angular training for free, right after the end of my last mobile mission and before starting my first Angular mission. Perfect timing! After asking for advices from colleagues on the *pertinence* of this training, I subscribed it. The training took place at SFEIR’s place. Around 25 people were attending. The two trainers were Fabien Zibi, Angular Tech Lead, and Cyril Balit, CTO Front and Google Developer Expert . Angular 2 training, by SFEIR As for the official quickstart, a simple app was built during the two days. It was a face gallery and you can find the source code on this github repository: https://github.com/Sfeir/angular2-200 . A git branch is available for each exercise and exercise solution. The training offered a very good balance between slides explaining Angular fundamentals and exercises allowing the audience to apply and master those fundamentals. What I really enjoyed in this training is the usage and deep explanation of the Angular CLI. This convenient tool is very powerful and massively increases the development flow. It can be used to create a whole new project, generate a component or a service, and many other useful tasks. The trainers, especially Cyril – Google Developer Expert – always answered questions with practical solutions. As for the official quickstart, I miss the Unit Tests and especially TDD practice. Only a few slides were focused on Unit Tests, they were at the very last chapter of the training and no exercises were dedicated to them. I would have prefered to discover testing throughout the 2 days. To conclude on this training, I definitely advise anyone to attend to. It is a very good starting point to discover and bootstrap on new topics. Thanks to this training, I felt very confident on my abilities when I started my very first WEB Front End mission. How-to comparison between mobile and WEB During this first times in the WEB development ecosystem, I had many good surprises. Some complicated and painful work in the mobile world are easy in the WEB one. Let’s have a look on some redundant problems developers are usually facing and relative pieces of code. HTTP call and JSON response parsing On iOS framework, calling a REST service and converting the JSON response file in swift/Objective-C classes is a common subject of debate. A quick search on iOS dependencies manager Cocoapods gives 445 results! Cocoapods search with JSON tag gives 445 results The common way is to use Alamofire and a JSON parsing library to achieve this, meaning you always use 3rd party libraries even for basic network operations. On Angular framework, things are slightly different. You can see from this piece of code, extracted from official tutorial, how easier it is. getHeroes (): Observable < Hero []> { return this . http . get ( ‘api/heroes’ ) . map ( this . extractData) . catch ( this . handleError ); } Binding data between view and component/controller Separating responsibility across the different files constituting an application is a common matter. There are several pattern designs used on iOS framework. They may not agree on all concepts. But they all agree on one point: separating view from data. This is where databinding comes to help the developer. On the iOS side, the view is often an Interface Builder file showing on a visual editor the screen you are designing. Then you have to manually drag and drop to link UI elements to your source code, and develop all the logic needed to display/refresh/store data. It can be a huge work on most complex screens, and it is always quite painful even on easier screens. As stated in the documentation: ”Angular has a rich data-binding framework with a variety of data-binding operations and supporting declaration syntax.” And that is just true: data-binding offered by Angular is simple and very powerful. Using only the same name in your component and template, embraced with correct brackets and/or parenthesis (depending on the binding flow), you can have a fully featured UI displaying up-to-date data, storing them, responding to user actions, and so on. <input [( ngModel )] = “currentHero.name”> The input field is displaying currentHero.name and typing in the field updates the variable. Forms and field validation On iOS framework, building a form is a very painful task – maybe the most painful one. If you add fields validation, with complex rules, e.g. showing validation during user input flow, this can turn into nightmare. With Angular, I discover that it can be built in a few minutes. Adding custom validation is smooth and the way data are easily binded between view and component gives the ability to update models and UI very easily. Angular offers two differents built-in ways to define a form , with validation on each field and even with the ability to group some fields for validation purpose (e.g. this can be useful if you have two phone number fields but only one is required). Doing this would take days on iOS, and few hours in WEB In the iOS world, as you have to build all on your own, the task is very time-consuming. You may want to use third-party frameworks, but it is not always that simple. When you want a custom UI, which is usually the case, you will have to make a hard choice: adapting the whole library, or building your own… Conclusion This second month of learning was mainly focused on Angular stack. I feel more and more comfortable with this new environment and am very happy to see that WEB development maturity is far away from the iOS one. I still miss two main things: Test Driven Development (TDD). As I did not find much resources on it and thus did not experiment it. As my new mission is led by a software craftsman, I feel confident that I will soon discover and practice TDD combined to Angular development. Clean Architecture: Uncle Bob never appeared once in all my search for the moment. I do not know if software design is less important on WEB development or if I just did not discover it for the moment. I’m hoping to cover this topic soon. I already have some ideas of application to build as my first concrete exercise to get deeper into my learning. Next month, I will tell you all about my progresses. Stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Consulting Chronicles , News and tagged angular , front , iOS , iOS , Mobile , mobile , Web , Web . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-05-29"},
{"website": "Octo", "title": "\n                Build your first Instant App!            ", "author": ["Nicolas Telera"], "link": "https://blog.octo.com/en/build-your-first-instant-app/", "abstract": "Build your first Instant App! Publication date 25/05/2017 by Nicolas Telera Tweet Share 0 +1 LinkedIn 0 You are an adept of Android App Links and you find wonderful being able to suggest your users to install your application or to redirect them from your website to your application with specific data relative to their requests? Well, forget (almost) everything you know. Since Google I/O last week, Google released to developers what comes as the biggest news in the Android development world: Instant Apps . Instant Apps are applications that can be executed instantly, from anywhere, without having to be installed on your smartphone. You can thus display a map, place an order or show news through your application, without installing it, from a simple tap on a link coming from a website or even from a simple text message. This article will be an overview of Instant App in order to let you start using it in your own development. We will not go into details as the Android developer website already does it very well. And the good news is that you will be able to use it in your existing application without having to rebuild it from scratch! What you’ll need In order to start developing with Instant App, you will have to install the early version of Android Studio 3.0 (Canary 1) and the Instant Apps SDK (available through the SDK Manager). You will also have to set your project to minimum API level 23 (Android 6.0), support for API level 21 (Android 5.0) will come soon. Also, Android App Links are used in Instant Apps but you don’t necessarily need to be familiar with them to start building your first app. How does it work? Understanding how Instant Apps work is pretty easy and can be summed up in 3 big parts: 1 – Imagine your application not as a whole, philosophically separated in features but as independent features grouped under a common base (yes, there is a difference). 2 – Now, clean that up and only keep the common bare minimum to all the features of your application in that same common base, and split the rest between these features. 3 – Finally, associate each feature to a unique URL and let Google Play take care of the rest. Google Play will analyze the request, check if there is an Instant App matching the URL pattern and will send the common base code ( base feature APK ) and the requested feature code ( feature APK ) to the user. If the URL does not match, the Android system will be alerted and an intent will be broadcasted. Thus, when a user wants to use feature n°3 of your application, he will only receive the necessary code to execute feature n°3, which will then be removed by the system after use. Let’s assume that the installable application has 10 features, we easily guess the performance gain, and so the user experience, when that same user will only receive the necessary code allowing him to use only one of these 10 features. Also, when the user will have received the code of a feature and its common base, if this user wants to use another of the features, he will just have to download the code of that other feature without having to download the common base code again. Performance and user experience gain, again. Instant Apps are secured and can only obtain the same capabilities as installable applications (camera, location…) by using the Permissions at Run Time API . For that same sake of security, some capabilities are not available on Instant Apps like access to external device storage. That’s it! You now have an overview of how Instant Apps work. You liked it so far? Let’s continue then. How to use it? In order to build you first Instant App, let’s create a new project on Android Studio 3.0. You can now include Instant App support which will let you configure the route of your main feature. Thus, when a user will tap the link https://instantbasicapp.octo.com/feature , Google Play will only send the code allowing him to execute the feature “feature” of your application. You then obtain 4 modules: The app module : This module uses all of the features of the application in order to build the installed app APK. Its build.gradle file applies the com.android.application plugin and depends on the base and feature modules necessary to build the installable APK (in this case: base and feature). app/build.gradle apply plugin: 'com.android.application'\r\n\r\nandroid {\r\n    //...\r\n}\r\n\r\ndependencies {\r\n    implementation project(':base')\r\n    implementation project(':feature')\r\n} The instantapp module : This module pretty much has the same responsibility as the app module, but for the Instant App. It is empty and is only necessary to build the Instant App APK with all the features of the application. Its build.gradle file applies the com.android.instantapp plugin and depends on all the modules used to build the feature APK (in this case: base and feature). instantapp/build.gradle apply plugin: 'com.android.instantapp'\r\n\r\ndependencies {\r\n    implementation project(':base')\r\n    implementation project(':feature')\r\n} The base module : Each feature module depends on the base module. It only contains the shared resources and builds a base feature APK for Instant Apps. However, all of the application code can be in this module if the application only has one feature. Its build.gradle file applies the com.android.feature plugin and references all of the features of the application. It also specifies that it is the base feature with the baseFeature property. base/build.gradle apply plugin: 'com.android.feature'\r\n\r\nandroid {\r\n    //...\r\n    baseFeature true\r\n    //...\r\n}\r\n\r\ndependencies {\r\n    feature project(':feature') \r\n    //...\r\n} The feature module : Each of the features of the application is defined within a specific module. This module holds all of its own resources, nothing more, and builds a feature APK for the Instant App and a AAR for the installable application. It must contain at least one entry-point as an activity which will be the main element displayed to the user. Its build.gradle file also applies the com.android.feature plugin and specifies its dependency to the common base module. feature/build.gradle apply plugin: 'com.android.feature'\r\n\r\nandroid {\r\n    //...\r\n}\r\n\r\ndependencies {\r\n    implementation project(':base')\r\n    //...\r\n} Here is a diagram taken from the Google documentation which represents the above architecture. We can see the instant app module and the installable app module which are dependent of each feature module, themselves dependent of the base feature module. Want more features? In the following, the architecture of our application has been updated: “feature” becomes “feature1” and a “feature2” has been added. You will then be able to add as many features as you want, each of them containing its own resources (colors, strings, drawables…) and, if needed, its own entry-point. Adding a new feature to your Instant App is very simple, as Android Studio 3.0 now provides two new types of modules: Instant App and Feature Module . Select Feature Module to add a new feature to your application. Note that each feature is built as a library and will then have to be declared with a different package name from the application one. Within each feature, you will now have to add specific entries to the AndroidManifest.xml . Thus, you will specify the matching URL with a host , a pathPattern and a scheme . The entry-point of your application will need an <intent-filter> with the android.intent.category.LAUNCHER and the android.intent.action.MAIN attributes to allow Google Play to discover your app. In that same entry-point manifest, you must also define the default-url of the app using a <meta-data> element providing a valid HTTPS URL. It is also necessary to add a priority level to the matching of the URLs of your features in order to enable Google Play to determine which feature to pick. For instance, if your application provides two features based on the same URL that you can eventually complete with an ID, a different level of priority will be attributed as follows: https://instantbasicapp.octo.com/feature will have a priority of 1 https://instantbasicapp.octo.com/feature/<ID> will have a higher priority of 100 feature1/src/main/AndroidManifest.xml <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<manifest\r\n    package=\"com.octo.instantbasicapp.feature1\"\r\n    xmlns:android=\"http://schemas.android.com/apk/res/android\">\r\n\r\n    <application>\r\n        <activity android:name=\".FeatureOneActivity\">\r\n            <intent-filter>\r\n                <action android:name=\"android.intent.action.MAIN\"/>\r\n                <category android:name=\"android.intent.category.LAUNCHER\"/>\r\n            </intent-filter>\r\n\r\n            <meta-data\r\n                android:name=\"default-url\"\r\n                android:value=\"https://instantbasicapp.octo.com/feature1\"/>\r\n\r\n            <intent-filter>\r\n                <action android:name=\"android.intent.action.VIEW\"/>\r\n                <category android:name=\"android.intent.category.DEFAULT\"/>\r\n                <category android:name=\"android.intent.category.BROWSABLE\"/>\r\n\r\n                <data android:host=\"instantbasicapp.octo.com\"/>\r\n                <data android:pathPattern=\"/feature1\"/>\r\n                <data android:scheme=\"https\"/>\r\n                <data android:scheme=\"http\"/>\r\n            </intent-filter>\r\n\r\n        </activity>\r\n    </application>\r\n\r\n</manifest> Once that your second feature is ready, you may want to navigate to it from the previous feature. How? Well, each feature can be independently accessed from a unique URL so… Yep, you just guessed right. When running as an Instant App, you can navigate through features using intents configured with the Intent.ACTION_VIEW action and a Uri . Now that your application is ready, select the instantapp module in the target selection dropdown and launch it on your device or on your emulator. You now have the feature running on your device, but if you check the launcher, you can see that the application has not been installed! Manage your URLs Android Studio 3.0 also offers an assistant allowing you to test your URLs matching. Select Tools –> App Links Assistant to display this assistant. It will allow you to manage your URLs by checking locally your matching, add logic to your URL-mapped activities enabling you to transmit data to them, associate your application to your website through Digital Asset Links , or execute manually typed URL directly to your device or emulator. Note that if the domain ownership of a URL is not verified, you will not be able to publish your Instant App. Voilà ! Your first Instant App is ready and you now have all the tools in hand to start using this in your development. Want more? This article is mainly based on the Android Instant Apps documentation from Google. In this documentation, you will find numerous information about good practices of Instant App development, managing of permissions, authentication and In-app Billing. There also is a really good FAQ section containing many interesting information like how to deploy your Instant App on Google Play. I also highly recommend that you go and check the “ Building an Android Instant App (Google I/0 ’17) ” which shows a live-coding of the refactoring of an existing Android application to an Instant App and ça vaut le détour ! Finally, you will find samples here which demonstrate single and multiple features Instant Apps. Enjoy! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Android , android , androiddev , instantapps , Mobile , mobile . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Build your first Instant App!” Chi 31/03/2018 à 08:43 The installable application has 10 features, we easily guess the performance gain, and so the user experience, when that same user will only receive the necessary code allowing him to use only one of these 10 features. https://sfy.co/tinder-for-pc/ leemidi 10/11/2018 à 22:01 Will it operate on rooted devices?  If so what access will it have to user info? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-05-25"},
{"website": "Octo", "title": "\n                From Mobile developer to WEB Front developer (1st part)            ", "author": ["Guillaume Lagorce"], "link": "https://blog.octo.com/en/from-mobile-developer-to-web-front-developer-1st-part/", "abstract": "From Mobile developer to WEB Front developer (1st part) Publication date 24/04/2017 by Guillaume Lagorce Tweet Share 0 +1 LinkedIn 0 OCTO is a great place to work and even greater place to grow. As an Octo consultant I’m used to learning and improving on a regular basis. Sharing knowledge is a central value of OCTO’s culture. I have been told to always try to find a better way and to push the boundaries of knowledge. As sharing is a core value, I could help by sharing this new career path I’ve choosen. After 5 years spent in the iOS development field, I had the strange feeling that I wasn’t learning anymore. Not that I had mastered everything there is to know about iOS development, but I felt the need for a “reboot”. Talking with HR and managers, and I decided to face new challenges by joining the WEB Front team (WEBF). I started my career a while ago in embedded software (for automotive and aeronautic industries). I never touched JavaScript and HTML/CSS always appeared to me as black magic. Although I will officially join the WEBF tribe in May, the 2nd, I already started to take some MOOC and attended some of WEB Front team’s days for a month. Therefore I can already explain a few things I found out about my new challenges. “I can’t get no motivation” As I said at the beginning of this article, it all started when I felt my daily motivation weakening. I wanted to embrace completely new challenges on both technical and functional sides. I always liked to pay attention to details and especially in the area of user interface, so I decided to move to the World Wide Web development. This choice offered the opportunity to both reach and impact users on many new fields of expertise, and give my career this fresh “reboot” it needed. Framework? Frameworks! So many available frameworks When I started reading some documentations on where to start learning web development, I quickly discovered the insanely high number of available frameworks. Web development is definitely not a single ecosystem affair. While iOS development is dictated by Apple (you have to use Apple Xcode IDE, with Apple Swift programming language on Apple OSX architecture), web development offers much more IDE, languages and frameworks. Thus, I decided to learn native JavaScript basis besides HTML/CSS principles. First, I chose the Coursera online course  “ HTML, CSS, and Javascript for Web Developers “. It gave me a very good overview and practical skills on HTML, CSS and on how to use JS to manipulate the DOM. M**C ME, I’M CURIOUS Following my WEBF tribe fellows’s advice, I completed JavaScript MOOC from CodeSchool. I started with JavaScript Roadtrip part 3 (part 1&2 were not necessary according my colleagues). Code School is a very pleasant and practical way to learn new programming skills: the online editor offers a great environment to try, fail and learn new concepts. I obviously advice anyone interested in learning JavaScript to buy a one-month subscription(or longer if needed)  to Code School. After this first module, I quickly got “ ES2015: The Shape of JavaScript to Come ” and “ JavaScript Best Practices ” figured out in just a week! I cannot get enough of all this learning! Cannot get enough of achievement badges! First impressions Working for the first time on a non-compiled language offers new possibilities: faster toolchain (no need to recompile to show any changes), and working on the final device (no simulator or external devices required). Being used to a strong typed language, I still miss the guarantee of types and numbers of functions arguments. Even if I haven’t learn how to build Unit Tests yet, as there is no compile time, TDD will certainly be much easier and quicker (and therefore even more powerful) than on iOS development flow. I find all swift nice aspects like map and closures but I need to embrace if conditions with parenthesis. This might not be such a big deal. As for now, I cannot wait for my next learning! Next article should be much more practical as I will soon start a real application development with an OCTO team. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “From Mobile developer to WEB Front developer (1st part)” Brindusa Duma 12/05/2017 à 10:28 All the best in the new endeavour Gull!\r\nLooking forward to more articles from you. Achraf Samidi 14/12/2019 à 22:45 Cool article, thanks for sharing. Just in case anybody having difficulty with web development, check out wordsphere.com which has some great insight and services. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-04-24"},
{"website": "Octo", "title": "\n                Asynchronism in mobile            ", "author": ["Nicolas Mouchel"], "link": "https://blog.octo.com/en/asynchronism-in-mobile/", "abstract": "Asynchronism in mobile Publication date 11/04/2017 by Nicolas Mouchel Tweet Share 0 +1 LinkedIn 0 In Android, asynchronous tasks are done to avoid long operations in the main thread. Android documentation gives a good advice to the community to avoid ANR (Android Not Responding): Therefore, any method that runs in the UI thread should do as little work as possible on that thread. In particular, activities should do as little as possible to set up in key life-cycle methods such as onCreate() and onResume() . Potentially long running operations such as network or database operations, or computationally expensive calculations such as resizing bitmaps should be done in a worker thread (or in the case of databases operations, via an asynchronous request). — Keeping Your App Responsive – How to Avoid ANRs The point is that long operations like network, file system or database can freeze the UI, and that these kinds of operations must be done in a worker thread rather than in the main thread. What is often misunderstood here is where to put asynchronism. The most current pattern is to protect the UI from long operations. Thus, long operations are wrapped with AsyncTask , Service , Thread , Executor or libraries which can be called asynchronously. MVP architecture with isolated long operations But there is another approach to this problem: instead of isolating long operations from the main thread, it is possible to isolate the main thread from all other operations (long or short). MVP architecture with isolated view The theory is pretty simple. With an architecture like MVP, View is isolated from the rest of the code base. There is also an abstraction between View and Presenter: interfaces. Some say that interfaces are a waste of time, but in this case, there will be two useful implementations for each layer: a decorator for the presenter that executes methods on worker thread a decorator for the view that executes methods on the main thread Handling threads with Executor and Decorator A example of this principle in kotlin code: interface View {\r\n    fun display(viewModel: ViewModel)\r\n}\r\n\r\ndata class ViewModel(val name: String)\r\n\r\ninterface Presenter {\r\n    fun doOperation()\r\n} Define interfaces for View and Presenter , and data class for ViewModel class DecoratorPresenter(val executor: Executor, val decorated: Presenter) : Presenter {\r\n    override fun doOperation() = executor.execute { decorated.doOperation() }\r\n}\r\n\r\nclass DecoratorView(val executor: Executor, val decorated: WeakReference) : View {\r\n    override fun display(viewModel: ViewModel) = executor.execute {\r\n        decorated.get()?.display(viewModel)\r\n    }\r\n} Implementations of Decorator s for View and Presenter val mainThreadExecutor = object : Executor {\r\n    val handler = Handler(Looper.getMainLooper())\r\n\r\n    override fun execute(action: Runnable) {\r\n        handler.post(action)\r\n    }\r\n} Implementation of an Executor with a Handler to move from the Worker thread to the Main thread val otherThreadExecutor = Executors.newSingleThreadExecutor() Use a simple Executor to leave the Main thread for a Worker thread class RealPresenter(val view: View) : Presenter {\r\n    override fun doOperation() {\r\n        view.display(ViewModel(\"name\"))\r\n    }\r\n}\r\n\r\nclass RealView: View{\r\n\r\n    lateinit var presenter: Presenter\r\n\r\n    fun onCreate() {\r\n        val view = DecoratorView(mainThreadExecutor,WeakReference(this))\r\n        val realPresenter: Presenter = RealPresenter(view)\r\n        val presenter = DecoratorPresenter(otherThreadExecutor, realPresenter)\r\n    }\r\n\r\n    override fun display(viewModel: ViewModel) {\r\n        // do stuff\r\n    }\r\n\r\n    fun onAction(){\r\n        presenter.doOperation()\r\n    }\r\n} Real implementations of Presenter and View Decorators are boilerplate codes that can be easily generated, and there is a library for that Executor Decorator To conclude, managing threads is a real responsibility that you need to handle on your own. By mastering it, you can easily build acceptance test suites with Robolectric and even Espresso (see custom IdlingResource ) Note: If you want to avoid parallelism in your application, share the single thread executor between Presenter s. And if for one case you need parallelism, just create another executor. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Methodology and tagged android , mobile . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-04-11"},
{"website": "Octo", "title": "\n                Android Themes & styles, a real architecture            ", "author": ["Pierre Degand"], "link": "https://blog.octo.com/en/android-themes-styles-a-real-architecture/", "abstract": "Android Themes & styles, a real architecture Publication date 23/05/2017 by Pierre Degand Tweet Share 0 +1 LinkedIn 0 In modern Android development, there is a huge rise of advanced architectures like MVP, MVVM or Clean Architecture, crazy libraries like RxJava or Dagger and even new languages like Kotlin. But on most projects, theme and styles are still written in an oldschool way with no consideration on how to architecture them. But these XML are part of your code base and you should show them the same love you show to your Java Code. A common problem On a new project, Android Studio generates for you a single style.xml file with an embryo of AppTheme. The more your project is growing, the bigger this AppTheme will be as this is a common place to put your theme attributes. But, at one point of your project, you may need to add some device or API level specific values to your theme. For example, your app has a minSdkLevel of 16 but you want to add the windowTransluscentStatus attribute to your theme. This attribute was introduced with the API 19 therefore you can’t put it directly in your AppTheme because you will have an error. In Android, you can put some resources under specific folders to target specific API level. For our example, you can create a res/values-v19/ folder and put a new style.xml file inside and this file will be used with a higher priority than the one in res/values/ when running on a device on API 19. Great, back to the theme. You have now 2 easy solutions for the new API 19 attributes. First you could Copy all your AppTheme from the res/values/styles.xml Paste it on the -v19 version Add the new attribute only on the -v19 file. This will work because the -v19 theme will be only used on API 19 and more and you new attribute will work. But please, don’t do this. This will be a hell to maintain both files because as soon as you’ll want to add a new value to the base theme, you will have to remember to copy it as well on the -v19 theme. Another solution is: Create a BaseAppTheme in the res/values/styles.xml file with all the non-specific attributes In the same file, change the AppTheme to make it inherit from the BaseAppTheme In the -v19 version, you just have to make your AppTheme also inherit from the BaseAppTheme Add the new specific attribute. With this, if you want to add a new attribute for all API level, you just have to put it inside the BaseAppTheme and you don’t have to worry about copying it to the other file. Great! values/styles.xml <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<resources>\r\n    <!-- Base application theme. -->\r\n    <style name=\"BaseAppTheme\" parent=\"Theme.AppCompat.Light.NoActionBar\">\r\n        <!-- Customize your theme here. -->\r\n        <item name=\"colorPrimary\">@color/colorPrimary</item>\r\n        <item name=\"colorPrimaryDark\">@color/colorPrimaryDark</item>\r\n        <item name=\"colorAccent\">@color/colorAccent</item>\r\n        <item name=\"android:windowBackground\">@color/windowBackground</item>\r\n    </style>\r\n    <style name=\"AppTheme\" parent=\"BaseAppTheme\"/>\r\n</resources> values-v19/styles.xml <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<resources>\r\n    <style name=\"AppTheme\" parent=\"BaseAppTheme\">\r\n        <item name=\"android:windowTranslucentStatus\">true</item>\r\n    </style>\r\n</resources> More API level ! Now, you learn about a new cool API 21 specific new attribute and you want to use it ! You create a new res/values-v21/ folder, you add a new styles.xml file, you write a new AppTheme which extends the BaseAppTheme and you put your new attribute ! values-v21/styles.xml <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<resources>\r\n    <style name=\"AppTheme\" parent=\"BaseAppTheme\">\r\n        <item name=\"android:windowSharedElementEnterTransition\">@android:animator/fade_in</item>\r\n    </style>\r\n</resources> You run the app on a Lollipop+ and … the status bar is no longer transluscent ! Of course ! The system is using the AppTheme from the v21 folder and it’s bypassing the ones from v19 and the base folder. The attributes from the v19 AppTheme is not in the final theme when running on v21. Introducing the theme inheritance chain To fix this problem, let’s rewrite all our themes. In the res/values/styles.xml , you create a Base.V0.AppTheme with all the generic attributes. This theme inherits from the standard AppCompat theme. In the same file, you also add the AppTheme and make it inherit from the Base.V0.AppTheme. values/styles.xml <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<resources>\r\n    <style name=\"AppTheme\" parent=\"Base.V0.AppTheme\"/>\r\n\r\n    <style name=\"Base.V0.AppTheme\" parent=\"Theme.AppCompat.Light.NoActionBar\">\r\n        <!-- Generic, non-specific attributes -->\r\n        <item name=\"colorPrimary\">@color/colorPrimary</item>\r\n        <item name=\"colorPrimaryDark\">@color/colorPrimaryDark</item>\r\n        <item name=\"colorAccent\">@color/colorAccent</item>\r\n        <item name=\"android:windowBackground\">@color/windowBackground</item>\r\n    </style>\r\n</resources> In the res/values-v19/styles.xml , you create a Base.V19.AppTheme and put the api 19 specific values. The parent of this theme is Base.V0.AppTheme. In this file, you add the AppTheme and make it inherit from the Base.V19.AppTheme values-v19/styles.xml <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<resources>\r\n    <style name=\"AppTheme\" parent=\"Base.V19.AppTheme\"/>\r\n    \r\n    <style name=\"Base.V19.AppTheme\" parent=\"Base.V0.AppTheme\">\r\n        <!-- API 19 specific attributes -->\r\n        <item name=\"android:windowTranslucentStatus\">true</item>\r\n    </style>\r\n</resources> In the res/values-v21/styles.xml , you finally create a Base.V21.AppTheme and put all the api 21 specific values. The parent of this is, you name it, Base.V19.AppTheme. And finally you add the AppTheme and make it inherit from the Base.V21.AppTheme. values-v21/styles.xml <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<resources>\r\n    <style name=\"AppTheme\" parent=\"Base.V21.AppTheme\"/>\r\n    \r\n    <style name=\"Base.V21.AppTheme\" parent=\"Base.V19.AppTheme\">\r\n        <!-- API 21 specific attributes -->\r\n        <item name=\"android:windowSharedElementEnterTransition\">@android:animator/fade_in</item>\r\n    </style>\r\n</resources> With this architecture of themes, for every API level, the AppTheme will have all the attributes from all API levels and it’s very easy to extend and add new api level specific attributes later. This can also be used for styles because this is not only useful for API levels specific attributes but also for overriding specific values on tablet for exemple. Here is a full example for a view that should have a different width on phone and tablet and that should have an elevation on API 21+: values/styles.xml for default and phone attributes. <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<resources>\r\n    <style name=\"Style.MainContent\" parent=\"Base.Style.MainContent\" />\r\n\r\n    <style name=\"Base.Style.MainContent\" parent=\"Base.SW.Style.MainContent\" />\r\n\r\n    <style name=\"Base.SW.Style.MainContent\" parent=\"Base.SW0.Style.MainContent\" />\r\n\r\n    <style name=\"Base.SW0.Style.MainContent\" parent=\"Base.ApiLevel.Style.MainContent\">\r\n        <item name=\"android:layout_width\">match_parent</item>\r\n        <item name=\"android:layout_margin\">8dp</item>\r\n    </style>\r\n\r\n    <style name=\"Base.ApiLevel.Style.MainContent\" parent=\"Base.V0.Style.MainContent\" />\r\n\r\n    <style name=\"Base.V0.Style.MainContent\" parent=\"\">\r\n        <item name=\"android:background\">#FFFFFF</item>\r\n    </style>\r\n</resources> values-v21/styles.xml for API21+ attributes. <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<resources>\r\n    <style name=\"Base.ApiLevel.Style.MainContent\" parent=\"Base.V21.Style.MainContent\" />\r\n\r\n    <style name=\"Base.V21.Style.MainContent\" parent=\"Base.V0.Style.MainContent\">\r\n        <item name=\"android:elevation\">4dp</item>\r\n    </style>\r\n</resources> values-sw600dp/styles.xml for tablet attributes. <?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<resources>\r\n    <style name=\"Base.SW.Style.MainContent\" parent=\"Base.SW600.Style.MainContent\" />\r\n\r\n    <style name=\"Base.SW600.Style.MainContent\" parent=\"Base.SW0.Style.MainContent\">\r\n        <item name=\"android:layout_width\">480dp</item>\r\n        <item name=\"android:layout_margin\">0dp</item>\r\n        <item name=\"android:layout_gravity\">center_horizontal</item>\r\n    </style>\r\n</resources> Every attribute is only wrote once, it is easy to maintain, to add new attributes to this style and to extend it for more API levels or using other qualifiers . I showed you an alternative way of writing and composing themes and styles and I hope you will find it useful in your Android projects. Styles and themes are a really good way of factorizing code and I highly suggest you use them more and if you are not used to, you can read more about it on this blog post . Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Android , android , code , Mobile , mobile , UI . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 6 commentaires sur “Android Themes & styles, a real architecture” Anonymous Scribe 29/05/2017 à 08:26 It was very interesting! I hated Android for raw styles/themes management. And hated it twice as hard when thought about versioning. But now I think I might become to like it. Your article was a great help for me. Thanks a lot Chris Jeon 29/05/2017 à 21:06 Good straightforward tips for organizing your style code for android. Thanks. Sven Bendel 11/06/2017 à 12:26 Great post! However, how do you apply the styles then? If they are named differently you would need to create different layout files for different configurations and copy paste the XML, just exchanging the name of the style. So you would essentially just move the copy & paste problem outlined in your first trivial solution to the layout level. Or am I understanding something wrong here? Jason 22/06/2017 à 19:06 To extend what Sven asks, how would you *combine* styles? In your last example, what happens if you have a v21 tablet?  How would you combine `Base.SW.Style.MainContent` and `Base.ApiLevel.Style.MainContent`? yahoo support 28/06/2017 à 12:31 this is a great post. it talked about the Android theme and style.  every user want to good theme and Style on his Android mobile.\r\nThanks \r\n you talked about the Android theme and style. Ashutosh Gupta 13/06/2020 à 11:48 What if situation like this ::\r\n    \r\n    \r\n\r\n    \r\n        true\r\n    \r\n\r\n    \r\n        true\r\n    \r\n\r\nAs we can see that attribute \"android:windowLightStatusBar\" duplicate Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-05-23"},
{"website": "Octo", "title": "\n                AppDevCon 2017            ", "author": ["Guillaume Lagorce", "Rémi Pradal", "Thomas Dalous"], "link": "https://blog.octo.com/en/appdevcon-2017/", "abstract": "AppDevCon 2017 Publication date 03/04/2017 by Guillaume Lagorce , Rémi Pradal , Thomas Dalous Tweet Share 0 +1 LinkedIn 0 Formerly known as the mdevcon, the Amsterdam conference related to mobile development – now called AppDevCon – took place on March, 17th. We had the chance to attend this edition in the Pathe Arena. Here is summary of what we have learned and our thoughts concerning some talks. MVP – Lean The power of small (opening keynote), by Cesare Rocchi The speaker praised the smallness. In three aspects. First, small market: consider attacking a niche, there is no big companies and less competition. You have a faster access to the decision makers (CEO etc.) Second, small team: there is less inertia, less meetings, no process (which is more pleasant), less distance between the employees and the customers. Third, small launch: make soft launch with a reduced amount of people and testers, for instance: Buffer started with only Twitter support announced on a landing page. Later, they added a pricing page. And so on. The smaller the product the easiest it’s discoverable by the consumer. Less features leads to simple and effective marketing. Finally, Cesare suggested to “add nenuphar” before your product to increase the conversion rate. For example, a free RSS feed validator to make the customer discover the paying solution of your RSS-related product. To sum up, the talk was mainly about lean startup and in our opinion the audience (mainly composed by developers) was already convinced about the topic. Those who need to be evangelised about a such approach are the decision makers who were not in the room. Craftsmanship practices – not so basic Many conferences were related to basic software craftsmanship practices. At least three of them were about core practices. Hidden mysteries behind big mobile codebases, by Fernando Cejas In this talk, Fernando puts the light on how to deal with big team maintaining a single codebase. SoundClound Android team has grown from 5 to 21 people in around 9 months. This led to find answers to some uprising questions related to such a team growth: can we add a new functionality fast? Is our codebase prepared to scale? Is it hard to maintain? What about technical debt? How to keep it healthy and safe? Is it easy to onboard new people? Thus, they define a new way of working like putting in place some release train with a dedicated release captain. When the train is leaving, all ready features are shipped with it. The train is always leaving on time. Unready features are postponed to next release train. The release captain is changing every new release train. Another role they put in place is the test sherif, in charge to avoid flaky tests (those automated tests that do not pass every time). This role is also changing every week. Fernand puts emphasis on their practice of constant refactoring and boy scout rule. To handle the technical debt, they put in place a technical debt radar, showing each piece of code to refactor in perspective to their complexity and pain. In our opinion, this talk did not show any new or magical trick to deal with large codebase but it gives a good reminder of the constant evolution of software, reminding the audience of software craftsmanship practices related to legacy code. Getting clean keeping lean, by Joe Birch Joe Birch works on the Android app for Buffer. When he arrived on the project the technical debt was important. There was no unit test, a lot of coupled concepts. He mentioned the fact that in the beginning the knowledge of the big picture is easy but as the code base grows the onboarding is more complex. On android when the logic is in activities/fragments it is really difficult to test, to maintain and there is a lot of code duplication. For the 1st refactoring, they chose to move to MVP (Model-View-Presenter). It led to a first split between presentation layer and business logic but there was still no tests and the presentation was coupled with data sources. During the 2nd refactoring they tried to implement a clean architecture inspired pattern. It offered a clear separation of concerns and it was framework agnostic. Then they could easily test all their business logic. Joe ended his talk by mentioning some cons: a clean archi could be overkill for some task, you need time to get used to, and the conversion of models between the different layers can be painful. In our opinion, clean archi is not a goal. It is a way of building very maintainable applications when the code base becomes big. You may not need it for a simple application. A good approach when you design your architecture is to make it emerge and not to anticipate it too soon. Test driving Swift to the max, by Phil Nash from JetBrains Test Driven Development was clearly the centre of attention in this talk, showing the audience how to use Swift playground and Phil’s open-source library to speed up the Red-Green-Refactor cycle. The swordfish library lets one developer to write tests in playground, adding a simple `required()` method in front of the expression he expects. It then gives a nice output in the emoji form showing if the required expression is fulfilled or not. In practice this gives the following lines of code. Even if Phil warns about his library maturity (`This is just a proof-of-concept – not recommended for serious work – and subject to change.`), this talk was interesting because of the reactions it aroused. The audience asked many questions, more on the pros & cons of TDD than on how it can be practically a tool for everyone in its own project. Our analysis is that the mobile developers’ community is quite far from software craftsmanship practices and that’s a pity as those are very powerful tools to ensure good quality software. This type of talks, that may sound a bit basic to craftsmen developers as we are in OCTO, is very useful to pursue software craftsmanship evangelism. Practical technical talks Mixins over Inheritance, by Olivier Halligon In this Swift based talk, Olivier Halligon used Sci-Fi universe to illustrate the limits of inheritance to describe behavior or property of objects. Then he came to a very nice feature of Swift 3: protocol extensions. Using it efficiently, he showed how this solves the issue of Sci-Fi characters representation. He has also highlighted that protocol extension is not a silver bullet and cannot solve all the problems. Finally, the speaker demonstrated how to use few protocol extensions to move from the stringy COCOA API when dequeuing cell in tableview and collection view to a very convenient and simple API. Check his new library https://github.com/AliSoftware/Reusable that contains all this abstraction code. Smartphone security, by Jan-Felix Schmakeit and Daniel Zucker Security might be underestimated by developers and product owners who prefer to focus their work on delivering features to end-users. But this is a very important part of application development as once a threat appears it is too late. After a good explanation of what is security in the software field by Jan-Felix Schmakeit, Daniel Zucker showed many Android sample codes related to security best practices. All the presented code was accompanied with clear and useful explanations on the kind of protections it gives to your application. Library development, by Zan Markan The motivation for developing librairies is mainly the ability to share code between apps easily. A thing Zan told us is that the developer experience in context of using a library is similar to a user experience. It means that the API should be cared for as we would care for a GUI for the end users. And as developers are lazy people we should keep that in mind when designing libraries API. Few advices about designing an API: entry point: should be extensible (constructors, builders, factories) methods & models: be careful with naming, respect platform conventions rx: make it optional, not default style dsl: could be a good idea if the library deals with specific concepts errors should be nice Make a sample with your lib, it facilitates the API design and when you release it you have a documentation out of the box for “free”. About tracking & analysis: use “dogfooding” to understand the usage or if there is a network call you could add a header to track it. Talk to your users, they are the best source of information. When it comes to shipping it, use semantic versioning. To release the library choose between a public or private repository depending on the context. Finally, the documentation: provide a quick start, if possible a sample app. Generate API doc with the language doc tools. Create a wiki pages for everything else. The tests suites could also be a good documentation if your library is open source. Sharing code between web and native apps, by Sebastian Witalec The baseline of this talk was “you know angular (2) now you know mobile” Sebastian works for the company who created native script. He mentioned some statistics about website and mobile apps audience: a user spends 18x more time on a mobile app while there is twice more people on a website. The retention is better for a mobile app thanks to a richer experience but there is a larger audience on the web thanks to smaller friction. Then the debate is not the web versus the mobile but how the users decide to consume the service you provide. Sebastian then introduced the angular support for web and mobile app development. Since angular (2) is platform agnostic you don’t have to care about the final user interface primarily. You just have to develop the core and then address the device where your audience will be. The speaker reminded us that mobile ui and web ui are two different things and should be designed with different approaches. Technically, on the mobile part the markup is “binded” to native UI component with a 100% access to iOS (Javascript Core) and Android (V8) APIs thanks to Meta Generating Process (MGP). MGP is done at the build time and provides zero day support for every native library since it generates the bridges between native code and native script. Sebastian ended his talk with a demo of a color picker which has two implementations (one for iOS and one for Android), some animations running on the GPU, a shared navigation router between web and mobile. Our opinion about this technology is that it could be interesting for a Proof Of Concept but the lack of testing for instance (no example given) could be a real problem. IoT related talks Integrating Android ecosystem into all the connected (or potentially connected) devices of a house is a now an important concern of the Android developer team. It started with Google Cast which let developers the possibility to transform the user devices into a remote control. Now it continues with the area of the internet of things and notably with the recent Android Things framework introduced during the last Google I/O. We attended three conferences related to connected devices. These where mostly introductions to the possibilities and limitations of internet of things. The conferences might be trivial for people used to work with IoT devices but these problematics are quite new for us, mobile developer, who are accustomed to work on a “consistent” smartphone ecosystem. Things to remember when you build your robot army, by Nir Dobovizki This talk was a general presentation of some DOs and DONTs to keep in mind when we design IoT experiences. The advices were illustrated of pictures extracted from the quite funny twitter account @internetofshit. Here are the four main points we retain from this conference: Design for battery failure . We should keep in mind that devices, even if they are connected to the grid power, can be disconnected at any time. For instance, how a connected lock should behave when there is no power? Keep the door opened or closed? Design for intermittent internet connection . You can never be sure that your device will have a full time internet connection, even if the device is wired. You must make sure that the device is prepared to such intermittent connectivity and that it will not damage your user experience. Handle software updates gracefully . Firmware and software update, even if they are particularly important, should happen at the appropriate moment or it may lead to user frustration. Security should be a major concern for you . IoT is particularly appropriate for use case where insuring privacy is crucial: in house camera, sensors or other objects collecting private data. Therefore, you have to make sure that you treat security questions very seriously. Make your app for TV by integrating with New Google Cast SDK by Sonia Sharma This talk was a presentation of the new way of integrating the Google Cast SDK thanks to its version 3, released in last June. The speaker started with a sum up of what is Google Cast (a *communication protocol*) and the role played by the different devices (the smartphone is the remote , the Chromecast/Google Home is the receiver which will play the media) The takeaways of this conference are the followings: Adding Google Cast integration to your app is quite easy if the media you want to play on the TV is “simple” i.e. a pure media stream with no DRM All the Google Cast UI assets (casting icon, notification & tutorial) are provided by the the Google Cast SDK There are three levels of customization of the receiver app (displayed on TV) No customization: the interface is the one used by default. Style customization: a css is provided to theme the receiver app Full customization: it is possible to give a full webpage to have a more advanced receiver app customization. Our opinion concerning this presentation is that it shown well how easy it is to integrate this SDK. Meanwhile the usecases remain too narrow: casting an app on a TV makes sense for a small spectrum of the application we are used to develop. It’s About Time (Closing keynote), by Daniel Steinberg This talk was by far the best although the more complicated to summarize. At first, Daniel Steinberg used simple square representation to model all our to-do tasks during a week. He also used family pictures of his own to give the audience what was at the beginning some smiles and laughs. But as the talk advanced, the story of Daniel’s daughter and wife started to get deeply emotional. Daniel’s message was clear and simple: live the moment so you can avoid later regrets . This was by far the wisest advice offered to the audience get during the whole conference. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-04-03"},
{"website": "Octo", "title": "\n                Serverless sweetness: enforcing strict EC2 instance tagging policies with Lambda            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/serverless-sweetness-enforcing-strict-ec2-instance-tagging-policies-with-lambda/", "abstract": "Serverless sweetness: enforcing strict EC2 instance tagging policies with Lambda Publication date 20/03/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey everybody, TL;DR I coded something in order to enforce strict tagging policies on AWS EC2 instances using Python and a bunch of AWS services (Lambda, Cloudtrail, SNS, and S3). If you keep reading, I’m going to talk to you about AWS Lambda and Serverless computing, or FaaS (Function as a service). You can check the source code and permission related template files here: https://github.com/sebiwi/broom If you want to use it, create a Cloudtrail trail, activate it on every region, create an S3 bucket to store the Cloudtrail logs, create a SNS topic for notifications, create a Lambda function using the Python code (adapted to your resources and use case), an IAM role using the policy that comes with it, and finally activate events from the S3 bucket to the Lambda function on object creation. I can teach you how to do all of these things if you don’t know how. Just keep reading. Why? I like to be thorough and organized with my cloud resources. I usually work with and manage several shared AWS accounts for different teams. Without a proper tagging workflow, the EC2 resource set can become a mess. You’ll probably find yourself asking your team who created an untagged instance, and if they are still using it. That feels kinda dull, since we’re in 2017 and all. What do I want in order to fix this? I want to analyze each instance that is created on my AWS account, and destroy it if it is not properly tagged. I also want to get a notification when this happens, with useful information about the instance creation request, such as the instance type, or the username of the user that created the resource. Keep your stuff clean! I decided to call this thing Broom, since I use it to keep my things clean. Get it? To be honest, I also wanted to take Lambda for a ride, since Serverless is pretty trendy nowadays. Let us discuss it. Serverless and Lambda Serverless is another paradigm in the Something-as-a-Service universe. It allows you to execute code (functions) on the Cloud, without worrying about the underlying servers, middleware or configuration. The basic premise is simple: your code is executed only when it is needed. You’re also billed to the execution time of your function, so you actually pay (almost) exactly what you use. Even though the paradigm is called Serverless, this doesn’t mean that your code is executed without any servers. It means that you don’t have to provision, configure or manage the servers that are used to run your code yourself, and that your provider is going to do it for you. A corollary to the paradigm is the fact that your code can scale automatically (both up and down) indefinitely according to the demand. The only constraint is that your code must be stateless. Since Serverless is right now the new fad of IT, there are many new frameworks adopting the model. For example, Fission is a framework for serverless functions on Kubernetes. If you don’t know what Kubernetes is, or how it works, you can check it out over here . And Lambda? Well, Lambda is AWS serverless computing service. It originally came out in 2014, and only supported NodeJS at the time. Nowadays it has Python, Java and C# support, and it can also run native Linux executables, if you can run them from within one of the supported languages/frameworks. This allows you to run Go and Haskell binaries on Lambda, for example, if you do some hacking. In terms of billing, Lambda is metered in increments of 100 milliseconds, which is almost negligible compared to the EC2 minimum usage fee of one hour. How does this work? How can AWS spin up EC2 instances, deploy your code in them, and answer requests that fast? How ? Get the hint? It’s all about containers, really. When a Lambda function is triggered, Lambda launches a container with your code and the necessary dependencies in it. This takes some time, but its amount is negligible compared to the normal server provisioning delay. The container is also reused (on a best-effort basis) for further invocations of the function, if no changes have been made to either the code or the configuration. Exciting, isn’t it? You wanna know how to set it up? Let’s go! Just so you know what we’re going to do next: we’re going to log every API call made on AWS, put these logs on an S3 bucket, launch a Lambda function that analyzes newly-created instances for certain tags whenever a new log file is created on the bucket, and publish notifications on an SNS topic whenever an instance is destroyed due to lack of tags. Note: we’re going to be creating a list of AWS resources. Remember to create them on the same region whenever it is possible. First up, see the trail We need to be able to trace the AWS API calls in the target AWS account so we can see when EC2 instances are actually created. You can do this with Cloudtrail. From the Cloudtrail FAQs: AWS CloudTrail is a web service that records API calls made on your account and delivers log files to your Amazon S3 bucket You can create a Cloudtrail trail either using the AWS cli tool: aws cloudtrail create-trail –name broom-cloudtrail –s3-bucket-name broomtrail-bucket –is-multi-region-trail Or the AWS web console: You can create a new S3 bucket in the same single step if you’re using the console. You’ll have to do it separately if you’re using the CLI. You can also configure an SNS topic in order to receive notifications each time a log object is stocked in the S3 bucket. This is hell , and I encourage you not to do it. Remember to create a multi-region trail, since we want to be able to audit instances in all regions. Notify me then! Next up, we create an SNS (Simple Notification Service) topic in order to receive notifications whenever instances are destroyed. Same as before, you can do it using either the CLI or the console: aws sns create-topic –name broom-topic Remember to note the ARN of your topic, since you will be using it for the next step: { “ResponseMetadata”: { “RequestId”: “1469e8d7-1642-564e-b85d-a19b4b341f83” }, “TopicArn”: “arn:aws:sns:region:weird_number:topic_name” } Console version Don’t forget to subscribe to this topic if you want to receive notifications. You probably do, so do it. Know your role The Lambda function needs to be able to access some things: Cloudwatch, in order to stock execution logs The Cloudtrail S3 bucket, in order to recover the log files The EC2 API, in order to list instance tags and destroy instances when necessary The previously created SNS topic, in order to send notifications. In order to do all of these things, we’ll create a policy using the template policy file that comes with the project. You can find the template policy file here. You can use it almost as it is, just remember to replace the SNS ARN with the one you got in the previous section: { “Effect”: “Allow”, “Action”: [ “sns:Publish” ], “Resource”: arn:aws:sns:region:weird_number:topic_name } Then, you need to create a role, and attach the previously created policy to the role. The Lambda function will then be executed using this role, and it will have access to all the necessary resources. Lambda for all Finally, let’s create the actual Lambda function. You can create a package for your function and then upload it directly to AWS using the CLI. This is useful when you are using dependencies that are not standard and that are not already present in the AWS Lambda environment. This is not the case with the Broom function, so you can just use the AWS console to create it. I used Python and boto3 for Broom. You can see the source code here . Basically, I declare the SNS topic ARN, and the codes that I’ll be using to tag our instances. In this case, it can be the codes assigned to each person that is currently working on the team. I always refer to myself as ‘LOL’, for example: # Values SNS_TOPIC_ARN = ‘arn:aws:sns:region:weird_number:topic_name’ CODES = [‘LOL’, ‘GGG’, ‘BRB’, ‘YLO’] Then, I declare some helper functions, the first to decompress the Cloudtrail S3 log files: def decompress(data): with gzip.GzipFile(fileobj=io.BytesIO(data)) as f: return f.read() And the second one to generate a report on the destroyed instance: def report(instance, user, region): report = “User ” + user + ” created an instance on region ” + region + ” without proper tagging. \\n” report += “Instance id: ” + instance[‘instanceId’] + “\\n” report += “Image Id: ” + instance[‘imageId’] + “\\n” report += “Instance type: ” + instance[‘instanceType’] + “\\n” report += “This instance has been destroyed.” return report Then, the lambda_handler function, which is the actual function that is going to be triggered by the S3 event. First, I recover the bucket name and the object key: def lambda_handler(event, context): bucket = event[‘Records’][0][‘s3’][‘bucket’][‘name’] key = urllib.unquote_plus(event[‘Records’][0][‘s3’][‘object’][‘key’].encode(‘utf8’)) This might look a little bit weird. It’s only because the event message structure is kinda complex. Then, I recover the actual log (s3_object), decompress it, and create a JSON object with it: s3_object = s3.get_object(Bucket=bucket, Key=key) s3_object_content = s3_object[‘Body’].read() s3_object_unzipped_content = decompress(s3_object_content) json_object = json.loads(s3_object_unzipped_content) A log object may contain many API calls. I’m only interested in the ones that create EC2 instances. That means the ones with the “RunInstances” event name. If I find one of these events, I’ll connect to that region and recover the created instances: for record in json_object[‘Records’]: if record[‘eventName’] == “RunInstances”: user = record[‘userIdentity’][‘userName’] region = record[‘awsRegion’] ec2 = boto3.resource(‘ec2’, region_name=region) for index, instance in enumerate(record[‘responseElements’][‘instancesSet’][‘items’]): instance_object = ec2.Instance(instance[‘instanceId’]) I’ll check if the tag is present in the instance, and if it exists in the list of valid codes that I defined previously. If it is not, I’ll destroy the instance, and publish a report to the SNS topic: tags = {} for tag in instance_object.tags: tags[tag[‘Key’]] = tag[‘Value’] if(‘Code’ not in tags or tags[‘Code’] not in CODES): instance_object.terminate() sns.publish(TopicArn=SNS_TOPIC_ARN, Message=report(instance, user, region)) That’s it! You can adapt this in order to check your instances in many different ways, like a different tag, or a combination of many. Now, go to the Lambda section on the AWS console, and click on “Create a Lambda function”. Many function blueprints will be proposed to you then. Just go with “Blank Function”, since you already have everything that’s needed. On the “Configure triggers” section, choose S3, and select the Bucket you created previously. On “Event type”, choose “Object Created”, since you want your function to be executed when a new log object is created. You can enable the trigger right away, or afterwards, it’s up to you. Trigger happy Then name your function, select the runtime, and include your code with it: Be sure to specify the role you created earlier in the “Lambda function handler and role” section. If you don’t, the function won’t execute properly. You can change the assigned resources and the maximal execution time in the advanced settings section. I usually set this up to 128 MB and 20 second timeout, but it might depend on the specific characteristics of your team. After that, create the function on the review page. That’s it, you’re all set! If you create an instance without proper tagging, you will receive a notification like this one: Whoops, sorry! Final thoughts You can see all the Lambda limits here . For this use case, none of them are actually blocking. There’s a 100 concurrent Lambda function execution limit which might be a problem if you have a single account and people working on many different regions (shrug). This is actually a soft limit, which is meant to avoid excessive charges without your approval. You can request a limit increase if you feel like the default limit is getting in your way. Mocking the necessary resources in order to test this was slightly cumbersome too. I had to create valid logs, make sure that they had the necessary events in them, and then use the “Test” option in the Lambda dashboard on the AWS console, while specifying the mocked log in the test JSON object. I’d like to have a way to make this simpler somehow. I could have used a serverless framework to address this issue during development: something like Serverless , or chalice . I’ll definitely try one the next time I do some serverless development. These really come in handy, as you can test your code locally, generating input events without setting up the whole infrastructure. They also help you manage and update multiple environments, and create the resources needed for your function, with Cloudformation . You can also automate the creation of these Lambda management functions using a separate infrastructure as code tool, like Terraform. Anyways, I hope you enjoyed that as much as I did! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Serverless sweetness: enforcing strict EC2 instance tagging policies with Lambda” Abhishek Singh 16/10/2018 à 15:08 I got this error while doing the process,can you help??\r\nSTART RequestId: 42b9a413-d13e-11e8-bc9a-91fc50649741 Version: $LATEST\r\n'Records': KeyError\r\nTraceback (most recent call last):\r\n  File \"/var/task/lambda_function.py\", line 35, in lambda_handler\r\n    bucket = event['Records'][0]['s3']['bucket']['name']\r\nKeyError: 'Records'\r\n\r\nEND RequestId: 42b9a413-d13e-11e8-bc9a-91fc50649741 SB 30/06/2020 à 08:50 What happens if there are no tags in the instances? I tried this without any tags and the instance does not get deleted. I  guess it needs atleast one tag to work Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-03-20"},
{"website": "Octo", "title": "\n                Polar Expeditions and Agility: The 1911 Race to the South Pole and Modern Tales            ", "author": ["Alexandre Masselot"], "link": "https://blog.octo.com/en/polar-expeditions-and-agility-the-1911-race-to-the-south-pole-and-modern-tales/", "abstract": "Polar Expeditions and Agility: The 1911 Race to the South Pole and Modern Tales Publication date 12/03/2017 by Alexandre Masselot Tweet Share 0 +1 LinkedIn 0 A polar expedition and an IT project have much in common. They both share a goal, a team, and constraints. They share risk management issues, as failure is always a possibility even if the stakes are different. They also share a special relation with tooling and the influence of leadership style. But they mainly share the importance of the philosophy under which each project is undertaken. We will see in this article how the approach taken to face a challenge can have tragic outcomes. And how the fantastic race to the South Pole in 1911, or modern polar explorers methodology, can relate to our daily IT experience. My personal life as a software developer, a data scientist, a team leader and now an agile consultant is every day influenced by my ten years experience as a polar skier, and even more by the giants on whose shoulders I stood. Vision, team, leadership, decision making, continuous improvement, tooling… The source of inspiration never ends. This post is an attempt to share some of it. Slides are also available on slideshare . Amundsen (left) and Scott (right) teams reaching the South Pole. But the story is not over. Credits Olav Bjaaland and Lawrence Oates. “Victory awaits him who has everything in order  — luck, people call it.  Defeat is certain for him who has neglected to take the necessary precautions in time;  this is called bad luck.” Roald Amundsen [1] “Our luck in weather is preposterous […] How great may be the element of luck Robert F. Scott [2] Addressing challenges: big corporations vs. startups In the early twentieth century, the arctic regions were the last ones yet to conquer. European big powers had more or less finished to colonize the world and their once shining stars were slowly fading out. Among the challenges of those days, being the first to reach the South Pole was the greatest: a two thousands kilometers round trip in the coldest environment possible. At the time, it consisted in sending a boat with a party that would winter on the shore, making leaps inland to build intermediary food depots before launching the final assault. For almost one year, no contact would be possible between the explorers and the outside world. It looks pretty exciting, doesn’t it? The most powerful empire of the time could not see the South Pole escapes, for commercial and prestige reasons. And they addressed this coveted challenge in the way which has historically brought success: sending the Royal Navy, with an expedition led by Captain Robert Falcon Scott. But the Arctic was not only haunting the Admiralty dreams and those of ambitious captains. In the recently born Norway (which acquired its independence from Sweden kingdom in 1905), seamen were also relentlessly attracted by the frozen lands. Among them is Roald Amundsen, born in a family of shipowners from Oslo. He had therefore grown up in environment inclined towards outdoors and have learned young to become an accomplished skier. In 1906, he proved his skills as an explorer by breaking through the Northwest Passage, sailing the Gjøa with a team of thirteen from Canada to Alaska. If his financial means had nothing in common with those of the Royal Navy, he, also, was dreaming of being among the first men to set foot on the South Pole. Both expeditions were backed by strong cultures. Shall it be the British Navy history and its conquests, the undisputed master of the oceans. Or shall it be his Viking ancestors, the inspiration of the greatest explorer of the time (of all times?) Fridtjof Nansen or the energy boiling at the birth of the young norwegian nation. In summer 1910, the Terra Nova set sails from Cardiff and Fram from Oslo. An historical race was launched, a tour de force of courage and endurance, but also, as we will see, the direct confrontation between two radically different styles. Big corporation vs. startup. The confrontation between the two styles also appears all over the IT world. Company culture, financial means, intrinsic motivation have direct influence how projects are conducted, shall they be driven by a top-down processes or in a lighter, more adaptative fashion. But one shall not forget that if the organization size certainly correlates with corporate  philosophy, it is not by far the critical driver. Humans are. Robert F. Scott (left) and Roald Amundsen (right). Vision “Scott was marshaling his forces  for a ponderous campaign, while Amundsen was sailing on a raid” Roland Huntford [3] Any project shall be carried by a vision. Often verbalized as a short sentence. The statement is framed to share the project purpose inwards, to the team, and outwards, to stakeholders. Towards external stakeholders, so that anyone can simply understand the long term goal, the why and the how. Towards the team, so members can use it as a compass for decisions and fuel for motivation. Scott and Amundsen of course expressed their goals differently. A lot have been written on both explorers personal motivations, but we shall focus on what was actually publicly shared. Both men wanted of course to reach the South Pole and claim it. But Scott’s mission purpose was more intricate, as it also engaged scientific data collection and another expedition, lead by Campbell towards King Edward VII land. Moreover, in many writings, Scott has described the polar journey as the ultimate way to prove the bravery, the strength and the superiority of the so called “british race” [3]. Playing by the Navy book of the time, Scott’s views, doubts and ideas were not to be shared with his men. It was seen as the officer in chief to make decisions and to the men to implement them as told. On the contrary, Amundsen played a totally different piece. His goal was clear and loud: “Going to the pole and back” and a map with the major steps of the expedition pinned on the chart room’s wall, “for everybody’s use.” Emphasizing “and back” in his statement made it clear for everyone that the return trip was a crucial leg of the plan and shall be accounted for since the beginning (this sounds trivial, but is in radical opposition with Scott’s view, who bluntly hoped for the best, as we will see later on). Moreover, Amundsen’s ambition was to reach the South Pole for the unique sake of it, and that makes him the father of modern sport expeditions. A vision has been stated, more or less clearly, it is now time to assemble a team to realize it. Any IT project shall have a clearly stated vision. If it may sound too obvious, experience shows it is rarely the case. Most of the times, management might have a vision, buried “somewhere in a Powerpoint slide deck” . Once found, it often happens to be a long intricate sentence, a pile of generic verbs and buzzwords. And from the development teams, the message is even more blurred. One experiment you can make is to ask everyone in a team to write down on a post it the overall purpose of their current project (why? what?). Wait five minutes, stick all the notes on the wall and appreciate if you effectively have a shared vision… Assembling a team “There were many men, shoved at random” Edward Wilson, a Scott’s companion “The wrong companion is infinitely more dangerous than the worst blizzard.” Roald Amundsen “A small team of chosen specialists” Roland Huntford, on Amundsen [3] Recruitment Once more, the two approaches greatly diverge. Scott’s views on how to assemble a team were not explicit . If we refer to his companions words, coherence was not the primary driver. The assembly was driven by upper management influence, previous expeditions, personal resents, shipmen volunteering, rank balancing and expertise. On the contrary, as an independent entrepreneur, Amundsen was freer of a commanding chain or external sources of influence. Neither was he constrained to a closed pool of men (such as the Navy or Army personnel). And of course, like Scott, he was influenced by his previous expeditions. As for most of recruitment campaigns, a four steps process was set up: a) advertise position openings; b) receive resumes; c) check references; d) conduct interviews. This looks familiar, doesn’t it? Even though Amundsen was seeking for the most skilled experts in every domain (skiing, navigation, dog driving etc.), he also wanted to build a team he could trust as a whole and not only an assembly of fearless heroic experts. And letters from interviewees are enlightening on his process. He devised an insolvable situation concerning dried fish, to see who would stand up and simply say: “I don’t know how to do that” , and was actively plotting questions to figure out who would be able to answer in the face of their boss: “I cannot do it.” If Amundsen’s method looks rather sound in retrospect, let’s not forget the context, when assembling a ship crew for the most daring adventure in the beginning of the 20 th century. For that, again, Amundsen can also be saluted as an innovator. The IT world can be a perfect mirror of the situations described above. Too often are teams assembled at random, driven by personal agendas and dismantled after a project. Too often a “hero” is looked upon as a savior to a failing project, but will most often be no more than a temporary bandage (and maybe a powerful team spirit undermining). In fact, a team is an interdependent system, performing best when members, as good as they can be, can expose their technical weaknesses and seek for help around them. One leader’s role is to foster such an environment. If a certain level of skills is a requirement for a job, we can easily pursue the Amundsen’s strategy during interview. And push a candidate into situations where “I don’t know” is the only answer left, or watching how long they will hesitate before jumping on Google to seek for an answer during a coding evaluation session. Team organization “Lt Ewans, Campbell and Wilson have formed  a sledging committee and I am nominally the  secretary […] with not much to do.” Cherry Gallard, a Scott’s companion “It was the most astonishing boat  I have ever seen.No order were given but everyone  seemed to know exactly what to do.” A pilot on Fram [3] As soon as a goal has been set and a team assembled, realization could start. Two strategies were opposed: top-down or self-organization. At this stage, it should not be a surprise which strategy was chosen by whom. A British Navy officer carried wisdom and knowledge on how to implement his decisions. There was a Book by which the rules applied and usages in place (such as gathering a “sledging committee” to finally make no decisions). Amundsen’s pattern was about delegation. Once the goal stated, the decision shared as clearly as possible without much implementation details, a task owner was responsible and accountable for its completion. There are many reports from his men on his approach, and one of the most striking one of them was written during the Gjøa epic journey across the Northwest passage: during three years, no direct orders were explicitly given, although everyone on board did know what to do to implement the Captain vision. Amundsen’s party was also driven by the rule that everyone must participate to the common tasks, and no one was above or below any of them. For example, everyone would clean the dog excrements lying on Fram’s deck, Captain included. Olav Bjaaland was hired as a champion skier, a carpenter and a ski maker. The boss would not show up in his shop, except if  requested. [3] Leadership “Myself, I dislike Scott intensely […] He is not straight, it is himself first, the rest nowhere.” Lawrence Oates, a Scott’s companion “Discipline was instinctive. […] He often used to say that on board all were captains and all crews […] But nevertheless nobody was in any doubt who was the chief on board.” Helmer Hanssen, an Amundsen’s companion It’s hard to overdo those quotes… When Scott was backed by an explicit authority and a formal set of rules, the Norwegian shaped a more implicit style. The English flavor was easier to implement, as it was referring to reproducible standards, meanwhile Amundsen’s authority had to percolate differently. And one could see this second style act as a strongest cement. Stronger on an emotional level, but also on a practical one: if rules are enforced, it is hard to blame anyone for seeking ways around them. On the contrary to British situation, few reports exist about tensions within the norwegian party. There were nevertheless some disagreements between Hjalmar Johansen and his leader, illustrating the fact  that no team is ever perfect. But one vigilant observer could note that Johansen was the only team member not selected by Amundsen himself, but taken on at the request of Nansen. Under Amundsen’s party (left), captain and crew are to share an equal level of space and privacy, while Scott’s is driven by the Navy’s rule, where comfort comes with rank. On the paper, many managers welcome delegation of responsibility and accountability. And everything is perfect in good weather. In the best situation, an equilibrium is reached by the team to deliver a software at regular pace and quality, with iterations and features flowing down. Developers can explain the customer value of features they create, demos are applauded by everyone. However, such an equilibrium can easily be disrupted by upper management layers: shortcutting the once beloved Product Owner to get features “done quicker”; adding extraneous constraints  (even absurd technical ones); pressing everyone to do “more points per week”, and digging the technical debt for short term victories. A different posture can be adopted: “the business deadline is approaching and we are are going to miss it. What features can we removed? Hei techies, what solutions do you propose so that we ship a valuable product in time?” But autocratic and micro-management reflexes are sometimes strongly sticking. But to be fair, extraneous perturbations are not the only risk for a self-organized team. To many people, it is discomforting not having someone dictating every steps, every task and every process. If decision delegation means accountability, it also requires a lot of self discipline. Self discipline instead of a Book of rules, the path is not easy Tools to move We have discussed until now about vision and team organisation. It is now time to focus on the daily bread of any expedition team: tools. Clothes, food, camp, navigation: everything is tied to equipment quality. And “how to move” scores high in the priorities of a polar traveler. If nowadays such discussions are still passionate, one can imagine the dilemma at the dawn of polar travels. The problem looks simple: X must move from A to B in the fastest and safest manner. The ground is of snow and ice, often uneven, and sometimes (in the Arctic) broken by open water channels. Simple, no? During the preparation of their expeditions, both Scott and Amundsen spent plenty of time thinking about the best equipment to move on snow. They both had access to the same resources, the same information, the same technology and prototyping opportunities. The first reflex in such a situation was naturally to turn to experts to seek for advice. At that time, the most successful polar explorer was without any contest Fridtjof Nansen. He had led a team across Greenland in 1888 [4] and the epic Fram Arctic drift between 1893 and 1896 [5], before embracing an amazing public career rewarded by a Nobel Peace Prize in 1922. Being Norwegian and a strong Amundsen supporter, he also had the honesty of providing both explorers with the same information whenever they were asking: how ski and dogs were the solution and a Siberian contact that would sell them the best animals – “dogs, dogs and more dogs” [6]. Less accessible but even more experienced were the Inuit people, to whom moving on snow is not even thinkable without dogs. So the answer to a simple question “how to move on snow?” looked simple: “use ski and dogs.” Almost. The cost of tradition “No Ski. No dog” Sir Clements Markham, 1888 Father of  modern British Antarctic Exploration [3] To paraphrase a french show by Les Inconnus [ 7 ], there are two categories of traditions: good traditions and poor traditions. The first one produces technical excellence, patiently shaped through time by trials and errors. The later one is stuck into beliefs and looks at the world with blinkers and certitudes. It can still be true in modern polar exploration, at a lower scale, but the phenomenon was exacerbated at the time. Even if all facts were advocating for the use of dogs, Scott looked down on the animals. To his defense, he was immersed in a culture emphasizing the supreme merit of the British (and more generally the westerners) over the rest of the world. He repeatedly wrote his defiance of dogs [3], how they could not be trusted nor managed and their vicious passion for fighting. Scott finally embarked dogs, but also manchurian ponies and even motor sledges. However, he was ultimately betting on man hauling to reach the Pole. The ponies were of poor quality, bought on a second grade market, and even if equipped with custom snow shoes and cover with coats, they quickly died. The motor sledge destiny was even shorter. One fell from the ship at offloading, while the other was abandoned after merely eighty kilometers. Once again, we see the lack of vision, of focus, with the multiplication of solutions and none taken seriously. On the contrary, Amundsen transport strategy was simpler. He focused exclusively on dogs and ski and put all his team energy towards mastering these two complementary means. Even though dogs were eaten on the way back when they proved needless (a common practice by the time), his writings are full of enthusiasm and empathy with his animals. Guess who is is who, and which team had to rely on man hauling for most of the journey. Focusing, apprehending and perfecting the tools Wisely selecting a tool is a crucial point. But it is only the beginning. The team has to discover, learn and then master it. Amundsen pushed his team in continuous training, emphasising technical improvement on skis, sledges, clothes, food packing during the depot laying parties and the long winter night. Once more, Scott downplayed preparation. Even if he followed Nansen’s advice, hiring the Norwegian skier Tryggve Gran, he gave to Gran little influence on the overall team, preferring to rely on football sessions for physical training. Even in modern expeditions, most of the preparation effort is not focused at first  on physical training but on selecting and perfecting the equipment. Let’s now step aside our 1910 race for a moment and embrace a couple of other aspects critical to the failure or the success of a polar expedition. Innovation We discussed in the previous section about embracing the best technique of the time and improving it in the sake of better fitting one’s particular needs. However, times come when innovation is necessary. Innovation have different flavors, and as disruptive the outcome can be, it rarely if ever falls out of blue sky [8]. It often results from the synchronisation of previous experiences, a receptive environment, an adequate timing and a catalyst innovator. Nansen sticked a mast and sails on his sledge for his 1888 Greenland crossing [4]. More than one century later, we experienced different kiting techniques on the same lands. When facing the challenge of crossing an “ocean of snow” like in Greenland (or in Antarctic), one can focus on the word “snow” and head for ski and glide. One could also note the word “ocean” . And what have men done for millennia to move across seas? This observation convinced Fridtjof Nansen to give a try at using sails for his amazing crossing of the Greenland icecap in 1888 (where dogs, for practical reasons, were not an option) [4]. Even though the practical realization was not easy, Nansen made a major step, which has been endlessly improved until nowadays. And still can be… If using sails to cross oceans of snow can be seen as a natural evolution, some innovations are more disruptive. And let’s now head North for a moment and meet the King of modern polar expeditions for an illustration: Børge Ousland The Norwegian Børge Ousland crossing open water channels. There are crucial obstacles on the arctic ice pack and we see Ousland hoping for the best on ski, fearlessly paddling across or finally swimming through. Credits Børge Ousland/National Geographic. Traveling on the arctic is radically different than on the southern lands. There is no more a static icecap, but a tormented, ever moving ice pack. Open water channels are recurrent obstacle to overcome, and no one knows that better than the Norwegian Børge Ousland, once quoted “arguably the most accomplished polar explorer alive!” by the National Geographic [9]. His accomplishments, both in the Arctic and Antarctic are unrivaled [10]. Of course, his physical and psychological strengths have been key to his numerous successes, but also has been his imagination to overcome obstacles. The “classic” way in front of a water channel, is to circle around, losing hours and eventually days. Sometimes, a thin layer of ice looks like a passage, but skiing through it feels like playing russian roulette. Therefore, heading straight through water can seem appealing. Ousland broke a first barrier using sledges as a canoe, leading to efficient but rather scary paddling parties. But he has disrupted the game even more, when he packed a dry-suit in his sledge, and wore it to swim across the channels. Security and speed! As for a polar skier, tools are the bread and butter of our IT departments. No matter which one, mastering the technique is a crucial aspect to the success of any software project. Even in the age of global information, too often do we find developers entrenched in obsolete or inappropriate technical choices, by fear, laziness, or simply by stubbornness. No need today to take long trip to learn how to build an igloo or to manage dogs. The Silicon Valley experts and word class references are a click away, even if the tool is only a part of the solution with its mastery being the key. Indeed, organizations must foster conditions to make their people grow and innovate. Methods exist and have proven their efficiency: pair programming, internal developers fora, trainings, conferences, blue sky projects… Automate wherever possible One aspect a long polar trip which shall not be left aside is the prevalence of recurring tasks. Setting up camp, preparing food, de-icing the ski bindings. Even the simplest gesture of pulling a bottle of tea out of the sledge can mean: opening the sledge with mitts, seeking for a bottle, having a couple of thing falling of and putting everything back in can easily take 2 or 3 minutes. And ten times a day, those are 25 minutes lost, for an operation that can be set up to last ten seconds. Losing time on repetitive tasks is not only theoretical waste. It directly results in less sleep, thus psychological and physical strain. It translates into pain, when standing up in a freezing and windy environment soon drills through the toughest character. It translates into security risk, when multiplying steps is error prone. And errors can translate into… Let’s focus on setting up camp for a party of two, a daily task occurring at the end of every strenuous day. The todo list is straightforward: pulling out the tent and setting it up; tying it with ropes and building a snow wall for the wind not to blow it out; getting it ready for sleep, with two layers of mattress, a sleeping bag and a vapor barrier inside; getting the ice and snow out the clothing; melting snow into drinking water; cooking a 2’500 kcal evening meal. This simple pipeline can easily take two hours and will have to be reversed in the morning. Inconvenient side effects can be: losing the tent, when the wind rushes straight  through thousands of kilometers of ice lands. Losing one finger, when standing motionless in the same wind. Losing a crucial piece of equipment in the snow, because of disorganization. Losing your self control, for all the previous reasons. And losing the fun part of it. Being a repetitive task, one should focus on streamlining the process. Let’s consider a few details: the tent is not rolled into its small pack, but folded on the top of the sledge, with the tension poles only broken in two; idem for sleeping bags, with the vapor barrier and a mattress inside; mattresses for the floor are sewed together; the snow is carefully chosen to melt faster; as soon as the tent is up, one will start cooking and the other will reinforce the camp, use sledges as walls. The full list is rather long, but the end point is that one hour and fifteen minutes can be saved every evening. And as a consequence, camping by -39°C becomes a breeze. But such a process does not come up straight away at the first try. It is based on experience of others and self experiments. And let’s thank Børge Ousland and Sjur Mørdre, the great Oslo skiers, who have opened their books and shared their countless tricks and tools. Process automation is at the technical core of agility. Shall it lay at low level, with unit tests running automatically on the developer laptop or when the database evolution path is stored in the code repository and seamlessly applied by every source code stakeholders. Automation is taken to the next level with a build factory (such as Jenkins, Bamboo or TFS), where code is built, and integration/system/end to end/performance/security tests suites run continuously, before the application is deployed to production. It can even be pushed further by DevOps, with tools such as Ansible, Puppet, Kubernetes, with the whole infrastructure being described in code ( akka “Infra as Code”), to be tested and deployed on all the application environments. Continuous improvement On any journey, one will inevitably make mistakes. The best team, with the best tools, the best preparation cannot avoid them (maybe precisely because such a team will seek for higher challenges). Small or big, those mistakes shall be recognized and addressed, as we have previously illustrated how they can take their toll. There is no universal recipe to improve oneself, but there are obvious ones not to do so: never acknowledge you are wrong, embrace suffering as an end in itself, don’t leave room for trying new solutions and tear down any initiative to do so. Continuous improvement is a mindset but it shall certainly be organized. Let’s consider some aspects of improving a kite system to pull a skier. Even in a constrained environment, many aspects are to be chosen: how to link the sail to the skier to be comfortable and better transmit the power to the sledge? What rudder system to be handled for hours (a good windy day can push a party to last 24 hours on ski in a row)? How to eject the system in case of emergency without losing it? How to launch or bring down the system in strong winds? How to pack? What line material to use to limit knots? How to repair? How to tie two skiers on one sail? Again, a pattern can be applied to focus and avoid the energy and time being dissipated pursuing multiple topics simultaneously. The problems are to be broken down during a brainstorming (even a single person can brainstorm!) and hypotheses for improvements written down. Each hypothesis must be verbalized to be challengeable against a measure, or a hard fact. Each of them can then be sorted according to the expected  benefit  versus the resources estimated for implementation (hence the benefit of having broken them into smaller bits). One of them is then elected and a time frame is allocated to implement it, check the estimation and eventually adopt it. Finally, a new hypothesis is taken out and pursued, in light of the experience recently gained. And the whole process can loop. However, the process described above can only be applied during the expedition preparation stage, which typically lasts a full year. It is not likely to be undertaken during the expedition itself. However, the journey is the richest mine to gain knowledge. The bright ideas and painfully acquired lessons can easily be forgotten before the next expeditions. Therefore, the first action after a trip (or maybe right after a strong meal and a shower) is simply to write down the lessons learned and eventually share them as an appendix to a web story, for the benefit of everyone A snapshot of a “lessons learned” report written down in the plane on the way back from  an expedition and shared on the net Although experience cruelly shows that continuous improvement is among the first dimension to be neglected in “agile” IT projects, it is a core value of agility. Every methodology carries it explicitly. Scrum prescribes the retrospective meeting, at the end of each sprint. Various formats [11] can be used, but the common outcome is to let the team acknowledge what went wrong or well and decide which initiatives shall be tried before the next gathering, together with measures against which the outcomes are to be evaluated. It is a common saying that any other meeting could be dropped by a very mature team, but the retrospectives. Other methodologies also include continuous improvement, and many tools exist to pursue it: Plan-Do-Check-Act, 5 whys, speed boat… Facing the unplanned Decision making We have covered many aspects of a polar expedition in this article. But until now, we have also acted as if the process were linear. Strenuous maybe, but without too many surprises down the road. Surprisingly enough, this cannot be further from reality… Weather, equipment, navigation, food, camping. Every aspect brings its load of unexpected events. The reward of having set up every components is to make most incident minors, but surely enough, some complex situations will happen. In a system where everyone is accountable for the project success, everyone certainly has an opinion on the problem solution and shall be given room to voice it. The goal is then to build a decision that will appear the best to group, in the shortest amount of time. For such, we propose the following method: phrase the question and the scope; welcome and listen to everyone’s opinion, trigger alternatives if none comes spontaneously; encourage everyone to defend every other’s opinion and seek for a solution take the best out of each of them; be sure that everyone has understood the reasons of the selected solution; stick to the decision and do not re-evaluate, except if the scope clearly evolves. It is important that the method presented above is not designed to converge towards a weak consensus. The median ego level among the group is not the correct metrics to optimize. The commons reflex of everyone sticking to his/her original idea must be relaxed, as well as the natural feeling “I’m stupid because my solution was not taken” . All this can only happen if the group has grown a mindset with enough trust. Referring back to a previous session, we can fully apprehend here the role of a talented leader, who will catalyze the team vitality rather than force feed them by authority or lack of self confidence. But as with everything, the ability to take group decision must be trained, and not tested at first in a major crisis. And when the skill is acquired, it can be used efficiently, even when team members are on the verge of exhaustion. In some situations, a consensus process is not possible. An experienced party fellow, recognized by his peers, could have to make the decision for the group. This can happen in emergency situations, or when everyone is too tired and fall back on a natural leader. Should the group remain a team, this should be rare enough and the decision explained, possibly afterwards. Of course, all those arguments do not apply in a journey where a professional guide was hired and all burdens willingly delegated. During a crossing attempt of the Southern Patagonian icecap in 2001, accessing to the glacier plateau required us to enjoy 17 days in this ice labyrinth, with backpacks over 40kg. Every 65 minutes, the two of us would break 5-7 minutes to rest and have a short discussion about which trajectory we shall head for the next period. Sure enough, our initial opinions were rarely aligned, but within a couple of minutes we could build the best out of them. The same decision pattern was intuitively applied for small decisions or for more radical ones during this journey. Managing risk “- No risk, no fun. – No brain, no pain. – Indeed” Anonyms Risk is a part of any accomplishment. It is defined by the the Oxford dictionary as “the probability or threat of quantifiable damage, injury, liability, loss, or any other negative occurrence that is caused by external or internal vulnerabilities, and that may be avoided through preemptive action.” Possible risks in an expedition are to be assessed, based on experience, their potential consequences and the cost to mitigate them. Following an equipment failure, an expedition can try to limit the consequences by doubling it or carrying adequate repair kits (turning a tool into a degraded version, less efficient but still useful). However, carrying more weight is a risk in itself, as a party will be slower, spending more time in a “dangerous” place, therefore increasing the exposure to adverse events. Risk can also come from external hazards, such as falling into the water or meeting a polar bear. Obviously, such situations cannot be addressed by a polite decision jabber, as seconds literally count. Nor it is the time to improvise, as the stakes can be high. Let’s consider a situation of a two week camping party to watch polar bears, in Eastern Svalbard. First research quickly shows that a) there are definitely a lot of polar bears in the area; b) they can occasionally consider anything as food; c) “don’t go there, you’re doomed to fail” seems a general advice. Pushed by the upper motivation of enjoying a first hand sight of those incredible animals, let’s consider pursuing the project. Evaluating the risk is conducted by a careful study of past human/bear encounters, and the factors that might have clustered a “bad” from a “good” encounter (an encounter is to be labeled as “bad” in case of a physical contact). Although such a study might greatly reduce risks, it will not totally eliminate them. Therefore, common scenarios are pulled out from the study and a plan agreed upon by the party in case of emergency situations, e.g. “X handle the gun and is ready to shoot; Y talks with the bear and decides when the situation is out of control; Z shoots a video for posterity.” Such plans have proved not to be applicable in every situation ( e.g. when guns were found to not to be rented to foreigners) but they definitely help when a problem actually occurs. The worst situations can be observed when the risk is known but not acknowledged and luck is called upon to solve issues. ours_l A bear passing by, in Eastern Svalbard (video) in 2001 . Risk assessment in IT is a popular and prolific discipline. Whether in agile or other type of methodologies, tools and methods do exist to face it. Without digging into architectural or functional concerns, let’s just point out the use of Proof of Concept projects, sound software craftsmanship practice, design and code review, security testing, “what can go wrong?” brainstorming sessions and pre mortem analysis. Measures, predictions and outcomes Let’s close our contemporary discussion and roll back to 1911, where we left Scott and Amundsen. And let’s follow them until the end of their adventures. As they were about to launch their final assault, the big question was: “ do we have enough resources to complete the Journey, plant our flag at the Pole and head back to our harbor?” Each of them had built resupply depots down the route during winter, as carrying all the food and fuel at once seemed impossible at the time. As displayed in the figure below, Amundsen had taken particular care to install his depots at regular one degree intervals, up to 85°S, while Scott party was more driven by the circumstances and set them up based on a “best effort” driver. Amundsen and Scott’s routes – with a zoom in on the right. Resupply food depots for the Amundsen’s team were set up at strictly one degree interval, before the final Plateau rush, while Scott’s are more irregularly placed. Amundsen setup measuring tools: a wheel was attached to a sledge to report the daily covered distance, as astronomical observations are time consuming and imprecise under those latitudes. With measures and regular iterations (the food depot distance), the team was able to predict how a constant effort would lead them to the goal or not. As the team leader, Amundsen also wrote how a decent amount of sleep and food was important for the men to be in shape to sustain the moving pace for a long time. Regular iterations, measures, sustainable effort. The tempo was set and led the team to a success, with men coming back to the harbor and reporting to have gained weight! Scott journey is another story. Designed from the beginning to be an heroic march, it quickly became so. Those men diaries are a great inspiration on how one can push their physical and psychological limits to an end. But to an end it would be. Reaching the Pole a month after Amundsen party, they discover the norwegian flag slapping over a tent and a message to them. If deception wasn’t hard enough, men also wrote down their doubts about making it back alive. An heroic march back started, and starving men would walk to their death, caught in a last storm twenty kilometers away from a major food depot. One of the most dramatic burndown chart ever, where delivering means making it back alive. The effort is the number of kilometers to go from the winter base camp to the Pole and back. Amundsen team had a slightly shorter round trip, but Scott team did benefit from a known route and an “easier” access to the Plateau, scouted by the previous Ernest Shackleton’s expedition. The chart is extrapolated for Scott’s return trip, as his logbook does not contain daily or regular position reports (progress was not as linear in reality). On the contrary, Amundsen’s party increased speed on the end, travelling lighter from one depot to the other. Regular iterations and throughput are the ultimate goals of an agile delivery. Measuring, via velocity, lead time, throughput certainly are ways to assess this goal and more importantly to predict it. However, if everything can be measured, IT project shall not drown under useless figures, that no one will watch but once. For example, it might be worth for a while to time the process of deploying an application to production if the step is felt to be painful. Actions can be undertaken to improve this part of the delivery and evaluatued against hard metrics. But once the process has reached an acceptable pace, there may be no longer the need to report them. In other situations, it might even be wise to keep track of the team “mood level” by asking everyone, after each iteration, to grade their current “happiness in the project” on a scale from 1 to 5, without further justification. Measures are endless but they shall be used on dedicated purpose. As Lord Kelvin once said: “you can only improve what you can measure ”. Another blasting parallel between the agile methodology and Amundsen approach is the attention to build a sustainable pace. Extreme Programming, for example, explicitly states a “40 hours per week” in its core value. Bypassing that limit shall only be exceptional for one week, and pushing the exception further is considered a failure. Oslo, the Silicon Valley of polar expeditions Throughout this article, we have focused on Roald Amundsen and taken a couple of slight detours mentioning other Norwegian explorers, Fridtjof Nansen and Børge Ousland. But they are only three out of an impressive ecosystem. We could have mentioned Sjur Mørdre, Vegard Ulvang, Liv Arnesen or many others. How such a small country can have produced so many great accomplishments? The culture of a country can explain it partially (even though that can be another chicken and egg dilemma). Another explanation can also come from the ecosystem which has grown around Oslo. Small entrepreneurs certainly have reached amazing success, but they are the tip of the iceberg. Many more modest enterprises have set off, a network of tool developers has emerged. And more importantly, a culture of innovation and sharing has grown. Even if considered as a star, Børge Ousland always took the time to answer our questions (even by fax at the time), opened his workshop, shared tips, gave advice. And the parallel with the Silicon Valley cannot be more blasting. If it is often summarized to a few successful giants, the real magic of the San Francisco Bay lies elsewhere. It is of course grounded in historical roots (Nasa, HP, Rank Xerox, Bell) and has seen its culture ingrained by the Hippie movement. But the real energy of the Bay area is an incredibly dense network of small interactions (meetups, bus discussions), a will to share and fearless garage dreamers. Conclusions If organization processes, tools and rituals are key to a successful agile transformation within a company, deep and lasting success roots are deeper. It must be backed by a profound will of the upper management to alter the fundamental decisions channels. If not, bringing Agile in a corporation will be a varnish of Scrum certified peoples, with technical teams applying rituals at best and improving their software craftsmanship practices. And business deciders believing they can constantly change the project directions at any time because: “hei! You’re agile now!” We presented in this article a confrontation in approach between a small Norwegian enterprise and one of the largest and most powerful organization of the time. We could have found other examples alike, making similar oppositions between a Nansen expedition and that from another Royal Navy dramatic expedition, led by Sir Franklin. It would then be easy to simply oppose startup and big corporations. But the organization size is not a determinant. We could also have focus on how Ernest Shackleton, another Royal Navy captain, led his expeditions in a style much closer to Amundsen’s one and with great achievements. And pinpointed small modern expeditions, engulfed in top down decision making, stubborn technical choices and failure. Finally, to answer those who hide behind the fatalist “my company cannot change” ,  we cannot forget to mention the extraordinary adventure of David Marquet, US Navy Captain on a nuclear powered submarine [13]. He totally transformed the chain of command and the decision process in a domain with the highest process constraints. The “agile” word was nowhere, but the spirit was. References [1] T he South Pole: An Account of the Norwegian Antarctic Expedition. Roald Amundsen, 1912. Gutenberg.org [2] Scott’s Last Expedition . Robert Falcon Scott. Gutenberg.org [3] Scott and Amundsen . Roland Huntford, 1979 [4] The First Crossing of Greenland. Fridtjof Nansen, 1890 [5] Farthest North. Fridtjof Nansen, 1897 [6] Letter to Sir Clements Markham from Fridtjof Nansen, 4 April 1913 [7] Les chasseurs, Les Inconnus, 1991 [8] The Myths of Innovation. Scott Berkun, 2010 [9] http://www.nationalgeographic.com/explorers/bios/borge-ousland/ [10] http://www.ousland.no/about/borge-ousland-explorer-and-adventurer/ [11] Agile Retrospectives: Making Good Teams Great .  Esther Derby & Diana Larsen, 2006 [12] http://www.artofmanliness.com/2012/04/22/what-the-race-to-the-south-pole-can-teach-you-about-how-to-achieve-your-goals/ [13] Turn the Ship Around!: A True Story of Turning Followers into Leaders . David Marquet, 2013 Tweet Share 0 +1 LinkedIn 0 This entry was posted in Culture , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Polar Expeditions and Agility: The 1911 Race to the South Pole and Modern Tales” ulf edvinsson 03/10/2017 à 17:48 Hello, more, much more on the above subject (including data) can be found in a new book Captain Scott: Icy Deceits and Untold Realities (Three Volumes Bound as One) by Krzysztof Sienicki. Look at Amazon. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-03-12"},
{"website": "Octo", "title": "\n                How does it work? Kubernetes: Episode 5 – Master and Worker, at last!            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-kubernetes-episode-5-master-and-worker-at-last/", "abstract": "How does it work? Kubernetes: Episode 5 – Master and Worker, at last! Publication date 27/02/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey everybody, TL;DR I hacked something together in order to create a Kubernetes cluster on CoreOS (or Container Linux) using Vagrant and Ansible. If you keep reading, I’m going to talk to you about Kubernetes, etcd, CoreOS, flannel, Calico, Infrastructure as Code and Ansible testing strategies. It’s gonna be super fun. The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts. This is episode 5, regarding the actual installation and configuration of the Kubernetes Master and Worker nodes. If you want to try it: git clone https://github.com/sebiwi/kubernetes-coreos cd kubernetes-coreos make up This will spin up 4 VMs: an etcd node, a Kubernetes Master node, and two Kubernetes Worker nodes.  You can modify the size of the cluster by hacking on the Vagrantfile and the Ansible inventory. You will need Ansible 2.2, Vagrant , Virtualbox and kubectl . You will also need molecule and docker-py , if you want to run the tests. Let’s install our Master node! Absolutely. So basically, we need to create an API server, a Scheduler, and a Controller Manager Server on the Master node. First, we’ll add the TLS resources needed for the master node. That means the CA certificate and the API server certificate and key. Nothing complicated about that. Next up, networking. For this we’ll use flannel and Calico . In order to configure flannel, we just add configuration environment variables under `/etc/flannel/options.env`. These specify that the flannel interface is this node’s public IP, and that the cluster configuration is stocked in etcd cluster: /etc/flannel/options.env Then, we add a system-drop in (a method for adding or overriding parameters of a systemd unit) for flannel, in which we specify that we want to use the configuration specified above when the service launches: /etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf System drop-ins are pretty cool because they only modify the specific settings you modified, and everything else stays the same. The flannel configuration is actually stored in etcd. We create it under the coreos.com/network/config namespace using a simple uri task . After that, we gotta configure Docker on the virtual machine. Actually, the only thing we need is to be sure that flannel is used for networking. Basically, flanneld needs to be running when Docker starts: /etc/systemd/system/docker.service.d/40-flannel.conf Now that the basic requirements are configured, we’re going to configure a whole set of components that are necessary in order to run a Kubernetes cluster: the kubelet, the Kubernetes Proxy, the Controller manager, and the Scheduler. With the exception of the kubelet, all the other components will be deployed in a container form. This is why I said in the first article that the Master node actually does run some containers. How cool is that? First up, the kubelet . This is the agent on the node that actually starts and stops containers, and communicates with the Docker engine at a host level. It is present on every node in a Kubernetes cluster: both master and worker nodes. It talks with the API server using the certificates we created earlier. The kubelet does not manage containers that are not created by Kubernetes. The master node configuration of the kubelet does not register for cluster work (since it is a master and not a worker). The kubelet may use different standards for networking. One of these standards is the Container Network Interface , or CNI . The CNI is a set of specifications and libraries for writing plugins to configure network interfaces in Linux containers. The only concern of CNI is network connectivity of these containers, and then removing allocated resources when the containers are deleted. So when using Calico, the kubelet uses CNI for networking. Calico is aware of each pod that is created, and it allows them into the flannel SDN. Both flannel and Calico communicate using CNI interfaces to ensure that the correct IP range is used for each node. The kubelet configuration can be seen here . As of this moment configuration files start to get really verbose so I’ll just mention the most important parts. This one specifies the address of the API server, the network plugin to use, the DNS service address and general kubelet configuration, like log files location and configuration directories. The configuration also creates a “manifests” directory, and watches it. This means that for every Pod manifest that is stored in that location, a matching Pod will be created by the kubelet, just as if the Pod was submitted via the API. We will take advantage of this functionality extensively from now on. After that, we gotta set up the API server. This stateless server takes in requests, process them and stores the result in etcd. It’s one of the main components of the Kubernetes cluster. What we would normally do in order to create a server of this kind is to send a request to the API server, with the Pod manifest of the Pod we want to create. But we don’t have an API server yet. Huh. We’re going to use the manifest directory that we mentioned on the previous paragraph. When we place the API server manifest inside of the “manifests” directory, it will be automatically created as soon as the kubelet is launched. Pretty neat, huh? We’re going to use this same strategy for the Proxy, the Controller manager and the Scheduler. The API server configuration is pretty long and might be confusing at first. Between many other things, it needs to: Be accessible on the host machine address Be able to access etcd Be aware of the service range we’re going to use for service IPs Access the SSL certificates we created earlier And so on. If you want to take a look at the configuration, the template is right here . Then, there’s the Kube Proxy. It redirects traffic directed to specific services and pods to their destination. It talks to the API server frequently. In this case, it is used in order to access the API server from the outside. It’s configuration can be found here . Let’s take on the Controller manager. This component basically applies the necessary changes based on the Replication Controllers. When you increase or decrease the replica count of a pod, it sends a scale up/down event to the API server, and then new containers are scheduled by the Scheduler. It’s configuration can be found here . Last but not least, we need to add the Scheduler configuration. This component watches the API for unscheduled Pods, then he finds a machine for them to run and informs the API server of the best choice. We haven’t configured Calico yet, have we? Let’s add it as a service. We will create the “/etc/systemd/system/calico-node.service” file. It’s configuration can be found here . Basically, it talks with etcd in order to store information. Now every container launched will be able to connect itself to the flannel network with its own IP address, and policies created using the policy API will be enforced. Calico also needs a policy-controller in order to work. This component will watch the API and look for changes in the network policy, in order to implement them on Calico. It’s configuration can be found here . Finally, (yes, this time it’s true) the kubelet service configuration specified a cni configuration file we need to create. This file specifies which CNI plugin needs to be called on startup. This creates the flannel plugin, but then delegates control to the Calico plugin. This might sound convoluted at first, but it is actually done so that Calico knows which IP range to use (which is determined before by flannel). It’s a pretty short configuration so I’ll just put it here: Easy After that, we’ll just start the services and cross our fingers (don’t worry, it will work). You can see that this role actually has another yaml file embedded into the main.yml file . It is called namespaces.yml. It is included because we need to create a Kubernetes namespace for the Calico policy-controller to run (we specified that here ). And that needs to be done after the API server starts responding, since it is a Kubernetes functionality. So we just create the calico-system namespace if it doesn’t exist already . By the way, this Master node is not highly available. In order for to have high availability on the Master node, I would need to add a component in order to manage the virtual IP of the master node, like keepalived , or to handle it with fleet . I might do something about this in the future. That’s all for the master node configuration. We should have a master Kubernetes node running alive and well by now. Are you tired? There’s still more! Kubernetes worker node The master node does not run any containers, it just handles and manages the cluster. The nodes that actually run the containers are the worker nodes. We’re going to configure two of them. We’ll just start by configuring the SSL resources the same way we did it on the Master node. Nothing new here. The code just puts them under the “/etc/kubernetes/ssl” directory. Moving on. Network-wise, we’ll use flannel the same way we did on the Master node. I didn’t create a flannel role because I figured that the flannel configuration might change from Master to Worker in the next Kubernetes release (turns out it did with 1.5!). Same with the Docker configuration. We just want flannel to be running before we run Docker. Next up, the kubelet. We will not disable the `register for cluster` work flag, since we want for these nodes to do the heavy lifting. We will also configure it to talk to the master node, to use the CNI network plugin, the specified DNS server, and its own advertising IP. Here’s the configuration if you want to check it out. We’re going to tell the kubelet to call flannel as a CNI plugin, and then to delegate the control to Calico, the same way we did it on the master node. We’ll need to specify the master node’s IP on the configuration here instead of localhost , since the the configuration needs to access the API server. The configuration template can be seen here . As we said before, there’s a kube-proxy instance in each node. That means there’s one on the worker nodes too. It’s configuration specifies the master nod. Nothing fancy. It’s configuration can be found here . We’re going to add a kube-config configuration, in order to specify the TLS resources that are necessary for secure communication between the different Kubernetes components. It can be seen here . Finally, we need a Calico Node Container too, that will fulfil the same role it did on the Master node. It’s configuration can be found here . After that, we start every service, and we should have our full Kubernetes up and running. It might take a while, depending on your internet connection. Let’s recap for a second. What just happened? We created one etcd node (if you didn’t touch the defaults), then we spawned one Kubernetes master node that uses the etcd cluster to persist the cluster state, and two Kubernetes worker nodes that can host container workloads. Yay! We still need a way of interacting with the cluster. For that we’ll use the standard Kubernetes management tool, kubectl. Configuring kubectl You can download kubectl and put it wherever you want in your computer. Try to add it to a directory that’s in your PATH, so that you don’t have to reference the whole path every time you do a kubectl action. Make sure it is executable, too. After that you can configure it by specifying the certificates and the Master host’s URL: There is no kube This configuration is only applied when the cluster is not configured, though. That way, we keep our idempotence neat and clean. That was easy, kinda. Now what? Oh yeah, cool stuff on our cluster. Add-ons Add-ons are… well… things you add-on to your Kubernetes cluster in order to have improved functionality. They are created using Kuberntetes native resources. Most of the time they will be pods, so we can juste create a manifest for them and then create them using the API server (through kubectl). There are two add-ons that are commonly used on almost every standard Kubernetes installation: the DNS add-on, and the Kubernetes Dashboard add-on. The first enables service discovery for your containers. They can have a DNS name, and they can be reached by other containers with it. The manifest is huge. It’s because we’re creating a Service, which provides DNS lookups over port 53 to every resource that demands it, and also a replication controller, which makes sure that there is one replica of the pod at all times. The configuration can be seen here . The Dashboard allows you to see general information about your cluster. That means Pods, Services, Replication Controllers, and all that under different namespaces. It’s a pretty cool tool, honestly. We’ll create a Service and a Replication Controller for it too. You can access the Kubernetes Dashboard by using the port forwarding functionality of kubectl: kubectl get pods –namespace=kube-system kubectl port-forward kubernetes-dashboard-v.1.4.1-ID 9090 –namespace=kube-sytem And now, your Kubernetes Dashboard should be accessible on port 9090. Show me the money! Woot! Almost done. Now we only need to test that it works, and that our code is working as intended. So, `molecule test` says that: Our infrastructure is created without any hiccups. The playbook runs as intended. And that our code is properly linted, and it is idempotent as well! Let’s have some fun with our cluster now. Sample app We’re going to use the guestbook example that’s included on the Kubernetes samples. This is a good application to test a proper installation, since it’s a multi-tier application, with multiple components speaking to each other, on different nodes. The guestbook application is created under the kubernetes-resources directory. It can be launched using kubectl: kubectl create -f guestbook.yml We can see that the resources are properly created on the Dashboard: The gang’s all here! And then we can even test the application by port-forwarding to the frontend application: kubectl get pods kubectl port-forward frontend-ID 8080:80 The application should be accessible on port 8080. You can test it by adding a message to the guestbook: Get it? Great. It works. Mission accomplished! So what did we learn in the end? CoreOS (Container Linux) is a lightweight Linux distribution that runs almost everything inside of containers An “Ansible” is a fictional device capable of instantaneous or superluminal communication. It is also a pretty powerful IT automation tool. You can move around on remote hosts using Ansible, even when you don’t have Python installed on them etcd is a distributed key-value store, which is used as the standard distributed storage by Kubernetes Flannel and Calico can be used together to provide SDN-based connectivity and network policies for containers located on different hosts You can use Molecule to continuously test important aspects of your Ansible code The egg came before the chicken Final thoughts Phew, yeah, that was kinda long. Anyways, I hope you had fun. I know I did. Still, I would have loved to work on some other things too: Use etcd3 instead of etcd2 Install Python “The CoreOS way”. The PyPy installation works, but it feels kinda hacky. I’d have loved to run Python from inside a container and provision the host with Ansible somehow Use SSL on the etcd cluster Use fleet for certain tasks, like deploying unit files, or handling the Master node high availability I might work on these things in the future. Anyways, see you on the next adventure! You can find the whole article in PDF format in this link: how-does-it-work-kubernetes Seb Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 9 commentaires sur “How does it work? Kubernetes: Episode 5 – Master and Worker, at last!” Dalvo Trombeta 16/03/2017 à 01:51 First, thanks for the series of posts, that was great!\r\nOne qq: Can you share instructions on how to get the actual  kubeconfig file to allow access to the cluster that was provisioned? Sebastian Caceres 16/03/2017 à 13:58 Hello,\r\n\r\nThe kube config file is generated automatically by the kubectl role. You can also launch the commands used in kubectl/tasks/main.yml by hand in order to generate it, while replacing the variables by hand. I'd really recommend that you let Ansible do that for you though.\r\n\r\nSee this link for further information: https://github.com/sebiwi/kubernetes-coreos/blob/master/roles/configure/kubectl/tasks/main.yml Dalvo Trombeta 27/03/2017 à 14:04 Wow, thanks! :) Bill Milligan 21/07/2017 à 23:27 This is really great work.  I've spent days wrestling with kubeadm and incomprehensible hangs and errors.  I found your blog and though there were a couple of hiccups, I not only have a cluster up and running within a couple of hours, but I feel like I learned a ton more than I could have even had I gotten kubeadm to work.\r\n\r\nA couple of things I can share that I learned that might supplement what you hve here:\r\n\r\nRunning on a bare metal Ubuntu machine, apt does not know about Ansible versions greater than 2.1.  To get 2.2 or greater,\r\n\r\nsudo apt-add-repository ppa:ansible/ansible\r\nsudo apt-get update\r\nsudo apt-get install ansible   #gives me 2.3.x\r\n\r\nTo install Molecule and docker.py:\r\nsudo apt-get install gcc python-pip python-vagrant libssl-dev libffi-dev\r\nsudo pip install molecule\r\nsudo pip install docker.py\r\n\r\nNext, you will need some additional resources for Vagrant to run these scripts successfully:\r\nsudo apt install ruby-dev\r\nsudo vagrant plugin install vagrant-hostmanager\r\n\r\nFor some reason one of the etcd tests fails for me.  The test however doesn't seem completely necessary (we've already verified it is listening on the right port and that etcdctl ls works) and so I removed this test:\r\n\r\n- name: Test that the coreos.com node exists\r\n  assert:\r\n    that:\r\n      - \"'/coreos.com' in etcd_nodes.stdout\"\r\n\r\nfrom ./roles/configure/etcd/tasks/test.yml\r\n\r\nIf you remove this, be sure that you do not leave a trailing end line in test.yml!  Molecule will crap out if you do.\r\n\r\nIf you have run minikube previously and have shut it down, delete your minikube context from your local kubectl or else kubectl configuration will fail with a timeout.\r\n\r\nI'm down to where I have one last molecule test failing on one of my worker nodes -- for whatever weird reason, flannel will work on kube-worker-01 but not kube-worker-02.  I don't think this is a huge deal however (or is it?) -- dashboard works and I'm ready to start tinkering with the setup until it matches my needs.\r\n\r\nThanks a ton, Sebastian!!! Bill Milligan 22/07/2017 à 16:55 Stupid question.  I have created a new project based on yours with only small changes.  It does not execute ansible, getting a series of errors that \"Failed to connect to the host via ssh: ssh: Could not resolve hostname rogue-etcd-01: Name or service not known\".  The names in inventory/vagrant.ini should be the same as what my Vagrantfile is now producing.  I realize I do not understand how ansible is resolving these names.  Documentation indicates the inventories/vagrant.ini file does it, but in your original inventory file there is no indication of ssh port.  So now I'm wondering, why does yours work??? Bill Milligan 22/07/2017 à 17:13 Never mind my last question -- was being stupid and did not include your ansible.cfg. Rob 21/06/2018 à 14:51 What does this mean?\r\n\"First, we’ll add the TLS resources needed for the master node. That means the CA certificate and the API server certificate and key. Nothing complicated about that.\"\r\n\r\nCould you please give a little more info so those of us who are confused may figure out what we're supposed to do in this step? It's easy to figure out what a CA certificate is but there's nothing else in this description which tells me what you mean by an API server certificate, or where I would put it if I knew what you were talking about. Rob 21/06/2018 à 19:59 \"The flannel configuration is actually stored in etcd. We create it under the coreos.com/network/config namespace using a simple uri task.\"\r\n\r\nCould you be more specific? I see that you link to the code, this appears to be PUT command we need to run, but we have to work out the dynamic parts of the URL... Couldn't you include an example of the cURL command? I'm assuming you sent something to this URL using cURL but I don't know what. Himanshu Dureja 06/12/2018 à 18:45 Thanks, very well explained Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-02-27"},
{"website": "Octo", "title": "\n                How does it work? Kubernetes: Episode 4 – How to Ansible your CoreOS, and etc(d)!            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-kubernetes-episode-4-how-to-ansible-your-coreos-and-etcd/", "abstract": "How does it work? Kubernetes: Episode 4 – How to Ansible your CoreOS, and etc(d)! Publication date 20/02/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey everybody, TL;DR I hacked something together in order to create a Kubernetes cluster on CoreOS (or Container Linux) using Vagrant and Ansible. If you keep reading, I’m going to talk to you about Kubernetes, etcd, CoreOS, flannel, Calico, Infrastructure as Code and Ansible testing strategies. It’s gonna be super fun. The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts. This is episode 4, regarding the usage of Ansible on CoreOS, and etcd. If you want to try it: git clone https://github.com/sebiwi/kubernetes-coreos cd kubernetes-coreos make up This will spin up 4 VMs: an etcd node, a Kubernetes Master node, and two Kubernetes Worker nodes.  You can modify the size of the cluster by hacking on the Vagrantfile and the Ansible inventory. You will need Ansible 2.2, Vagrant , Virtualbox and kubectl . You will also need molecule and docker-py , if you want to run the tests. Can I see the code now? Right, code. The first step is to actually create the CoreOS virtual machines. I used a really simple Vagrantfile in which I specify how many instances I want, and how much computing resources each one of them is going to have: Amazing complexity You can modify the amount of instances you want to create in this part. Be aware that if you add hosts, you will also need to add them in the inventory for the Ansible code to target all the machines. I also created a simple IP addressing plan. The principle is that each machine subtype is going to have less than 10 nodes. So I just number them from 10.0.0.1×1 to 10.0.0.1×9, with x being 0 for the etcd nodes, 1 for the Kubernetes master nodes, and 2 for the Kubernetes worker nodes: Absolute power By the way, you can configure your virtual machines by specifying an Ansible playbook, using the Ansible provisioner in your Vagrantfile like this . I choose not to do so because I like having my `vagrant up` actions separate from my actual configuration. Extra coupling does not abide by this philosophy. Now you’re only a `vagrant up` away from having your CoreOS virtual machines running on your computer. After that, you can export the SSH configuration used by Vagrant with `vagrant ssh-config > ssh.config`. You can then use this configuration for the Ansible configuration, if you include it in your ansible.cfg file: Like this I really like Makefiles. I use them quite often when I have write more than one long command, or many different ones that are going to take a while. I’m also kinda lazy. So I just did this: make: automating your life since 1977 This way, `vagrant up && vagrant ssh-config > ssh.config` becomes `make vagrant`, which saves me 3,7 seconds every day. You can check the Vagrantfile , the ansigle.cfg and the Makefile on the GitHub repo. Let’s set up our testing configuration now. Molecule uses a yaml configuration file called molecule.yml. You can use it to define the configuration of your test infrastructure, and to specify the playbook that you want to test. Usually, I try to test each role independently. That means that I’ll have a molecule.yml file per role directory, and also a playbook.yml file that uses the role that I’m testing. Like this The molecule.yml file specifies an infrastructure (I usually use a single Docker container), and it also specifies which playbook needs to be used on the previously defined infrastructure. For the example in the image above, the playbook.yml file would just include the docker-gc role. Molecule also does this for you automatically if you type `molecule init –driver docker` inside of your role directory. Dope. This is a pretty good testing strategy when you have roles that are independent from each other. It works like a charm when I test my docker-gc role. The thing is that for this project, the different components depend on each other. I cannot test that my kube-master role is working if I don’t have an etcd cluster running. I cannot test that my kube-worker role is working if I don’t have at least one Kubernetes master node. Tricky. So instead of creating a molecule.yml file for each role, we’re going to create one at the root of the project, that is going to test our master playbook. Inside of it, we’re going to specify a sample test architecture, which is going to be the same one we defined in our Vagrantfile. We’re also going to specify the playbook that we want to test. We’re going to name it kubernetes.yml (creative right?). You can see the molecule.yml file here . You can test the roles using the `molecule test` command. This will: Create up the infrastructure (create) Verify the syntax of the Ansible roles (syntax) Execute the playbook (converge) Check that the playbook is idempotent by checking for the diffs that a new execution would apply with a dry run (idempotence) Destroy the previously created infrastructure (destroy) These actions can be ran separately by typing `molecule <action>`, and replacing action with one of the expressions between parentheses above. For example, `molecule converge` will just play your playbook on the hosts. You get the idea, right? I added the `molecule test` command to my Makefile , under the `test` target. That means that I can run `make test` instead of `molecule test`. That makes me gain 0.4 seconds per day. We’re up to 4.1 seconds in total. Sweet! Now that we got our virtual machines running, and our testing configuration ready, let’s start configuring stuff. We just gotta take care of a little problem before we can go YOLO with Ansible though. CoreOS and Ansible Remember back then when I said that CoreOS ships only with the basics? That means no Python. By extension, that means no Ansible . That’s why this time it’s a little bit more tricky. So we need to do something about that. If CoreOS won’t come to Python, Python must go to CoreOS. Since we cannot use Ansible to install Python because we need Python to execute Ansible modules in the first place, we’ll just install Python manually in each one of the machines, right? Well, no, not really. By the way, if the previous question made you think about the chicken or the egg causality dilemma, just know that the egg came first . There are three Ansible modules that do not need Python installed on the target host: the raw module , the script module , and the synchronize module . The first allows you to execute an SSH command, without going through the module subsystem. The second one allows you to copy a script to a remote host, and execute it using the remote host’s shell environment. The third is a wrapper around rsync, which just uses rsync on the control machine and the remote host. Using these modules, we can install a lightweight Python implementation called PyPy . The workflow is as follows: I verify that Python is installed using the raw module, and if that is not the case, I install it using a more raw tasks. Right on Why use the `changed_when: false` flag on the tasks inside the block, you say? The thing is that each time you execute a raw task, a change will be made, since the shell command is actually being executed on the target host. That means that each time you run your playbook, you will execute the tasks, no matter what. So if you’re downloading things, creating directories, or adding lines to configuration files, you will do so multiple times. This is not idempotent. That’s why I verify the state of the Python installation before installing Python, and only execute it when Python is not installed. I just add the `changed_when: false` flag to the verification tasks, since they only verify the existence of the Python associated resources; there are no system modifications because of them. I feel this is slightly better than executing a shell script with every task embedded into it. It allows me to replay tasks when certain script do not exist, and to have a clear idea of what failed when I get an error: I know right away which task failed, which helps in the debugging process. Thanks mom [ 1 ] I did not create this approach , by the way. I just optimised it to make it actually idempotent. I guess you need to stand in the shoulders of giants in order to further from time to time, right? Let us revisit testing for a second. As a general rule when writing Ansible code, I try to tag roles and tasks as much as possible. This helps a lot if you want to execute only one part of your playbook, or only one role. I also try to use smoke tests whenever it is possible. This means that I’ll check that the main feature of my role is working after executing it. If I’m installing Python, I’ll just do something like `python –version` and check that I don’t get any errors. For that, inside of each role’s tasks directory I’ll try to create a main.yml file, which will in turn include a configure.yml file and a test.yml file. The configure.yml file will do all the installation/configuration of the specified component, and the test.yml file (tagged with the test tag) that will test the component, if possible. Smoke test all the way! By doing this, you will actually test that your infrastructure is running and that it is probably properly configured. Then, if you want to run nothing but your tests, you can do it if you specify the `test` tag while running a playbook. Something like `ansible-playbook -i inventories/vagrant.ini kubernetes.yml –tags test`. And thus, the ‘smoketest’ target on my Makefile is born. Let us continue. SSL I won’t go really deep into this part. OpenSSL exists since 1998 , so it’s not exactly recent news. I create a CA for the whole cluster, and then create keys and sign certificates for the API server, for each one of the workers, and for the administrator (the one that’s going to be used by you when configuring kubectl). etcd In this deployment we’re using a single etcd node. You can modify the number of instances from the Vagrantfile, the Ansible code is able to handle a multi-node cluster. Just use odd numbers, because of fault tolerance . Anyways, configuration is pretty straightforward on the single-node scenario. I just configure it to listen on every interface, and then add the advertise client url to the etcd unit using environment variables: Straightforward And it gets slightly trickier with a multi-node configuration, since the nodes need to be aware of each other, using the ETCD_INITIAL_CLUSTER variable. You also need to provide a node name, and a cluster token for everything to work. There are other options, like using an existing etcd cluster as a discovery mechanism (but we don’t have one at the moment), or a public etcd discovery system . Less straightforward All of these configurations can be made either with environment variables or with flags when starting the etcd2 service. The ansible_env.COREOS_PUBLIC_IPV4 variable will be replaced by the node’s public IP. I do this often on this project. Then, I just start and enable the service. This is done with the systemd module, and that’s why we need Ansible 2.2. The test part of the role verifies that machine is listening on port 2379, that the etcd cluster is reachable via etcdctl, and then it verifies that the “ coreos.com ” default namespace exists. It’s a simple, effective smoke test. With our working 1-node etcd cluster (get it?), we’ll configure the Kubernetes master node. Excellent, how? Aren’t you tired already? I know I am. That’s all for today. I’ll talk to you about the really really fun part in the next article. Stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 5 commentaires sur “How does it work? Kubernetes: Episode 4 – How to Ansible your CoreOS, and etc(d)!” none 10/05/2017 à 13:19 Hi there, thanks for that great post...\r\nI have just a question, i've already download all requirements and the git project, but when running \"make up\" for the first time i get an error in the etcd part: \r\n\r\nERROR! no action detected in task. This often indicates a misspelled module name, or incorrect module path.\r\n\r\nThe error appears to have been in '/home/user/workspaces/kubernetes/kubernetes-coreos/roles/configure/etcd/tasks/configure.yml': line 15, column 3, but may\r\nbe elsewhere in the file depending on the exact syntax problem.\r\n\r\nThe offending line appears to be:\r\n\r\n\r\n- name: Start etcd service\r\n  ^ here\r\n\r\n\r\nThe error appears to have been in '/home/user/workspaces/kubernetes/kubernetes-coreos/roles/configure/etcd/tasks/configure.yml': line 15, column 3, but may\r\nbe elsewhere in the file depending on the exact syntax problem.\r\n\r\nThe offending line appears to be:\r\n\r\n\r\n- name: Start etcd service\r\n  ^ here\r\n\r\n\r\nBut i cannot get it resolved in any way based on my knoweledge... any help can be provided?\r\nThanks anyway, your post has cleared some concerns about kubernetes. \r\nCheers. none 10/05/2017 à 13:48 I'll will answer my self... \r\nI think it was my ansible setup fixed on version 2.1. Updating to 2.3 solved my problem. I should read twice posts then... sorry to trouble you.\r\nThanks anyway. Sebastian Caceres 10/05/2017 à 14:01 Hi, thanks for your feedback!\n\nIndeed, you need at least Ansible 2.2 in order to be able to start services using systemd.\n\nFrom the article itself: All of these configurations can be made either with environment variables or with flags when starting the etcd2 service. The ansible_env.COREOS_PUBLIC_IPV4 variable will be replaced by the node’s public IP. I do this often on this project. Then, I just start and enable the service. This is done with the systemd module, and that’s why we need Ansible 2.2. Cheers! none 10/05/2017 à 15:33 Thanks for answering so fast Sebastian. \r\nCheers. Rob 23/06/2018 à 03:08 Isn't the discovery mechanism needed for all members in the cluster? How does this project work without a cloud-config or discovery token? https://coreos.com/os/docs/latest/cloud-config.html Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-02-20"},
{"website": "Octo", "title": "\n                Visualizing massive data streams: a public transport use case            ", "author": ["Alexandre Masselot"], "link": "https://blog.octo.com/en/visualizing-massive-data-streams-a-public-transport-use-case/", "abstract": "Visualizing massive data streams: a public transport use case Publication date 21/02/2017 by Alexandre Masselot Tweet Share 0 +1 LinkedIn 0 Public transport companies release more data every day and some of them are even opening their information system up to real time streaming ( Swiss transport , TPG in Geneva, RATP in Paris are a couple of local ones). Vast lands are unveiled for technical experimentations! Beside real time data, these companies also publish their full schedules. In Switzerland, it describes trains, buses, tramways, boats and even gondolas. In this post, we propose to walk through an application built to visualize, in fast motion, one day of activity, as shown in this movie . As real time data are not yet available, they were simulated, based on available schedule information. This pretext is too good not to dig into a stack containing Play/Scala/Akka on the backend, Angular2/Pixi.js/D3.js/topojson in the browser, linked together by Server Side Events . This prototype is intended to explore the possibility of doing massive geographical visualization in the browser, applying techniques described in a previous post . The backend and frontend code is available on github, and tests continuously ran on travis-ci . Figure 1: A couple of snapshots linked to Youtube screen recordings. The left figure shows transports across all Switzerland, with a dynamic chart indicating the number of circulating vehicles. The right part is a zoom on Zürich. Red is for trains, yellow for buses, blue for tramways, green for funiculars or gondolas and light gray for boats. The video shows the actual web animation, as the time is “accelerated” on the server side when pushing vehicle positions. Why this post? Public transport visualization is well addressed problem. For example, GeOps tracker proposes an impressive tool to dynamically map transit information. Most of the currently versatile solutions are based on schedule information, by opposition to Real Time. Moreover, they usually update vehicle positions by batch to limit traffic: in one HTTP transaction, all the current and forecasted vehicle positions are sent to the browser. Finally, web applications are classically limited to a few tens of vehicles, to cap the load on the infrastructure. In this article, we remain at the schedule level, but take the opportunity to explore the possibility of asynchronous massive data update: thousands of vehicles see their positions updated independently, opening up to more dynamic interactions. Beside the transport domain at hand, this prototype is an excuse to explore dynamic visualization. Figure 2: prototype architecture sketch. Backend: publishing the vehicle positions feed [ code ] Scheduled data: the GTFS format Let’s start with acquiring some data. The General Transit Format Specification , or GTFS Static , proposes a format to describe a public transport system. This format, created at Google, is widely used , from local companies up to full country layouts. For example, data for the official Swiss schedule are available to download , grouping information for more than 400 companies. A system comes as a set of .tsv files, typically worth for one year. The full format is described here , but the main components are the files: agencies.txt : companies running line (~400 elements, e.g. AGS (Andermatt Gotthard Sportbahnen)) ; stops.txt : localisation names, and geographical coordinates (~30’000, e.g. Jungfraujoch) ; trips.txt : route head name (~470’000); stop_times.txt : a link between a trip, a stop, a time (~8 millions); calendar_dates.txt : trips and date exceptions; and more, such as fares, transfer, frequencies information etc. Being a tabular format, information refer to each other via textual keys. Our prototype being based on Scala+Akka+Play, we will briefly describe a couple of components. Playing the schedule Loading a system A GTFSSystem , containing all the aforementioned data, is simply loaded into memory at start time from the original text files. For the sake of speed, the versatile tototoshi CSV parser was abandoned, to the benefit of a custom one . In the other hand, to save memory, BitSet were used to handle calendar date exceptions. Filtering trips The goal is to play one day of data, so the first obvious goal was to be able to filter trips by date or by agencies (no need to return city transit vehicle on a country wide map). Plotting geographic data also come with bounding coordinates: only trip with at least one stop laying within the viewport bounds are to be reported. Scala collections primitives definitely offer comfortable way to handle such filtering. Firings playing events The overall goal is to have simulated position events of vehicle across the day, let’s say every minutes (with some randomization). Based on the GTFS data at hand, we can then interpolate the positions between stops (these interpolations being linear, as the routes actual shapes are not provided). This approximation is a limitation of the simulator not a one of the visualization: with real realtime data, we will receive real positions. Our GTFS system can therefore be transformed into a logbook of vehicle positions, where the time scale can be accelerated (typically by a factor of 500 to play a full day in a couple of minutes), Actually scheduling these events is the next stone. We used the Akka Scheduler in two layers: schedule the start event of all the trips simulations across the day; once a trip is started, it actually schedules all its coming simulated positions. This two steps approach ensure not to overload the scheduler. Of course, less costly strategies could be put in place to run at a larger scale and lower footprint, but this one proved to be reasonable enough for the sake of this prototype. On the practical point of view, Akka scheduling consists in delaying the sending of a message to an actor along time. Just to get a taste of it actually working : simulatedPositionList.foreach({ simulatedPosition => val delay = timeAcc.inMS(simulatePositions.secondsOfDay) scheduler.scheduleOnce(delay milliseconds, actorSink, simulatedPosition ) }) Where: simulatedPositionList is the list of simulated positions for a trip; timeAcc is a time accelerator, transforming a schedule time into an accelerated and relative referential; actorSink is an actor that will receive all the simulated positions. Where to send them? This question is answered in the next section. Feeding Server Side Events One way to stream data from a backend to a web browser are Server Side Events . Well adopted by frameworks such as Angular 2 (see another OCTO post on the matter), they are particularly well suited to handle unidirectional flows, where a browser receives updates from the server As we’ve seen above, vehicle positions are published across a time window and sent to an actor ( actorSink ). This actor can then be connected to a Akka stream, used as the the source of the SSE flow. This can be done at the Play controller level. A naive implementation ties a source to an actor (the one used above as a sink by the scheduler) val (actorSink, publisher) = Source.actorRef [SimulatedPosition](10000, dropHead) . toMat (Sink.asPublisher(true))(Keep.both).run() //transform the object stream into JSON stream val jsonSource = Source.fromPublisher(publisher.map(Json.toJson) //the time is accelerated by a factor 500 val timeAcc = TimeAccelerator(500) //launches the event scheduling,\r\n//using the ref actor as a sink val actorSimulatedTrips = actorSystem.actorOf( Props(new ActorSimulatedTrips(actorSink,\r\n                                timeAcc,\r\n                                trips))) //pipe the stream to an EventSource protocol Ok.chunked(jsonSource via EventSource.flow) .as(ContentTypes.EVENT_STREAM) This implementation was named “naive”, as its major flaw is for the akka stream not be closed if ever the HTTP stream is canceled (browser tab is closed, reloaded etc.) This can be overcome by tying the publisher to a Future : if the stream is closed by the consumer, an ending message can be sent to the publishing actor, so the next scheduled events can be canceled. A code extract: //ties a Source to a Future def peekMatValue[T, M](origSource: Source[T, M]): (Source[T, M], Future[M]) = { val promise = Promise[M] val source = origSource. mapMaterializedValue { mat => promise.trySuccess(mat) mat } (source, promise.future) } val (queueSource, futureQueue) = peekMatValue(Source.queue[JsValue](10000, OverflowStrategy.fail)) futureQueue.map { queue => … //capture end-of-consumer queue .watchCompletion() .map { done => actorForward ! PoisonPill actorSimulatedTrips ! StopSimulation } } Ok.chunked(queueSource via EventSource.flow) At this stage, vehicle positions can be shipped down the line via an HTTP request. We therefore need to consume them from a client. Frontend: Angular 2, Flux and RxJS [ code ] From the browser side, various high quality JavaScript frameworks are available. ReactJS and Angular 2 certainly are the most prevalent for large data set handling and we opted for the later one, with a Flux architecture and the reactive RxJS libraries. A classic Angular 2 flux architecture The F lux architecture was originally proposed by Facebook and gained a large popularity with ReactJS, mainly through the Redux implementation. Described in the figure below, the main idea is to handle states in a centralized store. Components register to state change events issued by the store. The store is updated via actions, triggered either by user interactions or external events – in our case, Server Side Events popping in. Angular 2 is a strong proponent of the architecture, with an elegant implementation via ngrx/store and observables . Figure 3:  Flux architecture schema, by Adobe’s folks . The store In our situations, the stored state contains four components: URL route status (provided by default ) to handle parameters (focus coordinates etc.); vehicle positions : tied to the SSE stream; a coordinate system : referring to rendering component size, latitude/longitude boundaries and the associated projection; metrics : number of events received per second, number of active vehicle per type… Rendering components and services can then subscribe to store state changes via observable . At this stage, we are therefore receiving vehicle positions and are able to send them to a rendering component (see details below). However, geographic rendering usually also requires a map. Geographical concerns Once more, TIMTOWTDI. Several solutions are available to provide rich map information. The most prevalent certainly are Google Map API, or Open Street Map, without forgetting the more “artistic” versions from Stamen or high quality local systems such as the swiss GeoAdmin API or the french GeoPortail . Even though we have had positive experiences with those systems, we chose for this application the D3 + Topojson approach (check out Mike Bostock’s “command line cartography” post). Topojson offers an elegant vector based approach, where geographical entities (country, cantons, municipalities, lakes etc. are actually described by independent paths. This presents the advantage of a comfortable rendering with D3.js enter/update/remove pattern, the variety of packaged projections and customization through CSS. Moreover, it also provides a convenient way to link a geographical entity ( e.g. canton=Zürich or municipality=St George ) to a path, therefore to an optimal scaling on a viewport [ code ]. Using Topojson & D3.js We provide a short example of code to demonstrate how a topological feature can be converted to an SVG. The projection referential is used to fit the viewport and can be used to add other features. This later functionality will come handy in the next section, when time comes to append moving vehicles on the map. svg.attr('width', width) .attr('height', height); //topoCH is built from here , and get all municipalities const allFeats = topojson.feature(topoCH,topoCH['objects'][municipalities]) .features; //Just get one municipality const myFeat = allFeats.find((g)=> g.properties.name == 'Evolène'); //Pickup a projection among the dozens available //and adapt it both to the feature and the viewport const projection = d3.geoAlbers() .rotate([0, 0]) .fitSize([width, height], geoJson); //A function to transform a list of (lat,lng) into an SVG path string const path = d3.geoPath() .projection(projection); //And finally add the path svg.selectAll(\"path\") .data([myFeat]) .enter() .append(\"path\") .attr(\"d\", path) .style('stroke', 'blue') .style('fill', 'none'); //Projection can also be used to add other objects const cityCoords = projection([7.494, 46.112]); svg.append('circle') .attr('r', 10) .attr('cx', cityCoords[0]) .attr('cy', cityCoords[1]) .style('fill', 'red'); Displaying vehicle positions (and motion) (a) (b) (c) (d) Figure 4: various rendered views: (a) the full country, without city transit (as it is not relevant to be shown at such a scale ( Youtube ); (b) Zürich city transit ( Youtube ); (c) a dynamic histogram displaying the count of vehicles  across the day and throughput of received events; (d) for the sake of performance measurements, the full country is shown with city transit (although intra city motions are not visible at such a scale). Implementation Vehicles positions are consumed from the SSE stream and stored within a Flux store. Every second or so, the positions can be updated on the screen, with a smooth transition to illustrate the shiftings. Such transition are usually achieved via D3.js selection transitions but performance issues are faced when one handles several thousands of data points. The solution we used, mixing Pixi.js & d3.js, is discussed at length in a precedent post . It consists in leveraging the power of GPU processing. In the current application, pixi.js will nonetheless use d3.js projection function to convert geographic coordinates into pixels unit. On the other hand, d3.js is still used to render the map. Real time statistics, counting the number of vehicles at any given time, are also displayed, as shown in figure 4-c. These numbers are aggregated on the JavaScript side and plotted with d3.js A few words on performances Figure 4a shows a video capture for the vehicle transit across the country (without intra city transit), while 4b focuses on the Zürich inner traffic. The full day can be watched on Youtube ( a and b ). Time is accelerated on the server side, when scheduling the events, while the movie are recorded without any fast forward effect. No lag can be observed, even when approximately 1500 vehicle positions are updated every second in figure 4-a. In this situation, the server component consumes approximately 35% of one CPU core, while the browser JavaScript consumes 30% and the GPU 40%. The measures were done on a 2015 MacBook pro laptop. Even if it makes little sense from the visualization point of view (due to the large scale), city transit was added in figure 4d to push the computations limit even further. 4500 vehicles are then observed, with a slight but bearable lag in the point motions. In this situation, CPU numbers are respectively 40/80/70%. Finally, loading and serving the whole schedule for Switzerland consume 2.5 gigabytes of memory and takes approximately 1 minute to load. Installation For the impatient, a packaged application is ready to be ran, with Java 8. Download the latest zip archive from here and unzip it. From within the extracted directory: #linux/mac\r\nbin/gtfs-simulation-play \r\n\r\n#windows bin/gtfs-simulation-play.bat With large data, such as the full Swiss schedule for one year, it can take up to 2.5 minutes to start on a Macbook pro. Then head to your http://localhost:9000 and enjoy! Head to git to have more details on how to configure the launch, of, for developers, the route to go is to head for git and clone the two application components  ( front and back ), the associated data set and head for the readme files. Conclusions We have seen in this post how to render a large amount of moving data on a? map. We believe the presented technique can be used in other situations and pushes further the frontier of rich dynamic data visualization, mixing d3.js and GPU powered framework. Nonetheless, only a prototype was presented here. Major improvements are yet possible to allow the approach to scale and the footprint on network and CPU to be lowered. The presented scheduling method will hardly scale above a few tens of clients but it could be revisited, or replaced by Kafka once large amount of real time data are to be spread. WebSockets would also allow a more comfortable “dialog” (changing request parameters) and HTTP/2 binary protocol certainly are an interesting promise for lighter and more efficient data exchange. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Big Data . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-02-21"},
{"website": "Octo", "title": "\n                How does it work? Kubernetes: Episode 3 – Infrastructure as code: the tools of the trade            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-kubernetes-episode-3-infrastructure-as-code-the-tools-of-the-trade/", "abstract": "How does it work? Kubernetes: Episode 3 – Infrastructure as code: the tools of the trade Publication date 13/02/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey everybody, TL;DR I hacked something together in order to create a Kubernetes cluster on CoreOS (or Container Linux) using Vagrant and Ansible. If you keep reading, I’m going to talk to you about Kubernetes, etcd, CoreOS, flannel, Calico, Infrastructure as Code and Ansible testing strategies. It’s gonna be super fun. The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts. This is episode 3, regarding Infrastructure as Code, and the tools of the trade. If you want to try it: git clone https://github.com/sebiwi/kubernetes-coreos cd kubernetes-coreos make up This will spin up 4 VMs: an etcd node, a Kubernetes Master node, and two Kubernetes Worker nodes.  You can modify the size of the cluster by hacking on the Vagrantfile and the Ansible inventory. You will need Ansible 2.2, Vagrant , Virtualbox and kubectl . You will also need molecule and docker-py , if you want to run the tests. Okay, I understand the SDN-related issues. Tell me about Infrastructure as Code already! Alright. Whenever I think about automating the creation of a platform or an application using Infrastructure as Code , I think about three different stages: provisioning, configuration and deployment. Provisioning is the process of creating the virtual resources in which your application or platform will run. The complexity of the resources may vary depending on your platform: from simple virtual machines if you’re working locally, to something slightly more elaborate if you’re working on the cloud (network resources, firewalls, and various other services). Configuration is the part in which you configure your virtual machines so that they can behave in a certain way. This stage includes general OS configuration, security hardening, middleware installation, middleware-specific configuration, and so on. Deployment is usually application deployment, or where you put your artefacts in the right place in order to make your applications work on the previously configured resources. Sometimes, a single tool for all these stages will do. Sometimes, it will not. Most of the time I try to keep my code as simple and replaceable as possible (good code is easy to delete, right?).  Don’t get me wrong, I won’t play on hard mode or try to do everything using shell scripts. I’ll just try to keep it simple . But let me talk to you a bit about CoreOS first. Less is more: Enter CoreOS (yeah yeah, Container Linux) Container Linux by CoreOS (formerly known as CoreOS Linux, or just CoreOS) is a lightweight Linux distribution that uses containers to run applications. This is a game changer if you’re used to standard Linux distributions, in which you install packages using a package manager. This thing doesn’t even have a package manager. Now what?! It just ships with the basic GNU Core Utilities so you can move around, and then some tools that will come in handy for our quest. These include Kubelet , Docker , etcd and flannel . I’ll explain how these things work and how I’m going to use them in the whole Kubernetes journey later. Just keep in mind that CoreOS (yeah yeah, Container Linux) is an OS specifically designed to run containers, and that we’re going to take advantage of that in our context. CoreOS as a distribution [ 1 ] So we’re going to manually create these CoreOS virtual machines before installing Kubernetes, right? Well, no, not really. Provisioning: Vagrant to the rescue Vagrant is pretty rad too. It allows you to create reproducible environments based on virtual machines on many different backends, using code. So you just write something called a Vagrantfile, in which you specify all the machines you want and their configuration using Ruby. Then, you type `vagrant up` and all your virtual machines will start popping up. For this we’re using VirtualBox as a provider. There are many others , in case you’re feeling creative. So, once we have all the virtual hosts we need running in our computer, we’re just going to manually configure everything in them, right? Well, no, not really. Configuration and Deployment: Ansible zen You do know Ansible, right? Just so you know, a n ansible is a category of fictional device or technology capable of instantaneous or superluminal communication. The term was first used in Rocannon’s World, a science fiction novel by Ursula K. Le Guin. Oh, it is also an IT automation tool. I really like Ansible because most of the time * , the only thing you need in order to use it is a control machine (which can be the same computer you’re using to code) and SSH access to the target hosts. No complex architectures or master-slave architectures. You can start coding right away! *: This is not one of those cases, but we’ll get to that in the next article. So, we can configure our platform automatically using Ansible. We’re just going to create our machines automatically, configure our resources automatically, and just hope it works, right? Well, no, not really. Test and conquer: Molecule Molecule is a testing tool for Ansible code. It spins up ephemeral infrastructure, it test your roles on the newly created infrastructure, and then it destroys the infrastructure. It also checks for a whole range of other things, like syntax, code quality and impotence, so it’s pretty well adapted for what we’re trying to do here. It’s an actual molecule! There are 3 main Molecule drivers: Docker, OpenStack and Vagrant. I usually use the Docker driver for testing roles, due to the fact that a container is usually lightweight, easy to spin up and destroy, and faster than a virtual machine. The thing is that it’s hard to create a CoreOS container, in order to install Kubernetes to create more containers. Like, I heard you like containers so let me put a container inside your container so you can schedule containers inside containers while you schedule containers inside containers. Besides, there are no CoreOS Docker images as of this moment. Therefore, we’ll be using the Vagrant driver. The target platform just happens to be Vagrant and VirtualBox. Huh. So we’re going to test our code on VirtualBox virtual machines launched by Vagrant, which is exactly the platform we’re using for our project. Great. Yeah, great. Just show me the code already! I could, but I won’t. That’s all for today. I’ll talk to you about the really really fun part in the next article. Stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-02-13"},
{"website": "Octo", "title": "\n                Finovate Europe 2017 : our first impressions            ", "author": ["Adrien Pigeot", "Svetlana Baranov", "Samuel Brunard"], "link": "https://blog.octo.com/en/finovate-europe-2017-our-first-impressions/", "abstract": "Finovate Europe 2017 : our first impressions Publication date 09/02/2017 by Adrien Pigeot , Svetlana Baranov , Samuel Brunard Tweet Share 0 +1 LinkedIn 0 The Finovate Europe 2017 conference has just ended in London after 2 days where software editors, financial institutions and startups met to present their latest innovations. This year again, OCTO consultants were present to identify the latest trends. As we get off the Eurostar, here are our first impressions. Svetlana’s review The demo sessions were a good illustration of all the current trends of the market : A new topic, the RegTech covered through onboarding and KYC ( Hooyu , Signicat ) Biometrics and customer authentification , not mainstream this year but some good demos ( EyePrint ID , Veridium ) Overall, the Security appears as a hot topic ( Token , Vasco ) Strong focus on APIs at many presentations No surprise with the presence of Artificial Intelligence ( Comarch , Backbase ) and a few chatbots ( Dorsum ) Many Big Data use cases ( crowd process , Sentifi , EdgeLab ) Blockchain was missing, with only 2 companies ( Caxton , Finacle) Crealogix offered a demo of a Virtual Reality use case The switch already mentioned last year from FinTech to TechFin is confirmed. So, we are wondering if the demo only format is still suitable for the Finovate show. Again, same trend as last year, with a strong part of software editors at demos. Although, Finovate takes place at London, PSD 2 was in the spotlight. The “PSD2 compliance” was a pitch-lane used in many demos (with even one who promised to be “Brexit” compliant … as soon as we will get some details). Even Yodlee, the US star of PFM proposed its platform for the PSD2 use cases. Through the Finovate sessions we see many clues to the coming switch towards a B2B trend for banking solutions. As a consequence, banks may be more opened to integrate ready-to-use components and benefit from good specialization and implementation. On the overall market we are more and more close to a real move toward modular or platform banking. This year Finovate offered a good coverage of main trends for banking, however we may be disappointed by a “showroom” effect due to the presence of a lot of well known companies and few early-stage FinTech. The cost of  the entrance for demoing, may be dissuasive for small FinTech companies, probably an assumed choice by the event organization. Bonus: A special mention to the Brett King’s show on stage , who presented Moven’s well-known solution and announced the launch of the neobank in the UK. The big surprise was also the return on stage of Finovate dinosaurs: Dynamics Inc. And eToro (with one more best of show award). My 3 favorites : Token for its secure, innovating and easy-to-integrate authentification solution, very relevant according to the stakes of the market HooYu for its KYC management solution with a great user experience OpenTech has demonstrated its implementation of the MasterCard APIs, which illustrates some of the stakes of the PSD2 Samuel’s review We particularly noticed the near absence of the Blockchain in the presentations. Indeed, only two companies have presented their offer, without really convincing on their innovative character. CAXTON , for example, wants to make international payments faster and cheaper. This is probably a prospect that will find application in the not too distant future, but it remains to study more closely the foundations of this offer. Moreover, the realization of this experiment on an “overlay” of the Microsoft Blockchain left us perplexed. My 3 favorites : Memento payments and its social networking interface for making payments without interaction with a banking application W.UP and its behavioral marketing solution Dorsum for its chatbot which allows to choose a credit card offer and the associated monitoring interface Adrien’s review What has particularly marked me is the almost systematic evocation of the theme of onboarding in the presentations. This omnipresence is most certainly explained by the fact that with its ingenious process of onboarding guaranteed in less than 8 minutes, the neobank N26 has something to worry the banks. To create an experience of the same quality, technological levers are numerous: intelligent document capture for the KYC process, biometrics for customer identification, use of APIs of competing establishments (PSD 2) to capture data and evaluate the financial health … Software editors did not hide their pleasure. The other thing that struck me was that, contrary to my expectations, I did not see any genuine disruption . The conference allowed us to see beautiful performances of what we already knew, but not any great discovery that leaves you speechless and makes you want to say “Why had not we thought of it before?”. Nevertheless, it is quite normal as it reflects the fact that Fintech is at a certain level of maturity / productivity. Now that banks and Fintech have understood the value of working together, they are focusing more on process reliability and compliance. My 3 favorites : backbase which presented us an artificial intelligence engine which allows to detect key moments to propose new services to the client via a ChatBot SaleMove for its solution coupling chat, video, voice and cobrowsing to guide the customer in an online process. W.UP and its SalesUp solution to push relevant real-time offerings to customers according to their behavior Tweet Share 0 +1 LinkedIn 0 This entry was posted in Digitalization and tagged bank , banking , digital bank , finovate . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-02-09"},
{"website": "Octo", "title": "\n                How does it work? Kubernetes: Episode 2 – Kubernetes networking            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-kubernetes-episode-2-kubernetes-networking/", "abstract": "How does it work? Kubernetes: Episode 2 – Kubernetes networking Publication date 06/02/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey everybody, TL;DR I hacked something together in order to create a Kubernetes cluster on CoreOS (or Container Linux) using Vagrant and Ansible. If you keep reading, I’m going to talk to you about Kubernetes, etcd, CoreOS, flannel, Calico, Infrastructure as Code and Ansible testing strategies. It’s gonna be super fun. The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts. This is episode 2, regarding Kubernetes networking. If you want to try it: git clone https://github.com/sebiwi/kubernetes-coreos cd kubernetes-coreos make up This will spin up 4 VMs: an etcd node, a Kubernetes Master node, and two Kubernetes Worker nodes.  You can modify the size of the cluster by hacking on the Vagrantfile and the Ansible inventory. You will need Ansible 2.2, Vagrant , Virtualbox and kubectl . You will also need molecule and docker-py , if you want to run the tests. I don’t even understand what the problem is dude Well, as I said in the previous article, communication between pods that are hosted on different machines can be a little bit tricky. By default, Docker creates  a virtual bridge called “ docker0” on the host machine, and it assigns a private network range to it. Super bridge (172.17.0.1/16 in this case) For each container that is created, a virtual ethernet device is attached to this bridge, which is then mapped to eth0 inside the container, with an ip within the aforementioned network range. Note that this will happen for each host that is running Docker, without any coordination between the hosts. Therefore, the network ranges might collide. Because of this, containers will only be able to communicate with containers that are connected to the same virtual bridge. In order to communicate with other containers on other hosts, they must rely on port-mapping. This means that you need to assign a port on the host machine to each container, and then somehow forward all traffic on that port to that container. What if your application needs to advertise its own IP address to a container that is hosted on another node? It doesn’t actually knows its real IP, since his local IP is getting translated into another IP and a port on the host machine. You can automate the port-mapping part somehow, but things start to get kinda complex when following this model. That’s why Kubernetes chose simplicity and skipped the dynamic port-allocation deal. It just assumes that all containers can communicate with each other without Network Address Translation (NAT), that all containers can communicate with each node (and vice-versa), and that the IP that a container sees for itself is the same that the other containers see for it. Aside from being simpler, it also enables applications to be ported rather easily from virtual machines to containers, since they do not have to change the way they work network-wise. There are many different networking options that offer these capabilities for Kubernetes: Contiv , Flannel , Nuage Networks , OpenVSwitch , OVN , Project Calico , Romana and Weave Net . For this project, we will use the combination of two of these options: Calico and Flannel, or Canal . Show me the Canal! Alright. Let’s talk about Flannel and Calico then. Calico’s and flannel (these are the real logos, I’m not just putting random images here) Flannel allows inter-pod communication between different hosts by providing an overlay software-defined network (SDN). This solves the main issue we had the Docker networking model. As I said before, when using Docker, each container has an IP address that allows it to communicate with other containers on the same host .  When pods are placed in different hosts, they rely on their host IP address. Therefore, communication between them is possible by port-mapping. This is fine at a container-level, but applications running inside these containers can have a hard time if they need to advertise their external IP and port to everyone else. Flannel helps by giving each host a different IP subnet range. The Docker daemon will then assign IPs from this range to containers. Then containers can talk to each user using these unique IP addresses by means of packet encapsulation. Imagine that you have two containers, Container A and Container B. Container A is placed on Host Machine A, and Container B is placed on Host Machine B. When Container A wants to talk to Container B, it will use container B’s IP address as the destination address of his packet. This packet will then be encapsulated with an outer UDP packet between Host Machine A and Host Machine B, which will be sent by Host Machine A, and that will have Host Machine B’s IP address as the destination address. Once the packet arrives to Host Machine B, the encapsulation is removed and the packet is routed to the container using the inner IP address. The flannel configuration regarding the container/Host Machine mapping is stored in etcd. The routing is done by a flannel daemon called flanneld. Like this, see? [ 1 ] Calico secures this overlay network, restricting traffic between the pods based on a fine-grained network policy. As I said before, the default Kubernetes behaviour is to allow traffic from all sources inside or outside the cluster to all pods within the cluster. Little reminder from the Kubernetes networking model: all containers can communicate with all other containers without NAT all nodes can communicate with all containers (and vice-versa) without NAT the IP that a container sees itself as is the same IP that others see it as For security and multi-tenancy reasons, it is coherent to restrict communication between sets of pods on the Kubernetes cluster. Calico supports the v1alpha1 network policy  API for Kubernetes. Basically what it does is that it enables network isolation to limit connectivity from an optional set of sources to an optional set of destination TPC/UDP ports. This does not limit the access to the pods by the host itself, as it is necessary for Kubernetes health checks. With that in mind, inter-pod communication can be restricted at a namespace level, or using particular network policies, using selectors to select the concerned nodes. I chose Flannel for the SDN part because it is the standard SDN tool for CoreOS (Container Linux), it is shipped with the distribution, it is rather easy to configure, and the documentation is great. I chose Calico because I wanted to use test policy-based security management on Kubernetes, and because of its tight integration with Flannel. They both rely on etcd, which is rather cool since I’m running an etcd cluster anyways (Calico can be used without an etcd cluster , using the Kubernetes API as a datastore, but this is still experimental). By the way, Calico can be used as a standalone component, that will handle both the SDN and the network policy-based security management. Then again, Flannel has a few additional networking options, such as udp, vxlan, and even AWS VPC route programming (in case you ever need it). Ok, now I get it. So, how did you do it? I think I have to talk to you about the way I see Infrastructure as Code, and explain the tools of the trade first. That’s all for today though. The fun part starts in the next article. Stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 3 commentaires sur “How does it work? Kubernetes: Episode 2 – Kubernetes networking” Gary 18/05/2017 à 15:32 Great comment about the logos! Siva 02/09/2017 à 12:37 Good Narayanan 27/11/2018 à 05:06 Great series. I noticed you mentioned \"This is fine at a container-level, but applications running inside these containers can have a hard time if they need to advertise their external IP and port to everyone else.\". Mostly these days apps talk to each other using service discovery. Could you please mention a scenario where apps will advertise their ips? This will help me to understand better Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-02-06"},
{"website": "Octo", "title": "\n                How does it work? Kubernetes: Episode 1 – Kubernetes general architecture            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/how-does-it-work-kubernetes-episode-1-kubernetes-general-architecture/", "abstract": "How does it work? Kubernetes: Episode 1 – Kubernetes general architecture Publication date 30/01/2017 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey everybody, TL;DR I hacked something together in order to create a Kubernetes cluster on CoreOS (or Container Linux) using Vagrant and Ansible. If you keep reading, I’m going to talk to you about Kubernetes, etcd, CoreOS, flannel, Calico, Infrastructure as Code and Ansible testing strategies. It’s gonna be super fun. The whole subject was way too long for a single article. Therefore, I’ve divided it into 5 parts. This is episode 1, regarding the Kubernetes general architecture. If you want to try it: git clone https://github.com/sebiwi/kubernetes-coreos cd kubernetes-coreos make up This will spin up 4 VMs: an etcd node, a Kubernetes Master node, and two Kubernetes Worker nodes.  You can modify the size of the cluster by hacking on the Vagrantfile and the Ansible inventory. You will need Ansible 2.2, Vagrant , Virtualbox and kubectl . You will also need molecule and docker-py , if you want to run the tests. Why? The last time I worked with Kubernetes was last year . Things have changed since then. A guy I know once said that in order to understand how something complex works, you need to build it up from scratch. I guess that’s one of the main points of this project, really. S cientia potentia est. I also wanted to be able to test different Kubernetes features in a set up reminiscent of a production cluster. Minikube is great and all, but you don’t actually get to see communication between different containers on different hosts, node failover scenarios, scheduling policies, or hardcore scale up procedures (with many nodes). Finally, I thought it would be nice to explain how every Kubernetes component fits together, so everyone can understand what’s under the hood. I keep getting questions like “what is a Kubernetes” at work, and if it is “ better than a Docker” . You won’t find the answer to the last question in this article, but at least you will (hopefully) understand how Kubernetes works. You can make up your own mind afterwards. Okay, I was just passing by, but what is Kubernetes? Kubernetes is a container cluster management tool. I will take a (not so) wild guess and assume that you’ve already heard about Docker and containers in general. The thing is that Docker by itself will probably not suffice when using containers in production. What if your application is composed of multiple containers? You will need to be able to handle not only the creation of these containers, but also the communication between them. What if you feel that putting all your containers on the same host sucks, since if that host goes down, all your containers die with it? You will need to be able to deploy containers on many hosts, and also handle the communication between them, which translates into port mapping hell unless you’re using an SDN solution. What about deploying a new version of your application without service interruption? What about container failure management, are you going to do go check on every container independently to see if it is healthy, and relaunch it manually if it is not? Grrrrrrrraaaaah . Kubernetes is a tool you can use if you do not want to develop something specific in order to handle all the aforementioned issues. It can help you pilot your container cluster, hence its name, which means pilot or helmsman in greek. Just a little heads up before we start talking about architecture: I will often talk about pods during these series. A pod is a group of one or more containers, that run in a shared context. A pod models an application-specific “logical host”, and theoretically it contains one or more applications that are tightly coupled: the kind of applications that you would have executed on the same physical or virtual host before containers. In practice, and for our use-case, you can just think of a pod as a container, since we will only have one container inside each pod. A standard Kubernetes installation consists of both Master and Worker nodes: General Kubernetes Architecture, for real [ 1 ] Wait, where is the Master node? This architecture diagram specifies a set of master components, but not a Master node per-se. You will probably notice that the Distributed Watchable Storage is considered among these components. Nevertheless, we will not install it on the same node. Our Distributed Watchable Storage will be a separate component ( more on this later ). So the Master node actually becomes everything that is inside the red square: Here, see? [ 2 ] All these components are part of the Kubernetes control pane. Keep in mind that you can have these on one single node, but you can also put them on many different ones. In our case we’re putting them all together. So basically, we have an API server, a Scheduler, and a Controller Manager Server. The API server exposes the Kubernetes API (duh). It processes REST operations, and then updates etcd consequently. The scheduler binds the unscheduled pods to a suitable worker node. If none are available, the pod remains unscheduled until a fitting node is found. The controller manager server does all the other cluster-level functions, such as endpoint creation, node discovery, and replication control. Many controllers are embedded into this controller manager, such as the endpoint controller, the node controller and the replication controller. It watches the shared state of the Kubernetes cluster using the API server, and makes changes on it with the intention of making the current state and the desired state of the cluster match. What about the Worker node? The Master node does not run any containers, it just handles and manages the cluster * . The nodes that actually run the containers are the Worker nodes. *: This is not actually true in our case, but we will talk about that later, during episode 5. The Worker node is composed of a kubelet, and a proxy (kube-proxy). You can see these components inside the red square, in the diagram below. Right here [ 3 ] The kubelet is the agent on the worker node that actually starts and stops the pods, and communicates with the Docker engine at a host level. This means it also manages the containers, the images and the associated volumes. It communicates with the API server on the Master node. The kube-proxy redirects traffic directed to specific services and pods to their destination. It communicates with the API server too. And what about that Distributed Watchable Storage thingy? Oh, you mean etcd. This is one of the components that is actually included in Container Linux, and developed by CoreOS. It is a distributed, fault tolerant key-value store used for shared configuration and service discovery. It actually means “something like etc, distributed on many hosts” . This sucks as a name because it is not a filesystem, but a key-value store. They are aware of it though. Just in case you missed it in the previous diagrams, it is the square inside the red square in the following diagram: You never know [ 4 ] All the persistent master state is stocked in etcd. Since components can actually “watch” components, they are able to realise that something has changed rather quickly, and then do something about it. And that’s what I deployed on CoreOS using Ansible. All of these things, and then some more. That’s rad, how did  you do it? You need to understand a little bit about Kubernetes networking before we get to that. That’s all for today though. I’ll talk to you about networking in the next episode. Stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 5 commentaires sur “How does it work? Kubernetes: Episode 1 – Kubernetes general architecture” Siva 02/09/2017 à 09:26 Good article on kubernates architecture Badri 05/04/2018 à 16:27 Super article. This guide gives you a high-level overview of the architecture and moving parts of a Kubernetes setup. Nicole Kristen 06/06/2018 à 13:17 Great Article. Thanks for sharing Information about Kubernetes.  Keep up the good work here! Really very informative and creative contents. These concept is a good way to enhance the knowledge. Here we have some stuff regarding Kuberenetes Architecture i think may helpful to you.\r\nhttps://mindmajix.com/kubernetes-architecture Rob 25/06/2018 à 18:39 You haven't provided links between the episodes of your article. It states at the end, \"No related posts\". Linking them together would be helpful. Harshal Wagh 27/02/2019 à 07:04 Hi, that was really awesome to understand the concept of kubernetes. i m keen to learn more, can you please provide the links for next episodes it ll be really helpful for everyone new to this. i dint see any link in to further articles. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-01-30"},
{"website": "Octo", "title": "\n                BOG’OPS’ challenge : our recipe for Team Spirit            ", "author": ["Eric Fredj", "Yann Rouillard", "Mihail Burov"], "link": "https://blog.octo.com/en/bogops-challenge-our-team-spirit-recipe/", "abstract": "BOG’OPS’ challenge : our recipe for Team Spirit Publication date 16/01/2017 by Eric Fredj , Yann Rouillard , Mihail Burov Tweet Share 0 +1 LinkedIn 0 When I arrived at OCTO Technology in 2014, I immediately loved and embraced its culture of team spirit and sharing and improvements of practices, which are an integral part of OCTO’s DNA. And thus the « I » rapidly turned into a « We ». This culture also lies in the different community teams, a.k.a tribes. Each team organizes itself depending on its size and what they want to work on, to gather regularly and to share. In this article, the OPS tribe will show how 3 of us organized an epic day called “The Bog’Ops” challenge, for over twenty people passionate by Cloud, “ Infrastructure as Code” , automation and, most of all, by challenges. One of the objectives of this event was to organize a friendly get-together while mobilizing in the same place at the same time all those brilliant minds on one single subject with an optimal cohesion effect. And to spice up the whole thing: a competition between 2 teams. Ingredients a big room with the usual devices (projector, speakers and a network connectivity worthy of its name) a comfortable work surface (tables, chairs, and computers) an open-bar account on AWS as a playground a great, motivated team 3 crazy cooks ready for anything Recipe Our major concern was to ensure that nobody could solve the challenge by himself in one day, so we worked on a scenario that would require the implementation of several technical components: a data store layer that would need the implementation of a Ceph cluster a RESTful API a valid certificate recognized by an official CA to expose the API over HTTPS a continuous deployment with Concourse , that we’d been wanting to test for quite a while, using standardized job names exposed via its own API, the whole deployed in the AWS cloud while making the best use of its possibilities to make the application resilient and auto-scalable. We reviewed the knowledge and motivations of everyone to be sure to build 2 teams able to take up that challenge, which will be called: The Cuddling Sea Otter Pups The Laughing Red Panda Cubs Tasting Overview of the menu The challenge’s content was finally revealed to the whole tribe the afternoon before, via an 8-page PDF document containing the instructions and the description of the architecture to implement. Starter 9.00am: everybody is already here With the game scheduled to begin at 9.30 am, the tribe has never been so punctual. At 9.00, everybody was gathered in the room, waiting for the « go ». Considering all the tasks to do, it was to be the start of a full thrilling day. The amazing dashboard filled with red indicator perfectly complemented the challenge’s environment. To strengthen the team spirit, each participant received a customized T-shirt with the team’s name and logo, his trigram and a personal, funny nickname. Main course To meet that challenge, the « Pandas » and the « Otters » were free to organize themselves to create working groups and process the different tasks in parallel: API development, CI installation, certificates implementation, storage infrastructure, AWS infrastructure configuration… Inspired by the DevOps’ culture, each team chose a kanban on the wall, then filled it with a backlog of the tasks to be done and with 1-hour iterations. With 10 people by team and 5 bricks to implement, pairs naturally formed to share the tasks and define the target architecture. Team decisions The Cuddling Sea Otter Pups The Laughing Red Panda Cubs Language Node.js Ruby Certificate Let’s Encrypt AWS Certificate Manager Object Storage Use of S3 while waiting for the Ceph cluster to be ready Use of S3 while waiting for the Ceph cluster to be ready Application packaging Docker Docker 10.00 am: time to work! At the end of each 1-hour iteration, the team would present the progress report : architecture implementation status team organization (who is doing what) a stand up to present the retrospective on the last iteration next actions difficulties encountered and attention points With the first performance test expected at 1 pm, they had less than 3 hours left to develop the application, publish it and deploy it using the CI on AWS. High availability, scalability and Ceph clusters? Let’s leave that for later! Results weren’t long to come: the « Otters » took the lead! During each iteration, the teams were facing several kinds of difficulties. When a pair was blocked, they would ask the rest of the team for some help, and by working together as a team, ideas emerged to solve the problem. The goal of the morning was to bring up the entire infrastructure to make all the dashboard red indicators go green thus validating the proper functioning of implemented architectures. 12.23 pm sharp: a well-deserved break Quick lunch: between the last ones that need to be kicked out of the room and the first ones coming back from lunch, no more than 30 minutes had passed. 12.49 pm: resumption of hostilities The first performance campaign initially scheduled at 1 pm was finally postponed to 2 pm in order to let the teams finish assembling the bricks. The « Otters » still had a slight advantage on the « Pandas ». And the stressful (but not for everyone) performance campaign session could finally begin. The stress scenario was as follows: Ramp-up traffic injection on the platform for 30-40 minutes PUT (to put files) GET (to retrieve files and apply an unpredictable operation on them) will solicit each platform to pressurize it During the performance tests, the organizers trigger the following events: “ unexpected ” VM crashes to disturb the platform and analyze its behavior (cf. design for failure ) during load, application deployments launched through Concours API to simulate an upgrade of the application version upgrade Whereas the « Otters » were leading the race with a slight advantage for a while, the following performance campaign reversed the trend, and it was with goodwill that the « Pandas » were celebrating this event with modesty. 18.00 19.00 pm: end of the game and results The tribe successfully met the challenge: by the end of the day, each team had an entirely working solution with only tiny differences. Dessert With a ROTI of 5 on this day, the objective was reached: the tribe’s team spirit is even stronger than before, each participant was able to work and improve himself thanks to the team, on the subjects we love DevOps’ culture which drives us, has proved itself once again even on such a short term period: c ommunication, tasks and information sharing, mutual help and solidarity, t ests and measures, t eam rituals (kanban, iterative development, stand-up), i nfrastructure as code and tooling: Git, Concourse, Docker, Ceph, AWS with EC2, S3, Route 53, ELB, ECS… On top of that In addition, here are a few verbatims of the tribe who enjoyed the whole day: “ Thank you, it was awesome. Congrats to all the OPS and to the organizers (Eric, Yann and Arnaud) who really rocked ! ” “ Thank you guys it was really great ! Ok to do it again tomorrow ;) Big up to all the bogops !!! ” “ Simply exceptional. Thank you for everything, and congrats to panda team : we kicked some otters’ asses. ” “Thank you to the Otters team who kicked pandas’ asses the whole day, as expected :) ” “A great moment of open friendship and healthy friendly competition ! ” “Great day :) Thanks to the organization team (which was magical and enlightening) and to the player team. ” With a message shared several times and which still echoes: “ It’s awesome to learn while having fun ! ” Next time, come and play with us . And if you behave, we will disclose to you the backstage of the preparation of such a great day. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Infrastructure and Operations and tagged AWS , Ceph , Continuous integration , DevOps , Docker , Infrastructure as code , Team Spirit . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-01-16"},
{"website": "Octo", "title": "\n                A quick summary and some thoughts on the Scikit-learn workshop            ", "author": ["David Luz", "Laurent Ribiere"], "link": "https://blog.octo.com/en/a-quick-summary-and-some-thoughts-on-the-scikit-learn-workshop/", "abstract": "A quick summary and some thoughts on the Scikit-learn workshop Publication date 17/01/2017 by David Luz , Laurent Ribiere Tweet Share 0 +1 LinkedIn 0 On december 2nd was given at Telecom ParisTech the workshop : “Using Scikit-learn and Scientific Python at Scale” with top contributors from the project as speakers. This workshop was divided into four talks : Scikit-learn for industrial applications, basic research and mind reading – Alexandre Gramfort Distributed computing for predictive modeling in Python – Olivier Grisel Scikit-learn at scale : out-of-core methods – Thierry Guillemot An Industrial application at Airbus Group – Vincent Feuillard Scikit-learn is currently the most widely used open source library for Machine Learning applications. It has been developed in Python (Cython and C/C++) and, with over 1000 documentation pages, has become the major contribution for democratizing machine learning for a large audience. A detailed presentation and outline of the talks can be found here . Introduction (by A. Gramfort) : The program focused on the following subjects : a background story about scikit-learn, an explanation of why the project worked, an overview of the methods with out-of-core support available in scikit-learn, new libraries for scaling the development of scikit-learn applications, a business application with an industrial use-case. Scikit-learn for industrial applications, basic research and mind reading (by A. Gramfort) This first talk gave a general introduction and presentation of the sklearn project. The speaker, who is one of the top committers and a major contributor, told us about the beginning of the project and highlighted some of the reasons that made it so successful. Some facts : Start of scikit-learn : official start in 2010 at Université Paris-Saclay (started in 2006 actually at Google’s Summer of Code) 650+ contributors 20K+ commits Funding : INRIA, Paris-Saclay Center for Data Science, Télécom ParisTech, NYU, Google, Criteo. Installed on 1% of Debian systems, 1200 job offers on stack overflow, Usage : 60% academy / 40% industry Biggest python library for machine learning … Scikit-learn was thought to be domain agnostic (with the exception of text vectorization which focuses on text analysis) and designed to be able to perform some highly non-trivial tasks in a few lines of code. Some quotes : “Machine Learning is easy, there is scikit-learn” – Gaël Varoquaux “But making scikit-learn was not easy !” – Anonymous scikit-learn developer The Ingredients of success : Technical reasons nice web site with doc and examples code tests, continuous integration mailing list rules on how to contribute short release cycles version control (use git) Even more important reasons improve upon existing project rather than creating something from scratch clearly defined goal and scope keep bounds on the technical difficulty minimize dependencies focus on not owning the project good choice of license – scikit-learn uses BSD allowing commercial use Social reasons grow a community of contributors git, github – review and give feedback coding sprints with pair programming, code reviews Researchers’ contributions « alone you go fast, together you go far » understanding that good software is crucial to advance research a single API to learn a model : scikit-learn’s simple API is often copied by others (most python machine learning packages, spark mllib) Scaling the development of the scikit-learn ecosystem Fork scikit-learn in smaller projects (such as imbalanced learn , lightning , hdbscan ) facilitate new contributions (best practices are built into the process, for instance a ready-to-use project template is available for new contributors) Examples of scikit-learn on some use cases : The Yhat (company in NYC) Blog : a technical blog that gives an example of churn reduction with scikit-learn Reading minds with sklearn : demo of a brain study showing how it is possible to learn a model to classify functional MRI brain scans using sklearn. At 10 sec intervals, MRI subjects were shown either a house or a human face, and based on the MRI data it was possible to predict what they were seeing at a given instant. The Jupyter notebook is available at http://bit.ly/sklearn_fmri ( http://nbviewer.jupyter.org/urls/dl.dropboxusercontent.com/u/2140486/demo_fmri_sklearn.ipynb ) Our take on this talk : Scikit-learn’s success surely lies on the expertise in machine learning of the team who developed it, but first and foremost because its earliest contributors and founders were good coders convinced that code quality and maintainability were crucial assets for the project. It is also a case study for a hugely successful open source project made on a small budget and with the mindset to serve a community of diverse users and to democratize machine learning. Distributed computing for predictive modeling in Python (by O. Grisel) The slides of this talk can be found here . This talk was given initially at PyData Berlin – 2016. NB : outdated informations on slide 28, you can distribute merge & group by today. This talk began with an introduction questioning the real need for distributing predictive modeling as of today. The speaker based his reflexion on an article (« Big RAM is eating big data » by S. Pafka ) stating that for the most part, datasets size is increasing by 20% year on year on average, but Big EC2 instances RAM size is increasing by 50% y/y. So why do distributed computing at a time when you can do almost anything in memory ? This analysis was tempered by the fact that this study relied on KDnuggets surveys (conducted yearly since 2006) that could possibly be biased, and that some datasets of several petabytes captured in the surveys do actually require the need for distributed computations. The talk then focused on the approach for running predictive models. There are basically two ways : the “fast lane”, with distributed events stream processing for real time applications, and the “slow lane”, based on distributed storage and offline distributed batch processing. There are several alternatives to do this but the speaker focused mainly on the current Spark/Scala/Python paradigm and on Dask as an alternative. PySpark has the limitations of latency, which is induced by network architecture, and that traceback is complex due to the mix of Python and Scala code. There is no pure python mode ! The alternative is to use Dask and distributed. In summary, the paradigm is to wrap the functions in delayed mode (which means a promise that the function will be executed in the future), then pass the delayed objects to the cluster for scheduled computation. This approach has the advantage of lower overhead than the Hadoop/Map Reduce framework. With Dask we can compute the delayed evaluation in parallel (multiple threads on a single machine or multiple threads on multiple Python processes running on several machines) or on a  single machine (single thread sequential code, easier to debug). Our take on this talk : Dask distributed seems to be a promising tool to distribute tasks on a cluster using python, with some interesting advantages over the current PySpark approach. Scikit-learn at scale : out-of-core methods (by T. Guillemot) The definition of out-of-core is what does not fit in RAM. From Wikipedia : “Out-of-core or external memory algorithms are algorithms that are designed to process data that is too large to fit into a computer’s main memory at one time. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory ( auxiliary memory ) such as hard drives or tape drives.” What are the strategies to scale Scikit-learn computationally? The speaker presented examples of incremental learning : too many samples ? → Use mini batch, but for some algorithms the final result is not exactly the same (as with the classical algorithm) too many descriptors ? → Use dimensionality reduction techniques Scikit-learn proposes several methods to solve out-of-core problems (basically classes with a ‘ partial_fit’ method). Rather than call ‘ fit’ , you must call ‘ partial_fit’ . Some algorithms recently added to the sklearn library using the partial fit method : Classification : MultinomialNB BernoulliNB Perceptron SGDClassifier PassiveAgressiveClassifier Neural nets (part of last release) Clustering : MinibatchKMeans Other : IncrementalPCA This link will give you all available out-of-core method of scikit-learn. Scikit-learn proposes some tools that can be useful to deal with these problems : “ Hashing Vectorizer” for text analysis “ RBFSampler” to apply a kernel approximation For more information about out-of-core problem in scikit-learn, you can read this article . For more information about feature extraction, you can read the related documentation . A notebook about large scale text classification can be found on that page . Our take on this talk : Some new algorithms are now available that use incremental learning for training models with large datasets. An Industrial application at Airbus Group (by V. Feuillard) This talk was given by Vincent Feuillard, R&D engineer in applied mathematics at Airbus Group Innovation. Unlike the other talks, this one was not especially focused on how one can go to production using sklearn. It was more of a storytelling on how the team could setup a prototype on a specific use case : predictive (condition based) maintenance using several signals from the airplane engine’s Auxiliary Power Unit (APU). Before the prototype maintenance was based on engine health indicators defined by the expert engineers. The idea behind the prototype has been to approach the problem from a machine learning perspective and to validate the results by experts from the AiRTHM team. The python stack they used for this project was : for data munging : pandas for dataviz : matplotlib, bokeh for machine learning : scipy, sklearn In contrast with R, Python Scikit-learn is best suited and easier to use when prototyping from the beginning to the very end of the pipeline, because it is better maintained, more stable and has a clear API. The main lesson learned is that feature engineering is the most important step when doing anomaly detection with functional data. Our take on this talk : The POC presented is a nice application of the Python/sklearn stack to an industrial business case. The speaker highlighted the multidisciplinary teamwork and the Agile organization of the project, with direct transfer of R&T development to Airbus operational support. Conclusion The workshop wrapped up with a comment from A. Gramfort who explained that the project has grown to a size that is hard to maintain at the moment. The funding of scikit-learn development requires about 300-400 Keur/year, and so far this has been provided mainly by public funds, but this situation is not sustainable. Hence the founders are looking for alternative solutions. Since many companies, ranging from startups to established industrial players are currently prototyping with scikit-learn, and some contractors are willing to fund its development, there are prospects to create an entity that could accept and manage donations as done by the Wikipedia Foundation. We warmly thank the speakers for their help and input with reviewing this article. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data and tagged machine learning , scikit-learn . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-01-17"},
{"website": "Octo", "title": "\n                “pet vs. cattle”, from server craftsman to software craftsman            ", "author": ["Arnaud Mazin", "David Alia"], "link": "https://blog.octo.com/en/pet-vs-cattle-from-server-craftsman-to-software-craftsman/", "abstract": "“pet vs. cattle”, from server craftsman to software craftsman Publication date 06/01/2017 by Arnaud Mazin , David Alia Tweet Share 0 +1 LinkedIn 0 The evolution of Ops follows a path that we observe regularly in our interventions. It is through this fable that we will see the 4 stages that mark this path paved with pitfalls. Let’s see how an Ops proceeds concretely to carry out the operation “ fix_mysql ” which consists in changing the configuration of MySQL on production servers. The first age : the server craftsman Many system administrators have an almost affective relationship with their machines. Under the name “machines”, you can alternatively talk about physical servers, virtual machines (VMs), storage, network equipment. It is comforting to know one’s machines one by one, to do a tour for each of them in the morning (ssh, consultation of the munin dashboard machine by machine) as a doctor does the tour of the rooms in a hospital. When a machine falls ill, we are at its bedside, we practice heroic care, we meet in committee to examine the treatment to be prescribed. In short, we make a fuss of it. Violette, queen of the herd This phenomenon we observe here has a name, we call it the “Pet” approach. You feel like you are dealing with pets you call by their name, who you pamper and spoil, to whom you put a little knot on the head to prevent his hair from falling back on his eyes. You take care of your machines with all the professionalism, empathy and humanity which you are capable of. It’s what makes you feel good, the concern of job well done, the attention to detail so that the machine “feels good” and that its users obtain the service they expect. You are a craftsman whose masterpieces are servers and your tools: a terminal, ssh, vi, tmux or screen , some scripts and configuration files with the greatest of care. Organisations that are centered on this behavior are detectable in their propensity to personify the machines, to set up meetings whose objective is to clean up the server “daniela”, to carry out the version upgrade of the DBMS of acme-dc1 -lnx-db001-prd, to plan a weekend for the upgrade of the machine kernel of site B. A whole poem. Samples of commands for fix_mysql , first age version: # Connecting to the first server $ ssh amz@acme-dc1-lnx-db001-prd # Checking ongoing processes amz @ acme-dc1-lnx-db001-prd ~ > ps # Checking disk space amz @ acme-dc1-lnx-db001-prd ~ > df -h # Editing mysql configuration file amz @ acme-dc1-lnx-db001-prd ~ > sudo vim /etc/mysql/my.cnf # Restarting the mysql service amz @ acme-dc1-lnx-db001-prd ~ > sudo systemctl restart mysqld # Checking logs amz @ acme-dc1-lnx-db001-prd ~ > sudo tail -F /var/log/mysql.log # Disconnecting from the first server amz @ acme-dc1-lnx-db001-prd ~ > logout # Connecting to the second server... $ ssh amz@acme-dc1-lnx-db002-prd amz @ acme-dc1-lnx-db002-prd ~ > sudo vim /etc/mysql/my.cnf amz @ acme-dc1-lnx-db002-prd ~ > sudo systemctl restart mysqld amz @ acme-dc1-lnx-db002-prd ~ > sudo tail -F /var/log/mysql.log amz @ acme-dc1-lnx-db002-prd ~ > logout # Connecting to the third server (still 17 to go)... $ ssh amz@acme-dc1-lnx-db003-prd amz @ acme-dc1-lnx-db003-prd ~ > sudo vim /etc/mysql/my.cnf amz @ acme-dc1-lnx-db002-prd ~ > sudo systemctl restart mysqld amz @ acme-dc1-lnx-db003-prd ~ > sudo tail -F /var/log/mysql.log amz @ acme-dc1-lnx-db003-prd ~ > logout # a long time later... Behind these cabalistic signs, a very manual, laborious and repetitive work on each of the servers is hiding. In many steps errors can slip and lead to dysfunctional and / or heterogeneous environments. First age : comfort level Why it works My servers don’t need to be frequently upgraded A few applications to manage A few deployments Small number of machines Why it does not work New versions of applications must be deployed more than once a month I manage more than X machines I manage more than Y applications I go on holidays or I fell sick The second age : virtualisation rocks the boat With the advent of virtualization, the situation changes. We tend to use many VMs. Heaps. So that knowing them all by their name becomes almost impossible. So how do you maintain an ability to deal with so many machines with the same love and dedication? In addition, VMs are managed as physical servers (created at the beginning of the project, deleted at the end), cleaning phases are not frequent. And we must admit it, removal of VMs is clearly not a priority. We will do this when we have time or when the hypervisor is full. The “Pet” approach is jeopardised. Systematic tasks performed by hundreds appear meaningless, in addition to being frankly tedious. You can not hold such a rhythm durably without risking a complete overheat. You then launch various actions: creation of snapshots / clones to duplicate the machines, you only swear by your golden-image . For settings specific to each machine, it’s still hell. Samples of commands for fix_mysql , second age version: # Nice shell loop $ machines=”001:16M 002:32M 003:16M 004:32M 005:32M ...” # Action on all servers in one pass, I have tested on a server, it seems to work... $ for m in $machines; do\r\n  IFS=: read machine_number mem <<< $m\r\n  ssh -tt amz@acme-dc1-lnx-db$machine_number-prd -- “sudo sed -ie 's/.*key_buffer_size.*/key_buffer_size=$mem/' /etc/mysql/my.cnf && sudo systemctl restart mysqld”\r\ndone In this example, you start writing automation code to switch to all machines. Little control, high risk of mistakes, you like to live dangerously … But after all, your scripts and your one-liners, you know them by heart, no matter if nobody understands anything. Second age : comfort level Why it works My images ensure that what worked yesterday will continue to work tomorrow Why it does not work Sometimes I forget to backport the patch X from prod to an image. There are so many images that we always end up taking the last one, and we hope that it is the most up to date. Deploying without downtime is hard: my pictures take a long time to boot, you have to keep a close eye on them, and I have to update the loadbalancers configuration manually. My shells are sometimes a bit unstable and complicated to maintain The third age : infrastructure as code to the rescue Automation is now a must. Following the misunderstanding of your colleagues to understand your home scripts, you start working with a tool, a real one. There are now so many automation tools that it may even be difficult to choose, but that is not the question. Thanks to them, facilities / configurations / deployments are greatly facilitated, on a very large scale. You feel ineluctably slipping into the “Cattle” approach. This industrial and bestial management of heaps of machines, all identical, without personality gives you more the impression of managing a farm of 1 000 cows than a small flock of 20 ewes on the foothills of this small village in the Australian bush. There are some teams that go as far as regularly destroying and rebuilding VMs, just for the sake of verifying that it is possible and that it would even allow to validate a DRP strategy. Perfectly scandalous. A machine is made to last. And what about a practice as wild as inhuman as the blue-green deployment ? This involves performing version upgrades by creating VMs with the new version of the application, making a toggle on the load-balancer before destroying the old VMs. Are you kidding? We have to face the obvious. This approach is devilishly effective. Manual and tedious tasks are reduced, part of the slips due to heterogeneous configurations disappear. Some time is saved and it is possible to easily manage hundreds or even thousands of machines. The automaton becomes your armed wing, it is he who, by construction, laboriously performs all the repetitive operations that you have coded. A small snag however, when you ask him to do a stupid thing, he does it with obedience, on 1000 machines at a time … Then comes the moment of questioning  the meaning of your action: Am I doing a least good job? Are my scripts, cookbooks, playbooks, modules and roles slowly replacing me? How to continue practicing my craft and my love of job well done in such a context where some VM live only a few hours or even minutes? Have I become insensitive to all the maltreatment of these defenseless VMs? These questions begin to accumulate in your head. You also become the owner of a small asset of code that describes your infrastructure and which begins to accumulate discreetly … fix_mysql , third age version: # Editing Ansible template file for MySQL configuration $ vim roles/mysql/templates/my.cnf # Editing Ansible MySQL role $ vim roles/mysql/tasks/main.yml # Launch in dry-run mode to list modifications to apply on all database servers in production $ ansible-playbook -i inv/prod -l db-servers configure.yml --check --diff # Real launch on the platform on all database servers in production $ ansible-playbook -i inv/prod -l db-servers configure.yml --diff -vvv # Ouch, after a manual check, we need to pass on all servers to fix our mistake... $ vim roles/mysql/templates/my.cnf # Launch on the platform on all database servers in production $ ansible-playbook -i inv/prod -l db-servers configure.yml --diff -vvv # Phew, it's fixed, no one has seen it... Dans cet exemple, un outil (Ansible, un choix parmi tant d’autres) permet de déployer un changement de configuration sur tous les serveurs de base de données. Comme il n’y a pas de garde-fou, si on fait une coquille, celle-ci se déploie sagement sur toutes les machines concernées. Reste alors à vite repasser derrière pour réparer… In this example, a tool (Ansible, a choice among many others) allows to deploy a configuration change on all database servers. Since there is no guardrail, if you make a mistake, it spreads out wisely on all the machines concerned. And you have to quickly pass over all of them to repair … Third age : comfort level Why it works As with the golden images: what worked yesterday will continue to function tomorrow. A rolling update is equivalent to setting values in my code. The code represents “the truth” of what is supposed to happen in production, everyone can refer to it to make a decision. Why it does not work I do not write tests: I have code but I do not know if it works at a given moment. I do not trust the idempotence of my code and therefore I can not apply it in production to fix it. I modify the machines behind my automaton’s back manually and this breaks everything when it runs again The fourth age : the software craftsman Concernant la gestion du code que vous donnez à votre automate, une transformation s’opère et elle pourrait être la voie du salut. En allant voir ce qu’il se fait du côté des développeurs d’applications, vous vous rendez compte que c’est un nouveau monde parfois mal-connu qui s’ouvre à vous. Regarding the code management you feed your automaton, a transformation takes place and it could be the way of salvation. It is done by looking at what application developers do, you realize that it is a new world, sometimes not well known, that opens up to you. Practices to improve code: code review, peer-programming Code versioned in a tool like Git which allows not only to store and to version its code, but also to share it and to ensure its traceability. Besides, where your code was only visible by some hand-picked sysadmins, you deign to set read-only rights to everyone. Passwords are not stored there, there is no risk. It might even give ideas to others, or even better, explain what you’re doing on the platforms. A little more visibility can not hurt. Using tools to write readable code: linter to check its syntax and compliance with writing standards. It’s always better if everyone writes code the same way. And especially this annoying mania of systematically writing and automatically (and continuously) executing tests to catch all errors (present or future regressions) that could get hidden in your work. A quick tour on the web shows that there is a plethora of these tools to test its infrastructures ( bats , serverspec , test-kitchen …). Developers have implemented a whole ecosystem of tools and practices to do their job. By talking to them, you realize that they have done this because they aspire to something that resonates deliciously within you. They like to do quality work, make their code beautiful, expressive, maintainable, in short, treat it with the greatest care… They even have a term for it: software craftsmanship . Fourth age version of fix_mysql : # Switching on master branch to checkout the latest code version $ git checkout master # Updating with the main repository $ git pull --rebase # Creating branch for the fix $ git checkout -b amz # Recreating a dev platform from scratch $ ansible-playbook -i inv/amz vm-reinit.yml # Launch on a dev platform $ ansible-playbook -i inv/amz -l db-servers configure.yml --diff -vvv # Editing the MySQL serverspec test to check the new expected behavior $ vim tests/spec/nodetypes/db/mysql_spec.rb # Checking the test has not passed $ ENV=amz rake -f tests/Rakefile spec:mysql # Editing the ansible template file for MySQL configuration $ vim roles/mysql/templates/my.cnf.j2 # Editing the ansible MySQL role $ vim roles/mysql/tasks/main.yml # Checking the ansible MySQL role syntax $ ansible-lint roles/mysql [ANSIBLE0012] Commands should not change things if nothing needs doing /home/amz/projets/trucs/infra-as-code/roles/mysql/tasks/main.yml : 17 Task/Handler: Ch{mod,own} file # Ouch, I've made a mistake, ansible-lint has caught it before I launch the command $ vim roles/mysql/tasks/main.yml # Checking the ansible MySQL role $ ansible-lint roles/mysql # This time it works, launching on a dev platform in dry-run mode $ ansible-playbook -i inv/amz -l db-servers configure.yml --check --diff # Launching on a dev platform $ ansible-playbook -i inv/amz -l db-servers configure.yml --diff -vvv # Launching tests that should all pass $ ENV=amz rake -f tests/Rakefile spec:mysql # Checking modified files $ git status # Adding modified files to the next Git commit $ git add tests/spec/nodetypes/db/mysql_spec.rb roles/mysql/templates/my.cnf.j2 roles/mysql/tasks/main.yml # Git commit with item reference enclosed $ git commit -m “#678 ajout de la gestion des key_buffer_size” # Git push in the branch to ask for a merge-request + pair review # The CI/CD platform will deal with it automatically $ git push origin amz # Purging the temporary dev platform $ ansible-playbook -i inv/amz vm-destroy.yml In this example, the Ansible code is supported by a code best practices analyzer ( ansible-lint ) and tests (in serverspec ) that are written (if possible) before the implementation. The ability to have disposable environments on demand (via test-kitchen or any other solution) makes it possible to validate the changes on an ante-prod environment. The branch and code-review strategy contributes to the overall quality and to the share of the code repository. This is especially the occasion to have the code and the parameterization read again to a newbie on the project or a DBA who will check the settings. A continuous integration platform re-runs all the tests and a promotion mechanism (manual or automatic) rolls out the changes into production. Fourth age : comfort level Why it works There are several testing levels that contribute to the code quality. I make the most of the versioning provided by the SCM to version my whole infrastructure. Why it does not work Stop it now, it works. Conclusion : from craftsman to craftsman, the circle is complete Therefore,for the sysadmins / devops, the deployment tool becomes a kind of shepherd dog. It is ultimately the one who becomes the new pet, the one you really need and take the greatest care of. The one whose every line of code is written with love, with all the possible quality since he is the one who allows you to manipulate such a park of servers and applications. The transformation of the profession of ops is ultimately not a questioning of the intrinsic values of it. It is rather a change in the object they focus on. Instead of becoming attached to machines, it is now about becoming attached to the automaton (and to the code that guides it) which keeps them alive, while retaining the same concern for a job well done, this flavor that give us the pleasure to rise every morning. What about tomorrow? You are proud of your work, yet new challenges are coming ahead: cloud (IaaS, auto-scaling, PaaS), containerization (Swarm, Kubernetes …), Application clusters (MongoDB, Cassandra, Kafka, Spark … .)… The difference? Even shorter containers lifetime, a cluster mechanism that participates in the livestock life. Finally, no matter, you should be ready: you have implemented all the good practices preparing you for changes (tools and technologies). Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations and tagged Ansible , craftsmanship , DevOps , Infrastructure , Ops . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse", "date": "2017-01-06"},
{"website": "Octo", "title": "\n                D3.js transitions killed my CPU! A d3.js & pixi.js comparison            ", "author": ["Alexandre Masselot"], "link": "https://blog.octo.com/en/d3-js-transitions-killed-my-cpu-a-d3-js-pixi-js-comparison/", "abstract": "D3.js transitions killed my CPU! A d3.js & pixi.js comparison Publication date 05/01/2017 by Alexandre Masselot Tweet Share 0 +1 LinkedIn 0 D3.js certainly is the most versatile JavaScript data rendering library available: turning data into mind blowing visualizations is only limited by your imagination. A key component to turn static pages into animated ones are the powerful selection transitions . However, too many simultaneous transitions on a web page will soon bring you CPU on its knees. Hence this blog post. We faced this problem when displaying swiss transport real time data on a map, within an SVG layout: rendering was lagging, event sourced data were not consumed consistently and laptop batteries were drowning at a dramatic speed. A video from a first attempt can be seen, and compared to a newer implementation with the technique presented in this article. Another surprise came from rendering a simple clock , burning 20% of CPU with a single transition. If d3.js has no serious concurrents for many rendering problems, we decided to give try to a JavaScript library used for building games and leveraging the strengths of HTML5 and GPU: pixi.js . At first, we will propose in this post a comparison between the two libraries in terms of rendering performance. For the sake of completeness, we will also discuss native CSS transitions. We will then dive into a couple of tricks to enhance dynamic visualizations with each of the two libraries and will even combine them to get the best of both worlds. The project source code with benchmark data are hosted on github and a demo is available on github.io . Experimental Setup To simulate a simple system with many transitions, we built a model with three parameters. n particles are randomly scattered in  a 2D box. Every dt milliseconds, a new random relative position is attributed for each particle. Particles cannot exit the box and the average leap length for the next move is the third model parameter. The transition type we will focus in this experiment are linear ones, but the same phenomenon can be observe with others (rotations, opacity, scaling etc.) The project source code makes a vanilla use of Angular 2 and ngrx/store to implement a flux pattern. A store handles the particles state at each dt interval and the rendering components subscribe to an observable to react upon changes. The transitions themselves are managed by the rendering components. Measures are mainly done on Chrome v55, running on a Macbook pro (2015, 2.5Ghz i7, 16G RAM). Several metrics were captured: CPU : via task manager, allowing tab centric monitoring; GPU : via task manager “GPU process”; number of frames per second (FPS) : via the developer console rendering tab; cycle time : if new particle positions are set by the store every 1000 milliseconds, we also want to measure the time between two actual rendering iterations. In a perfect world, it should be 1000ms + epsilon, epsilon being the time for computing those new positions and setting up the transition mechanism. This measure is captured via console log, where the current time minus the one at the previous iteration is displayed. Transition implementations As most of the implementation is a straightforward Angular 2 + ngrx/store use, we will  focus only on the transition part. D3.js [ code ] [ simulation ] D3.js core strength is to tie a data structure to DOM elements and opens the doors for rich visualization. In our case, an array of particles are associated with circles within an SVG element. The association between the x and y particle properties and the cx and cy circle attributes is achieved via an update pattern : self.svg.selectAll('circle')\r\n      .attr('cx', function (b) {\r\n        return self.width * b.x;\r\n      })\r\n      .attr('cy', function (b) {\r\n        return self.height * b.y;\r\n      }); One of the great “magic” of d3.js is the ability not only to modify a DOM element attribute at once, but to make it transition from its current value to the new one via  continuous transitions . This mechanism is set in the following example by three calls: self.svg.selectAll('circle') .transition() .duration(1000) .ease(d3.easeLinear) .attr('cx', function (b) { return self.width * b.x; }) .attr('cy', function (b) { return self.height * b.y; }); Et voilà! Pixi.js [ code ] [ simulation ] Pixi.js is a library to render 2D animation. It benefits from HTML5 features and leverages the power of the WebGL rendering engine. The library success comes from the proposed abstraction above the classic low level implementation details. Even though its “market share” is more turned toward online games, we will demonstrate here how it can also play a role in data visualization. Pixi.js documentation is rather comprehensive, so we will not cover here the full implementation story. Each particle is tied to a Graphics circle object. The original and target positions are stored and the actual coordinates are updated via a requestAnimationFrame call. const tFrom = new Date().getTime(); const animate = function () { const tRatio = (new Date().getTime() - tFrom) / 1000; if(tRatio>1){ return; } for (let i = 0; i < n; i++) { let g = self.graphics[i]; g.x = g.xFrom + tRatio * (g.xTo - g.xFrom); g.y = g.yFrom + tRatio * (g.yTo - g.yFrom); } self.renderer.render(self.stage); requestAnimationFrame(animate); }; animate(); Results Benchmark data was collected as described in the experimental setup section and processed with R. The analysis notebook code (knitr) is available [ here ]. CPU and GPU Figure 1: CPU & GPU measured usage are displayed versus the number of particles in the box. Both rendering engines are compared. Figure 1 shows CPU & GPU consumptions, depending on the number of particles moving across the screen. Those numbers confirm the intuition based on CPU monitoring (or simply feeling the laptop fan striving): as soon a a few hundred particles are transitioned simultaneously, CPU peak is reached. Moreover, we can confirm that D3 transitions do not use any GPU power to slim the process. On the contrary, Pixi, using HTML5, hits the CPU much lighter and leverages GPU to gain efficiency. With this later approach, up to 10’000 particles can be moved comfortably. It can also be noted that even a single particle being transitioned already consume 16% of CPU! Of course, CPU is flat when no particles are displayed). Frame per seconds & cycle time Figure 2: the number of frames rendered per seconds, as indicated by the Chrome developer console. If 60 is the peak value, lagging is perceived with rates below 20 FPS. Another intuition is confirmed: when more are present, the particle movements starts to be jerky. The number of frames rendered per second diminishes much sooner with d3.js than with Pixi. Even with more than 10’000 particles, Pixi sustains a rate greater than 20 frames per second, hence a comfortable experience for visualization. Figure 3: the effective time between two loop is displayed as a box plot, as several measures were collected for each situations. A “costless” situation prduces cycle times at 1000 ms. We can see the measures diverging as soon as the number of particle reaches 1000, with a clear disavantage to D3. Finally, looking at the cycle time nails the point, as shown in figure 3. As the CPU is harassed by transitions, it cannot be used as efficiently to address the other tasks, such as computing the next particle positions (or in real life, all the other tasks to be taken care of). This induces lags between each simulation steps and therefore less smooth animations. This phenomenon is of course less intrusive with Pixi. Enlarging the scope Other measures are not presented here. We made simulations to see if semi transparent objects would impact performances, but the alpha factor proved to have no effect. Looking at other transition types ( e.g. rotations, scaling) did not raise contradictory effects neither. For the sake of the exploration, we also considered using Pixi without enabling the WEBGL, and falling on a canvas renderer. Results are not displayed here (thus they are available on github ), but this third option falls as an intermediary between the two solutions proposed in this post. To broaden the browser range, an attempt to benchmark Firefox (v49) was done, but the lack of comfortable tooling did prevent us to pursue this goal. However, approximative measures gave the same kind of outcome as with Chrome. Wait! Why didn’t you use native CSS transitions? [ code ] [ simulation ] Among the first comments to this post was the question: “d3.js does not use native CSS transitions. Wouldn’t they be more appropriate for your problem?” CSS indeed proposes native transitions , that can, in some conditions leverage the GPU. We use d3 to set the CSS transitions, as shown in the code below . A check has be done with only setting the translation with CSS + D3, without transition effect: CPU levels is neglectable, proving the lack of an eventual overhead by D3. self.svg.selectAll('circle') .data(self.particleSet.particles, function (b) { return b.id; }) .enter() .append('circle') .attr('r', 2) .attr('cx', 0) .attr('cy', 0) .style('transition-timing-function','linear') .style('transition-duration', '1000ms') self.svg.selectAll('circle') .style( 'transform' , function(b){ return 'translate(' +(self.width * b.x) + 'px, ' +(self.height * b.y) + 'px)'; }) Unfortunately, if transitions can be described in CSS for SVG, figure 4 shows that performance are even poorer than those with D3. But what about the claim of CSS transitions using the GPU? We made another experiment, with particles displayed as <div> moving on the screen [ code ] [ simulation ]. In this situation, GPU is effectively triggered, with better performance although lower than Pixi’s. However, the alternative of stepping out from the SVG container lays outside of the original problem at hand. Figure 4: a duplicate from figure 1, with native SVG CSS transitions. They show to be slightly worse than D3 selection transitions. More & better rendering Limiting d3.js transitions [ code ] [ simulation ] We have seen how with D3, the CPU is loaded by selection transitions. In practical situations such as simulating public transport vehicles, some moves may be shorter than others. If the leap is below 2 pixels of distance per time increment, a continuous transition is not necessary and may be replaced by an atomic jump without too much visual impediment. From the implementation point of view, it is a matter of filtering particles with jumps smaller or larger than 2 pixels and applying either an instantaneous modification or calling for a transition. //add a delta property as the leap distance self.svg.selectAll('circle') .each(function (b) { b.delta = Math.max(Math.abs(b.xFrom - b.x) * self.width, Math.abs(b.yFrom - b.y) * self.height); }); //a direct jump is applied if the distance is lower than 2 self.svg.selectAll('circle') .filter(function (b) { return b.delta <= 2 ; }) .attr('cx', function (b) { return self.width * b.x; }); //and so with 'cy' // a linear transition is triggered in other situations self.svg.selectAll('circle') .filter(function (b) { return b.delta > 2 ; }) .transition() .duration(1000) .ease(d3.easeLinear) .attr('cx', function (b) { return self.width * b.x; }); //and so with 'cy' The impact on CPU is directly linked to the number of objects filtered out from the transition loop. Adding tail effect with Pixi.js [ code ] [ simulation ] Pixi let us step into the gaming world, where transitions effect can be enhanced. A classic way to do so is to add a fading out tail to each particle. Pixi allows to render a scene to the window, but also to a background texture. One trick is to render the latest stage to the screen, then add  the full stage to a buffered texture by adding a light opacity effect. self.stage.addChild(self.outputSprite); //and within the animation loop const temp = self.renderTexture; self.renderTexture = self.renderTexture2; self.renderTexture2 = temp; self.outputSprite.texture = self.renderTexture; self.renderer.render(self.stage); self.stage.alpha=0.8; self.renderer.render(self.stage, self.renderTexture2); //turn the stage opacity back to 100% self.stage.alpha=1; Such a texture superposition has been measured to have little overall performance impact. Combining Pixi & D3: the best of both worlds [ code ] [ simulation ] For massive transitions, Pixi has clearly proven its value. But it can hardly replace the power and versatility of D3 for other types of complex visualizations. For the sake of the example, we proposed to add a circle below the Pixi scene. The center shows the average particle positions while the radius shows the standard deviation of the distance to this center. Therefore, the circle attributes are also to be recomputed at every time increment. The method proposed in the previous section ensures the stage background to be transparent (to the contrary of other Pixi tailing effect methods proposed on the web). We simply used two <div> elements  to superimpose a D3 and a Pixi layers. <div> <div class=\"container\"> <div class=\"layer\"> <app-particle-stats> </app-particle-stats> </div> <div class=\"layer\"> <app-particlebox-pixi-tail> </app-particlebox-pixi-tail> </div> </div> </div> div.container{ position: relative; } div.layer{ position: absolute; top: 0; left: 0; } But everything is not only about speed Even though we have focused on performance issues, we shall not close this discussion without mentioning a few points worth remembering when comparing the two libraries: d3.js extends far beyond moving a few circles across an SVG box. Its real power comes with the possibility to interact with any element, CSS and the vast toolbox made available. Pixi.js offers other abstractions to manipulate a lot of similar objects ( SpriteBatch et ParticleContainer ). These tools are particularly well suited to render a large fixed number of images. Another advantage of Pixi is to asynchronously add or remove elements to the stage. Although it is not demonstrated in the current example, we can use this pattern to tie objects to a data stream (think of Server Side Events). No easy solution seems to exists to throttle d3 transitions (even though mist.io proposed an alternative) . It is however rather straightforward to implement such a throttle in a Pixi animation loop. The focus on massive transitions in this post shall not make us that there are more than one way to do animation. Head for Bourbon.io and snap.svg to rock ! Conclusions No doubt Pixi is more efficient to render a lot of transitions! As soon as the number of objects reaches a few hundreds, aiming to such a technology should seriously be considered. Even though the Pixi coding part is not as straightforward as D3, it leverages the power of the GPU cards, available on any modern computers. But the versatility of D3 to render the vast majority of other graph types makes it hard to forget (think of drawing maps with topojson or any plot indicators). The good news therefore comes by the easiness to combine both approaches. D3 scales can even be used from inside Pixi code to benefit from a coherent domain to screen coordinates system. Coming soon… Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Big Data , General -- DO NOT USE and tagged CSS , d3.js , JavaScript . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 6 commentaires sur “D3.js transitions killed my CPU! A d3.js & pixi.js comparison” Sebastien 05/01/2017 à 11:36 Hello,\r\nthanks for the detail article but some points are not clear to me.\r\nFirst, why adding such an heavy thing as Angular 2 to compare two full front-end redering solutions? I can get it that it'll be more representative of a real life application but the objective is to compare CPU / GPU usage. Knowing that 3D.js uses DOM objects and move them, it may also implies Angular to consume CPU due to update states which might not the case with Pixi using canvas (maybe using Vuejs as databinding lib would reduce this overhead given latest comparaisons between them).\r\nFor the CSS transform section, maybe you should apply \"will-change: transform;\" style as well but it may not be usable if applied to too many objects.\r\nIn the end, I also agree that not only this particular point has to be considered but the global need as 3D.js is more complete Alexandre Masselot 12/01/2017 à 08:05 Hi Sebastien,\r\nWe sorted out Angular2 eventual price by measuring CPU load with discrete transitions (particle are just jumped to the next position without smooth transitions). In those cases, the CPU consumption was proven neglectable. Handling 10'000 particles positions consume around 1% CPU.\r\n\r\nThat said, not being a big fan of Angular 1.x for massive data display (and more attracted by React), I was surprised by the the framework.\r\n\r\nNot having tried Vue (which looks promizing), I'd love to see more on the comparison.\r\n\r\nAlex Fred 23/01/2017 à 08:26 Nice project. Did you extract the data from a GTFS source ? Alexandre Masselot 10/02/2017 à 09:00 Hei Fred,\r\na post with all the details on GTFS extraction, scheduling and streaming is almost ready to be shipped. We're nailing down the last details.\r\nAll this will be presented at VoxxedZurich in a couple of weeks\r\n\r\nAlex Antonio 30/09/2018 à 21:46 For what I know (I'm not an expert), Pixi uses Canvas while D3 uses svg in your examples. I suspect that the difference in the tools conditions the difference in the results.\r\nA more proper comparison should be canvas animation in D3 vs canvas animation in PIxi. Alexandre Masselot 30/10/2018 à 07:44 @Antonio. You are perfectly right (and that is one of the topics of the article). We raise how the apparent simplicity of CSS transition performance can cost and how the use of canvas (through Pixi) offers a solution. We could have used D3.js on canvas, even though it lies a bit outside of its natural scope. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-01-05"},
{"website": "Octo", "title": "\n                An IoT application using IoT framework? Here it is            ", "author": ["Can Liu"], "link": "https://blog.octo.com/en/an-iot-application-using-iot-framework-here-it-is/", "abstract": "An IoT application using IoT framework? Here it is Publication date 16/01/2017 by Can Liu Tweet Share 0 +1 LinkedIn 0 According to the market researchers at IDC , there were 9.1 billions Internet of Things (IoT) devices installed at the end of 2013. They expect that number will increase stably and will reach 28.1 billions in 2020. In front of this huge number of connected devices, there is an enormous potential market of thousand services and application products, which will innovate rapidly at the same time just like the explosion of Android or iOS applications. How could the relevant company adapt to this market environment? More accurately, how could the developers in this kind of company create a quality application more efficiently? The answer may be using a powerful framework to simplify the development and reduce the development time. In this article, we will give a brief introduction about Standard OneM2M and its implementation OM2M. In order to give a more concrete global view of OneM2M as well as OM2M, we realized a PoC dedicated to a connected vehicle user case, in which we implemented two classic services (GPS tracking, real-time speed display and remote control). OneM2M, a worldwide standard for IoT platforms and systems OneM2M is a standard that provides a common M2M service layer that can be embedded within various hardware and software to connect IoT devices [1] . It was established by a consortium of ICT standards development bodies in July 2012 and the consortium includes ETSI (European Telecommunications Standards Institute), TIA (Telecommunications Industry Association Japan), TTA (Telecommunications Technology Association Korea), TTC (Telecommunication Technology Committee USA) and CCSA (China Communications Standards Association). The layered model of oneM2M comprises 3 layers: Application Layer, Common Services Layer and the underlying Network Services Layer. Just like the graph below depicts, this model supports end-to-end (E2E) M2M Service [1] . Reference: OneM2M TS-0001 version 1.13.1 Release 2 This clause describes the services provided by the Common Services Layer in the M2M System. Such services reside within a CSE and are referred to as Common Services Functions (CSFs). An instantiation of a CSE in a Node comprises a subset of the CSFs [1] . Those CSFs are listed in the graph above. Mca, Mcc, Mcn are the reference points, which provide the communication between two different entities. The CSFs contained inside the CSE can interact with each other. The graph below depicts all the possibilities to construct a system and the reference points listed below. Reference: OneM2M TS-0001 version 1.13.1 Release 2 Mca: the interface point between AE and CSE which provides the applications access to the common services. Mcc: the reference point between two CSEs. Mcn: the reference point between CSE and underlying NSE like transport and connectivity services. Mcc’: the interface between two M2M service providers. IN: Infrastructure Node NoDN: Non-oneM2M Device Node OM2M – The implementation of OneM2M OM2M project is an open source implementation of OneM2M and SmartM2M standard that initiated by LAAS-CNRS. It provides a horizontal M2M service platform for developing services independently of the underlying network, with the aim of facilitating the deployment of vertical applications and heterogeneous devices [2] . OM2M provides a horizontal Service Common Entity (CSE). Those CSEs can be deployed in an M2M server, a gateway, or a device. The services provided by CSE like Application Enablement, Security, Triggering, Notification, Persistency, Device Interworking, Device Management, etc. can be used by developers to simplify the application development. Reference: OM2M website EclipseCon France 2014 Slide Proof of Concept using the OM2M solution With the purpose of testing OM2M and to gain the retro-experience of this PoC, I, as an intern at OCTO Technology have the chance to realize this PoC dedicated to the user case of connected vehicle. Use case description and Macro functions of the implemented system The Internet of Things is breaking fresh ground for car manufacturers by introducing entirely new layers to the traditional concept of a car. This upgrade — the connected, smart car — comes as a revolutionary way for us to drive and stay in touch with the world around at the same time. On the inspiration of this new concept of the car, we decided to try to apply the IoT framework to the system of a connected vehicle with the most classic service asked by the driver and provided by the manufacturer in various ways — GPS function and real-time speed display function. Besides the GPS function, a remote control function was anticipated to test the bi-directional of the dataflow. 1, Localization: The application on the smartphone can display where your device is on the map and the speed of your device. 2, Remote control: The application on the smartphone can remotely control your device. Here, we are using the application to start and stop a part of the program on the device. Architecture of the prototyped system An architecture dedicated to the realization of this use case is described in the diagram below: Three parts are found in this architecture: the side of vehicle, the cloud and the user terminal. The detail information listed in the next paragraph: Module GPS: Pixnor Waveshare UART GPS NEO – 7M(B). This device will receive the GPS, speed signals and generates the GPS, speed data in the form of norm NMEA. Raspberry pi: Raspberry Pi 3 supports WiFi and BT/BLE. It will work as a gateway and manager the data of GPS and speed. PC: MacBook Pro OS X El Capitan 10.11.4. In the prototype, we replace the cloud service with personal computer for the reason of simplification. Here, the PC will work as a server. Mobile phone: An Android cellphone with the version 6 on which, there is an Android application running as a terminal to the user. WiFi LAN: OCTO WiFi network. But, how does the actual architecture represent the operational architecture OneM2M mentioned in the previous paragraph? The diagram below gives the answer. The GPS module will take the place of Non-oneM2M Device Node with the reason that lack of “intelligence”, we cannot install the required package library to run the whole OM2M on the device. The solution is to add a Middle Node (Gateway), which will provide the connection service and control the GPS device. The cloud service, replaced by a PC in the prototype architecture, runs as a server acting like an orchestrator and takes the role of infrastructure Node in the whole architecture. Because no Android version of OM2M exists, the user terminal was realized in the prototype architecture is represented in the operational architecture OneM2M like an IN-AE (Infrastructure Node – Application Entity). The technical implementation of our prototype using OM2M OM2M requires JAVA 1.7 and Apache Maven 3. The integrated development environment shows in the below diagram. The entities organization of the system below gives us a panorama that how we integrate our proper program in an OM2M environment. A GPS control program is written on the MN side to communicate with the GPS device, at the same time, two programs (initialization and running) are integrated in the MN as Plugin on OSGi framework that will collaborate with other OM2M OSGi plugins to realize the purpose functions. Here is the detailed entities structure of the system. Request Sender : The request sender is a utility class that will create oneM2M requests to send to the CSE using Java objects. ObixUtil : The ObixUtil is here to create oBIX representation for the devices. Convert the description and the measured data of the device to oBIX XML representation. Activator : It is the entry point of OSGi plugins. The Activator class contains the implementation of the state() and stop() methods used to activate and deactivate the plugins. Controller : Used to make the interaction between the devices following the orders coming from the oneM2M interface. As your plugin receives requests, the controller performs the corresponding operation on the devices. Connector : It communicates with the GPS module by UART and pass the data to the parser. Parser : GPS NMEA data format processing, to get the final useful data format. Router : Router class implements the InterworkingService interface and will be the entry point for requests that target your plugins. It will send the request to the corresponding method of the controller. Request redirector : Redirect the receiving message upon the inscriptions and subscriptions. Database : This class implements the persistence services and deals with DB operations. Event notifier : Transfers the notification messages. REST client : Used to create the necessary component in the system as a REST client. Web Listener : A socket based listener to receive the notifications. Displayer : A part of the program to communicate with the user by using a google API. How the system works? The system works based on a subscription and notification system. The subscribers subscribe to the data that they need and the notifications will be sent to the subscribers when there is a modification of the data. In our PoC, the gateway was recognised by the cloud after the configuration (exchange of the IP addresses and accessed port). In other words, MN-CSE connected with the IN-CSE. Step 1, The creation of the entities in the system: the device and android application entities like MN-AE MY_GPS, MN-AECONTROLLER, IN-AEGPS, IN-AE MY_CONTROLLER are created by launching the initiation plugin program. Step 2, The creation of subscription: it’s the key point of the system, the subscription allows to distribute the data to the right destination and this distribution is achieved in the form of sending notification messages. In the localization use case, the subscription of IN-AEGPS to the data of MN-AE MY_GPS is created. In the remote control use case, the subscription of MN-AECONTROLLER to the data of IN-AE MY_CONTROLLER is created. Step 3, Notification messages sending: after the successful creation of a subscription, the notification will be sent when the data is modified. In the use case of localization, when there is a creation or modification of data, for example, a new GPS coordinate data was created, the notification will be sent to IN-AEGPS and then, the android application can parse the coordinates in the notification message and display it on the map. For speed data, the processes are the same, GPS coordinate data was replaced by speed data. In the use case of remote control, when the button is clicked, a new control data is generated, the notification will be sent to MN-AECONTROLLER and then, the related program will be executed, for example, to stop creating GPS coordinate data instances. A test of this prototype has been launched by the head of the IoT tribe and myself. The test environment was chosen as a real car, outdoors with a 4G connection. The system was installed temporarily in the car and powered by the battery of the vehicle. Result: Prototype system ran correctly in the car and showed a correct traces tracked by the system and speed displayed correctly in the application like on the photo taken during the test. ioture-presents-2016new Conclusion OM2M, the implementation of OneM2M worked well in this use case, thanks to the subscription and notification services provided by the framework. Data flows were transmitted fluently between the Android application, gateway and server sides. The prototype system is constructed on a simplified network. A real world network is more constrained on security and network management. But this PoC gives us an expect of the possibility to develop a real system by using this framework. We also stand to wait the maturity of the OM2M implementation and the welling to test other OneM2M based implementations. An inspiration news came out a few days ago that standard OneM2M published its second release in which will enable the interworking between systems using AllSeen Alliance’s AllJoyn, Open Connectivity Foundation’s OIC, and the Open Mobile Alliance’s Lightweight M2M (LWM2M). [1] OneM2M release 2 specifications TS 0001 2.10.0 Functional Architecture [2] OM2M website http://www.eclipse.org/om2m/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Framework , Internet of Things , IOT , OM2M , OneM2M . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 4 commentaires sur “An IoT application using IoT framework? Here it is” Yassine Snoussi 28/01/2017 à 16:28 Hi,\r\n\r\nI would like if possible to have the source code of your PoC.\r\n\r\nBest Regard Madhu 07/02/2017 à 12:21 Hi, You have made this article about your application is very explanatory. \r\n\r\nIt will answer lot of  my questions on OM2M if you share your source code for reference. Can LIU 16/02/2017 à 16:44 Hi,\r\n\r\nThank you very much for reading this article. \r\nI am so sorry that i couldn't share the source code because of confidentiality regime. \r\nBut if you have some question about this article or OM2M, feel free to contact me.\r\nIt will be my pleasure to answer it. Hai Bui 19/03/2017 à 20:56 Dear Can Liu,\r\nThank you very much for your post, it is very helpful for me.\r\nI am new to OM2M and oneM2M. And I have some stupid questions that are:\r\n1. How can you create AE (MN-AE MY_GPS, MN-AECONTROLLER, IN-AEGPS, IN-AE MY_CONTROLLER). You have to make or only have to configure something ?\r\n2. I read the lamps sample and encountered the notion of IPE:\r\nhttps://wiki.eclipse.org/OM2M/one/Web_Interface\r\nhttps://wiki.eclipse.org/OM2M/one/IPE_Sample\r\nCan you show me the IPE in your example, is it the GPS controller in the MN? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2017-01-16"},
{"website": "Octo", "title": "\n                Are you self-deluding when measuring sprint velocity?            ", "author": ["Maria-Dolores Manzanedo"], "link": "https://blog.octo.com/en/are-you-self-deluding-when-measuring-sprint-velocity/", "abstract": "Are you self-deluding when measuring sprint velocity? Publication date 21/12/2016 by Maria-Dolores Manzanedo Tweet Share 0 +1 LinkedIn 0 I‘ve seen the outcomes of making high velocity an objective in young agile teams you people wouldn’t believe. Attack ships on fire off the shoulder of Orion… Um.., maybe I am not going to talk about Tannhäuser Gate. But certainly, I am convinced that if one understands velocity the impact will be the improvement of the velocity. Even without c-beams glittering. Let’s slow down a bit. As a matter of fact, focus on going fast could mislead me. So, do I want to run fast or do I want to run well? Let me ask you a question. What are you going to do to have a high velocity? In fact, why do you track velocity? Since we decided to measure our velocity when developing in agile at the beginning of the sprint we agree to Assign the negotiated value points to some user stories (US) and Pick enough US to keep us busy Then we develop them and at the end of the sprint, we add the story points in order to calculate our velocity. Easy, isn’t it? As I realized by working with new agile teams and this concept of velocity is not so obvious . The most common reflexes I have observed is to relate story points with productivity. Once, a developer had carried out some designer tasks. The whole team counted the time the developer had spent as story points and added those points to the sprint velocity. They were dissatisfied when I announced that this work didn’t account for the team velocity. “But, she was working!” they argued “Her work must be taken into account.” “No,” I told them. “This work is not comprehended in our team velocity. We have had a developer working as a designer this sprint, this has an impact for sure as we have not yet delivered value. It is important to know that this issue has effectively slowed down our velocity. What could we do the next time in order to maintain it? Or do we choose not to maintain our velocity? Because the designer tasks were really necessary and it prevents us from rework developed US with the PO” My interpretation is that the team was self-deluding to keep a high velocity, seeing their velocity as a justification of their daily work . Another common mistake I have seen is the measurement of the velocity as the amount of US’s points that have been developed, even if the PO didn’t test and accept them . In that case the velocity was restricted to the developers’ team, therefore eluding the responsibility of continuous testing by the PO and giving a wrong velocity as the done-done US had not been respected. This behaviour puts delivered quality at risk and shifts the burn-down on the developers’ shoulders. Are you really going to demo US to your stakeholders without knowing if they have been correctly implemented? If you are lucky, the US will be correctly implemented. If some bugs are present you are creating technical debt that will be slower to fix and will increase cost of development. Since the time needed to fix the US will be included in the next sprint you are in fact deteriorating your lead time, i.e., the time between the creation of the US and its delivery in production. Once I told to a PO that I didn’t agree with her measured velocity as the real velocity was zero. None of the US had been accepted for the sprint. She answered “yes, but a zero velocity is sad”. What to do about it? Is my car running fast enough? We are in a car and we want to know when we will arrive to destination. We measure our average velocity every 50 Km. This is the velocity that the team reported: This is what we understand with this measure: This is the real velocity (done-done US): And this is the reason of the real velocity: What I mean is that if you are not strict when calculating your velocity, you reinforce the chances of not detecting the issues that sway your work. When your team is facing obstacle, it is normal that velocity goes down. When the team removes obstacles, then the velocity goes up again. Do I want to run fast or do I want to run well? Let’s assume that my purpose is to run faster and I am self-deluding (in the same way I observed when working with the teams) to measure a high velocity. I can forsake some weight, eliminate seats and other stuff, I can strip down the air conditioning system, put narrower pneumatics to reduce friction, change my windows for the lighter Plexiglas windows… Ok, ok, at the end my car will look like something you people wouldn’t believe. After such extreme car tuning the velocity will rise. Nevertheless, if there is a cow on the road in a known area with domestic bovines free, my previous measures won’t be very helpful. I didn’t objectively observe and measure my previous sprint. As a consequence I am scorning how to predict obstacles. Even more, I am refusing the knowledge obtained from testing related solutions to that hitch, thus neglecting any possible improvement (other way of self-deluding to have a high velocity could be to overestimate the assigned points to the US in the planning game. Fortunately, I never saw such a practice with my teams). In the other side if I consider that my purpose is to run well I will possibly consider other priorities, as the comfortability of the cabin, the isolation… and I will be likely more receptive to collateral impacts of my decisions. I will analyze which elements contributed to my velocity. If the event the produced the slowdown of velocity cause to occur again, I will take advantage of my previous experience . This is why I am not afraid to measure a zero velocity for a sprint, even if it is sad. And the reason that makes a high velocity is not my aim. Velocity is a tool that I use to work continuous amelioration by analysis (for example in the retrospective) and, so doing, I will improve my velocity. Time to live. Thanks specially to Christophe Thibaut. This article would not have beee possible without his help. I also thank  Julien Vignolles, Marc Bojoly, Maxence Modelin, David Luz, Samy Amirou et Dominique Lequepeys for providing support and advice Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology and tagged agile , user story , velocity . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-12-21"},
{"website": "Octo", "title": "\n                I have tested Amazon Athena and have gone ballistic            ", "author": ["David Alia"], "link": "https://blog.octo.com/en/i-have-tested-amazon-athena-and-have-gone-ballistic/", "abstract": "I have tested Amazon Athena and have gone ballistic Publication date 05/12/2016 by David Alia Tweet Share 0 +1 LinkedIn 0 If Athena only evokes this (traumatic, isn’t it?) scene to you, you’ll be disappointed: this blog post is dedicated to Amazon Athena, the latest analytic tool recently announced at Re:invent 2016 . What is Athena? Athena is self defined as a “Serverless Interactive Query Service”. Let’s detail. “Interactive Query Service”: easy to figure out, there are already plenty of tools that are able to interactively query data sources. Toad, Hive or Business Objects are all interactive query services, in their own kind. The freshness resides in the Serverless adjective. After Lambdas, which are defined as serverless computing services, Athena provides an all-in-one query service without the burden of setting up clusters, frameworks and ingestion tools directly on top of S3 with a pay-per-query model. Athena allows to query very large sets of data in S3 with SQL-like language, from within the Athena console. How does it work? This is the beauty of Athena, it takes 3 easy steps to be able to query your data on S3: Create a database to hold your data Create the tables to match the files format stored on S3 (several formats are available: CSV, Json, Parquet, etc.) Query! Create your database and point to your S3 bucket Cheat tip : don’t forget the / at the end of the S3 url. Choose your file format. Parquet and ORC are useful for specific Read/Write performance optimisations Then comes the flaw. Athena was supposed to be the Goddess of Wisdom, this is what reminded me of this terrible picture of Athena wounded by a golden arrow right in the heart: you have to manually declare each column with its type. Like in 1998. I had a proper and nice DDL -Data Definition Language- to upload with my 19 columns (only!) but I had to define each column one by one, to finally find out that her majesty Athena was building it for me. One by one… And thankfully I had no partition for this example… The final “CREATE TABLE” query Note the specific SerDe. Hive allows more concise script with ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ But after a few more clicks, you’re ready to query your S3 files! It’s really fast, and can naturally be compared to an EMR instance running Hive queries on top of S3 in terms of performance. As one would expect, queries are processed in the background, making the most of parallel processing capabilities of the underlying infrastructure. Under the hood, Athena uses Apache Presto to process data in the background. The expected results… in a snap! Multiple queries can be run in the background The other features of Athena are: Saved queries : for frequent queries History : history of all queries, and this is where you can download your query results Catalog Manager : very simple database and table manager The results can be exported instantly in CSV files Now compare these operations to: the setup of an ephemeral EMR cluster on 3 m3.xlarge instances the creation of a dedicated step for Hive (or equivalent) to create the table and execute the query the termination of the cluster when the processing is over … and the automation of these 3 steps when you run saved queries frequently… When is it suitable for me? Of course, Athena is not the panacea for all use cases. It is “just” a query service in a console: you cannot plug it to any publishing layer (Business Objects, Tableau or even d3.js data visualisation framework), there is no way to use Athena APIs so far, or to parameterize queries, and it works only on top of S3. Amazon did not shoot itself in the foot though: companies will always need EMR clusters to process data. But they innovated to provide easy-to-use straightforward query services for these use cases: Unfrequent querying of archived data : legacy system’s structured data are stored and archived in S3 but must be queried regularly (once a week for ex) for auditability or regulatory purposes. Instead of setting up a temporary RDS instance to store the data so it can be queried, Amazon Athena makes it immediately available Easy exploration : provided you have the proper rights, you are now able to test your queries against a vast amount of data for $5 per terabyte . No need to call the BI team to set up a EMR cluster for you, or to extract data for you with multiple back-and-forth when defining the query. (Manual) pipeline for demonstrating model/reporting : now you can extract and process data directly from your S3 repository, it is very simple to download the generated csv file to connect to any visualisation or drill-down capable tool to demonstrate a model or a report without having it industrialised If your company lands all its data into S3 as a datalake, it must be tempting to think that Amazon Athena could query it all. However, permissions and rights in corporate companies in general do not allow such queries across all domains and groups, if only Athena could query multiple buckets at the same time. Don’t jump too fast on the conclusion then: it won’t be your natural enterprise wide query tool. The actual limits As said previously, Athena is powerful though lacks of several features: No integration with Amazon existing services (besides the infrastructure ones, eg. security) No API yet , and no possibility of passing parameters to queries Exports in CSV files only Data must reside in S3, in one single bucket But If I was Athena’s Product Manager, I would be proud of my MVP (if it is): feedbacks from actual users will stream in and enhancements will be made quickly. Conclusion If Athena is tantamount to War or Wisdom, Amazon Athena is almost perfect for instant queries on top of S3. It’s fast, it’s relatively cheap, it’s easy to use. Beside the lack of DDL upload feature, that Amazon may add or improve in the future, it’s the ultimate replacement for an EMR cluster when simple querying S3 and will be the watershed in this space as Lambdas are for ephemeral, stateless and quick processing to EC2. Next steps for Athena should be its APIsation: with APIs and the ability to parameterize queries, Athena would probably sort out 50% of common BI actual problems and that would be the killer app for Amazon. It’s available in Virginia region at the moment, but no doubt it will be deployed everywhere around the world swiftly. Stay tuned to make the most of this product soon! IMPORTANT EDIT – 06/12 Thanks to Rudy Krol who alerted me, the conclusion should be amended: jdbc is available for Athena as described here . Thanks Rudy! Let’s query everything! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “I have tested Amazon Athena and have gone ballistic” Lydon 07/12/2017 à 11:02 Note that Athena now lets you \"bulk add columns\" :) Tom Harrison 07/12/2017 à 16:54 Hello -- just came across your post from a few years ago. I had the same reaction to Athena, and am happy to report that many of the limitations you found have been addressed.\r\n\r\nAWS has made many substantial improvements to the original product.  My company is using it in several different contexts now.\r\n\r\nFirst, we use it for analysts who query the results of our ETL pipeline.  Originally, we had found that the data we produced by our Cloudera Hadoop cluster and that we backed up to S3 was directly queryable by Athena and that it was an order of magnitude or two faster than Hive, and far more reliable.  Indeed, it was our move to Athena that caused us to abandon Cloudera Hadoop and move to AWS EMR, which is far, far better for our use cases.\r\n\r\nAthena now also supports the unified metadata catalog provided by AWS Glue.  We have not yet looked into this, but it seems like a valuable tool that will further extend our capabilities and reduce need to synchronize our various processes.\r\n\r\nWe used their JDBC interface to make queries from a production API endpoint that does various reports for our customers.  We had also had some processes that used ODBC (e.g. Excel, Tableau) and AWS recently released support for ODBC, as well.  Both work very well.\r\n\r\nWe also use a product called Looker to create visualizations, dashboards, and pivot-table-like tools that we're exposing for use to our customers.  Looker uses the JDBC interface and it works brilliantly, against Athena.\r\n\r\nFinally, AWS has released several iterations of API support, including support in the AWS CLI -- we are using that also as part of our production workflows. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-12-05"},
{"website": "Octo", "title": "\n                How I removed two databases in one pull request            ", "author": ["Thibaut Gery"], "link": "https://blog.octo.com/en/how-i-removed-two-databases-in-one-pull-request/", "abstract": "How I removed two databases in one pull request Publication date 15/11/2016 by Thibaut Gery Tweet Share 0 +1 LinkedIn 0 My first project right after school was called KISS which means Keep it simple, stupid Three years and a few IT projects later, I thought that I got it right and applied the following rules: keep your applications small and simple, remove most of its complexity, don’t over-engineer, all code and user stories have to be maintained ( YAGNI ) Yet a few weeks ago, I learned that you can always remove useless things. You just don’t know it yet. Story time I wanted to create an API above the drugs data set ( freely provided by the French government as CSV files ). I knew that I needed a script to parse and store the data and a web API to expose them. The code is on github and the application is live . When I start a web application, I have 2 main choices to make: Choose a platform : I picked nodejs, for the only reason that I am proficient in it Choose a database : I took part to a similar project where I used readDB, commonly called mongoDB. It was painfully slow when upserting lots of data, so I chose couchbase instead, a key/value store with a document query language. After a while I needed “full text” search, so I added elasticsearch, simply because I knew how to use it. It requires a few (ugly) scripts to automate its synchronization with couchbase. And suddenly came the epiphany ! Or to be more precise, I asked for a review and after detailing the context, my colleague simply asked: “why don’t you remove couchbase?”. He was spot on: the dataset fits in memory and it is readonly. It was replaced with a simple import to load the JSON in memory ready to be queried by the application. But it got me thinking, couldn’t I remove the second database too? And I learned that there are indeed multiple libraries to search into local documents: I just used lunr and could therefore remove both databases. The application is now more simple and reliable: The code base has been reduced: 122 untested lines of script, 227 lines of configuration The application is now deployed with one image : less reasons to fail No more scripts to load the data in the database (the file is read by the application) Nonetheless, after the refactor, the application was a bit slower even if everything was done in memory. Having said that, response time is still below 20 milliseconds, so the advantages brought by this removal still exceed the loss of a few milliseconds. This resulted in a pull request with twice as many deleted lines as added lines and my takeaway would be: Following KISS principles is more difficult than it seems: you can always question yourself but you should more likely seek for feedback from your peers . And, instead of choosing a database straight away, I would now choose a place to store my data. Happy coding! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-11-15"},
{"website": "Octo", "title": "\n                Joyful wind of change: A software craftsmanship short tale            ", "author": ["Wassel Alazhar"], "link": "https://blog.octo.com/en/joyful-wind-of-change-a-software-craftsmanship-short-tale/", "abstract": "Joyful wind of change: A software craftsmanship short tale Publication date 16/11/2016 by Wassel Alazhar Tweet Share 0 +1 LinkedIn 0 This is the story of a team. A bunch of 11 aspiring software craftsmen who decided to change things around and get their job done in a better way. The story takes place between the 30th and the 50th iteration of the development process of a software. This software is a website serving over 2 million regular users and providing legal information and services to 65 million French citizens. Chapter one: Start from what hurts and set a direction Leaky pipeline It is normal that the build fails. There are some automated E2E tests that fail randomly. Just try again, hopefully it will work next time. This was the common answer to all new developers who struggled passing their first user story. And for information, the build then used to be about 45 minutes long. Almost nobody would get shocked anymore, as if everyone just got used to the pain . Starting from the 30th iteration new people joined the project. With all the fresh blood in the team, this situation became less and less acceptable. So, we decided to sit for one hour and discuss about our pains and how to deal with them . Initially, this workshop was supposed to focus on enhancing our development flow. But it was no surprise when the test strategy became our main subject. The 45 minute unstable build was pointed by almost everyone. Here is a non exhaustive list of pains that have been raised: Build duration Instability Randomly fails Depends on external systems Hard to maintain Too many integration testing Unknown test coverage Lack of confidence Long feedback loop Losing time analysing the build failure which is often not due to a regression We first thought to just run tests once a day instead of doing that continuously to validate any change to our code base. Fortunately enough, we did not follow this direction for a long time, since nobody was yet ready to surrender to mediocrity. But we soon figured out that there was something wrong with the way we were writing tests . Even if End-to-End tests would simulate a real user and seem pretty easy to write, we could not rely our entire test strategy on them anymore. They had already cost us too much so far. However, not everyone in the team shared the same experience regarding tests. For instance, differences between unit and integration tests were not clear for most of the team . But that was an opportunity to get things clearer. We started by defining a common vocabulary and the intention behind each test category: Unit tests: To test a unique behaviour of a code unit (i.e., mostly class in our case) Integration tests: To test collaboration between classes and components End to End tests: To test a functional scenario (the app is considered as a black box and we test it from the user perspective) After that, we set a direction to move towards and gather all our energy. We needed to test faster and more reliably. Which brought us to the same newly born desire: more Unit tests than Integration tests than End to End tests. Let’s be clear: we did not invent the testing pyramid model . We have just adopted it collectively. Once we decided to take care of our testing pyramid — Which at that time looked more like a UFO —, we needed to define how to do so. But this is another story. The one hour workshop was over. Distribution of tests when we first measured it. The journey How many iterations are you ready to give up in order to improve your development process? The challenge was not to stop making features and start writing tests until we got a perfect pyramid. Taking care of the pyramid never was the purpose . It was more like a tool we had collectively adopted to help us deliver more value, more reliably and faster. And sometimes taking a step backward can be the best way forward. As we would not stop writing code, we had to make sure not to get things any worse. So we thought about using TDD , a coding technique that consists of writing tests before writing code. The problem was that only a few team members were familiar with TDD and used it everyday. Most of us barely knew what the acronym stands for. And here came a little miracle. Somehow, and among all the urgent upcoming very important features, the tech lead managed to persuade our sponsors to get a 3 day TDD training for all the team. Not only the “bad students” in the developers class got trained, but everybody was involved. In other words, during 3 days we did not move any post-it in our kanban. We came to the office and, instead of talking about user stories with the product owner, we focused on red – green – refactor cycle with the trainer. And instead of coding features we would code a Connect Four game. After the training, everyone was keen to TDD his next ticket. When I look back to what happened after, I think it was an important turning point. Our team is now more faithful about what we can accomplish in the future. However, we still had to overcome our legacy. And we knew that it would have been more like a marathon than a sprint. We realized that we would not have to bear the burden of bad tests for a long time as soon as we started deleting some of them. Oh yes! We did so, and we were not even afraid. We were not obsessed about testing everything, anymore. Instead we were focused on testing things in the right way . The most painful tests were the first to get dropped. And by the most painful ones, I mean those which failed more often. A few E2E tests were also quite easy to remove, as they were just unnecessary. There was not much of value on testing each business rule at this level. For the tests we were not able to easily drop we created a test-to-shot table next to our Kanban. Whenever a build failed because of a test, we wrote the name down or add a stick next to it if it was already there. The tests on the table were added to our backlog and got prioritized according to the number of sticks. Handling these tickets was mainly about answering a few questions: Does it match a user scenario? What is the intent of the test? Does it provide any additional value to existing tests? Is it possible to move it to the bottom of the pyramid (i.e., replacing it by integration and unit tests)? We relished each victory no matter how small it was. Things went quite fast, indeed. Build after build, story after story, and sprint after sprint, we could clearly see the shape of our pyramid. Deleting bad tests was not the only way to reverse the trend. We have written far more tests than before ( see the figure below ). But the ones we wrote were running faster. The 45 minute build is now 13 minutes long. Chapter two: Keep learning and sharing A virtuous circle How could we improve things? Improving continuously became our new obsession. By improving the way we tested, we became more confident about our code. And by adopting TDD, coding became more and more fun. But beyond the development process, communication was the key to make the code cleaner, to let our design emerge, to manage our technical debt, to create new apps… And above all, efficient communication led us to define our own way to do so. We all know that communication between developers can sometimes be a dangerous exercise. Passionate developers talking about code may lead to endless conversations. On the other hand, avoiding communication will certainly not lead to any change and will lock the team repeating the same mistakes over and over. Code review: Find defects, spread knowledge and challenge ideas At the beginning our code review used to look like: “I’ve taken a look at your code. It is quite OK. I’ve just modified a couple of things and it is ready to be merged”. In this discipline too, we have walked a long way. Here is the way it works now. The reviewer reads the code and tries to understand it. Whenever he finds something wrong or not easy to understand, he writes it down on a piece of paper. He tries to localize the defects, which may be functional (how the code accomplishes the desired feature) or technical (does the code follow our code standards? Is there a risk regarding our non functional requirements?). After that, the reviewer adds his suggestions to solve those issues, and comes with his check-list to discuss with the author face to face . The conversation is pretty simple: for each point in the list, the author can either accept or reject it. In case of rejection, he exposes and explains his point of view. Here, the reviewer can either agree and just drop the point, or there can still be a divergence. When this happens, all the team may be interested in the discussion. This is why the point is removed from the review and becomes a topic to be collectively addressed during the DevOps retrospective. What is a DevOps retrospective? Before defining what it is, let’s start by what it is not. A DevOps retrospective is not: The sprint retrospective ( which involves the whole team and during which we try to enhance our process for the next sprint ) The developers supreme court It is a ritual that allows us to take decisions and to share about our code and infrastructure . In addition to the sprint retrospective, we have adopted this ritual to focus on technical issues. This is a one hour meeting between DevOps only (all the team members co-work on development and infrastructure). We first start by dot voting the topics in the retrospective backlog — each team member has 3 votes to dispense. Then we address topics starting from the most voted one. Each topic is time boxed from 5 to 15 minutes, depending on the topic itself. When the time is over, each topic can have one of the following outcomes: A specific subject has been presented A new code standard is adopted by the team A technical task is created and added to our product backlog If we still need to dig deeper into a specific subject, then we will schedule a dedicated workshop. By the way, our team loved these workshops, since they often gave us the opportunity to code together. It was pretty fun and we have experimented different approaches such as mob programming or randori . Why code standards? We strive to write clean code for both machine and humans. Code standards help everyone in the team read and write code. Moreover, once a standard is displayed on the “law” table, it helps avoiding endless conversations about naming and conventions. In other words, standards are the best way to prevent the parkinson’s law of triviality and this allows us to focus on what is more fun! How the design has emerged This is what Wikipedia says about emergent design in agile software development: Emergent design is a consistent topic in agile software development , as a result of the methodology’s focus on delivering small pieces of working code with business value. With emergent design, a development organization starts delivering functionality and lets the design emerge. Development will take a piece of functionality A and implement it using best practices and proper test coverage and then move on to delivering functionality B. Once B is built, or while it is being built, the organization will look at what A and B have in common and refactor out the commonality, allowing the design to emerge. This process continues as the organization continually delivers functionality. At the end of an agile release cycle, development is left with the smallest set of the design needed, as opposed to the design that could have been anticipated in advance. The end result is a simpler design with a smaller code base, which is more easily understood and maintained and naturally has less room for defects. In other words, don’t bother yourself with big upfront design. Instead, focus on making software working properly and if you craft it well, design will naturally emerge from refactoring cycles. In real life, many teams fall into the trap of easiness. That is to say, by referring to the same example in Wikipedia article: once B is built nobody will look at what A and B have in common and refactor out the commonality. Here are some actions that, I believe, have helped us doing a better job when it comes to let our design emerge. Don’t be afraid of talking about design As we saw earlier, talking about code might be dangerous. It’s easy to understand that talking about design can be even more. But talking about design helps seeing the big picture of the software we are making. This provides us with a clearer view of its boundaries and we get a better understanding about what should be decoupled and where abstractions and details belong to. This also helps us to define how to structure our code and to set a direction for our energy to go towards. We held a few workshops on this topic. And even when talking about design, we were never too far from the code. One of the way to refactor the code is to split fat classes into more skinnier ones. There are plenty of reasons to do so: reuse reasons, SOLID reasons or simply to divide and conquer the complexity. Obviously enough, this produces a large amount of collaborating classes which may confuse the developer. The aim of the workshops was to imagine how the components we were building should have looked like. Doing this exercise provided us with concrete answers about how the classes should be structured in packages. In the end, talking about design leads sometimes to decide about a direction to move on. While performing this exercise, the need to remain pragmatic and avoid dogmatic rationalization should be kept in mind. The keyword to start with should always be “why”. Apply the boyscout rule Always leave the campground cleaner than you found it. Let’s get back to the example of the pieces of functionality A and B and the need to look at what they have in common and refactor out the commonality. This is where the boyscout rule applies. Leaving the code “cleaner” than you found it, should be the aim. And this can lead to spectacular changes. The main trap is to think you need to start a big bang refactoring. This is not what the boyscout rule is about, what we want to leave cleaner is the campground not the whole forest! Big refactoring is not even Agile. Again, over-refactor and over-design are not the purpose. If this would not help deliver value more reliably and faster, then YAGNI ( You Aren’t Gonna Need It ). Chapter 3: Happily ever after To conclude, let’s take a look to what happened around during this 20 iteration journey. The team has delivered more features ( knowing that, the number of features follows the same trend as velocity ). We have far less bugs. Our delivery process is more stable. And by enforcing the collective property of the code, everyone in the team is happy and can take a break whenever he wants. Vacations are no more a problem to deal with. We do not depend on 10x superstars developers. Instead, we have built a 10x team and we trust it. Why have I written this article? Beyond the philosophy, I wanted to share a concrete implementation of software craftsmanship mindset. It is not a magic formula. It is just about sharing the pride of a group of software professionals getting their job done in a better way. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Consulting Chronicles , Culture , Methodology and tagged agile , automated tests , Best Practices , code quality , development , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-11-16"},
{"website": "Octo", "title": "\n                Angular 2, SSE and changes detection            ", "author": ["Nicolas Carlo"], "link": "https://blog.octo.com/en/angular-2-sse-and-changes-detection/", "abstract": "Angular 2, SSE and changes detection Publication date 02/11/2016 by Nicolas Carlo Tweet Share 0 +1 LinkedIn 0 In this post, we will make a feedback on a surprising bug we had to solve due to the auto-magical changes detection of Angular 2. The context We are currently working on a Dashboard-like application: a screen on which we render different tiles that display various information from an API. Tiles should refresh to keep showing the most up-to-date information. However, there is no need for websockets: Server-Side Events − SSE − make possible for the server to push new information to the client when necessary. But the number of SSE connections is limited for the client . Having a SSE connection for every single tile of the Dashboard is not an option. Knowing that Angular 2 has adopted RxJS , we decided to go with Observables to propagate events reactively between the composing tiles of the Dashboard. Real-time updates with Server-Side Events and Observables In the Dashboard component, we create a mainStream Observable that emits a new event anytime the server is pushing one. Roughly speaking, we are converting our SSE stream into an events one with RxJS: getIndicatorsStream(): Observable {\r\n  return Observable.create((observer) => {\r\n    let eventSource = this.sseService\r\n                        .createEventSource('http://localhost:8080/api');\r\n    eventSource.onmessage = (event) => observer.next(JSON.parse(event.data));\r\n    eventSource.onerror = (error) => observer.error(error);\r\n}); In the OctoNumber component − a tile which role is to display a numeric value − we create a new Observable from mainStream by filtering values we are interested in. Then, we just need to give this Observable to the async pipe so Angular 2 subscribe to it and update the displayed value. Problem is, it doesn’t work . Debugging It doesn’t work, still: If we replace mainStream with Observable.interval( 1000 ) − which increment a counter every second − then it works! If we log values that go through mainStream , we can actually see they are emitted. The thing is, they don’t show up. If another tile − let’s say OctoQuote − make the view being refreshed, then our tile − OctoNumber − is correctly refreshed with the latest value too. At this point, debugging is filled with “WTF?!?” as assumptions pop and seem to contradict each others: Does the async pipe even work? It seems so when we give it a basic Observable… Are server events correctly pushed in mainStream ? It seems to be so. Furthermore, if we can see logs, that means we are actually doing a subscribe to the Observable with the async pipe, otherwise nothing would happen . Is that an conflict between instances of our components? Well, results are the same even with a single tile… The enlightenment Actually, it’s an automatic changes detection issue from Angular! Indeed, if the async pipe in our template is really doing a subscribe to the Observable it gets, it is not directly refreshing the DOM. To do that, Angular has to detect a change. That’s why the value gets updated when others tiles are making updates: DOM refresh make the latest value of our tile appear at the same time. Why is Angular not detecting when we emit a new event in our Observable? Because it is executed in the EventSource: it’s the “ eventSource.onmessage ” callback which does the job here. Angular is not notified about that! To solve this problem, we need Angular to be notified when callback is executed. This is the role of zone.js from which Angular 2 depends. Therefore, “all we have to do” is to instantiate a NgZone and call “ zone.run( callback ) ” in place of our callback. That way, Angular is notified when change happens. getIndicatorsStream(): Observable {\r\n  return Observable.create((observer) => {\r\n    let eventSource = this.sseService\r\n                        .createEventSource('http://localhost:8080/api');\r\n    eventSource.onmessage = (event) => {\r\n      this.zone.run(() => observer.next(JSON.parse(event.data)));\r\n    };\r\n    eventSource.onerror = (error) => observer.error(error);\r\n}); And it works! We can now consume mainStream just like any regular Observable in our components: the async pipe will “understand” there has been some changes when a new event occurs. Lessons learned From this point of view, Angular hasn’t changed: it is still a black box which is doing a bunch of magic so everything works easily for everyone. But if you have a non-standard use case, you should better know all of the framework concepts so you can cleanly solve your problem… and there are a loooot of concepts . Also, folks searching for reactive programming might be disappointed. If Angular 2 is using RxJS Observables, it’s still passive programming under the hood. This could be surprising because a truly reactive code wouldn’t have cause such a trouble: view rendering would have been a single function, called as a subscribe of the Observable. No concept of zone, out-of-application detection, etc. Just a function… Which always work. Edit of nov. 3rd, 2016: an issue to support EventSource is currently open on zone.js. Here are a bunch of resources that have helped us figure it through, if you want to dig further: Zones in Angular 2 Understanding Zones Angular 2 Change Detection explained Change Detection in Angular 2 [StackOverflow] Creating an RxJS Observable from a SSE [StackOverflow] Angular 2 View Not Changing After Data Is Updated [StackOverflow] How to update view after change in Angular 2 after Google event listener fired Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-11-02"},
{"website": "Octo", "title": "\n                GraphQL: A new actor in the API world            ", "author": ["Cédric Nicoloso"], "link": "https://blog.octo.com/en/graphql-a-new-actor-in-the-api-world/", "abstract": "GraphQL: A new actor in the API world Publication date 08/11/2016 by Cédric Nicoloso Tweet Share 0 +1 LinkedIn 0 In 2012, Facebook teams were in the process of rebuilding their native mobile applications and they had to adapt their server queries to get the data they needed. When they started to refactor some parts of their code, some got frustrated and stepped back to think of a better way. GraphQL was born. They changed their point of view in terms of resources, and they preferred the idea of a graph of objects instead. The whole idea is to call a single endpoint, passing a query as the content of the request. Throughout this post, I will give an overview about what GraphQL is, including advantages and drawbacks. Then will come the best part with being how to create a GraphQL API. Some major IT actors are already using it and that gives us hints about its current stability. Here is a growing list of current users (Github, Coursera, etc…). Let me highlight some reasons to think about GraphQL: Your API has more than one consumer app and these apps are out of your scope. You need to maintain specific endpoints for some of your consumers. Your consumers usually need more than one request to build a complete view. Some of your consumers are mobile apps (potentially slow connections). That being said, if your application is fairly simple, it wouldn’t make sense to start using GraphQL. Or if your app requires a lot of caching and that you don’t want to use a complex client app, Rest will continue to be your best choice. And if it’s too early for you to switch everything to GraphQL, you can still work internally with it while still exposing public APIs through Rest. Where does it come from? GraphQL comes from Facebook where it has been internally used for over 4 years, serving billions of API calls everyday. It became interesting when they decided to open-source it mid-2015. The detailed story can be found in their blog post . As we can see from Google Trends, GraphQL has risen more and more interest over the past months and reached its highest peak of popularity last week. What is it? The name of “GraphQL” is almost self-explained: Graph Query Language. It’s a specification for a language to query data represented as a graph of objects. The query is a string with a specific GraphQL syntax that is sent to a server to be interpreted and fulfilled, which then returns a JSON document back to the client. Example here from Facebook, get some information about a user: We can see that the shape of the query mirrors the response itself. Let’s have a look at the full workflow: GraphQL doesn’t require to use HTTP to send your query, but that’s how it has been used by everyone so far. You can either use a GET request and give your query as a URL parameter: http://localhost:4000/graphql?query=query{students{id,name}} Or use a POST request with a specific “Content-Type” header: “application/json”, and the body would contain a unique attribute called “query” with its value to be the GraphQL query as a pure string “application/graphql”, and the body would contain the raw GraphQL query in the body. GraphQL also allows us to write complex queries using nested queries and fragments. Nested queries are simply parts of the query that could be independent. We can then ask for children of our main object, which we call connections . Fragments are a way to extract parts of a query and are useful to avoid any duplicate portions: So far, we have only covered read access to our data. What about writing data? Apart from queries, there is another operation called mutation . You can decide of these mutations, depending on what you choose to declare in your schema, but most likely they will represent C (R) UD operations. For instance, we can create a student object by defining it in the “variables” window: A mutation is nothing more than a query, but by convention we know that they will modify data. As an example, Github currently proposes 15 mutations in its GraphQL API, all related to the Project feature inside repositories. Advantages Less backend requests If we think in terms of resources, a client usually requires to communicate more than once with a server, as one resource is hardly enough to build a complete view. As a simple example, if I want to display a list of 10 students from a class, I would need one request for the list + potentially ten other requests, one for each student’s details. I could easily create a dedicated resource for this use-case, which would get me everything in one call, but it means that I need to adapt my server to handle this specific client. HATEOAS (Hypermedia As The Engine Of Application State) aims to facilitate navigation through an API, but it doesn’t change the problem. Indeed, it doesn’t decrease the number of requests you might need to render your view and doesn’t give any flexibility to the client regarding what data it needs. Smaller payload Let’s pretend my students API gets consumed by two different clients. They both want to display a list of students, but one wants to display student’s date of birth, whereas the other client doesn’t need it. A simple solution is to create one resource including everything. But then the second client will get student’s date of birth in all its server responses, without needing it. With GraphQL, you can specify in your query which attributes from which entities you want. This could greatly improve mobile apps performance. Servers become easy to generalize Since clients are now in charge of explaining data they want through GraphQL queries, the server can then only focus on its graph of objects, instead of being adapted to each of its clients. Only one endpoint will serve everyone. Easier to test In a GraphQL query, we ask for entities, also called “Types”. Since each type gets a dedicated handler, it will be fairly easy to unit test them. Hierarchy By its nature, GraphQL is hierarchical and follows relationships between objects, as opposed to Rest where you might need additional requests to fetch linked data. Once again, when writing your query, you control the structure of the answer you want. Strongly typed Unlike Rest APIs, GraphQL strongly relies on types within its schema, and gives you the ability to create your own types on top of its internal types. This Type System allows GraphQL to provide descriptive error messages before executing a query. Example of a bad parameter type: This Type System highly contributes to all the tooling available around GraphQL, such as mock-server from graphql-tools for instance. Documentation Every GraphQL server comes with a tool called GraphiQL , which is a UI to execute queries, explore the schema and access documentation. Since the whole schema is easily available on the client through GraphiQL, we can enjoy efficient auto-completion and light but complete documentation, depending on how well description is put on each field: We can also query the schema itself and ask for its structure. This is called “ introspection ”. With Swagger UI (a well-adopted standard to document APIs, example here ), all you need as a consumer is a list of endpoints with a description of all parameters. Here with GraphQL, all you need is the schema. Apart from the schema, the problematic is the same and will depend on how much effort the dev team has put in the doc, by adding helpful comments to every field for example. Drawbacks As you might have guessed, GraphQL also comes with some disadvantages. Let’s have a look at the main ones. Server code is more complex but at the end, same code will be used to answer multiple use-cases. Defining the schema is a bit verbose but once it’s done, our server will be able to answer multiple use-cases. To consume a GraphQL API, several clients already exist, depending on your programming language. But they lack maturity compared to any client consuming Rest services. The need to rethink caching since getting data through POST requests disables any caching by any proxy between the client and the server. Even if the query is in the URL using GET, caching will still be compromised, as the same entity / “resource” could be part of many different queries. If you want to learn more about this, official doc will help as well as this Relay doc page explaining how to cache a graph. And last but not least, parallelism and concurrency of processing will now have to be handled on the server-side, as we’ll have only one request potentially asking for many relations. How to expose a GraphQL API? We will look at an implementation in Node.js / express. This working example is available at https://github.com/cedric25/graphql-example . A quick demo has also been recorded: https://youtu.be/mZrSV8eRyhc . It shows some examples on how to use our schema through GraphiQL. The server part is quite straightforward and expose one endpoint: “/graphql”: The most time-consuming part of our GraphQL server will probably be the definition of the schema. Communication between the schema and previous data services are done through the resolve() functions. The schema includes two parts, queries and mutations: Now let’s have a look at the first query, getClass(), which takes an “id” as argument: As we can see from the top of the file, we need to define some custom types. Here is the “class” type, where we simply list fields it contains, their type, if they can be null, etc…: We can now go to our /graphql route and start playing with our data through our schema, thanks to GraphiQL, since we’ve activated it in the server part. If Javascript is not your best friend, you’re not alone, a lot of other implementations exist. A good way to find them is here , or in the “Questions” part at the end of this post. We haven’t seen it in this simple implementation, but one of the main challenges using GraphQL will be to replicate its flexibility to our data access services, meaning for example to get only necessary fields when querying our SQL / NoSQL database. Going further We have seen that consuming a GraphQL API is pretty straightforward by passing our query in a POST request for example. But for a real business application, we might need to use a dedicated client such as Relay for React or Apollo Client . GraphQL will continue to evolve and will include following features in the near future: Subscriptions . Ability to subscribe to a feed, in case of the result of a query changes. (Just the specification part, implementations will be free to use web sockets, server-side events, etc…) Deferred queries . Ability to identify part of a query as not necessary before displaying a page. More details from the React Europe conference last June. Questions > Will GraphQL replace Rest? It seems too early to answer, probably not in the near future as the vast majority of existing web services are developed with a Rest architecture. Some say that both solutions can peacefully coexist, as they answer different use-cases. > How active is the GraphQL community? The best way to answer this question is probably to look at the public repos of different implementations: JS , Ruby , Python , Java , Scala , .NET , PHP , C++ , Go , Elixir , Haskell , Lua , Elm , Clojure To put it in a nutshell, yes, GraphQL benefits from a fairly big and growing community, increased by big actors starting to use it. > Does it mean that anybody can query your entire database? No, as long as you don’t directly map your database schema to your GraphQL schema. GraphQL could be seen as a layer where you only expose the data you want. > Would it be possible to query the entire schema to crash the server? Theoretically, yes. But you can define timeouts and set a maximum query depth allowance, or assign complexity points to your entities and define a maximum complexity. > How is it different from Falcor? GraphQL and Falcor aim to solve the same problems. But Falcor doesn’t have the type power provided by GraphQL. I was curious and took a look at what Google Trends say: Sources and links – Facebook first presentation https://code.facebook.com/posts/1691455094417024 – Complete spec from Facebook https://facebook.github.io/graphql/ – Github explaining why they wanted to expose their API through GraphQL http://githubengineering.com/the-github-graphql-api/ – A huge list full of interesting resources related to GraphQL: https://github.com/chentsulin/awesome-graphql – A GraphQL example with schema definition and data https://github.com/graphql/graphql-js/blob/master/src/__tests__/ – 30min talk of a Shopify developer explaining GraphQL https://www.youtube.com/watch?v=eD7kLFGOgVw – Utility functions of graphql-js (introspection query, print schema, validate a query, etc…) https://dev-blog.apollodata.com/graphql-js-the-hidden-features-effaca7a81b3 Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-11-08"},
{"website": "Octo", "title": "\n                A money pot in the blockchain            ", "author": ["Loup Theron", "Eric Favre"], "link": "https://blog.octo.com/en/a-money-pot-in-the-blockchain/", "abstract": "A money pot in the blockchain Publication date 26/10/2016 by Loup Theron , Eric Favre Tweet Share 0 +1 LinkedIn 0 The blockchain is a trendy subject, and we believe one needs to apply its principle to an actual project to understand its technology and progress status. We chose to implement an online money pot in Ethereum, a kind of blockchain that focuses on smart-contracts instead of money transfer. This article demonstrates how we can leverage Ethereum to develop this use-case. Reminders (Blockchain, Bitcoin and Ethereum) If you are not familiar with any of these concepts, you should first read this article (French) which presents an overview of these notions. If you already know of them, you can skip this paragraph. And if you are “in between”, I will simply remind you that in addition to allowing the transfer of tokens embodying a value ( ethers ), like Bitcoin, Ethereum can be considered as a replicated, timestamped, immutable database storing contracts and allowing them to be executed in a given environment . These contracts are actually code representing business processes. Just like Bitcoin, all transactions are charged. Some additional fees are added so that a transaction gets validated and added to the blockchain. These fees do not end up in any natural or legal person’s pocket, but they are used to encourage the network members to validate transactions , therefore to certify that they are not fraudulent. Online money pots Online money pots have known a blazing success these last few years, 2 famous examples being Leetchi (created in 2009) and Le pot commun (created in 2011). Unlike crowdfunding platforms, there is no need to reach a minimum amount , there is no reward for contributors and they handle more types of projects. A money pot can be created to celebrate a birthday, to make a gift, or even to finance construction works after a flood. Most of the online services are available for free, but a free service can be very lucrative. Indeed, advertising on Internet is more and more targeted and personal and tracking information is a gold mine . In these cases, the user is not the consumer, but he is the product itself . Existing online money pots which do not draw off commissions have another way to make money, because they require to use VISA, Paypal or other vendors for transactions, vendors usually not free. They count on our trust to deal the collected amounts to the proper person when the money pot is closed. What about Ethereum? Ethereum can provide a transparent, auditable and almost free money pot. Pay as you go With Ethereum, the money pot is written as smart-contracts . When they are deployed to the blockchain , everyone can interact with them. Ethereum applies a pay-as-you-go model. You pay each time a function is executed, the cost is explicit (the gas ) and the payment is much simpler: no need to authenticate on a third party system or to take out you credit card, only Ethereum’s wallet is required . Here is an overview of the costs related to our contracts (these costs will decrease after Ethereum’s next release, Metropolis (French) ): To contribute to a money pot, you need to pay 0.002295 ethers , which is about 0.0367€. This price may slightly vary depending on the size of the submitted username because it will be stored by the blockchain. To create a money pot, you need to pay 0.01617368 ethers , which is about 0.259€. This price may slightly vary as well depending on the size of the variables stored into the contracts The ether, like any cryptocurrency, is very volatile, it is subject to the supply and demand laws, without any currency policy . The ethers sent to a money pot can therefore have a euro value that fluctuates over a week time. This volatility is only a consequence of the comparison with other currencies. This is actually one of the main stakes of the Ethereum platform to be broadly adopted: benefiting from a stable Ether price . Transparency of execution Ethereum users can also review the money pot’s code by comparing the bytescodes written in the blockchain with the ones they can compile from the source code publicly disclosed. Every coin making its way to the money pot will end up at the recipient address registered in the contract. The funds are now accountable, which was impossible with “regular” money pots , which can be of great interest for charity donation for instance. This traceability stops at Ethereum’s exit nodes (the exchange platforms). Indeed, today you eventually need to trade your Ethers for euros to spend your money, but one can dream of a time when Ether is as good a mean of payment as any. Conclusion Because of its community aspects, Ethereum blockchain seems like a perfect candidate to this money pot use-case. We will see in later posts that its implementation does come with some limits. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-10-26"},
{"website": "Octo", "title": "\n                Android Styles & Themes for developers            ", "author": ["Pierre Degand"], "link": "https://blog.octo.com/en/android-styles-themes-for-developers/", "abstract": "Android Styles & Themes for developers Publication date 09/12/2016 by Pierre Degand Tweet Share 0 +1 LinkedIn 0 For beginner Android developer or the more experienced ones who don’t do much of the UI work, understanding the difference between styles and themes and how they should be used can be very difficult to understand. With AppCompat being a must have in every app and as it’s relying A LOT on themes and style, understanding all this can be very frustrating when it comes to customizing its default behaviors. With this article, I’ll try to explain what this is all about and how it can helps you into your app’s UI code. What is a style ? As an Android developer you probably already wrote some UI code in your XML layouts like this one : <android.support.v7.widget.Toolbar\r\n  android:layout_width=\"match_parent\"\r\n  android:layout_height=\"wrap_content\"\r\n  android:background=\"@drawable/bg_gradient\"\r\n  app:title=\"My app title\"/> How does Android actually knows that this Toolbar should be drawn with our gradient background and with the proper title ? If you look at the source code of Android, you’ll notice that every single widget class have at least 3 constructors (or 4 if you are reading the Lollipop and more source code). Let’s explain the purpose of each constructors. Here are the constructors of a very cool widget, the Toolbar from appcompat-v7 : public Toolbar(Context context) {\r\n  this(context, null);\r\n}\r\n\r\npublic Toolbar(Context context, @Nullable AttributeSet attrs) {\r\n  this(context, attrs, R.attr.toolbarStyle);\r\n}\r\n\r\npublic Toolbar(Context context, @Nullable AttributeSet attrs, int defStyleAttr) {\r\n  super(context, attrs, defStyleAttr);\r\n  ...\r\n} The first one is simply the constructor you use when you are creating views directly in Java code. The second one, with only the AttributeSet is the constructor used by the LayoutInflater when your XML is inflated. This AttributeSet contains all the attributes we specified in our view when we wrote it in XML. The Toolbar will read this AttributeSet and look for some known attributes like title or background (among others attributes used by Toolbar and View ) and then call the corresponding methods like setTitle() or setBackground() to properly configure the widget the way we wrote it in XML. Let’s say that in our XML, we instead wrote our Toolbar like this : <android.support.v7.widget.Toolbar\r\n  style=\"@style/DefaultToolbar\"\r\n  android:layout_width=\"match_parent\"\r\n  android:layout_height=\"wrap_content\"\r\n  app:title=\"My app title\"/> with our res/values/styles.xml file containing the following : <style name=\"DefaultToolbar\"\r\n  <item name=\"android:background\">@drawable/bg_gradient</item>\r\n  <item name=\"titleTextColor\">@android:color/red</item>\r\n</style> The set of attributes given to the the constructor will contains each of the attributes specified in the view’s XML and the attributes specified in the style . If a specific attribute is defined in both style and directly in the XML of the View, the one in the XML will be selected. In other word, we can finally say that, in Android, a style is a set of attributes given to a View either directly in XML or through the style parameter. Introducing Themes As I said earlier, widgets typically have 3 constructors but we only covered the first two. Before going deep into the third constructor, let’s talk about Themes. Theme is basically a super Style applied to an entire Activity and all it’s containing Views. This means that if I declare the following Theme : <style name=\"CustomTheme\" parent=\"@style/Theme.Appcompat\">\r\n  <item name=\"android:text\">Default text.</item>\r\n</style> and I apply this theme to my Activity in the AndroidManifest.xml file, every single views inflated in this Activity will at least receive in its AttributeSet the attribute text with the value specified in the Theme. So, if we declare in our Activity’s view XML the following TextView : <TextView\r\n  android:layout_width=\"wrap_content\"\r\n  android:layout_height=\"wrap_content\"/> The TextView will be displayed with the text “Default text.” even though we never wrote it in the Activity’s XML. Of course, if you specify a theme attribute like we just did but in your Views’s XML or style you override the same attribute, the overriden value will be used. Theme’s Style-Attributes Even though a Theme is mainly a big Style applied everywhere, it can also store some references, or style-attributes , to some others resources. Theses references can the be used by other styles or views to have a specific behavior defined by the theme. For example, in AppCompat themes you could find the colorAccent style-attribute. Let’s explain how I could define our own style-attribute like colorAccent . First, the attribute is defined in an <attr/> block (usually this block is written in res/values/attrs.xml ). <?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<resources>\r\n  <attr name=\"favoriteColor\" format=\"color\"/>\r\n</resources> This tells Android “OK, I’m defining a new style-attribute called favoriteColor , it will reference a color and any theme can specify the value of this attribute.” Now, in our App theme, I can specify this style-attribute like any other attributes: <style name=\"AppTheme\" parent=\"@style/Theme.Appcompat\">\r\n  <item name=\"favoriteColor\">#FF00FF</item>\r\n</style> And finally, this attribute can be used anywhere as long as this Theme is applied in the Activity. For example, in the xml of an Activity with the AppTheme , I could do <TextView\r\n  android:layout_width=\"wrap_content\"\r\n  android:layout_height=\"wrap_content\"\r\n  android:textColor=\"?attr/favoriteColor\"> Note the notation with ?attr/ , it means that the value is a style-attribute defined in the theme. If this attribute is not found in the Activity’s theme, the app will crash. The real power of the style-attributes is that they can refer to anything ! A color, a drawable, a dimen, an integer and even … another style ! Default Styles Let’s jump all the way to the beginning at the different constructors of the widgets because I actually never explained the third one. As you probably noticed, the second constructor is only calling the third one with a fixed parameter : R.attr.toolbarStyle . This 3rd parameter is actually … a reference to a style-attribute referencing a style resource that supplies default values for the view ! This attribute toolbarStyle is the Default Style of the Toolbar ! If you read the source code of AppCompat, you will see in any Theme.AppCompat (or its parent themes) the following reference : <style name=\"Base.V7.Theme.AppCompat.Light\" parent=\"Platform.AppCompat.Light\">\r\n  <item name=\"toolbarStyle\">@style/Widget.AppCompat.Toolbar</item>\r\n</style> And you can see the values of this default style on AppCompat source code . When constructing a View , the default style is used with the AttributeSet to resolve the final attributes of the view. Every attributes specified in the default style that are not present in the AttributSet will be retrieved from the default style. If an attribute from the AttributeSet overrides on defined in the default style, the value from the AttributeSet is used. Here is a couple of default styles from AppCompat with default value of some common widgets : TextView : ?android:attr/textViewStyle with default value in Material theme Widget.Material.Light.TextView EditText : ?attr/editTextStyle with default value Widget.AppCompat.EditText Button : ?attr/buttonStyle with default value Widget.AppCompat.Button Toolbar : ?attr/toolbarStyle with default value Widget.AppCompat.Toolbar Checkbox : ?attr/checkboxStyle with default value Widget.AppCompat.CompoundButton.CheckBox Customizing AppCompat Now that you understand how all of theses themes and styles are working, you are now ready to customize every widgets of your applications. You want a custom Toolbar all over your app without having to copy/paste 10 lines of XML in every Activity’s view ? You can just define a custom style that extends from the default Toolbar style and apply your new custom style to you application theme : <style name=\"SuperCustomToolbar\" parent=\"Widget.AppCompat.Toolbar\">\r\n  <item name=\"titleTextColor\">#FF00FF</item>\r\n  <item name=\"subtitleTextColor\">#00FF00</item>\r\n  <item name=\"background\">#222222</item>\r\n</style>\r\n\r\n<style name=\"AppTheme\" parent=\"Theme.AppCompat.Light\">\r\n  <item name=\"toolbarStyle\">@style/SuperCustomToolbar</item>\r\n</style> In your AndroidManifest.xml , apply this theme to your application <application\r\n ...\r\n theme=\"@style/AppTheme\">\r\n  <activity android:name=\".MainActivity\"/>\r\n</application> And in your activity_main.xml layout file, simply written <android.support.v7.widget.Toolbar\r\n  android:layout_width=\"match_parent\"\r\n  android:layout_height=\"wrap_content\"/> And VOILA ! Your Toolbar will be styled with your custom style. On a final note, I can only advise you to read the source code of Android and AppCompat to find what are the default style-attribute for every widget and to which style theses attributes refer to. You should probably have the source code of the support libraries available directly in Android Studio (cmd+click on a class name to go to the source) or you can browse it directly in Github here . Always look for the constructors of the different widgets, this will help you find the default syle-attribute then browse the AppCompat’s theme you are depending on to find the exact definition of the default style. Here is a quick how-to (finding the default toolbarStyle value) : via GIPHY If you want to read more about themes and style, you can read the official documentation or you can watch this talk from Google I/O 2016: Android themes & styles demystified Have fun customizing your AppCompat themes ! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-12-09"},
{"website": "Octo", "title": "\n                A Journey into Industrializing the Writing and Deployment of Kibana Plugins (riding Docker)            ", "author": ["Alexandre Masselot", "Catherine Zwahlen"], "link": "https://blog.octo.com/en/a-journey-into-industrializing-the-writing-and-deployment-of-kibana-plugins-riding-docker/", "abstract": "A Journey into Industrializing the Writing and Deployment of Kibana Plugins (riding Docker) Publication date 19/09/2016 by Alexandre Masselot , Catherine Zwahlen Tweet Share 0 +1 LinkedIn 0 by Alexandre Masselot (OCTO Technology Switzerland), Catherine Zwahlen (OCTO Technology Switzerland) and Jonathan Gianfreda. The possibility of custom plugins is a strong Kibana promise. We propose an end to end tutorial to write such plugins. But this “end to end” approach also means “how to continuously deploy them?”, “how to share an environment with seeded data?” Those questions will bring us in a full fledged integration infrastructure, backed by Docker. The Elasticsearch has grown from a Lucene evolution to a full fledged distributed document store, with powerful storage, search and aggregation capabilities. Kibana has definitely brought a strong component for interactive searching and visualization, transforming the data storage tier into an end user browser. Customizable dashboards via a rich library of graphical components made its success, but soon, the need for real customization arose. If plugins were thought to be integrated from early on, the actual customization often lied into forking the master project and adapting to on particular purpose. Merging back fixes was soon to be a daunting effort to keep up with the high pace of the Github repository evolution . Fortunately, as of version 4.3, the Kibana project took a more structured way to integrate custom plugins. The promise of maintainable external plugins became true. Those plugins, written in JavaScript, can be as simple as a standalone widget (e.g. a clock), a field formater (an up/down arrow instead of positive/negative number), a graphical representation of a search result (a chart) or a full blown application. So, that should be easy. Just google and you would craft wonderful shiny visualizations. But not fast, young Kibana Padavan! Documentation lacks, resources are valuable but scarce. But the promise is still shiny and we want to reach it. In this post, we propose to share our journey into the writing of Kibana plugins, the little pitfalls we fell in and the setup of continuous deployment into a Docker environment. There is no dramatic discovery or stunning breakthrough today, but a tentative to write a map to make your journey easier. The purpose of the Quest The goals was to meet the Kibana 4.3+ promise, to be able to customize the platform without forking the orignal code branch. Our *Definition of Done* was: we should develop different plugin types: independent widget, formatters, aggregation visualization. NB: at this stage, they don’t need to be pretty or even particularly meaningful; they should be resizable and offer the classic comfort of the Kibana experience; we should be able to continuously build and deploy them via Jenkins or such; we want to use Docker to run Jenkins, Elasticsearch and an integration Kibana instance; with Docker, we want the infrastructure to start with preloaded data and visualization, to allow user driven or automated tests; we want the plugin development itself to be as smooth as possible (short reload time when source code has changed); we want others to be able to reproduce the experience; finally, we want to be able to give a feedback on which extent the technology is mature for a full blown project. Figure 1: Elasticsearch is fed with 10’000 geolocalized tweets. We display a snapshot of the deployed dashboard with custom plugins. From upper left to lower right: a) a simple clock, untied to any data; b) the default Kibana date histogram, for time range selection; c) an aggregation visualization, where tweets are counted per country; d) a search results with a custom formatter, altering the color for #hashtags and @accounts. Wasn’t there any full detailed map available? If only there had been one, this whole effort would have been a straight tweet: “amazing tutorial on how to build and deploy #kibana plugins #elasticsearch http://wonder.land/build/your/own/kibana/plugins.” Obviously, none was found. However, there were inspiring sources of information. Although some might have been incomplete, slightly out of date or simply flying at a too high level, we cannot thank enough their authors for having put us on track: enlightening talks from the ElastiCon conferences in San Francisco, 2015 and 2016 . Way more blasting presentations are available there! the most comprehensive piece of documentation at the time and ubiquitously cited, a four parts post by Tim Roe . Although pretty descriptive, some information is missing, the committed code not fully working straightforwards (Kibana version?). The Journey log book For the impatient, head to Github , follow the instructions and run it on your laptop. The architecture To fulfill our quest, we need at least: Github repositories to host the plugins source code; a local development Kibana server, where plugins are refreshed upon saved; an Elasticsearch server, to store test data, and Kibana configurations; an integration Kibana, where packaged plugins are deployed canonically; Jenkins for continuous integration, to build those plugins and deploy them on the integration Kibana server. For the sake of isolating an integration environment, we propose in figure 2 a Docker container set with Jenkins, Elasticsearch and the integration Kibana, while the development Kibana instance runs locally. This is of course only an example setup, but we believe it is sufficient to back our point. We will now walk through this architecture, see how to deploy it and how the initial data (a list of tweets) and configuration (Kibana and Jenkins) can be seeded. Versions notes. As of writing, we used: kibana 4.4 & 4.5 Elasticsearch 2.3 Jenkins < 2 NodeJS 4.4 Docker 1.12 (both with VirtualBox 5.0.16 and Mac native) Docker infrastructure In this part, we will review how to launch a set of containers with the aforementioned services. If this part is rather classic, we will also see how we have populated each service with initial data, to offer the experience of an infrastructure ready to go . Docker is a powerful container platform to encapsulate lightweight containers. Perfectly suited for development, one can easily build upon pre-existing images (e.g. a Kibana v4.5.1 ), then custom them via DockerFile (e.g. tuning configuration). docker-compose brings the system even further, as it allows to generate a full set of containers linked by a private network, while some ports and volume are exposed to the outside world. In the age of micro services, we believe that lightweight container systems have deeply altered the developper’s life. The project presented here is a typical example of such an architecture. Spawning the infrastructure First, install Docker locally, see Docker documentation for instructions. To setup the continuous deployment environment for plugins development, clone the current repository: git clone https://github.com/alexmasselot/kibana-plugin-howto-infra.git\r\n   cd kibana-plugin-howto-infra The configuration for Elasticsearch, Kibana, Jenkins and their relative dependencies are specified in docker-compose.yml . Now the Docker container can be started: export DOCKER_MACHINE_NAME=kibanahowto\r\n   docker-machine create --driver=virtualbox --virtualbox-memory 4096 --virtualbox-cpu-count 2 --virtualbox-host-dns-resolver $DOCKER_MACHINE_NAME\r\n   eval $(docker-machine env $DOCKER_MACHINE_NAME)\r\n   docker-compose build\r\n   echo \"now start the machine and, when it's ready, open http://$(docker-machine ip $DOCKER_MACHINE_NAME):5601/app/kibana#/dashboard/kibana-howto-plugin?_g=(time:(from:'2016-06-17T10:30:12.574Z',mode:quick,to:'2016-06-17T10:36:14.545Z'))\"\r\n   docker-compose up What have you just done??? You have deployed an Elasticsearch server, a Kibana and a Jenkins servers for continuous deployment. Demo data is populated in Elasticsearch and a visualization is available in Kibana through default dashboard. To access the servers in the container, you can find the IP address of the docker-machine with the following command: docker-machine ip $DOCKER_MACHINE_NAME The different services are accessible here: http://my_docker_ip:9200 for Elasticsearch server http://my_docker_ip:5601 for a Kibana, or for a direct dashboard access: http://my_docker_ip:5601/app/kibana#/dashboard/kibana-howto-plugin?_g=(time:(from:'2016-06-17T10:30:12.574Z',mode:quick,to:'2016-06-17T10:36:14.545Z')) http://my_docker_ip:8080 for Jenkins continuous integration & deployment Allow a couple minutes for the data to warm up. Congratulations, you have setup the environment for continuous deployment of Kibana plugins! Populating initial data and configuration One of our goal is to boot containers with preloaded data and configurations: Elasticsearch contains 10’000 tweets with geographic coordinates. Kibana is available with default dashboard, searches, visualizations and plugins. Jenkins shows jobs ready to be ran. For each plugin, source code is pulled from Github repository, packaged and deployed to the Kibana server via ssh. We believe that only seamless integrations process have a chance of being adopted by fellow developers. Remember Larry Wall (Programming Perl, 2nd edition, 1996): “laziness, together with impatience and hubris, is one of the three virtues of a good developer. As mentioned earlier, the overall infrastructure is described in the docker-compose.yml file, while individual containers are instantiated via docker-containers/*/DockerFile . More information can be found on the Docker compose page. The only original part here is how we actually capture and populate initial data ElasticSearch (the tweets) As mentioned before, we want to populated 10’000 geolocalized tweets, to demo our plugins. Creating the set A set is already available in the containers/elasticsearch-initial-data/data/tweets.jsonl file. To create such a set, a tweet-download.js NodeJS script is provided. In this script, the Twitter client registers to a stream, filters messages with a location field, adds a timestamp field and appends each of them as a JSON object in the tweets.jsonl file. Edit and source a secret-env.sh file with your API keys. Refer to the Twitter API to discover more. Uploading the set into Elasticsearch The containers/elasticsearch-initial-data/entrypoint.js contains call to the Elasticsearch API . The only pitfall to avoid was to wait for the ES server to be up, and only populate the tweets, at start time, only if they are not already in. As the ES container can be started several times, we obviously want to push the data only once. This check is achieved via a couple of search call to the API. Kibana The first step is to install an initial version of each of the three plugins at building time. As those are not yet available through our continuous integration component, we cheat and install them directly from Github via the containers/kibana/entrypoint.sh . The second step is to set tweets as default the index, create the demo searches, visualisations and dashboard.This is done populating Elasticsearch indexes from a priori saved data, and is actually achieved by the ES data instantiation step. The last step is to import mappings, searches, visualization and dashboards, in order to have Kibana already setup when opening it the first time. As for the tweet data, those features are conveniently stored in a .kibana index in Elasticsearch. But here is a chicken and egg problem: we needed to create a dashboard, in order to save it, in order to download it. Initial configurations had to be built at once by hand. A script to save them is available: containers/elasticsearch-initial-data/dump/kibana-download.js . Jenkins Being file based, Jenkins configuration consists in deploying a snapshot. However, we face again another chicken and egg problem… We solved this configuration issue by mounting the /var/jenkins_home directory in Docker ( docker-containers/jenkins/data/jenkins_home ). Then, the Jenkins itself was configured, jobs added, plugins installed through the classic web interface as the change are persisted in /var/jenkins_home , thus in the git repository. The key point is to avoid polluting the Github repository with thousands of generated files (job runs, log, etc.) They were generously excluded via the .gitignore file. Why a chicken and egg pattern For both the dashboard and Jenkins, we follow the same pattern: set up the tool by hand (typically via its web interface); export the configuration and commit to git; re-import the saved configuration when deploying new instances; to modify the setup, go to 1. This approach can be challenged. Why not using ex nihilo setup creation? One could read the Kibana or Jenkins configuration documentation and build up the desired instance. This approach is encouraged, specially for Jenkins, where a documented API exists. Opting for a programmatic creation will ease collaboration of multiple developers. Moreover, merging generated and cryptic JSON configuration files can prove to be a challenge in itself… However, the chicken and egg pattern offered us a gain in time and flexibility. Writing custom plugins At last, we talk about plugins! Sorry for the impatient, but we had to set the infrastructure up first. Get the toolbox ready and start building seemed pertinent. We built three plugins above our tweet data, strongly inspired (when not shamelessly forked) by the four parts blog post by Tim Roes (see figure 1): a clock ( Github ), simply forked out from Tim’s, except for the packaging mechanism (in package.json ) leveraged with the other examples; a string field formatter ( Github ), which turns #hashtags and @accounts in different colors; a search result graphic visualization ( Github ), where the filtered tweets are counted by country, and a flag displayed with a relative size. This view is more complex than the previous one, using AngularJS and d3.js. As there is nothing particularly original compared to other available resources, we won’t dive into plugin intricacies. We will only point out some implementation hints that we found either lacking or unclear. The development process The good part about customizable plugins is that one should be able to… customize them. And preferably in a smooth way. One solution is to: modify the code; push modified code to Github; launch a Jenkins job to pull/package/distribute it; head to the Docker Kibana server to see the changes. This easily take a couple of minutes, which can hardly be quoted as “smooth” by the XXI th century JavaScript standard. The other solutions is to work locally (on the developper laptop): install at once Kibana and launch it in development mode; fork or clone at once the plugin source code into the `installedPlugins/` directory; modify the code; refresh the local browser to see the modifications; go back to step 3; only push the code to Github when a meaningful step has been achieved. The 3-4-5 feedback loop is way faster than the first method (even though refreshing the browser can take up to ten seconds.) A few practical hints Tim Roes and others have explained in great details the nuts and bolts of writing plugins. However, some information was not readily available to go further than the hello world stage. We propose here a few hints to avoid common traps in the journey of writing production ready plugins. Resizable components It seems obvious that components rendering should often adapt to their size.This is even more true with Kibana customizable dashboards. Although this feature is ubiquitous, little has been written. The common underlying library to build visualization components is the versatile AngularJS. The watcher mechanism allow to regularly watch the widget dimension and redraw when needed. Even though this solution is often proposed, it only partially works. When moving around and resizing a full dashboard, the watcher function can be called too many times simultaneously, leading to exception generations. Beside being a poor programming pattern, the user experience can be impacted. An more elegant solution is to listen to some resized event, but little information was available. The solution can from diving into Kibana source code and locate the change:vis event. Then, the implementation comes to (if render is your actual component rendering function): rootScope.$on('change:vis', render); You said “AngularJS”? Basing plugin code on AgularJS certainly is a maturity choice and one could be attracted by recycling prior knowledge on the framework. If it can be seen sometimes as an inconvenient, AngularJS is strongly opinionated, splitting an application into functional modules, organized in controllers, factories, services and directives. Above this architecture, its success is certainly correlated to a versatile ecosystem and the easiness, for example, to isolate components and write tests. Unfortunately, most of the popular examples we found on Kibana plugins do not take advantage of the proposed split of concerns and add too much functionalities (such as rendering) into the controller. However, nothing seems to intrinsically make a proper AngularJS decoupling impossible. Our conclusions are only based on this experience, where we may have left the battle too early. Packaging a plugin Packaging a plugin consists in building a deployable .zip archive. Although several methods are proposed by various authors, we converged towards the @elastic/plugin-helper module, allowing npm run build . Once the zip archive is packaged, it can either be made available on a url (Jenkins published artefacts) or copied to the kibana server. The plugin deployment itself is achieved by sshing onto the server and executing a /opt/kibana/bin/kibana plugin --install ... To have more specifics about those command, the easiest way is to head to Jenkins an open the configuration of one of the kibana-plugin-*-deploy jobs. So, shall we use customizable Kibana plugins? Or “shall we write an independent classic rich web application, backed by a REST API on top of ES?” The short answer is: “yes and no.” The Elasticsearch+Kibana stack certainly deserves its success. And a huge part of it is due to the versatility of Kibana visualization, with default and community plugins already available. Pushing the dashboard further on seems natural. Then, turning the default exploration tools into an open production front end seems an appealing and inexpensive solution, compared to writing a full rich web client from scratch. This choice can make sense up to a certain limit. Customization comes to often underestimated costs: the major one is the Elastic development velocity. It goes fast, and architectural changes (minor or major) are often not backwards compatible. Kibana 3 & ES 1.7 have been adopted at large and the pace of changes is hard to follow for developers. Elastic 5 coming down the corner will keep us rocking for sure. As a corollary to the previous point, if the stack is not mature, resources are scarce and answers often diverging. The plugin development itself, even as presented in this article is not radically smooth. In development mode, the lead time for a minor change ( “just modify this css width”) to reach the screen takes a few (easily up to ten) seconds and makes the process cumbersome compared to today standards, where we are more used to have screens refreshed within a second. Tying up a whole project architecture to the Elastic perspective can make sense if we are ready to follow all their choices and willing to pay extraordinary prices for customization (stepping sideways for authentification, for example) Those costs taken into account, it may often not seem totally unreasonable to head towards an independent front end development, backed by classic stack such as AngularJS, or even better for the topic at hand, ReactJS + flux application. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “A Journey into Industrializing the Writing and Deployment of Kibana Plugins (riding Docker)” Juan Carniglia 04/11/2016 à 14:00 I have found your article very comprehensive and interesting. I'm doing Kibana Plugins myself, for Kibi mainly, and I wanted to let you know I have created a group on LinkeIn (\"Kibana Plugins\") where a few of us who are on the same page can get together and discuss over all of this.\r\n\r\nYou are more than welcome to join! Thanks! Canon Support 26/10/2018 à 07:08 these plugins are just awesome and you have shared all your ideas very nicely. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-09-19"},
{"website": "Octo", "title": "\n                Protocol Buffers: Benchmark and mobile            ", "author": ["Sandra Dupré-Pawlak"], "link": "https://blog.octo.com/en/protocol-buffers-benchmark-and-mobile/", "abstract": "Protocol Buffers: Benchmark and mobile Publication date 27/07/2016 by Sandra Dupré-Pawlak Tweet Share 0 +1 LinkedIn 0 Going faster on a mobile has become essential. Putting aside the actual choice on the means of communication, the data format used weighs in quite a bit when it comes to the speed. As of Today, JSON has proven to be a standard media for APIs. Still, is this data format suitable for mobile? For instance, handling JSON in an Android environment is difficult. Other data formats are emerging in recent years like Thrift, Avro, Message Pack or Protocol Buffers. Protocol Buffers has the ability to have a binary format that is easily adaptable and which can be manipulated. It also has a very basic structure to write and understand and easily generate source code for several languages. This blog featured two articles on Protocol Buffers (protobuf) in 2012. The version used was 2.4.1, and the standard was proto2. Evolution Protobuf development began in 2001 and the latest stable version was released in October 2014. This time around, the beta-3 3.0.0 will be discussed (Edit (01/08/2016): V3 was finally released!). With this beta, standard proto3 appeared. The purpose of this standard is to simplify a little more protobuf structure declaration files. message Hello {\r\n  message Bye {\r\n    required string name = 1;\r\n    optional int32 count = 2 [default = 1];\r\n  }\r\n\r\n  repeated Bye bye = 1;\r\n  optional bool check = 2;\r\n} message Hello {                            \r\n  repeated Bye bye = 1;\r\n  bool check = 2;\r\n}\r\n\r\nmessage Bye {\r\n  string name = 1;\r\n  int32 count = 2;\r\n} proto2 proto3 Among visible changes between proto2 and proto3, we note the disappearance of attributes (“optional” and “required”). The attribute “required” was already not recommended by Google: it prevents subsequent suppression of a field in the object future versions. Indeed, delete, edit or add a field “required” brings reading bugs of the protobuf data. In addition, now, default values can not be changed. The first argument in favor of these changes remains the simplification of the objects. The second is the protobuf extension to other languages, which they do not necessarily accept the defaults values. This choice allows to standardize the generation in present and future languages. Nevertheless, the proto2 standard is always accepted by the Protocol Buffers generator. In its version 2.6.1, Protocol Buffers can be declined in four languages: C ++, Java, Python and Go. Currently, the 3.0.0-beta 3 version includes new variations. C # and Objective-C were added in beta. JavaScript, Ruby and Javanano are currently in alpha. Protocol Buffers is BSD licensed. Everyone then has the opportunity to participate in its evolution. This allows quick expansion. We can now find a lot of protobuf generators for many different languages. This list contains some of the existing implementations. Benchmarks To visualize Protocol Buffers performance, a client (Nexus 5, Android 6.0.1) calls a server (Go) to transmit text content. The call is made locally via HTTP/2. Tests Environment To compare results, other data formats are used: XML , JSON and MessagePack . For JSON, three libraries are tested: Jackson , Jackson Jr and Moshi . In Protobuf side, there are two libraries used: the official implementation of Google (bêta-3.0.0) and Square Wire (2.2.0) Processing Time Processing time is the time between end of packets reception and the moment to send the final objects for display. The client requests the server for 10 files of varying numbers of subsections (50 to 500). These subsection are randomly selected so, the data received are different everytime. Each request is repeated 50 times for each number of paragraphs to get an average. The “parser” initialization time (whether for JSON, MessagePack or Protobuf) which occurs in the first round, is diluted. This choice is made to approach reality: the “parser” object is set on the first turn and is reused throughout the application usage. Benchmark Results: ms time by the number of subsections by file First finding already known XML is totally disqualified. For readability, we remove XML in the schema. Results: without XML – Benchmark Time in ms by the number of subsections by file For other formats, processing time is already superior than Protocol Buffers. For gzipped JSON, the processing cost is multiplied by 1.2 compared to JSON. The factor between JSON (with Jackson, the lowest) and Protocol Buffers is almost 2. The conclusion is, for processing time, on text data, Protocol Buffers is clearly to his advantage. Memory Regarding the memory cost, we see quite easily Protocol Buffers consumes almost as little as Jackson for JSON. Weight At the compression of the initial data, we can see that the binary protobuf is lighter than MessagePack or JSON gzipped. Android Despite these encouraging results, the establishment of a new solution may seem perilous. For Protocol Buffers, it is relatively simple. Take Android as an instance. We have the choice between the Implementation of Official Google (IOG) and Square Wire version. Beyond efficiency, compare several other points is necessary. Code Generation: Two ways to generate your code: command line or pre-build. Command line: For IOG, the latest version package is available and contains an executable: protoc --java_out *.proto For Wire, a jar is also available . java -jar wire-compiler-2.2.0-jar-with-dependencies.jar --java_out =. --path_proto =. *.proto Pre-build: On the IOG side, it’s not possible for the proto3 standard. A Gradle plugin exists but requires gradle 2.12 and includes only proto2. Wire is more effective in this regard. Square officially has a Gradle version of Wire plugin , but the repository is not updated since a year. Moreover, there is no tutorial on how to use it and it cannot be found in the official repositories of jcenter or mavenCentral. One of the forks solves these problems: the plugin Wire Gradle by Jiechic. Then, establishment is then very simple: In project build.gradle classpath 'com.jiechic.librairy.wire: wire-gradle-plugin: 1.0.0' In app build.gradle: apply plugin: 'com.squareup.wire'\r\n dependencies {\r\n compile 'com.squareup.wire: wire-runtime:2.2.0'\r\n} Then simply place the proto files in src/main/proto. At compile time, objects will be generated in build/generated/source/proto. Generated Objects: Square wins. The generated files by IOG are unreadable and repulsive. The Square ones are much simpler to understand. In addition, the generated objects are also simpler to use. Encode / Decode: In my case, only the part ‘decode’ of each version was used. The syntax is simple. For example, the structure is: syntax = “proto3”;\r\npackage hello;\r\noption java_package = “com.sdu.testprotoreceive”;\r\n \r\nmessage Hello {\r\n  string name = 1;\r\n} IOG Version: HelloOuterClass.Hello hello = HelloOuterClass.Hello.parseFrom(byteArray); Wire Version: Hello hello = hello.ADAPTER.decode(byteArray); The objects generated by Wire is simple, they are also more natural to use. Other languages The tests server is in Go. Objects generation from the proto3 file is: protoc –go_out=. *.proto The command is generic a language to another, which makes very easy to use. To interpret generated files, the library is not on the same than other languages: go get -u github.com/golang/protobuf/protoc-gen-go\r\ngo get -u github.com/golang/protobuf/proto Example of use: func sendProto(w http.ResponseWriter, r *http.Request) {\r\n  hello := new(hello.Hello)\r\n  hello.Name = “Marie”\r\n  hello.Number = 32\r\n  w.Header().Set(\"Content-Type\", \"application/x-protobuf\")\r\n  helloCompress, err := proto.Marshal(hello)\r\n  if err != nil {\r\n    log.Fatal(\"Compress Problem (Marshal error)\")\r\n  }\r\n  w.Write(helloCompress)\r\n} The last tested language is C#. Again, it is easy. public static void Main (string[] args)\r\n{\r\n WebRequest wrGETURL = WebRequest.Create( \"http://xx.xx.xx.xx:8000/\");\r\n Stream streamHello = wrGETURL.GetResponse().GetResponseStream();\r\n Hello hello = Hello.Parser.ParseFrom(ReadFully(streamHello));\r\n Console.WriteLine (hello.Name+\" \"+hello.Number+\" \"+hello.Bool);\r\n} Here, the function ReadFully is only to turn Stream in byte array. Implementation Implementation: repository , Go Client , Go Server , Android Clients A Playground on Github allows you to see an implementation more real. A Go repository contains the protobuf file and an application to generate objects in different languages. A Go server find in this repository a Go object, a Go client did the same. The Android Wire client find protobuf file and generates its objects in pre-build. The Android IOG client retrieves the jar file created by the Go repository. Conclusion Objects generation for Java, Golang and C# was done in three command lines with one protobuf file. Objects use is very similar in language tested in this article (Android, C# and Go), and looks like decode functions for JSON. For a text content, on Android client, Protobuf has several advantages such as simplicity (“build”) or the gain in processing time ( “run”). Substitute JSON may even be advisable in a new project. It should nevertheless think that some tools are little or not updated (as plugin gradle by Square). The V3 is trying to simplify use and expand the user fields, with more new languages. The binary apprehension, using new formats and JSON in place since a long time curb changement. This, I think, is the reasons which leaves Protocol Buffers little used. References: Official Documentation Official Library Square Wire Wire Gralde Plugin by Jiechic Official Go Library Wikipedia Sérialisation Thrift et Protocol Buffers Thrift Protobuf Compacite Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-07-27"},
{"website": "Octo", "title": "\n                DockerCon 2016: “Nobody cares about containers!”            ", "author": ["Sebastian Caceres", "Thomas Pepio"], "link": "https://blog.octo.com/en/dockercon-2016-nobody-cares-about-containers/", "abstract": "DockerCon 2016: “Nobody cares about containers!” Publication date 27/06/2016 by Sebastian Caceres , Thomas Pepio Tweet Share 0 +1 LinkedIn 0 docker run busybox /bin/echo ‘hello boys and girls!’ As humans, we like new shinny things. And working in a wannabe devops world, that means solving problems with containers , too. We have been working with them for quite a while now, and we are more than happy with the results. We like them so much, that we decided to travel more than 8000 km, to Seattle, Washington, in order to go to this year’s DockerCon . So if you’re into containers as well, and you’re dying to hear the hot news, you’ve come to the right place. Fasten your seatbelts, and enjoy the ride! Docker and the future: What’s coming up? Let’s talk about numbers first: During the first keynote, Ben Golub (Docker CEO) was all about exposing Docker’s exponential growth in every single aspect: from DockerCon’s size (five hundred attendees in 2014 vs four thousand in this year’s edition), to the amount of containers downloaded up until the conference (4.1 billion). If that isn’t impressive enough for you, then check this out: 986 persons have interacted with the repository, with a 300 pull requests per week on average: median time to process is less than one day mean time to process is 6 days 90% of them are processed within 20 days of submission 800 paper submissions for this year’s DockerCon ⅔ of contributions to the Docker Engine are external 250+ meetups held all over the world, with more than 125k+ participants… Anyway, you get the point: Docker is pretty big nowadays. Docker development is driven by its userbase needs . This means that most of its new features are either solutions to common community-wide problems or tools intended to be catalysts for enterprise adoption. Based on this feedback, the guys at Docker came up with three main topics for this year’s keynotes : A seamless user experience from the developer perspective Built-in orchestration Integration with operations-oriented cloud resources such as AWS and Azure We’ll discuss each one of these subjects right away. One order of coolness with fries, coming right up! Developer’s experience: Introducing Docker for Mac and Docker for Windows. Solomon Hykes said it himself: “ nobody cares about containers” , and he’s right. Docker’s main focus is to make life easier for everybody, both for developers and operations teams, by eliminating friction in the development environment. Let us all become developers for 30 seconds here: I don’t want to install 27 different tools before I can start coding stuff. Basically, I need a tool that helps me, while getting out of the way at the same time. I don’t want to be worried about the internal gears of this thing : simplicity is the key . This wasn’t the way Docker used to work on OS X. It wasn’t simple . At all. First you had to install VirtualBox and then rely on a Boot2Docker VM, since OS X isn’t Linux, right? So you were running the Docker client natively on your Mac, but your Mac wasn’t the Docker host: Boot2Docker was. This process got simplified with the creation of Docker Toolkit, which was a single package that contained the Engine, Compose, Machine and Kitematic. It was an improvement, you were still a bit uneasy about all these different tools in your machine. Why do I have to `eval $(docker-machine env seb_cool_vm)` each time I want to work with Docker? Graaaahhhh. You could even see it on the official documentation What about now? Well, you can just go ahead and install Docker for Mac , which runs as a native Mac application , and uses xhyve to virtualize the Docker Engine environment and Linux kernel-specific features for the Docker daemon. There’s similar support for Windows as well. So you’re still using an hypervisor to run Docker on your machine , but you don’t have to interact with it, or worry about it at all. This is seamless . It helps you, then it gets right out of your way. So how did they pull this off? They hired a lot of people, both directly and by acquisition. They bought Unikernel Systems , for a start. They also hired a lot of people from the videogame industry, as they were “five years ahead of everyone else” in terms of exploitation of IT systems. Docker’s built-in orchestration Basically, Docker is a tool we can use in order to build and run immutable “boxes”. That way, you can ship applications without having to worry about the deploying environment, since everything that the application needs in order to run already exists inside the container environment. That’s already remarkable. Still, application deployment is a recurrent issue. It is usually painful. How do I deploy an application that needs to be replicated multiple times? How do I notify my load balancer when scaling up or down? How on earth can I update the application version inside the containers, without downtime? You could find answers to these questions with systems like Kubernetes, Mesos  running Marathon (even if it’s not container-oriented by design) or Docker Swarm. They all fall into the “container orchestrator” category, and we also use the CaaS (Container as a Service) acronym to describe them. Using a Swarm cluster you can deploy containers by notifying the whole cluster that it has to run them. Combine that with Docker Compose, and you can reduce the deployment effort drastically. Up until now, you still had to deal with load balancing notifications or perform all the rolling updates by yourself. That was tedious and complex, and as Hykes himself said it: “[this] is an expert thing”, meaning that you could either hire an army of experts, or get another company do it for you. That’s no longer the case with Docker 1.12 and its built in orchestration. The main goal of this release is to make orchestration really simple and manageable for everybody: Swarm mode: Every Docker Engine can link up with another engine in order to form a swarm, and therefore self heal, run containers and attach volumes, all in a distributed and fault tolerant way. It is also decentralized in the sense that any node can become a swarm manager or a node at runtime. No external consensus datastore is needed (no need to install etcd). TLS/Cryptographic node identity: Each node that joins the cluster has his own certificate, generated by a Certificate Authority inside a Swarm Manager node, with a random ID, and their current role in the swarm. Every node in the swarm is therefore identified with a cryptographic key during his participation within the cluster. Any action within the cluster can be traced back to the node. And each node also rotates his certificates, so compromised certificates are no longer valid after a rotation. That’s literally PKI out of the box. Docker “Service” API: You can now tell your swarm the desired state of a service , and then just let the magic work. A swarm cluster will handle rolling updates, scaling, advanced scheduling, healthchecks and rescheduling on node failure. Routing mesh: Overlay networking, container native load balancing, DNS based service discovery, all with no separate cluster to setup. It also works with your existing load balancers and rock solid kernel only data paths with IPVS. You can see that there are new abstractions, like services for example, that already existed in other orchestration tools, like Kubernetes. It’s good to see that the whole “container orchestration” ecosystem is converging to common abstractions that make sense. All of these features were demonstrated during the keynotes, but we setted up a quick demo so you can try it yourself. We will be creating a 3 node cluster using Docker Machine 0.8 . First, create your 3 nodes using docker-machine: docker-machine create -d virtualbox swarm-master docker-machine create -d virtualbox swarm-node-1 docker-machine create -d virtualbox swarm-node-2 Note the ip address of the manager node: docker-machine ls NAME           ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER        ERRORS swarm-master   –        virtualbox   Running   tcp:// 192.168.99.100 :2376           v1.12.0-rc2 swarm-node-1   –        virtualbox   Running   tcp://192.168.99.101:2376           v1.12.0-rc2 swarm-node-2   –        virtualbox   Running   tcp://192.168.99.102:2376           v1.12.0-rc2 Now, access your swarm node: docker-machine ssh swarm-master And then you just have to initialise your swarm: docker@swarm-master:~$ docker swarm init Swarm initialized: current node (1uj1fqtzz7whgz97mbwcpiki8) is now a manager. Afterwards, join the swarm cluster with the swarm nodes: docker-machine ssh swarm-node-1 And then: docker swarm join $MASTER_IP_ADDRESS:2377 docker@swarm-node-1:~$ docker swarm join 192.168.99.100:2377 This node joined a Swarm as a worker. You can follow the same procedure for swarm-node-2… and that’s it! You now have a working swarm cluster. You can verify it from your manager instance: docker@swarm-master:~$ docker node ls ID                           NAME          MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS 1uj1fqtzz7whgz97mbwcpiki8 *  swarm-master  Accepted    Ready   Active        Leader 2mryx3xbz7z7krhdaqtq5o8lu    swarm-node-2  Accepted    Ready   Active 9lwaccv0qezyki3ho4rtwcw6j    swarm-node-1  Accepted    Ready   Active Let’s run a service on our newly created cluster: docker@swarm-master:~$ docker service create –replicas 1 –name nginx -p 8080:80 nginx:1.10 cyhu3cztvjs4llpj7ewy9pobh Now we can see the nginx welcome page on “ http://localhost:8080 ” on any node in the swarm (note: according to our tests this feature is not stable yet). Awesome, right? Right now our service only is running on one node, but we can scale it up by running: docker@swarm-master:~$ docker service scale nginx=3 If you run a `docker ps` on either swam-node-1 or swarm-node-2, you will see that a container is running on each one of them. The nginx welcome page will now be served alternatively by each container. By doing this you specify your desired state, which will be enforced by Swarm from now on. Now let’s assume we want to update the version of nginx to 1.11, we simply run: docker@swarm-master:~$ docker service update –image nginx:1.11 nginx This command will perform an update on all three nodes simultaneously, and will cause a small downtime. If you want to perform a rolling update instead you just have to add the `–update-parallelism=1` flag (which specifies the number of containers that are updated at the same time) and the `update-delay=2` flag (time interval between two updates). Swarm mode also reschedules your containers on node failure. For instance, if you shut down swarm-node-1, you will only have two nginx containers running on your cluster, when you actually specified that you wanted three of them when you scaled your service. Therefore, the engine will launch a new container on either swarm-master or swarm-node-2 in order to abide to your desired state. This is pretty much the definition of a tool that “gets out of your way”. Note that Docker for Mac/Windows already uses Docker 1.12. The beta is now open, so go ahead and try it ! Last but not least, Docker announced a portable format for multi-container applications: the Distributed Application Bundle (or DAB). It is built from a compose file. Once built, you just have to pass it to your swarm cluster and everything’s taken care of automatically. This feature is still in experimental state, but the documentation is already out there , in case you’re feeling like reading something. Operations experience: integration with AWS and Azure Docker also wants to improve the experience of operations teams by providing a better integration with widely used cloud providers, starting with AWS and Azure. This is currently only available on beta, and consists mainly of templates of VMs (AWS Cloudformation and Azure Resource Manager) which can be used to instantiate a cluster on either of these platforms. The goal is to make all cloud provider specific configurations “transparent”: none of the Docker CLI commands must change. For example, in AWS a service is accessed through an Elastic Load Balancer in front of your applications. This ELB needs to be aware of every backend he has to serve: each time an application scales up or down, the ELB must be notified. Docker integrates with AWS in such a way that when you run `docker service scale <service_name>=x` , the ELB is automatically reconfigured, and is therefore capable of redirecting traffic towards these new endpoints. We don’t know exactly how much time is required by the ELB to reconfigure itself, since we haven’t tried it yet, but it seemed to work quite fast during the demo. Access Control Lists are also automatically generated. The same kind of features are available for Azure too. Docker in the corporate world: the need for security and simplicity A recurrent message of this DockerCon is that Docker is for everybody, from startups with greenfield applications, to big non-tech oriented companies with legacy infrastructure. It’s true that Docker makes the deployment of stateless microservices easier (“one process per container”), but it’s also very true that you can benefit from Docker’s immutability and workflow when dockerizing your monoliths. You can also get used to the technology while at it. That way, you can make the transition from a monolithic application to a service-decoupled one in a progressive fashion. In the past, big non tech companies had concerns about vulnerabilities in images pulled from the Docker Hub, about the difficulty of migrating to Docker when you operate at the scale they do (thousands of apps). This caused a drag in the adoption of the technology, and to address these concerns Docker plans to release: Universal Control Plane: This actually has been around for a while, but now it provides a better integration with the trusted registry and improved features. It’s basically a GUI on top of a Docker cluster. It centralizes the logs, does metric aggregation and shows resources utilisation. It has multitenancy and group based access control. It’s also capable of deploying DAB bundles which were introduced earlier. Trusted registry: This has been around for a while too, but now images pushed on the registry will be automatically scanned for vulnerabilities against a database maintained by Docker. This way you will get notifications if you have security issues on your images. Hooray! Be careful though: even if don’t have any vulnerabilities on your images, you can still have security issues within your architecture. Docker Store: This is actually a public store of images digitally signed and checked for vulnerabilities by Docker, much like the App Store or Google Play, but intended for Docker Images. We have yet to see if all these new features will increase Docker’s utilisation in the enterprise world, but so far they seem pretty well oriented and functional. Final thoughts DockerCon was a great experience. It felt good to meet the american community, and to see that it is growing at a fast pace. It’s good to see that Docker is really listening to its community. Thank you to all the staff at Docker who pulled this off: all your hard work clearly paid off, and was really appreciated by all of us. Thanks for the party at the Space Needle too! We had a blast! So, in short: What we loved: All the orchestration giants converge toward similar abstractions. Docker’s vision about the seamless experience. Deploying a Swarm cluster is really really easy today. This is actually encouraging competitors to reduce the complexity of the deployment of their tools . For us, it’s a win-win scenario. Docker Inc. really listens to its community. The support to Hack Harassment . We took the pledge, we encourage you to do it as well. What we didn’t like: We were excited about swarmkit before the DockerCon, and we didn’t hear a word about it during these 3 days. We don’t really know if it’s been integrated to the Docker Engine like Swarm, or if it has been dropped completely as a project. Huh. (Edit 28/06/2016: This point was clarified by Solomon Hykes himself in the comments below. Check his response if you’re curious too.) We will also release a second article containing our analysis of the sessions we saw this year during the conference. Stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “DockerCon 2016: “Nobody cares about containers!”” Solomon Hykes 28/06/2016 à 07:38 Thank you for such a thoughtful writeup! It's a great encouragement for the team to see their hard work appreciated.\r\n\r\nTo your question about SwarmKit: it is alive and well! It is a key system component under the hood of Docker 1.12.\r\n\r\nWe chose not to mention it in the keynote to avoid confusing the audience with too many different uses of the word \"swarm\". We focused on one simple thing: Docker now includes a state-of-the-art orchestration engine, with all the benefits of a pluggable architecture and huge ecosystem.\r\n\r\nUnder the hood, we have split up the platform in many discrete components: if Docker is the porcelain, then they are the plumbing. SwarmKit, Containerd, HyperKit, runc, libnetwork, notary, to name just a few. You can find them all at https://github.com/docker . They are designed to be easily reused by projects other than Docker. Sometimes even competitors reuse Docker plumbing and contribute back. We do the same with theirs. It's a well-understood convention that we all benefit from helping each other out on open-source plumbing while competing on the porcelain.\r\n\r\nWe mentioned those in the community and blackbelt sessions, where most of the \"plumbers\" hang out.\r\n\r\nI hope this helps answer your questions. We hope you enjoy the release. We appreciate bug report you'll send our way! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-06-27"},
{"website": "Octo", "title": "\n                USI Mobile Applications: A success in figures            ", "author": ["Guillaume Lagorce"], "link": "https://blog.octo.com/en/usi-mobile-applications-a-success-in-figures/", "abstract": "USI Mobile Applications: A success in figures Publication date 22/06/2016 by Guillaume Lagorce Tweet Share 0 +1 LinkedIn 0 The 9th USI Conference organised by OCTO took place this year on June, 6th and 7th. If you do not already know what USI is, you should have a look at www.usievents.com and the great video talks on their YouTube channel ( https://www.youtube.com/user/usievents ). As USI lovers,  OCTO Mobility team cared to deliver the best experience to all attendees providing them good-designed, well-crafted mobile applications. It is now time to have a deeper look into usage analytics during these two amazing days at the Carrousel du Louvre. User analytics All analysis are conducted on both Android and iOS platforms. During the conference 1 303 unique users opened the applications, generating 13 299 sessions and 161 893 seen screened. It represents 85% of the 1500 attendees. Pretty huge, isn’t it? Using Google Analytics, we were able to see the number of users through the whole conference duration. This led to following chart. (Data are aggregated by hour) We can see that the maximum number of users was reached on the first day at 9.00 AM, when 844 attendees used the application. The 800 users line has been crossed several time during the event, showing a real user engagement. This engagement is also revealed by the average number of sessions per user: 10 sessions/user. To gain a deeper understanding of how users spent there time on the apps, let’s have a look at the most seen screens. It comes as no surprise that the Agenda is the most seen screen by far, in front of Map and Session screens. The About and Fullscreen Map sections represent less than 3% of the whole navigation. As the social network subscription buttons (Twitter, mail and YouTube) were only available from the About section, these features have not been so often used, as seen on the following table. Twitter button has only been hit  42 times , Youtube button 35 times and Mail button 21 times . The apps. did not impact the USI network visibility which comes as a disappointment to us. Technical analytics Integrating Fabric.io SDK in the application allowed us to track and analyse overall stability. This led to following chart for Android. And this chart for iOS. As we can see, very few users have been impacted by crashs. With only 57 users impacted by crashs , the crash-free users’ rate is really high, meaning the applications stability did not impact user engagement. Failure or success? With a very large amount of attendees connecting about 10 times to the application during the whole event, and very few of them impacted by crash, the USI Mobile Applications were definitively a success. With all the usage data gathered during those two days, and the real user engagement encountered, the OCTO Mobile team is motivated and committed as ever to deliver an even better User Experience for what will be a legendary 10th USI conference ! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-06-22"},
{"website": "Octo", "title": "\n                Immersive Experience app using VR holds productivity promise            ", "author": ["Vincent Guigui", "Muriel Caron"], "link": "https://blog.octo.com/en/immersive-experience-app-using-vr-holds-productivity-promise/", "abstract": "Immersive Experience app using VR holds productivity promise Publication date 15/06/2016 by Vincent Guigui , Muriel Caron Tweet Share 0 +1 LinkedIn 0 Ah, the old office directory: page after page, or screen after screen, of names and job titles. Wouldn’t it be great to have a virtual directory that displayed employees’ faces, included the company hierarchy, and grouped employees according to responsibilities and expertise? Better yet, what if that directory could be navigated by using natural hand gestures? By combining the power of the Microsoft Kinect sensor and the Oculus Rift virtual reality (VR) headset, we have created just that – a VR company facebook that lets users traverse the company’s human resources and find the right employee with a flip of their hand. Our Immersive Experience developers Muriel Caron and Vincent Guigui used BabylonJS , an open-source 3D/VR web framework, to display a VR representation of the company directory in which employees are grouped by expertise, skill, or interest. The resulting display is a floating image of the company’s workforce, as shown in the video below. The head-tracking capabilities of the Oculus Rift follows users as they look in any direction of the display, while the body-tracking technology of Kinect for Windows allows users to grab or release employee entries by moving their hand forward or backward. “ Mixing technologies which were not made to work together was a bit of a challenge ,” says Vincent. “ We had to devise solutions to properly track user movements using Kinect, feed them into a VR environment created using web technology (everything is running inside the Internet Explorer web browser), then display it inside the Oculus Rift headset. All with a very low latency, to reduce the motion sickness often encountered during VR experiences .” Challenging to be sure, but the OCTO team succeeded in creating an immersive world that feels real and where the movement and position of objects seem consistent with the user’s point of view and actions. Moreover, Vincent sees this as just one example of the many ways that we can use VR in productivity applications. “ VR will unleash productivity ,” he predicts, “ freeing users from the limitations of common displays and interfaces, letting them focus solely on the information and the objects they want to interact with. Think of an employee having any number of virtual screens floating above her desk, or even following her while she moves from one room to another. Those virtual screens could be any size or shape and have dynamic opacity” . OCTO is proud to let you know that our team is already working on these future immersive experiences, using mixed-reality devices like HoloLens to break or bridge the boundaries between realities, allowing physical and digital entities to coexist and interact. This article was originally posted on Microsoft Kinect for Windows blog Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Digitalization and tagged Immersive Experience , innovation , Kinect , Natural User Interfaces , NUI , User experience , UX , Virtual Reality , VR . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-06-15"},
{"website": "Octo", "title": "\n                Cloud Ready Applications            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/cloud-ready-applications/", "abstract": "Cloud Ready Applications Publication date 30/05/2016 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 Days of traditional servers are counted. Flexibility of cloud platforms towards traditional datacenters is the root cause of the shift. For a developer, the question is no more where will be my application deployed but on which cloud platform. And designing an application in the cloud is more complex than on premise. As developers we must be ready for that, we must write code ready for the cloud. But, what is exactly a cloud ready application? Is it a one push action to Heroku? Yes, the Cloud can make life easier for the developer. But in order to unleash all its benefits you must design your code for the cloud. And this article aims to explain you the why and the how through 3 highlights of the shift of applicative architecture from the on-premise to the cloud world. The fork lift time Majority of experiences on the cloud begins like that: you carry your application like a pallet from the datacenter to the cloud. You are building an application and when comes the time to deploy it, flexibility of the cloud makes it the natural choice: startups see the benefit of no-upfront cost and larger companies see the benefit to externalize activity that is not a key differentiator. And, as developers, we see benefits too: easier to get an environment (at least in term of provision time) and smart automated stuff to deploy quickly your application as it is. Lots of applications go to the cloud this way, with a fork lift approach , by pushing the artefact on that new kind of server. Evil is in the detail when we try to copy/paste something Things start to get harder when we have to go into details. A lot of configuration is required as with an on-premise server. But cloud environment comes with some hypothesis in mind. In brief you start to think “cloud is not designed for real enterprise application” or “On the cloud there are more Ops stuff than on premise”. More and more services exist, but more and more configuration has to be done. First devops opportunities Fork lift is not something to avoid. It brings some immediate value. It can be the opportunity to discover the cloud and to start pushing your application forward. One important concept of the cloud is automation. Most of the time, cloud migration must come with an infrastructure as code approach. As a developer, it can just look like another operation action. But devops approach is much more than just infrastructure as code. In order to provide a seamless automation from code commit to code in production, dev and ops need to work together. Let’s see two examples of software architecture best practices that will make ops love your work and help them automate the installation. Smart configuration Configuration value, database password for the most common one are an old subject of synchronization between dev and ops. Let the ops automate their deployment and they will be very happy with their JSON config file! Things are not always so simple. For example as a developer you want to organize your code according to convention and in a way that help you work smoothly. Ops have the same need and they probably want to split you super JSON files into environment variables. Or they have their big file and totally ignore 80% of your file. The best way to build a smart configuration that fits with the automation process is to work with the Ops from the beginning, making the architecture, design and convention choice together. Here are some questions that may help you segregate your configuration data in a way that may help you work with Ops: Ask yourself what is the lifecycle of each piece of data that you name configuration. Do they change from one environment to the other? Do you like to let them change after seeing your program run (all tuning parameters should be in that case)? If they don’t it is probably not configuration from an operation point of view. It can be business or development configuration but not configuration for runtime. Database storage can be a choice for business configuration (configuration whom ops don’t care). For configuration data identified, externalize them in a way that they are readable and easily writable in the package of your application. Documentation and convention are key for that. By default choose files to store configuration and make their location customizable by an environment variable. Health check URL Another improvement of your application is its ability to be monitored. With one to three servers by application in a plain old deployment, the system administrator knows the application, its dependencies, how to analyze the logs to check its state. In a cloud world, the ops job has changed. Through automation one person can administer hundreds of servers. Your application is necessarily unknown and diagnostics have to be done through pre-defined metrics. In a near future, diagnostic should be able to be done partly by artificial intelligence. In order to push your application forward that way you have to implement heath check URL. A health check URL is a technical URL, used only for an administration purpose that returns a 200 HTTP code if all is fine, and another code in case of problems. I recommend you to put in the payload detailed results of the test performed in order to ease the diagnostic. A health check URL’s sample could be a simple program that make a dummy query in the database and ping the required dependent web services. This provide very useful informations for the operations. Such kind of deployment brings flexibility but it is still a copy of what you do on premise. Your granularity of flexibility is a server that you configure and run. Let’s go further. Micro-services in micro-slice What is the next step? Previous paragraph describes an IaaS approach, this one is about more or less CaaS. Why more or less? When I talked to an ops colleague he told me that the kind of container is a purely design choice. The way you use it has much more impact. This paragraph is about using several containers that you coordinate together. As a software architect I am facing the micro-service architecture for one year. Deploying a monolith in a container and log every time to inspect its log is the first reflex. And it has less impact on my code than splitting an application in four micro-services that run in containers which are destroyed and recreated in case of error. An application split in micro-services has to be designed for location transparency and design for failure. Location transparency Deploying an application as a set of micro-services has a real advantage on a cloud platform because it allows to scale and adjust the capacity for each service (if the services are scalable). Cloud platform are designed to provision a lot of small machines. Even if big or dedicated servers are available, high prices reflect that the cloud is not designed for that. It allows cloud providers to far better optimize their physical hardware by combining lots of small workloads. To do that, you are encouraged to not have the ability to choose where your virtual machine will be executed. Maybe it can be moved during a run. As a developer it means that your code should be transparent to the location. Clusters have required to have a stateless code. Cloud platform requires to be stateless and still less demanding. For example, no assumption on latencies, on attached file system and so on. To reach that goal, consider that the only assumption is to have access to services through the network. If you want to provide a state semantic you either have to use a service that propose it (recommended choice) or implement a replication mechanism with all the associated problem (only if you’re AWS or Google). Design for failure At large scale, so in a cloud environment, everything fails all the time says Wermer Voegels , CTO of Amazon. So, the more micro-services you use, the higher probability you have that a call to a service fails or times out. Your code has to take care of that. As developers we have to design for failure, in particular when calling other services. Several patterns exist for such design and it will be too long to describe them here. It will be the subject of a further article. Prepare your application to restart every time Granularity in the cloud is about smaller machines. It also about smaller time frames. In order to fully benefit the pay what you use of the cloud, you should be able to stop your application when you don’t use it. When only two persons use your application, it should be able to run on one process. When 10,000 persons use the application, it should be able to work as well on 1,000 processes. Requests will come the right way thanks to the cloud provider’s load balancer, but your code has to be able to stop and start smoothly and quickly. So designing in micro-services is a good starting point for the cloud, but it has to be completed with other requirements: design for failure and frequent restarts. Serverless architecture So, my application is split into micro-services, they can be started, stopped upon demand, be resilient to the crash of other services. What do we need now? We need to benefit from the high level services provided by the platform and we need to go a step further in term of scalability. Developing on platforms Cloud platforms provide high order services that can automate some tasks like database backups, data migration. But it comes with some drawbacks. You have to adapt your code to the platform. But how to develop on such platform? Pushing directly your code into the platform is appealing, but how will you debug? When do you do the transition from a local process to the platform? Most of the platforms foundations are open source projects that can be run locally. And this is from my point of view a must have. Every project should be able to run locally in one command. It allows developers to test quickly and to debug easily. Then, plan to deploy on the cloud platform at the early stage of your continuous deployment process. Containers are gaining into popularity quickly as they allow to run the same runtime locally and on the cloud. But latest high level services are often closed-source and not available as Docker images. I personally prefer to accept a little difference between my development runtime and the target runtime rather than being limited in my runtime choice. For the debugging purpose, cloud logging services are your best friends. But they are limited, and building custom methodology and tooling will probably be one of your first job as a cloud platform developer. Developping on serverless architecture The second is still more flexibility. AWS has a new offer called AWS lambda. The promise is really pay per use. No more pricing relative to the uptime. You can now pay only according to the number of requests you are processing. It is very interesting for a startup which can not have up-front investment. The scalability is far better because it is billed at the request. Without customer, you have no bill. But it can be a good option for a department application which is used only several days a year. But what does it mean for me as a developer? Server has completely disappeared. You have to code events-handlers, whose messages can be HTTP requests through an API, messages or updates in a database. The requests are totally independent. The idea of cache for example has to be handled totally differently. As a platform you will have the same problem as described before: which should be your local platform? And finally how do your organize your code? Business logic is spread in the assembling of event-handlers. Is the approach with promises the best option? How do you separate low level logic of the events? Some frameworks like serverless or node-lambda start to emerge. We are at the very early age of what can be a new wave of software architecture. At OCTO, we are still exploring this ecosystem. My first recommendation is to deeply invest on code organization. Components should be above all designed to simplify the complexity. Deploying on a lambda requires, besides, that the size is small enough to be loaded quickly at startup. Finding the good split into lambda is so my first priority. Then, I need to install the right frameworks in order to build a strong development platform with debugging capability both locally and on the cloud. Finally, coupling with proprietary platforms should not be escaped but managed with the correct patterns. Serverless architecture is also the promise of big changes and big opportunities. Conclusion To conclude, in order to take the full benefit of the cloud, design of your application has to be adapted. Lot of today best practices are just must-have practices in the cloud. Micro-service approach allows to go one step further to benefit from the cloud. Serverless architecture is the more advanced scenario and requires to rethink globally the way to organize code. What will replace the plain old layered architecture? Which best practices will emerge from promises and events handling? The future is to be written, but it is one of our core challenge for the next few years. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged AWS lambda , Azure Functions , cloud , cloud native , cloud ready applications , Google Cloud Functions , serverless . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-05-30"},
{"website": "Octo", "title": "\n                fpaste-cli: Share content with magic and style            ", "author": ["Sebastian Caceres"], "link": "https://blog.octo.com/en/fpaste-cli-share-content-with-magic-and-style/", "abstract": "fpaste-cli: Share content with magic and style Publication date 27/05/2016 by Sebastian Caceres Tweet Share 0 +1 LinkedIn 0 Hey people, TL;DR I hacked something together in order to highlight text and send it automatically to fpaste , then put the fpaste link in your clipboard automatically. Why? Well, I just happen to share a lot of content (code snippets, application/middleware logs, ASCII art, you name it!) with other people using both fpaste and pastebin. It makes it easier to read text when trying to debug something. Basically, I really dislike this kind of format: It makes me want to rage quit . And then throw my computer out the window. What I’ve realised is that people don’t use pastebin/fpaste because it takes approximately 13 seconds to open the website, paste the content, click on post, and then share the URL. Everyone knows that 13 seconds is way too long in 2016. So I just do this instead: I highlight the stuff I want to share, I right click it, and then I click on “Send to fpaste”: This is a pretty cool picture by the way. It has send to fpaste , vim , and zebras on it. Then, magically , I have an fpaste link on my clipboard, which contains the content I just highlighted: Pretty cool huh? And stylish , while at it. That being said, I use an Automator workflow in order to do the magic, so this part only works on OS X . I would really like to do something similar on Linux, but I don’t know any tools similar to Automator that would allow to create a similar workflow. If you think of any, please let me know! Why fpaste and not pastebin? Mostly because I was working at a place where pastebin was blocked when I wrote this, and fpaste wasn’t. Besides, someone already did it for pastebin. It gave me a good starting point. So how do I set it up? You just have to clone the GitHub project , and then create an Automator service that sends the highlighted text to your script as stdin. You can use the one that’s already on the GitHub repo, but you will have to adapt it depending on the location of fpaste.py in your system. You can create a symbolic link in your path if you’re really diggin’ it. For the script, I didn’t wrap the whole API, since I just wanted to get something working fast. Maybe I’ll do it sometime in the future. Feel free to contribute as well if you feel like it. Anyway, I just thought I’d share. Seb. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-05-27"},
{"website": "Octo", "title": "\n                Identify performance issues in your Android application            ", "author": ["Rémi Pradal"], "link": "https://blog.octo.com/en/identify-performance-issues-in-your-android-application/", "abstract": "Identify performance issues in your Android application Publication date 28/07/2016 by Rémi Pradal Tweet Share 0 +1 LinkedIn 0 A smooth user experience is a common attribute of all the successful Android applications. It might sound as an obvious statement but many are the applications that are a little bit “laggy”. Moreover android developers are use to test their application mostly on high-end devices (generally their own devices), forgetting that their app will be run on cheaper devices, which will deteriorate the app smoothness a lot. It is pretty easy to find documentation on the Internet that will give you advices to have an efficient android application (for instance the great android performance documentation page itself). In this article we propose the opposite: presenting tools that will help you benchmarking an existing Android application in order to find some room for improvement. It can also be a way to measure improvements when we are doing some performance related tasks. We will try to have first a macroscopic approach and then will go deeper and deeper in the analysis in order to find the hidden cause of a performance issue. Some of the tools I will talk about are part of the android standard build tools , some are “external” products. All of them have proven their efficiency on the projects I have worked on, including Le Monde applications which are massively downloaded (over one million downloads) and need to be performant even on low end devices . In the first part we will see how we can identify app launch slow downs thanks to two tools: Nimbledroid and AndroidDevMetrics. In the second part we will focus more generally on User Experience (UX) issues by using standard Android tools. 1. Monitor app start up time App start up time is a critical metric every developer (and the product owner (PO) ) should monitor carefully. A long start up time will create a lot of friction between the user and its utilization of the application. This is particularly true when it comes to applications that are meant to be used quickly. Would you use regularly your favourite public transportation application if you need 5 seconds each time you want to use it ? In this first part we will focus on global tools i.e. benchmarking tools that can be used by a non technical person (for instance the product owner of the application). Indeed if it is developer’s responsibility to produce efficient code which will ensure good performance, the product owner must be able to quantify the application slow downs. Doing so, the PO will be able to set objectives instead of emitting a subjective statement such as “the app seems to be slowing down these days”. Nimbledroid Nimbledroid is a free (without advanced support) service dedicated to measure Android application start up duration. The key value added of this service is the measure of the start up time on a real device and its monitoring over a middle/long term. An example of interesting output we can obtain is the following: We can see on this chart that it is possible to compare cold start up time for different versions of the application. In that case for instance, there is a downward trend. All the information generated by Nimbledroid analysis are accessible directly on their website which makes this tool convenient to use for anybody. The list below go over the tree ways to provide inputs to Nimbledroid, sorted in ascending order of the facility to give the input to the service. Play store channel. Easy as pie: you type the name of a store application you want to analyse , click on “request profiling”, wait  few hours and you are good! The profiling operation is stored in Nimbledroid database so everyone will have access to the profiling information. If you want to be sure that the profiling information correspond to the last version of the app you can request a new profiling to have updated information. Manual upload channel. This channel is particularly useful when you want to test an app version that is not on the store. You just have to drag and drop the apk file of the application and wait a few hours. With this channel your profiling is private (you need to log in), you are notified by email when the profiling is ready. Automated channel. This channel is the “hardest” to set up but also the most powerful. Nimbledroid provides a gradle plugin to make CI integration possible . After a few configuration steps you will be able to push automatically apk to Nimbledroid and ask for a profiling. This channel is particularly interesting as the profiling is now automatic and a tracked metric. This tool is quite powerful as it is very easy to feed the service with inputs and that the results are readable by anyone (no need to be a developer). If we sum up this tool characteristics we end up with the following table: AndroidDevMetrics AndroidDevMetrics is a tool embeddable directly in your application (the debug version). It will give you several performance-related information such as: activity lifecycle methods execution time, Dagger 2 object instantiation duration, and fps frame drops. All these information are available as you crawl your application from an activity to another in the notification center (see screenshots below). (1) AndroidDevMetrics live notification (2) Dagger objects generation duration The other great asset of this tool is that you can monitor carefully how long does Dagger 2 take to inject objects in your activity. Why is important ? Dagger 2 is a great tool but may leads to important slow down if not used careful. Indeed, when you inject your graph, Dagger 2 will create all the objects needed by the activity (based on the scope you defined): he will call the code of your providing methods defined in your module class. If the provider method code takes too much time it will slow down the whole activity start up. Screenshot 2 (above) shows an object ( InitializeDataManager ) which takes too much time to be instantiated on a UI thread. By identifying these bottlenecks it is easy for the developer to set up strategies delaying this object instantiation (for example with the utilization of lazily computed value ).The following table summarizes strengths and weaknesses of this tool: 2. Identify precisely UI slow down issues Let’s now review the most useful tools when it comes to optimize the UI performances of an application. Of course there are many tools that can help identification of UI issues and give information on the performance issue reasons. In this article we will focus on three tools that have a different granularity and different purposes. We could also have put AndroidDevMetrics in that category, meanwhile I considered that the key value of AndroidDevMetrics is to provide information related to activity startup not general information about what is made in the UI thread. GPU overdraw visualizer Let’s start with the most “basic” UI analyser available on the android platform: the overdraw visualizer. This tool is available directly in the “developer options” of your Android device. You just have to enable the field “Debug GPU overdraw”. You will now see many colors superimposed on your screen. Each of these color tells you how many times the corresponding area has been overdrawn by the GPU. A transparent/blue/green color corresponds to a low number of overdraw (respectively 0, 1, and 2), a red color corresponds to a “high” overdraw number (3 or 4). Of course, the lower the overdraw number is, the better. Indeed if you spot too many area with a high overdraw number in you application it means that the GPU is spending too much time for doing a job he has already done previously. It may seem insignificant on your device but it might lead to important slow down on low-end devices. As you can guess, this tool main’s asset tool is that he is really easy to use: no need to plug your device to the computer and read some unclear stacktrace dumps. By simply identifying the different colors, the potential issues will become straightforward. Another asset of this tool is that it allows you to do exploratory investigations over your application. Even if you do not know where is the performance issue of your application, you can browse it “randomly” to discover some UI performance issues. These assets come with a major drawback: once you have identified the global pixel area which is overdrawn, you will have to do a deep analysis to identify the problematic view and why it is overdrawing. You can find some common overdraw reason and how to solve it in this Android perf video . Traceview When you want to do a more advanced analysis (or when you want to identify a performance issue reason), the Traceview tool will be a powerful ally.  Traceview is part of the Android Device Monitor toolset provided by Android Studio. There are two ways of analysing a trace thanks to Traceview: Programmatically. You have to programmatically put instructions in your code to indicate where you want to start the profiling and when you want it to end ( see method tracing related methods of the Debug singleton ). This solution advantage is that you will mark out precisely your analysis. Meanwhile, it means that you need to know where problematic instructions will be called. When you will execute your app and go through these two methods, you will end up with a .trace that you will be able to import in Traceview (file->open). Directly in Traceview. When you open Traceview you will be able to start profiling directly by clicking into your process in the device panel and then clicking on “start method profiling”. You can now execute on your device the cinematic you want to profile on your device. When it’s over, simply click on the “stop profiling” icon You have now a trace dump displayed in Traceview. Traces can be quite complicated to read so here are some information that you might find useful, complementary to the Traceview official user guide : Traceview outputs table includes many columns. Most of the time, many of them will not be interesting. The two most important metrics you should focus on are the following: Incl Cpu Time % and Excl Cpu Time % . The first metric is related to the CPU duration percentage spent inside the method (for instance doing a primitive operation, waiting a system I/O input…). The second one designate the CPU time spent executing the whole method i.e. it includes the subcalls of your method. Typically, you will use the first metric to identify inefficient algorithm you might use, the second one will be powerful to spot bottlenecks calls that could be made previously or asynchronously. The trace dump shows all the thread running in your process. It means that you will find all the operation executed asynchronously. When you want to check if a suspicious CPU-consuming method is called on the main thread, just click on it in the list and the calling thread will be highlighted in the thread chart in the upper view. Do not use the duration displayed as a precise benchmarking tool. As you can notice the UI is particularly slowed down when the method tracing is running (this is due to the deep analysis of the call stack made in “runtime”). Therefore, the durations displayed are longer than the “regular” duration. Prefer percentage metrics. As you have noticed, Traceview is a tool allowing very precise analysis. Meanwhile it is also a complex tool that can be used only when we have an idea of where is the problem. Moreover the poor user experience of the interface requires a few times to get use to it. Logcat analysis It might seem to be the most rudimentary tool but logcats outputs can sometimes be useful to have a brief idea of eventual performance issues in the application. Indeed if you read carefully the logcat output of your application, you might notice some entries that look like something like that: Choreographer: Skipped 41 frames!  The application may be doing too much work on its main thread. This entry description is self-explanatory: as an important amount of computation is done on the main thread, the Android Choregrapher is not able to refresh the screen display as it should. The Android Choregrapher is an internal Android class dedicated to display views on the screen. If too much work is down on the main thread, the rendering method of the Choregrapher will not be called between two frame-refresh which means that some frames have not been displayed on the screen. If it happens when the screen is “static” it will not be perceptible by the user. However, if it happens when there is an animation being displayed, then the animation might look laggy. As for other performance warnings, a single occurrence of this message is not a problem in itself, particularly if your app is running on a low end device. It is when you notice several occurrences of this message that you might have to consider investigating. Another UI-related performance log message you may encounter is something that look like this: dalvikvm: WAIT_FOR_CONCURRENT_GC blocked 176ms an equivalent message for devices using ART is the following (the previous one is for a Dalvik powered device): art: Starting a blocking GC Background These messages are displayed when the Java Garbage collector (GC) is running. The GC typically runs when the Java heap limit of the system is reached. During this garbage collecting operation, the GC is freeing all the non longer used Java objects. This operation can be quite long and needs to pause all the running threads (including the main thread) to avoid inconsistency. The consequence of this is that your screen will freeze until the garbage collecting is over. A typical origin of too many occurrences of this message is a memory leak in your app which leads to too much consumed memory and therefore a GC that has to run too frequently. Conclusion With this article you are now well equipped  to have a fair idea of how efficient your app is and what are the main bottlenecks that you have to address in order to make your app more confortable for every users. The tools are presented in this article with a “theoretical” point of view, if you want an example of how some of this tools can be applied on a concrete app, check out this great Romain Guy’s article . Of course this knowledge is only the first (but not necessarily the easiest) step. Once you know where is the problem you have then to fix it. In some cases it may be really easy and quick but sometimes it will take be a long-span task. The diversity of the ways to fix a performance issue is so big that it is not possible to make an exhaustive list, meanwhile you take a look at the Android Performance Playlist which presents many useful tips. It is also important to notice that the easiest performance issue to fix are the one which have not been created in the past! Take care to follow all the Android performance best practices and you will not have to spend to much doing a posteriori analysis. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged android , Audit , performance , profiling . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-07-28"},
{"website": "Octo", "title": "\n                Serverless real-time architecture on AWS: there is a way !            ", "author": ["Oliver Baillot"], "link": "https://blog.octo.com/en/serverless-real-time-architecture-on-aws-there-is-a-way/", "abstract": "Serverless real-time architecture on AWS: there is a way ! Publication date 02/02/2016 by Oliver Baillot Tweet Share 0 +1 LinkedIn 0 Tired of using EC2 instance, creating your infrastructure, instantiating as many node as possible to handle the amount of data? Good news! it is now possible to get rid of these servers and focus only on the code. Pretty nice, isn’t it? To illustrate this topic, this article is dedicated to a real-time “serverless” platform. Let’s dive into it! Kinesis: the cloud has its message queue Amazon Kinesis Stream is a platform devoted to collect and process records in real-time. If you are familiar with Kafka, it refers to the same concept, the only difference lies in the terms you use. You can consider Kinesis as a messages queue for high message throughput . The core feature of Kinesis is its streams. Streams can be considered as the equivalent of topics in Kafka. Each stream is separated into shards which are a “uniquely identified group of data records in a stream” (one feature is the simulation/indication of how many tuples you can handle by modifying the number of shards) There are different kinds of “Kinesis” : Kinesis Firehose – To load your data into S3 or Redshift Kinesis Streams – To perform custom processing The one we are interested in is Kinesis Stream because we want to manipulate the platform features at a deeper level. For those who know how distributed messages queue works, the core feature is the high volume of messages handled by the platform. Kinesis is supposed to be able to handle 1MB/sec data input and 2MB/sec data output by shard. What does it mean? Regarding real-time Big-Data platform, you usually have to deal with millions to billions messages per second. The first question arising is: Is Kinesis able to handle this amount of data? Let’s admit the average record size is 65Kb and the maximum records written per second is about 1,000,000. With this amount of data AWS recommend to instantiate 97,500 shards knowing that the service is limited to 50 (unless you contact them). 97,500 shards or threads seem really high even if the platform is able to handle this amount. If you are interacting directly with a consumer or/and a producer on an EC2 server, you would have to size your server with enough resources to deal with this amount of threads. In other words, if you want to handle large amount of events, it seems compromised. However, if you have the necessity to change the number of shards after creating a Kinesis instance, you can do so without re-creating the stream. Yet, it would be great if the resizing was done automatically. How does it work ? As a distributed message queue, you need a consumer and a producer to interact with Kinesis. The producer purpose is to inject inside the streams records (of any type) and the consumer purpose is to retrieve records from the streams. There is no intelligence behind it. It works as a queue and allows to handle high volume in records without failure and with high availability guarantee. Indeed, Kinesis does not lose records, it replicates over its nodes the data it needs to keep. Everything is done automatically, we do not have access to the infrastructure and even less to the replication factor. Amazon Kinesis Streams Key Concepts Design Advantages : Easy to manipulate, enables fault-tolerant consumption, simple data analysis and reporting in real time. Drawbacks : Still important issues need to be fixed: record skipped while being consumed, records belonging to the same shard processed by the same consumer at the same time, consumer applications not reading the latest data from the stream. Time to create your first stream When you create your stream for the first time, there are a few things to know: Create a role with every necessary rights for CloudWatch and DynamoDB . CloudWatch will monitor you events and insert logs coming from Kinesis Dynamodb will store “states” (offset to keep track of what record is within the stream if one fails) Described below the interface to create your first stream: Creation Stream User Interface Once you click the create button, you get a Kinesis Stream up and running. It is now time to process data! When the words “message queue” and “real-time” occur, usually, what comes next is the real-time framework combined with the data flow. Kinesis can be used by real-time frameworks such as Storm or Spark Streaming. However our goal here is to create a full AWS real-time stack. And this can be done by “AWS Lambda”. What is lambda? Lambda is a compute service where you can upload a “Lambda function”. A Lambda function needs to be implemented in order to be executed as any regular code (Java, Python, NodeJS) reacting to an AWS Events. By doing so, you will not have to worry about any infrastructure or managing the servers, and you will pay just regarding the amount of CPUs used. In other words, you can forget about EC2 and just deal with your code. Here is an example of Lambda Function: Lambda Function Sample In our case, what matters here is the ability to connect easily Kinesis to a Lambda function. Lambda can behave as an event-driven compute service, such as new data getting into Kinesis. Therefore, a Lambda function can act as a consumer. The perks of using a Lambda are the high availability, provisioning and automatic scaling. However, when you consume data from Kinesis and the connection is severed, you may encounter data loss. A fault-tolerant mechanism needs to be design to prevent any risks. Advantages : Useful to just handle the code without worrying about the infrastructures, services easy to handle, offer multiple languages (Java, Python, NodeJS). Drawbacks : Lack of flexibility when performing multi-threading applications (no access to workers-JVM), limited in terms of resources allocation , 5 minutes timeout… Going deeper into Lambda To implement a lambda, you need to implement a function call a “handler” and attach a role to it (so it can interact with DynamoDB and Kinesis for instance). Lambda Function Declaration Interface One way to code your own function is to use a regular IDE with the appropriate SDK. Once your code is ready, you just have to package it and upload it on the Lambda service. FYI: if your package weighs over 10Mb you will have to use S3 to launch you code. Lambda High-level Interactions When the code is uploaded, the link between Kinesis has to be set. An easy way to do it is to select as source the type of resources you want. In our case, it would be “Kinesis” and it will automatically detect your stream (mind that your resources need to be in the same region). A major aspect of coding is testing, you should obviously not deploy a code in production without testing it. Indeed, with lambda you can easily test your code by using the “test” interface. However, it is limited if you want to test several Lambda in parallel and AWS does not provide tools to automate the deployment and test everything in an automated way. One answer could be to use common tools, for instance if your code is written in Java, such as JUnit, Cucumber or FitNesse. Then, package everything as a “normal” application and perform your tests in local before deploying the code on AWS. Test User Interface In this state the lambda function is always listening to Kinesis, you can let your resources launched because you will only pay for the amount of resources used. Which is pretty nice. You can now conceive that the major perks of using  Kinesis + Lambda are definitely better in terms of cost compared to EC2 instances with Kafka + Storm/Spark cluster or with Kinesis as well. Described below how you can use Kinesis with EC2 instances: Streams High-level Architecture Last missing part of our design is the storage system because as every real-time processing architecture, the data need to be stored somewhere and this is achieved through Amazon NoSQL database: DynamoDB. DynamoDB: the fully managed NoSQL database service Be aware that good documentation about DynamoDB is hard to find. You will find piece of codes here and there to create your own application. However once you are familiar with AWS it is not that difficult (in AWS: credentials are the key, once everything is well configured coding skills are all that matters). By instantiating the right object in our lambda, creating the right table in DynamoDB and making all the right links, the only thing left to do is to run a producer which will inject the records into Kinesis. Then our lambda function will consume the data to let them end up in Dynamodb. Process events triggered by DynamoDB As every NoSql database DynamoDB enables to store large amount of data. However, one cool feature is DynamoDB Stream. DynamoDB Stream enables to get every event generated by DynamoDB (insert, delete, type of events etc…). All necessary monitoring informations can be found in CloudWatch. Indeed, DynamoDB is directly plugged to this cloud monitoring services but if you want to go further and perform your own analysis Dynamo Stream allows to extract all necessary informations. DynamoDB Stream is based on Kinesis, as a layer on top of DynamoDB with the same API, so it is easy to adapt your code from a Kinesis app to a DynamoDB Stream one. DynamoDB Streams High-level Architecture We now have Kinesis + Lambda + Dynamodb able to handle a high volume of records at a minimum cost in the cloud. We tested as well if Lambda could store directly to S3. The answer is yes: the same principle applies as for DynamoDB except that instead of inserting into Dynamo, an S3 bucket is used. Advantages : Triggered associated with DynamoDB Drawbacks : Lack of documentation The whole picture As a result, it is seducing to be able to easily create a real-time platform without worrying about infrastructure, provisioning, automatic scaling, high availability or OS maintenance. Nevertheless, real-time platform does not mean Big Data real-time platform. If you want to digest millions of messages per second you will probably encounter latency issues, bottlenecks, etc… Nonetheless, It would be interesting to see how the architecture would react by analysing each components: Capacity for Kinesis Streams to be consumed Amount of resources used by Lambda Performance of DynamoDB to handle reads and writes Final Architecture : Kinesis + Lambda + DynamoDB At this stage of maturity, it would be not wise to consider this stack as a Big Data platform. The risks of too many errors occurring would be too high and the design would have to be twisted. What about data aggregation? Yet, this is the main issue here. How do we store data that need to be aggregated? There is no magic answer : You can trick the lambda function by creating an in-memory array. Yet, you would have to perform a grouping before because you would have to care on which node your data is going to end up whereas you are not supposed to care about the infrastructure… You can put everything in cache. But what kind of solutions can we use? It does not seem wise to use an external component such as Memcached, Redis or even Cassandra. It would be better to use an AWS stack component such as ElastiCache or DynamoDb. But even by doing so, you would have to be careful with the implementation of your lambda function (fault-tolerance, high availability, data loss, memory clean up). Deploying automatically The way to automate provisioning and configuration at scale is to use CloudFormation which is a configuration management tool such as Puppet, Chef or Ansible. It is difficult to handle but once everything is configured, it makes things easy. It allows to create, modify or update an existing stack on AWS. Described below key templates to instantiate our components with CloudFormation: AWS Kinesis Stream : AWS Lambda : AWS DynamoDB: Once your Cloud Formation template is done, you will be able to deploy a full stack without manually set parameters or interactions between components. It makes things easy to create automatically services and end up with a full real-time architecture up and running on AWS. However, questions arise: How many tuples can the platform handle? Regarding high availability, are we sure to not lose any messages when a service crashes? Can we deploy without interruption? How long does it takes? Can we develop easily in local mode and then deploy onto the cluster? Is it easy to automatically test our programs? Do we have enough flexibility to implement complex data stream processing? All of these questions will have to find answers. But for the moment, The AWS stack Kinesis, Lambda and DynamoDB is a good start to make real-time distributed system easier to start with. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Serverless real-time architecture on AWS: there is a way !” sanjay 28/04/2016 à 14:40 Hi\r\nVery good blog on Serverless architecture for processing real time data using Lambda , thanks.\r\nCan you please share the source code of this blog to see how data ingestion done kinesis stream and lambda processing data and storing into DynamoDB. ken 13/10/2016 à 16:36 Beautiful article. Can you please share the source code on GitHub ? Thanks. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-02-02"},
{"website": "Octo", "title": "\n                Geo localizing Medline citations            ", "author": ["Alexandre Masselot"], "link": "https://blog.octo.com/en/geo-localizing-medline-citations/", "abstract": "Geo localizing Medline citations Publication date 12/01/2016 by Alexandre Masselot Tweet Share 0 +1 LinkedIn 0 Where are the scientific publications coming from? Geolocalizing Medline citations When and where are the scientific publications coming from? Which country are collaborating the most? To investigate those questions, we focused on Medline, the major biology and biomedical peer reviewed citations repository. Big Data is not only a buzz word. A rich ecosystem of tools have emerged, together with new architectural paradigms, to tackle large problems. Open data are flowing around, waiting for new analysis angles. We have focused on the Medline challenge to demonstrate what can be achieved. To provide some insights on how an interactive web application was built to explore such data, we will discuss the geographic localization method based on free text affiliation, Hadoop oriented treatment with Scala and Spark, interactive analysis with the Zeppelin notebook and rendering with React, a modern JavaScript framework. The code has been open sourced on github [ 1 , 2 ] and the application is available on Amazon AWS . Medline citations geographical heat map, per country production and collaborations in 2012. Click on images to open the application. Introduction Analyzing the geographic origin of scientific publications has been addressed in previous papers, such as [ 3 , 4, 5]. Although these papers present an interesting sociological, political and historical view on the matter, the underlying data has been aggregated by hand (see figure 1) and offers little possibilities of further digging. Figure 1 – International scientific collaborations network, 1994 [6]. More comprehensive dataset are nonetheless available, such as arxiv.org [6] , but we decided to focus on the biology and biomedical citation repository Medline. As the underlying source of data for Pubmed [7] , the major entry point for any bibliographic research in the field, it offers both a large volume of citations and a parsable format. If countless questions can be digged out of such materials, we were wondering which countries are the most prolific across the years. Furthermore, are there any patterns in country collaborations? Even though localization information is not directly made available, it can be deduced from the authors’ affiliations in a majority of cases (when available). Finally, we wanted to provide both an interactive tool to navigate through those data and a framework to analyze them in a programmatic way. The current article has no other claims than offering a visualization tool and an access to that data for further analyses. Figure 2: collaborations involving at least one author resident at Genentech, San Francisco. This image is not directly generated by the tool presented in this article but it demonstrates how the underlying data can easily be exposed for other usages. Materials Citations data were downloaded from Medline archives. They can be made available via an NLM agreement [8] . Mirrored in October 2015 for this study, they consist in 789 files (15GB) containing 23 millions citations in XML format. Each citation is comprehensive, does not include the article content but meta information such as publication date, title, authors and partial affiliation, references etc. An example of such an output can be found here [9] . Localizing the publication with geographical coordinates and country is derived from the author’s affiliation, whenever available. Details on this process will be discussed below, but two major sources of localization were used. If the Google map geocoding API [10] is convenient for this purpose, limitations on the free usage forbids a large scale usage. A first home made geocoding pass was therefore designed to parse the free text affiliation fields, based on GeoNames dumps [11] . The last source of external data used in this project are country geographical shapes, derived from the Natural Earth Data repository [12] . Finally, such a programming project relies on countless bricks of open source software. Without citing them in details, the main components are Play [18] , Spark [ 16 ] and the Scala language for the backend. The front end web application was developed in JavaScript, using the React library [13] and heavily relying on d3.js [14] for graphical components. Although the published code was executed on a laptop (with Solid State Disk), the used technology is originally aimed to connect to an Hadoop cluster, for the sake of scalability. Data crunching The data crunching step consists in downloading original data and turning them into an accessible and exploitable format. In this article, it mainly consists in importing citations and assigning one or more geographical locations whenever possible. Method Download Medline archives As mentioned in the material section, Medline citation archive can be downloaded (once the license agreement has been exchanged). Although the method seems trivial, the command for the sake of easier reproducibility is: lftp -e 'o ftp://ftp.nlm.nih.gov/nlmdata/.medleasebaseline/gz && mirror --verbose && quit' Geo localization One key step of data preparation is to assign a latitude, a longitude and a country to citations. Medline citations XML format exposes a list of authors, such as for [9] . One of the author’s details looks like: <Author ValidYN=\"Y\"> <LastName>Lill</LastName> <ForeName>Jennie R</ForeName> <Initials>JR</Initials> <AffiliationInfo> <Affiliation>Genentech, South San Francisco, California 94080, USA.</Affiliation> </AffiliationInfo> </Author> The Affiliation field is therefore to be mapped to geographical coordinates. Google GeoCoding API The easiest way to translate an address text field into such coordinates is to use Google Geocoding API [10] . It is the underlying layer mapping an address to a position when using the Google map or Google Earth applications. A REST API is available, accessible via a URL like: https://maps.googleapis.com/maps/api/geocode/json?address=Genentech,+South+San+Francisco,+California+94080,+USA&key=YOUR_API_KEY Where the YOUR_API_KEY value is to be requested by registering to the Google API system. The JSON output can be parsed for resolution (is the address unequivocally resolved? which coordinates? which country?). A Java API is also available to leverage the parsing and make the process even simpler [15] . However, a big limitation is the Google free plan over the API. The daily limit is defined to be 2’500 requests, hardly usable when approximately eight millions unique affiliation addresses are to be resolved. Other online solutions exists (such as MapQuest) but suffers from the same usage limitations or were shown to be too strongly biased towards the United States. A handmade offline solution had therefore to be designed. Our implemented geocoding method relied on the attempt to resolve affiliations by the GeoNames solution described in the next section; the unresolved affiliations were then sorted by recurrence (the most common first) and a daily call to the Google API made to try to locate the next 2500 top most. For the sake of this project and due to time constraints, this second step has only been applied for a few days, to test the approach relevance. A GeoNames based solution GeoNames is a comprehensive geographical database [11] . Similarly to Google Maps API, it provides a REST API to be queried, but with similar limitations. However, the underlying database can be freely downloaded in tabular format, with several views. The ad hoc solution presented below is not perfect but proved to be relevant enough to solve most of the situations at hand in the current project. A lot of information is available in GeoNames database (like currency, postal code format…), but only a subset of it is actually used in the current implementation. Our solution uses three files, all cross linked with proprietary id fields: countryInfo.txt : country ISO codes and names; cities15000.txt : names, latitudes, longitudes, countries and population for cities larger than 15’000 inhabitants; alternateNames.txt : countries and cities alternative names or spellings. The actual algorithm to localize address field is not straight forwards for two major reasons. It is filled as a free text by authors at paper submission time, without clear indications on the format. Although they can be unequivocal for a researcher reading the paper, they offer a challenge from an automation point of view. Here are a few examples: “College of Physicians and Surgeons, Columbia University, New York, NY, USA”, “CJ. N. Adam Memorial Hospital, Perrysburg, N. Y.” , without any country; “Tripoli”, besides various spelling, is a major city in three countries; email information sometimes ends up in the affiliation field; multiple affiliations for one author can be filed; Italy capital can be spelled “Rome” or “Roma” … Many places have various common names: “New York” , “New York City” , “NYC” . “Springfield, USA” can point to eleven different towns with more than 15’000 inhabitants. To make the problem even more intricate, some affiliations do not provide any country such as “Oxford College” or “Department of Medicine, University of Washington, Seattle 98195″ or provide different depths, such as street address, postal code etc. Finally, to give an ultimate flavor of the problem, we could also mention country that do not exist anymore or had their named changed: “USSR” , “Hong Kong” or “Tchecoslovaquia” . If the Google Maps API has proven to be efficient to disambiguate many cases, it relies on much more information, large code base and unpublished methods. Therefore, we have designed a multi steps strategy to address the most common situations. The goal is to extract city/country pairs and look out in the database for unique such matches. The major steps are: remove emails, head reference numbers, and extract the first affiliation when multiple are present, splitting on dots and semi columns (but no dots following initials or common abbreviations); split the string in blocks along comma separators; remove blocks with postal code structure; with all combinations of two elements among the last three blocks, look up in the GeoNames database for matching: city/country pair, city/country pair, with alternate names, city only, city only, with alternate names; the process is stopped at the first match on the previous step; if multiple locations are matching, they are sorted by population count and the first one is kept when the population ratio with the second is greater than 10. As discussed below, a lot of affiliations are not resolved unambiguously, but a majority are. Imperfect as it may be, the presented method relies on empirical tradeoffs. To illustrate the method, examples can be found in the project unit tests [ 1 ]. Implementation Out of the original Medline citations, only a few informations were kept for the sake of this project: publication date, title, abstract and author list. They were saved in parquet format,  in a Hadoop hdfs fashion to allow for distributed processing with the versatile Spark framework [ 16 ]. Although it relies on hdfs oriented format, the crunching part was actually done at once on a mac laptop, executing steps located in the ch/fram/medlineGeo/crunching/tools directory [ 1 ]. Results It is now time to review numbers and measure the enrichment of the crunched data. As the data are processed on the Hadoop platform, Apache Zeppelin was used for the analysis [ 17 ]. It is a web based notebook, allowing users to interactively manipulate hadoop hosted data while programming in Scala (Java or Python)  and Spark [ 16 ]. Geo localization method performance Out of the 23 millions citations, 8.7 millions unique affiliation fields were extracted. For example, “Department of Biology, Massachusetts Institute of Technology, Cambridge 02139” refers to 724 publications. How many of them could be attributed to a geographical location by each of the previously described methods? 65% were resolved by the custom GeoNames based strategy. Among the remaining ones, 75% were assigned by the Google Geocoding API. Although this second step has not been applied to the whole set for time reasons (it would have taken 1218 days with the free access limit!), one can predict that 91% of the overall addresses could be resolved. We can now measure how many publications could be linked to a geographical location. Evolutions of located citation counts through time As of October 2015, more than 23 millions citations were downloaded. Although some publications from the late XIX th century were registered, the volume drastically increased after WWII, as shown in figure 3. Figure 3: Medline citation count per year (a Zeppelin notebook chart output). Among those 23 millions citations, 9 millions could be associated with at least one location, as shown in figure 4. Taking up in the mid eighties, the percentage is stable, around 70% of the overall production. Figure 5 shows the numbers of publications associated with two or more countries . The comparatively low numbers are due to the tendency to fill the affiliation only for the first author. Moreover, the rate of localization resolution deeply affects citation with authors coming from multiple research centers. Figure 4: Medline citations counts with at least one affiliation with a geographical resolution. Figure 5: Medline citation counts related with multiple countries over the year. Browsing through geographically located publications Once the data crunching steps are executed, citations are annotated with location and stored in parquet format. Although they can be read by several tools from the Hadoop stack, we present two components built for this project: a backend server: reading, transforming and serving the data through a REST API; a frontend rich web application, to display and interact with graphical representations, which communicate with the backend via the API. Such a modularized approach allows to change the presentation layer, which could easily be another web application, R scripts or countless other solutions. The backend server [1] The backend goal is to provide access to data through an HTTP REST API. For example http://localhost:9000/countries/citationCount/2012 is expected to return a list of countries with the number of assigned publications in 2012. The output is to be structured ( json ) or tabular ( tsv ). Another example, to extract all country collaborations for year 2002, one can head to http://localhost:9000/countryPairs/citationCount/2002 and get: [\r\n  {\r\n    \"year\": 2002,\r\n    \"nbPubmedIds\": 1,\r\n    \"countryFrom\": \"DE\",\r\n    \"countryTo\": \"RU\",\r\n    \"nbPubmedIdTotalFrom\": 18100,\r\n    \"nbPubmedIdTotalTo\": 3573\r\n  },\r\n  …\r\n] Among many possible choices ( nodejs/express , java/Spring , ruby/rails , python/django etc.), we selected the Scala/Play/Spark combination. Based on the JVM and combining both functional and object oriented paradigms, we believe Scala [20] to be a language of choice for scientific computations and more broadly, distributed programming. Play is a web framework, for both Scala and Java. Besides practical developper comfort, it is naturally designed to serve reactive programming [20] , with large, interactive and resilient transactions. The third component of the trio is Spark [16] , a powerful engine sitting originally on top of Hadoop. It allows to distribute computations over large volumes of data and  is accessible in Java and Python, but the natural language is… Scala. The Spark project has got a strong traction in the recent years, leveraging Hadoop MapReduce batch processing into a more interactive and comfortable abstraction. The framework continuously grows, including event streaming, machine learning, graph algorithms and an R interface. Aggregating data via Spark launches a Hadoop job. Even though the framework made huge speed improvements over the original MapReduce system, it is hardly enough for a convenient interactive usage (common response times can be as long as a minute). Therefore, all classic views are computed once, cached and can then be served within a tenth of a second. As mentioned a couple of times, the selected storage architecture for this project was Hadoop oriented. It shall nonetheless be mentioned that this choice could be challenged. The volume at hand is not that large and other solutions, such as ElasticSearch [21] could be considered. We gave an attempt to  mongodb [22] but data aggregation did not show to scale up. The frontend web browser [2] The second component consists of a rich web application. Once again, modern Javascript offers a plethora of frameworks, such as AngularJS, Ember or Backbone. Although all of them can show clear advantages in some situations, we opted for React [13], an open source rendering library provided by Facebook. Together with the Flux design pattern [23] , it has proven to meet the requirements of massive data display. In complement, one can hardly avoid the versatile d3.js library [ 7 ] to build interactive graphical representations. We propose three different type of views: a world map, with a heat map to represent citation counts (figure 6); a bar chart, with publication counts per year (figure 7); a network graph, displaying collaborations among countries (figure 8). Each of these views is enriched by a year selection slider to navigate in the past and mouse interactions to reveal more details. A world heat map Figure 6: a world heat map, where colors ranging for light orange to red show the number of publications for the current year. Once citations could be assigned to geographical coordinates, it felt natural to display them on a world map. The results presented in figure 6 shows an hexagonal paving, with the color indicating the local number of publications in the area. Using the slider to navigate across years reveals for example the recent dramatic increase of production from the BRICS countries. D3.js is combined with topojson for a near optimal rendering of country borders, allowing to superimpose other information, such as the links displayed in figure 2. Publication count per country Figure 7: citation counts per country. The localization method allows not only to return coordinates, but also the country. It is therefore possible to build a bar chart in figure 7. Scrolling through the years makes the bars slide from one position to another, therefore following one country progression, such as China in the last twenty years. Country collaboration graph Figure 8: collaborations per country pairs, where the flag size reports the relative number of publications and the link strength (the vertice attraction) the number of collaborations. Finally, multiple affiliated citations allow to pull out cross country collaborations (at least as of 2012, as revealed by figure 5). The force layout attracts more strongly countries with more collaborations while the overall number of publications is reported by the flag size. Moreover, d3.js allows to draw trapezoidal edges, where the thickness of one end shows the relative importance of the collaboration. For example, in the upper left corner, Russia is exclusively linked with Germany, whereas this latter country as other collaborations. The same observation can be made for the Israel-USA relation. One should keep in mind that this representation is only a sample of the actual human co-publishing, as either the authors might not have filed multiple affiliations or they could have not been solved by the localization method. We can only only address a sample of the actual collaborations. Numbers are nonetheless more relevant in the recent years, following the increase of citations with multiple affiliated authors. Finally, the 65% address elucidation rate for a single address falls down to 42% for publications with two addresses (0.65 2 ). Extending the geo localization method to Google Map API would therefore further resolved 83% such cases. Running the application For development purpose, both components can be cloned out from github [1, 2] and started independently, as described in their respective README.md files. For a more convenient usage, a docker image can be pulled out and ran from http://hub.docker.com . This image has been published for demonstration only. To limit memory and CU footprint, the JVM backend is not packed and data are cached within the NodeJS application: docker run -p 80:5000 alexmass/medline-geo-standalone Continuous testing is hosted by Travis [24] , an online continuous integration system with free access for open source projects. Conclusions We have presented an approach to attribute geographical information to scientific citations, based on the Medline citation index. Moreover, we have seen how a Big Data stack can be used with a modularized architecture via an API data exposition. If those data can reveal interesting phenomena, we believe that this approach can be extended to dig further on this data set or others (arxiv.org), analyzing abstract contents for example, or aggregating other sources of information (such as yearly GDP). At a larger scale, we have demonstrated how the Hadoop ecosystem paves new routes to dive into the analysis of massive datasets, at a relatively low cost. Acknowledgments I’d like to thank many contributors for the presented work. Among them, Nasri Nahas and Catherine Zwahlen for seeding the idea. Martial Sankar for enlightening discussions concerning the geo localization methods. Last but not least, my colleagues at OCTO Technology Switzerland for their thorough feedback. References [1] https://github.com/alexmasselot/medlineGeoBackend [2] https://github.com/alexmasselot/medlineGeoWebFrontend [3] “Les formes spécifiques de l’internationalité du champ scientifique” , Yves Gingras. Actes de la recherche en sciences sociales, 2002 [4] “L’internationalisation de la recherche en sciences sociales et humaines en Europe (1980-2006)” . Yves Gingras et Johan Heilbron. L’espace intellectuel en Europe, 2009 [5] “International scientific cooperation : the continentalization of science” ,  M. Leclerc et J. Gagné. Scientometrics, vol. 31, 1994, p. 261-292. [6] arXiv.org http://arxiv.org/ [7] PubMed http://www.ncbi.nlm.nih.gov/pubmed [8] https://www.nlm.nih.gov/bsd/licensee/access/medline_pubmed.html [9] http://www.ncbi.nlm.nih.gov/pubmed/25428506?report=xml&format=text [10] https://developers.google.com/maps/documentation/geocoding/intro [11] http://www.geonames.org/ [12] http://www.naturalearthdata.com/ [13] https://facebook.github.io/react [14] http://d3js.org/ [15] https://github.com/googlemaps/google-maps-services-java [16] http://spark.apache.org/ [17] https://zeppelin.incubator.apache.org/ [18] https://www.playframework.com/ [19] http://www.scala-lang.org/ [20] http://www.reactivemanifesto.org/ [21] https://www.elastic.co/ [22] https://www.mongodb.org/ [23] https://facebook.github.io/flux/docs/overview.html [24] http://travis-ci.org Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse", "date": "2016-01-12"},
{"website": "Octo", "title": "\n                A chat with Doug Cutting about Hadoop            ", "author": ["Nelly Grellier"], "link": "https://blog.octo.com/en/a-chat-with-doug-cutting-about-hadoop/", "abstract": "A chat with Doug Cutting about Hadoop Publication date 14/01/2016 by Nelly Grellier Tweet Share 0 +1 LinkedIn 0 We had the chance to interview Doug Cutting during the Cloudera Sessions in Paris, October 2014. Doug is the creator behind Hadoop and Cloudera’s Chief Architect. Here is our exchange below: A question is: how does it feel to see that Hadoop is actually becoming the must have, the default way of storing and computing over data in large enterprise companies? Rationally it feels very good. It’s a technology that’s supposed to do that. Emotionally it’s very satisfying, but also I must say I must be very lucky. I was in the right place at the right time and happened to be the person. Someone else would have done this had I not, by now. Download our white paper “Hadoop Roadmap” It’s funny because yesterday you were mentioning how Google released that paper about GFS and then about MapReduce, and you seemed surprised that no one else has gone and implemented the paper. How would you describe this, because it was a very big, big task that some people were daunted by taking on or…? I think, again, I have the right experience from having put some work in open source . I worked on search engines and I could see the value in the technology, I understood the problem, and that combination. And I think I’ve also been in the software business long enough so that’s why I knew what it’d take to build a project that would be useful, that would be used. And I think no one else was positioned ready enough in the competition with that combination of properties. I’ve been able to take advantage of these papers and implement them as open source, and get them out to people. My guess, I don’t know. It wasn’t my plan. Were you expecting that it would get take such a big, big impact? No, not at all. OK, now I guess it’ll be more of a technical question: you mentioned yesterday (I was there yesterday and today) that you know there are all these tools that are coming out, like, building on top of Hadoop and bringing a new technology and a new usage of data – how do you see Hadoop changing, architecturally speaking, to be able to provide even more capabilities in the future? It’s a very general architecture . It’s in many ways, much like I said, an operating system. An operating system, I’ve been showing, has storage, has a scheduler, has security mechanisms. Already the challenge is to support all kinds of different applications. So I think that the design it has right now is more or less sufficient to permit a very wide range. Just like operating systems haven’t changed since Windows. Yeah, not fundamentally. Since the 60’s. Those basic capabilities give you a platform you can develop lots of different applications on that can share the hardware, in a sense. It’s really… Well, a Java OS is sort of “get out of the way” and let applications share the hardware. Provide abstractions as Jim Baum said. Exactly. To deal with complexity. And so I think that’s a role that Hadoop is filling more and more. I know it needs a radical re-architecture to do that. Whether people will implement alternate file systems…That might happen, we’ll see. OK. Thank you so much. And do you see, all these tools, like, you see Kafka for log aggregation across DC, and we see Storm for stream processing, and all these things. Do you see new usages that haven’t come out yet for data? You can search on it, you can index it, you can stream it and process it in real time… We think there are lots of opportunities for more vertical applications in different industries that are very specific. Things that can process images, tools that can process data… There are lots of different areas where there aren’t tools today. Not to mention verticals like insurance and banking and so on. Some people see commercial offerings and some people see open source offerings. I think right now what people are seeing are more the lower-level tools that can be plugged. I think more and more, higher and higher upper stack will see open-source implementations commoditizing the value of the stack. That’s an ongoing process. Want to know more about Hadoop and the Hadoop ecosystem? Have a look at our Hadoop map (English version)! Our Hadoop White Paper and Roadmap are also available for free download (French only). Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data , News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “A chat with Doug Cutting about Hadoop” Nick 08/01/2018 à 09:59 Link to Hadoop map is broken :( Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-01-14"},
{"website": "Octo", "title": "\n                ScyllaDB vs Cassandra: towards a new myth?            ", "author": ["Marc Alonso"], "link": "https://blog.octo.com/en/scylladb-vs-cassandra-towards-a-new-myth/", "abstract": "ScyllaDB vs Cassandra: towards a new myth? Publication date 15/12/2015 by Marc Alonso Tweet Share 0 +1 LinkedIn 0 Disclaimer : all the tests described in this article were performed on ScyllaDB 0.10 and might not be relevant for recent versions. For a more up-to-date description, go to the official website http://www.scylladb.com/ On September 22th 2015, a community of developers announced having designed and released a new database management system described as the fastest in the world. This system, named ScyllaDB is part of the NoSQL world whose ambitions are: Design scalable systems by distributing the workload and the storage over multiple machines. Design fault tolerant systems Provide higher throughputs, larger storage with lower latencies In this very competitive environment, ScyllaDB presents an interesting characteristic: all its structures and mechanics are copied from the very popular database: Cassandra. Main difference announced: ScyllaDB is written in C ++ when Cassandra is in Java . Using the same structures allows for the users to use any of those two managers indifferently in their cluster . Cassandra developers don’t need to rethink their data structures and can apply them directly to ScyllaDB. The learning curve is reduced to a minimum for regular Cassandra users. The first tests made by the ScyllaDB team report rates from 10 up to 20 times higher than Cassandra. Thus, they present themselves as a direct competitor. All this just moving from Java to C++ ? Not only. The advantage of having an application written in C ++ is actually to reduce CPU usage by avoiding the program to be loaded into a JVM. Scylla also provides a custom network management that minimizes resource by bypassing the Linux kernel . No system call is required to complete a network request. Everything happens in the userspace, limiting the number of expensive switches with the kernel space. Another advantage with C++ is the ability of   having a finer but more complex memory management. Indeed, in Java, the garbage collector takes care of regularly browsing  the allocated memory to release the unused space. This step is extremely costly in terms of processor cycles and it can stop the application up to several seconds on large memories, it is also known as “stop the world”. Although these times of garbage collection tend to decrease with new, more efficient algorithms, it is now recommended not to allocate more than 8GB of memory to the JVM when running Cassandra, even if your machine has over 100 GB of memory (something more and more common with the decrease in the cost of RAM). ScyllaDB boasts to fully use the hardware resources of your machines. In particular, when Cassandra is primarily based on caching offered by the operating system, a new cache system specific to Scylla was set up to compress and store recently read data in memory. Our tests Following these attracting statements on paper, we wanted to get to the bottom and check by ourselves, to see if the facts would finally concur with the theory. Scylla’s ambitions maysound too great to be true: having a 10 fold faster database without having to change anything in hardware and code, as claimed by the homepage of the site scylladb.com (see image below). scylladb.com : according to their website, it would be possible to stop Cassandra and to start Scylla without any problem on any machine of the cluster, and then multiply our throuhput by 10 We ran some tests on Amazon Web Services EC2 following the guidance we found on ScyllaDB website. It provides an image (AMI) to be loaded on an EC2 instance, on which Scylla is already fully installed on Fedora 22 OS. First test We set up three clusters : A 3 nodes ScyllaDB cluster A 3 nodes Cassandra cluster 12 shooter nodes that stress the two clusters sending requests for a short period of time All Scylla and Cassandra nodes use the same type of instance the same machines: m3.xlarge. All Shooters nodes are also the same: c3.8xlarge. The ScyllaDB cassandra-stress tool is used to benchmark the clusters. We got the following results, more detailed in the table below: 215,000 writes/second on ScyllaDB 113,000 writes/second on Cassandra First comment: Scylla is sharply better than Cassandra at write throughput and write latency. Some cautions, however: Scylla runs on Fedora and Cassandra runs on Ubuntu We used pre-configured AWS AMIs without modifying them Second test We decided to leave these results besides and realize new tests. This time: Scylla and Cassandra run on the same machines, on Fedora and with a 10Gbit network. We have now 17 shooters We used default configurations The goal of this second test is to push Scylla to its limit to check the maximum throughput it can handle. And the result was surprising: Scylla can’t handle the throughput and only returns timeouts to the shooters, this means that the load is not absorbed. When reducing the number of shooters, we actually get back a great throughput without any timeout. Indeed, Scylla can’t handle the load whereas Cassandra doesn’t have any problem under high loads: when Cassandra is overloaded, the shooter’s throughput automatically decreases (because queries are synchronous) without any timeout. This means that Cassandra successfully pulls up the pressure to clients. In Scylla’s case, we’re exposed to tuples loss when we query it at a throughput that is over its limit. In retrospect, when having a closer look at the results of the first test (see table above), we also note higher performance disparities on Scylla than on Cassandra: On write latency, there is a 11% standard deviation on Scylla, against 1% on Cassandra On write throughput, there is again a 11% standard deviation on Scylla, against 1% on Cassandra The standard deviation allows us to characterize the database management systems’ stability. In our case, Scylla seems really less stable than Cassandra. This means that besides the doubts on its capability to adapt to high loads, it can be tricky to precisely predict a Scylla’s cluster performances . We should obviously investigate further on these instabilities to establish their causes. Moreover, the tests should be run several times on correctly configured machines to be worthly benchmarks. Nevertheless, they allow us to have a preview on Scylla’s potential and status. Beyond those experimental notes, we can notice several things about Scylla: Scylla uses CQL 3.0 (against CQL 3.2 for Cassandra 2.1 and CQL 3.3 for Cassandra 2.2) as query language. The CQL ALTER function is currently not supported, like counters that have not been implemented yet. Because of this, Scylla’s cassandra-stress is slightly different from Cassandra cassandra-stress. The Cassandra version of this tool doesn’t work with Scylla. Scylla uses its own gossip protocol between nodes, that makes impossible to run a cluster containing both Cassandra and Scylla nodes. Scylla doesn’t support SSL secured communication yet (node to node and node to client). Conclusion The results we draw from these experiences are that it is possible to improve Cassandra’s performances with a finer memory management and by saving some processor cycles. Scylla seems to handle a throughput at least 2 times higher than Cassandra can (let’s recall that Scylla’s team announces a 10 factor). However, the program is far from Cassandra’s state of maturity and Scylla currently misses many essential features. Today, Scylla is in beta version and further tests should be made after its first stable release that is planned in January 2016, so we can assess this new competitor’s seriousness. Will Scylla preserve its advance in terms of latency when it will be more stable and it will have implemented all of the missing features? Moreover, Cassandra is a proven technology, running on clusters containing more than 1000 nodes (see this presentation ). Will Scylla be able to reach such a level of maturity? Thank you OCTO guys for having contributed to this article. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 6 commentaires sur “ScyllaDB vs Cassandra: towards a new myth?” Dor 15/12/2015 à 12:21 Thanks for kicking the Scylla tires! \r\n\r\nI'm glad that you find Scylla 2X faster in throughput and latency\r\nthan Cassandra. Usually we test on much larger machines than m3.xl which only has 4 vcpus and capped networking (no enhanced networking) and disks. \r\n\r\nIn general, we get awesome out-of-the-box performance on physical machines on Softlayer and Rackspace. On Amazon, we recommend the i2 family with good ssd:vcpu ratio. Even there the storage is relatively slow and Scylla doesn't handle it that well. There is an active development around it by Glauber Costa which should be ready for merge in 1-2 weeks. \r\n\r\nOne comment about the latency - It's advised not to test latency any device under 100% load. Better test it under the desired normal operation mode. In your test, take for instance half of the load that Cassandra can handle, around 50k OPS and measure the latency for both databases. \r\n\r\nAbout the timeouts with the larger machine, it probably was caused by lack of back-pressure (for slow disks and also for RF=3, CL=1). We have fixes in place, some are part of 0.13 and the rest will follow in 0.14.\r\n\r\nAlter table and encryption/authentication are being developed while we speak. We're still in beta but do try to move as fast as we can both in terms of closing the gap and in terms of stabilization.\r\n\r\nDo continue to provide feedback and we on our side will try to be as transparent as possible.\r\n\r\nCheers,\r\nDor Thomas Mouron 22/12/2015 à 11:19 Hi Dor,\r\n\r\nThanks for your interesting comment. We made those tests just to make sure ScyllaDB was a real Cassandra’s competitor in terms of performance. And it actually is. I believe the configurations we used were not suitable to proclaim that this is a serious benchmark : we just aimed to get a feel of Scylla’s position.\r\n\r\nWe keep watching Scylla’s status. Once a stable version will be released, we will probably run new tests on a typical production configuration and try not to be biased by a weak hardware. We’ll also consider your recommendation and maybe run these tests under different workloads.\r\n\r\nGood luck for Scylla’s development,\r\n\r\nThomas Dor 23/12/2015 à 08:48 Thanks Thomas, the tests are definitely helpful and the second test exposes an issue we have and is partially fixed (on the way for a full fix). We'll release another 1-2 beta versions and then a 1.0, stay tuned :)\r\n\r\nThanks for the translation to English, the post receives more audience and better than google translate's version of 'AMI' (friend according to them..) Benoît CANET 09/06/2016 à 15:30 Hi Thomas,\r\n\r\nScylla 1.0 is out for some time and we are navigating 1.2.\r\nSome serious works has been done to alleviate the AWS SSD timeout issues\r\nbefore releasing 1.0.\r\n\r\nBest\r\n\r\nBenoît anonymous 03/08/2016 à 05:25 \"Average latency\" in isolation is a poor metric for measuring latency; more useful is e.g. 95th percentile and 99th percentile.\r\n\r\nThanks Roy 23/12/2017 à 04:03 Very useful.hopes giving a  latest version test result. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-12-15"},
{"website": "Octo", "title": "\n                Centralize logs from Docker applications            ", "author": ["Thibaut Gery"], "link": "https://blog.octo.com/en/centralize-logs-from-docker-applications/", "abstract": "Centralize logs from Docker applications Publication date 22/12/2015 by Thibaut Gery Tweet Share 0 +1 LinkedIn 0 This article aims at showing how we can centralize logs from a Docker application in a database where we can then query them. This article is built around an example where our application consists of an nginx instance, an Elasticsearch database, and Kibana to render beautiful graphs and diagrams. The code of the example is available on github . We need to collect and transport our logs as a data flow from a distributed system to a centralized remote location. That way, we can get an aggregate vision of the system in near real time. The logging system is plugged at the container level because the application should be loosely coupled with the logging system. Depending on the environment (development, pre-production, production) we might not send logs to the same system: a file for development, Elasticsearch for pre-production, Elasticsearch and HDFS for production. Architecture Choosing our middleware We need a tool to extract the logs from the Docker container and push them in Elasticsearch . In order to do that, we can choose between several tools like Logstash or Fluentd . Logstash is built by Elastic and is well integrated with Elasticsearch and Kibana. It has lots of plugins. Fluentd describes itself as an open source data collector for unified logging layer. Docker provides a driver to push logs directly into Fluentd. Fluentd also has a lot of plugins like one to connect to Elasticsearch. I’ve chosen Fluentd because Docker pushes it, and Kubernetes (an important Docker project) uses it. Furthermore, in our example, Fluentd Elasticsearch’s plugin plays well with Kibana. Infrastructure We can use two types of infrastructure: either a classic architecture with servers or a cloud-based one. I’ve chosen the classic one for simplicity’s sake. Therefore two servers are needed, one for our application and Fluentd, and one for our Elasticsearch database and Kibana. Process The process can be described in 6 steps Users connect to our application (nginx) and this generates logs Our containerized application sends its logs to stdout and stderr Docker intercepts logs from the container and uses its native Fluentd output driver to send them to the Fluentd container running locally Fluentd parses and structures logs Structured data are sent to Elasticsearch in batches, we might have to wait a minute or two for the data to arrive in Elasticsearch. We can parameterize this behavior with the Buffer plugins Data is exposed to administrators through graphs and diagrams with Kibana Application As the application is a simple nginx, I’ve packaged a new image since the official one uses a custom logger that is not appropriate for our purpose. We can run the app using Docker-compose up with the following Configuration $ cat ./Docker-compose.yml\r\nnginx:\r\n  image: thibautgery/Docker-nginx\r\n  ports:\r\n    - 8080:80 Fluentd Fluentd is a middleware to collect logs which flow through steps and are identified with tags. Here is a simple configuration with two steps to receive logs through HTTP and print them to stdout: $ cat ./fluentd/fluentd.sample\r\n<source>\r\n  @type http\r\n  port 8888\r\n</source>\r\n\r\n<match myapp.access>\r\n  @type stdout\r\n</match> In this sample, each step defines how the data is processed: The first step defines how to capture the data, here on port 8888 using HTTP. The second step defines how to output the data, in this case by printing it to stdout. The data is streamed through Fluentd. Each chunk of data is tagged with a label which is used to route the data between the different steps. In the previous example, the tag is specified after the key match : app.access . The tag of the incoming data is the URL of the request. For example running curl http://localhost:8888/myapp.access?json={\"event\":\"data\"} outputs {\"event\":\"data\"} to stdout. This slideshare explains the basics of Fluentd. Each step is a plugin. There are more than 150 plugins divided into 6 categories. The most important ones are: Input plugins to accept and parse data Output plugins to send the data to external systems, in our example Elasticsearch Configuration Docker pushes its logs to Fluentd First of all, the Fluentd agent can run anywhere, but for simplicity’s sake we run it on the same node as the application. The official image can be found on the Docker hub. We need to configure the Fluentd agent : $ cat ./conf/Fluentd\r\n<source>\r\n  @type forward\r\n</source>\r\n\r\n\r\n<match nginx.Docker.**>\r\n  @type stdout\r\n</match> Fluentd accepts connections on the 24224 port and prints logs on stdout thanks to two default plugins in_forward and out_stdout We can run Fluentd with Docker-compose -f Docker-compose-fluentd.yml up $ cat ./Docker-compose-fluentd.yml\r\nFluentd:\r\n  image: fluent/Fluentd\r\n  restart: always\r\n  ports:\r\n    - 24224:24224\r\n  volumes:\r\n    - ./conf:/Fluentd/etc The default logging format option for Docker is json-file. We can use the log-driver option to specify Fluentd. By default it connects to localhost on the 24224 port. $ cat ./Docker-compose.yml\r\nnginx:\r\n  image: thibautgery/Docker-nginx\r\n  ports:\r\n    - 8080:80\r\n  log_driver: Fluentd\r\n  log_opt:\r\n    Fluentd-tag: \"nginx.Docker.{{.Name }}\" The Docker driver uses a default tag for Fluentd: Docker._container-id_ . We override it to be nginx.Docker._container-name_ with the log_opt , Fluentd-tag: \"nginx.Docker.{{.Name }}\" . The tag in the Docker driver must match the one in Fluentd. We should be able to see the Nginx logs in Fluentd container log. Right now, our system is useless. We need to send logs to a distant database, Elasticsearch. Fluentd pushes its log to Elasticsearch Fluentd needs the fluent-plugin-elasticsearch in order to send data to Elasticsearch. I have packaged the image here We need to update the Fluentd agent configuration $ cat ./conf/Fluentd\r\n<source>\r\n  @type forward\r\n</source>\r\n\r\n\r\n<match nginx.Docker.**>\r\n  type elasticsearch\r\n  hosts http://elasticsearch.host.com:9200\r\n  logstash_format true\r\n</match> Don’t forget to change the hosts to point to the Elasticsearch instance. The logstash_format true configuration is meant to write data into ElasticSearch in a Logstash compliant format, hence allowing the leveraging of Kibana. We can run Fluentd with: $ cat ./Docker-compose-Fluentd.yml\r\nFluentd:\r\n  image: thibautgery/fluent.d-es\r\n  ports:\r\n    - 24224:24224\r\n  volumes:\r\n    - ./conf:/Fluentd/etc Then we can run the application and query it with our favorite browser to fetch some lines form Elasticsearch in the Logstash index. Since Fluentd buffers the data before sending them in batches, we might have to wait a minute or two. Unfortunately, only the Docker metadata are sent (like the Docker name, label, id…) but the log field contains the raw log lines of nginx and it is not structured. For example, we cannot query all failed HTTP requests (status code >= 400) This line of log need to be parsed. Structure the application logs Fluentd needs the fluent-plugin-parser in order to format a specific field a second time. I have packaged the image with it here We need to update the configuration : $ cat ./conf/Fluentd\r\n\r\n<source>\r\n  @type forward\r\n</source>\r\n\r\n<match nginx.docker.**>\r\n  type parser\r\n  key_name log\r\n  format nginx\r\n  remove_prefix nginx\r\n  reserve_data yes\r\n</match>\r\n\r\n<match docker.**>\r\n  type elasticsearch\r\n  hosts http://elasticsearch.host.com:9200\r\n  logstash_format true\r\n</match> The second block of configuration : uses the plugin parser parses the log field parses it using the pre-build Regex of Nginx removes the prefix nginx on the tag nginx.docker.** keeps the previous informations in the message and emits it as docker.** Here we use the tag concept to route the data through the correct steps: the data arrives with the tag set by docker driver : nginx.docker._container-name_ Fluentd sends it to the second step the tag is modified to docker._container-name_ data goes through the third step data is sent to Elasticsearch Here is the structured data we can now used to create diagrams : We can then run the application and query it with our favorite browser to see the data correctly formatted in Kibana. Run it Our system collects logs from our application and send them to Elasticsearch. The Docker engine requires to have Fluentd up and running to start our container. Even if Fluentd dies, our containers using Fluentd continues to work properly. Furthermore, if Fluentd stops for short periods of time, we do not lose any piece of log because the Docker engine buffers unsent messages, so that they can be sent later when Fluentd is back online. Finally, since Docker 1.9 we can show labels and environment variables with the logging driver of Docker. In our example, we added: service: nginx and it shows up in Kibana. We are now able to create graphs such as this one: Conclusion So far, we have seen how to collect and structure logs from Docker to push them in Elasticsearch. We can easily change the Elasticsearch plugin to the Mongo or HDFS plugin and push logs to the database of our choice. We can also add an alerting system like Zabbix We can add nodes to our infrastructure and add several containers in one node. Keep in mind that this article doesn’t cover everything. For instance we have not answered the following questions : how to monitor the Docker running Fluentd ? how to keep high availability in the monitoring system ? Run everything in two commands with the Ansible scripted repository Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-12-22"},
{"website": "Octo", "title": "\n                Which small bicycle with chrome-plated handlebars..            ", "author": ["Christophe Thibaut"], "link": "https://blog.octo.com/en/which-small-bicycle-with-chrome-plated-handlebars/", "abstract": "Which small bicycle with chrome-plated handlebars.. Publication date 07/01/2016 by Christophe Thibaut Tweet Share 0 +1 LinkedIn 0 TL;DR: TDD is like biking: – it’s faster than walking – it’s counterintuitive at first – once you know how, you never forget – I heard that you’re working on the new XXL project. Congratulations! How is it going? – Not bad. We’re facing a few difficulties setting everything up, but nothing too bad. – what kind of difficulties? – Well you know, integration with new frameworks, a few bugs and quirks here and there… – Bugs? do you have tests? – The Product Owner does the tests for now… – No I meant unit tests, automated tests, written by developers, against their code. You know, like I showed you on the XL project. – Oh! Unit tests, OK! Yeah I told the team about it, but they don’t really have the time right now, we have to ship stuff. – Oh dear, stop. – What? – They said: “We don’t have the time to write unit tests because we have to ship”. – Yes. – Is this a joke? – I swear it’s not. What are you thinking? – Your team doesn’t know the kind of tests that I’m talking about. If it were the case, they would write their tests right now instead of putting them off. For now, it seems to me that they fell into the beginner developer’s trap. – The beginner developer’s trap? what’s that? – “We will improve quality later when we have the time” – How is it a trap? – It will never happen. – How do you know? – Today, you have some quality issues. – Yes, only a few, nothing overwhelming. – Was it in your roadmap? – Not really. – Therefore, you already have more work needed than planned. – Well, that’s the reason behind putting off automated testing, you see… – Because later, you’ll have less work to do? – Maybe… – Just to be sure: Tomorrow you’re going to tell your Product Owner “Indeed we spent some unplanned time fixing bugs, we are a bit off schedule , but we made a release. Now, what we would like to do is to deliver a bit less features and take the time to improve the quality of our codebase”. – Well I wouldn’t put it like that… – There are not a thousand ways to put it, in my opinion. – Alright, OK. But today, the team needs to show some results. – Of course. It’s essential. And they also need to learn. – What do you mean? – Your team says: “We’ll do the unit tests later because now we need to deliver features.” I infer that the developers in your team don’t know unit tests and thus need to learn. – And why unit tests? Why not rely on acceptance tests at the end of the sprint? – You can see that this is not enough. You already have several bugs. – Well, we’ll ask the product owner to do more testing. – Don’t do that, it’s a waste of time and effort. – How is it a waste of time? – Acceptance tests will become harder and will uncover less bugs than you would with unit tests – How are you so certain? – First, your product owner can validate the product you are making but cannot validate your code for you. – Granted… – Second, even if your product owner validated your code at the end of each sprint, it would be too heavy a load of information to be efficient. Do you honestly think you can fix a week’s worth of code each friday afternoon? – Don’t tell me about it, we already have a hard time merging versions from two workstations… – Third, even if it were an acceptable feedback delay, once the bugs are uncovered and fixed, you would certainly look for a way to avoid the same kind of bugs in the future. – Obviously! – To detect bugs in the code efficiently, you need a systematic approach: write unit tests as close as possible to the code you are writing. – What is the closest as possible? – The sooner, the better: at the same time you are writing the code and in the development environment. Remember what I showed you on the XL project – You mean Test Driven Development? – TDD or any other technique which allows to write tests close to the code. – I should talk about it with my team. – I wouldn’t wait too long if I were you. The developers in your team have everything they need to write code of good quality: JUnit and a working brain. – Yeah, but you know how it is, tests are considered like a chore – That’s the first thing that needs to change. It’s because your team is not trained. – Hmm. – Do you remember the time when you were learning how to bike? I remember. I had a brand and shiny new one! Because I didn’t know how to ride it, I use to walk alongside holding and wheeling it. It was fun at first but it quickly became frustrating, even for others, because they had to wait for me. My sister then explain to me that the “thing” with bikes is to hop on it and ride. It goes a whole lot faster. The bike carries you, not the other way around. – What happened next? – I crashed quite badly, obviously. It didn’t work out. My sister, who must have been tired of hearing me whining, climbed down from her bike. She explained how it worked and she helped me by holding the saddle while walking beside me. When she had to run, I knew how to ride, simple as that. – What’s the connection with TDD? – TDD is like riding a bike: it makes you go faster provided you invest a little time to learn how. Your approach to testing “later, maybe, if we have time” actually slows you down. It’s exactly like walking along your bike. – What do you recommend? – Give it a try! You need 3 days to learn, 3 weeks to make it a habit and in a few months you’ll completely master the approach. – Ok but right now, we don’t have much time… – You’re doing it again… – Just joking! Thanks for the advice. I’ll talk to the team! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Which small bicycle with chrome-plated handlebars..” Christophe Thibaut 07/01/2016 à 20:46 Thanks Florent Jaby for english translation. :-) Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-01-07"},
{"website": "Octo", "title": "\n                Microservices architecture without the hype: what it is, what’s the point, do I need one?            ", "author": ["Julien Kirch", "Oliver Baillot"], "link": "https://blog.octo.com/en/microservices-architecture-without-the-hype-what-it-is-whats-the-point-do-i-need-one/", "abstract": "Microservices architecture without the hype: what it is, what’s the point, do I need one? Publication date 03/12/2015 by Julien Kirch , Oliver Baillot Tweet Share 0 +1 LinkedIn 0 In 2015, a peak in microservices was reached: there is no conference without a Netflix engineer to sell you a dream, not a week without new magic framework to do it all without asking any question. Result: a focus on the tools and beautiful stories rather than substantive issues. It therefore seemed useful to us to review the architectural aspects of microservices, because choosing a style of architecture for an information system has structural consequences on projects lifecycle and company organization. Vocabulary note Before getting into the substance of this subject, it is necessary to clarify two concepts: Service stands for business service, ie a group of technical services (REST, SOAP …) that, as a whole, provide a feature that has a business sense. Project stands for an IT development project throughout its lifecycle and not only during its “project phase” before  switching to maintenance mode. 1) Why microservices: large projects issues Architecture microservices was invented to solve some of the problems caused by large projects. Over time, IT projects tend to grow: we extend gradually existing features, we add others, and rarely remove old ones. While the code and the project extend, a certain number of pain appear: Complexity When the amount of code increases, the code becomes more and more complex. Even with a solid software architecture, interdependencies between different bricks increase over time. This complexity has two disadvantages: Scalability and Reliability As time goes by, when new business functionalities become more complex, the different bricks have more interactions. It doesn’t matter how much we organize code into layers and components, there are always special cases and patches that make things more blurry. Beyond a certain threshold, it becomes impossible to have in mind a global model of the project. Even with solid tests foundations, the multiplication of side-effects of each action makes the system less reliable, and it becomes more difficult to add new features and perform proper refactorings. Horizontal Scalability Improving the scalability of a system may require to modify structural elements of the project. The bigger a project is, the more costly and risky these interventions become. The risk is to end up with a system that is impossible to evolve with a new use case. Innovation Technological innovation To capitalize investments and facilitate the management of people, it is normal to want to have consistency between projects of a company: same way of working, same programming languages, same tools. Each project is invited to follow transverse choices and may deviate depending on its specific features, provided to justify it. For large projects, the same tension takes place within the same project: to avoid fragmentation, each technical change must be propagated to the entire code. Over time, the changes become more and more expensive, and it is more difficult to introduce new tools for specific needs. Business innovation To meet the new business requirements, we must be able to arrange an area for innovation within the projects. Because some innovations are implemented by new projects, most are done on existing projects. Thus, bigger the project is, more critical it is for the company, less we will take risks to change it to test new products or new markets, and gradually the challenges of stability will prevail over the ability to innovate. 2) The idea of ​​the microservices architecture The problems just described above have long been known, and practices from agile methods and software craftsmanship can limit their impact. Unfortunately, these recipes require to be constantly rigorous, and even more as a project gets bigger and people change. So they are often implemented on paper or with flaws, resulting in unmaintainable projects that we drag for years before replacing them. At this danger, the response of microservices is simple and rather radical: to avoid large projects issues, you only need to have small projects. So we will limit the size of projects to a few people in order to have feature teams with a maximum size of seven people all-inclusive. By cutting projects and existing teams when necessary to comply. This is not a question of separating big projects into sub-teams but independent projects: each has its organization, timing, code base and data. Exchanges between projects are made by services, whether service calls (REST / JSON) or messages. Rather than big projects, we prefer small independent projects, each with its team, its code and its data The cutting is done by business area, grouping services and data types that have strong links and separating them when they are sufficiently independent. If the typical configuration is a deployment unit per team, the rule is rather to have at most one team per deployment unit. If the business domain is implemented by multiple applications, they will be carried by the same team. If you do not understand why this is called “microservices”, no worries: it is a buzzword to make the concept attractive. Likewise, fans of this approach called “monoliths” conventional applications, to highlight their negative side, even threatening. This architecture is thus at the confluence of several underlying computing trends: SOA has highlighted the benefits of services approach; Agile and lean startup provided the organizational models teams; The industrialization deployments and virtualization enable to lower the costs of production and exploitation; NoSQL has relaxed things on the data integrity side. 3) The perks of the microservices approach Complexity Scalability and Reliability Constrain the size limit of individual cases and allow to have in mind all the behaviors. Technical debt is kept under control, and the code is thus able to evolve. Go through service calls to communicate with other areas formalizes exchanges. Interface contracts are then more strict, and it is easier to consider all cases, including cases of errors. Horizontal Scalability With applications of limited size, it is easier to increase the scalability by refactoring the code or by rewriting it completely based on new requirements. Innovation Technological innovation Code bases and teams are independent and can therefore make technical choices according to their own needs. Business innovation If all the information system is structured in services, it is easy to experiment by starting a new project based on others’ data and easier to remove features because it is the whole project that will be deleted. 4) Requirements and limitations If microservices architecture has many advantages, it has many requirements and a certain number of limitations. The microservices being a variation of the classic SOA architecture, we will find the same characteristics, but with an additional level of criticality. The system becomes distributed Conventional architectures make it possible to ensure to have independent states between different applications: everyone is the master of his business field. When switching to microservices, the system becomes widely distributed. This introduces new particularly difficult issues. The most complicated case is about transactions: each time a transaction is shared between two applications, we must manage transactions in two phases or manage cancellations. In a system based on services, there is no tool that allows to take it into account in an automated way. We must do it manually at each location of the code. And even when you can bypass transaction: there are always references to cross-application data, and therefore a management system of asynchronous events or cache to be implemented to ensure data consistency. Then there is the case of external services unavailability. Because using services of another application means to depend on it. The design for failure approach allows to limit risks but require to have a rigorous engineering. It is also important to master all the service quality (SLA) for different applications in order not to be surprised. Eventually the system becomes more difficult to test: integration tests increase, and require to prepare the data and be well equipped to test cases of technical and business errors. Value-added services Although the REST approach suggests to handle simple features, there is always a proportion of calls with “value-added” that involve multiple business areas. Regarding microservices, it means dial calls between several applications. This has the effect of multiplying cases of errors to manage (problem of distributed systems) and adding network latencies. In the most critical cases, it becomes necessary to add specific services in different applications or add data caches, causing consistency issues. Transverse evolutions With separate projects and thus independent teams, transverse evolutions are harder to implement. This requires to synchronize different groups or to establish a complex lifecycle versions system. The problem is even worse when you want to iterate quickly because it requires that everyone synchronizes themselves all the time. To maintain flexibility, the natural solution is to isolate clusters of other projects by limiting the interconnections between groups (pattern Kingdom / Emissary). The risk is to add a middle management which is not directly linked to projects. Bundled projects and specific services for inter-group communications DevOps and provisioning Multiply applications means multiply the number of deployments and server instances. To avoid error and excessive additional costs, we need a very efficient workflow in terms of tools and processes with as much automated deployments as possible. This is even more true for tests and POCs where we want temporary environments as sandbox. Projects quick start and people allocations Choose people, arrange transfer, establish a budget…: within a traditional organisation, create a new project can take a lot of time and money. In order to make it possible to have multiple projects each living their own life, it is necessary to industrialize this organizational aspect. Within a large project, the production capacity can be reallocated between different parties, whereas smaller structures are more sensitive to workload changes. It is therefore necessary to be able to expand or reduce teams without causing too many constraints. We are not talking about setting up shared pools of developers or moving people as pawns, but to have a certain flexibility. Maturity operation and monitoring Highly interdependent services requires : a very good monitoring flows tool to find out quickly where problems arise great operating maturity because it will multiply outages a status state available for  services consumers so they can understand where outages come from when they have consequences for them. Technology and maintaining skills Technology choices happening in every team in a decentralized way, it is easier to make mistakes: The trade-offs between innovation and sustainability are more difficult. Enable innovation to meet new requirements means accepting to make mistakes sometimes. There is also the risk of neglecting good development practices because there are fewer challenges and risks. Finally, smaller applications have more often break periods during which there is no evolution to develop, with for instance a  change to TMA mode. In this case, team members are spread elsewhere and the risk of loss of consciousness is important. Strategy and governance For large projects related to companies products , strategic vision comes directly from the business. The partners are not many, it is easy to arbitrate between the different demands depending on the weight of each. With microservices, many technical projects will be away from the business and have numerous interlocutors. Therefore we need a mature organisation in its communication, management of priorities and prioritization mechanisms. 5) Do we need it? The fundamental SOA approach is to keep control of organisational and business complexity by distributing it. By separating the projects, the complexity is reduced on some axes in exchange for an extra cost in other places, including having a distributed system. You can have well organized monolithics, scalable, evolutive…, but it requires strong discipline at all times. Microservices architecture chooses not to take those risks to make sure to keep control. However, if this is implemented in an unsuitable environment or in a bad way, we will combine disadvantages without enjoying the benefits, and we therefore take much higher risks than in conventional service architecture. So, do not tell yourself that you need microservices, ask yourself: If you have issues that this approach solves; If you have necessary requirements, or if you are ready to reach them before starting the migration. Only in this case ask yourself this question. And do not forget that an architecture is a tool that we adapt to our need and not a dogma to follow: if what suits you is a hybrid solution taking some microservices ideas and not others, go for it! 6) How do I go there? Once decided that a microservices architecture is the right solution, we need to find a way to setting it up. If there is no magic solution, some approaches seem to emerge. The difficult case: from scratch The most attractive situation is to create a new system from scratch, nothing to challenge or to manage, this seems the ideal situation. Unfortunately, build microservices from scratch is the most difficult case: It is complicated to determine, so it seems, the limits where we need to cut out the different projects because it is not clear how the system will evolve. As we have already seen, the evolutions are more costly because you have to make cross-project refactoring. Unless to be already mature on a subject, it is better to go for a monolith approach to begin with. The favorable case: peel a monolith The most favorable case is the monolith that we “peel”. In reviewing its organisation and its structure, we will outsource pieces to the edge of the system following the cutting lines that emerged naturally. The goal is not to end up with 50 mini-projects but rather: one or several “core” applications of average size, consistent with each other; microservices moving around, which are going to move away with time. To cut a monolith, we isolate consistent functionality groups between each other to turn them into projects. This operation is made easier as the initial application is well structured in technical layers, business bricks and that this reorganisation is respected. The best practices of software developments allow to have “microservices-ready” projects. Otherwise, it takes a lot of investigation to extract some parts of the code. Automated tests are essential to limit risks. In their absence, it is necessary to consider the application as a legacy and use the proper techniques to remove the technical debt. Before getting into the “cutting” phase, we must examine the data distribution issues: this is the most structural element and can make the operation impossible. Finally, we must avoid to be dogmatic considering that the operation is necessarily one-sided. If later, others projects evolution are getting close to each other and more issues arise by separating them than they are solving, we must not hesitate to merge them back. Merge two projects back is not an admission of failure but rather a good sign because it shows that when your business evolves, your information system is able to adapt. To go further How we ended up with microservices sur l’expérience de SoundCloud Microservices. The good, the bad and the ugly Out of the Fire Swamp – Part III, Go with the flow sur les questions de données Introduction to Microservices sur le blog de Nginx MonolithFirst par Martin Fowler Manœuvre de Conway inversée chez ThoughtWorks Domain-driven design Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged REST . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-12-03"},
{"website": "Octo", "title": "\n                Keep your gradle dependencies up to date seamlessly            ", "author": ["Thomas Dalous"], "link": "https://blog.octo.com/en/keep-your-gradle-dependencies-up-to-date-seamlessly/", "abstract": "Keep your gradle dependencies up to date seamlessly Publication date 11/12/2015 by Thomas Dalous Tweet Share 0 +1 LinkedIn 0 Keeping your dependencies up to date is not the funniest part of a project dev process. Especially if the dependencies list becomes long. However, it is crucial to keep your dependencies as possible close to the up-to-date versions available in order to benefit from the latest upgrades (such as bug fixes). The longer you wait, the harder the upgrade will be. So what if you receive an email every week to inform your team about the last version available of your projects dependencies? Some tools like Lint (or other static code analyzers) already provide such features, but as long as you don’t keep an eye on their reports you will not be warned about new versions. In this quick tutorial we will setup a Jenkins job running a gradle plugin as a task in order to receive something like this by email: The following dependencies have later milestone versions: – com.android.support.test.espresso:espresso-core [2.0 -> 2.2.1] – com.facebook.android:facebook-android-sdk [3.23.1 -> 4.8.2] – com.fasterxml.jackson.core:jackson-annotations [2.5.3 -> 2.7.0-rc1] – com.fasterxml.jackson.core:jackson-core [2.5.3 -> 2.7.0-rc1] – com.fasterxml.jackson.core:jackson-databind [2.5.3 -> 2.7.0-rc1] Step 1: install the gradle plugin We use the following gradle plugin: https://github.com/ben-manes/gradle-versions-plugin As described in the gradle-versions-plugin doc you just have to add these line to your gradle file: Step 2: create a new jenkins job Then go to your Jenkins web interface and create a new job, lets call it “[YourApp]-DependenciesCheck”. Just add the following command in your task: dependencyUpdates -DoutputDir=${WORKSPACE} where you define the target directory (thanks to “-DoutputDir”) of the result using the “${WORKSPACE}” constant (defined in your Jenkins configuration). Then specify the app root folder name in the root build script field: Invoke gradle script sample configuration You may want to make the gradle plugin output more specific with only latest release versions (no beta, rc etc.), there is an option for that: - Drevision = release and plenty of others that you will find on the github repository readme . Step 3: configure the email notification Once your job is ready, we need to add a plugin to Jenkins in order be able to send an email with custom content: https://wiki.jenkins-ci.org/display/JENKINS/Email-ext+plugin (the default one is not as customizable as this one). Then specify the project recipient list, the default subject, default content and the attachments fields: Email notification sample configuration Note: ${FILE,path=”report.txt”} refers to the command output and print its content in the mail body. And voilà! Choose the frequency of the job execution, we recommend you at least weekly, for instance every Monday at noon: H 12,12 * * 1 (find more info about this syntaxe here ) Step 4: no step 4 To conclude these easy steps to follow will allow you to maintain your dependencies up to date and as a consequence improve your project quality. However be careful, a major version update may introduce some breaking changes that need to be analysed deeper in the changelog. Feel free to ask any question in comments or to suggest improvements/others solutions. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged android , CI , code quality , dependencies , Gradle , Jenkins , Maven , mobility , productivity , tips&tricks , Version . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-12-11"},
{"website": "Octo", "title": "\n                HTTP/2 arrives but sprite sets ain’t no dead            ", "author": ["Benoît Beraud", "Alexandre Masselot", "Alexandre Masselot"], "link": "https://blog.octo.com/en/http2-arrives-but-sprite-sets-aint-no-dead/", "abstract": "HTTP/2 arrives but sprite sets ain’t no dead Publication date 21/12/2015 by Benoît Beraud , Alexandre Masselot , Alexandre Masselot Tweet Share 0 +1 LinkedIn 0 In this study, we show that even if the new HTTP/2 protocol significantly enhances page load performance, time has not yet come to totally forget front end optimizations. Today, we will focus on sprite sets. HTTP/2 became available in 2015, as an alternative to replace the venerable HTTP/1.1, in use since 1997. Many authors [ 1 , 2 ] foretold the deprecation or even the counter productiveness of front end optimizations. Among those classic optimizations are sprites: the encapsulation of multiple small images (the sprites) within a larger one (the sprite set). Despite the rapid adoption of this new HTTP standard by both browsers and servers [ 3 , 4 ], we were not able to find published hard measures to support the claim. As web architects, we were then legitimately wondering whether we should deprecate the sprites pattern or not. As W. Edwards Deming famously quoted it: “In God we trust, all others bring data.” We therefore launched our own benchmark. The first part of this article describes the main differences between HTTP/1.x and 2 and why they could favor sprites deprecation. In the second part, we will then present various results. Sprite sets Sprite sets are a front-end optimization . Instead of individually loading multiple images from the server (think about a collection of icons used all over a web page), a unique sprite set is loaded once and individual sprites can later be cropped out . The sprite set used in our benchmark, from www.facebook.com, is displayed in figure 1. Figure 1: the benchmark sprite set. This sprite set is composed of 72 sprites, which can be cut very precisely in small individual image for each sprite. The first observation is that the sprite set as a single global image is 71 kB, while the sprites as individual images represent a total of 106 kB, an increase of almost 40 %. The union volume is then smaller than the sum , thanks to a better image compression and a reduced image header overhead. Moreover, a single request to the server is sufficient to load whole site icons with a sprite set , instead of multiple requests querying individual images. In order to render images inside the browser, the sprite set is cropped in small icons through CSS code [ 6 ]. In figure 2, you can see the HTML used with or without sprites, and the corresponding CSS codes. The resulting load timelines are also plotted. CSS with individual images HTML (Common) CSS with sprite set div.sprite-icons {\r\n background-repeat: no-repeat;\r\n}\r\ndiv.sprite-icons-pause {\r\n background-image:\r\n       url('icon-pause.png');\r\n width: 60px;\r\n height: 60px;\r\n}\r\n... <div class=\"sprite-icons\r\n            sprite-icons-pause\">\r\n</div>\r\n<div class=\"sprite-icons\r\n            sprite-icons-pause-disabled\">\r\n</div>\r\n<div class=\"sprite-icons\r\n            sprite-icons-play\">\r\n</div> div.sprite-icons {\r\n background-image: url('icons.png');\r\n background-repeat: no-repeat;\r\n}\r\ndiv.sprite-icons-pause {\r\n background-position: 0 -60px;\r\n width: 60px;\r\n height: 60px;\r\n}\r\n... Figure 2: with and without a sprite set. Styling syntax is displayed for both solutions on the upper line. The two images capture http request(s). Without sprite set, one can note the multiple concurrent small requests, with an overall longer completion time. We can observe how CSS code is much simpler without a sprite set, but the load timeline is way shorter with one . A couple of technical burdens car arise when using set, as they will be discussed below. However, the main limitations to the sprite set approach are : a more complex development since one has to create or adapt the sprite set and  maintain CSS code to split it in individual icons. This process can nonetheless be automated in the build factory with tools such as glue; a browser cache invalidation at each sprite set modification , even if it is just about a single sprite added, removed or modified. HTTP/1.x and sprites Under HTTP/1.x, at most one request is allowed per TCP connection between client and server. Subsequent requests have to wait in order to reuse the TCP connection. In order to get shorter load time and to avoid a page completion to be held back by a single long request, modern browsers open multiple parallel TCP connections with the server (typically between 2 and 8 connections, depending on the browser [ 5 ]. However, such a  parallelism is limited and having many requests still means that load time will be longer, network overloaded, without mentioning the backend service. Loading all independent images at once leads to many serialized requests in HTTP/1.x and strongly impacts the overall loading time, hence the user experience, while using a sprite set results in a single request which is a huge optimization on many websites. HTTP/2 and sprites With HTTP/2, all requests between the browser and the server are multiplexed on a single TCP connection [ 7 ]. Bypassing the cost of opening and closing multiple connections, this induces a better usage of the TCP connection and limits the impact of client-server latencies. It could then become possible to load tens of images in parallel on the same TCP connection. As a consequence, using sprite sets to reduce the number of requests could become meaningless. All these sentences are still hypotheses: this article will show how reality is quite different from theory. Benchmark methodology All the code to reproduce this benchmark has been made available on GitHub [ 8 ]. To reproduce various situations, six HTML pages have been crafted. The first one takes advantage of the sprite set when the later ones incorporate various quantities of individual images. Setup name Images How many images Single sprite set 100% (72) AllSplitted individual 100% 80pSplitted individual 80% 50pSplitted individual 50% 30pSplitted individual 30% 10pSplitted individual 10% The last four pages, with only a fraction of the sprites, represent a common situation, where all the sprites are not simultaneously used: only an image subset is actually needed, depending on the context (language, geographical location, application state…). By using individual images instead of a sprite set, it would hence allow to load only required images instead of the whole set. But our results will demonstrate how efficient the grouping still remains. In our benchmark, a JavaScript code has been developed  to measure the timespan between the end of page HTML load (execution of scripts in the page header) and the last image load (last ‘onload’ event). This timespan is measured and recorded for each case. On the server side, these pages and images have been installed on two NGINX 1.9.5 servers located in the same datacenter on two identical virtual machines. One server is supporting HTTP/2, while the other only supports HTTP/1.1. These pages are requested in HTTPS , even with HTTP/1.1, to allow a fair comparison with HTTP/2 which supports only secured transmission. On the client side, a Python script has been developed to request pages through two browsers, Firefox 41.0 and Chrome 45.0, driven by Selenium WebDriver [ 9 ]. Selenium allows to have a new navigator context at each request , in order to avoid caching effects. Indeed, if images were to be cached by the browser, we would not really test the protocol since there would be no actual transfer (only an empty body reply with 304 code). Selenium finally allows to easily inspect the DOM to retrieve the timespan measured by Javascript and displayed on the page. Figure 3: the test code architecture. In order to gather robust metrics, the protocol is executed a hundred times for each case , as showed in the pseudocode below. for i = 1 to 100\r\n  for page in ('Single', 'AllSplitted', '80pSplitted',\r\n               '50pSplitted', '30pSplitted', '10pSplitted')\r\n    for protocol in ('HTTP/1.1', 'HTTP/2')\r\n      for browser in ('Firefox', 'Chrome')\r\n        #load page and measure load time For each case, the median value is recorded . Indeed, when looking at the time distribution of one case (cf. figure 4), we observe outliers, due do the inherently stochastic network process. The average would hence be too heavily influenced by these points. On another hand, the median is a trustworthy indicator since the distribution almost follows a homogenous distribution. Figure 4: original loading times, when repeating the measure a hundred times. To broaden the range of actual situations, the protocol has been repeated on 3 client configurations : configuration description avg latency upload bandwidth #1 VM in well deserved datacenter 10ms 80Mb/s #2 laptop with good internet connection 40ms 20Mb/s #3 laptop with poor internet connection 35ms 1.3Mb/s Benchmarks results The three configurations have produced very coherent results, displayed in figure 5. Figure 5: median overall loading time for various page setup, browsers, http protocols and network configurations. One can observe that: the sprite set load time is equal or smaller than the load time of 10% of the individual images, even with a low latency connection. In other configurations, the sprite set is much faster to load than individual sprites, disregarding the used HTTP protocol ; HTTP/2 brings a clear decrease of load times compared to HTTP/1.1, but the HTTP protocol enhancement is not sufficient to reduce the front-end optimisations usefulness ; for this problem, the browser makes no significant difference (the difference of load times in configuration #1 are probably induced by  CPU and memory constraints on the virtual machine). To further analyse these results, one can also plot the median load time as a function of the number of requests or as a function of total image volume. Figure 6 displays results for the aforementioned configuration #3. Figure 6: the same experiments as figure 5, with configuration #3, but plotting timings versus number of image and overall image volume. With these two last figures, one can observe that the sprite set pattern is very different from the individual sprites due to the single request to perform , while the total volume seems to be a second order effect only. Conclusions This benchmark clearly advocates that sprite set optimisation is still relevant, even when upgrading to HTTP/2 protocol . Even though this new protocol offers significant load time enhancements (up to 50%) compared to HTTP/1.1, it may not be enough. If HTTP/2 optimizes network usage, it will not be sufficient to totally dismiss  front-end optimisations , among which are sprite sets, CSS and JS minification and bundles . References [1] https://mattwilcox.net/web-development/http2-for-front-end-web-developers [2] http://http2-explained.haxx.se/content/en/part3.html [3] https://en.wikipedia.org/wiki/HTTP/2 [4] http://w3techs.com/technologies/details/ce-http2/all/all [5] http://stackoverflow.com/a/985704 [6] http://www.w3schools.com/css/css_image_sprites.asp [7] http://qnimate.com/what-is-multiplexing-in-http2/ [8] https://github.com/benoit74/http2-sprites/ [9] http://www.seleniumhq.org/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-12-21"},
{"website": "Octo", "title": "\n                Gather shopping receipts: architecture overview            ", "author": ["Thomas Mouron"], "link": "https://blog.octo.com/en/gather-shopping-receipts-architecture-overview/", "abstract": "Gather shopping receipts: architecture overview Publication date 24/11/2015 by Thomas Mouron Tweet Share 0 +1 LinkedIn 0 Following our first post (in French) concerning the business challenges raised by the data collection and analysis in the retail sector, we will now present a use case with its associated issues. We will see how to face them based on modern technologies that have already proven themselves in Web giants: Kafka, Spark and Cassandra. Context A retail store actor would like to retrieve and process data in real-time such as issued receipts, the flow of goods between his suppliers, warehouses and stores, and user path on his e-commerce website. For example, he would like to get in real-time the turnover performance of each product category compared to the same day of the previous week (see graph below) so as to initiate marketing actions quickly. We will later describe an architecture that allows the development of such a feature while still having the possibility to meet use cases related to the monitoring of the stock, flow of goods, or user path on the e-commerce website (and drive store). Here are the issues we have to address: High information flow Each store banner of our 10,000 stores produces an average of 10 receipts per minute. As part of our study, we have worked with denormalized receipts, approximately 3 kilobytes each, it creates a flow of receipts of a few megabytes per second. Goods movements can also represent a flow rate of same order. By contrast, monitoring customer actions on the e-commerce website (clicks, page views) may generate a stream of tens of megabytes per second. As a first approach, we need to handle a global flow in the order of 100 megabytes per second. Large amount of data to process The previous point logically leads to a second problem: the amount of data to be stored can become significant over time. Our data warehouse will have to ingest terabytes of data each year. This is where conventional relational DBMS start to suffer: beyond a few terabytes of data, performing simple queries with joins, groupings and aggregates can become an ordeal. We no longer talk about time required to grab a coffee, but to come back the next day! Besides the problem of required storage space (the cost of storage is falling), we have to be able to handle efficiently such volumes of data: if one hopes to be responsive at D+1, a few hours must be enough to our batch execution, allowing to fix in real time the turnover calculation. To meet this need, our architecture will have to include following characteristics: Distributed All data will roughly weigh tens of gigabytes per day. Such data volumes need a high storage capacity and processing power that may be beyond what is feasible for a single machine. Fault-tolerant Falling (temporary failure, or permanent loss) of any node should not be fatal for the data processing, for a simple reason: the absolute necessity to have access to our real time turnover, so that we can make decisions quickly and at any time, even if a machine has fallen. We do not tolerate the failure of our batches: if treatment lasts a few hours, say 5h, the failure of one node after treatment shall not interfere with it, to effectively exploit the results of our batches at D+1. Without single point of failure Regardless of the node that falls, the system must keep on running properly (loss of performance due to the reduction of resources within the cluster is however inevitable, off-cloud). One should also avoid bottlenecks that would disrupt our architecture performance in case of over-activity. Horizontally scalable If the volume of data handled or treatments on these become too substantial for a cluster, the addition of one or more nodes in the cluster must split the overload and bring the system back to a stable state. Indeed, we work mainly on our 10,000 physical stores. The takeover of a competing group, opening stores abroad, or the resurgence of our drive / e-commerce business should not call into question the architecture. Available rather than consistent The CAP theorem forces us to adjust the slider between availability and consistency of our system. In our use case, it is better to have our indicators about turnover always available, even if they are slightly incorrect. Indeed, a difference of a few receipts is acceptable. Modeling data and its flow: Each receipt is composed of meta information such as the store number, the emission time, the loyalty customer card number (if any) and lines corresponding to purchased items (item, quantity, unit price). The example below illustrates a receipt in JSON: Each store pushes the receipts in the tail of a central messenger system to be treated. Such a system allows to decouple the emitting applications from the receiving ones , but also to take over the peaks of receipts (Saturday at 11am, for example) by smoothing the load for processing applications. Then, the receipt is stored in an immutable database. Along with this, it is directly processed by a calculation engine for the appropriate data to be extracted (turnover per category). The result of this treatment along the way updates the view, offering real-time performance of the group. Regularly, every night for example, a batch will be run to recalculate the turnover per day, category, store etc… to correct deviations and take into account the after-sale service returns. The following illustration provides an overview of the architecture: We will now see what technologies we can use to address the problem. Choice of the data warehouse: We can think of two ways to store our receipts within the database: JSON: The receipt is stored as such within the database and we index the relevant fields, such as date and category in our case. This can be done using a conventional distributed file system such as HDFS or a document oriented DB such as MongoDB or Couchbase. Columns format: The receipts having a fixed structure and already known , a column-oriented database such as Cassandra or HBase is a viable solution for our use case. Cassandra has remarkable performance. In writing , tuples are first added sequentially to a commitlog as well as in in-memory tables, thus insertions are in constant time, with a latency that can be of the order of a few milliseconds (this depends on the cluster topology and on the selected level of consistency). Furthermore, the column-oriented design and the physical grouping of data by partitioning key make Cassandra competitive as well in readings when loading large amounts of data in Spark for analytical batches. Cassandra will then be our first-choice solution for our database. Note that it is perfectly possible to choose any other technology for the data warehouse, or even to use several (eg Cassandra and HDFS). Data Modeling in Cassandra In the relational world, we would use several tables: one for the header of receipts containing meta-information, one for each line of the receipt containing the product, one that would be the repository of products, etc… In the NoSQL world, it is quite common to de-normalize  data : we prefer high performance in reading and writing times rather than reducing the cost of storage. Simplistically, one could model the receipts as follows: Migrating from a relational DB to Cassandra needs to rethink its data schema because Cassandra’s philosophy is thought as a distributed system, and therefore it is totally different from conventional DBMS. One has indeed to think of modelling its tables based on predicted queries rather than data structure. In our example, the chosen compound key will only allow us to do a limited type of queries: we have used as partitioning key “day, store_id” and as clustering key “id_receipt”. Thus, possible filters on our queries must imperatively contain a day and a “store_id”. For example, the query: Give me all receipts purchased on “2015-04-12” in the store 345 will be an acceptable query, but the following will not: Give me all the receipts purchased in the “tidbit” category. Data processing In our use case, we have several needs: compute the turnover of the current day in real time and compute the turnover of the day before in order to allow comparison. The first operation requires real-time computation, whether the other one requires a D+1 batch. D+1 Batch Apache Spark is a distributed computing framework that generalizes MapReduce , it allows you to chain jobs while keeping intermediate datasets in memory, increasing performance over traditional MapReduce implementations. Featuring a Cassandra connector and a rich ecosystem including machine learning or real-time processing of large data volumes, this solution is ideal. Real Time Two tools stand: Apache Spark Streaming and Apache Storm. Storm is a CEP type of framework (Complex Event Processing) used to process events (logs, visited pages, alerts …), one event at a time. Spark Streaming is based on the Spark calculation core, it performs its jobs by micro-batches . In our use case, the one in which the calculation itself has to be done, rather than event management, Spark Streaming is naturally more appropriate: we receive structured digital data (receipt lines) that we will gather and process in parallel. We will also benefit from the homogeneity of Spark ecosystem: Spark batches code can be reused to write Spark Streaming jobs without even needing to change the core. This is the power of Spark ecosystem: we manipulate RDD (or Data Frames, they are the data sets in Spark) in both cases, we could then apply the same operations, with the same code . Finally, Spark also has SparkML API for machine learning, which extends the range of possible use cases. Input flow It is now missing one more stone to the edifice. We have seen how to store and process receipts, but it lacks a messaging system to allow stores to supply our Spark / Cassandra with high throughput. Apache Kafka is a MOM (Message Oriented Middleware) distributed, scalable, fault tolerant and without single point of failure. It can be both used as a publish / subscribe messaging system and as a queue . Originally developed by LinkedIn, Kafka has experienced a rise in popularity partly because it is open-sourced and it offers impressive  performance compared to its competitors. In our case, it will not only decouple client applications (stores) from processing ones, but also smooth the load so that Spark Streaming and Cassandra can work at optimal speed. Complex Queries in Cassandra? As we know in Cassandra server side, joins do not exist, like group by or even aggregate functions like sum , max or min . In fact, we can only retrieve raw data, without pre-processing. We have the ability to filter the data ( where clause), but without forgetting the constraints due to the composite key concept. Implement secondary indexes in Cassandra is not recommended because it is often misunderstood by users accustomed to conventional DBMS, so we must understand the concept and handle them with care. All this brings us to a question: given the constraints that go with Cassandra, is it possible for us to make customized requests, not predicted when we did the table design? Give me the value of turnover by category for the store No. 420 August 12, 2015. This is possible, for example using Spark. An example of code to respond to such a request: // We set the structure of the tuples that we will manipulate in our DataFrame\r\ncase class Produits(category : String, turnover : BigDecimal)\r\n\r\n// We get the tuples from Cassandra table\r\nsc.cassandraTable(\"retail\",\"raw_receipts\")\r\n   .select(\"products\")\r\n   // We filter at Cassandra level whenever it is possible, in order to give only meaningful information to Spark\r\n   .where(\"day=? and id_store=?\", \"20150812\", \"420\")\r\n   // splitProductLust bursts products lists from each of our rows into multiple Products class objects\r\n   .flatMap(splitProductLust)\r\n   // We map our tuples using the category as a key\r\n   .map(product => (product.category, product.turnover))\r\n   // The reduction step will calculate the sum of the values (turnover) by key (category)\r\n   .reduceByKey(_ + _)\r\n   // We display the result for each category\r\n   .foreach(println) Running this b Running this batch will take at least several seconds (or minutes, or hours, depending on the volume). In all cases, such a batch does not respond to queries with time of the order of one second. We must then provide in advance the answers of expected queries through running regular batches which will build the views or continually feed views through Spark Streaming. We can even combine these two techniques: it is the foundation of the lambda architecture . Kafka-Spark-Cassandra platform So here are our architecture and associated technologies: In the following paragraphs, we will explain the operation of the interfaces between Kafka and Spark and between Spark and Cassandra. From Kafka to Spark Messages are sent by client applications (our banks in this case, or shops) to topics , and then consuming applications can subscribe to those topics. Topics (in our case, we will use a topic “receipts”) contain a number of user-defined partitions. For a given topic, each partition is assigned to a Kafka broker (messenger node). Increasing the number of topic partitions can increase the level of parallelism as well as the topic messages consumption. Each message is a pair (key, value), with the key that will define which partition the topic will handle the message and value. Our message (here the receipt). Spark Streaming will subscribe to a ‘receipts’ topic and get each receipt as a stream: // The stream is splitted into RDDs, each part representing a 2 seconds interval\r\nval ssc = new StreamingContext(sparkConf, Seconds(2))\r\n// We subscribe to the “receipts” topic\r\nval topicsSet = Set(\"receipts\")\r\n// Kafka brokers list\r\nval kafkaParams = Map(\"metadata.broker.list\" -> \"kafka1:9092,kafka2:9092\")\r\n// We create the Spark stream from Kafka stream\r\nval lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)\r\n// We are now able to deploy the turnover calcultation The stream is divided into RDDs, which is the data format used by Spark. RDDs are also divided into partitions, which are composed of tuples. In our case, a tuple can represent a receipt. By default, an RDD partition is processed by CPU core by “worker” Spark (node): for example, 24 partitions and not more could be processed by 3 Spark worker nodes with 8 cores each. The Kafka-Spark connector offers a high-level interface that will allow to directly map each RDD partition to each Kafka topic partition. The first operation of our micro-batch Spark is to retrieve messages from the Kafka topic. By default in Spark, a CPU core is allocated for each task. However, retrieve messages from a partition of a Kafka topic is considered as a task. Thus, it may be wise to partition its Kafka topics as follows: count(partitions_topic_kafka) = 2 * (coeurs_CPU_cluster_spark) In this way, it effectively takes advantage of our CPU resources of our Spark cluster. Indeed, if count(partitions_topic_kafka) < count(coeurs_CPU_cluster_spark), the message recovery phase within Kafka will result in a RDD containing less partitions than the number of CPU cores available. To fully take advantage of CPU resources in the following operations, it will be useful to reorganize our RDD (upwards), which leads to shuffle (network traffic, to avoid because of performance cost). So as to reduce the impacts caused by unbalanced Kafka topics and to avoid OutOfMemoryError, it is preferred to use 2 or 3 as the multiplication factor in the above equation, as suggested by documentation . One can also take advantage of partitions mapping between Kafka and Spark from adapting our key upon issuance of our Kafka messages. For an operation such as reduceByKey(), having its RDD already partitioned with the correct key avoids shuffle and thus improve performance of its jobs. Once our data, ordered by store and category, are available via our Stream, we can then persist them into Cassandra. From Spark to Cassandra In parallel to “live” treatment of our receipts, we store them in our database in raw format using spark-cassandra-connector. This project developed by DataStax allows to easily interface Spark and in an effective manner with Cassandra. // We save our calculation results into Cassandra\r\n   turnover.saveToCassandra(\"retail\", \"raw_receipts\") Using the default settings, this operation will persist batch receipts by taking care to group together tuples with the same Cassandra partition key. More generally, the connector leverages partitioning concept in Spark as well as in Cassandra during reads and writes because it allows to map a Spark tuple group to Cassandra partition (similar to Kafka – Spark connector). Conclusion We have just given ideas to develop an architecture answering a use case based on distributed, fault-tolerant and scalable open-source technologies. This example shows potential of the Kafka-Spark-Cassandra platform for the implementation of real-time architectures. Thanks to the features offered by Spark, we have a nice set of tools to process data. Increasingly used, this trio encounters positive feedback. However, it is required to deeply understand how each of these technologies works to correctly model its data, avoid mistakes and take full advantage of its cluster resources. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Big Data . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Gather shopping receipts: architecture overview” siddharth 26/06/2016 à 13:02 Quite informative\r\n.. I was actually looking for mechanism to process real time JSON over Rest data. The data is supposed to show some visualizations as well that could do today vs last day (batches for history) calculations.. Any quick thoughts would be quite good.. thanks!! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-11-24"},
{"website": "Octo", "title": "\n                The Reactive Revolution            ", "author": ["Erwan Alliaume", "Philippe Prados", "François-Xavier Bonnet", "Fabien Arcellier", "Emmanuel Fortin"], "link": "https://blog.octo.com/en/the-reactive-revolution/", "abstract": "The Reactive Revolution Publication date 07/12/2015 by Erwan Alliaume , Philippe Prados , François-Xavier Bonnet , Fabien Arcellier , Emmanuel Fortin Tweet Share 0 +1 LinkedIn 0 It is the morning, at dawn, before the fortifications. Men are ready. For some time now, things have been moving with small changes, from here to there. The foundations are cracking, challenging them. Moreover, some have already made the leap. Others hesitate. The question is no longer about whether one is doing it, or if one is resisting, but rather when will one go. All these developments converge to the same goal: a new revolution in information technology. In this series of articles, we will elaborate on an increasingly spreading development model. Where does this model come from, why, what are the impacts on testing, on development languages, performance, etc.? We will try to answer all these questions. What is the reactive model? The reactive manifesto defines a reactive application around four interrelated pillars: event-driven, responsive, scalability et resilience . A responsive application is event-driven, able to provide an optimal user experience, makes better use of the computing power and better tolerates errors and failures. The strongest concept being the event-driven orientation, everything else is defined through this concept. A reactive model is a development model led by events. Several names are used to describe it. It’s a matter of perspective. We can call this model: « event driven », driven by events « reactive », that reacts to events « push based application », the data is pushed as it becomes available Or still better, « Hollywood », summarized by the famous « don’t call us, we’ll call you » On our side, we prefer the term « reactive ». This architectural model is very relevant for applications interacting in real time with users, including: Shared documents (Google docs, Office 360) Social networks (broadcast stream, Like/+1) Financial analysis (market flows, auctions) Pooled information (road traffic or public transport, pollution, parking spaces, etc.) Multiplayer games Multi-channel approaches (the same user uses his PC, mobile and tablet) Mobile applications synchronisation (all at the same time on all three user devices) Open or private API (impossible to predict usage) Indicator management (GPS position sensors, connected objects) Massive user influx (sport events, sales, TV ad, Startup launches, opening a new mobile platform, etc.) Direct communications (chat, hang-out) And more generally to manage complex algorithms more effectively (booking tickets, graphs management, semantic web, etc). One of the key points for all these applications is the latency management. For an application to be responsive, the user must perceive the lowest possible latency. There are several possible architectures to address scalability and resilience but we will dedicate other articles for them. For now, let’s focus on decreasing the latency. Decreasing latency For many years now, the competing processes have been performed in different threads. A program is basically a sequence of instructions that run linearly in a thread. To perform all requested tasks, a server will generate several threads. But these threads will spend most of their time waiting for the result of a network call, a disk read or a database query. There are two types of threads: soft-threads and hard-threads. Soft-threads are simulations of competing processes dedicating portions of the CPU to each process alternately. Hard-threads are competing processes performed by different processor cores. Fortunately, the soft-threads allow machines to simultaneously run many more threads than they have cores. However, to optimize performance, Intel recommends to: Create a thread-pool sized according to the number of hyper-cores and threads reuse Avoid calls to the kernel Avoid sharing data between threads The reactive model aims to remove as many soft-threads as possible and use only hard-threads. This allows better use of the modern processors. For a long time now, network technologies embedded in routers have taken advantage of this development model, with excellent performance. The reactive approach aims to generalise this principle of development. To reduce the number of threads, you must not share the CPU on a time basis, but on an event basis. Each call involves the processing of a piece of code. It should never be blocked, so that the CPU is released as quickly as possible to process the next event. To enable this, we must intervene in all software layers: from operating systems to development languages passing through frameworks, hardware drivers and database. All these software layers are being migrated, allowing widespread consideration of this architectural model. But why should we work like this? For several reasons: Reduce latency! Improve performance by increasing parallelism Manage peak loads by eliminating the arbitrary limit on the number of simultaneous processes Better use of the increasing number of cores in CPUs Be able to manage the process flow (in addition to request/response) Reduce memory consumption These improvements allow an increase in the number of users per server. In the same proportions, it reduces the cost of Cloud. On a technical level, this results in: No cost for synchronizing competing processes (only if there is only one core) Less memory footprint to keep the states (not stacks) Better scheduling based on the applications’ real priorities The reactive mode makes it possible to alleviate the limit on the number of simultaneous users implied by an arbitrary fixed parameter on the thread pool. This approach is more likely to respond to load peaks. These potential gains are sometimes challenged in different studies . Evolution of the OS, threads implementation, processors and virtual machines (JVM-like) may affect the actual benchmarks. Finally, it’s a subtle combination of these elements which allows to compare one architecture to another. Therefore, it is necessary to generally provide several very different developments: one reactive, one classic. Not so easy! This explains why it is difficult to have reliable benchmarks. As part of our research that we conducted and presented to PerfUG , gains are noticeable for architectures processing a flow at high frequency. Similarly, our work on JEE for classic web-use confirms performance gains and scalability of the architecture. A data structure that avoids locks is certainly a very important lever on system performance. New functional data models are then good companions to reactive models. Among new software making the most buzz, many are using the internal reactive model. To name a few: Redis , Node.js , Storm , Play , Vertx , Axon or Scala . Similarly, Web giants publish their feedback experience on migrating to this model: Coursera , Gilt , Groupon , Klout , Linkedin , NetFlix , Paypals , Twitter , WallMart or Yahoo . Why now? « Software gets slower faster than hardware gets faster. » Niklaus Wirth  – 1995 The reactive model is not new. It is used in all user interface frameworks since the invention of the mouse. Each click or keyboard input generates an event. Even client side JavaScript uses this model. There is no thread in this language, yet it is possible to have multiple AJAX requests simultaneously. Everything works using call-backs and events. Current development architectures are the result of a succession of steps and evolutions. Some strong concepts have been introduced and used extensively before being replaced by new ideas. The environment is also changing. The way we respond to it has changed. Have we reached the limits of our systems? Is there still space to conquer? Performance gains to discover? In our systems, there is a huge untapped power reservoir. On doubling the number of users, we can just add a server. Our customers talked about handling about 20x more requests since the advent of mobiles. Is it reasonable to multiply the number of servers in proportion? Is that enough to make your systems function? It is not certain. It sounds better to review the architecture to finally harness the power available. There are much more available processor cycles. That is obvious. As programs spend most of their time waiting for disks, networks or databases, they don’t harness the potential of servers. A development model based on events is called “reactive”. Now, this becomes accessible to everyone. It is becoming better integrated into modern development languages. New development patterns are now offered. They integrate latency and performance management from the start of the projects. It is no longer a challenge to overcome when it is too late to change the application architecture. At OCTO, we believe that this model of development will dominate in the coming years. It is time to pay attention to it. Applications based on the request/response model (HTTP / SOAP / REST) ​​can tolerate a thread model. In contrast, the applications based on flows like JMS or WebSocket will have everything to gain from working off a model based on events and some soft threads. We will see in future articles where this evolution comes from and what are the impact on all software layers (database, mainframe, routing, failover, high availability, etc.). Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Reactive . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-12-07"},
{"website": "Octo", "title": "\n                Reduce your Android build duration            ", "author": ["Rémi Pradal"], "link": "https://blog.octo.com/en/reduce-android-build-duration/", "abstract": "Reduce your Android build duration Publication date 05/01/2016 by Rémi Pradal Tweet Share 0 +1 LinkedIn 0 Build duration is a metric that every Android developer should monitor carefully. Indeed (even if you are very confident in the code you produce), you will have to run your project many times every day. When you re-run your code, you need to be able to see the result of your modifications really quickly. Otherwise, two things may happen: something will distract you and you will loose your focus or you will go back to your code and forget to check the effects of your previous run. Of course this statement seems overplayed when you are working on a small project which will be able to be re-run in less than 30 seconds, but when it comes to huge applications this problematic is real. We can divide the re-run in two steps: the building phase and the deployment phase. As we can barely reduce the duration of the second step (apart from running your app on an emulator), we will focus in this article on the different levers we can work with to reduce the building phase duration. Diagnose your build time We can define two build durations: The “from scratch” build time. It is the build duration when you are running a project for the first time (or when you are performing a gradlew clean before running your project). The “incremental” build time. This duration refers to the minimal build time we have when we re-run the project after a really small source code modification, for instance commenting a single java line. Of course, in this article, the goal is to be able to have a smaller build duration run after run: we are trying to minimize the “incremental” build time. Thereafter, every mention to a build time will refer to the “incremental” build time notion defined above. Reducing the “from scratch” build time is of course interesting, meanwhile, it is a kind of build we do not perform frequently (i.e. once or twice per day when we clean the project/switch of branch). On the contrary, you will improve significantly your “developer experience”  if you reduce the “incremental” build time as it is a type of build an android developer will run dozens of times per day. To diagnose your build time you can use the very useful Gradle option --profile . This option will generate a report containing the duration of each subtasks of the task you are running. For instance if you run the command line gradlew assembleDebug --profile , Gradle will generate a report file in the build/reports/profile-[date-of-your-build] . The following screenshot shows an example of a generated report file on a “big” project. We can see here the main steps composing an application building: Configuration A usually very quick (a few seconds) step. It depends on how complex your Gradle script is. Dependency resolution This step is almost always instantaneous as the dependencies have already been cached on your computer, even if you have performed a “clean” before. It can take a while if it is the first time ever you build the project or if you add/bump a dependency in your build.gradle . In that case, the duration will only depend on how many libraries you have to pull and how fast your Internet connection is. Task execution This is usually the longest step, which is subdivided in all the different tasks necessary to achieve the main task you want. It includes generally a compiling task, a dexing task and a lot more depending on the task you run and the context. We can notice that some subtasks are way lengthier than others: in the screenshot above the subtasks related to multidexing take a large amount of time. In almost all the cases, the task execution step represents a huge percentage of the total duration. For instance, in the build task from which the screenshot above is extracted, the execution task step was 42 seconds long over a total time of 50 seconds. The consequence of this observation is that in this case it is in the “task execution” step that we can obtain some build duration cuts. Set up your IDE and your Gradle configuration There are some small tricks that you should know that could reduce your Gradle build time in some particular cases. StackOverflow has plenty of scattered tricks. I will try to do here a compilation of what you may read, why the trick can be useful and the situation where you do not have to spend some time trying it because it will not change anything. Gradle daemon. Add the line org.gradle.daemon=true in your gradle.properties file. This is a tip that you can find really often. The result of this configuration is that Gradle is started before you run an actual Gradle command. It may lead to a few seconds reduction of the build duration. Meanwhile, this solution is useful only if you use the command line for every builds you run. Indeed Android Studio uses natively the Gradle daemon for a long time now [6]. Therefore, this tip will be useless most of the time. Parallel building. Add the line org.gradle.parallel=true in your gradle.properties file.This Gradle parameter allows parallel module building. It is only interesting if you have numerous modules in your project. The more evenly the build time distribution across the different module is, the better the build time diminution will be. Note that this feature is still experimental: you may experience unexpected behaviours when this option is activated Configuration on demand. Add the line org.gradle.configureondemand=true in your gradle.properties file. This option will have an impact on the “Configuration” step we described in the first part of this article. When this option is activated, the configuration step of a particular module will be done only if this module has a role to play in the Gradle task you want to run. As for the parallel building, this option will be useful only if your project is split in multiple projects. Even in that case, unless your Gradle scripts are very complicated (or performing time consuming tasks such as a network call), you will not gain more than a few second on your build time. Offline work. In Android Studio, click on the “Android Studio” menu, then “Preferences”. Navigate in the preferences hierarchy: “Build, Execution, Deployment”-> “Build tools” -> “Gradle”. Finally check “Offline work”. The name of this option speaks for itself! It can have a positive impact if you are working with a not reliable Internet connection. Indeed, depending on your build.gradle configuration, the build can make some network calls (to check if a new library version is available). In offline mode Gradle will use cached versions of the libraries. Of course, if you add a new dependency (or upgrade a dependency to a not cached version), you will have to deactivate this option. Set your min target to 21 This trick is probably the most efficient in the context of a huge project needing multidexing and which has a minSdkVersion strictly lower than 21. One of the interesting evolutions in the build process brought by the android sdk version 21 (Lollipop) was the introduction of the Android Runtime (ART). ART and its predecessor Dalvik are the custom Java Virtual Machine (JVM) used by Android. They are compatible with each other : if your application has a minSdkVersion of 15 (for instance), then the dex files generated can be executed on both ART and Dalvik. Meanwhile, if your application has a min sdk of 21 then some optimizations specific to ART can be applied during the app building. One interesting optimization is that ART does not need a main dex file with all the classes invoked before the MultiDex.install() . So, the time-consuming step of identifying what are the classes to put in the main dex file can be skipped. The MultiDex’s Android developers documentation page [1] provides extensive explanation of the different optimization ART brought. Of course, it is not acceptable to put all our big projects to a min sdk of 21! So, we have to find a way to have different build configuration. A first one with our regular min sdk that we could use to generate the apk we deploy in production, and an other one, that would be used by developers when they want to be able to perform fast incremental builds. A natural solution would be to use a Gradle feature that every developer use (or should use): the buildTypes . Unfortunately it is not possible (yet) to specify a min sdk for a particular build type [2]. Therefore, we have to use another powerful feature proposed by the Gradle android plugin: the flavors. Specific min sdk targeting thanks to flavors Flavors allow developers to have different build configurations, ressources or even code, if they want to be able to generate apk with different characteristics. This can be used when you want to generate easily two application with the same source code but with a different branding. The syntax of a build.gradle file using flavors to have a fast incremental building configuration and a regular one is the following: The result of this is that your build variant number will be duplicated. By selecting the fastBuildDebug variant you will have a significant build time reduction. If you want to test how your app looks with a pre-Lollipop device it is still possible, you just have to select the regularDebug variant. Likewise, the variant you have to select for your production application is now regularProd . Dealing with your project already having different flavors This exact method is not adapted when your application has already different flavors. For the rest of this part, let’s imagine that your two already-present flavors are brandA and brandB . If we use the code described above we will not be able to have a fast build with the same characteristic as those specified by brandA . Luckily, there is a solution to this issue: using the multi-dimensional flavors [3]. Multi-dimensional flavors allow us to create different sets of flavors. During the build variant generation, Gradle will not perform substitution of two flavors with different dimension id, instead it will do a juxtaposition. The following Gradle script shows how to use multi-dimensional flavors: Assuming that you have two build types debug and prod, you will now have 2x2x2=8 different build variants expressing the different possible configuration combinations. Results and limitations of this method Changing the min sdk reduces tremendously the build duration. For a big project of around 150k lines of code, this method reduces the incremental build time from 2min30s to 1min20s. This contraction can be observed on every computers: I did the same analysis with another computer with approximately the same result 3min to 1min45. Meanwhile, this method has a few drawbacks: it forces us to increase the build configuration complexity. Indeed if you use some explicit task name in your Gradle scripts or in your continuous integration (CI), you will have to perform changes in each of these references. Moreover it doubles the build task number. So, if you use some build variant agnostic tasks in your CI, such as gradlew test , the duration of this job will approximately double. Indeed these commands run for every build variants… It can be solved easily by not calling build variant agnostic tasks, but you will have to modify your CI configuration, which can sometimes be painful to do. Upcoming features: Jack & Jill build system, Android Studio instant run There might have some features in the future that will impact the build duration. The most significant are probably the new android build chain Jack & Jill [4] and the Android Studio instant run feature brought by the version 2.0 [5]. Jack & Jill build system Jack & Jill is the new toolchain developed by Google. The goal is to replace the current toolchain which is composed of two complex steps “javac” and “dex” corresponding respectively to the conversion from .java files to .class, and to the conversion from .class files to .dex (the executable format that the android JVM, ART or Dalvick, will be able to read). The new Jack compiler is able to perform the compilation directly from .class files to .dex. Jill is a tool that converts existing .jar files (generated thanks to the regular tool chain) to files directly usable by the Jack compiler. The following diagram summarizes the inputs and outputs of this new build system. Jack supports incremental build facilitation. The consequence of this point is that the utilisation of this new toolchain is supposed to increase build speed. It is already possible to try this toolchain in a very simple way: you only have to add useJack = true inside your buildType or flavor block. After some tests on different projects, I was able to compile only a few times: in most projects, I got multiple errors during the Gradle synchronization especially for huge projects. If this new tool chain is quite promising, it remains too experimental to count on it to reduce our current projects build duration. Android Studio instant run The instant run is a feature available in Android Studio 2.0 (still in preview stage) [7], which allows pushing application modifications on an emulator almost instantaneously. Is this feature the definitive solution for our build duration problems? It could be, as it has, on the paper, no more drawbacks than the techniques explained above and is way more convenient to use. Meanwhile it seems that this feature is not perfectly stable yet : sometimes the modified code is marked as “pushed” whereas the modification is not really applied on the emulator. It is currently stable when it comes to hot resources or xml file swapping. It is really a great improvement, as we generally have to perform numerous incremental builds when we are tuning our xml files. Conclusion The build duration is a metric that should be monitored carefully. Indeed it is very easy to let your build duration grow as your app is getting bigger and bigger, without even being aware of it. There are many ways to reduce your build duration in order to be more efficient when you develop. Meanwhile no tip is universal: you have to identify the bottleneck in your build duration and apply the most appropriate method. We have seen that there are upcoming features that should bring great solutions to tackle this problematic. If the “stable” tips are not useful enough for you, you should keep a constant eye on the evolutions of these features to see if they are becoming stable enough to be used everyday. References [1] http://developer.android.com/tools/building/multidex.html#dev-build [2] https://code.google.com/p/android/issues/detail?id=80650 [3] http://tools.android.com/tech-docs/new-build-system/user-guide#TOC-Multi-flavor-variants [4] http://tools.android.com/tech-docs/jackandjill [5] http://android-developers.blogspot.fr/2015/11/android-studio-20-preview.html [6] https://plus.google.com/+AndroidDevelopers/posts/ECrb9VQW9XP [7] http://android-developers.blogspot.fr/2015/11/android-studio-20-preview.html Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged android , Gradle . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Reduce your Android build duration” Mary Green 11/01/2016 à 09:12 thank you for this great sharing! very nice information here.. Dale King 26/05/2016 à 23:53 Here is what I am doing to set minSDK to 21 for devs without using flavors.\r\n\r\nIn your build.gradle declare it like this:\r\n\r\n\t\tminSdkVersion myProjectMinSdkVersion\r\n\r\nChange myProject to a name specific to this project.\r\n\r\nIn gradle.properties file add a line like this to specify the default minSdk version:\r\n\r\n    myProjectMinSdkVersion=14\r\n\r\nTo override it for a dev machine edit gradle.properties in your global gradle directory (e.g. ~/.gradle/gradle.properties on OSX) and set it like this:\r\n\r\n    myProjectMinSdkVersion=21\r\n\r\nSince you are putting it in your global gradle configuration that is why you want an identifier for your project in the name to be able to configure it per project. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2016-01-05"},
{"website": "Octo", "title": "\n                Save the date USI 2016            ", "author": ["Joy Boswell"], "link": "https://blog.octo.com/en/save-the-date-usi-2016-2/", "abstract": "Save the date USI 2016 Publication date 12/11/2015 by Joy Boswell Tweet Share 0 +1 LinkedIn 0 See you on June 6 and 7, 2016 Brace yourselves, USI is back June 6 and 7, 2016 at the Carrousel du Louvre! A unique speaker line-up Over the past 8 years, the conference has grown to become a benchmark for major international conferences on digital transformation . 1600 attendees are expected this year. USI’s unique speaker line-up provides a fresh outlook on new technologies. Explore the trends, expand your network and experience inspiring talks in the very heart of Paris. USI speaks to all players involved in corporate digital transformation . Join-us Registration is now open! Take advantage of our special Early Bird rate and receive a 20% discount. Check out our latest blog post to get all the information you need to attend USI . Tweet Share 0 +1 LinkedIn 0 This entry was posted in News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-11-12"},
{"website": "Octo", "title": "\n                Strengths and weaknesses of a cloud-hosted Android CI Server            ", "author": ["Louis Davin"], "link": "https://blog.octo.com/en/strengths-weaknesses-cloud-hosted-android-ci-server/", "abstract": "Strengths and weaknesses of a cloud-hosted Android CI Server Publication date 02/11/2015 by Louis Davin Tweet Share 0 +1 LinkedIn 0 Using a CI Server is a programming practice that is well established and not opened to debate anymore. Sometimes, it’s even a topic on which the IT Department has regained control of, managing and rationalizing the servers. Yet, as is often the case, mobile is following its own way: the technologies used can be considered non-standard among the company, the ecosystem is updated way more frequently than in other computing areas, and the need of running on a specific OS for iOS can be the fatal blow that leaves mobile developers on their own. Therefore, they frequently end up installing a mac mini in the open-space, in order to run their builds on it. Whereas the problems related to using such installations are mostly shared between Android and iOS, the solutions differ: this article focuses on Android. Limitations of an on-premise CI Server When he sets its Jenkins Server up, the mobile developer will have to deal with the following issues: How to install Jenkins on the machine? How to make it boot alongside the OS? How to allow or forbid network access to the machine from the company’s intranet or the whole Internet? On which privileges should Jenkins run (root or not)? On which session are the needed binaries and the Android SDK installed? Are the read/write rights managed correctly? How to deal with the Jenkins and OS updates? And, more frequently, how to deal with the Android SDK updates? The mobile developer is not a devops! If he’s lucky enough, he will manage to deal with these issues and end up with a fair install. Occasionally, a weakness of this setup will arise and mess with the team, but overall, it’ll be livable. Regardless the configuration and quality of the installation, a serious limitation will be present on this in-house CI Server: the need to have an Android simulator, emulator or device connected to run automated tests. Why this need for an emulator? Android does not run on a standard JVM, but on Dalvik (or more recently, ART). Before a relatively recent update (and still incomplete to this day), tests written by the developers had to run on a Dalvik/ART VM, thus on an emulator or a device. ( Note 1 ) Using a device is hard to industrialize: it has to continuously stay on and plugged in. Data transfer through the USB cable is slow, but more important, the computer sometimes loses its connection with the device, requiring someone to physically disconnect and reconnect it to the machine. Unthinkable for a CI Server. The Android emulator is, therefore, the only viable solution. As it is quite slow to boot (it can take up to several minutes), developers often let an emulator run continuously on the server. If this allows a saving of a few minutes on each build, it also brings a few limitations: All the projects built on the CI Server must work on the emulator’s Android version. The server will only be able to run one job at a time. Otherwise, jobs may end up fighting for the emulator during the tests execution stage. The CI Server will only be able to run the tests against the specific emulator’s Android version. All the builds will be broken when the emulator will crash… For the last few years, we have made it a habit to move to the cloud all the vital services we use, but find painful to maintain. Does the cloud have anything to offer from an “Android CI Server” perspective? What the cloud offers Mobile specialized CI solutions can be found, alongside more standard platforms supporting Android. The following table was assembled by running the following project (available on github) on different platforms. This project contains a non-instrumented (JVM) test, and an instrumented espresso test, checking that a TextView contains the text “Hello world!”. On the following table, the “Git” row tells whether the platform can read from any git repository accessible using SSH/HTTPS, or if it’s tied to Github. “Job count” states the number of different jobs it is possible to define then launch separately. “Distinction CI / Deploy” indicates if it is possible to differentiate CI jobs from jobs that go as far as deploying the APK. “Emulator configuration” tells the different Android emulator versions available, if the configuration has to be done manually (with code) or through a web interface. “Deploy” lists the plugins available from a web interface to deploy the app. Manually indicates that deploying is possible, but needs a gradle task or a shell script. Note: Configuring the emulator on SnapCI must be done manually through code. I could not manage to start an emulator within a one-hour delay. The interface test could therefore not be run on this platform. The first striking point is the lack of maturity from the different players, even if they all advertise Android support as being one of their feature. GreenHouseCI and Travis might be sufficient on “basic” projects. The main limitation comes from the inability to differentiate a CI job from a delivery job. It is also impossible to follow any code quality metrics on those platforms. CloudBees, which simply offers a Jenkins instance “as a service”, is the only platform that seems mature enough today to host a professional project. To differentiate the players regarding the price, the example project ran before is not relevant anymore. To assemble the following table, I considered an existing project, of about 140k LOC, on which every build takes 15 minutes, and is run 10 times per working day. The plans I selected are the cheapest ones allowing at least 5 users, and the execution of a minimum of 2 builds in parallel. Note: Prices are in USD. Every player but CloudBees offers fixed-price plans, depending on the user number and the number of builds that can be run in parallel. On CloudBees, the monthly plan starts at $60. You then have to add $1,32 per build hour. The price thus varies with the load. With the same parameters as told previously, the total monthly price would drop to $104 with 10 minutes builds, and climb to $148 with 20 minutes builds. An Android CI Server on CloudBees It is not really a surprise to find that Jenkins, the reference solution regarding continuous integration, is still the most powerful option when hosted on the cloud. In its “Dev@Cloud” offer, CloudBees brings you a hosted Jenkins master instance. It will launch each of your builds on a dedicated virtual machine instantiated specifically for this occasion. As you are probably already using a Jenkins server today, migrating your jobs and plugins is easy. It’s important to ensure that every instance booted to run one of your build has an up-to-date Android SDK installed. The use of Jake Wharton’s “sdk-manager” gradle-plugin is thus mandatory. The plugin will check that the sdk, build tools, support repositories and google libraries are up to date before letting you compile your project. All the “OPS” issues that we enumerated in the first half of this article disappear on CloudBees. The master Jenkins never crashes (unless, of course, the whole platform experiences downtimes). Everything is siloed: each build leads to a slave instance being booted. This instance can be as fast as you want. Finally, you can choose, for each and every job, on which Android version you want the emulator to run to play your test suite. This last bullet-point is a strength and a weakness at the same time. Each project can now run its instrumented and interface tests on the lowest Android version you support on your app. But you can also choose to run those tests on a configuration matrix mixing for example the Android version, the device’s locale, and the screen size. Playing with the emulator versions leads to reaching one of the limitations of CloudBees: the difficulty to boot emulators running on the latests Android versions. Official Android emulators are known to be “heavy”, slow to boot and laggy. These flaws worsen as you increase the Android version. Additionally, it is impossible to benefit from hardware acceleration (whether GPU or HAXM) or from x86/64 versions on CloudBees. It is thus very difficult to reliably boot an emulator starting from version 4.4.4: the success rate is lower that 25%. ( Note 2 ) This limitation is frankly annoying, but is mitigated by various factors: It is recommended to run your automated tests on the lowest Android version your project supports, because it is “easy” to unintentionally call methods that appeared later on the SDK. The current market status forces us to support at least Jelly Bean (Android 16), on which a third of the Android devices are still running. On our projects, the Jelly Bean Android emulator boots reliably in less than 90 seconds. Solutions are emerging to allow executing instrumented tests “as-a-service” on emulators or real devices. We can for example mention Amazon’s “Device Farm”, or the upcoming Google’s “Cloud Test Lab”. Those tools will probably be mature enough when existing apps with large instrumented test harnesses will target KitKat or Lollipop as the lowest supported versions. Projects currently launched or launched recently should not run into the same problems. New application architectures (MVP or even MVVM), in conjunction with improvements on development tools, allow the developers to write mainly non-instrumented tests, leaving the use of emulators to the most complex scenarios and interface tests. Conclusion On my previous Android project (140k lines of code, 3 years of development), the migration from an on-premise Jenkins server to CloudBees halved the build time of the CI job, and split by 4 the build time for the delivery job. Despite a few troubles discussed before with emulators during the migration, it has clearly been beneficial to the project. The caveats around emulators should only slow down projects that are currently in production, supporting “very recent” Android versions (such as KitKat) as their minimum, and relying on large instrumented tests harnesses. I hope that the rise of better Android emulators or more mature solutions to access emulators and devices “as-a-service” should allow me to reconsider this limit soon. Notes : A non-official tool allowing to run tests on the local JVM, Robolectric, has been around for a few years. This tool has trouble following the developer-tools update rhythm imposed by google. As a result, your whole test-suite can be broken by a simple build tools update. It is thus a solution that raises debates in the community, and that I decided to ignore in this article. On this topic, the CloudBees support is powerless and recommends using custom slaves hosted on-premise, or on a cloud supporting hardware acceleration. It is however specifically to avoid having to maintain custom slaves or VMs that we started this talk about migrating to the cloud. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged android , Android Emulator , CI , cloud , Continuous integration . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Strengths and weaknesses of a cloud-hosted Android CI Server” Marie 02/10/2016 à 16:41 Hi! Great piece!\r\n I came across your blog via Twitter. Although the piece is a year old our GreenhouseCI team has worked hard night and day and a lot have improved. We have added many new features and awesome integrations ( eg. testing on real devices). Would you be interested in taking it a new spin and wrapping your ideas into a blog post? I promise GreenhouseCI is the CI tool out there for mobile app developers that one can't resist;)\r\nWith best, \r\nMarie Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-11-02"},
{"website": "Octo", "title": "\n                FinTech what are the next steps            ", "author": ["Sylvain Fagnent"], "link": "https://blog.octo.com/en/fintech-what-are-the-next-steps/", "abstract": "FinTech what are the next steps Publication date 04/11/2015 by Sylvain Fagnent Tweet Share 0 +1 LinkedIn 0 Introduction In 2014, my colleague Stephen Perin and I wrote “FinTech Is Cannibalizing Banks!” a White Paper that had a certain impact in France and even in Canada. We have been following the innovation in retail banking since 2010, and this WP was meant to create a sense of urgency. In March, 2015, we organized a small Finovate in Paris ( FinTech Day ) where 20 French FinTech came to demonstrate their solutions. It was the first event of its kind in France. Sure, we are running behind when compared to London but now the French FinTech ecosystem is really expanding fast, is self organizing ( Association Finance Participative ). The funds raised, the acquisition, and the scaling of some actors is now underway. Worldwide global Fintech investment jumped 201% between 2013 and 2014, breaking the $12B mark with more than 730 deals. Investment in FinTech, Jan 2010 – June 2015, in USD (src: IC Dowson and William Garrity associates Ltd) Insofar as FinTech domains expand faster and faster, many questions remain regarding this new eco system. Before the Fintech Montreal Event to which I was invited to participate, some of these remaining questions were sent to me. Here are the answers I gave. The questions of 2015 Is the opportunity real or are we looking at another technology bubble like 2001? Unlike what happened in 2001, the FinTech business model relies on the fact that FinTechs have identified customers’ pain points and, in response, provide them with outstanding user experience (customer centricity). Their transparent (no hidden fees) business models rely on data they analyze and collect. Data is definitely their new black gold. Their business models do not rely on clicks or page views. Besides, the technologies they use are rich and mature (mobile, cloud computing, Big Data, API, etc.). That was not the case back in 2001. Promises that were made in 2001 can be kept this time! Nonetheless, we think that at some point there will be a phase of consolidation: mergers and acquisitions. It has already started in 2014 & 2015 and it will accelerate. Significant and recent examples: LearnVest (PFM-like, that helps people reach financial security and save for their most important goals) was acquired by NorthWestern Mutual for $250million (March, 2015). Yodlee (PFM, financial data aggregator) was purchased by Envestnet for $600million. “ Envestnet buys Yodlee and its treasure trove of ‘permissioned’ data by selling its vision of the future of financial advice ” (August, 2015). PayPal bought several FinTechs in the online payment and mPayment sectors – Braintree (2013), Venmo (2013) and recently in the remittance sector: XOOM (July, 2015) BlacRock bought the robo advisor FutureAdvisor (July, 2015) BBVA (Spanish bank) bought Simple last year (2014) as well as several FinTechs like Madiva (Big Data company). At last, peer-to-peer small business lending platform Funding Circle which acquired Germany’s Zencap (footprint in Germany, Spain, and the Netherlands) (October, 2015). BlackRock acquired FutureAdvisor (Robo Advisor) What have been the conditions for their success? Is it just technology or are there other forces at play? FinTechs address a customer pain point in a specific business line for a specific client group; they are specialists. They focus on a single problematic. FinTechs are customer centric not product centric. They do reinvent (re-design I should say) the way we do banking, with or without banks. People understand this new approach because FinTechs care about their life goals and their everyday problems. They are design-driven companies. Lige goals (Src: www.laserspinewellness.com) As a response to this pain point, FinTechs provide outstanding user experience in a digital world. They are pure digital actors. After the 2008 crisis a society trend emerges and banks are rejected: we need banking not banks . Besides, the government pampered new entrants (France, UK). In October, 2014, France established the first legal framework for crowdfunding , allowing the new entrants to do business legally. Basically, the first wave of innovation came at the end of the 20th century with Internet. At that time, Internet was just used as an extension of previous business models. Roughly speaking, we were doing the same business through Internet. The 2nd wave is now. New business models are emerging, like the sharing economy, and tomorrow it will be the blockchain that will provide decentralized, smart systems/applications. Digital Wave (src: images.forwallpaper) Despite their phenomenal user growth and ubiquity in everyday life, many small tech companies are struggling to make money. Can this trend be reversed? Can the smaller players be successful? Success goes through ‘scaling’ and FinTech have many options to scale. We are only at the beginning. Here are the weak signals and illustrations of this trend. Scaling by international expansion In France PretD’union (P2P lending) will attack the Italian and Spanish markets which are more lucrative than the French ( they raised €34 million to do that ). Stripe has had a strong international expansion (21 countries so far). With Coinbase (bitcoin wallet) one can buy or sell bitcoin in 25 countries while their wallet is available in 190 countries. Fidor bank is opening in the UK (September, 2015). Kabbage started in the US and last year went across the Atlantic and settled in the UK. Recently, they also started a partnership through a pilot launched with ING in Spain . Moven rounded up $12 million in funding for international expansion (October, 2015). Peer-to-peer small business lending platform Funding Circle acquired Germany’s Zencap, giving it an immediate footprint in Germany, Spain, and the Netherlands (October, 2015). FundingCircle (UK) acquired German Zencap platform Scaling for FinTech can rely on other FinTech customer base or even bank customer base if both of them succeed in establishing partnerships A very good example is BBVA Compass which teams with OnDeck to bolster banks small business offerings (May, 2014). OnDeck also teams with other partners like Intuit (September, 2015). Another good illustration is LendingClub (since September, 2014) that allows Union Bank to sell loans through its platform. The two organizations will also work together to develop new credit products and services to be made available to customers of both companies. “OnDeck connects mainstreet to capital” (src : OnDeck) Ripple (private bitcoin platform) established several partnerships with Fidor , CBA (Common wealth Bank of Australia), and recently (October, 2015) with Santander that invested (through its venture subsidiary) $4 million in their solution. Fidelity uses Betterment (Robo Advisor) in a white label partnership for its own clients (October, 2014). Kabbage is also licensing its platform to third parties. Recently (October, 2015), ING has signed a partnership with Kabbage . Besides, Kabbage uses data from a multitude of sources that small businesses use each day as a way to qualify borrowers. Kabbage understands a small business’ performance based upon data sources, which also include Intuit QuickBooks, eBay, PayPal, Amazon, Authorize.net, relevant shipping data, and more. Kabbage partnered with Sage, Etsy, Xero, and Intuit (among others) to access data, do better scoring, and build an eco-system around SME businesses and hence increased the rate of its solution adoption. Kabbage Ecosystem and partners Scaling by Open Innovation and Open API (Application Programming Interface) APIs are a way for different IT systems to transact with each other. Through partnerships, in-house development teams can integrate the functionality of another offering as part of their own. FinTech CurrencyCloud (automated multi-currency international payment) had multiple partnerships with Fidor bank , MangoPay , Kantox , and TransferWise for international, multi-currency payments (B2B) – processing $10B in payments every year, across more than 40 currencies in 212 countries – before they offered an Open API on the internet that increased the use and adoption of their solution. Thanks to their Open API, other companies are able to develop new applications that need currency transfers (src. Currency Cloud ). CurrencyCloud API “Xignite, the leading provider of market data cloud APIs (application programming interface), launched the#FintechRevolution API Ecosystem, an industry-wide initiative that connects FinTech developers with financial APIs. The goal of the initiative is to inspire a new generation of financial applications by assembling the best APIs in each market category. The #FintechRevolution API Ecosystem is open to financial technology vendors, data providers, service providers and accelerators. Founding members are 21 leading vendors that provide workflow, analytics and data APIs, including Yodlee, Stocktwits, Tradier and NASDAQ and FinTech accelerators including Level39 and ValueStream” (September, 2015, src. Xignite ). By the way xignite surpasses 50-billion API calls in a single month . The #FintechRevolution API Ecosystem Scaling for FinTech can go through banks that can massively adopt their platforms In P2P lending, there is a very recent trend, perhaps because the sector has gained respectability: Scaling for FinTech can go through banks that can massively adopt their platform like Lending Club for instance. Bank and financial institutions took a major investment in Lending Club by funding their loans. In 2014, Lending Club moved from 80% of funding by individuals to 80% by financial institutions. Lending Club – Bank Partnerships: Leveraging Each Other’s Strengths: “Lending Club offers strategic partnerships to banks in order to bring affordable loans to consumers and small business bank customers. Through a partnership with us you can buy whole loans that fit your own credit policy and risk guidelines; and offer personal and business loans to your customers, benefiting from Lending Club’s low cost of operations and national scale. A partnership creates value by combining the complementary strengths of Lending Club and your bank.” Lending Club and Prosper loans by type (src: Quartz | qz.com; data: Orchard) Banks have large scale integrated distribution offering physical and digital channels. Will clients ever go digital only or is Multichannel required? Branches must be reinvented, not stay like they are today that’s for sure (in France). They should transform: not to generalize, but more to become something specialized where the banking activity is only an element in the value chain. Branches should focus on customer’s problems and on their life goals: buying or renting a house, buying or renting a car… From there, they can focus on the customer’s life goal and approach the problem globally not merely financially. Branches should have skilled human advisors! Advisors should be more like life coaches => new skills, larger vision of running the business. “Branches are not somewhere you go but something you do” (Brett King). Digital should be present as a link between the pure online interactions and the physical ones. Digital is something that lets you do anything (not only banking) by being connected anytime, anywhere. What we call “Digital” has a huge impact on society and on companies. But all channels must interact together – an omni-channel approach. Digital natives might act differently and adopt full online interactions, maybe using skype or video chat, because they are already used to it. It’s natural for them. You may dedicate (specialize) your channel to certain types of clients and adapt them to their habits/behaviors: digital natives, wealthy people under 45 use mobile while those over 50 use branches  this is not universal for all channels. You might use a branch for your first interaction and for KYC but as soon as you can switch to a digital channel, you do it. Digital Impact (src www.brookes.ac.uk). How traditional players like banks / insurers and wealth firms will react to this disintermediation? Can they move fast enough? Banks still have a large customer base, money, and knowledge of regulation. On the other hand, FinTechs’ DNA is made of inspiration focusing on specific client pain points (they are specialists), agility to innovate fast, and having employees with strong digital culture. The question is to know how banks will integrate these new innovations. Partnerships, investments, integration into subsidiaries, or buy-outs. They will probably end up doing all of these things and the early adopters, like BBVA, have already started. BBVA is adopting the disruptions – not fighting them – they are building and sharing a vision : what will your business be in 10 years. Re-invent your business through design thinking (design-driven approach) Reinvent the business and the way customers should interact with the bank by reinventing the customer journey. To boost this strategy BBVA and Capital One acquired Spring Studio (April, 2015) and Adaptive Path (October, 2014), respectively. Both are design companies whose purpose is to reinvent the way of doing banking. Design Thinking process (src http://www.irdg.ie/) Define a mission, build a vision and share it with the employees Defining a mission and a vision: bank’s top management should ask themself what kind of bank they want to be in 10 years or less. Concentrate on functionalities, those they wish to put forward, and prioritize them. This allows to define what initial actions should be and which clients they should target first. Innovate with small steps, with small autonomous team, take affordable risks, measure the success or the failure, and adapt the next step of innovations (the lean start-up approach). This will provide the bank with MVP (Minimum Viable Product), which they can then rapidly deploy on the ground in order to make any necessary adjustments to the implementation before enriching it. Move from a delivery culture to a learning culture. Fore sure, big rationalization projects won’t help banks in reducing the progression of disintermediation. With their business model, Fintechs offer some kind of cannibalization of the old banking business models. Inside the banks the issue is that there will be very strong resistances that will slow or even stop the transformation or in other words the “immune system of the banks will start to kill the innovation from the inside” . To succeed, it would be better to either create a new structure outside of the existing one or from the inside, like BBVA, you might have to change your structure to accelerate transformation and boost results (Src: timoelliot.com) Legacy IT infrastructure that comes from the 70s and the 80s is a pain for banks Legacy IT infrastructure will definitely slow them in their transformation. Massive investment should be done to have real-time banking infrastructure. Banks still have time and money, but they should start investing in their IT renovation because it can take upwards of 10 years to modernize or replace the old core banking system. Capital One – one of the most innovative banks is undergoing digital transformation through infrastructure investment (amongst others). CapOne CEO: “Digital Is the Centerpiece of Our Agenda” : “ We are not primarily motivated by cost saving with digital because I think that’s about fourth on the list of things that the power that digital provides. Right now, it is still a net negative and probably for an extended period of time measured purely by cost, digital will probably be a net negative.…Most of the leverage is really in infrastructure in terms of things like rationalized and simplified core infrastructure, increasingly we’re focusing on cloud computing and building the underlying capabilities such that product development will be faster and faster and more effective over time. In the end, he said, Capital One doesn’t do digital because it’s cool or to save money. It’s just that it’s the future, and Capital One wants to get there first. ” There are a significant number of Robo-advisors and wealth management sites hitting the market. Will they replace professional advisors or complement their practice? Soon, just having the best human advisor will not be a sustainable competitive advantage in the wealth management business. Src: Wallstreetdaily With no human advisors, be data-driven & automatically build yourself a 360° overview of your customer: by analyzing customer needs (capturing their life goals, risk aversions) and preferences through segmentation. To improve customer segmentation capabilities, you should aggregate external sources of data and create 360° customer models. Offer PFMs tools ( Personal Finance Management ) to aggregate account data transactions from several accounts and offer a consolidated vision of the customer’s financial situation. Use this aggregation to improve your customer segmentation. Automated software (robo advisor) are used for: Asset allocation and portfolio rebalancing with visualization Tax-loss harvesting algorithms Advanced visualization to enable aggregation and goal based reporting. Additionally, use machine-learning algorithms from your existing historical data of customer profiles, investment preferences, declared goals and asset allocation and improve your automated decision Robo-Advisor should work in tandem with human advisors: human advisors will manage tax systems and legal aspects which are quite complex and region/country dependent. Human advisors might also be able to explain the robo’s choices to their customers. Robo Advisor by Matt Groening Regulation & compliance: how is the regulatory regime being applied to new entrant? Is regulation a barrier to entry that will protect traditional lenders and managers’ market share? It’s a threat in the short and mid-term. FinTechs should not ignore regulation and compliance. Ripple & regulation: U.S. Treasury slaps virtual currency start-up Ripple with $700K fine: “Virtual currency exchangers must bring products to market that comply with our anti-money laundering laws” said FinCEN Director JC Shasky Calvery. The CBS (Core Banking System) infrastructure is a barrier for new entrants. “For GAFA, if any of them try to get into core banking (which is licensed) it would kill them because it’s too riddled with regulation compliance and government control. This part is not a very profitable part” Chris Skinner told us during our July interview at our USI event . And he added: “In the UK, regulators, technologists, academics, and financial institutions all sit alongside one another and work together to understand what’s happening and how to innovate in a way which is acceptable and can be implemented for all” . Besides, as global tendency, regulation might come to protect the customers and not the bank (in Europe). Chris Skinner What is the next big thing that will be a disrupter or a disintermediator to the old banking system ? All actors that will provide new services relying on the blockchain Blockchain by nature and by essence disintermediates any system that hitherto was based on a trusted third party, such as banks or Swift for instance. One of the first examples of blockchain use is in cryptocurrencies like Bitcoin. The distributed ledger records all Bitcoin transactions, shared by all participants, to the BitCoin blockchain which is accessible by anybody (public) and validated by complex and secured distributed processes which do not require a trusted (central) third party. Bitcoin allows remittance around the world at low cost (sometimes volatility problem is mitigated by the platform by offering a guaranteed rate and by using Bitcoins only for technical transfer). Platforms like Coinbase , Rebit (Philippines), ArtaBit (Indonesia) or BitPesa (Africa) provide or will soon provide services around remittance through Bitcoin. The next steps could be services like Chain which for instance teams up with Nasdaq to develop on the blockchain protocol. Chain provides software allowing Nasdaq to privately clear trades in real-time. Chain focuses on private blockchain software developments. The Chinese competitors In a recent post Chris Skinner exposed his vision “I recently found some exceptional insights into how China is leading the world of banking, as we saw the opening of digital banks WeBank and MYBank. … – Alibaba and Tencent – are setting the benchmark for innovation in China and, from a financial markets viewpoint, the world … ” They are definitely mature to operate in Europe or anywhere else and the range of financial services offered is large. They moved massively and fast. Now we know that Alibaba is about to investigate the cloud infrastructure market in Europe , with a next step that could be financial services ( Alibaba future of banking ? ). But how long will it takes to see Alibaba offering its first financial services in Europe. My opinion is: not so long ! WeBank Conclusion It’s hard to conclude because we are facing a world of innovation that accelerates and never stops. Digital is reinventing the world and as Marc Andreessen said “software is eating the word” . We are facing exponantial organization (see Salim Ismael presentation @Usi ) which are extremely difficult to understand. The only thing we know is this: in two years the banking environment will be quite different because of the digital transformation that will occur from inside the bank or from outside – it will happen, with or without them. Software will have an even bigger impact on society and banking. There Will Be No Exception. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Digitalization and tagged digital bank , digital transformation , FinTech . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse", "date": "2015-11-04"},
{"website": "Octo", "title": "\n                An APM for every project and environment            ", "author": ["Alain Faure"], "link": "https://blog.octo.com/en/an-apm-for-every-project-and-environment/", "abstract": "An APM for every project and environment Publication date 30/10/2015 by Alain Faure Tweet Share 0 +1 LinkedIn 0 Application Performance Management (APM) is a tool to monitor and analyse the performance of software applications. With APMs, end-user response time, response times of various servers and server activity (CPU, Memory) can be collected. It is also possible, mainly for Java and .Net, to detect methods that seem problematic as well as the most costly SQL queries or blocked threads. According to the Gartner, an APM covers the following functionalities: End User Experience Monitoring (EUM) Application topology discovery and visualisation User-defined transaction profiling Monitoring of components resources usage Analysis and visualisation of collected data One might say that VisualVM for Java or .NET CLR profiler cover at least some of these aspects. This is true but these tools require you to always keep an eye on the screen, literally! Moreover, it’s often impossible and dangerous to use these tools in a production environment. In contrast an APM will collect, store and aggregate this information into graphs that are easy to analyse at a later time. An APM can also send email alerts when certain thresholds are detected. This first article in the series provides an overview of APM functionalities and shows the benefits of these tools in production, their initial target. It also demonstrates the interest to expand their use to other environments. Next posts will focus on various products available on the market. APM on production An APM helps to establish a culture of metrics without having to modify applications or perform specific developments. An APM provides system and application measurements without the need to modify or recompile the applications they are monitoring. In addition to the collection and archiving of metrics, an APM can automatically analyse variations or abnormal behaviors and raise alerts. APMs also allow to view summarised data in order to analyse the causes of problems. HTTP requests changes over time An APM improves communication within the company. Beyond the purpose of accelerating data collection and use of metrics in the business, APM enables better problem solving. The first reason is the supply of reliable metrics instead of highly speculative guessing games that generate unnecessary tension. It allows teams to focus on finding solutions, not finger pointing. The second reason why APM improves problem solving is that it facilitates communication between production, development and operations teams. Through an APM instance, production metrics can be shared with the development and functional teams in order to achieve a comprehensive understanding of system operations from the business/user viewpoint to a fine-grained level like a SQL query or a function call. Why put an APM in the development or test environments? An APM improves performance problem-solving at a lower cost. Consider a case where a newly implemented feature is very slow: an APM will allow to directly see where the problem is: client, application server processing, database. Of course on test environments, it is possible to activate the appropriate level of logs or use a profiler; but the advantage of an APM is that the information will be directly available to the developer. For more complicated problems like memory leaks, an APM will graphically show summary information from the Garbage Collector (GC) no time is lost analysing the logs of the GC. In qualification or performance test environments, it will be possible to obtain statistical information on the response time and the resources used by the application. Well, we already know how to do this… but often it does not get done because it requires expertise and time: you need scripts to collect measurements, you must store, sort, filter, aggregate and visualise… and furthermore you must have the skills to put all this into practice. The main argument in favor of an APM is simple: summarised information is available without the need of specialised expertise and at a lower cost. Setup cost of APM The cost of setting up an APM includes: License cost Infrastructure set up and administration costs Product installation and configuration costs The license cost varies depending on vendors and features. For example, some vendors offer a free license model with data storage restricted to only one day. Other vendors provide special licenses for development environments. Most APM vendors offer SaaS solutions. This approach simplifies implementations and drastically limits the associated human and material costs: no server to provide, install and manage. All you have to do is create an account online, download a zip file and register an agent (CLR or Java agent) on your servers. That’s it. You can then log onto the APM site and see how your application behaves. From a security point of view, the application connects to the cloud and sends authenticated data in a secure manner: no incoming port to open within the firewall. The connection to the APM’s user interface is secured by HTTPS and your username/password. Finally for an investment of a couple of hours and a fraction of the cost of labour, an APM solution on a SaaS platform provides the initial results that were simply unthinkable before. In a production environment, the configuration of the APM tool may require some effort and cost to refine and validate the settings in order to minimise the impact. Some APM solutions allow to quantify their impact in terms of resources consumption. The leading products we tested are New Relic, Ruxit, Dripstat, AppDynamics and Dynatrace. They have the advantage of being easy to implement, cloud-based and have an affordable licensing system. Note that the APM industry is erupting and there are many other products available on the market. Let’s give it a try! New Relic, along with AppDynamics and Dynatrace, is a leader according to Gartner ( Magic Quadrant for Application Performance Monitoring, October 28, 2014 ) As an example, we will use New Relic to showcase the interesting features for test and development environments. Installation The installation is very simple. After registering on the site, you can download a zip file. Specifically, it contains the settings file named newrelic.yml already customised with your license key and the java agent: newrelic.jar. Extract the zip to a directory on your server and add Directive -javaagent:/<path to new directory Relic>/newrelic.jar in your application’s start settings. That’s it. After having used your application, you can directly view its behavior. Transactions and WEB pages In our case, our application publishes RESTs services. Without changing any settings, the product is able to analyse various requests sent to the application and provide an activity overview. With a simple click, it is possible to perform a drill down on a query and get the longest functions and the number of calls over a period of time. Modern APMs have a very intuitive interface for fast handling. Drill down processing an HTTP request Database The tool can capture database queries and identify the most load intensive ones. At a glance, the timing analysis shows whether the queries are performed regularly or only at certain times. Here, for example, ‘vat Insert’ is executed at the start of the application. List of SQL queries and their execution time Resources The tool lets you to visualise the evolution of memory usage within its different spaces. This helps diagnose problems like memory leaks or caches that never empty. Evolution of memory usage The time spent in Garbage Collection can help identify application’s slowdown problems due to excessive memory activity that are not necessarily memory leaks. Garbage Collector activity It is also possible to look at thread use information: Threads used by the JVM Various information The APM captures very useful information such as JVM start parameters, the various Jars used as well as their software version. JVM parameters and dependencies Conclusion In production environments, an APM is an essential component to monitor applications execution and to diagnose and understand the performance issues that may arise. Specialised developments will never be able to compete with features provided by any APM software. Note that there are also solutions to specific technologies, such as Javamelody for JEE, that integrate part of APM functionality in the application itself. On test or development environments, APM serves as the Swiss Army knife of performance. It allows teams to analyse an application execution without going through tedious settings, time-consuming logs analysis and other tools that require specialised expertise. Finally, An APM is a great communication tool for stakeholders to gather a complete view of an application’s execution or the outcome of performance tests execution. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations and tagged Performance . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-10-30"},
{"website": "Octo", "title": "\n                A look at Xamarin            ", "author": ["Dorian Lamandé", "Eric Favre", "Thibaut Cantet"], "link": "https://blog.octo.com/en/a-look-at-xamarin/", "abstract": "A look at Xamarin Publication date 27/10/2015 by Dorian Lamandé , Eric Favre , Thibaut Cantet Tweet Share 0 +1 LinkedIn 0 In this article, we’ll talk about Xamarin, a C# .NET tool enabling development of cross-platform mobile applications. We’ll focus on the missing part: the reuse of native libraries. What is Xamarin? Xamarin is not only a product but also a company. The product addresses a common issue, the unified cross-platform development. Xamarin allows to create native applications on iOS, Android and Windows Phone platforms. Its upside lies in the reuse of code, reducing the time to market. Xamarin also provides its own development environment, Xamarin Studio. With Xamarin, it’s important to grasp that applications are run natively. Every iOS or Android APIs are available from C# code. It’s true for push notifications, contacts integration, Bluetooth… Sure, but what’s going on in real life? Regarding code sharing, in particular? Today, there are two different ways to build a Xamarin project. The first one consists of building a solution with an Android project, an iOS project, and a common project (created as a Portable Class Library). This Portable Class Library (based on a curtailed subset of .Net features) will bear most of the reusable code. It will embed our “Models”, our service access layer, … Since 2014, Xamarin supports the MVVM pattern, famous among rich or Modern XAML-based applications developers. Actually, ViewModels and Models will be shared through this common library. For more inputs on the subject, Xamarin’s documentation is fairly thorough. The second way is to use the Shared Project solution In a Shared Project, we can develop reusable code while injecting target platform specificities. In a Shared Project, each file is copied into sub-projects (iOS, Android, Windows Phone) during compilation. Compilation directives will let us execute platform specific code. It’s also possible to share views. Xamarin.Form relies on this kind of project to build very simple applications with little visual specificities. This feature applies to business applications, prototypes or applications with a strong visual identity requiring a single rendering on whichever platform. Therefore, a Shared Project is less binding than a Portable Class Library (PCL), since the latter curtails access to the .NET Framework depending upon the PCL’s targeted platform. I wrote libraries on each platform, I don’t want to rewrite everything for Xamarin, is that an option? The answer is yes. Xamarin provided a feature to address this issue a year ago: Binding projects. Binding projects build a DLL (therefore usable with C#) that exposes iOS and Android native library APIs. This kind of project is interesting for framework publishers, since they can release their native framework on the Xamarin platform within short delays and low cost. The Android Binding Project Xamarin.Android provides two ways to expose a java library: Use Java Native Interface (JNI) to directly call methods. Create a Java Binding Library project which exposes the library’s APIs in C#. The latter will be developed in this article. The binding is implemented using 2 mechanisms: the Manage Callable Wrappers (MCW): the MCW is a JNI bridge which is used at each call to the java library embedded into Xamarin C# code The Android Callable Wrappers (ACW): conversely, ACW is a JNI bridge which is used each time Android runtime code (ART) calls “managed code” To bind a java library, follow the following steps: Create a Java Binding Library project Add the jar as an “EmbeddedJar” and all its dependencies as “EmbeddedReferenceJar” (here we used jar files as example, but aar files can be binded as well with little more effort) Resolve binding related issues that may occur due to automated generation (you may need to delete nodes that shouldn’t be exposed) This can be done by looking into 3 generated xml configuration files: EnumFields.xml: binds java int constants to C# enums EnumMethods.xml: binds java int return types and parameters to C# enums MetaData.xml: change the generated API (rename a namespace, hide irrelevant methods…) For instance, we may want to hide a few classes within the jar. This is done with one line in the MetaData.xml file as follows <remove-node path=\"/api/package[@name='com.octo.android']/class[@name='MyClass']\" /> Once the binding project configured and compiled, you only need to add it as a reference from your Android Xamarin project. There are still several things to do in the MetaData.xml file, including renaming the methods parameters. Indeed, on a java method defined as such: public static void log (string logLevel, string message) The output will be: public static void Log (string p0, string p1) Not very user friendly for the developer who’ll use the SDK. To rename the method parameters, add these lines: <attr path=\"/api/package[@name='com.octo.android']/class[@name='MyClass']/method[@name='log']/parameter[@name='p0' and @type='java.lang.String']\" name=\"name\">logLevel</attr>\r\n<attr path=\"/api/package[@name='com.octo.android']/class[@name='MyClass']/method[@name='log']/parameter[@name='p1' and @type='java.lang.String']\" name=\"name\">message</attr> For further information about java library (.jar) binding, you can refer to the Xamarin documentation: http://developer.xamarin.com/guides/android/advanced_topics/java_integration_overview/binding_a_java_library_(.jar)/ Now that you have a DLL embedding the jar, you need to add it to the Xamarin Android project. You can now access it as if it was a regular C# DLL! To properly use your new library, you should obviously take a thorough look at its documentation. You need to make sure that the required methods are accessible, the configuration files are added and the resource keys are properly set in these files. The iOS Binding Project This kind of project requires to be developed on a Mac OS system (or at least to have one usable as a build host, as explained further). First, you’ll need the latest version of XCode, then Xamarin for Mac OS. Reversing this order may cause issues during the project compilation. To bind a static iOS library, follow these steps: Create an iOS Binding Project Add the library (.a) to your project Declare the interfaces to be mapped in the ApiDefinition.cs file Once the iOS library is added, you may need to edit the *.linkWith.cs file to import the library dependencies. For instance, this line imports the lsqlite3.dylib required by my iOS library: [assembly: LinkWith (\"[myLib].a\", LinkTarget.ArmV7 | LinkTarget.ArmV7s | LinkTarget.Simulator, ForceLoad = true, Frameworks = \"CoreFoundation CoreGraphics CoreLocation Foundation QuartzCore UIKit\", LinkerFlags = \"-lsqlite3\", IsCxx = true)] Once the dependencies are properly imported, you need to define the interfaces that should be made available from the resulting DLL. To do that, browse the .h file and add a C# interface in ApiDefinition.cs for each Objective-C interface found. For example, from the following Objective-C interface: @interface MyClass : NSObject\r\n+ (void)init;\r\n- (NSString *)reverse:(NSString *)value;\r\n- (NSString *)concat:(NSString *)a and:(NSString *)b;\r\n+ (NSUUID *)currentId;\r\n@property bool allOk;\r\n@end The ApiDefinition.cs file mapping this interface will be: namespace myLib.iOS\r\n{\r\n[BaseType (typeof (NSObject))]\r\npublic interface MyClass \r\n {\r\n\t\r\n\t[Static, Export (\"init\")]\r\n\tbool Init();\r\n\r\n\t[Export (\"reverse:\")]\r\n\tvoid Reverse(string value);\r\n\r\n\t[Export(\"concat:and\")]\r\n\tstring Contact([NullAllowed]string a, [NullAllowed]string b);\r\n\r\n\t[Static, Export(\"currentId\")]\r\n\tstring CurrentId { get; set; }\r\n\r\n\t[Export(\"allOk\")]\r\n\tbool AllOk { get; set; }\r\n }\r\n} Here are a few tips to bear in mind when writing the binding: The interface to be mapped must be decorated by a BaseType attribute having the interface type as a parameter (NSObject in our example, but it depends on the interface). The interface methods, variables and properties must be decorated with the Export(method/property/variable name) property. Some types need to be converted to C# (refer to Xamarin documentation Type mappings section ) The nullable parameters must be preceded with [NullAllowed], otherwise the code generated by Xamarin will raise an exception on null parameters without calling the iOS library method! The methods without any parameter only need to be declared with their name (iOS wise) as an Export parameter: Export(“init”). Methods with only one parameter, must be followed by “:” after their name: Export (“reverse : “)]. The Objective-C syntax allows for a method name to be split by its parameters. For instance, the second method’s name above is “concat” and “and”. This method is mapped by including “:” after each part of the method name: Export(“concat:and:”) Static methods, properties and class variables must be decorated with the Static attribute. For further information about iOS library (.a) binding, you can refer to Xamarin documentation: http://developer.xamarin.com/guides/ios/advanced_topics/binding_objective-c/binding_objc_libs/ To use your now binded library, all that’s left to do is adding your binding project into your Xamarin iOS project to access all of the features mapped in the ApiDefinition.cs file. I am not too comfortable with Mac OS, can I keep using Windows? You can configure a Mac to delegate compilation from a Visual Studio on Windows. Indeed, Xamarin provides a Mac OS tool allowing for a Windows / Visual Studio developer to compile both binding project and Xamarin iOS project from his workstation. It’s called Xamarin.iOS Build Host. Once the service started on the Mac, the developer only needs to reference the Mac IP address from the Windows computer and to fill in the PIN code generated by Xamarin.iOS Build Host: Conclusion Thanks to Xamarin, the .NET developers used to Visual Studio C# can create iOS (iPhone, iPad, Watch) or Android (smartphone, tablet, wear) applications, and of course share code with the Microsoft ecosystem (Phone, Tablet). Therefore, you can choose Xamarin for a cross-platform development if your teams are already familiar with .NET. However, they won’t improvise themselves Android or iOS developers, and a considerable learning effort is still necessary to grasp each platform’s specificities. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged .NET , android , iOS , iphone , Microsoft , Mobilité , Windows , Xamarin . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-10-27"},
{"website": "Octo", "title": "\n                Chris Skinner interview: the future of banking            ", "author": ["Sylvain Fagnent", "Stephen Périn"], "link": "https://blog.octo.com/en/interview-chris-skinner-the-future-of-banking/", "abstract": "Chris Skinner interview: the future of banking Publication date 21/10/2015 by Sylvain Fagnent , Stephen Périn Tweet Share 0 +1 LinkedIn 0 During our USI Event last summer we (Stephen Perin & Sylvain Fagnent) had the chance to interveiw Chris Skinner on the future of banking. Chris also did a presentation at this event ( The future of money trade and finance ). This interview has been transcribed by Natalie Schmitz (Octo Technology). Chris Skinner S. Perin & S. Fagnent : Banks around the world. What is your perception of European banks and, for instance, digital banks? What do think about what Francisco Gonzalez, CEO of BBVA, said about “In 10 years, only 100 banks will have survived the digital wave.” And could you give us a small focus on the UK, even though your presentation already gave us some information. C. Skinner :  Sure. If you’re looking for innovation around the world the countries that tend to show it and demonstrate it the most are those that have the least legacy. China is doing amazing things with Tencent and Wechat, and Alibaba, and what we’re seeing now with MYbank (Alibaba) and Webank (Tencent). Africa, I mentioned, is doing amazing things with mobile and innovation around mobile financial inclusion using a completely different model that is nothing like the old banking system. And in Europe, the main innovation countries are Turkey and Poland – if you include Turkey in Europe – because most of their systems weren’t around in 1990 , so they were implemented after the internet. MYBank WeBank I was just in Istanbul and some Turkish banks were saying to me that they have a study tool of America. And in fact, very strange, the American banks are all talking about replacing core systems because their core systems are not new , you know. They’ve been refreshed but they weren’t meant to, you know, after the internet was developed. The problem for most banks – this is true in France, U.K., Germany, America – is that the systems were implemented before Mark Zuckerberg was born. Before the internet was envisaged, in some cases. I knew a bank in the day that was very pound of the fact that they were the first bank to digitize in their country. And they implemented an IBM mainframe in 1961. I did ask the question: Is it still running? It’s in a museum, thank goodness. But I think the system is still involved in a lot of… Um. A UK bank I know is still running a system that was pre-1971 because it’s in old shillings and pence which is pre- our currency as it is today. IBM Mainframe In Europe it is mainly better, because when the Euro came in our older core systems were updated for transaction management in most European countries. Y2K did a refresher on old systems. But it’s quite clear here in France, for example, that BNP Paribas had to launch Hello Bank (I thought it was a separate bank) because they couldn’t develop a system that could be appropriate for the front end of a mobile bank, although I’ve learned since that it’s actually still used in quite a lot of their old core systems. [In the back, yeah.] In the back end. C.Perin & S. Fagnent : As is Soon, too. Soon from AXA Banks. The core banking system is the same. C. Skinner : The problem was demonstrated by Simple. When BBVA bought Simple, Simple was working fine. And yet suddenly it couldn’t do real-time updates, mainly because the banking systems aren’t fit for real time. And this is the core issue , which is that if you don’t have a real-time banking system you’re not fit for purpose. You know, if things are operating in batch they’re not fit for the 21st century, 24/7 mobile internet. And most banks are irritating me mainly because they believe by putting front-end systems in place they give a great user experience and that that’s all they need to do for a digital project, whereas actually that’s just lipstick on a pig cause that’s the front end. The back end is where the problem is. [Exactly.] You need a digital banking system. C. Perin & S. Fagnent :  Last year with Stephen we started with Fintech cannibalizing banks. Our next step was to do an event where we changed perspectives and said: Now they have to collaborate. What is your point of view on cannibalization and collaboration between Fintech and the whole of banking? C. Skinner : There’s a lot of discussion around “Fintech will eat the banks’ lunch” and “10 years from now banks won’t be around, and branch will disappear everything will be on a Fintech platform on a digital device.” I thoroughly disagree. Totally disagree. Banks are actively engaging with intercommunity and collaboration partnerships, nurturing innovation, investing in incubators… Really creating the next generation of financial system and making sure they own a good part of that system, rather than being eaten by it. If you add on to that that right now most Fintech operations, take the Prospers, Lending Clubs, Stripes of this world, aren’t really doing that much business compared to the global banking system in its entirety which is in trillions of dollars, not billions. Then you sort of say, well some of these things will become a threat to the core operations of banks but the question is did your bank do anything about it before it became a threat? And what I mean by that is that peer to peer lending will take a lot of business away from the credit operations of the traditional banks. If the traditional bank doesn’t wake up and do something about that, then obviously it isn’t going to be capable of surviving. So some banks will disappear. Actually, going back to the comment from Francisco (Gonzalez) of BBVA about 100 banks being left in the world, I don’t agree with that either. And the reason I don’t agree with that is that it would be a system that’s definitely too big to fail. There’s over 200 countries; you can’t have 100 banks serving the whole planet, the systemic risk would be far too great. Now, you may have a 100 banks in every country, which actually might be more likely, with 3 or 4 big trusted brands that still retain their trust because they’ve evolved and invested, and made sure that they’re collaborating and partnering, and internalizing innovation. But, equally, there might be another 96 banks that serve boutique needs. Francsico Gonzalez (BBVA) So if I want a purely digital bank on a mobile device that’s actually usable just by my shoes and watch, because it’s the Internet of Things, then maybe there’ll be a boutique bank that serves that specific need 10 years from now. So it’s really about large banks – as they are today – making sure they evolve, internalize, understand, and invest in building their futures. Which might actually mean having separate banks, boutique banks, of their own. Just to illustrate something in the final bit there, BNP Paribas for example with Hello might end up with 12 Hellos 10 years from now. 12 different banks all run by BNP Paribas, just serving different boutique specialist needs. And of those 12 they may actually have an ecosystem of partnership and incubation that’s part of those. So they may have built the back-end system themselves and offer the front-end system as their own interface to that service. It’s really being far more adaptable, agile, and flexible than ever before and, in fact, that was the key thing I was saying in the presentation. In the old days when we controlled everything (the physical structure), we also controlled all of our developments internally which is why most banks made proud they have thousands of developers. When you have more developers than Microsoft has on Windows, that’s stupid. What banks need to do is just to say: We don’t need all this internalized stuff. Just go out and buy it. Partner with it, integrate it. Bring it in so that it’s the best experience for the customer and for the aim of the particular service the bank’s offering . S. Perin & S. Fagnent : An option of Open Innovation, in a broad sense. C. Skinner : Yes. S. Perin & S. Fagnent : Do you think that Google, or similar, could be a bank some day? C. Skinner : No. S. Perin & S. Fagnent :  No? Why not? C. Skinner : To be honest, Google, Facebook, Amazon, Apple, if any of them try to get into core banking it would kill them because it’s just too riddled with regulation, compliance, and government control. The government control of the financial system is wrapped into the financial system incestuously. I mean, the government even had sister and brother government banks. The reason why Google, Apple, Amazon won’t get into it is that that’s not where the profit is. The profit is in the ancillary services that go around the financial system. Profit’s in credit, loans, mortgages, payment transactions. It’s not in deposit accounts. Apple Pay’s already taking a good squeeze of the action by forcing American banks to reduce their transaction charges to be in their wallet. That’s where they’re making the money. When you have a powerful, dominating brand you can have the banks subservient to you. And that’s exactly what Apple, Google, Amazon are doing. They’re saying: If you don’t fit in with what we want, then we’ll go to your competition. S. Perin & S. Fagnent : Regarding regulation, the UK is set apart. But you said that, for instance, Google can’t be a bank because of regulation and so on. Would that still be true in the UK, which is move favorable to new entrants? C. Skinner : Well, there’s quite a lot of spin around London in terms of what we’re doing with Fintech and regulation. I see numbers that say that we have more people involved in the London radius than in Silicon Valley and Wall Street put together. If you believe those numbers, good. That’s good for marketing, branding UK. But the real thing that’s happening in London which is unique, which I don’t see happening anywhere else which is why I’m excited because it’s where I live, is that you have the regulators, the technologists, the academics, the financial system all sitting side by side together and working together to understand this stuff and to innovate in a way which is acceptable and can be implemented for all. London wants to be in the center of that because, as a global financial center historically, how will you be one in the future – you’ll only be one if you’re leading Fintech because that’s the next generation from actual service, so if you’re not in that space you have a problem. So when I see regulators saying they’re banning bitcoin cryptocurrencies, or when I hear say all the criminals are using bitcoin for payments, I don’t hear that when I talk to the regulators. What I hear is that there is some criminal activity around bitcoin cryptocurrencies but that it’s not massive yet, compared to the cash-based system of crime. They’re monitoring it very carefully and then working out how to institutionalize innovation in a way that regulators of the financial system can make operational. By way of example, I’m working with a shared ledger group which consists of UK Treasury, UK Bank of England, UK regulators, UK big banks, UK big corporations, consultants, academics, and people like me. On the basis that we’re all trying see how to build a next generation system using shared ledgers rather than leaving out in the wild for the criminal fraternity and others to use it. I don’t see that everywhere else, I mean, I see it in America in that the White House has been actively engaging Fintech recently to do the same thing. To understand what this is all about. [France is at the beginning.] I recently went to Ukraine and Thailand and countries where, as soon as you say shared ledger and innovation and Fintech, they say “We have banned it. This is bad.” You can’t ban something because you think it’s bad, because it’s out there. And if you leave it unregulated, it becomes even more disturbing. It needs to be understood and incorporated. Going back to the original question, that’s why London is actually leading this. For Australia I mentioned CoinJar, which is a bitcoin wallet along with other currencies, and the Australian bank basically wouldn’t allow them any development capability or support so they came to London. Because that’s where they could get support. Not just support in terms of regulatory understanding but, equally, support that’s in the ecosystem of all the professions that need to be involved. S. Perin & S. Fagnent : One key point about England is also the regulations for the consumer. They are facilitators, making it easier for consumers to move their banking data from bank to bank… And all these other regulations which are very unique in Europe, maybe even the world. C. Skinner : That touches another point that I raised with the Fintech guys, which is that if you look at Michał Panowicz at mBank who is now to lead their digital projects, one of the things he keeps saying is that, at mBank, they have 4.3 million customers. Now, if you look at Moven, Simple, Fidor, they haven’t even really broken over a 100,000 customers yet. The only bank – new bank, neo-bank – that I see succeeding in growing a good portfolio is Chebanca in Italy which has gotten a good 500,000 customers in 7 years, but that’s actually a proper bank with branches. It’s not a digital-only bank. And one of the key things about Chebanca is, where they have a branch, they say they have two and a half times more assets deposited than where they don’t because customers trust the bank more when they see a physical representation of the bank. CheBanca And so, at mBank, they have converted about 3 million of their 4.3 million customers across to their new digital platform but a lot of that is supported with a physical branch operation representation. Take Barclays, they’re converting a lot of customers across to digital platforms but they have Digital Eagles in their branches who train customers about how to use this stuff. And any people who don’t want to be trained to use it are people around the age of 30, male, arrogant, and they just don’t want to be shown how to use this stuff. Often people say “Oh, old people don’t use iPads.” Well of course they use iPads. But they might not use it for banking because they don’t trust it until someone explains that this is just the same as when you go on the internet. It’s exactly the same! The only difference is you’re accessing from somewhere that’s far more convenient. S. Perin & S. Fagnent : How do you improve current banks? Where to start? Traditional banks, like BNPP and so on, want to be digital so where to start? Their legacy? Build another bank with a new CBS (Core Banking System)? C. Skinner : I’ve written a couple of posts around where banks should start and I think the first thing is: Do you really buy into the change program that you think the bank has to take? And what I mean by that is … There’s an old joke that goes: At breakfast, the chicken is participating but the pig is committed. This is where you have to start. Are you just participating? Most banks I encounter are doing digital projects and they’ve got a head of digital. They’ve given that person a budget and a staffed team, and that’s completely wrong because that’s delegating the change and basically saying “I’m going to give this to someone else to create the bank of the future.” So the question then is, well… If you have a head of digital and a budget and a project, that’s participating. That’s the chicken approach. If you want to be committed, the pig approach is to say that actually all of the effective team have to buy into this and do it together. Every C-level person in the institution has to be part of a digital agenda of change and then that has to start with asking how we replace our physical assets and structures with digital assets and structures. As I said in the presentation, and as I keep saying all the time, is that you have to turn the model of the bank upside down so instead of delegating digital and keeping physical, you actually become digital and get rid of physical except where it’s needed on top of the digital platforms. And that’s an internalization thing that the executive team have to bite into. For example, you have to get rid of the product as part of a digital structure. You need customer structures, not product structures. Quite clearly that’s a massive change because you’re taking a lot of empires away from people currently at C-level in the institution. The CEO often doesn’t understand what digital really means to a bank, he just knows it has to be done. Which is why they bring in a head of digital – then the head of digital gets frustrated because none of the executive team is supporting their digital projects. The CEO has to be the digital leader, not the person who’s head of digital. Once you’ve gotten over that hurdle, then you can start implementation because implementation begins with core systems, core structures, and core processes being completely ripped apart into a digital form. You can’t rip those apart into digital form until you’ve gotten over the first hurdle which is the executive team buying into it lock, stock, and barrel. S. Perin & S. Fagnent : What about the social impact of digitization on banks and the financial industry’s business? C. Skinner : Well, there’s obviously a social impact as we go into a new generation of finance based on technology and the social impact is … maybe three-fold. The first is that traditional banking will start to become unprofitable in the physical form. So those people who want branches and want to use paper will be excluded from the new banking system, because it’s digital. And, simply, the people who want to use branches and paper are those who are less sophisticated. So what it actually means is that the less financially competent – the less financially well-off – will probably be the ones who’ll suffer in the digital divide, just because the people who are competent and capable tend to be higher-income, higher-earners and are happy to serve themselves. If the digital user customer is the competent, capable customer how do you serve those who are incompetent and incapable in a digital world? And actually there is a way, but the question is how do you get this part of society to move in this direction which is mobile financial inclusion where the under-banks, the un-banks, the less competent, can get very simple services through apps on devices and very soon through wearables with the Internet of Things. The only reason we have the physical structure there is because that’s what we have as a legacy of the last century. When it doesn’t make sense and we start getting rid of it all, you can’t have this outcry of “Oh! You’ve gotten rid of our branch!” when you don’t need a branch. Maybe what you have for those who are less capable and less competent is someone who comes and visits them and shows them how to use their mobile services; and then they have to use the mobile service from then-on or they have to visit the über-branch, which is not the taxi branch but the big branch out in the center of La Défense, or somewhere. The showcase branch . Think of it as our Genius Bar. If you have problems using our services, the Genius Bars are 20 across Paris instead of 20,000. S. Perin & S. Fagnent : Any profound thoughts about banking to share with us? C. Skinner : Well the only bit we missed, part of the social piece, is that traditionally banks have only focused on trade, commerce, profit, and money. What I keep saying in the value where dialogue, what we’re moving into, is that they need to be a part of understanding life, relationships, and the things we value. And our memories, for example. I don’t see many banks doing this, in fact the only one I regularly talk about is Fidor Bank, in Germany, and the reason I talk about them is that their spend on onboarding a customer is about 17€ total, including KYC, compared to 1500€ for an average, traditional bank. That’s because traditional banks push products through channels using traditional media whereas Fidor Bank developed conversations through social communities online, where they have become relevant in the conversation. Banks need to start to move into being far more engaged where the customer has relationships, which is on social networks not on traditional media. Then I think they’ll find they’ll succeed far faster in this new world than those that don’t. A great example from ICICI Bank (and I must get this updated): when I saw them present a couple of years ago they moved all their branch people into being social media customer service representatives. They dedicated the whole bank into Facebook; they launched full-Facebook banking, full-service social banking not just conversations. They conversed 24/7 through their Facebook bank services with their customers and within 15 months they had moved from a negative score, in terms of net promoter score speak, of about -47% to +63% because they were engaging where the customers want to engage, which is in the network not in the branch. ICICIBank S. Perin & S. Fagnent : About what you said this morning during the presentation that everybody’s connected (since everyone on Earth has a mobile) so there is a new way of banking through mobile. Will these countries show the “old” countries (Europe, the US) a new way of banking? C. Skinner : I can’t say that I’ve seen much around mainstream Europe – going back to the first question around where innovation is coming in. I haven’t seen much about a new bank of the future coming from France or Germany, or Brittan. I’ve seen it more from Turkey… Spain actually is interesting because they’ve had quite a lot of activity. Santander, BBVA are all doing as far as … But also Banco Sabadell, Caja Navarra, La Caixa. If you look at Spain then probably more of the new things are coming from there, about where you see the next generation of European bank. Again, one of the things I did say this morning was that 7 billion people will now be included in finance in some form through the network and the question is what sort of bank form do you want to be in the future. CajaNavarra Tweet Share 0 +1 LinkedIn 0 This entry was posted in Digitalization and tagged Banque digitale , chris skinner , FinTech . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Chris Skinner interview: the future of banking” Guy Mounier 06/11/2015 à 23:08 Chris Skinner has identified a key dynamic many commentators miss, namely that the global banks are already expanding their innovation efforts.  They are reaching out to technology innovation companies like us in Cognitive Computing and Machine Learning, and other technologies, to not just get ahead of the crowd but in some cases lead it.  We definitely see it in London, Silicon Valley and New York City, but almost as important is that we see most of the global banks creating innovation centers in multiple geography, including Asia.  Hong Kong and Singapore sport some pretty supportive regulators and an innovation infrastructure that is factory level in its production.  With a lot already accomplished on the consumer side of their businesses, that innovation is now being focused on the B2B side of the banks.  What we see now are global banks spreading that innovation across the globe and leveraging local strengths as entry points for new technologies to maintain a competitive edge in corporate banking.  If they maintain their innovation focus at the C-level, and continue a global innovation strategy, they capitalize on their biggest differentiating asset --- customer data, derived from years if not decades of relationships.  In the B2B banking world that is a hard barrier to entry for fledgling B2B banking challengers to overcome.  Thinking globally in innovation is essential. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-10-21"},
{"website": "Octo", "title": "\n                The evolution of bottlenecks in the Big Data ecosystem            ", "author": ["Benjamin Joyen-Conseil", "Oliver Baillot"], "link": "https://blog.octo.com/en/the-evolution-of-bottlenecks-in-the-big-data-ecosystem/", "abstract": "The evolution of bottlenecks in the Big Data ecosystem Publication date 12/10/2015 by Benjamin Joyen-Conseil , Oliver Baillot Tweet Share 0 +1 LinkedIn 0 I propose in this paper a chronological review of the events and ideas that have contributed to the emergence of Big Data technologies of today and tomorrow. What we can see regarding bottlenecks is that they move according to the technical progress we make. Today is the JVM garbage collector, tomorrow will be a different problem. Here is my side of the story: The early days of the web The Web era has dramatically changed things in term of data. We have progressed from a rather rigid model of RDBMS that was designed for the IT management (inventory management, accounting) to IT content, extremely diversified and in large quantities: At the beginning, Google’s index referenced 26 million web pages (in 1998) . Assuming that a web page weighs 100 Ko at the time, we can estimate at 2.5 TB the data volume to index! Two years later, the volume index already exceeded 100 To … A quote by Nissan Hajaj, an engineer at Google Search, symbolizes this awareness: “We’ve known it for a long time : the web is big.” First observation: Scalability Google needs to combine two elements so its idea can work A high computing power A large power storage At the time, two ways existed for intensive computations: The super computer (eg. IBM with Big Blue) Computer cluster The first uses the principle of scale-up , in other words: just add RAM and CPU on the motherboard to increase the power. The second applies the principle of scale-out , add a machine in the cluster to increase its power. It is found under the term horizontal scalability . Second observation: the variety of information to be processed The content of sites that Google must index is highly variable. The content of sites that Google must index is highly variable. Indeed, HTML produces pages all very different (in size, content, etc …). It is not conceivable to store it in an RDBMS designed to handle specific types with a fixed size (INTEGER, VARCHAR (255), timestamp, etc …). To anticipate the evolution of the Web, it is necessary to not define any structure, any type, at the moment of the data storage in order to change the structure seamlessly. We therefore need a system that Google knows how to interpret when the data type is read, instead of defining it when it is written. A sort of brute force to perform Web indexing batches. Parallelization techniques One is said shared memory paradigm. This is the case in a single machine that owns multiple CPUs (or cores). CPUs share the same RAM memory in which they access. The other paradigm is said distributed memory. We have several machines, each one with its own memory. The idea is to transfer information at each step. A loop runs on each machine, and at each iteration, there is a synchronization mechanism to communicate information between machines (Message Passing). In the clustering case, the strategies used with Message Passing for the distribution and the data retrieval (scatter, gather, broadcast, reduction) use a master node with dataset and workers (running nodes). The data will be downloaded and processed over the cluster, then the result will be retrieved on the master. If the first solution to process our dataset is to communicate over the network, the network will form a bottleneck for a data-intensive workload. Doing the math quickly: (26 million pages * 100 KB in size) / 100 Kbits bandwidth per second / 60s / 60min / 24h = 2.5 days of data transfer. At the time, these kind of problems rarely occured. MPI (Message Passing Interface) was the library used for intensive computing, it was not designed for large data processing. But according to Google, it is a new use case that is not treated by any existing solution. Google’s treatments (compute the PageRank , building a reverse index) are long-term processing. Within a distributed environment, the probability of a failed treatment increases with the number of machines running parallel processing. This means that Google should think about a way to tolerate the error (known as fault tolerance ). From these successive observations, Google has been inspired by functional programming to create a new approach within a distributed environment: MapReduce Use case 1: Batch processing to index the Web The basic principle is to remove this bottleneck known as the network: Moving binaries, rather than the data. Instead of distributing the data at the reading stage, we will distribute the data at the writing stage over multiple machines. At the reading stage, there will be only the compiled code that will transit over the network instead of 10TB of HTML pages. Moreover, the MapReduce paradigm aims greatly to simplify the writing of parallel programs. Progress existed so MPI could co-locate data but it never really materialized because it was not very intuitive and added more complexity. The fault tolerance is easier to implement: when the data is written, the data is then replicated to different locations. In case of a machine failure containing the information, we will be able to perform our treatment on the replicated data. Hadoop Hadoop came in 2006. To build itself, Hadoop requires a distributed file system Nutch File System (currently HDFS ), and an implementation of Google MapReduce’s paper . The framework is written in Java , the simple reason is: “ Java is portable, easy to debug and widely acclaimed by the community ”, said Doug Cutting, its main creator. With MapReduce, we are using another data-model to represent our recordings: key/value. The perks of this model is its simplicity to be distributed. Each machine gets a key sequence to manage. It is easy to iterate in parallel over each key and perform a treatment on the value. The number of sequences defines the degree of parallelism (number of machines which process a parallel sequence). // map phase. Ce code est exécuté sur chaque worker\r\nwhile(recordIterator.hasNext) {\r\n    record = recordIterator.next;\r\n    map(record.key, record.value);\r\n} The user (developer MapReduce) only needs to specify the purpose of the map() méthod. The new pairs of key/value generated by the mappers are then distributed to the reducers through the network. This sending stage is called the “ Shuffle & Sort ”. We can notice that MapReduce is not really a complete solution regarding network issues. Indeed, the shuffle & sort stage mainly uses the network as an exchange medium output data of the map stage to the reducers. The good practice to adopt in the mappers is to filter records, plan (reduce the number of columns), pre-aggregate and compress to significantly reduce the data travelling over the network in the next phase. Unfortunately, these practices are applicable only in individual cases, it is difficult to find a comprehensive solution to this problem. Use case 2: iterative processing for analytics Around 2008, we begin to see a new use case : the need for a higher level of language to express a distributed processing. A language designed for the data, the SQL . Facebook develops Hive , a SQL engine generating MapReduce in 2009. Google introduced Dremel , a distributed datastore interpreting SQL and “optimizing” the full scan (fun), which will give birth to the project Apache Drill (worn by MapR). Impala , Presto , Apache Phoenix , Druid , Pinot , are all so many technologies that will address this need of large volume analysis. In short, the Analytics is the new use case. Meanwhile, we begin to see tests in the field of artificial intelligence. The Machine Learning is entering in the Hadoop ecosystem with Apache Mahout and opens new perspectives that stimulate the specialized press. These two new possibilities on Hadoop (analytics and machine learning) have a common characteristic: they consist of iterations . Thus, an SQL query is made of several joints that will be split into multiple MapReduce jobs, storing the intermediate result on HDFS. A clustering algorithm such as K-means proceeds by successive iterations until the centroids reach a steady state. They are given the name of iterative algorithms . At the end of each MapReduce iteration, we store the intermediate result on HDFS for the next iteration. Not only we stored on any hard drive, but the data will be replicated (3 times by default). Then, in the next iteration the intermediate data need to be reload in memory (during the map) to send them directly to the reducers (with shuffle!). It turns out that HDFS is a rather slow abstraction (30MB / s write, 60MB / s average read SATA) that just increase the overall processing time. As you would have guessed, I/O disks are the new bottleneck. This bottleneck already existed in reality, but it was masked by the I/O network, and it was not too painful for batch processing. To minimize the impact of an iteration, we will do what we usually do when we have a need to read / write the data quickly: cache in RAM. Random Access Memories : Fortunately, the price of memory has dropped a lot and it is common for some time to find servers with 128 or 256 Go of RAM. So we can imagine that a dataset of 1TB (replicated) could be hold in memory on a small cluster of about 12 servers… That is how, Spark occurred in 2010 ( the research paper ). The Framework is introduced by UC Berkley to solve precisely this type of iterative workload. They think about an abstract data model going through the iterations: the RDD ( Resilient Distributed Dataset paper ) An RDD is a collection of records, distributed across a cluster, recovered from a context initialized at the beginning of the job. //records représente un RDD JavaRDD<String> records = spark.textFile(\"hdfs://...\"); Transformations or actions are successively applied. These two concepts, specific to the Spark’s vocabulary can be generalized by what is called a transaction. It exists map side operators and reduce side operators (shuffle operator). The difference between one and the other is that the reduce side is an operator type that triggers the shuffle (redistribution phase by the network of key / value pairs) before the execution. Spark is not the only abstraction based on MapReduce, most of the frameworks, dataflow type (to express a sequence of transformations) use these same concepts: Apache Flink , Apache Crunch , Cascading , Apache Pig , etc … records.filter(...) // est une opération map-side\r\nrecords.groupByKey() // est une opération reduce-side With Spark, one action generates a run with a shuffle phase. There are different types of RDD, some offering caching functions particularly to clarify that a dataset will be used on several iterations (thus hiding the intermediate results). The dataset will be stored either in a local cache in RAM or on disk or shared between RAM and disk. records.cache(); But this is not the only factor of intermediary time acceleration. Spark recycles “executors” – the JVMs used to execute treatments – cleverly. We realized that some of the Hadoop latency was coming from the JVM’s startup. Recycling enabled to reduce the time processing of 20 seconds per iteration (in the Hadoop case, a container takes 30 seconds to initialize). The trick was already in Hadoop (the property mapred.job.reus.jvm.num.tasks in the Hadoop v1 configuration) but was not by default enabled. With improvements in terms of latency by recycling JVMs and through the use of caches, we begin to see more and more interactive systems. Spark offers an interactive console (bin/spark-shell) to type and run code (Scala and Python languages). Use case 3: Interactive queries It is a kind of iterative algorithm. SQL, as we have seen, has become an important point on Hadoop. The shortcut quickly made about Big Data technologies supporting SQL, will be to compare the latency with older systems which themselves used SQL (but processing small volumes): under an average of one minute to display the result of a query. Business teams, analysts, BI engineers have access to all that power so they can “interact” with the data and extract value. Hive – with initiatives such as Stinger and Stinger.next – are moving in this direction. It offers new runtimes environment, more suited to the “interactive” (Tez, Spark) to decrease processes on adhoc queries . The challenge of these new analysis engine ( Spark , Flink , Tez , Ignite ) is able to store very large amounts of data in memory. We therefore increases the size of the JAVA Heap containers (Xmx the options ans Xms JVM). It will take more RAM. From 2 to 4 GB on average for MapReduce, it will grow to 8 or 16GB for Spark, Tez and Flink. For I/O disk, they will be reduced by increasing the Heap simply because we had the ability to store more objects in memory. Interactive processing therefore lead to more substantial memory sizes to manage for the JVM, although many small objects are allocated (dataset records), frequently (with every new adhoc query). We find ourselves in a new bottleneck: the garbage collector GC limits The JVM‘s garbage collector is a very powerful mechanism, greatly facilitating the management JVM’s memory (Heap). It automates the deallocation of unused objects. To manage its objects, it is a graph wherein a node represents an object, and an arc connected to another node corresponds to a reference (an object that references another). If an object has no arc connected to itself, it means it is no longer referenced by any other object, so it can be deallocated when the next collection occurs (it is “marked”). Logically, time travel of the objects collection is proportional to the number of existing objects in the Heap, and therefore the size of the Heap (the bigger it is, the more you can put in). The garbage collector puts the whole system on hold in some cases (concurrence issue). Here are some examples causing pause : Resizing the Heap (it oscillates between Xms and Xmx ) Garbage collection (GC compaction) Promotion of objects in the generation Tenured Tagging live objects etc… If you are interested to dig deeper the GC process, Martin Thompson gave a very good explanation . In the diagram below, we see that breaks completely stop the system to complete their treatment, whether performed in parallel or not. It is also called STW pauses, standing for Stop The World. Spark collections are often composed of millions or billions of records, represented by instances. All these fast allocations and deallocations are fragmenting a lot the memory. The garbage collector is triggered at a frequency which varies depending on the use of the Heap. If it no longer offers enough memory to requests for allocation, the garbage collector will conduct a collection to free up space. If it is too fragmented, the JVM will perform a compaction following a collection. These passages are sometimes too long for the interactive (imagine 30 seconds of GC pause). That is why the main current work on performance Spark is about the garbage collector. Daoyuan Wang’s company Databricks (main contributor Spark) gives an overview of possible tweaks in a post titled “ Tuning Java Garbage Collection for Spark Applications ”. Philippe Prados raises the question of the future of the GC in a previous article Blog OCTO: The next death of the garbage collector? “Today, the configuration has evolved enough to put the garbage collector on. Incremental developments, which are real, respond less and less to requests. GC are evolving, but not fast enough in relation to their environment.” This is why engineers will start looking for solutions elsewhere… How to fool the GC? In distributed data systems processing (using Java technology), we observe two variants to try to reduce the impact of garbage collection in the JVM. Hide objects during garbage collector Use the native memory rather than the Heap Both variants use the same JDK class: the ByteBuffer A ByteBuffer is a type of object that encapsulates an array of bytes with access methods. The binary information is serialized in advance before storing it into the ByteBuffer. It is a kind of low-level container, in which we will store our collections of objects. According to the documentation, there are two types of ByteBuffer: Non-Direct ByteBuffer is a buffer allocated in the Heap up to a maximum size of 2GB. The Non-Direct ByteBuffer is used to conceal the object references in the GC. All our objects are serialized (e.g instances of records in Spark) and then inserted side by side in some Non-Direct ByteBuffer having the role of page, memory segment. Thus, there are only ByteBuffer which are found in the reference graph that travels the GC (it means fewer items). It allocates ByteBuffer when the application is starting, it is arranged that they occupy a fixed portion of the Heap (often 70%). They are long-lived objects, so the JVM does not have many allocations during processing (memory remains stable). And as the memory capacity are reserved to start, there is no resizing the Heap. The diagram refers to an area of ​​the Heap called “Managed Memory” divided into 32K MemorySegment. These Memory Segment are ByteBuffer. Direct ByteBuffer a s its name suggests, accesses the native memory directly. The native memory? You get it, this ByteBuffer is not allocated in Heap but in the memory of the operating system (OS), the mechanism underlying is no more than calls to the Unsafe API. Therefore, the Direct ByteBuffer is not managed by the garbage collector and objects serialized either inside (this process is also called “off-heap”). The Javadoc recommends the use of it for large buffers and long term. The ByteBuffer should be managed by the program that calls it, which brings us to the fact that if an error occurs then the program will crash, throwing an exception BufferUnderflowException . An error can quickly arrive because the ByteBuffer has no method size() so you just have to know the size of its memory objects (varying depending on the hardware) to allocate new objects beside, without rewriting above, or without exceeding the buffer. His second asset is not to copy the buffer content to an intermediate buffer when accessed. The GC is not put aside. It always manages de-serialized objects and temporary structures that the user allocates in its program (the developer who uses the Spark API). Flink Management Memory Flink first implemented the first solution, e.g the use of Non-Direct ByteBuffer allocated in the Heap. The ByteBuffer are called MemorySegment, they are fixed to 32 KB in size and are managed by the MemoryManager. The purpose of this one is to distribute good operators segments (filter, join, sort, groupBy, etc …). Flink plans to migrate to the second method: the use of off-heap memory to further accelerate access (no copy in a temporary Buffer) and almost completely bypass the garbage collector (there is always the de-serialized object stored in the Heap). With these new tricks (the ByteBuffer), serialization is the mechanism that becomes the most important thing. That is why it becomes important to develop serializers for each object type. Flink includes serializers “homemade”, but the task is rather heavy (what about Java generics that converts any Object type?) And users classes are serialized from a serialization based on reflection ( Kryo ), though less powerful than the version “homemade”. SPARK-7075: Tungsten Project Spark plans to catch up on Flink by launching the Tungsten Project , and for the latest version (1.4) incorporates an interesting mechanism to generate “custom-serializer” code. Code generation facilitates the life of the developer while being much more specific than using libraries such as Kryo. They are only at the beginning but the project plans to incorporate a MemoryManager with memory pages in the manner of Flink in the version 1.5, and the appearance of efficient structures using the processor caches (L1, L2, L3). The shared cache, the next bottleneck? ByteBuffers are very useful but they have a big flaw is that they are costly in terms of serialization / de-serialization (even with custom-serializer). Frameworks such as Flink and Spark spend their time performing these tasks to access their records. To remove the load of systematic de-serialization of objects in the operators, we are starting to see data structures called “cache aware”. In other words, a structure able to efficiently use the shared cache of the processors. The model is: Storing a contiguous collection of key + pointer in a ByteBuffer. When an operator (join, groupBy, sort, etc …) performs the comparison between the records, it does it on the keys. It is therefore more interesting to separate the value from their value (often larger). Keys have a fixed size which makes the looking of collection effective without de-serialization (you remember the RDBMS at first, it’s the same thing). We will do a binary comparison between the keys (or the first bytes of string keys type) and access the value only when it is needed. This access is made via the pointer near the key. Once the bytes retrieved from the ByteBuffer, we de-serializes into an object. The following chart summarizes this explanation: Why these structures are “cache aware”? In fact, these structures are accessed very frequently, and they use little memory (because they have only got keys and pointers), so the OS will place them in the shared cache of the processors (L1, L2 , L3). This is the end By analyzing all this, I made the following reflection: Seeing all these efforts to bypass the garbage collector, we are entitled to wonder why we use a platform whose main asset is to offer a managed memory, if it is to avoid using it? In practice, the user using frameworks such as Flink and Spark has the best of both worlds. These frameworks limit the impact of GC for their internal mechanics, optimizing the management of large data sets, making them extremely powerful. But they allow developers to use a high-level language, abstracting them of memory management, which is a strong argument towards their utilisation. Solve bottleneck problems, it is like starting over and over again (infinite loop). Even before Spark has incorporated these evolutions, we can already bet a coin on the fact that the shared cache of the processor will be the next bottleneck of big data systems. Quoting Carlos Bueno: Cache is the new RAM The future will tell us how to get around it… Thank you OCTO for their review. Stay tuned, the Big Data Whitepaper OCTO is coming soon! Références How does BigMemory hide objects from the Java garbage collector? Difference between Direct, Non Direct and Mapped ByteBuffer in Java Juggling with Bits and Bytes Javadoc Class ByteBuffer Further Adventures With CAS Instructions And Micro Benchmarking Le Garbage Collector de Java Distillé Java Magic. Part 4: sun.misc.Unsafe Issue 7075 JIRA Spark Deep Dive into Spark SQL’s Catalyst Optimizer Project Tungsten: Bringing Spark Closer to Bare Metal Tuning Java Garbage Collection for Spark Applications Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Big Data . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “The evolution of bottlenecks in the Big Data ecosystem” Roma Sharma 11/04/2018 à 09:07 Privileged to read this informative blog on Hadoop.Commendable efforts to put on research the hadoop. Please enlighten us with regular updates on hadoop. Friends if you're keen to learn more about AI you can watch this amazing tutorial on the same.\r\nhttps://www.youtube.com/watch?v=1jMR4cHBwZE Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-10-12"},
{"website": "Octo", "title": "\n                OpenShift 3 : private PaaS with Docker            ", "author": ["Edouard Devouge", "Yann Rouillard"], "link": "https://blog.octo.com/en/openshift-3-private-paas-with-docker/", "abstract": "OpenShift 3 : private PaaS with Docker Publication date 24/07/2015 by Edouard Devouge , Yann Rouillard Tweet Share 0 +1 LinkedIn 0 OpenShift is a private PaaS solution (Platform-as-a-Service) to build, deploy and run applications into containers. Open source, it is available under Apache 2.0 licence and released into 2 versions: Origin (community) and Enterprise . The genesis of the third version Since July 2014, OpenShift has been working on an ambitious refactoring project of its technical architecture to integrate – the now inevitable – Docker and Kubernetes . Launching this project a year ago was a notably bold and risky strategy for OpenShift. Indeed, while the competition with Cloud Foundry was at its peak, OpenShift chose to launch an important reengineering project with side effect of freezing development of new features and compromising compatibility with previous versions.  We now believe  they made the right choice. To date, the Origin project federates 86 contributors on Github (of which the 10 most active work at Red Hat ). This quite active community made about 16 iterations in 12 months and has just published its already very promising first release . Although the solution is mainly driven by Red Hat , it heavily depends on Kubernetes (by Google ). This raises the complex question of governance which depends on good collaboration and visions alignment of the two companies. What will happen with future features developed by Google: will they focus on Kubernetes or OpenShift? Obviously, the question is not yet settled. However, if the support of Google is confirmed, a two-headed governance of these giants can only be beneficial to OpenShift. It will confirm its competitive advantage over Docker Enterprise (Machine, Compose and Swarm). Automatically build and deploy applications: how it works OpenShift v3 offers 3 native ways to build automatically an application : The Docker-File mode : automatically build a Docker container by providing OpenShift with the URI to a source code manager pointing at a Docker-File and its dependencies. The Source-To-Image mode (STI) : allows to automatically build an application by committing  the application source code in OpenShift (like buildpacks in Heroku ). The Custom Build Mode : allows to provide one’s own application building logic by providing an OpenShift Docker image designed for this purpose. OpenShift 3 also allows to define an automated deployment strategy of an application when a new image version is published in the registry or when the configuration of the application is updated. To complete these build and deployment features , OpenShift 3 provides the ability to define its own application blueprints as template files described in Json or Yaml format. These blueprints describe both the topology of the application architecture and the containers deployment policy . The diagram below illustrates the assembly of the different components of a template for a 3-tier application in OpenShift. Diagram : example of a 3 tier application architecture hosted in OpenShift v3 The components assembled in a “template” are partially inherited from Kubernetes concepts . The main objects to remember are: A “POD” is a Docker container runtime environment local to a server (we will deploy two types of containers on a single POD if it is necessary to share local resources) A “Service” is an entry point (VIP) abstracting a load-balanced access to a group of identical containers. In principle, at least one Service per architecture tier is deployed A “Service Deployer” or “Deployment Config” is an object that describes a deployment policy of a container based on triggers (eg redeploy when a new version of an image is available in the registry Docker) A “Replication Controller” is a technical component in charge of the resilience of POD A “Route” exposes an entry point (DNS hostname or VIP) outgoing of an application Through its multiple deployment mechanisms and the ability to set its own “blueprints” OpenShift v3 comply to the most complex application architectures. An elegant architecture under the hood A deployment of an OpenShift 3 infrastructure can be done in standalone mode (using the docker image “ openshift/origin ”), or in a distributed way using the ANSIBLE playbooks provided by OpenShift. In the latter case, two kinds of server roles are used: “Master” and “Node”. The “Master” nodes purpose is to: process administration API requests coming from the CLI or the Web Portal build images and deploy containers ensure PODS resiliency through replication “ Masters ” rely on a distributed directory based on etcd to provide configuration sharing and services discovery . “ Nodes ” host the PODS and run containers (applications and/or Registry ) . Diagram : Deployment Architecture of an Openshift v3 infrastructure The architecture is at the same time distributed, scalable and resilient . However, automatic scalability is not yet supported by the platform itself: provisioning and capacity planning of the underlying servers currently have to be done manually . The platform can be accessed and managed through its REST API , its CLI or its Web portal (but currently only in read-only mode for the latter). OpenShift v3 administration web Portal OCTO’s point of view OpenShift 3 offers an interesting bridge between the “ Plateform-as-a-Service ” world and the “ Container-as-a-Service ” (CaaS) world. Red Hat came up here with a bold solution and a state-of-the-art architecture . We strongly appreciate the “ blueprints” specification format for architecture definition and deployment orchestration. In the beta 3 version, OpenShift platform didn’t put a strong focus on platform operability . It shouldn’t be used in production yet. However, user stories on that matter can be found in the roadmap : log aggregation with ElasticSearch – Logstash – Kibana or Fluentd monitoring with Heapster, metrics collection , ease of cluster deploymen t . We believe that modeling an application in OpenShift 3 is a new job that will require new skills in order to ask the appropriate questions: how to organize the containers? Should routes or services be used? How to handle data (persistence, replication, backup)? How to manage multi-tenancy? How to integrate Development and Deployment Software Factory? To sum up, OpenShift is a very promising private PaaS solution. It allows to reduce the time to market by automating application building and deployment processes from the very beginning of the project. It supports the most complex web architectures , even if the topics of data management and external services integration are not fully addressed currently. We believe Openshift 3 has everything in hand to become a reference in the area of D ocker based private PaaS . To go further: http://fr.slideshare.net/NewHopeKim/open-shift-and-docker-october2014 Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations and tagged cloud , DevOps , Docker , Kubernetes , OpenShift , PaaS . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “OpenShift 3 : private PaaS with Docker” Alexander 23/08/2016 à 18:43 Good article, thanks a lot! Short and concrete! Keep going... Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-07-24"},
{"website": "Octo", "title": "\n                New digital challenges in life science: the Swiss Lemanic area and beyond            ", "author": ["Alexandre Masselot", "Anthony Favre"], "link": "https://blog.octo.com/en/new-digital-challenges-in-life-science-the-swiss-lemanic-area-and-beyond/", "abstract": "New digital challenges in life science: the Swiss Lemanic area and beyond Publication date 24/09/2015 by Alexandre Masselot , Anthony Favre Tweet Share 0 +1 LinkedIn 0 The life science sector faces great challenges when the time comes to meet modern digital opportunities. This sector is incredibly dynamic, both from an economic and a scientific point of view, but innovation in the lab often comes in pair with a deep evolution in the information system strategy. In this article, we will first focus on the Swiss Lemanic area landscape and see how the main computational trends encountered there are representative of the domain at large. We will then try to extract the most prevalent technical issues, either methodological or technological. More importantly, we will see that life science has a particular culture and faces original computational issues. But a lot of the digital aspects are strikingly similar to those of other industries, like finance, retail or social media. Therefore, a great deal is to be learned from how companies in those sectors have undertaken their own transition towards a new digital era (and vice versa). Welcome in Health Valley The life science ecosystem consists of many actors: the pharmaceutical industry, biotechs, medical instrumentation companies and institutional centers. In Switzerland, this sector is dynamic, both scientifically and economically, and plays a major role in the country’s economy [1] [2] . If Basel is the historical center and still remains the most active area, the Lemanic bassin (aka “the Health Valley” ) has no reasons to be shy. Science journal presumably even called the area “the number One cluster for life sciences research in continental Europe” [3]. Beside many private players (cf. figure 1), the public sector is also a key driver. Several remarkable public/private initiatives have been launched and provide a fertile breeding ground: Geneva Campus Biotech [4] , EPFL Innovation Park [5] , Lausanne Biopôle [6] etc. We are right in the middle of it! A 2010 panorama, but still relevant [7] . Where is the domain heading to? It is far beyond this article’s scope to depict the long term goals of all the private actors. Let’s remain modest, and focus on the information system side of the challenges at stake. A few initiatives can be seen as representative: Health2020 [8] : the federal level comprehensive strategy, decided in 2013. Two of its highest priorities raise serious computational challenges: the electronic patient dossier and the handling of gigantic flows of personalized data. The Human Brain Project [9] : involving more than 130 partners in 26 countries, the project vision is to build a low level model of the human brain and to structure all the information on this subject. European by nature, the project is directed by EPFL, and its largest center is hosted in Geneva. CHUV Biobank [10] : one of the Lausanne University Hospital Biobank’s mission is to collect patient data, like genomic sequences, at a very large scale. All consenting patients entering the hospital are targeted, with an estimation of 15,000 samples per year [11] . This project opens the path for a larger regional and country wide strategy. These initiatives certainly shoot for the sky. But rather than focusing on technical (or scientific, ethical, political or even psychological) concerns raised by each of them, we should see these projects as representative of the stimulating local ambiance in life science and appreciate how they shape the Lemanic area landscape, both inwards and outwards. Information System challenges Addressing the core biological questions is also out of the scope of this paper and we’ll focus on information system issues. Like other experimental sciences, the digital age has fully entered the wet lab. And some numerical challenges raise important, yet exciting questions. Big Data Let’s shoot the elephant in the room first: laboratory instruments generate a huge information flow. Genomic sequencing is not the only data provider, but it is nowadays the most prominent one. The advent, a decade ago, of the Next Generation Sequencing (NGS) technology dramatically changed the paradigm. From three billions dollars in the nineties down to less than a thousand dollars today for a single individual [12] , sequencing is making its way to the modern physician’s toolbox. “The $1,000 genome”, Nature [12] . Genomic sequencing covers a large variety of methods. A typical whole genome analysis, on a single patient sample, will generate roughly 200GB of data. And if we go back to the previous section, where sequencing 15,000 patients per year in a single hospital is the plan, we can estimate the total yearly volume to be around three petabytes (3×10 15 bytes). The term “big data” gets all its weight. As discussed in the aforementioned Nature paper, NGS broke the Moore’s Law pattern. Keeping pace only with the data storage aspect of it is a tremendous challenge (not to mention analysis). On the one hand, compression efforts are undertaken to reduce data volume but hardly change the problem due to the exponential growth of said data. On the other hand, some scientists advocate to simply destroy the raw data and only keep some analyzed projections while storing the original samples in a freezer. They argue that freezer storage is much cheaper than disk space and if the raw sequence data are needed in the future, they could simply be acquired again (with certainly more performant technologies). Those ideas are still debated in the field as they face all sorts of ethical, scientific and technical arguments. Visualization Once terabytes have flowed to your hard disks and brilliant analysis methods have run on your large scale cluster, the problem is not over. In fact, it starts there: you have to try to make some sense out of this deluge of data. Static analysis reports are useful in some situations, but scientists often need to have a more intimate relationship with their data to question them, to navigate through them or to put them in perspective with other contexts. Visualization, and more precisely reactive and interactive analysis, comes into action. Without requiring every tool to be as sophisticated as the Allen Institute Brain Atlas [13] , interacting with data is often the best way to get a feel for and understand the underlying complexity. It is to be noted that visualization can also encompass the “scientific art” category, producing striking (and beautiful) images [14] . But beside making a nice journal cover, or the best decorated office wall ever, such representations rarely convey deep and relevant scientific meaning. Fortunately, the advent of powerful web browser technology, high speed network and strong cross domain emulation, paves the road to new possibilities. Publications and conferences do already exist in this field [15] , but a new era is beginning. The electronic patient dossier Its goal is to digitize as much patient’s data as possible, at a canton scale, and make them available to health sector actors (physicians, hospitals etc.) while hiding them to others (insurance companies, hackers). This goal stands high among the swiss Health2020 priorities. Without diving into the nitty-gritty details of such an endeavor, one can imagine the complexity of the underlying information system architecture. Until now, only Geneva and Valais cantons have migrated to the electronic patient dossier. In fact, Valais almost made it, as the service was stalled after 4 days. To make a long story short, on August 27 th , after four millions swiss francs of investment, the project was released. The communication department streamed messages about how secure the system would be, as “it had been heavily audited and proven unbeatable” [16] . So long for the unbeatable security: within a couple of days, the Pirate Party pinpointed “serious security concerns” , waived out by freely available tools [17] . On August 31 st , the Valais Minister of Health suspended the project [18] , and everyone is now back to work… © Fran, cartoonstock.com Biomedical sensors Stepping across the three previous issues, this topic embraces the gathering, storage and analysis of physiological measures. Many devices allow for real time monitoring of heart rate, activity etc. both for medical and comfort reasons. This subject, sometimes referred to as “Quantified Self” [19] , ranks high in the news and many actors eagerly look in that direction, although it is hard to predict its real importance on the longer run. Laboratory Information Management System (and other heterogeneous data management) A LIMS tracks all the laboratory operations on a biological sample. Where does it come from? How was it preprocessed? Which instrument did it go through? And which analysis steps were performed? What (and where) are the analysis results? This information is crucial for proof, audit, reproducibility, etc. and an efficient LIMS platform is a Grail for most laboratories. If such platforms are not a new need, their actual implementations raise recurrent problems in the lab. Reality is often, at best, an assembly of custom and third party software. In fine , the scientist often falls back on his paper lab notebook as the single source of truth. Technical background and issues in life science information systems Stepping back from particular domain silos, we discussed how life science actors are facing recurring issues from an information system perspective. Let’s now try to identify some of the reasons underlying those patterns and the different dimensions in which they occur. We’ll finally realize how similar those aspects are to other domains and how lessons learned in other industries could be applied in life science. Software development culture in life science Let’s consider the software development culture as the overall environment in which digital projects are built. It encompasses low level aspects, like programming languages and tools used by developers, but also processes, relationships with customers (aka the wet lab scientists) and within the development teams. Last but not least, it also covers how upper management envisions the information system at the organization scale. Why is life science so special? Because dry geeks (the ones with a strange look and a keyboard) have to talk to wet geeks (the ones with a strange look and a white coat). And also because trying to understand biological phenomena can be incredibly complex and always drives people into unforeseeable journeys. Because of its culture, the life science digital ecosystem is original. A key factor is the porosity between academic and private companies and the historical importance of one’s scientific knowledge to be recognized by his peers. This leads to situations where a large proportion of software development players, at all levels, are former biology or chemistry trained people (with a strong inclination towards a keyboard, indeed). When the time comes to selecting a new hire, the balance often tips towards a biology fluent person with IT knowledge rather than towards a hard core software engineer, architect or manager, with proven abilities to dialog with people outside of her domain. Some companies try to counter that trend, but unfortunately “mirror hiring” is a natural tendency for most of us [ [20] . Technical bottlenecks Understanding the culture can prove to be illuminating to understand the ecosystem, but it hardly is a factor one can directly influence. However, we can highlight technical aspects, identify axes along which common situations could be improved. In the last section, we will see how to actually cope with them. Software culture: the foundations As we saw earlier, software people in life science are often wet lab “defectors”. Driven by domain knowledge, a backend developper is commonly more recognized by her peers when she can explain the intricacy of methylation X or Y in the chromatin rather than having deeper arguments on a sound decoupled software architecture. Well, that’s a side effect of life science being ruled by life scientists (which is, for plenty of other reasons, a very good thing). In consequence, programming language adoption, frameworks, libraries, methodology, testing, tooling and continuous building can be very far away from the state of art. Software culture: new technologies Hadoop, reactive programming, actors, modern web frontend, DevOps, cloud… These are not only hype words (again!). All these technologies could have been invented for life science purposes, as they perfectly answer many practical problems. However, their adoption level is way below their promises. This is partly due to reasons described in the previous paragraphs, but we can also admit that these solutions are scary. To make the matter worse, various implementations are continuously appearing and fading away, sometimes entangled in each others. As a matter of fact, these technologies are looked at with fear and envy, like strange, dangerous but promising new worlds. Without engaging in a long discussion on the topic, we can still make one last observation. The fantastic innovation level in the lab methods, the imagination demonstrated when designing a new drug are far more disruptive than changing an IT technology. Why don’t we find in the IT system of life science the creativity and disruptiveness one handles when creating drugs? Software development methodology “Agile” is like “big data” : yet another buzzword. But there are reasons why the word took such a momentum. Projects rarely fail only because of the lack of developers’ low level technical skills [21] . It’s the overall organization of the project, and even the overall organization of the information system at the institution scale, that often carries in its genes the reasons of a digital project success (or failure). We will not try to describe here the agile philosophy and its countless implementations (a massive literature does exist, indeed), as the Agile Manifesto [22] gives the best possible introduction. Every aspect of the agile approach makes it perfectly suitable for building digital projects in the life science field. However, “Agile” is not just a word and pronouncing it like an incantation in a yearly meeting is not enough to transform a whole corporate culture [23] . Among the most frequent failure patterns, we can observe: a lack of top-down support: a development team tries to adopt an Agile methodology but lacks support to open smooth communication channels with stakeholders and customers . The management does not embrace the “respond to change” nor the “customer collaboration” paradigms and feels uncomfortable without long term requirements set in stone and a matching well defined budget. Only top-down pressure: Agile is presented as a new religion by the management and the way to exorcise all old evils. But no room is made for training people and transforming the organization. Two years later, the converted manager flies to another position, the then out-of-fashion religion is banished and the team moves back to the old ways. So what. Now what? Addressing the technical aspects Software crafting The software skills can be addressed at the individual level. Many tools do exist: organizing a biweekly forum, pairing developers to cross-pollinate know-how, reviewing code to increase quality, arranging 20% of blue sky time slots or bringing an outside coach for specific needs. Challenging established software practice is an investment. It will slow down productivity on the short term, but the return is soon to be high on software quality, productivity and employee motivation. Last but not least, seeking technical excellence is certainly the most efficient way to hire the best talents… and to keep them. Our experience shows that evaluating a new technology is even harder to pursue in house than improving one’s specific skill. More than a fearless and motivated lone software engineer, it relies on actual experience with the technology, field comparisons with alternatives and a clear picture of how it would fit in a global architecture. Outside help can therefore be a solution to consider before making long term radical commitments. Methodology: moving to Agile No out-of-the-box method exists and technical processes shall be adapted to the people, the structure, the company business and culture. But even if software development cycles are paced by bi-weekly sprints, if team members stand up every morning for a ten minutes chat and if a software factory continuously deliver working software, it does not mean that the Agile transition is over. Agile is a philosophy, a mindset ingrained at every level in a digital company. Roles are often to be redefined, processes redesigned, communication channels widened and transparency embraced. Not only top management must be committed to it, but all the concerned actors must understand and be part of it. It does not mean that the whole organization has to change at once. A pilot project can be launched to test and seed the approach. However, within this project boundaries, from the top management sponsor to the software engineers and in house customers, roles and relationships have to be adapted. We deeply believe in the benefits of such a transformation. It has proven to be efficient in many domains and organizations and fits particularly well the challenges at stake in life science. But we also have seen how such a transformation is multi dimensional. Therefore, seeking outside guidance might be an option to consider. Life Science’s digital situation is not unique At this stage, you should be convinced that life science challenges are unique… only to some extent. Our day-to-day experience with clients in other industry is actually very similar. If their business is different (and often much simpler than biology), banks, retailers, insurances, social media companies are faced with the same issues and challenges. They also often struggle to make their digital organisation evolve. Yet their situation is strikingly similar, as the ultimate goal is to keep pace with customer evolving needs, in a sustainable way. Therefore, they also need to: build modern, interactive and useful tools; handle massive flow of complex data; have transparent knowledge shared across stakeholders; continuously deliver running software; and not forget to have some fun. Towards digitalisation. Boarding for a new journey? The journey towards digitization is a great and perilous one but the software industry has matured and the time has come to embrace it. By the complexity of the underlying scientific domain, the data heterogeneity, its volume and analysis processes, the life science industry would benefit greatly from all those new trends and architectures. With a careful and educated strategy, some companies have demonstrated the benefit of such an endeavor. Are you ready to embark? References [1] “The Importance of the Pharmaceutical Industry for Switzerland” , S. Vaterlaus et al , 2011 [2] Swiss Biotech Report 2015 http://www.swissbiotechreport.ch [3] the citation is repeated as a meme, but we could not locate the original reference [4] http://www.campusbiotech.ch/en/ [5] http://epfl-innovationpark.ch/ [6] http://www.biopole.ch/en/index.html [7] http://www.forumdes100.com/2010/03/health-valley-romande-la-carte.html [8] Health2020, Federal Office of Public Health http://www.bag.admin.ch/gesundheit2020/index.html?lang=en [9] https://www.humanbrainproject.eu/ [10] http://www.chuv.ch/biobanque [11] “Au CHUV, une nouvelle biobanque unique en Europe” , Le Temps, 12/13/2012 [12] “The $1,000 genome” , E.C. Hayden, Nature 3/19/2014 [13] Allen Brain Atlas http://www.brain-map.org/ [14] “Best science graphics visualization” Wired, 2014 [15] Visualizing Biological Data conference http://vizbi.org/ [16] “Le dossier électronique du patient arrive” J.Y. Gabbud, le Nouvelliste, 8/27/2015 [17] https://www.partipirate.ch/2015/08/30/nouveau-dossier-medical-eletronique-valaisan-la-protection-du-patient-bradee/ [18] “Esther Waeber-Kalbermatten suspend le projet cyber-santé en Valais” M. Atmani, Le Temps, 8/31/2015 [19] https://en.wikipedia.org/wiki/Quantified_Self [20] “What Differences Make a Difference?” E. Mannix & M.A Neale, Psychological Science In The Public Interest, 2005 [21] “The Most Common Reasons Why Software Projects Fail” , T. Putnam-Majarian & D. Putnam, InfoQ, 6/13/2015 [22] http://www.agilemanifesto.org/ [23] “8 Reasons Why Agile Projects Fail” L. Cunningham, 4/9/2015 http://blogs.versionone.com/agile_management/2015/04/09/8-reasons-why-agile-projects-fail/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Culture , Methodology and tagged Suisse . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-09-24"},
{"website": "Octo", "title": "\n                Management 3.0: interview with Jurgen Appelo at USI 2015            ", "author": ["Alban Dalle", "Dominique Buinier"], "link": "https://blog.octo.com/en/management-3-0-interview-with-jurgen-appelo-at-usi-2015/", "abstract": "Management 3.0: interview with Jurgen Appelo at USI 2015 Publication date 23/07/2015 by Alban Dalle , Dominique Buinier Tweet Share 0 +1 LinkedIn 0 Management 3.0 is definitely a trendy topic this year and the lucky participants of the USI 2015 event could directly hear about it by its author: Jurgen Appelo. In addition to his talk at USI, Jurgen Appelo kindly accepted to answer our questions. OCTO: What would you highlight as new / disruptive with Management 3.0? Jurgen Appelo: What is new is to manage the system, not the people. For example for our merit money bonus system, I don’t decide who gets how much money, I don’t see that as my job. I think the crowd knows better who has what kind of performance, so I let them make the decision together. What I do is to make sure that this process works as well as possible. That is my responsibility, introducing such an idea. This is the same for many practices traditionally used by managers with higher positions than their employees. I don’t see it as my responsibility to use a carrot and a stick in order to get someone to perform better. My job is to set up things in such a way that people love developing their own performance. OCTO: Who builds the system? Did you or your team propose this bonus system? Jurgen Appelo: I came up with the idea, and the team said: « It sounds like a great idea, let’s try! » . So I first got buy-in from the team members and then we talked about what worked and didn’t work. In some cases I am the only one who can make the decision, like to decide how much money we can spare or to define the purpose of the company. Many other things are delegated to the people so they work it out with each other. OCTO: Do you still have one-on-ones with your directs? Jurgen Appelo: No, we have many opportunities for individual conversations but I don’t have a regular cadence of one-on-one conversations with each of my subordinates. For me it’s a bit old-fashioned. OCTO: Have you identified a particular profile of companies interested in Management3.0? Jurgen Appelo: I don’t think there are particular profiles, but I see categories of organizations: – startups who already experiment many things and don’t want to end up with a big bad hierarchy when they grow. They are looking for new, modern management practices and get drowned to these ideas that I borrowed from many different sources – none of these are my own ideas, I just find them around the world. – more traditional organizations who feel that they are falling behind and want to modernize. It is difficult for them because they have to deal with an entire system already in place. But it’s never useless to try! There are examples of traditional companies that made such a transition. OCTO: Do you have an example of such a traditional company? Jurgen Appelo: There is company in the Netherlands referred to by Frederic Laloux in his book “Reinventing organizations”. They completely transformed themselves. My own experience is with startups, I’m not a coach or consultant so I don’t follow companies in their transitions. But I have read every now and then that companies have been successful with the transition. OCTO: What do you think managers can primarily expect from Management 3.0? Jurgen Appelo: First of all: less work! There are many things that you don’t have to do anymore, like performance appraisals. Research shows that they don’t work and you save a lot of time by not doing them anymore. Then, you save on the fewer managers that you need. Plenty of people became managers because it was the only way to make a career. They actually don’t want to be manager but it was the only way to earn more money. They will be happy if you give them alternatives. If you ask me, I started enjoying my job much more and so did my subordinates! OCTO: Can you profile managers eager to try Management 3.0? Jurgen Appelo: The kind of people not afraid to try things out. And also to be creative because many managers work inside a bigger environment that demands process that we don’t want anymore. These managers try things within their own scope of control and have to translate it for the outside world still practicing the old way. It often means extra work to make this translation. OCTO: Have you already seen big traditional companies where a manager experiments with his team and keeps the old way with his own management? Jurgen Appelo: Yes, years ago, a friend of mine from the U.S. told me that he got bonus money from the CEO who expected him to distribute it among team members. He decided to let them choose how to do and they rapidly came up with the decision to give half to someone and to split the rest between the others. It was amazing! The team was happy to recognize the special contribution of one of them, and nobody was blaming the manager for doing a bad job. But then you have to explain this to the CEO. You need courage to run experiments and make the translation outside your team, to defend your team. OCTO: What is the main difficulty when you start? Jurgen Appelo: All of these practices that I share in my book are suggestions, things that you can try but it doesn’t guarantee that they will work for you because every organization is different. Yoga is a good metaphor: you do yoga practices because in principle it’s healthy. But not every practice is good for every person and you have to be smart about that or ask a yoga teacher. That is the same with Management 3.0 Workout practices: stuff that work for some other companies, that you can try, see how it works and then adapt. The problem is that many companies say: it won’t work for us without even trying! Sometimes things fail. For example in my team, we are going to change the way we work with OKR (« Objectives / Key Results ») because it doesn’t work. We keep the principle that is behind: people have to define their own target, we don’t do it for them. But we are going to change the way we do that. We share our experiments online so that people can see what we try and they seem to appreciate it. OCTO: How have your convictions evolved these last few years, what lessons have you learned? Jurgen Appelo: The things I learned are mainly about new individual practices, but it all fits within the philosophy « Manage the system, not the people ». For example in the past I was part of an Innovation Committee with an Idea box initiative. It failed because the first few ideas posted were evaluated by the committee and rejected. After that, nobody proposed any other idea. I would call that a Management 2.0 practice: well intended but it failed. Now I would say: let other employees rate the ideas! Practices keep emerging, come and go but the principles stay. OCTO: Do you regularly check the system? Jurgen Appelo: Yes, we hold a regular retrospective meeting, a sanity check with the whole team every couple of months about what works and what doesn’t. We experiment different ways to run it. But to be honest, we have created a culture where most problems are addressed when they appear. We don’t need to wait for the retrospective to talk about it. But we still think it is a good thing to take this moment. OCTO: Where to start? Jurgen Appelo: Start anywhere! Anywhere that makes sense for you. Some areas are easier than others: it’s much easier to discuss « Personal maps » than to start changing the salary and compensation system. It is better to work first on an new organizational culture before changing how people are being paid. As in yoga, some postures are for advanced practitionners. But there is no order between easy practices. Just try something that looks really easy and harmless to you : “ Kudo box ” or “Personal Maps” are very easy and everyone like them. Want to know more? Check out Jurgen Appelo’s talk “Manage yourself!” at USI! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology and tagged agile , jurgen appelo , Management , Management 3.0 , usi event . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-07-23"},
{"website": "Octo", "title": "\n                The Web Giants: the book is being translated!            ", "author": ["David Alia"], "link": "https://blog.octo.com/en/web-giants-the-book-is-being-translated/", "abstract": "The Web Giants: the book is being translated! Publication date 26/06/2015 by David Alia Tweet Share 0 +1 LinkedIn 0 We are very proud to announce that our best-seller book “Les Géants du Web” is currently being translated into English. The PDF version should be available for free during July, the print version should be available at the end of August on Amazon platforms. As for the French version, we’ll publish chapters on this blog every week while you wait for the definitive version. Stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Uncategorized. Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “The Web Giants: the book is being translated!” Timur 22/07/2015 à 06:37 Where can I get the link to download the PDF book? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-06-26"},
{"website": "Octo", "title": "\n                Develop a secured Android application            ", "author": ["Rémi Pradal"], "link": "https://blog.octo.com/en/develop-secured-android-application/", "abstract": "Develop a secured Android application Publication date 30/09/2015 by Rémi Pradal Tweet Share 0 +1 LinkedIn 0 Android applications are commonly used to process very sensitive data. It is the developer’s responsibility to make sure that the information prompted by the user cannot be intercepted easily by a malicious people. The Open Web Application Security Project (OWASP) [9,10] tries to enumerate the potential security issues of a mobile application. Some of them are the system architect’s responsibility (such as issues related to weak server sides control), some are the back end developper responsibility (issues related to authentification checks) and finally, some are purely related to the mobile application. In this article we will focus on the issues which can be tackled thanks to the Android mobile developer’s action in itself. Therefore we will address here three potential vulnerability sources : risks when we communicate with a webservice (WS), potential leak of information when we store data on the device storage and vulnerabilities of having your application easily editable by a third party. 1. Secure Webservices calls In sensitive applications which use a WS, the most important thing is to make sure that the data we share with our backend is secure. Indeed the safest application is useless if the requests made over the Internet are easily catchable. Threat : Man in the middle attack (MITM) There are two major risks when an application can be affected by a MITM attack. Information leak If a pirate can control the local network where the user uses the application, he can easily intercepts all the communications between the app and the WS stealthily. Webservice (WS) mimicking Someone with certain knowledge of the WS format can block the application call and provide its own fake response. In that case the user thinks its request has been performed whereas the request has never reached the backend. It is quite easy to test how vulnerable to a MITM attack your application is : you just need to use a software which will be used as a proxy (for instance CharlesProxy [12]) and then set up your device to use the machine which has the proxy installed. If your application is not protected against MITM attack you will be able to see every request performed by your application. Now, imagine that one of your app user connects to your webservices through a “not safe” network : it is effortless for a pirate to install a proxy on the network routeur which will sniff all the requests in clear. Attack origin : the TLS/SSL certificate chain A minimum to ensure that our communications are safe is to use the HTTPS protocol i.e using a communication which is crypted thanks to the Transport Layer Security (TLS) or its predecessor the Secure Sockets Layer (SSL). Meanwhile, if this condition is necessary, it is not the only one that our system has to comply with. To understand that, let’s have a look of how the SSL protocol works. A SSL certificate is composed of (at least) three certificates Root certificate. It is a certificate issued by a Certification Authority (CA), i.e. a trustable organization that will make sure that the whole transaction is safe. Intermediate certificate(s). There can be several intermediate certificates. They make the link between the end-user certificate and the root certificate. It is a certificate which is dedicated to the server exposing the WS and which is signed by a root certificate. End-user certificate. The end user certificate is a certificate proper to the WS physical server. Android SSL native protection: The Android network layer has an embedded list of CA certificates (more than one hundred, you can check the list in the preferences of your devices). Every HTTPS network call has to have one of these CA certificates at its certificates chain root. Meanwhile there is nothing to make sure that the rest of the chain corresponds to the server we want to contact. For instance a pirate can do a man in the middle attack by buying an intermediate certificate to the CA. All the network transaction will be seen as valid by the system. This vulnerability is very common : a study has shown [1] that 73% of the application using HTTPS protocol do not check the certificate in a proper way. How to make sure that we are connected to our back-end and that this connection is safe? The solution to the issue described above is to manually check that the intermediate certificate (which is proper to a particular server) is a known certificate. This means that we have to store in the application this particular server certificate. It can be done in a resource file or directly in source code file as a constant. We may wonder why we have to check the intermediate certificate instead of the end user. There is two reasons for that. The first one, as we will see later is that end user certificates have a short life time. The second one is a security reason : imagine that a hacker takes full control of your system, then we will own your private key (your server need that to sign his requests). Your application will see that the request is signed with the correct end user key and will allow the connection. If the verification is done thanks to intermediate server check, it is possible to remotely revoke the certificate by contacting the intermediate CA. The Java class that is verifying that a SSL connection is secure is called an SSLSocketFactory. To create an SSLSocketFactory that will perform an intermediate certificate check we have to follow these different steps. Create a class inherited from X509TrustManger. This class is an abstract class from the java.net.ssl package which is dedicated to check the server side validity of a SSL socket. Set a new default SSLSocketFactory. This code should be run before any network call. Certificate check potential drawbacks An intermediate certificate can expire (their lifetime is approximately 10 years). A solution can be to anticipate this change by adding the new certificate in the white list way before the certificate is changed. The intermediate CA can be compromised. If the intermediate CA is compromised, the security mechanism described will be completely useless. Indeed if the private key of the intermediate CA is detained by a hacker he will be able to forge a certificate chain which will have the same intermediate certificate as your certificate chain. In that case the pirate will be able to perform a MITM attack. Even if the CA are in theory safe it might happen such as in 2011 when DigiNotar was compromised [13]. If it happens the only thing to do is to change all the SSL certificate chain of the server and push a new version embedding the new intermediate certificate. The SSLSocketFactory trust policy is applied to all network calls in the application. If a sdk is embedded, it is necessary to embed the intermediate certificate of the remote server of these sdk. This can be problematic as it is not easy to anticipate the certificate modifications of these servers. This problem can be faced by dynamically injecting certificate. The application allows only one certificate (the main server one) and retrieves a dynamic list of authorized intermediate certificates when the application starts. Then, these certificates are added in the SSLContext trust manager. To conclude, in most of the situations this intermediate check mechanism will ensure protection against MITM attack. When the hacker intercept the communication he will have to include its own certificate in the chain, the TrustManager will not recognize this certificate and will refuse the HTTPS connection. 2. Safe storage on device The Android platform provides a convenient way to store preferences and even big files thanks to the SharedPreferences interface. Even if the data stored in these shared preferences is hidden in a masked directory, it is possible to retrieve the data easily if the device is rooted. Consequently, if the information stored by the application is sensitive, it might be necessary to encrypt the data stored in the shared preferences. It is possible to do so in two ways : Use a cryptographic library to encrypt/decrypt the values (and eventually the keys) of the SharedPreferences. There are many state of the art java cryptographic library javax.crypto, Bouncycastle[2] and Concealed[3] Use a library providing a SharedPreferences wrapper. These libraries are very convenient as the developer does not have to care about which algorithm has to be used. Meanwhile, using these libraries can lead to a lack of flexibility and some of them are not using safe algorithms. Consequently they may not be trust to store very sensitive data. One of the most used libraries providing this kind of wrapping feature is SecurePrefences [4]. In you choose this solution, you can instantiate a SecurePreferences extending SharedPreferences in a very straightforward way : These two methods are based on symmetric cypher algorithm such as AES (with an appropriate key size). It leads to wonder : which key should-we use ? Indeed if we use a static key, the preferences can be decrypted by retro-engineering the application. So, the best solution would be to use a pin-code/passphrase that the user has to type when the application starts. Another possibility is to use the Fingerprint API [15] (available since API 23) which provides a safe and fluent way to authenticate. Unfortunately this approach cannot fits every application’s user experience. For instance if we want to display some information stored before the pin code is typed, then we cannot use this secure encryption system. Hopefully Android provides a safe way to generate a key which will be unique for each couple application/device : the KeyStore. Android KeyStore’s goal is to allow applications to put private keys in a place where they cannot be retrieved by another application or by materially accessing the data stored on the device. The mechanism is pretty simple : the first time, you run your application to check whether a private key linked to your application is present or not. If not, you generate one and you store it in the KeyStore. If the private key is already present, you can use it as a cryptographically safe key to decipher a SharedPreferences data thanks to the algorithms described above. Obaro Ogbo wrote a detailed article [11] describing in depth how to use the KeyStore to generate a Private/Public Key couple. The main drawback of the KeyStore is that it is available only since API 18. Still, there is a backport library which provides compatibility since API 14 [14] (this not an “official” backport so you have to use it at your own risk). Consequently we can propose the following decision diagram when deciding which type of preference system we should use: 3.   Protect application against source code analysis and modification Sometimes, an Android developer wants to make sure that its application will not be analyzed, read and eventually modified by anyone. There can be different reasons for this request : We may want that a hacker will not be able to remove a lock in the application that is preventing non-paying user to use some features. A risk when we develop a sensitive application is that a hacker modifies the application in a way that all the typed information are returned to him. Even if it cannot easily happen on the play store, there are many other places where a user could download a forged application which will steal all his data in a perfectly transparent way. What every Android developer should keep in mind when developing a sensitize application is that retro-engineering an Android application is (quite) easy for an experienced people. That is particularly true if you use “native” Android application building. Indeed, due to the nature of most Android app (utilization of Java bytecode), it is easy to decompile the bytecode read it, modify and eventually rebuild a modified application [5]. In this part we will highlight some technical tools and some architectural rules that can mitigate these risks. Meanwhile we have to keep in mind that as the application is executed on a client device there is no 100% sure method to face these risks. 1. Write your valuable algorithms in server side This is an architectural guideline. Is all the value of your application is based on an algorithm, you obviously do not want that someone can easily read it, copy it and embed it is own application. In that case the best solution is to implement the algorithm in the server. The application will only feed a WS with the data to be processed and get the algorithm’s return. The obvious drawback of such an architecture is that the central feature of your app cannot be used offline. 2. Do not keep your WS wide open If the value of your application is in data retrieved thanks to a WS, you have to secure these WS by sending a session token obtained during an authentication phase or by giving the user/password in each request. If you only use an authentication flag in the app preferences it is really easy to modify your application code to put this flag on the “always connected” state. The risk of doing this is that the user will have to type its used id and password regularly to extend the session. 3. Use Proguard to obfuscate your code Proguard is a very common tool used in Java projects. The Proguard tool performs three operations : the shrinking step (Unused code removal), the optimization step (some methods are inlined, unused method parameters removed etc..), and the obfuscation step. In the last one, the tool will rename all the class, attributes and methods names in all the java files in order to make them unreadable if the bytecode is decompiled. Of course Proguard makes sure that the JVM will be able to identify the different compiled elements. This tool is very interesting as it harden a lot the readability of the decompiled bytecode. Meanwhile even if the code elements are renamed it is always possible to guess the role of the obfuscated methods and attributes by retro-engineering it. Proguard generates also a mapping file which can be used to convert an obfuscated stacktrace to a readable one [6]. There are plenty of tutorial on the web explaining in detail how to configure Proguard, for instance in the Android documentation [7]. 4. Use compiled library Thanks to the Java Native Interface (JNI), it is possible to use native code (compiled code) written in C or C++ and interface it with Java code. When developing Android application it is even easier to do that thanks to the Native Development Kit (NDK) which provides facilities to use compiled code in you applications. The overall mechanism is simple : you compile your C/C++ code (which must contain standard JNI entry points), and get a .so file. You will then include this library in you application project and its java interface. The major interest of compiled libraries is that the decompiled code will be much less readable as the .so library is made of native machine code and not of Java bytecode. A good practice (if it is practically convenient) would be to develop highly sensitive parts of the application in C or C++ (such as a top secret algorithm or a security layer) and interface it with the rest of the application classically written in Java. Still, there are several drawbacks of using NDK : we must compile the native library for all the different types of hardware architecture our application is targeting, we drop all possibility to have a decent stacktrace when a crash happens and it increases the code architecture a lot. Conclusion In this article, we proposed solutions which cover 3 OWASP’s top ten mobile security issues[9]. As we said in the introduction, an application can be secured only if the system architecture linked to it is secured as well. One can develop a technically safe application, if the server based authentification system is poorly designed, all these efforts are pointless. Meanwhile it is the mobile developer responsibility to make sure that its security perimeter is flawless, this article proposed solutions to cover this perimeter. References [1] https://www.fireeye.com/blog/threat-research/2014/08/ssl-vulnerabilities-who-listens-when-android-applications-talk.html [2] http://www.bouncycastle.org/ [3] https://code.facebook.com/posts/1419122541659395/introducing-conceal-efficient-storage-encryption-for-android/ [4] https://github.com/scottyab/secure-preferences [5] http://geeknizer.com/decompile-reverse-engineer-android-apk/ [6] http://proguard.sourceforge.net/manual/retrace/examples.html [7] http://developer.android.com/tools/help/proguard.html [8] http://www.javaworld.com/article/2076513/java-concurrency/enhance-your-java-application-with-java-native-interface–jni-.html [9] https://www.owasp.org/index.php/OWASP_Mobile_Security_Project#tab=Top_10_Mobile_Risks [10] https://www.owasp.org/index.php/About_OWASP [11] http://www.androidauthority.com/use-android-keystore-store-passwords-sensitive-information-623779/ [12] http://www.charlesproxy.com/ [13] https://threatpost.com/final-report-diginotar-hack-shows-total-compromise-ca-servers-103112/77170/ [14] https://github.com/pprados/android-keychain-backport [15] https://developer.android.com/about/versions/marshmallow/android-6.0.html#fingerprint-authentication Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged android , security . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Develop a secured Android application” Harsh 20/10/2015 à 11:43 nice content... learned a lot Android Example 29/06/2017 à 12:36 Hi dude, i have also find out one good example Android Session Management Using SharedPreferences Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-09-30"},
{"website": "Octo", "title": "\n                Why use Node to build my API?            ", "author": ["Adrien Becchis"], "link": "https://blog.octo.com/en/node-for-api/", "abstract": "Why use Node to build my API? Publication date 08/07/2015 by Adrien Becchis Tweet Share 0 +1 LinkedIn 0 Over the last few years, the REST approach has become the new standard to build API on top of HTTP. In the same period, server-side landscape is facing huge changes along with the Node.js breakthrough If it’s easy to build a small HTTP server with few lines of code of node, why not build a real API? A powerful technology for the Web If Node.js made a breakthrough in web application area, it’s mainly due to the paradigme shift that it introduced which brought considerable performance gains. Non blocking IO At the heart of node architecture, there is non blocking IO. This is crucial for web applications that are IO Bound. The latency introduced by reading from files, networks or databases is multiple order greater than the computations that happen on the CPU. With blocking IO, the thread or processus would be idle most of the time. Most of the node core is made of the non blocking library to handle filesystem, http, and the other low level networking protocol such as TCP and UDP. All these operations work with a callback, a computation that will be called once the operation is performed. The function returns immediately, so we don’t have to wait for the completion of our IO operation. Reactive approach Non blocking IO would be nothing without a reactive approach, event driven programming that is the essence of javascript and node. Node implements it through an event loop than will constantly handle events with their associated continuations . It’s the same schema that is on the browser, schema that is also used by the high performance server such has Nginx that can handle simultaneously numerous requests with a single thread. The node runtime keeps a stack of functions to be called when some event occurs, and the unique thread will sequentially handle these events. In concrete terms, the node engine will keep a stack of functions to be executed at some event triggering, and the only thread will handle these events one after the other. Lightweight and high-performance With an event driven approach along with non blocking IO, we don’t need several threads to concurrently handle requests. With a single thread, we don’t have to pay the context switch and other overheads induced by managing multiple threads. Besides, the remaning thread won’t be idle waiting for a blocking operation to be completed. It will continuously handle events as they appear with new requests, or read completion. However, we have to be cautious, and avoid at all costs blocking the event loop with CPU heavy computation that will waste all these benefits. With CPU being used at best, and memory footprint being low, we can use lighter servers without having to renounce to performance for our end users. Also with the absence of shared state that frees us from synchronization we can more easily replicate the application on several nodes behind a load balancer. Hence Node.js enables to horizontally scale easily with commodity hardware . Id est cheap and easy to obtain nowadays on the IAAS or PAAS clouds such as AWS or Heroku. The exact contrary of the traditional application servers such as WebSphere or WebLogic. A productive and industrialised Stack Unix-like Philosophy Node philosophy is one of the Unix philosophy ‘s descendants, that we could sum up as emphasizing building short, simple, clear, modular, and extensible code. Indeed node core offers a simple API and Application which are structured around small and reusable modules that focus on a single feature. This favors maintenance and reutilisability and this has influence on the culture of node ecosystem where numerous small and sharp libraries, packages have emerged. Tools to handle and share packages A huge number of small packages can easily lead to a dependency hell without the appropriate tools. Fortunatly, Node ecosystem has npm , Node Packet Manager . Packages are described in a package.json , a standard that describes the metadata of the project and the different dependancies. This not only favors code reusability, but code sharing. Indeed it’s very easy to make your modules available for the community. Thanks to this good package management, collections of library are available on npm , libraries that are well thought-out, collectively developed (most of the time on github), performing and tested by thousands of users. Whatever your need, you should have a look on npm registry , as we already pointed out few years ago , “There’s a module for that!”. Javascript at the heart of platform success. Certainly, javascript language is far from being perfect, but it’s getting better with the new EcmaScript versions on the go, and good linter were built. Besides, Ryan Dahl’s idea was not using javascript on the server side. It was to build an evented server. Hence javascript choice, language offering closures that are used as continuation, and also being used for years in our browser. Javascript has more advantages. First, Json is becoming the de facto exchange format on HTTP, and it’s natively handled by Js – which is one of main reasons beyond the json takeover of xml. This is both more convenient, and more efficient. Also, using the same language across the network enables us to reuse code or build isomorphic application . Being the predilection language of webfront developer, we can more easily have fullstack programmer than can both work with the GUI or the API. A strong and growing community As an open source project, it enjoys both strong community and industrial support. Community Node is a living platform with its 36 thousand stars – third most popular github project – and its 148 thousand packages on NPM . It benefits from the already existing pool of javascript developers coming from the front. Also, numerous free resources in most languages are available online.  You can easily learn from blog articles which are more or less focused, along with books and dedicated courses such has nodeschool.io . Besides there is also a Node meetup in every city with an IT sector. Industrial support and use Though still new, node platform has already been adopted, used and tested by web giants such as Walmart, Paypal, Linkedin, Yahoo! Many cloud and virtualisation companies are backing it, such as Joyent, which fathered it. Since our first article back in 2011 Node and its main libraries have gotten far more mature. Besides, many worldwide companies have opted for it and Node is referred has a reference stack in some CIOs has a Java replacement. A subtle doubt was introduced by the mini-rift triggered by io.js fork in late 2014. It was caused by development cycle management and the slowness to integrate Ecma6 Harmony features. However this is (almost) already ancient history with 2015 may settlement , the two factions joining in the Node.js Foundation . Go for Node With its reactive approach and the induced performance, its quality and the strong community surrounding it, Node is a very good choice to build REST API. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged API , Node , REST , WOA . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-07-08"},
{"website": "Octo", "title": "\n                How to fail at code reviews – S01E01            ", "author": ["Michel Domenjoud", "Jérémie Gomez", "Ali-Asghar Houssein"], "link": "https://blog.octo.com/en/how-to-fail-at-code-reviews-s01e01/", "abstract": "How to fail at code reviews – S01E01 Publication date 05/06/2015 by Michel Domenjoud , Jérémie Gomez , Ali-Asghar Houssein Tweet Share 0 +1 LinkedIn 0 In the previous article , we introduced a general overview of the code review practice as well as two specific formats we use in our projects. Nevertheless, successfully introducing a new practice is not an easy task. It’s a bit like setting sail for the first time: once in the water, the first meters are always chaotic. There are lots of waves, we wonder whether it was really a great idea. Wouldn’t it be wiser to go back ashore? However, after a bit of dedication, we finally get to the quieter open sea: we just had to hold onto it. During our code reviews, we came across several pitfalls that were detrimental and that could jeopardize code reviewing in the team. Let’s explore, through real-life use cases*, why the first code reviews will certainly be difficult and what are the essential fundamentals to carry out successful code reviews. This article is sequel of the following article: What format for your code review? * None of the characters in this paper are fictional. Any resemblance to real persons, is certainly not coincidental. Should you identify to these characters, we may help you. My team does not progress Within a team I just joined, I heard Bob complaining in front of his computer: “Damn! How many times did we tell Martin that we don’t build SQL request with Strings anymore but with Hibernate Criteria?!!!” I went to Bob to understand what was happening. He explained that he reviewed Martin’s code and once more time had to correct a SQL request that was not following the standards. Me: “But, do you keep a written version of your standards, are they stored or displayed anywhere?” Bob: “Nope, they aren’t, but we all know them. Martin is the only one who doesn’t follow these standards. But then again, I can understand, he arrived recently.” Me: “Indeed… I have another question : you said that you were the one who corrected the code?” Bob: “Yes, that’s how we do it, I’d rather correct the code myself to make sure it is well done. And I wouldn’t interrupt him for that.” Me: “Let’s go talk to him?” The review does not foster any exchange In the previous discussion, why hasn’t Martin integrated the standard practice? Proof-reading someone else’s code is a good opportunity to give him feedback, to help him improve his own coding practices. Without interactions, the proofreader will successfully detect the bits of code that need refactoring, however the need for refactoring will tirelessly resurface with time since the code author didn’t receive feedback about it. Code review should be prepared by the proofreader, by doing a first pass over the code to raise the defects or questions to ask. This method allows to do part of the review asynchronously, but might prevent to create the essential exchange, ideally verbally. The code owner does not correct the defects. The previous example also demonstrates a lack of trust from the proofreader towards the code author. It is a shame because, the best way to get better, to learn and assimilate a standard, is to apply it by yourself. If the code review is a good opportunity to share knowledge, the correction step is as important than the former one: it is of paramount importance that the author corrects the defects himself if the proofreader doesn’t trust the code author to correct the bugs himself, they can start a Pair Programming session and build the correction together This in turns empowers the code author: correcting his defects may infantilize him whereas helping him to correct his own mistakes will make him aware of his responsibilities. Standards are not shared among the team Using code standards is another compulsory practice to make code reviews efficient. These standards are built collectively by the team  and are written and kept accessible, for example in a wiki. If standards are not written, people will inevitably forget or reinterpret them and thus not assimilate them. Usually, we initiate the standards at the beginning of a project and make them evolve as necessary as the project goes along. We modify them as necessary during, for instance “tech hours” dedicated to improve the standards. My code reviews always end up becoming endless discussions or “trolls” During the first code review organised within the team: Kent: “hey Becky, look at line 984, the curly bracket is not in line, plus your variable $moneyList does not meet the standard case: we use snake_case here!” Becky : “So, first of all, I don’t agree, there is no such thing as a “standard case”. And anyway, this is has been!! If followed the norm, we would be using PSR 42, and we wouldn’t need to talk about it!” Kent: “Mmmh, you might be right, there might be some good things but I don’t agree with their naming rules!” The discussion keeps going for a while, let’s just stop here. Whatever type of review, collective or between pairs, the first code reviews organised within a team are often “polluted” by endless debates, generally about topics that should be straightforward. As we’ve seen previously, in order to curtail these discussions, it is mandatory to establish written standards from the beginning. This is even more important for things that look obvious like naming standards (“what is the PSR norm used in PHP?”) or disputed distinctive features of the language (“should we use var in C#?”). It is highly unlikely that the whole team will be perfectly aligned on these points. At OCTO, we neither agree on everything. The important point is to find a compromise and to write it down. Despite that, we also come across aspects that are not addressed by the standards, or even complex topics which need a more advanced discussion before considering a fix. This is why, to keep making efficient code reviews, rules of thumb are: Whatever the format of the code review, any topic which is not about detecting defects should be postponed until after the review In a collective code review, each defect should be discussed no more than one minute. A moderator and a time-keeper must make sure of it. If a specific topic spurs debate, it must be written down in a “to decide, not standard” list: it is utterly important that no debate occurs during the review. So, how to make successful code reviews? Through these two testimonies, we have already given a few clues about how to make your code reviews successful. However if there had to be one rule to remember: be extremely vigilant when organizing your first code reviews sessions. Should they go wrong from the beginning and should you not manage to quickly fix the problems, you will shoot yourself in the foot. Code reviews will certainly be abandoned among your team or will certainly be abandoned or misused. Here are the key points that we suggest: If your team is inexperienced with code reviews, start by training yourself and favour collective sessions, at least in a first phase Build a formal code review process and make sure it is respected: how should the team proceed? when? with what tools? Make the practice evolve and improve during retrospectives Use standards that you can develop overtime and establish checklists Next episode will be called: “We are swamped with bugs even though we are doing code reviews.” In the meantime, feel free to share your feedbacks through the comments section, whether you had a good or bad experience! Thanks to the OCTOs from the Software Craftsmanship tribe for their valuable inputs. More articles on the same topic: What format for your code reviews? Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology and tagged agile , code review , Software Craftsmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-06-05"},
{"website": "Octo", "title": "\n                Which format for your code review?            ", "author": ["David Alia", "Michel Domenjoud"], "link": "https://blog.octo.com/en/which-format-for-your-code-review/", "abstract": "Which format for your code review? Publication date 07/05/2015 by David Alia , Michel Domenjoud Tweet Share 0 +1 LinkedIn 0 We mainly use two code review formats in our projects: collective review which is rather formal and peer review, which is lighter. Both have advantages and drawbacks: Let’s look into these formats together and see how to implement them within a team. But first things first: what is a code review and what benefits can we expect? In most areas involving writing, we cannot imagine that what is written is issued without proofreading. An article will always be proofread before publication (e.g. the one you are currently reading), either to check the substance – is the subject of the article well treated? – or the form – spelling, grammar, structure and text readability. Similarly, the code review practice consists in having one’s code read again in order to track as many defects as possible, be it on the content – does this code work, and correctly implement the required feature? – or the style – clarity, readability, standards compliance, etc. What about you: how many lines of code have you ever deployed in production without proofreading? Code review is a practice almost as old as software development and widespread among companies like Microsoft or Google . There is a good reason for this: it allows defect detection earlier in the development process, yet the practice is relatively uncommon in the teams we meet. Sometimes this is because it is unknown, but it’s more usually for the wrong reasons: “We have no time, it’s too expensive” or “We tried but it turned into a troll discussion” or even “I do not want to, it’s just policing!”. We realize that it’s not a practice that simple to set up! Detect defects as early as possible The main objective of a code review is the same as other insurance methods of software quality: find code defects as soon as possible. The benefits of code review are well established: according to studies compiled by Capers Jones on more than 12,000 projects, collective code review helps to detect about 65% of the defects, 50% for the review by a peer, while tests detect only 30% of the defects on average. Note that we do not say you should give up other quality assurance practices: it is an essential practice to combine with other practices, such as testing, to maximize the product quality before making it available to users. There are other benefits to code review: Improve intrinsic software quality and develop standards Collective code ownership : having the code being read by someone other than its author helps reinforce the collective code ownership, and reduce the risk of excessive specialization, when only the author of the code is able to change it Learning : code review is a way to train younger developers while sharing standards, showing them some techniques, or how some features are implemented. It’s also an opportunity for those who arrived more recently to bring a different perspective and propose new approaches Improve the quality of exchanges between developers. All these points highlight code review as an indispensable collective practice that contributes to spreading a culture of quality within the team. Collective code review Collective code review consists in organizing formal inspection sessions, regular and within a group, dedicated to the detection of defects. We suggest the following mode: Bounded Time and Scope Sessions of 1h30 maximum, once every two weeks A review rate of 300 lines of code per hour is a good target Beware, spending more time or trying to read more code in one session decreases the quality of the review Dedicated to detection of defects: the session should be dedicated to point the defects, not to correct them straightaway or discuss standards and design issues: up to one minute should be spent on each defect Status and follow up: at the end of review, it is decided whether the reviewed code is accepted or rejected. If necessary, the selected defaults are logged and and their correction is verified Participants The author of the code is present, and several reviewers To facilitate the processing, should be elected: a moderator, to ensures that people focus on the discovery of defects, with exchanges of quality a scribe, to collect identified defaults a timekeeper The review has been prepared: the code to review is selected and communicated in advance, so the reviewers have time to read the code The review is tooled : we gradually build checklists to facilitate the conduct of the review How to select the right code? In order to detect new defects, we often select the code related to the latest features. Ideally, we should read all the code recently produced, but it is only feasible if the team is small enough Otherwise, we select in the recently modified code a sensitive or complex part, or one that introduces new concepts, new piece of software architecture, etc. You can also select existing code that “scares”, that is complex or that generates frequent regressions, for example if you know you will have to make it evolve soon. Peer code review The peer review consists in adding a step in the development process. This is a more widespread practice because it’s simpler and less burdensome to implement. There are several variations of this type of review, from pair programming to a format close to the collective review, except that the review is made by only one other developer. Here is a format that we successfully used on several projects: In common with the collective review Dedicated to detect the defects, not to fix them Prepared Status and follow up With a pair In order to obtain similar benefits with those of a collective review, the review entails an exchange between the author and the code reviewer During the review, once the author has submitted the code and shared the intention of the code that has to be reviewed, only the reviewer browses the code Restrain the scope to one feature : as soon as the development of a feature or a user story is considered complete by the developer, a review is triggered on this code Systematic : review is systematic and mandatory for all code to be deployed in production environment Tooled : Since the revised code is directly related to one or more commits, we generally use the differential changes as a skeleton A more advanced tool is frequently used for logging the defects, like Gerrit or directly Github . On some projects, we also simply attached the list of defects associated with the user story on the board. Who does the code review? Ideally, the first available person handles the review. As in the collective version, it shouldn’t always be the same person who handles reviews, to promote collective code ownership. This is a mandatory step in your development process, display it as a new column to your board before the functional review stage. What to do with defects identified during the review? To be effective, a code review should be dedicated to address the defects, and corrections or any discussions should be held separately: When the correction to be applied is clear and unanimous (functional defect, deviation from an established standard), the author of the code implements the correction after the review and a follow-up is planned to ensure that the defects have been corrected. Two implementation approaches can often be argued. In a pair review, involving a third party or the whole team will facilitate decision making. When the point of attention is a non-established standard, the team’s written standards must be updated, ensuring that they are shared by everyone. It is common that some complex issues require an exchange with the entire team: You can book an extra half an hour after the collective review or solicit the team for a dedicated point when you are in peer review On several projects, we also ritualized a timeframe of one or two hours at the end of a sprint, known as “The Tech Hour” or Dojo , in which we can address these points Collective or peer review? The table below gives a (non-exhaustive) list of comparison criteria between the two types of code review: Collective review Pair review Efficiency (number of defects detected) +++ ++ Collective Code Ownership and learning curve +++ ++ Quality improvement, standards evolution +++ +++ Cost +++ + Setup ease + +++ The various studies mentioned in Best Kept Secrets of Peer Code Review and the chapters on software quality and collaborative construction practices in Code Complete show that collective review is generally more efficient than peer review, but has the disadvantage of being more difficult and expensive to implement. These studies also show that the vast majority of defects are identified during the review preparation. This comforts us in the effectiveness of the peer review, provided it is well prepared: it provides a good compromise, avoiding the cost of a meeting with many participants. You can also use formal collective code reviews in addition to pair reviews: in short format, limited to the items which are not obvious and/or not covered by the standards for some algorithms or complex designs during the initial period of reviews setup, so the team will collectively learn to practice reviews Conclusion? There are so many blog titles labelled like “The [benefits | dangers] of [Code | Pair Review | Pair Programming]”, but in the end, the debate is elsewhere. Even if we issue an opinion in favor of a preferred format, it does not matter if you do pair programming, collective or peer review: the key is to find the approach that matches the culture of your team. This article offers an interesting analysis on the subject and we’ll sum up its conclusion as follows: analyze and admit your weaknesses in your collaborative development practices. If you have not yet chosen an approach, exchange with developers who regularly use these practices. Experiment and try to improve, during a retrospective for example. We have so far described code review formats: through a following article, we will share with you the pitfalls encountered that could put at risk this practice. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology and tagged code review , method , Software Craftmanship . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Which format for your code review?” mark thristan 25/08/2018 à 14:27 A nice piece, and I like the reference to the SmartBear book and Code Complete, which both have excellent advice on Code Review in place. It is also worth calling out the CMMI CMM and PSP and the nice O'Reilly book on \"Codermetrics\"... While collective review and peer/pair review are vital, so is individual inspection (as everyone is individually responsible for quality). Risk-based review methods are also worth calling out (as in the pragprog book \"Your Code as a Crime Scene\"). I completely concur with your comment: \"the key is to find the approach that matches the culture of your team.\", and would point out that a less mature team will need more collective review than a more mature team which, in turn, may rely more heavily on individual competence and adherence. The question is not, therefore, which formats to use, but which mix of formats to use, when, in what volume, and to fit which team scenarios and cultures. To be honest, this approach applies to requirements and documentation review equally as to code. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-05-07"},
{"website": "Octo", "title": "\n                Multitasking or reactive?            ", "author": ["Erwan Alliaume", "Philippe Prados"], "link": "https://blog.octo.com/en/multitasking-or-reactive/", "abstract": "Multitasking or reactive? Publication date 09/06/2015 by Erwan Alliaume , Philippe Prados Tweet Share 0 +1 LinkedIn 0 For years now, any process running in parallel of others has required a dedicated thread. We believe this paradigm to be outdated. As the number of clients rises at an unprecedented speed, it seems no longer possible to multiply the number of threads without negatively impacting performance. We showed in our JavaEE Bench (witten in French) how performance can be improved by reducing the number of threads. To understand this, we must first differentiate soft-threads (a multi-task simulation on the same core) from hard-threads (processes running on different cores). The soft-threads are organized by the OS in a circular list. For example, assume a distribution every 10ms and a dispatcher without priority. Every 10ms, the current process is interrupted to execute the next thread. For six threads, the loop lasts 60ms. This mechanism is called “context switching”. And it uses the CPU for 60ms. To complete the loop, each thread must run at least once. If a soft-thread initiates a network or file input/output (IO), the soft-thread is removed from the loop until the IO is completed. This makes more time for other processes to run. Adding threads is harmful to all the others running. Two strategies are used to dynamically adjust the time dedicated to each soft-thread. The first idea is to reduce the time span dedicated to each. A loop still lasts 60ms, but each process is stopped more frequently. This increases the number of context switches, and it takes up more time unnecessarily. Or, better yet, the dispatcher raises the total time to go through the loop in order to reduce the number of interruptions. Once the OS interrupts a task, it must take a full loop before it can resume its execution. In theory, with a loop of 10,000 threads, we need 10,000 x 10ms to make complete the loop; or 100 seconds before restarting the processing! Some audit tools such as jClarity alert the developer if there is too much “ Context Switching “. Meaning there is too much code dedicated to the transition from one thread to the next. These tools can measure the time lost moving from one thread to the next in increments of time. With more cores, the same loop is executed on several hard-threads. The processor manufacturers are making efforts to improve parallelism. Unable to increase the frequency, they multiply cores and improve the capacity of each core to perform several processes simultaneously. For example, hyper-threading ™ is used to multiply the capacity of each core. The processor runs multiple assembly instructions in parallel for each one via a multiplication of transistors. But for this to work, the instructions executed in parallel on the same core must be independent. One should not depend on the result of the other. To reduce dependencies, some versions of processors rename registries when possible. The 9500 series and following Intel processors Itanium ™ offer specific instructions (EPIC) to optimize thread management via flags included by compilers and interpreted by processors. Parallelism strategies are applied by the compiler ( JIT ), which organizes the code and injects specific instructions enabling the processor to improve parallelism. With these processors’ evolutions, multiple instructions of the same thread and/or multiple threads run in parallel according to the processor’s capabilities. It is then possible to increase the number of hard-threads, supported directly by the electronics. Despite the increased capabilities of processors, additional soft-threads reduce performance. The current model of thread management is probabilistic. In real life, the processed user requests spend most of their time waiting for resources (file reading, response to a query against database, web service response, etc.). To avoid having an idle processor, the threads are multiplied with the hope that one can run while waiting for another. There are actually three concurrency situations with only two threads: The two threads run alternately during the idle time (the ideal case) The two threads run simultaneously. In fact, it is the operating system that distributes the 10ms time slots between threads. At each changeover, a context switch is executed. The latest versions of processors improve this phase, but it uses resources. The two threads are simultaneously pending. Since the waiting period is proportionately longer than the processing time, there is a high probability that the two threads are waiting simultaneously. To improve this, we increase the number of threads. But, at the same time, we increase the probability of context switching. There is no quick fix. The classical approach is therefore probabilistic, with a low chance of hitting the best case scenario. One can bet that the processes will be alternated by adding more threads. In fact, CPU consumption higher than 35% on a server is rarely noticed (without triggering the GC). This is also a figure noted by Intel ! The queries spend most of their time waiting for resources. That means 65% of untapped CPU capacity! If your application performs practically only IO, it is perhaps simpler to multiply threads. With each IO added into the threads of the distribution loop, there is in fact little CPU capacity loss. On the other hand, it consumes lots of memory. But if your queries calculate pages or json flow, CPU is necessary. The reactive approach The reactive approach, described in a previous article (in French) , suggests a different approach: deterministic. The idea is to be limited to hard-threads. Only one thread per core. No more. Thus, all processes are actually performed in parallel. The CPU is maximized. For this, all IO calls must use asynchronous APIs. Thus, the processors don’t have to wait anymore. An engine must then distribute processing for each event response. More and more companies use this model. This is the case in retail for example. With a different development approach, based on events and fewer threads, it is possible to be more efficient. In fact, Intel recommends only the use of hard-threads to manage requests to the Information System. To conclude Hard-threads are physical threads, corresponding to different microprocessor cores. Eight cores? Eight threads (sixteen or more if we agree to use hyper-threading). But no more for the entire program. Additional soft-threads may be present if they are idle most of the time. This is the case for the algorithms of purges, timers, etc. For the garbage collector, the question arises: should we dedicate a core for it or slowdown reactive processes? Different strategies are possible. The standard language libraries now offer a pool of hard-threads shared by all frameworks. This is no accident. Scala offers an ExecutionContext , Java 8 offers a commonPool which is also a ForkJoinPool, .Net offers a TaskPool . This shared pool adjusts itself to the running platform. The developer will have only four threads, for example, while the production server will have sixteen. As a minimum, the program must be able to operate with a single hard-thread. Reactive development architecture enables this. This is also the approach chosen by node.js . In next articles, we will see how to write programs without threads, the French blog post already exists . Whenever possible, we recommend using a reactive approach. Then, it is possible to improve application performance without having to increase the number of machines or memory. Moreover, this architecture offers horizontal scalability, fault tolerance and is able to absorb peak loads. This approach can be imposed for an entire project or used tactically in some parts. Unless your application primarily computes calculations, it is time to consider the reactive approach. All languages are candidates for this paradigm. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-06-09"},
{"website": "Octo", "title": "\n                Big data : some myths            ", "author": ["David Alia", "Simon Maby"], "link": "https://blog.octo.com/en/big-data-some-myths/", "abstract": "Big data : some myths Publication date 03/08/2015 by David Alia , Simon Maby Tweet Share 0 +1 LinkedIn 0 At my hairdresser’s, on the coffee table, I came across one of those hype men’s magazines with a model on the cover and the promise to learn how to avoid 10 common mistakes when wearing a tie. I accidentally open the page 34: “The Big Data revolution.” The subject continues to spread widely, particularly among the neophyte public, which annoys many people. Each success has its criticism … One can legitimately ask how the public comes to understand big data. Among simplistic journalists, embittered experts irritated by the abuse of the term and sellers who promise miracles, it is not easy to navigate. I will try in this article to dismantle some myths and redefine the term. Big Data = Big Brother This shot is a real public enemy. No, I do not work for the NSA and no, I won’t fraudulently make use of your personal data! What is Big Data? A cultural and technological phenomenon at the origin of an exponential accumulation of data within our Information systems. We share, communicate and produce data, more and more, all the time and everywhere. Infrastructures, technologies and statistical methods to massively analyze data The observation regarding the amount of data produced, global human brain mass will not be able to analyze everything. Hence, the importance of Datascience, machine learning and artificial intelligence to transform, in an automated way, this ocean of data into information, or better, into knowledge. Phenomenons have no moral inclination. No more than tools or technologies. Data analysis has always existed.   As well as the creation of personal information files. Big Data application’s primary concerns are health services improvement, energy consumption optimization and reduction (smart metering, smart city), user experience improvement, human knowledge sharing, fights against bank fraud, open data and the idea of transparency… Big Data = Hadoop Yes, Hadoop and its ecosystem represent a major player and an incredibly rich tool. However, its usefulness and its role are often misunderstood. Hadoop does not replace the Datawarehouse. Hadoop is not originally made to perform interactive query but large efficient batch processing. Hadoop is not intended to deliver reporting to end-users below the millisecond. Hadoop is not made for real-time stream processing. It is for this reason that distributions such as Hortonworks expended themselves with various other projects like HBase, Solr or Storm. To understand Hadoop, we have to pay attention to user-patterns and data access. What do we need? Perform full scans of my data to compute aggregation, indicators -> MapReduce, Hive, Pig. Store large amounts of data in a format that can instantly query a specific object -> HBase Process data streams with minimal latencies and large volumes -> Storm Analyze or index text documents -> Solr, ElasticSearch Train predictive models by learning -> Mahout, H20 In fact, this ecosystem draws its power from its capacity to make atomic data and distribute across several machines what traditional databases try to do alone: Indexing Transactions Low latency/interactive querying Full scan / aggregations computing Multi-tenancy We must not see Hadoop as a miracle solution but as a complex bundle of heterogeneous solutions targeting use cases and various access patterns: Finally, Big Data is not just about large data processing, but their exploitation through sophisticated statistical methods and particularly through machine learning! We must admit Hadoop has a lack of maturity about this subject either regarding Mahout, or R connector or Python that does not actually allow to distribute algorithms. Besides, below the Tera, or even more, you can get away with a bit more of good will without Hadoop with postgesSQL and Python Pandas! We even talk about “Small Data”… Big Data = Large Data Volumes No! We actually just saw the complexity of the problem in dealing with access pattern. Big Data = Volume, Velocity, Variety? That is better, but the most interesting part dwells in the conjunction between the analytic and the abundance of data that we have at our disposal. Big Data must not be an IT topic, but targeting use-cases and business issue. Too many companies take on Big Data initiative by setting up a Hadoop cluster. Besides, three skills are usually needed to carry out a datascience project and they are not limited to IT: Big data = unstructured data We talk a lot about unstructured data and the ability of Big Data technologies. To be honest, let us be clear on the fact that except for companies specialized in the media, most of the data is structured (tables, fields, columns, rows, dates). Within the leftovers, can be attributed much of the free text. To finish? A minority of videos, pictures, sound files. These data are less structured but still possess formats, more or less unified. It would actually be better to speak of multi-structure. Big Data does not have smart ways to deal with this heterogeneity except to say: “load first, model second”. Thus, model the data after loading, if necessary. Besides, the marketing people are targeting this heterogeneity of formats issue for years. However, tools related to search allow to greatly simplify the use of poorly structured data and methods for process these formats which have been unified (Apache Tika is an example). Finally, there is a floor to exceed below which it is very complex to analyze at low cost video, data, sound, picture or free text. Big data = analyze social networks and perform feeling analysis Analyze social networks data can be very interesting but remains complex and often inapplicable/uninteresting for a large number of business. The main use remaining being analyzing the e-reputation. Companies’ internal data are already often an untapped gold mine that just satisfy itself with traditional Business Intelligence. A first step would be to transcend multidimensional reporting and reach predictive analysis or unsupervised to detect weak signals. Yes, this mood paper is full of bias. However, Big Data is a subject in full “Hype” and for which we must be really suspicious towards preachers of good words, gurus and bargainers. Some myths affect in a bad way projects lifecycle, leading towards a negative and erroneous vision. The potential economic benefit related to Big Data has to be studied with care, and driven by the business vision. The investment must be directed and designed more broadly through companies’ digital development to meet specific business needs. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Big Data and tagged Hadoop . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-08-03"},
{"website": "Octo", "title": "\n                How to design a REST API            ", "author": ["Antoine Chantalou", "Benoit Lafontaine", "Mohamed Kissa", "Florent Jaby", "Jérémy Buisson", "Augustin Grimprel", "Nicolas Laurent"], "link": "https://blog.octo.com/en/design-a-rest-api/", "abstract": "How to design a REST API Publication date 01/12/2014 by Antoine Chantalou , Benoit Lafontaine , Mohamed Kissa , Florent Jaby , Jérémy Buisson , Augustin Grimprel , Nicolas Laurent Tweet Share 0 +1 LinkedIn 0 We propose a “ Quick Reference Card ”, as a summary of best practices in REST API design. ➡ Download API Design – Quick Reference Card Abstract Main concepts KISS – « Keep it simple, stupid » cURL examples Granularity API domain names Security URIs Names > Verbs Plural > Singular Case consistency URI case Body case Versioning CRUD Partial responses Query strings Paging Filtering Sorting Searching Searching for resources Global search Miscellaneous key concepts Content Negotiation Cross-domain CORS Jsonp HATEOAS “Non-Resources” scenarios HTTP ERRORS Abstract As soon as we start working on an API, design issues arise. A robust and strong design is a key factor for API success. A poorly designed API will indeed lead to misuse or – even worse – no use at all by its intended clients: application developers . Creating and providing a state of the art API requires taking into account: RESTful API principles as described in the literature (Roy Fielding, Leonard Richardson, Martin Fowler, HTTP specification…) The API practices of the Web Giants Nowadays, two opposing approaches are seen. “ Purists” insist upon following REST principles without compromise. “ Pragmatics” prefer a more practical approach, to provide their clients with a more usable API. The proper solution often lies in between. Designing a REST API raises questions and issues for which there is no universal answer. REST best practices are still being debated and consolidated, which is what makes this job fascinating. To facilitate and accelerate the design and development of your APIs, we share our vision and beliefs with you in this article. They come from our direct experience on API projects. DISCLAIMER : This article is a collection of best practices meant to be discussed. We invite you to discuss and challenge them on our blog. Main concepts KISS – « Keep it simple, stupid » One of the objectives of an API strategy is to reach out to as many developers as possible, opening one’s system to the Internet. It is therefore critical that the API be self-describing and as simple as possible, so that developers barely need to refer to the documentation. We refer to this as affordance : the API suggests its own usage. When designing an API, the following principles should be kept in mind: The API semantics must be intuitive. URI, payload, request or response: a developer should be able to use them without referring to the API documentation. The terms must be common and concrete , rather than emanate from a functional or technical jargon. Customers, orders, addresses, products are all good examples. There should not be different ways to achieve the same action. ➡ Which “Blender” is the simplest ? The API is designed for its clients , the developers, and should not be a simple access layer above the domain model. The API must provide simple features that fit developers requirements. A common mistake is to base the design of an API on an existing data model, which is usually too complex. ➡ Which “Blender” is the simplest ? In the early design phase, focus on the main use-cases and leave exceptional ones for later phases. cURL examples cURL examples are widely used to illustrate API calls: the Web Giants do it, as does technical literature in general. https://developer.github.com/v3/ https://developers.google.com/youtube/v3/live/authentication#client-side-apps https://developer.paypal.com/docs/api/ https://developers.facebook.com/docs/graph-api/making-multiple-requests https://www.dropbox.com/developers/blog/45/using-oauth-20-with-the-core-api http://instagram.com/developer/endpoints/likes/ http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AESDG-chapter-instancedata.html … We recommend always illustrating your API call documentation by cURL examples. Readers can simply cut-and-paste them, and they remove any ambiguity regarding call details. ➡ Example CURL –X POST \\\r\n-H \"Accept: application/json\" \\\r\n-d '{\"state\":\"running\"}' \\\r\nhttps://api.fakecompany.com/v1/clients/007/orders Average granularity The “one resource = one URL” theory tends to increase the number of resources. It’s important to keep a reasonable limit. For instance : a person contains an address that in turn contains a country. It’s important to avoid making 3 API calls… CURL https://api.fakecompany.com/v1/users/1234\r\n< 200 OK\r\n< {\"id\":\"1234\", \"name\":\"Antoine Jaby\", \"address\":\"https://api.fakecompany.com/v1/addresses/4567\"}CURL https://api.fakecompany.com/addresses/4567\r\n< 200 OK\r\n< {\"id\":\"4567\", \"street\":\"sunset bd\", \"country\": \"http://api.fakecompany.com/v1/countries/98\"}CURL https://api.fakecompany.com/v1/countries/98\r\n< 200 OK\r\n< {\"id\":\"98\", \"name\":\"France\"} …especially since these 3 pieces of information are commonly used together. This could lead to performance issues. On the other hand, accumulating too much information a priori is likely to make API calls and exchanges too verbose. Designing an API with an optimal granularity is not straightforward. It is often cultural and is the result of past API design experiences. In doubt, try to avoid exchanges becoming too big or too specific . Pragmatically, we recommend: Grouping only resources that are almost always accessed together Not embedding collections having many components. For example, a list of current jobs is limited (it’s difficult to have more than 2 or 3 jobs at the same time) but a list of past work experiences can be much longer. Having at most 2 levels of nested objects (e.g. /v1/users/addresses/countries) API domain names In terms of domain names, All Web Giants don’t have the same practices. Some of them, such as Dropbox , use several domains or subdomains for their APIs. ➡ Below are a few Web Giants examples API Domain / Subdomain URI Google https://accounts.google.com https://www.googleapis.com https://developers.google.com https://accounts.google.com/o/oauth2/auth https://www.googleapis.com/oauth2/v1/tokeninfo https://www.googleapis.com/calendar/v3/ https://www.googleapis.com/drive/v2 https://maps.googleapis.com/maps/api/js?v=3.exp https://www.googleapis.com/plus/v1/ https://www.googleapis.com/youtube/v3/ https://developers.google.com Facebook https://www.facebook.com https://graph.facebook.com https://developers.facebook.com https://www.facebook.com/dialog/oauth https://graph.facebook.com/me https://graph.facebook.com/v2.0/{achievement-id} https://graph.facebook.com/v2.0/{comment-id} https://graph.facebook.com/act_{ad_account_id}/adgroups https://developers.facebook.com Twitter https://api.twitter.com https://stream.twitter.com https://dev.twitter.com https://api.twitter.com/oauth/authorize https://api.twitter.com/1.1/statuses/show.json https://stream.twitter.com/1.1/statuses/sample.json https://dev.twitter.com GitHub https://github.com https://api.github.com https://developer.github.com https://github.com/login/oauth/authorize https://api.github.com/repos/octocat/Hello-World/git/commits/7638417db6d59f3c431d3e1f261cc637155684cd https://developer.github.com Dropbox https://www.dropbox.com https://api.dropbox.com https://api-content.dropbox.com https://api-notify.dropbox.com https://www.dropbox.com/1/oauth2/authorize https://api.dropbox.com/1/account/info https://api-content.dropbox.com/1/files/auto/ https://api-notify.dropbox.com/1/longpoll_delta https://api-content.dropbox.com/1/thumbnails/auto/ https://www.dropbox.com/developers Instagram https://api.instagram.com http://instagram.com https://api.instagram.com/oauth/authorize/ https://api.instagram.com/v1/media/popular http://instagram.com/developer/ Foursquare https://foursquare.com https://api.foursquare.com https://developer.foursquare.com https://foursquare.com/oauth2/authenticate https://api.foursquare.com/v2/venues/40a55d80f964a52020f31ee3 https://developer.foursquare.com To normalize domain names while keeping affordance in mind, we recommend using only 3 subdomains for your production environment : API – https://api.{fakecompany}.com OAuth2 – https://oauth2.{fakecompany}.com Developer portal – https://developers.{fakecompany}.com This subdivision is intended to help developers easily figure out how to: Effectively consume the API Get an OAuth2 token to consume the API Access the API developer portal Some Web Giants, like Paypal, also provide a sandbox environment, which is very useful for testing the API before using it live on the production environment: https://developer.paypal.com/docs/api/ We recommend using 2 subdomains for your sandbox environment : OAuth2 – https://oauth2.sandbox.{fakecompany}.com API – https://api.sandbox. { fakecompany}.com Security Two protocols are widely used to secure REST APIs: OAuth1 : http://tools.ietf.org/html/rfc5849 OAuth2 : http://tools.ietf.org/html/rfc6749 ➡ What about the Web Giants? OAuth1 OAuth2 Twitter, Yahoo, flickr, tumblr, Netflix, myspace, evernote,… Google, Facebook, Dropbox, GitHub, amazon, Intagram, LinkedIn, foursquare, salesforce, viadeo, Deezer, Paypal, Stripe, huddle, boc, Basecamp, bitly,… MasterCard, CA-Store, OpenBankProject, intuit,… AXA Banque, Bouygues telecom,… We recommend securing your API with OAuth2. Unlike OAuth1, OAuth2 allows you to manage authentication and resource authorization for any type of application (native mobile app, native tablet app, JavaScript app, server side web app, batch processing…) with or without the resource owner’s consent. OAuth2 is the de facto standard for securing APIs. Using another technology would slow down your API development and adoption. Finally, resource security is a complex problem, and a homemade solution would most certainly result in security flaws. With regard to OAuth2 token validation, we recommend implementing Google’s solution, implicit grant flow : https://developers.google.com/accounts/docs/OAuth2UserAgent#validatetoken http://en.wikipedia.org/wiki/Confused_deputy_problem We recommend always using HTTPS wwhen communicating with : OAuth2 providers API providers To validate your OAuth2 implementation, you might want to try the following test: Develop a client consuming your OAuth2 implementation and make a call to your API Then, replace the domain names of your API with Google’s API domain names. If it works, you’re good to go ! URIs Names > verbs To describe your resources, we recommend you use concrete names and not action verbs. For decades, computer scientists used action verbs in order to expose services in an RPC way, for instance: getClient(1) createClient(1) updateAccountBalance(1) addProductToOrder(1) deleteAddress(1) By contrast, the RESTful approach is to use: GET /clients/1 POST /clients PATCH /accounts/1 PUT /orders/1 DELETE /addresses/1 Using HTTP as the application protocol is one of the core goals of a REST API. It adds consistency and facilitates interactions between information systems. It also keeps us from reinventing the wheel with a homemade SOAP/RPC/EJB-like protocol. It is acknowledged that HTTP verbs be used to describe what actions are performed on resources (see topic CRUD ). the use of these HTTP verbs makes an API more intuitive and helps developers understand how to manipulate resources without having to look at verbose documentation, therefore enhancing the API’s affordance. In practice, developers tools also help them generate HTTP requests with appropriate verbs and payloads based on an up to date object model. Plural > singular Most of the time, Web Giants have a consistent behavior with regard to resource names being singular or plural. Indeed, the main concern is not to mix them: having resource names vary between singular and plural reduces the “browsability” of the API. Resource names seem more natural to us when they are set to plural, in order to address collections and instances of resources with consistency. Thus, we recommend the plural form for 2 types of resources: Resource collections: /v1/users Resource instances: /v1/users/007 As an example, for the creation of a user we will consider that POST /v1/users is the call of the create action on the users collection. Likewise, GET /v1/users/007 to retrieve a user can be understood as “I want user 007 in the users collection” Case consistency URI case When it comes to naming resources in a program, there are 3 main types of case conventions: CamelCase, snake_case, and spinal-case. They are just a way of naming the resources to resemble natural language, while avoiding spaces, apostrophes and other exotic characters. This habit is universal in programming languages where only a finite set of characters is authorized for names. CamelCase has been popularized by the Java language. It intends to emphasize the beginning of each word by making the first letter uppercase. E.g. CamelCase, CurrentUser, AddAttributeToGroup, etc. Aside from debates about its readability, its main drawback is to be ineffective in contexts which are not case sensitive. Two variants coexist: lowerCamelCase: where lowercase is used for the first letter. UpperCamelCase: where the first letter is capital. snake_case has been widely used for years by C programmers, and more recently in Ruby. Words are separated by underscores “_”, thus letting a compiler or an interpreter understand it as a single symbol, but also allowing readers to separate words fluently. However, its popularity has decreased due to a lot of abuses in C programs with over-extended or too short names. Unlike camel case, there are very few contexts where snake case is not usable. Examples: snake_case, current_user, add_attribute_to_group, etc. spinal-case is a variant of snake case which uses hyphens “-” to separate words. The pros and cons are quite similar to those of snake case, with the exception that some languages do not allow hyphens in symbol names (for variable, class, or function naming). You may find it referred to as lisp-case because it is the usual way to name variables and functions in lisp dialects. It is also the traditional way of naming folders and files in UNIX and Linux systems. Examples: spinal-case, current-user, add-attribute-to-group, etc. These 3 cases have their variants, based on criteria such as first letter case, behavior with accents or other special characters. Using English is recommended, to avoid special characters. According to RFC3986 , URLs are “case sensitive” (except for the scheme and the host ). In practice, though, a sensitive case may create dysfunctions with APIs hosted on a Windows system. Here is a digest of the Web Giants’ practices: Google Facebook Twitter Paypal Amazon dropbox github snake_case x x x x x spinal-case x x camelCase x Regarding the URIs, we recommend choosing a consistent case convention between: spinal-case (which is highlighted by RFC3986 ) and snake_case (commonly used by the Web Giants) ➡ Examples POST /v1/specific-orders or POST /v1/specific_orders Body case There are two main formats regarding the data body. On the first hand, the snake_case is used noticeably more by Web Giants, in particular it has been adopted by OAuth2 specifications. On the other hand, the growing popularity of the JavaScript language contributes to the camelCase adoption, even if theoretically , REST should remain language independent and expose a state-of-the-art API over XML. We advise you to use a consistent case for the body, to be chosen between: snake_case (frequently used by the Ruby community) lowerCamelCase (frequently used by the Java and JavaScript communities) ➡ Examples GET  /orders?id_client=007         or  GET /orders?idClient=007\r\nPOST /orders {\"id_client\":\"007\"}   or  POST /orders {\"idClient\":\"007”} Versioning Any API will have to evolve over time. There are several ways of versioning an API: With a timestamp , a release number… In the path , at the beginning or at the end of the URI As a parameter of the request In a HTTP Header With an optional or mandatory versioning. ➡ In practice by the Web Giants: API Versioning Google URI path or parameter https://www.googleapis.com/oauth2/v1/tokeninfo https://www.googleapis.com/calendar/v3/ https://www.googleapis.com/drive/v2 https://maps.googleapis.com/maps/api/js?v=3.exp https://www.googleapis.com/plus/v1/ https://www.googleapis.com/youtube/v3/ Facebook URI (optional) https://graph.facebook.com/v2.0/{achievement-id } https://graph.facebook.com/v2.0/{comment-id} Twitter https://api.twitter.com/1.1/statuses/show.json https://stream.twitter.com/1.1/statuses/sample.json GitHub Accept Header (optional) Accept: application/vnd.github.v3+json Dropbox URI https://www.dropbox.com/1/oauth2/authorize https://api.dropbox.com/1/account/info https://api-content.dropbox.com/1/files/auto https://api-notify.dropbox.com/1/longpoll_delta https://api-content.dropbox.com/1/thumbnails/auto Instagram URI https://api.instagram.com/v1/media/popular Foursquare URI https://api.foursquare.com/v2/venues/40a55d80f964a52020f31ee3 LinkedIn URI http://api.linkedin.com/v1/people/ Netflix URI, optionel parameter http://api.netflix.com/catalog/titles/series/70023522?v=1.5 Paypal URI https://api.sandbox.paypal.com/v1/payments/payment We recommend including a compulsory one digit version at the highest level of the URI’s path . The version number refers to a major release of the API as to a resource. REST and JSON, compared to SOAP/XML, give us a lot of flexibility to develop the API without impacting all the clients. For example, adding attributes to an existing resource does not imply incrementing the API version number. Default versioning shall be forbidden. Indeed, in case of changes in the API, the developers would not have the control over the impacts on calling applications. The API version number is a key piece of information. Thus, considering affordance, we would rather have it appear in the URL than in the HTTP header. We recommend supporting at most 2 versions at the same time (the hype cycle of native applications is often longer) ➡ Example GET /v1/orders CRUD As stated earlier, one of the key objectives of the REST approach is using HTTP as an application protocol in order to avoid shaping a homemade API. Hence, we should systematically use HTTP verbs to describe what actions are performed on the resources and facilitate the developer’s work handling recurrent CRUD operations. The following table synthesizes the best practices usually observed: HTTP Verb CRUD action Collection : /orders Instance : /orders/{id} GET READ Read a list of orders. 200 OK. Read the detail of a single order. 200 OK. POST CREATE Create a new order. 201 Created. – PUT UPDATE/CREATE – Full Update. 200 OK. Create a specific order. 201 Created. PATCH UPDATE – Partial Update. 200 OK. DELETE DELETE – Delete order. 200 OK. The HTTP verb POST is used to create an instance within a collection. The id of the resource to be created does not need to be provided. CURL –X POST \\\r\n-H \"Accept: application/json\" \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\"state\":\"running\",\"id_client\":\"007\"}' \\\r\nhttps://api.fakecompany.com/v1/clients/007/orders\r\n< 201 Created\r\n< Location: https://api.fakecompany.com/orders/1234 The return code is 201 rather than 200. The resource URI and id are sent back in the header “Location” of the response. If the resource id is specified by the client, the HTTP verb PUT is used for the creation of an instance within the collection. However, in practice this use case is less frequent. CURL –X PUT \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\"state\":\"running\",\"id_client\":\"007\"}' \\\r\nhttps://api.fakecompany.com/v1/clients/007/orders/1234\r\n< 201 Created The HTTP verb PUT is consistently used in order to do a full update of an instance in the collection (all attributes are replaced and those which do not exist are deleted). In the example below, we update the attributes state and id_client. All the other fields will be deleted. CURL –X PUT \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\"state\":\"paid\",\"id_client\":\"007\"}' \\\r\nhttps://api.fakecompany.com/v1/clients/007/orders/1234\r\n< 200 OK The HTTP verb PATCH (which was missing in the initial HTTP specifications and was added later on) is commonly used for partial update of an instance within a collection. In the following example, we update the state attribute but the other attributes are left untouched. CURL –X PATCH \\\r\n-H \"Content-Type: application/json\" \\\r\n-d '{\"state\":\"paid\"}' \\\r\nhttps://api.fakecompany.com/v1/clients/007/orders/1234\r\n< 200 OK The HTTP verb GET is used to read a collection. In practice, the API generally does not return all the collection items (see section Pagi ng). CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/clients/007/orders\r\n< 200 OK\r\n< [{\"id\":\"1234\", \"state\":\"paid\"}, {\"id\":\"5678\", \"state\":\"running\"}] The HTTP verb GET is used to read an instance in a collection. CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/clients/007/orders/1234\r\n< 200 OK\r\n< {\"id\":\"1234\", \"state\":\"paid\"} Partial answers Partial answers allow clients to retrieve only the information they need. This feature is vital in mobile contexts (UMTS-) where bandwidth usage must be optimized. ➡ At the Web Giants’: API Partial responses Google ?fields=url,object(content,attachments/url) Facebook &fields=likes,checkins,products LinkedIn https://api.linkedin.com/v1/people/~:(id,first-name,last-name,industry) We recommend at least being able to select the attributes to be retrieved, over 1 level of resource, through the Google notation fields=attribute1,attributeN: GET /clients/007?fields=firstname,name\r\n200 OK\r\n{\r\n\"id\":\"007\",\r\n\"firstname\":\"James\",\r\n\"name\":\"Bond\"\r\n} In contexts where performance is a strong concern, we propose using the Google notation fields=objects(attribute1,attributeN). As an example, if we want to retrieve only the first name, last name, and the street of a client’s address: GET /clients/007?fields=firstname,name,address(street)\r\n200 OK\r\n{\r\n\"id\":\"007\",\r\n\"firstname\":\"James\",\r\n\"name\":\"Bond\",\r\n\"address\":{\"street\":\"Horsen Ferry Road\"}\r\n} Query strings Paging It is necessary to anticipate the paging of your resources in the early design phase of your API. It is indeed difficult to foresee precisely the progression of the amount of data that will be returned. Therefore, we recommend paginating your resources with default values when they are not provided by the calling client, for example with a range of values [0-25]. Systematic pagination also brings cohesion to your resources, which is good. Bear in mind the affordance principle: the less documentation developers have to read, the easier their appropriation. ➡ At the Web Giants’: API Pagination Faceboo k Parameters : before, after, limit, next, previews \"paging\": {\r\n\"cursors\": {\r\n\"after\": \"MTAxNTExOTQ1MjAwNzI5NDE=\",\r\n\"before\": \"NDMyNzQyODI3OTQw\"\r\n},\r\n\"previous\": \"https://graph.facebook.com/me/albums?limit=25&before=NDMyNzQyODI3OTQw\"\r\n\"next\": \"https://graph.facebook.com/me/albums?limit=25&after=MTAxNTExOTQ1MjAwNzI5NDE=\"\r\n} Google Parameters : maxResults, pageToken \"nextPageToken\":\"CiAKGjBpNDd2Nmp2Zml2cXRwYjBpOXA\", Twitter Parameters : since_id, max_id, count \"next_results\": \"?max_id=249279667666817023&q=*freebandnames&count=4&include_entities=1&result_type=mixed\",\r\n\"count\": 4,\r\n\"completed_in\": 0.035,\r\n\"since_id_str\": \"24012619984051000\",\r\n\"query\": \"*freebandnames\",\r\n\"max_id_str\": \"250126199840518145\" GitHub Parameters : page, per_page Link: <https://api.github.com/user/repos?page=3&per_page=100>; rel=\"next\",\r\n<https://api.github.com/user/repos?page=50&per_page=100>; rel=\"last\" Paypal Parameters : start_id, count {'count': 1,'next_id': 'PAY-5TU010975T094876HKKDU7MZ', Various paging mechanisms are used by the Web Giants. As no shared standard seems to emerge, we propose using: the request parameter ?range=0-25 and the HTTP standard Header for the answer : Content-Range Accept-Range ➡ Pagination in the request From a practical point of view, pagination is often managed in the URL through the query-string . HTTP headers also provide this mechanism. We propose only accepting the query-string way, and not taking into account the Range HTTP Header. Pagination is an important piece of information, it makes sense to have it in the request for the sake of affordance. We propose that you use a range of values through your collection’s resources index. As an example, resources from index 10 to 25 included is equivalent to ?range=10-25. ➡ Pagination in the answer The HTTP code returned by a paginated request will be 206 Partial Content , except if the requested values cause the return of the whole collection’s data, in which case the return code will be 200 OK . Your API response for a collection must provide in the HTTP Headers: Content-Range offset – limit / count offset : Index of the first element returned by the request. limit : index of the last element returned by the request count : total number of elements in the collection Accept-Range resource max resource : the type of pagination. Must remind of the resource in use, e.g: client, order, restaurant, … max : maximum number of elements that can be returned in a single request. In the event of a requested pagination not fitting the values permitted by the API, the HTTP answer should be a 400 error code with an explicit description of the error in the body. ➡ Navigation links It is highly recommended to include the Link tag in the HTTP Header of your answers. It allows you to add, amongst others, navigation links such as next page, previous page, first and last page… ➡ Examples We have in our API a collection of 48 restaurants, for which you can only request 50 elements at a time. The default pagination is 0-50: CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/restaurants\r\n< 200 Ok\r\n< Content-Range: 0-47/48\r\n< Accept-Range: restaurant 50\r\n< [...] If 25 resources are requested of the 48 available, we receive a 206 Partial Content return code: CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/restaurants?range=0-24\r\n< 206 Partial Content\r\n< Content-Range: 0-24/48\r\n< Accept-Range: restaurant 50 If 50 resources are requested of the 48 available, a 200 OK return code is returned: CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/restaurants?range=0-50\r\n< 200 Ok\r\n< Content-Range: 0-47/48\r\n< Accept-Range: restaurant 50 If the requested range is greater than the maximum number of resources for a single request ( Header Accept-Range ), a 400 Bad Request code is returned: CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/orders?range=0-50\r\n< 400 Bad Request\r\n< Accept-Range: order 10\r\n< { reason : \"Requested range not allowed\" } We recommend using the following notation for returning links to other ranges. It is used by GitHub and is compatible with the RFC5988 . It also allows to manage clients which do not support several Link Headers. CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/orders?range=48-55\r\n< 206 Partial Content\r\n< Content-Range: 48-55/971\r\n< Accept-Range: order 10\r\n< Link : <https://api.fakecompany.com/v1/orders?range=0-7>; rel=\"first\", <https://api.fakecompany.com/v1/orders?range=40-47>; rel=\"prev\", <https://api.fakecompany.com/v1/orders?range=56-64>; rel=\"next\", <https://api.fakecompany.com/v1/orders?range=968-975>; rel=\"last\" Another notation is frequently encountered, with a HTTP Header tag Link containing an URL followed by the type of the link. This tag can be repeated as many times as there are links associated with the answer: < Link: <https:<i>//api.fakecompany.com/v1/orders?range=0-7>; rel=\"first\"</i>\r\n< Link: <https://api.fakecompany.com/v1/orders?range=40-47>; rel=\"prev\"\r\n< Link: <https://api.fakecompany.com/v1/orders?range=56-64>; rel=\"next\"\r\n< Link: <https://api.fakecompany.com/v1/orders?range=968-975>; rel=\"last\" Or the following notation in the payload, which is used by Paypal : [\r\n{\"href\":\"https://api.fakecompany.com/v1/orders?range=0-7\", \"rel\":\"first\", \"method\":\"GET\"},\r\n{\"href\":\"https://api.fakecompany.com/v1/orders?range=40-47\", \"rel\":\"prev\", \"method\":\"GET\"},\r\n{\"href\":\"https://api.fakecompany.com/v1/orders?range=56-64\", \"rel\":\"next\", \"method\":\"GET\"},\r\n{\"href\":\"https://api.fakecompany.com/v1/orders?range=968-975\", \"rel\":\"last\", \"method\":\"GET\"},\r\n] Filtering Filtering consists in restricting the number of queried resources by specifying some attributes and their expected values. It is possible to filter a collection on several attributes at the same time, and to allow several values for one filtered attribute. We propose to use directly the attribute’s name with an equal sign and the expected values, each of them separated by a comma. Example : retrieve thai food restaurants CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/restaurants?type=thai Example: retrieve restaurants with a 4 or 5 rating , proposing Chinese or Japanese food and open on sundays CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/restaurants?type=japanese,chinese&rating=4,5&days=sunday Sorting Sorting the result of a query on a collection of resources requires two main parameters: sort : Contains the names of the attributes on which the sorting is performed, separated by a comma. desc : By default, the sorting is done in ascending order. If one wishes to sort in descending order, they need to add this parameter (without any value). In some specific cases, one may want to specify which attributes should be used as ascending sort keys and which as descending sort keys. Then, the desc parameter should contain the attributes that will be descending sort keys, the others will be ascending sort keys. Example: retrieving the list of restaurants sorted by name CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/restaurants?sort=name Example: retrieving the list of restaurants, sorted by descending rating, then by ascending review count, and finally by ascending name. CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/restaurants?sort=rating,reviews,name&desc=rating,reviews ➡ Sorting, Filtering and Paging Paging is very likely to be impacted by sorting and filtering. The combination of these 3 parameters should be usable with consistency in the requests to your API Example: Request of the first five Chinese restaurants sorted by descending rating. CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/restaurants?type=chinese&sort=rating,name&desc=rating&range=0-4\r\n< 206 Partial Content\r\n< Content-Range: 0-4/12\r\n< Accept-Range: restaurants 50 Searching Searching resources If filtering does not fit our needs (to make partial or approximate matches, for instance), we need the ability to search the available resources. A search is a sub-resource of our collection. As such, its results will have a different format than the resources and the collection itself. This allows us to add suggestions, corrections and information related to the search. Parameters are provided the same way as for a filter, through the query-string, but they are not necessarily exact values, and their syntax permits approximate matching. Being itself a resource, the search must support paging like all the other resources of your API. Example: searching for restaurants whose names start with “La” . CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/restaurants/search?name=la*\r\n< 206 Partial Content\r\n< { \"count\" : 5, \"query\" : \"name=la*\", \"suggestions\" : [\"las\"], results : [...] } Example: Searching for the first 10 restaurants with a name containing “Napoli”, which cook Chinese or Japanese food, located in Paris (zip code 75), sorted by descending rating and name. CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\n-H \"Range 0-9\" \\\r\nhttps://api.fakecompany.com/v1/restaurants/search?name=*napoli*&type=chinese,japanese&zipcode=75*&sort=rating,name&desc=rating&range=0-9\r\n< 206 Partial Content\r\n< Content-Range: 0-9/18\r\n< Accept-Range: search 20\r\n< { \"count\" : 18, \"range\": \"0-9\", \"query\" : \"name=*napoli*&type=chinese,japanese&zipcode=75*\", \"suggestions\" : [\"napolitano\", \"napolitain\"], results : [...] } Global search Global search should have the same behavior as resource-specific search, except that it is located at the root of the API and therefore must be pointed out in the documentation. We recommend Google’s notation for global searches: CURL –X GET \\\r\n-H \"Accept: application/json\" \\\r\nhttps://api.fakecompany.com/v1/search?q=running+paid\r\n< [...] Other key concepts Content negotiation We recommend handling several content distribution formats. We can use the HTTP Header dedicated to this purpose: “Accept”. By default, the API will share resources in the JSON format, but if the request begins with “Accept: application/xml”, resources should be sent in the XML format. It is recommended to manage at least 2 formats: JSON and XML. The order of the formats queried by the header “Accept” must be observed to define the response format. In cases where it is not possible to supply the required format, a 406 HTTP Error Code is sent (cf. Errors — Status Codes). GET https://api.fakecompany.com/v1/offers\r\nAccept: application/xml; application/json XML préféré à JSON\r\n< 200 OK\r\n< [XML]GET https://api.fakecompany.com/v1/offers\r\nAccept: text/plain; application/json The API cannot provide text\r\n< 200 OK\r\n< [JSON] Cross-domain CORS When the application (JavaScript SPA) and the API are hosted on different domains, for example: https://fakeapp.com https://api.fakecompany.com A good practice consists in using the CORS protocol which is the HTTP standard. On the server side, CORS implementation usually consists in adding a few instructions in HTTP servers (Nginx/Apache/NodeJs…). On the client side, implementation is imperceptible: the browser will send a HTTP request with the OPTIONS verb before any GET/POST/PUT/PATCH/DELETE request. Here follows an example of two successive calls made by a browser in order to retrieve, through GET, information of a user with the Google+ API: CURL -X OPTIONS \\\r\n-H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' \\\r\n'https://www.googleapis.com/plus/v1/people/105883339188350220174?client_id=API_KEY' CURL -X GET\\\r\n'https://www.googleapis.com/plus/v1/people/105883339188350220174?client_id=API_KEY' \\\r\n-H 'Accept: application/json, text/JavaScript, */*; q=0.01'\\\r\n-H 'Authorization: Bearer foo_access_token' Jsonp In fact, CORS is either badly or not at all supported by old browsers, especially IE7, 8, and 9. If your API is to be used by browsers which you do not control (on the Internet, with customers), it is still necessary to propose a Jsonp exposition of your API as a fallback of the CORS implementation. Indeed, Jsonp is a work-around of the tag usage aimed at allowing the cross-domain management: It is impossible to use content negotiation through the Accept Header => a new endpoint must be published, for example with a .jsonp extension, so that the controller is able to determine that it is a jsonp request. All requests are sent with the GET HTTP verb => a parameter method=XXX must be proposed Keep in mind that a web crawler could cause serious damage to your data if there is no authorization mechanism on a method=DELETE call… Request’s payload cannot contain data => all the data must be sent as request parameters To be CORS & Jsonp compliant, as an example, your API should expose the following endpoints: POST /orders       and  /orders.jsonp?method=POST&callback=foo\r\nGET  /orders       and  /orders.jsonp?callback=foo\r\nGET  /orders/1234  and  /orders/1234.jsonp?callback=foo\r\nPUT  /orders/1234  and  /orders/1234.jsonp?method=PUT&callback=foo HATEOAS Concept Let’s take Angelina Jolie as an example. Angelina is an Amazon customer, and she wishes to read the details of her last order. To do this, she has two steps to follow: List all her orders Select her last order On the Amazon website , Angelina does not need to be a web expert to read her last order: she just has to log-in into her account, then click on the “my orders” link and finally select the most recent one. Now let’s imagine Angelina wishes to use an API to do the same thing! She must begin by reading Amazon documentation to find the URL that returns her list of orders. When she finds it, she must make an actual HTTP call to this URL. She’ll see the reference of her order in the list, but she’ll need to make a second call to another URL to get its details. Angelina will have to figure out how to construct the proper URL from Amazon ‘s documentation. There is one main difference between these two scenarii: In the first one, Angelina just needed to know the first URL “ http://www.amazon.com ” then follow the links on the web page. Whereas in the second one, Angelina needed to read the documentation so as to elaborate the URL. The drawbacks of the second process are: In real life, the documentation is usually not up to date. Angelina may miss one or several available services just because they are not properly documented. Angelina is likely to be a developer, and developers do not like documentation. The API is less accessible Let’s assume Angelina develops a component to automatically create these contextual URLs. What happens when Amazon modifies its base URLs? Implementation In practical, HATEOAS is like a urban legend. Everybody talks about it but nobody ever witnessed an actual implementation. Paypal proposes one: [\r\n{\r\n\"href\": \"https://api.sandbox.paypal.com/v1/payments/payment/PAY-6RV70583SB702805EKEYSZ6Y\",\r\n\"rel\": \"self\",\r\n\"method\": \"GET\"\r\n},\r\n{\r\n\"href\": \"https://www.sandbox.paypal.com/webscr?cmd=_express-checkout&token=EC-60U79048BN7719609\",\r\n\"rel\": \"approval_url\",\r\n\"method\": \"REDIRECT\"\r\n},\r\n{\r\n\"href\": \"https://api.sandbox.paypal.com/v1/payments/payment/PAY-6RV70583SB702805EKEYSZ6Y/execute\",\r\n\"rel\": \"execute\",\r\n\"method\": \"POST\"\r\n}\r\n] A call to /customers/007 would then return the details of the customer , along with pointers towards linked resources : GET /customers/007\r\n< 200 Ok\r\n< { \"id\":\"007\", \"firstname\":\"James\",...,\r\n\"links\": [\r\n{\"rel\":\"self\",\"href\":\"https://api.domain.com/v1/customers/007\", \"method\":\"GET\"},\r\n{\"rel\":\"addresses\",\"href\":\"https://api.domain.com/v1/addresses/42\", \"method\":\"GET\"},\r\n{\"rel\":\"orders\", \"href\":\"https://api.domain.com/v1/orders/1234\", \"method\":\"GET\"},\r\n...\r\n]\r\n} For implementing HATEOAS, we therefore recomment using the following method, applied by GitHub, compliant with RFC5988 and usable by clients that don’t support several Header “Link”: GET /customers/007\r\n< 200 Ok\r\n< { \"id\":\"007\", \"firstname\":\"James\",...}\r\n< Link : <https://api.fakecompany.com/v1/customers>; rel=\"self\"; method:\"GET\",\r\n<  <https://api.fakecompany.com/v1/addresses/42>; rel=\"addresses\"; method:\"GET\",\r\n<  <https://api.fakecompany.com/v1/orders/1234>; rel=\"orders\"; method:\"GET\" “Non Resource” scenarios In RESTFul theory, any request must be seen and manipulated as a resource. In real life, it’s not always possible, especially when we have to deal with actions such as translations, computations, conversions, complex business services or strongly integrated services. In these cases, your operation must be represented by a verb rather than a name. For instance : POST /calculator/sum\r\n[1,2,3,5,8,13,21]\r\n< 200 OK\r\n< {\"result\" : \"53\"} Or else : POST /convert?from=EUR&to=USD&amount=42\r\n< 200 OK\r\n< {\"result\" : \"54\"} We therefore come to use actions instead of resources . In this context, we will use the HTTP POST method. CURL –X POST \\\r\n-H \"Content-Type: application/json\" \\\r\nhttps://api.fakecompany.com/v1/users/42/carts/7/commit\r\n< 200 OK\r\n< { \"id_cart\": \"7\",<i> [...] <i> }</i></i> To design properly this exception in your API, the simplest solution is to consider that any POST request is an action with an implicit or explicit verb. For a collection of entity resources for instance, the default action is a creation: POST /users/create                                        POST /users\r\n< 201 OK                                      ==          < 201 OK\r\n< { \"id_user\": 42 }                                       < { \"id_user\": 42 } Or for an email resource, the default action will be to send it to its recipient. POST /emails/42/send                                      POST /emails/42I\r\n< 200 OK                                      ==          < 200 OK\r\n< { \"id_email\": 42, \"state\": \"sent\" }                     < { \"id_email\": 42, \"state\": \"sent\" } However, it is important to bear in mind that explicitly specifying a verb in your API design must remain an exception. In most cases, it can and must be avoided. If several resources expose one action or more, your API design is flawed: you took an RPC approach rather than a REST approach, and need to quickly take action by going over your API design. In order to avoid any confusion in developers’ minds between resources (which you can access the CRUD way) and actions, it is highly recommended to clearly separate these two concepts in the developer documentation. ➡ Web Giants examples API “Non Resources” API Google Translate API GET https://www.googleapis.com/language/translate/v2?key=INSERT-YOUR-KEY&target=de&q=Hello%20world Google Calendar API POST https://www.googleapis.com/calendar/v3/calendars/calendarId/clear Twitter Authentication GET https://api.twitter.com/oauth/authenticate?oauth_token=Z6eEdO8MOmk394WozF5oKyuAv855l4Mlqo7hhlSLik Errors Error Structure We recommend the following JSON structure: {\r\n\"error\": \"short_description\",\r\n\"error_description\": \"longer description, human-readable,\r\n\"error_uri\": \"URI to a detailed error description on the API developer website \"\r\n} The error attribute is not necessarily redundant with the HTTP status: we may have two different values for the error key while keeping the same HTTP status. 400 & error=invalid_user 400 & error=invalid_cart This representation is taken from the OAuth2 specification. A systematic use of this syntax in the API will prevent the clients from having to manage two distinct error structures. Nota Bene : In some cases, it may be relevant to provide a collection of this structure in order to return several errors at the same time (this is useful in the case of a server-side form validation as an example). Status Codes We highly recommend using the HTTP return codes, as a code exists for every common case, which everybody understands. Of course, using the whole collection of codes is not necessary, usually the top-10 most used codes are enough. SUCCESS 200 OK is the usual success code for most cases. It is especially used when the first GET request on a resource is successful. HTTP Status Description 201 Created Indicates that a resource has been created. Typical answer to PUT and POST requests, including a HTTP Header “Location” which points toward the new resource URL. 202 Accepted The request has been accepted and will be processed later. It is a classic answer to asynchronous calls (for better UX or performances). 204 No Content The request has been successfully processed, but there is nothing to return. It is often returned to a DELETE request. 206 Partial Content The content returned is incomplete. Mostly returned by paginated answers. CLIENT ERROR HTTP Status Description 400 Bad Request Commonly used for calling errors if no other status matches. We can distinguish between two error types:Request behaviour error GET /users?payed=1\r\n< 400 Bad Request\r\n< {\"error\": \"invalid_request\", \"error_description\": \"There is no ‘payed' property on users.\"} Application condition error POST /users\r\n{\"name\":\"John Doe\"}\r\n< 400 Bad Request\r\n< {\"error\": \"invalid_user\", \"error_description\": \"A user must have an email address\"} 401 Unauthorized I do not know your id. Tell me who you are and I will check your authorizations. 1 2 3 GET /users/42/orders < 401 Unauthorized < {“error”: “no_credentials”, “error_description”: “This resource requires authorization, you must be authenticated and have the correct rights to access it” } 403 Forbidden You are identified, but you do not have the necessary authorizations. GET /users/42/orders\r\n< 403 Forbidden\r\n< {\"error\": \"not_allowed\", \"error_description\": \"You're not allowed to perform this request\"} 404 Not Found The resource you asked for does not exist. GET /users/999999/\r\n< 400 Not Found\r\n< {\"error\": \"not_found\", \"error_description\": \"The user with the id ‘999999' doesn't exist\" } 405 Method not allowed Either calling a method on this resource has no meaning, or the user is not authorized to make this call. POST /users/8000\r\n< 405 Method Not Allowed\r\n< {\"error\":\"method_does_not_make_sense\", \"error_description\":\"How would you even post a person?\"} 406 Not Acceptable Nothing matches the Accept-* Header of the request. As an example, you ask for an XML formatted resource but it is only available as JSON. GET /usersAccept: text/xmlAccept-Language: fr-fr\r\n< 406 Not Acceptable\r\n< Content-Type: application/json\r\n< {\"error\": \"not_acceptable\", \"available_languages\":[\"us-en\", \"de\", \"kr-ko\"]} SERVER ERROR HTTP StatusDescription 500 Server error This request is correct, but an execution problem has been encountered. The client cannot really do much about this. We recommend to systematically return a Status 500. GET /users\r\n< 500 Internal server error\r\n< Content-Type: application/json\r\n< {\"error\":”server_error\", \"error_description\":\"Oops! Something went wrong...\"} Sources Design Beautiful REST + JSON APIs http://www.slideshare.net/stormpath/rest-jsonapis Web API Design: Crafting Interfaces that Developers Love https://pages.apigee.com/web-api-design-website-h-ebook-registration.html HTTP API Design Guide https://github.com/interagent/http-api-design RESTful Web APIs http://shop.oreilly.com/product/0636920028468.do How are REST APIs versioned? http://www.lexicalscope.com/blog/2012/03/12/how-are-rest-apis-versioned/ REST World http://nodejsparis.bitbucket.org/20140312/rest_world/#/ // ]]>o Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged API , WOA . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 3 commentaires sur “How to design a REST API” François 05/02/2015 à 19:42 Thanks for the good summary, and especially the detailed references. \r\n\r\nAlthough based on good practices, some of your points are debatable - mostly because there is no REST specification to let everybody agree.\r\n\r\nFor instance, I think the body of a response to a POST request should not be empty. Don't force the API clients to fetch another resource to be able to display the newly created instance! \r\n\r\nI disagree with the suggested PATCH syntax. PATCH requests should not be sent with entity data in the body, but with one or several patch actions (cf http://williamdurand.fr/2014/02/14/please-do-not-patch-like-an-idiot/)\r\n\r\nAlso, you miss one important guideline in my opinion: the batch request handler, or multifetch handler. This good practice is a must for mobile clients where latency is high. It's used in production by Facebook (cf https://developers.facebook.com/docs/graph-api/making-multiple-requests).\r\n\r\nLastly, any decent REST guideline should mention the HTTP response code 418. miten mehta 13/07/2015 à 12:13 nice consolidation of various practices. Jaap 26/10/2015 à 18:57 A question about returning a 202 on a DELETE request.\r\n\r\nWhen my service receives a request to delete a resource, I don't want to immediately delete it. The reason for this is that the user (of the client application sending the delete request) may make mistakes. So I want to 'disable' my resource by putting it in a 'grace' state and return a 202 response.\r\n\r\nWhile the resource is marked for deletion (grace), PUT/PATCH commands will not be executed. If this gives problems in the overall business logic, the resource may be 'restored' to an 'active' state instead of grace. If, however, a month passes by without any problems in business logic, I want to really delete the resource.\r\n\r\nSome of my colleagues tell me this would be misuse of 202, since the intention is supposed to be asynchronous processing. Any opinions? Thanks! Comments are closed.", "date": "2014-12-01"},
{"website": "Octo", "title": "\n                DevOps, from continuous integration to continuous deployment            ", "author": ["Romain Felden", "Eric Favre", "Farhdine Boutzakhti"], "link": "https://blog.octo.com/en/devops-from-continuous-integration-to-continuous-deployment/", "abstract": "DevOps, from continuous integration to continuous deployment Publication date 19/05/2015 by Romain Felden , Eric Favre , Farhdine Boutzakhti Tweet Share 0 +1 LinkedIn 0 The first steps towards industrialized developments usually start with continuous integration. Although it is often seen as an achievement in itself, it’s actually only a piece of an efficient and managed solution. Translated from the article published in the July-August 2014 ICT Journal (french). Continuous integration, an unquestionable basis Keystone to a software factory, the continuous integration server is essential to any software development project. Whether you pick Jenkins, Bamboo or anything else, it ensures code compilation, running of unit tests,  packaging, deployment and integration tests execution. But its main perk is its independence from developers’ local settings. This guarantees unbiased quality rules matching (enforced with SonarQube, for instance) and asserts processes repeatability The packaged code is archived into an archetype repository. This archive will be deployed on each environment, including production. Continuous integration secures development activities. Once set up, it allows the project organization to push industrialization beyond the development team boundaries. Automated deployment, a must have It is now possible to gradually automate deployment actions that used to be manually executed by the operations team (database updates, deployments to an application server, etc.). Not only will this automation remove tedious tasks, but it will also secure the deployment process while increasing the shipping frequency for early user validation. These automated deployments extend from deployment scripts in simple cases to a complete setup of the destination environment (with tools such as Puppet, Chef or Docker). Just like regular code, these scrips and configurations are to be stored under version control. This step cannot be achieved without a genuine collaboration between development and operations teams. When it’s done, the deployment to integration can already be automated and triggered frequently, as long as it doesn’t affect any other team. Developers can then rapidly test their code in an environment similar to production. However, deployments are triggered manually for qualification and production environments, so as to keep stakeholders (project members, customers and decision-makers) reassured while providing a significantly shorter delay. Automated acceptance tests, an ideal To improve the time-to-market any further, execution duration of manual acceptance tests in qualification environment must be reduced. Testers who used to spend their time running these tests repeatedly can now use it to write automated tests. Thus, the quality of these tests and their coverage will reach a higher level, ensuring a better match between the application and the users’ expectations. Running automated acceptance tests requires an automated application deployment, which removes more manual steps and eases the delivery process. Continuous deployment to production, the holy grail? Nowadays, continuous integration is widely adopted and its efficiency is well established. It is a prerequisite to anyone willing to improve his development practices. Automated deployment to integration environment is a growing practice. It increases developers’ awareness of their own achievements. Acceptance tests automation is less widespread. It adresses an enhanced industrialization of the development processes and enables a shorter time-to-market. Once the development line until qualification is mastered, there is but one step towards continuous deployment to production. Only a handful of key players have crossed that line, because of the nature of their trade (Google, Twitter, Amazon, etc.). In such cases, the human impact is so big that this change is felt as the biggest one. Notable organizational obstacles remain (norms conformity, timidity, communication on new releases), and the upside may not be obvious to everyone. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-05-19"},
{"website": "Octo", "title": "\n                Present and future of system flow monitoring            ", "author": ["Erwan Alliaume", "Cédrick Lunven", "Julien Kirch"], "link": "https://blog.octo.com/en/present-and-future-of-system-flow-monitoring/", "abstract": "Present and future of system flow monitoring Publication date 25/05/2015 by Erwan Alliaume , Cédrick Lunven , Julien Kirch Tweet Share 0 +1 LinkedIn 0 Having an efficient flow monitoring is critical: integrating all data flows, it offers an overall view of the entire Information System. This article aims to help you compare your current system with generally observed good practices and provides you with suggestions to improve it. Flow monitoring? We call flow a collection of service calls and/or message sending which as a whole represents a business service . In a complex Information System, such flow often goes through multiple applications and sometimes uses several technologies. For example, a user can click on a screen that triggers a REST will eventually call which causes a SOAP process whose execution sends a bunch of JMS messages. Monitoring these flows means implementing Business Activity Monitoring . This involves collecting data in all application layers in order to correlate them. This provides an aggregated transversal overview vision of the activity of your Information System. This monitoring must at any time provide the state of health and performance of important business functions (Key Performance Indicators) . In the past, this information was often computed separately for each technical layer. The monitoring of flow does not replace the monitoring of components but completes it in the same way that integration tests complete unit tests. Each block must be monitored separately in a technical way to identify its own problems, while flow monitoring will look at transverse elements and activities that require an overview of the system. There is overlaps between the two but do not confuse them nor use one to replace the other. In the following article, a service message means either the content of a service or of a message. What’s a good flow monitoring? Features The essential feature is the ability to identify business flows in atomic messages. Typically this requires the use of a unique identifier ( correlation id ). All messages in the same stream will therefore contain the same identifier. This requires all applications to contain custom components in charge of generating the identifier string and transmitting it through the processes. These components will also be responsible for providing a copy of each message for the monitoring system. The system must be able to take into account heterogeneous events : if the messages sent by the various components have common elements (timestamp for example), they also contain specific information related to the business service (name of business service, object identifier). Being able to easily integrate these data will enable you to build more accurately functional metrics which will evolve along with the services. To make the most of this data, you must have a configurable dashboarding system : such systems are not only used for  predefining a set of fixed monitoring screens, but also for analysis or investigation. The monitoring systems usually used are often ill-suited to this type of use: their “old” ergonomics makes the investigation of data painful. Moreover, it uses monolithic solutions that integrates monitoring of data collection and storage. The database must provide functionality for indexing with maximum coverage, the ideal being to index all the data fields. This simplifies the investigations in case of errors: for example, you can identify all messages concerning a certain account number. For volume issues, we can limit the indexing time (at least 48 hours), keeping the possibility of re-indexing past messages. Constraints Never interfere with the business A failure in the monitoring components should never result in consequences to the business , so be aware to technically isolate them. Next, monitoring should not lead to loss of performance , the monitoring messages have to be sent as asynchronous events (pattern wiretap ) through a middleware of messages. The best solution is a dedicated messaging infrastructure, since this avoids any risk of overload. Limit custom developments Finally, one should limit business developments in the monitoring components : if the configuration or some custom development can’t be avoided, especially for very specific metrics, duplicating business behaviors must be avoided. The result is often fragile and will make business evolutions more difficult to achieve. Software components needed To meet these criteria, you can identify different components for monitoring flows: The information is transmitted as events in a dedicated message middleware A message processing system for aggregation and detection of events An indexed database where they are stored A monitoring console to operate them Current Practices Your Information System already has some of the technical components needed, such as message system or database. Unfortunately the specificities of monitoring often prevent the use of identical tools: Because of the large number of messages entering the system, the conventional middlewares do not provide sufficient throughput, it is therefore necessary to use a dedicated communication system for this volume: AMQP , SNMP , RSYSLOG , ZeroMQ . For message processing, a CEP (complex event processing) system such as Drool Fusion handles the technical aspects. It stores in memory a system status in which defined rules that trigger processes or alerts. The database should store messages with heterogeneous formats that evolve along with applications . One should then focus generally on NoSQL storage solution, allowing to have dynamic data patterns while providing partitioning and scalability to handle the volume and the incoming data throughput. Elastic Search is generally a good choice, thanks to its indexing functionality functionality. For dashboarding, Kibana is the reference product for viewing data stored in Elastic Search: it allows to build rich screens in a flexible way. The future This type of architecture based on standard components is limited in two aspects, technical and functional. Always more messages The first limitation is related to the increasing number of messages to be processed. In systems based on micro-services architectures and that incorporate new usages (mobile applications, Internet of Things) the number of messages is multiplied by 10, 100, 1000. As the goal of the monitoring system is to handle all the messages, it must increase its capacity by the same rate. This causes the introduction of several solutions: Even by choosing a fast broker, classic middleware solutions level off at thousands of messages per second. We must then turn to “ Fast Data ” solutions, like Kafka . The integration of messages in the storage system has to use an “ Event Streaming ” solution, like Apache Storm or Apache Spark Streaming . The storage of such volume of data will be achieved on a distributed storage system such as Apache Cassandra ou Hadoop’s HDFS . Storage in Elastic Search can be kept for quick querying on recent data. This kind of “big data” architecture allowing combination of multiple processing approaches while maintaining a unified storage is called DataLake . Further analysis Following the classical rules engines, we begin choosing more advanced analysis solutions for a better measure of what’s happening and for better predictions. In this context, the business part of monitoring is becoming more and more important and the distinction fades with classical BI. We believe that soon these systems will be improved by online machine learning solutions. Conclusion This is our vision about the value of monitoring flow. Today it is essential for operational reasons; tomorrow it will generate business value. The introduction of this kind of solutions in an Information System is a structuring project but is based on well-known and open components, so nothing should stop you from doing it Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Present and future of system flow monitoring” Diego 26/05/2015 à 07:49 Have you considered https://github.com/twitter/zipkin?\r\nBoth presentations are quite what you've shown here:\r\n\r\nhttp://www.slideshare.net/johanoskarsson/zipkin-runtime-open-house\r\nhttp://www.slideshare.net/dkuebrich/distributed-tracing-in-5-minutes Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-05-25"},
{"website": "Octo", "title": "\n                Back from DevOps Days Paris 2015            ", "author": ["Arnaud Bétrémieux"], "link": "https://blog.octo.com/en/back-from-devops-days-paris-2015/", "abstract": "Back from DevOps Days Paris 2015 Publication date 30/04/2015 by Arnaud Bétrémieux Tweet Share 0 +1 LinkedIn 0 I attended DevOps Days Pari s , for which Octo was a sponsor, on April 14 and 15. Here’s a small synthesis of the various talks I attended, and the one I presented. DevOps Days are mostly focused on the human side of DevOps , and therefore featured a lot of non-technical talks on the subject. The talks emphasized the importance of empathy and respect, of understanding peoples fears and wants, and of putting yourself in other’s shoes. There was also a strong focus on challenging our assumptions and being mindful of our biases. The Keynote was titled Containers, Germs, and Microservices , in reference to Jared Diamond’s Guns, Germs and Steel . In this talk, John Willis explained how the World is divided into haves and have-nots : those who have power, and those who don’t. Haves are created through the reduction of latency and distance that is brought about by cybernetic feedback loops between geography, civilization, agriculture, etc.At some point, a terminal velocity is reached that makes it impossible for the have-nots to compete with the haves . This was true for some indigenous people of the Americas when they encountered machine guns, it is now just as true for the non-digital companies encountering their digital competitors. Willis believes that the new haves vs. have-nots front is data. He thinks acceleration to terminal velocity in this field will be achieved through containers and micro-services, as we need to move computing to data and not the converse. Data is indeed getting too heavy to move, a phenomenon he calls “data gravity”. Data from IoT in particular is expected to come in huge quantities and therefore have a big impact on “data gravity”. In What Happens Without Traction , Steve Pereira gave ideas for introducing DevOps or bringing about similar changes in a company in the face of strong resistance. He suggests going for local changes, like automating yourself at the job, then using that as a demo for more, while always trying to have a holistic view. His preferred approach is finding gaps, hypothesizing fixes, changing something, measuring, and then sharing. He proposes a DevOps Checklist to identify gaps when it comes to DevOps . He emphasizes the need to target messages and actions on very specific aspects, with clear and measurable benefits, and avoiding possibly scary and misunderstood terms like “ DevOps “. One of his tips to overcome inertia is thinking of personas , to understand fears and what people care about, instead of always butting heads. He view a lot of the responsibility for change as residing with the workers, not the bosses, since we work in a “knowledge field”, where workers usually know more about the work than their bosses. He cautions, though, against mistaking novelty for innovation, or motion for action. Boris Feld, in his talk The importance of Why in DevOps insisted that people worry about the why of DevOps before they tackle the how, as the how is entirely contingent on the why. He thinks a lot people unfortunately worry about the how without having a clear idea of the why. In Bizdevops – from development to the customer , Sabine Bernecker-Bendixen talked again about the relational aspects of DevOps , and how they can apply as well to business-tech relationships (“business” being whoever pays the final bill). To her, it’s all about empathy, adapting your language, and translating emotions and feelings : “I’m OK, you’re OK”. She insists we always communicate status in our work, even if there’s nothing new, say what the problems are if any, so that people can emphasize. The alternative is people thinking you are incompetent or dumb. On the flip side, don’t assume that people are incompetent or dumb, put yourself in their shoes. Organizations should also use people’s diversity as an asset, putting each in the place where they can have the most meaningful impact. In DevOps Culture at BlaBlaCar – Keep CAMS and grow , Regis Allegre & Nicolas Blanc presented how DevOps work at BlaBlaCar , now that they have more than 10 million users and are experiencing exponential growth. One of the things I noted is that they don’t use cloud infrastructure, except for very specific stuff like tests and emailing. An interesting practice they have is the “Weekly free speech”, which is a whole company meeting where anyone can suggest subjects, and subjects to be discussed that day are then chosen by vote. In Cognitive biases and our poor intuitions around probability , Nigel Kersten from Puppet Labs presented ways in which our cognitive biases negatively affect DevOps and change management in general. He makes an analogy between the shortcuts developers (rightfully) take, which can introduce bugs, and our brain’s shortcuts, that can give us hard to overcome biases. The bugs introduced by developer shortcuts can be huge and incredible (his favorite example being the Xerox Copy Bug ). The biases introduced by our brain shortcuts can be just as spectacular. Kersten insists that we take steps, particularly in postmortems, to avoid these biases. Examples of known biases : More memorable events estimated as more frequent Hindsight bias, to avoid it, make predictions prior to results, review them after getting the results Attribution bias (exaggerating people’s roles in outcomes). He gives the example of bad drivers : when you’re the bad driver, there’s an external factor (you’re late, etc.), when others are bad drivers, it’s because they’re stupid In Designing the Enterprise for Manufacturing , Scott Russell outlined how manufacturing concepts could be applied to DevOps . To summarize his proposition : in IT, we’ve only been making things for a few decades, let’s learn from those who have been making things for centuries. He assimilates DevOps organizations to production lines, and sees the same kind of waste, in setup time, context switches, queuing issues… Techniques from manufacturing he proposes we apply to DevOps include : taking into account non working hours where there might be zero use -> low use percentage, look at utilization of hardware, not capacity; distinguishing productions between high volume, low mix (variability in product type) and low volume, high mix; using statistical tests instead of systematic ones : check 1 item in 5 and if there’s a failure check items produced around the failure; He quipped about “Enterprise projects” being projects that have more developers than end-users :) In Change management at scale: responsible agile delivery , Pierre-Yves Ritschard gave pointers for scaling the management of changes in production, such as opening firewall ports, while ensuring traceability, accountability and reversibility. Of course, automation helps a lot, and infrastructure as code gives traceability and reversibility to infrastructure changes. In this vein, he insists we treat machines as cattle and not pets: they should be homogeneous and managed globally. For things that can’t be automated yet, he suggests keeping a text based log of changes (in Git). Pull requests can be used for peer review. In “ Making the Elephant dance – Daily deliveries at SAP “, Dirk Lehmann explains how he managed to bring DevOps ideas into the very rigid structure of a company like SAP. While his team is essentially a startup within SAP, he was expected to work with SAP processes as a way to test and stress these processes. He managed to get to deliveries every two weeks, from an SAP standard of at most quarterly deliveries, but in then trying to get down to daily deliveries, he encountered huge resistance. “No customer wants daily releases”, he was told. He contrasts Innovation and processes, if processes are viewed as ways to ensure things are always done the way they were done before.He thinks breaking the rules a little bit is necessaries, asking for forgiveness later.Find allies, and split large processes in smaller ones so you can pick and choose the really necessary ones. Try to push trust over control. In “Screwing up for fun and profit”, Oliver Hankeln talked about mistakes in corporate environments, and the patterns and anti-patterns he sees around this topic. Patterns : transparency / trust / respect, proactive communication (professionalism implies being open about failure, and learning from postmortems) going to the root-cause accepting mistakes (postmortem is not a performance review. Focus on events not people, apply the prime directive ) embracing failure: Whatever you do is an experiment, be an adventurer trying to predict failures to avoid them: have a look at what people are not confident about, to try to find patterns Anti-patterns : hidden mistakes, which are wasted learning opportunities. This is linked to “CEO disease”, i.e. only talking about successes. blaming: fear of blame encourages hiding of mistakes and/or of their root causes the “arc of escalation” : “escalating” issues forces a blame game, and information gets lost through the ensuing Chinese whispers game. information bias (decisions based on too little knowledge) : talk to people, try to switch perspectives… cowardice (situations where no one wants to be responsible) He warns against incentives (financial incentives or even people getting fired for making mistakes) that encourage the anti-patterns, or make people avoid mistakes so badly that any meaningful changes are prevented. Finally, for my part, I presented my open-source delivery tool Git-deliver . The talk went rather well, and generated quite an interest, as evidenced by the attendance and dynamism at the open space session I proposed after the talk. Questions centered around possible use cases and very concrete deployment issues people are experiencing with their current tooling. It gave me ideas for new features, for example when it comes to automated provisioning of new delivery targets. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Culture , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-04-30"},
{"website": "Octo", "title": "\n                System flow monitoring: an example            ", "author": ["Erwan Alliaume", "Cédrick Lunven", "Julien Kirch"], "link": "https://blog.octo.com/en/system-flow-monitoring-flow-an-example/", "abstract": "System flow monitoring: an example Publication date 01/06/2015 by Erwan Alliaume , Cédrick Lunven , Julien Kirch Tweet Share 0 +1 LinkedIn 0 Having written about monitoring flow and best practices for setting it up, let’s move onto a practical example. We’ll define a mini-information system combining services and messages, then we will show how to monitor it, including technical explanation and the full code. The application components presented in this article have generic names (frontend, middle-end and backend) so that everyone can easily adapt them to their own context. We could, for example, imagine frontend like an mobile application or a single-page javascript application, the middle-end like a layer of REST services (APIs), and finally the backend as a persistent system or transaction analysis. About the code The code proposed in this article was written for illustrative purposes. It is therefore as simple as possible and does not include things like error management or optimisation. It’s up to you to inspire yourself and adapt it (we released it under the open source Apache license) but above all, do not reuse it as is. The steps to test the code yourself are described in the project documentation . In order to keep this article short, we used hyperlinks rather than inline source code. If portions of the code seem unclear to you or you have any suggestions, please create a ticket ; and if you have ideas for improvements, pull requests are welcome. The system The first article explained that business services have two properties: they can combine multiple service calls and/or message sending they can combine heterogeneous technologies An information system that combines these two aspects requires flexible monitoring tools and should be based on interchangeable technologies. The application architecture presented here takes into account these elements: Business side: A frontend server that exposes a service and uses a website with a form A middle-end server providing two services for the frontend A backend server processing asynchronous messages published by the middle-end on a bus Monitoring side: A monitoring module in each application component A dedicated bus receiving messages from each of these modules A CEP server for processing these messages to create alerts and produce KPIs A database indexing the messages and information generated by the CEP A dashboard tool to present monitoring information Format of monitoring messages Defining the monitoring messages format is crucial, even if the processing system must be designed to easily adapt to different cases. The use of a common template simplifies the messaging operation in the CEP and the dashboard. In our case, we chose to use messages in JSON format: {\r\n    \"correlation_id\": \"octo.local_MonitoringBase_24389_2015-01-30 11:05:29 UTC_36cddd01-7bcd-4ced-8024-919ff1dbe6ca\",  // correlation ID\r\n\r\n    \"timestamp\": \"2015-01-30T12:05:29.230+01:00\", // timestamp of the msg\r\n    \"module_type\": \"FrontendApp\", // name of the module name that sends the message\r\n    \"module_id\": \"FrontendApp_octo.local_001\", // identifier of the module that sends the message\r\n    \"endpoint\": \"GET /messages\", // name of the service\r\n    \"message_type\": \"Send message to backend\", // type of the message\r\n\r\n    \"begin_timestamp\": \"2015-02-19T22:11:15.939+01:00\", // optional: timestamp of the start of the action\r\n    \"end_timestamp\": \"2015-02-19T22:11:15.959+01:00\", // optional: timestamp of the end of the action\r\n    \"elapsed_time\": 0.020169, // optional: time elapsed during the action\r\n\r\n    \"service_params\": {\r\n        // optional: parameters of the service\r\n    },\r\n\r\n    \"headers\": {\r\n        // optional: headers of the service (http headers for example)\r\n    }\r\n\r\n    \"result\": {\r\n        // optional: return value of the service\r\n    }\r\n} Details of each component About monitoring To provide the necessary data, each component embeds code in order to retrieve the relevant information to copy them on a bus. To minimize the impact on the application code, message sending is done in a dedicated thread and exception management separates the monitoring code from the business code. Following the recommendations of the first article, we selected ZeroMQ as the monitoring bus for its excellent performance. Frontend The frontend server is made with Ruby and uses the web framework Sinatra which is perfect to easily demonstrate web services. app_base is where you set the core application parameters, and provides a way to call the services of the middle-end server The static directory contains the web site frontend_app exposes the business side service which calls the website and two middle-end services consecutively Monitoring The monitoring code is located in the class monitoring_base.rb . The Sinatra framework provides the entry points needed for the monitoring with before and after methods where all the information of the current request is accessible. To store information during the execution of the request, such as the start time of the execution, a field is added to the Request class . The method for calling services is overidden for two purposes: Send copies of service calls to the monitoring system Add http headers to the service call to transmit the correlation ID as well as the time of the call The data is posted in a queue and exists in a separate thread. Middle-end The middle-end server uses Spring . Spring Boot allows easy configuration of an application and Spring MVC allows the use of REST services. MiddleEndController contains the controller that exposes both exposed services RedisProvider provides access for the bus to send messages to the backend Monitoring Because of the choice of Spring technology, the monitoring set-up requires some special handling: A HandlerInterceptor provides an entry point at the beginning and the end of the execution of each HTTP request for creating messages that will be sent to the monitoring side It is necessary to subclass the HttpServletRequest to be able to store information during the execution of the request such as the start time of its execution Finally, HttpServletRequest that represents the query and HttpServletResponse that represent the response, do not give access to the content of the request nor to the response because their package is streamed. It is therefore necessary to bundle both classes to save the contents during transmission and thus be able to read them later The result is divided into 5 classes: MonitoringServletRequest represents the request; it provides some utility methods including correlation identifier recovery and uses RecordingServletInputStream to save the contents RecordingServletResponse represents the response and saves the contents using RecordingServletResponse MonitoringInterceptor is the interceptor that sends messages by retrieving information from the request and response The code responsible for sending messages is located in a shared project because it is used by the middle-end and back-end. The useful code is located in MonitoringMessageSender which uses a dedicated thread to send messages and rely on a queue. RedisProvider was changed to transmit the correlation identifier in messages sent to the backend. The application bus It is a Redis server: it is mainly used as a key-value cache, but its API makes it a suitable message bus. Its main advantages are its simple usage and its processing speed. The backend We simulated a message processing application using a thread pool: ApplicationBase provides the base application that receives the messages from Redis and gets processed by a Java thread pool Backend processes the messages Monitoring Since the reception code is specific to the application, monitoring is fully integrated with the application base. For sending messages, it relies on the same shared project as the middle-end. Complex Event Processing Principles The Complex Event Processing component sends messages from different modules (frontend, middle-end, backend). In parallel, it performs database inserts and updates memory status. The evolution of this state can generate alerts which will be persisted into the database. It is implemented in Java using the integration framework Apache Camel and comes as a standalone application. Messages are popped from ZeroMQ using a Connector that we had to rewrite using the jeroMQ library. The existing component worked with non-applicable Scala bindings. The memory state and alert notifications are implemented using the Esper framework. Camel provides the connector to bind to it. The rules are written with the internal DSL named EPL (Event Processing Language) in the Camel configuration file . Messages and alerts are persisted in an ElasticSearch cluster with the help of a home made connector using the library Jest (it is therefore not designed for scalability). The default connector is recorded as a node of the ElasticSearch cluster and that makes the local tests more complicated. Event interpretation The flow of events is analyzed to build real-time indicators thanks to Esper Process Language (EPL) analytical language. These statuses are queried periodically to detect abnormal behaviors such as threshold violations. We’ve tried to demonstrate three types of alerts: Excessive processing time for one of the components using the attribute elapsed_time of the messages. Excessive processing time for an execution on the entire chain , using the correlation identifier. Throttling: The number of calls exceeding a threshold set in a fixed time unit (here an average of more than 3 calls in 10 seconds). The monitoring database It is based on Elasticsearch which automatically indexes the data when it is inserted. For data to be indexed in the best way possible, simply create an index beforehand so that the fields are indexed the right way. The dashboard With the data already structured and stored in ElasticSearch, Kibana is the natural choice for the dashboard: assistant wizards make it easy to create various dashboards based on the data in the database. For example, a dashboard of the percentiles calls on different servers (the configuration is available in the source code ): Monitoring-adverse components The cases presented here are friendly because the components are not too complicated to monitor, even if the middle-end requires some special handling. Unfortunately throughout any IS of a certain size, there is always at least one “black box” brick, like a portal or an e-commerce platform, that is difficult to properly equip. For these components, there are generally two choices available: Use the extensions provided by the tool This solution is the most consistent. However, these APIs are often of inferior quality. So, before starting using them, three things must be checked: Is the documentation sufficiently detailed, particularly where internal objects are exposed? Are the APIs stable? Since these APIs are fairly close to the engine tools, they are more likely to change from one version to another. Are all the data you need exposed? Depending on the answers to these three questions, the second solution might be a better choice. Use a proxy If the exchanges with the component are made through http, another solution is to set up a proxy like nginx to generate the monitoring messages. By configuring the logs, you should be able to get all the information you need, and a custom component is needed to push them to the CEP server. This solution has the disadvantage of adding an additional infrastructure layer, but avoids the need to develop too much code for a specific tool. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-06-01"},
{"website": "Octo", "title": "\n                Feature team: beyond the buzzword            ", "author": ["Maxence Walbrou", "Eric Favre"], "link": "https://blog.octo.com/en/feature-team-beyond-the-buzzword/", "abstract": "Feature team: beyond the buzzword Publication date 31/03/2015 by Maxence Walbrou , Eric Favre Tweet Share 0 +1 LinkedIn 0 The team organization is the core issue when scaling out agile methods to the company scale. Here, many may mention “feature teams” but often forget the true meaning of these two words! You are willing to change your teams organization and understand the differences between a cross-functional team and a feature team? This article proposes a few approaches to understand these models, and more importantly to know which one to adopt. 1) Identify the pain points leading to a change of organization Changing an organization is a complex human process . It takes time and presents difficulties that should not be underestimated. The first step before contemplating any solution is to get back to the root issues and make sure that change is a solution. -> What pain points urge the will for a change of teams organization? -> Is such an organization change the actual solution to these pain points? For instance, here are a few difficulties we came across in clients contexts where the team were organized in traditional silos: A time-to-market considered too slow A repeated difficulty to synchronise big projects requiring the coordinated inputs of several teams A hard time to innovate efficiently, to work differently Some low quality issues related to the team segmentation (e.g: who assesses quality? The dev team or the QA team?) Some communication issues between teams / silos (e.g: IT vs business) In these contexts, it makes sense to plan a new organization aiming for the creation of multi-skilled teams. 2) Create cross-functional teams A cross-functional team is a team gathering all necessary skills , from concept to production, to achieve the product. The SCRUM methodology, among others, promotes this model. In such a team, there is no more silo: everyone works together and is collocated to enable an optimal communication. For the same reasons, a small sized team is preferred: 6 to 12 people per team ( pizza-team ). Such a team is made of enough people to achieve the project. Therefore, it must be given the necessary autonomy to take decisions according to the company’s strategy, and the responsibility to reach its objectives (which of course must be reachable ). We can schematize a cross-functional agile team and other actors with whom the team interacts closely this way: A few interesting questions arise here: -> Is the company ready to create small collocated teams mixing business and IT (developers, testers, even operators) for them to work together every day? -> Are the members of these teams motivated by this gathering and the direct communication it enables? Depending on the company’s culture, setting up such team structure may not be easy, especially when it comes to communication . Where it used to be ruled by documentation, it becomes more direct. Issues and their argued solutions become the responsibility of the team. Its members have to review their whole working habits. Another issue also emerges: how to share knowledge between people of a same trade (for instance, front-end developers) who are now scattered into different teams. A possible solution is the creation of communities of practice enabling periodic meet-ups for these people to share their expertise and good practices. At last comes the sensitive topic for any transformation: the (re)definition of roles within teams, including the new place for middle management who used to ease transit of information… To address these important matters, we recommend to test this kind of organization with 2 cross-functional teams comprising several people eager to experiment this organization. The try-out should last several months (at least 6 months). At the end of this pilot stage, the members will have apprehended and adapted the roles for a better match of the context, and will be able to provide a feedback on the experiment. 3) Create “Feature-” or “Component-oriented” teams Feature team To loosen coupling between cross-functional teams, some organizations specifically dedicate them to the development and support of a clearly defined functional scope . These are feature oriented teams, or Feature teams . Here is an example by Henrik Kniberg of Feature teams as seen at Spotify : -> Can the company identify within its activity independent features that could be handled by loosely coupled teams? -> Is the software architecture compliant with such organization? It is worth stressing that feature teams need to be as independent as possible from each other, on both organizational and product architecture levels. A feature team must be able to deliver a new version of its scope without impacting other teams, fostering an optimal TTM. Component team “Component-oriented” teams: have a strong technological specialty (specific language, software package…), known by a small group of people, and/or manage a “service” used by numerous teams (billing tool, emailing service…) which should be kept whole for SLA consistency issues. Dean Leffingwell (author of the SAFe framework) presents here this difference between Feature and Component teams: -> Are there any “component-oriented” teams within the company that are likely to stay organized this way? Component and Feature teams are not incompatible. However a Component team may face a few limitations: Its TTM is more significant than that of a Feature team because it handles requests pending from various stakeholders. Therefore, it has to stage its developments to deliver fairly distributed features to the requesters. To respond to these demanding teams, it tends to require (too?) early design Finally, as a silo of skills , it can slow down innovation since any idea issued by another team must come across an “administrative” request to be added to the backlog. To limit these issues as well as the global TTM of an organization, one should keep the number of Component teams lower than the number of Feature teams . We believe the proper proportions to be Feature team [70-80%] vs Component team [20-30%]. Conclusion An organization with specialized cross-functional teams solves many issues encountered in a company organized in silos. You can find many examples and resources regarding this topic on the web (starting with this article about Feature teams by Craig Larman, or this video dealing with agile at Spotify). Nonetheless, beyond the chosen theoretical organization, the true challenge is actually implementing this new communication model between the key players. An experimenting phase between employees motivated by change is a great way to initiate this kind of transformation. What about you? What team organization do you implement? Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-03-31"},
{"website": "Octo", "title": "\n                Software Craftsmanship: a culture to pass on            ", "author": ["Michel Domenjoud", "Eric Favre"], "link": "https://blog.octo.com/en/software-craftsmanship-a-culture-to-pass-on/", "abstract": "Software Craftsmanship: a culture to pass on Publication date 12/02/2015 by Michel Domenjoud , Eric Favre Tweet Share 0 +1 LinkedIn 0 The Manifesto for Software Craftsmanship is subheaded Raising the bar . Indeed, we believe that this is the main issue at stake behind this movement’s motivations. To create products that rock, you need to rely on people who know how to do it, enjoy it, and always strive to do it better. It’s not just about introducing a few practices. It’s a genuine culture of development aiming at changing the mentalities, the team operations and the company. But you can’t decree a change of culture. So how can one introduce the values of Craftsmanship into one’s company so that they become a standard instead of an isolated case? Values and vision of software development Reaching for higher quality isn’t a new stake, but it has got even more strategic in a society where software is everywhere. In this digital era, one has to be always faster to comply with the customer’s needs and keep ahead of the competitors. The word quality can hold many interpretations. I will use it here to refer to a well-designed and value adding software. Building a software of high quality doesn’t mean looking for absolute perfection and building a cathedral for any new product. It’s not either about quickly building an application that meets the requirement but might fall apart anytime. Building a software of high quality is looking for a balance ; it’s creating a system that meets every needs while relying upon sound basis. The lack of quality has a cost, not only strategic but also financial : an application stuffed with flaws will always evolve more costly, and a trustless customer might not want to pay anymore. Furthermore, an easily forgotten cost of a lack of quality is the human cost . To work in such an environment can be discouraging for developers, and the best amongst them might leave for another company. We believe that most people feel satisfied with a job well done, including developers who like to feel proud about their achievements. To reach for a high quality level isn’t only a matter of producing applications and quality code: it’s a culture. A culture of quality software. While it is important to look into the produced artefacts and the practices that allowed to achieve them, it is essential to understand the factors enhancing this culture within our companies. All Craftsmen? It would be a shame to assert that there are Craftsmen on one side , some kind of super developers gathering at meetups and always looking for perfection, and regular developers on the other , not sharing the same goal and lacking the right skills. There are people, developers, who work within a team and a company. Some, often thrilled by their job, have already assimilated these values. Some even have the necessary environment to realise products accordingly to their values. Unfortunately, many others have little or no knowledge that there is another way to develop software. Still today, many development practices that help build premium quality software are just not taught in schools . Some other people think these practices are simply not applicable to their company. They believe they can’t change anything. So yes, we could call Craftsmen this first category of developer convinced by the values of this movement and separate them from the others. But the very goal of Software Craftsmanship isn’t to rank developers. The other developers aren’t necessarily bad developers or developers from a different category. They just didn’t have a chance to try out this culture. The real issue at stake here is cultural . How to address this issue, then? To some people, if you can’t find the Software Craftsmanship values in a company, you need to change company. It can be an individual solution, but what can be done for those who stay? What can be done if your new company isn’t so perfect after all? I will relate my own experience as an example, and I’m sure some may identify with it. During my first years as a developer, I barely knew the existence of unit testing or continuous integration. I hadn’t learnt about it in school, and neither did the people I was working with. Yet, I strived to do my best, to produce applications which made me proud. Then I met a passionate developer who passed on some practices and encouraged his coworkers to use them. Later, I took part in a project where this culture was central. And so on, until I wanted to pass on this culture myself, convinced that it was the right way to do my work. I’m not saying that everyone should change and yield to some kind of Software Craftsmanship “religion”. Some might not agree stricto sensu, but that’s not the point of this article. I’m just convinced that we should share these values and that there is still a long way to go in many companies . So, how to broadcast this culture? How to make a difference? Adopt the culture of software craftsmanship When we talk about Craftsmanship, we think software development professionalism and everyone’s responsibility in the achievement of his work. A certain vision of the daily work of a developer allows the production of good quality products which fullfill at best a given need, the achievement of projects and their sustainability. It’s a pragmatic approach: you don’t look for perfection, you just strive to do your best with regards to the context. Some practices appear as vital to reach this high quality efficiently. To name a few, that’s the case of test driven development ( TDD , BDD …), Clean Code standards , or Evolutionary Design . Finally, we try to learn and learn some more , for our work is one perpetually and quickly evolving: reading books and blogs training , practicing new tools, new languages meeting other developers at conferences, meetups… identifying mentors But each one individually adopting this culture is not enough. No matter how much TDD they do or how many meetups they attend, many developers feel alone in their company. They may not share some coworkers’ values or they may be unheard by their management when they mention their own values. Pass on values and practices Most products are developed by a team, and these achievements are the fruit of team work. Several practices will help spread the culture of quality among the team: Learn and pass on development good practices through pair programming and dojos . Secure a collective knowledge of the code through code reviews , by pair or collectively. Avoid any developer working on his own on a specific piece of code. Establish together development standards and keep them up to date. Rely on tools such as continuous integration to shorten the feedback loop . For instance, on a previous project, we had planned a coding dojo ritual once every two weeks, during which we shared our practices with some perspective over the project. It was an occasion to experiment new technologies, assess their match for the project needs, share new coding techniques, and update our standards together. These practices are fundamental to promote the culture of quality, but it’s necessary to break a few habits anchored in the company to enable them. The developer’s work is all too often regarded as a lonely activity. This entails two downsides: non-tech members of a project regard collective development practices such as pair programming as an extra cost although some studies tend to prove otherwise. developers used to working alone may have difficulties working in a team, let alone showing their code and talking about it. On that matter, it’s necessary to adopt a positive stance, to learn how to give and receive feedback without finger-pointing. The principles of Egoless Programming are a way of addressing this problem. Fostering a team-wide culture of quality isn’t easy. It requires leading a true change management. A couple of days of practices training won’t make a team of hardened craftsmen. It takes time and you can’t force people to change. To achieve that, it’s useful to rely on people mastering these practices. You can rely on a tech lead who will help the team improve until it becomes autonomous. And if there isn’t such a person within the team, you can involve an outsider of the team who can raise awareness of these practices, train the team and accompany it. A company-wide culture As we have seen, the development of this culture is boosted by encounters with people who are willing to share their hard and soft skills, and who wish to learn more and more in our constantly evolving craft. Developing this culture company-wide may seem like a daunting task. That’s why the stakeholders must enable the conditions favorable to these encounters . Several actions can be introduced: Supporting the continuous improvement practices such as code reviews, allowing the teams to self-organise. The developers are development professionals, it is therefore essential to grant them autonomy in their area. Providing meeting spaces for teams, fostering the emergence of communities of practice. At OCTO for instance, we host BBLs almost every day, and the third thursday afternoon of each month is dedicated to BOFs , where each one can share his experience. Allocating a learning budget to every projects, so that anyone can learn outside of his personal time, and so that people are provided with a collective learning time. Rethinking the organisation of space and offices to ease collective work Identifying influential leaders and granting them the legitimacy to spread their values and their practices by driving the community, accompanying and coaching other teams. I’d like to emphasise that we must not try to scale out too fast . A few practices successfully adopted by one team doesn’t mean they should be forced into the whole organisation. You can’t change people, but you can inspire them , and a company-wide culture will contribute significantly. Where to begin? Next time you face difficulties while coding, ask any coworker for help, and suggest a pair programming period or a collective code review session. Do the same thing next time you use TDD (or any other practice), offer to pair program with a coworker who isn’t used to this practice. Take some time to discuss an interesting article with coworkers. Organise a dojo to try this new library you’ve been willing to add to your project. Go and meet this other team working on similar technologies and subjects, and exchange about each other’s practices What’s happening? What comes out of these meetings? What have you learnt or passed on? Conclusion The Software Craftsmanship movement offers a vision on the developer’s activity. This vision shouldn’t only relate to individuals, but also define a company-wide vision of the developer’s work , promoting a culture of quality and continuous learning . The software creation is a fascinating trade, and even a fun one actually, as long as you can practice it with diligence and professionalism. That’s fortunate, this issue is a critical stake for a company: our society relies broadly on software, and the trend is rising. It’s our responsibility to adopt values and practices leading to high quality software , and the Software Craftsmanship movement embodies these measures. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-02-12"},
{"website": "Octo", "title": "\n                Concaténation, Compression, Cache            ", "author": ["Timothée Carry"], "link": "https://blog.octo.com/en/concatenation-compression-cache/", "abstract": "Concaténation, Compression, Cache Publication date 24/02/2015 by Timothée Carry Tweet Share 0 +1 LinkedIn 0 Quand on cherche à optimiser les performances de son site web, il y a trois éléments essentiels à faire avant toute chose. Trois méthodes très simples à mettre en place et qui apportent un retour direct et flagrant sur la vitesse de chargement. Ces trois méthodes sont la concaténation, la compression et le cache. Nous avons déjà abordés ceux-ci lors d’une présentation aux HumanTalks de septembre 2014, mais nous allons les détailler dans la suite de cet article. 1. Concaténation Le principe de la concaténation est de regrouper plusieurs fichiers de même type en un seul , afin de se retrouver avec moins de fichiers finaux à télécharger. Les fichiers qui profitent le plus de ce système sont les fichiers CSS et JavaScript. La nature même du téléchargement d’assets fait que notre navigateur doit payer certains coûts, en millisecondes, à chaque nouvel élément téléchargé. Ces coûts sont de diverses natures : TCP Slow Start TCP, le protocole de connexion qu’utilise HTTP, possède un mécanisme de slow-start qui lui permet de calculer la vitesse optimale de transmission de l’information. Pour parvenir à ce résultat, il doit effectuer plusieurs aller-retours entre le client et le serveur, en envoyant de plus en plus en plus de données, pour calculer la vitesse maximale possible d’émission/réception. Si on envoie une multitude de petits fichiers, la transmission n’a jamais le temps d’atteindre sa vitesse optimale et doit recommencer ses aller-retours pour le prochain fichier. En groupant les fichiers en un fichier de plus grande taille, le coût de calcul n’est payé qu’une seule fois et le reste du fichier peut se télécharger à la vitesse maximum. À noter que maintenir les connexions à votre serveur en Keep-Alive permet de réutiliser une connexion d’un asset vers le suivant et donc de ne payer le coût de calcul qu’une fois. Malheureusement, activer le Keep-Alive sur un serveur Apache risque aussi de limiter le nombre de connexions parallèle que votre serveur peut maintenir. SSL De la même manière, si votre serveur utilise une connexion sécurisée, il y a un échange de clés entre le client et le serveur qui s’effectue pour vérifier que les deux sont bien qui ils annoncent être. Ici encore, le coût de cet échange est payé sur chaque asset téléchargé. Mettre les fichiers en commun permet donc de ne payer le coût de cet échange qu’une seule fois. Connexions parallèles Finalement, il y a une dernière limite, purement du coté du navigateur cette fois-ci : le nombre de connexions parallèles. La norme HTTP indique qu’un navigateur devrait ouvrir un maximum de 2 connexions parallèles vers un même serveur. Techniquement, les navigateurs récents ont augmenté cette limite à une valeur entre 8 et 12 car 2 était beaucoup trop restrictif. Cela signifie c’est que si vous demandez à votre page web de télécharger 5 feuilles de style, 5 scripts et 10 images, le navigateur ne va lancer le téléchargement que des 12 premiers éléments. Il commencera le téléchargement du 13e uniquement une fois qu’un des 12 premiers sera arrivé, et ainsi de suite. Ici encore, la concaténation vous permet de laisser plus de canaux disponibles pour télécharger les autres assets de votre page. Les fichiers CSS et Javascript se concatènent très bien. Il suffit simplement de créer un fichier final qui contient le contenu mis bout-à-bout de tous les fichiers initiaux. Votre processus de build devrait pouvoir s’en charger sans problème, mais une solution simple peut s’écrire en quelques lignes : cat ./src/*.css > ./dist/styles.css\r\ncat ./js/*.js > ./dist/scripts.js À noter que la concaténation d’images (CSS Sprites) est aussi possible, mais nous ne l’aborderons pas dans cet article. 2. Compression Maintenant que nous avons réduit le nombre de fichiers, notre deuxième tâche va être de rendre ces fichiers plus légers , afin qu’ils se téléchargent plus rapidement. Pour cela, il existe une formule magique formidable nommée Gzip qui permet de réduire de 66% en moyenne le poids des assets textuels. La bonne nouvelle c’est que la majorité des assets que nous utilisons dans la création d’un site web sont du texte. Les briques principales comme le HTML, le CSS et le Javascript bien sur, mais aussi les formats classiques de retour de votre API : XML et JSON. Et beaucoup d’autres formats qui ne sont en fait que du XML déguisé : flux RSS, webfonts, SVG. Gzip, et c’est assez rare pour le souligner, est parfaitement interprété par tous les serveurs et tous les navigateurs du marché (jusque IE5.5, c’est dire). Il n’y a donc aucune raison de ne pas l’utiliser. Si un navigateur supporte le Gzip, il enverra un header Accept-Encoding: gzip au serveur. Si le serveur décèle ce header dans la requête, il compressera le fichier à la volée avant de le retourner au client, en y ajoutant le header Content-Encoding: gzip , et le client le décompressera à la réception. L’avantage est donc d’avoir un fichier de taille réduite qui transite sur le réseau, avec en contrepartie le serveur et le client qui s’occupent respectivement de la compression/décompression. Sur n’importe quelle machine issue des 10 dernières années, l’overhead de la compression/décompression en gzip est absolument négligeable. Par contre, le fait d’avoir un fichier bien plus léger qui transite sur le réseau permet des gains très importants. Les librairies de compression Gzip sont disponibles sur tous les serveurs du marché, il suffit généralement simplement de les activer en leur indiquant les types de fichiers qui doivent être compressées. Vous trouverez ci-dessous quelques exemples sur les serveurs les plus connus : Apache <IfModule mod_deflate.c>\r\n  <IfModule mod_filter.c>\r\n    AddOutputFilterByType DEFLATE \"application/javascript\" \"application/json\" \\\r\n    \"text/css\" \"text/html\" \"text/xml\" [...]\r\n  </IfModule>\r\n</IfModule> Lighttpd server.modules += ( \"mod_compress\" )\r\ncompress.filetype = (\"application/javascript\", \"application/json\", \\\r\n\"text/css\", \"text/html\", \"text/xml\", [...] ) Nginx gzip on;\r\ngzip_comp_level 6;\r\ngzip_types application/javascript application/json text/css text/html text/xml\r\n[...]; S’il y a bien une optimisation de performance qui nécessite peu de travail à mettre en place et qui améliore grandement les performances de chargement, c’est bien le Gzip. Cela ne nécessite aucun changement sur les fichiers servis, uniquement une activation de config sur le serveur. Minification Pour aller plus loin, vous pouvez aussi investir sur la minification de vos assets. HTML, CSS et Javascript sont encore une fois les meilleurs candidats pour la minification. La minification est un procédé qui va ré-écrire le code de vos assets dans une version qui utilise moins de caractères, et qui donc pèsera moins lourd sur le réseau. D’une manière générale cela va surtout supprimer les commentaires et les sauts de ligne, mais des minificateurs plus spécialisés pourront renommer les variables de vos Javascript en des valeurs plus courtes, regrouper vos sélecteurs CSS ou supprimer les attributs redondants de vos pages HTML. L’ajout d’un processus de minification est plus complexe que l’activation du Gzip, et les gains sont aussi moins importants. C’est pourquoi nous vous conseillons de toujours commencer par la compression Gzip. 3. Cache À présent que nous avons réussi à limiter le nombre de fichiers et à faire baisser leur poids, la prochaine étape est de les télécharger le moins souvent possible . L’idée principale ici est qu’il est inutile de faire télécharger à votre visiteur un contenu qu’il a déjà téléchargé et possède donc en local sur son poste. Nous allons commencer par expliquer comment fonctionne le cache HTTP car c’est un domaine qui est généralement mal compris des développeurs. Il y a en fait deux principes fondamentaux à comprendre dans le cache HTTP : la fraicheur , et la validation . Fraicheur On peut voir la fraicheur d’un asset comme une date limite de consommation. Lorsque l’on télécharge un élément depuis le serveur, celui-ci nous l’envoie accompagné d’un header indiquant jusqu’à quelle date cet élément est encore frais. Si jamais le client à besoin à nouveau du même élément, il commence par vérifier la fraicheur de celui qu’il a en cache. S’il est encore frais, il ne fait pas de requête au serveur, et utilise directement celui qu’il a sur son disque. On ne peut pas faire plus rapide, car il n’y a alors absolument aucune connexion réseau impliquée. Par contre, si jamais la date de fraicheur est dépassée, alors le navigateur va lancer une nouvelle requête au serveur pour récupérer la nouvelle version. En HTTP 1.0, le serveur retourne un header Expires avec la date limite de fraicheur. Par exemple : Expires: Thu, 04 May 2014 20:00:00 GMT . Dans cet exemple, si jamais le navigateur demande à nouveau le même asset avant le 4 Mai 2014 à 20h, alors il le lira depuis son cache, sinon il interrogera le serveur. Cette notation a un défaut majeur dans le fait que les dates sont fixées de manière absolue . Cela signifie que le cache de tous les clients perdra sa fraicheur en même temps. Et vous aurez donc potentiellement tous les clients qui feront une nouvelle requête vers votre serveur en même temps pour se mettre à jour, ce qui peut générer un très fort pic de charge à cet instant. Pour limiter cela et donner plus de flexibilité dans la gestion de la fraicheur, en HTTP 1.1, un nouveau header à été introduit : Cache-Control . Celui-ci accepte plusieurs arguments qui permettent de gérer plus finement la manière de mettre en cache, et celui qui nous intéresse ici est max-age qui permet de définir une durée relative de fraicheur , en secondes. Votre serveur peut donc répondre Cache-Control: max-age=3600 pour indiquer que l’asset est encore frais pendant 1h (3600 secondes). En faisant ainsi vous pouvez espacer les appels sur une plus longue période. Validation La deuxième composante du cache est la validation . Imaginons que notre asset ait terminé sa période de fraicheur, nous allons donc récupérer une nouvelle version de celui-ci sur le serveur. Mais il est possible que l’asset n’ait pas réellement changé sur le serveur depuis la dernière fois. Il serait alors inutile de retélécharger quelque chose que nous avons déjà dans notre cache. Le principe de validation permet au serveur de gérer cela. Soit l’asset du client est identique à l’asset du serveur, dans ce cas le client peut garder sa version locale. Soit les deux sont différents et dans ce cas le client doit mettre à jour son cache avec la version distante. Lorsque le client a récupéré l’asset pour la première fois, le serveur lui a répondu avec un header Last-Modified , par exemple Last-Modified: Mon, 04 May 2014 02:28:12 GMT . La prochaine fois que le client fera une requête pour récupérer cet asset, il renverra la date dans son header If-Modified-Since , par exemple If-Modified-Since: Mon, 04 May 2014 02:28:12 GMT . Le serveur compare alors la date envoyée et celle qu’il possède de son coté. Si les deux correspondent, alors il renverra un 304 Not Modified pour indiquer au client que le contenu n’a pas changé. Celui-ci continuera alors d’utiliser sa version locale. Ainsi, on évite de transmettre du contenu inutile sur le réseau. Par contre si le serveur voit que le fichier qu’il possède est plus récent que la date envoyée, il répondra avec un 200 OK et le nouveau contenu. Ainsi, le client utilise désormais la dernière version. En faisant ainsi, on évite donc de télécharger un contenu qu’on possède déjà. Dans les deux cas, le serveur renvoie de nouvelles informations de fraicheur. Comme pour la fraicheur, il existe deux couples de headers pour communiquer des informations de validation au serveur. En plus de Last-Modified / If-Modified-Since qui utilisent une date de modification, il est possible d’utiliser des ETags. Un ETag est un hash qui identifie de manière unique chaque fichier . Si le fichier change, alors son ETag change aussi. Par exemple, le serveur retourne au client lors du premier appel un header ETag: \"3e86-410-3596fbbc\" , et lorsque le client fait à nouveau appel à la même ressource, il envoie un header If-None-Match : \"3e86-410-3596fbbc\" . Le serveur va comparer les deux ETags et retourner un 304 Not Modified s’ils sont identiques ou un 200 OK avec le nouveau contenu s’ils sont différents. Last-Modified et ETag possèdent des comportements très similaires, mais nous vous conseillons d’utiliser Last-Modified en priorité. En effet, la spec HTTP indique que si un serveur retourne un Last-Modified et un ETag , alors le navigateur doit prendre en priorité le Last-Modified . De plus, la majorité des serveurs génèrent l’ETag à partir de l’inode du fichier, de manière à ce que celui-ci soit modifié au moindre changement. Malheureusement, ceci pose des soucis pour peu que vous ayez des serveurs redondés derrière un load-balancer où chaque serveur possède son propre filesystem et donc ses propres inodes. Deux fichiers identiques, sur deux serveurs différents auront des inodes différents et par conséquent des ETag différents. Votre système de validation ne fonctionnera plus dès lors que votre client sera redirigé vers un autre frontal. À noter que ce problème n’apparait pas sous nginx, qui ne prends pas en compte l’inode dans la génération de son ETag. Sous Apache, l’option FileEtag MTime Size permet de le désactiver, ainsi que etag.use-inode = \"disable\" sous lighttpd. Récapitulatif À la lumière de ces explications, nous pouvons donc retracer le parcours classique du téléchargement d’un asset mis en cache. Le client effectue une première requête pour récupérer un asset. Il récupère son Cache-Control: max-age pour la fraicheur et son Last-Modified pour la validation. S’il demande à nouveau le même asset alors que celui-ci est encore frais, il le prends directement depuis son disque local. S’il le demande au dela de sa date de fraicheur, il fait un appel au serveur en envoyant son If-Modified-Since . Si le fichier sur le serveur possède la même date de modification que celle envoyée, il retourne un 304 Not Modified . Si le fichier sur le serveur a été modifié, il retourne un 200 OK avec le nouveau contenu. Dans tous les cas, le serveur retourne un Cache-Control et un Last-Modified . Invalidation du cache Mais le cache est un animal capricieux, et nous savons tous que : Il y a deux choses complexes en informatique : invalider le cache et nommer les choses. Et effectivement, invalider le cache de nos clients quand nous avons besoin de faire une mise à jour est extrêmement difficile. C’est en fait tellement difficile que nous n’allons pas le faire du tout. Comme le navigateur met en cache chaque URL, si nous souhaitons modifier un contenu, il nous suffit de modifier son URL. Et les URL, c’est quelque chose que nous avons en quantité illimitée. Il nous suffit de modifier le nom d’un fichier pour générer une nouvelle URL. On peut ajouter un numero de version, un timestamp ou un hash à notre nom de fichier original pour lui générer une nouvelle URL. Par exemple : style-c9b5fd6520f5ab77dd823b1b2c81ff9c461b1374.css au lieu de style.css . En mettant un cache très long sur ces assets (1 an est le maximum officiel de la spec), c’est comme si on les gardait en cache indéfiniment. Il nous suffit juste de mettre un cache plus court sur le fichier qui les référence (généralement le fichier HTML). Ainsi, si on pousse en production une modification sur une feuille de style ou dans un script, il nous suffit de modifier les références à ces fichiers dans nos sources HTML pour que les clients téléchargent les nouveaux contenus. Le cache sur les fichiers HTML est beaucoup plus court, de manière à ce que les changements introduits par notre mise en production soient rapidement répércutées sur nos clients. Les anciens contenus seront encore en cache chez nos clients mais cela n’a pas d’importance, nous ne les requêterons plus jamais et les éléments non-utilisés du cache des clients se vident régulièrement. La technique est en fait très proche des Etag vus précédement à la différence qu’ici nous sommes maîtres de la génération du nom unique de fichier et du moment où nous souhaitons invalider le cache de nos clients. Au final, nous utilisons un mélange de ces deux techniques pour gérer un cache optimal. Les éléments dont l’URL est significative, comme les pages HTML ou les retours d’une API définiront une fraicheur faible (de quelques minutes à quelques heures, en fonction de la fréquence moyenne de mise à jour). Ceci permet de s’assurer que le client aura rapidement la nouvelle version quand celle-ci est déployée, tout en limitant la charge sur le serveur et la quantité d’information transitant sur le réseau. Pour les éléments dont l’URL n’est pas significative, comme les feuilles de styles, les scripts, les polices de caractère ou les images, on utilisera une fraicheur maximum d’un an. Ceci permettra au client de garder indéfiniment la ressource dans son cache sans avoir besoin d’interroger à nouveau le serveur. On générera par contre une URL différente en fonction d’un hash du contenu à chaque fois que le contenu vient à changer. On prendra bien garde à modifier les références à ces fichiers dans les pages HTML. Conclusion Nous avons donc vu comment trois points très simples permettent de diminuer grandement le nombre de total de fichiers à télécharger, les rendre plus légers, et les télécharger moins souvent . La concaténation automatique des fichiers doit être intégrée dans votre processus de build, afin de garder un environnement de développement clair. La compression en gzip ne nécessite que quelques modifications sur vos serveurs. La mise en place d’une stratégie de cache optimale par contre nécessite à la fois des modifications sur le processus de build et sur la configuration des serveurs. Toutes ces modifications sont relativement peu couteuses à mettre en place et ne dépendent aucunement ni de la technologie utilisée pour le front-end, ni de celle utilisée pour le back-end. Elles peuvent être mise en place quelle que soit votre stack technique. Il n’y a donc plus aucune raison pour ne pas les déployer dès aujourd’hui. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Cache , HTTP , Performance , Web , Web Front-End , Webperf . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Concaténation, Compression, Cache” François 04/03/2015 à 22:11 Thanks for the article, a fast introduction for webperf beginners. \r\n\r\nHowever, I don't think I'd put these 3 items on top of the webperf checklist, and I think you should mention that there may be more important things to do. There is a lot of litterature on the subject, and even entire conferences. A good starting point is Google PageSpeed Insights (https://developers.google.com/speed/docs/insights/rules).\r\n\r\nI wish you'd add more details about the usual tool chain to do concatenation, because \"your build process can easily take care of that\" is a very big shortcut hiding a lot of complexity.\r\n\r\nAnd fortunately, HTTP/2 will soon make all that irrelevant. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-02-24"},
{"website": "Octo", "title": "\n                Volatile packaging container with Docker            ", "author": ["Arnaud Mazin", "François-Xavier Vende"], "link": "https://blog.octo.com/en/volatile-packaging-container-with-docker/", "abstract": "Volatile packaging container with Docker Publication date 09/02/2015 by Arnaud Mazin , François-Xavier Vende Tweet Share 0 +1 LinkedIn 0 We’ve been discussing a lot about Docker as a mean to build and expose Web applications, here is another way to use it as a native distribution package building tool. You want to product deb and rpm package for applications written in Ruby, Nodes.js, Python… These technologies usually rely on specific packaging tools which need internet to work (gem for Ruby, npm for Node.js, pip or easy_install for Python). Having a compiling chain or an internet access on a production server is bad for security reason so you must use a specific server. It’s difficult to maintain different compiling chains on a single server. You have to manage different versions of different kinds of technologies. Moreover, provisioning a server just for that need has a cost you can’t always afford. The solution is to use container. One container embeds one compilation chain. Docker can help you for that. An other difficulty concerns the production of the rpm and deb packages. Some tools exist to do it simply like the Opscode’s Omnibus framework. Our example combine Docker and Opscode’s Omnibus framework. Let’s dig into more details by building a Capistrano package as a DEB (for Ubuntu Precise) and as a RPM (for Centos 6). Capistrano is a CLI Ruby application with some gems as prerequisites. Build Omnibus images First we need to build generic omnibus-enabled images for each distro we want to manage. Here are the Dockerfiles to use : # Ubuntu precise Dockerfile (into omnibus-precise dir)\r\nFROM ubuntu:precise\r\n\r\nRUN apt-get update\r\nRUN apt-get -y install -y vim git-core curl\r\nRUN gpg --keyserver hkp://keys.gnupg.net --recv-keys D39DC0E3\r\nRUN curl -sSL https://get.rvm.io | bash -s stable --ruby=2.1.5\r\nRUN /usr/local/rvm/bin/rvm-shell -c \"gem install omnibus\"\r\nRUN cd /root && git clone -b omnibus/3.2-stable https://github.com/opscode/omnibus-software.git && cd omnibus-software && /usr/local/rvm/bin/rvm-shell -c \"gem build *.gemspec && gem install *.gem\" && cd .. && rm -rf omnibus-software\r\nWORKDIR /root\r\nENTRYPOINT [\"/usr/local/rvm/bin/rvm-shell\"] # CentOS 6 Dockerfile (into omnibus-centos6 dir)\r\nFROM centos:centos6\r\n\r\nRUN yum install -y which tar yum-utils git rpm-build\r\nRUN gpg2 --keyserver hkp://keys.gnupg.net --recv-keys D39DC0E3\r\nRUN curl -sSL https://get.rvm.io | bash -s stable --ruby=2.1.5\r\nRUN /usr/local/rvm/bin/rvm-shell -c \"gem install omnibus\"\r\nRUN cd /root && git clone -b omnibus/3.2-stable https://github.com/opscode/omnibus-software.git && cd omnibus-software && /usr/local/rvm/bin/rvm-shell -c \"gem build *.gemspec && gem install *.gem\" && cd .. && rm -rf omnibus-software\r\nWORKDIR /root\r\nENTRYPOINT [\"/usr/local/rvm/bin/rvm-shell\"] We can now build our Docker images: $ cd omnibus-precise\r\n$ docker build -t omnibus:precise .\r\n$ cd ../omnibus-centos6\r\n$ docker build -t omnibus-centos6 .\r\n$ cd .. At this point, we now have two ready-to-use images. The way they have been built (with rvm in this case) doesn’t really care as soon as you have both the omnibus command line and the omnibus-software gem installed. The omnibus-software gem we manually installed contains «recipes» to package some very common software stacks (PHP, Java, Ruby, Node, Python, RabbitMQ, Redis…). Create a new omnibus project We can now use the freshly built images to create an empty omnibus project. We simply mount a local volume to the container to keep the project from being destroyed with the temporary container. $ mkdir project\r\n$ docker run --rm -ti -v $(pwd)/project:/root/project omnibus:precise\r\nroot@743bc3900afb:~# cd project\r\nroot@743bc3900afb:~/project# omnibus new capistrano\r\n      create  omnibus-capistrano/Gemfile\r\n      create  omnibus-capistrano/.gitignore\r\n      create  omnibus-capistrano/README.md\r\n      create  omnibus-capistrano/omnibus.rb\r\n      create  omnibus-capistrano/config/projects/capistrano.rb\r\n      create  omnibus-capistrano/config/software/c-example.rb\r\n      create  omnibus-capistrano/config/software/erlang-example.rb\r\n      create  omnibus-capistrano/config/software/ruby-example.rb\r\n[...]\r\nroot@743bc3900afb:~/project# We now have an omnibus structure to host our capistrano package project. We just have to write a few files to make this work. We can either do this within the current container or into the host. Customize the omnibus project # omnibus-capistrano/config/projects/capistrano.rb\r\nname 'capistrano'\r\nmaintainer 'Arnaud'\r\nhomepage 'http://octo.com'\r\ninstall_dir     '/opt/capistrano'\r\nbuild_version   '3.3.3'\r\nbuild_iteration 1\r\n# creates required build directories\r\ndependency 'preparation'\r\n# capistrano dependencies/components\r\ndependency 'capistrano'\r\n# version manifest file\r\ndependency 'version-manifest'\r\nexclude '\\.git*'\r\nexclude 'bundler\\/git' # omnibus-capistrano/config/software/capistrano.rb\r\nname \"capistrano\"\r\ndefault_version \"3.3.3\"\r\n\r\ndependency \"ruby\"\r\ndependency \"rubygems\"\r\n\r\nrelative_path \"capistrano\"\r\n\r\nbuild do\r\n  env = with_standard_compiler_flags(with_embedded_path)\r\n  gem \"install capistrano\" \\\r\n      \" --version '#{version}'\" \\\r\n      \" --no-ri --no-rdoc\" \\\r\n      \" --bindir '#{install_dir}/bin'\", env: env\r\nend The dependency statement refers to compilation/installation recipes defined into omnibus-software that we reuse. Your packages are ready to get built. However, you may want to add few more tweaks into postint and postrm scripts. In this example, we simply symlink cap to get it in the regular PATH. #!/bin/bash\r\n# omnibus-capistrano/package-scripts/capistrano/postinst\r\nPROGNAME=$(basename $0)\r\nfunction error_exit\r\n{\r\n  echo \"${PROGNAME}: ${1:-\"Unknown Error\"}\" 1>&2\r\n  exit 1\r\n}\r\nln -s /opt/capistrano/bin/cap /usr/bin || error_exit \"Cannot link cap to /usr/bin\"\r\nexit 0 #!/bin/bash\r\n# omnibus-capistrano/package-scripts/capistrano/postrm\r\nrm /usr/bin/cap\r\nexit 0 Use your project At this stage, you can build your packages: $ cd omnibus-capistrano\r\n$ docker run --rm -v $(pwd):/root/project/ -w /root/project omnibus:precise -c \"omnibus build capistrano\"\r\n$ docker run --rm -v $(pwd):/root/project/ -w /root/project omnibus:centos6 -c \"omnibus build capistrano\" It’s going to take a while (several minutes) to recompile the whole bunch of softwares needed. The volatile Docker containers created will die at the end of their runs and leave a clean host. After both runs, you will find your brand new packages ready to be deployed: $ ls -lh pkg/\r\ntotal 40M\r\ndrwxrwxr-x 2 arno 4,0K  5 déc.  20:17 ./\r\ndrwxr-xr-x 6 arno 4,0K  8 déc.  18:05 ../\r\n-rw-rw-r-- 1 root  21M  5 déc.  20:02 capistrano_3.3.3-1_amd64.deb\r\n-rw-rw-r-- 1 root  574  5 déc.  20:02 capistrano_3.3.3-1_amd64.deb.metadata.json\r\n-rw-rw-r-- 1 root  20M  5 déc.  20:17 capistrano-3.3.3-1.el6.x86_64.rpm\r\n-rw-rw-r-- 1 root  571  5 déc.  20:17 capistrano-3.3.3-1.el6.x86_64.rpm.metadata.json Packages are pretty fat (around 20 megs). Conclusion This kind of approach is pretty interesting to keep the ugly / complicated building stuff confined into a volatile Docker container. At the end of the building process, the container is wiped and the hosting system remains clean. You therefore get a ready-to-go native distribution package with no further Internet access required. On the Omnibus side, the main benefit of such a solution is that it addresses several distributions at the same time. You just have to build one Docker image per-distro to ensure the portability of your apps. You can use pkgr exactly the same way. The main difference between thoses tools is that pkgr is faster but produces packages that have a few dependencies with system packages whereas omnibus embraces the no-dependencies paradigm. On the Docker side you can begin to use it even if your organization is not yet ready to get it up to production. You’re now ready to integrate the building process into an CI tool such as Jenkins , add the produced packages to a repo (YUM, APT or even Nexus). Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2015-02-09"},
{"website": "Octo", "title": "\n                N°1 betting operator in Europe lets you dive in an immersive horse race            ", "author": ["Vincent Guigui", "Nelly Grellier", "Muriel Caron"], "link": "https://blog.octo.com/en/n1-betting-operator-in-europe-lets-you-drive-in-an-immersive-horse-race/", "abstract": "N°1 betting operator in Europe lets you dive in an immersive horse race Publication date 30/12/2014 by Vincent Guigui , Nelly Grellier , Muriel Caron Tweet Share 0 +1 LinkedIn 0 What if you could race like a jockey ? It is now possible with “ LeTrot 360 Digital Immersion “, an immersive experience created by PMU (N°1 parimutual operator in Europe) and OCTO Technology! On the 25th november, our team was proudly on stage during the “J-60 Grand Prix d’Amérique” Countdown Gala to provide guests an immersive horse racing experience using Oculus Rift virtual reality headset. Fine example of Open Innovation: PMU, LeTrot in partnership with Radio France and supported by OCTO launched this experience, called “ Letrot 360 Digital Immersion “, which allows the user to virtually become driver (jocke) during a race surpassing the limits of realism using advanced 360° sound and visual immersion . This digital immersion was made possible by OCTO team collaboration on 360 capture, Oculus Rift expertise and dedicated sound spatialization algorithms. “ The contribution and the technological expertise of OCTO Technology on innovative interfaces enabled us to create, in record time, this incredible immersive experience of horse racing using Oculus Rift ” says Gérard Beaufort, Chief of Innovation and PMU Lab. This event operation is part of the PMU’s business 2020 strategy, which aims to define a new model of development and growth by PMU touching new categories of bettors especially among new generation using their attractiveness for digital innovations. In the press: Press release download (FR) http://blogs.lexpress.fr/nos-vies-numeriques/2014/11/26/un-experience-inedite-de-realite-virtuelle/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged Experience , Immersion , innovation , Marketing , NUI , Oculus Rift . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-12-30"},
{"website": "Octo", "title": "\n                Discovering CoreOS            ", "author": ["Gabriel Guillon"], "link": "https://blog.octo.com/en/discovering-coreos/", "abstract": "Discovering CoreOS Publication date 17/11/2014 by Gabriel Guillon Tweet Share 0 +1 LinkedIn 0 OS choice for a VM in cloud is usually between the old bare metal’s one : Ubuntu, RHEL, CentOS. They are doing the job but I’m pretty sure the can job can be done in a better way. Isn’t there a more suitable OS for VMs in a Cloud environment ? An OS on a VM is usually treated the old way: install your favorite OS, make an image of it, clone that image. OS update ? Package management? Application deployment ? As usual, as before : use your favorite package manager and app deployer, and orchestrate them with your favorite orchestrator. But VMs in cloud are not as your good old servers : they are many, they are volatile, and there is Docker and such. You feel that your old tools are less suited for those particular OS running in datacenters, especially if you want them to be scalable. And then comes CoreOS. Based on Chrome OS , the first image was released on 07/25/2013 . It aims to be an OS that runs on data centers, hosting Docker containers. Indeed, CoreOS is specialized for that task. Here are the promises : Minimalistic : the OS is tailored to run only needed software for running containers, which will host your applications. Reliable : we’ll talk about that later, but let’s say for now that update is done automatically. You can run CoreOS on any virtualisation platform and cloud provider you can think about. Forget iso images, all is done through network. The documentation will guide you through install process. The good Install is easy, and you ends up with a running OS in no time. Looking at ps output : 1 ?        Ss     0:01 /usr/lib/systemd/systemd --switched-root --system --deserialize 19\r\n  372 ?        Ss     0:00 /usr/lib/systemd/systemd-journald\r\n  398 ?        Ss     0:00 /usr/lib/systemd/systemd-udevd\r\n  449 ?        Ss     0:00 /usr/lib/systemd/systemd-logind\r\n  453 ?        Ss     0:00 /usr/sbin/ntpd -g -n -u ntp:ntp -f /var/lib/ntp/ntp.drift\r\n  460 ?        Ss     0:00 /usr/bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation\r\n  473 ?        Rsl    0:14 /usr/sbin/update_engine -foreground -logtostderr -no_connection_manager\r\n  478 ?        Ss     0:00 /usr/lib/systemd/systemd-resolved\r\n  510 ?        Ssl    0:00 /usr/lib/locksmith/locksmithd\r\n  554 ?        Ssl    0:00 /usr/bin/etcd\r\n  555 tty1     Ss+    0:00 /sbin/agetty --noclear tty1 linux\r\n  559 ?        Ssl    0:03 /usr/bin/fleetd\r\n  661 ?        Ss     0:00 /usr/lib/systemd/systemd-networkd As said before, CoreOS is about Cloud and managing a large fleet of servers. As such, one VM is not enough, and you are encouraged to run at least three. But what the need of three VM, even shipped with Docker? It seems like I have … three VM running independently, as I had when I installed Ubuntu, Centos or such. And here comes the strengths of CoreOS : etcd and fleet . Both are developed by the CoreOS team, but can be compiled and used on other Linux distribution. Here is a sneak peak of the CoreOS architecture : etcd etcd is a distributed , resilient and highly available (through Raft consensus ) key-value store. Its goal is to share data between all instances of CoreOS, provided that they run etcd. Posting and retrieving data to etcd is just a matter of using the RESTful API or one of its clients. Note that etcd runs on host , not within a container. It has nice features, such as : TTL on entry or directory , waiting for an entry to change and history of those changes , key ordering to simulate a queue , hidden directories , statistics on etcd cluster or node , and more . With such a thing, it becomes possible to share configuration across all hosts, without the need of a third party software. curl -X PUT -L http://127.0.0.1:4001/v2/keys/hello -d value=\"world\"\r\n{\"action\":\"set\",\"node\":{\"key\":\"/hello\",\"value\":\"world\",\"modifiedIndex\":9664,\"createdIndex\":9664}} core@core-03 ~ $ curl -L http://127.0.0.1:4001/v2/keys/hello\r\n{\"action\":\"get\",\"node\":{\"key\":\"/hello\",\"value\":\"world\",\"modifiedIndex\":9513,\"createdIndex\":9513}} But you can also use etcdctl : core@core-03 ~ $ etcdctl set /hello you\r\nyou core@core-02 ~ $ etcdctl get /hello\r\nyou Okay, now I have hosts sharing data. I have Docker containers that can run on CoreOS hosts. But they are quite “stuck” to these host, don’t you feel? CoreOS seems versatile to me, I can install it everywhere in a snap but why are my Docker containers so stuck to a VM? fleet fleet controls the systemd daemon of CoreOS instances via d-bus, and is controlled via sockets (Unix or network). It uses etcd (default to localhost) to communicate with others fleet daemon on other CoreOS hosts. Systemd is an alternative to the System V init daemon, and does much more. Describing it is outside of the scope of this article, but remember that it replaces bunch of shell scripts by “configuration” files, called Units. Fleet helps you deploy units (and docker containers could be units) given several distribution strategies: Some units can be spread over every fleet-enabled hosts by the etcd cluster (they are named global units) Some rules can prevent two units from running on the same host (can make sense for H/A) Some rules enforce that two units must be running on the same host Some units can only be run on specific tagged hosts. Each fleed daemon can be tagged with several labels to help categorize them, given functional or geographic criteria (metadata). Those tags can be injected by cloud-config. Basically, when you do nothing specific, if you ask fleet to run an unit, it runs on the CoreOS you are on. Power off (or destroy) that host and the unit (a.k.a your service) will be ran on another host without explicit scheduling maneuver on our side. Of course, you can know where the units are. Lets introspect our fleet cluster : core@core-01 ~ $ fleetctl list-machines\r\nMACHINE         IP              METADATA\r\n1aaab8e5...     172.17.8.102    -\r\n379ba5b7...     172.17.8.101    -\r\n57d0cc63...     172.17.8.103    - Here is my unit file, this is the example provided in CoreOS doc: core@core-01 ~ $ cat m.service\r\n[Unit]\r\nDescription=MyApp\r\nAfter=docker.service\r\nRequires=docker.service\r\n\r\n[Service]\r\nTimeoutStartSec=0\r\nExecStartPre=-/usr/bin/docker kill busybox1\r\nExecStartPre=-/usr/bin/docker rm busybox1\r\nExecStartPre=/usr/bin/docker pull busybox\r\nExecStart=/usr/bin/docker run --name busybox1 busybox /bin/sh -c \"while true; do echo Hello World; sleep 1; done\"\r\nExecStop=/usr/bin/docker stop busybox1 Let’s start it: fleet start m.service Let’s see where it runs: core@core-01 ~ $ fleetctl list-unit-files\r\nUNIT            HASH    DSTATE          STATE           TARGET\r\nm.service       391d247 launched        launched        379ba5b7.../172.17.8.101 172.17.8.101 is the IP of the host I’m on. Let’s kill it: Wait a bit, then: core@core-02 ~ $ fleetctl list-unit-files\r\nUNIT HASH DSTATE STATE TARGET\r\nm.service 391d247 launched launched 1aaab8e5.../172.17.8.102 My service has migrated ! But not its data. For this, you can check out flocker , dedicated to data migration in container environments. Okay, my containers are moving, I can play with them (suddenly, I want to put CoreOS on dozens of Raspberry Pi …), but what can I do with the host? Automatic update Short answer : nothing else. Long answer : and this is not the goal of CoreOS. CoreOS is for you to play with containers. The CoreOS team wants you to concentrate on your containers, and they deal with the host. You don’t have man pages, nor package managers. You feel powerless, don’t you? Actually, there is exactly what you need, no more, no less, and update is made automatically and transparently. All the bricks are here for that : you play with containers, CoreOS handle everything else. You just give it a few hints : reboot as the update is made, reboot all hosts one by one, don’t reboot. Remember that your containers are being moved automatically be fleet (provided you use fleet). OS updates are made by swapping between an active and a passive partition : the update is made on the passive one. On reboot, this is the one used. The doubts What if all your CoreOS don’t have access to Internet, as in a private cloud ? Could it be updated ? Could it even work? As CoreOS schedules updates automatically, do you have to get rid of Chef / Puppet? I also feel uncomfortable that a company owns my Linux distribution, especially when this company makes the update by itself. What if the company fails? Or the repos are hacked? Even if updates are said to be signed, I don’t like the idea of depending on a single entity for my updates. It remembers me the questions and fears I had about RHN , which, with time, are not justified anymore… What CoreOS corp provides? As a company, what can I expect from CoreOS? I’ll dive into those questions in a future article. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-11-17"},
{"website": "Octo", "title": "\n                Web Scale IT (Or, The Patterns of the Giants of the Web) in Gartner’s Top 10 for 2015            ", "author": ["Stephen Périn"], "link": "https://blog.octo.com/en/web-scale-it-or-the-patterns-of-the-giants-of-the-web-in-gartners-top-10-for-2015/", "abstract": "Web Scale IT (Or, The Patterns of the Giants of the Web) in Gartner’s Top 10 for 2015 Publication date 28/10/2014 by Stephen Périn Tweet Share 0 +1 LinkedIn 0 Gartner’s long-awaited Top 10 strategic technological trends  for 2015 1 have just been released this month and, for the second consecutive year 2 , one our most cherished topics at OCTO is prominently featured. In Gartner terminology, what we call the Practices of the Giants of the Web 3, 4 becomes Web-Scale IT , and is inspired by “ large cloud services firms “ 5, 6 . Here, we offer a brief overview of this trend for 2015. Definition, source and scope of Web-Scale IT Let’s start off by defining Web-Scale IT. Find below the meaning of the concept according to Gartner and read on for their rationale behind it: « What is web-scale IT ? It’s our effort to describe all of the things happening at large cloud services firms such as Google, Amazon, Rackspace, Netflix, Facebook , etc., that enables them to achieve extreme levels of service delivery as compared to many of their enterprise counterparts . » 5 « Large cloud services providers such as Amazon, Google, Facebook , etc., are re-inventing the way IT in which IT services can be delivered . » 2 It stands to reason that the astonishing IT capabilities delivered by the Giants of the Web, and their re-invention of IT, are what pushed Gartner to investigate their practices. Taken from that angle, the term Web-Scale IT used by Gartner is quite explicit: it’s about a new way of delivering IT services inspired by the practices of the leaders of the Web and operated at a worldwide web scale. Now that the concept and its origins having been made explicit, what are the practices within the scope of Web-Scale IT? Gartner’s research note 7 dedicated to Web-Scale IT mainly focuses on the competitive practices of these large cloud service operators. A large portion of the patterns we detail in our book, “ The Giants of the Web ”, are therefore also featured in Gartner’s list. Most notably: Commodity Hardware and scale-out Open API et API first NOSQL solutions and eventual consistency Feature teams 2-Pizza teams DevOps Design for failure Gartner nevertheless places great emphasis on DevOps practices (within Web-Scale IT ) probably due to a scope focused more on infrastructure-oriented practices of the “large cloud services firms”, whereas “The Giants of the Web” also includes practices rooted in the Lean Startup approach (cf. MVP , Lean Startup …). Finally, and for aficionados of Gartner ‘s predictions, the one concerning this approach states that “ by 2017 Web-Scale IT will be an architectural approach found operating in 50% of global enterprises , up from less than 10 percent in 2013″ 8 . Conclusion The underlying idea of Web-Scale IT is that traditional companies can be inspired to adopt some of the relevant, competitive IT practices of the Giants of the Web at organizational, cultural, or technical levels in order to improve their TTM, agility, and IT capabilities in general. The name Web-Scale IT , proposed and promoted by such a first-class opinion leader as Gartner, will certainly become broadly diffused in our industry; we will no doubt use it ever frequently in the future. For our part, we will closely follow further developments of Web-Scale IT , and its diffusion in our industry, all the while continuing to contribute to it – at our own scale! Sources [1]  Gartner, Gartner Identifies the Top 10 Strategic Technology Trends for 2015 , Press Release, 8 octobre 2014. [2]  Gartner, Gartner Identifies the Top 10 Strategic Technology Trends for 2014 , Press Release, 8 octobre 2013. [3] geantsduweb.com (in French. English version in progress) [4]  OCTO, Les Géants du Web, Culture – Pratiques – Architecture , ouvrage collectif, novembre 2012. [5]  Cameron Haight, Enter Web-scale IT , Gartner, 16 mai 2013. [6]   Free preview of Gartner research, Use Web-Scale IT to Make Enterprise IT Competitive With the Cloud , Gartner.com, 15 mai 2013. [7]   Cameron Haight, Daryl C. Plummer, Use Web-Scale IT to Make Enterprise IT Competitive With the Cloud , ID: G00250754,  Gartner, 10 p.,  15 mai 2013. [8]   Gartner, Gartner Says By 2017 Web-Scale IT Will Be an Architectural Approach Found Operating in 50 Percent of Global Enterprises , 21 mai 2014. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles , News and tagged GAFA , Giants of the Web , Web-Scale IT . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-10-28"},
{"website": "Octo", "title": "\n                Static Analysis Tooling for C# and .NET, NDepend in Depth            ", "author": ["Zakaria El Marouri", "Vincent Guigui"], "link": "https://blog.octo.com/en/static-analysis-tooling-for-net-ndepend-in-depth/", "abstract": "Static Analysis Tooling for C# and .NET, NDepend in Depth Publication date 07/07/2014 by Zakaria El Marouri , Vincent Guigui Tweet Share 0 +1 LinkedIn 0 As a software architect I often have to analyze many applications code in order to perform a quality check. Is the code looking good? What about its complexity and test coverage? Can I consider the code as maintainable with a good scalability? Of course I won’t spend my whole time reading each source file, it would be too long and for sure useless. Hopefully a set of rules and tools can help if you are in this same situation. For those using . NET as their main technology, you should check out each of these tools and see how they can improve your work experience: Sonar is a debt analyzer and supports more than 25 languages ReSharper is a productivity enhancer that offers code analysis and many more features JustCode is also productivity enhancer FxCop performs static analysis following best practices and recommendations NDepend offers static code analysis for .Net  and focuses on relations between objects Each of them has its pros and cons and has its specific field of use thus this is why in some cases Sonar can be a better choice than NDepend or ReSharper and vice versa! In this article I will focus on NDepend. What is NDepend? NDepend is a static code analysis for .NET .  It’s a commercial tool and you can try it for 14 days freely. Patrick Smacchia created NDepend in 2004. NDepend can be used in both standalone and Visual Studio integrated mode. Personally I prefer Visual Studio Integrated mode since it has all the features and more, plus you can use your IDE features. NDepend gives you a quick access to many metrics of your code and illustrates each metric or other aspect, i.e. assembly dependency, through graphical representation. This is a real plus! For me the main feature of NDepend is its query language it is built upon: CQLinq. CQLinq , meaning Code Query LINQ , is a query language that you can run on an abstracted model of your code. Basically you can query your code the same way you query views or tables from a database. For instance to get all methods having more than 30 lines of code I would simply open the CQLinq editor and write: from m in Methods where m.NbLinesOfCode > 30 select m In Visual Studio you will get this view, side by side the query and its graphical representation in the so called Code Metrics view : Thanks to CQLinq there is not much left to do, all metrics can be translated into simple CQLinq queries and this is why NDepend is very powerful! Also NDepend offers all its logic in an API that you can find in the lib folder of its installation: NDepend.API.dll You can use NDepend API to build your own project analysis tool. For that, all you need is to include the NDepend.API.dll file in your project and then make a reference to the needed namespaces. For instance here is a piece of code that shows what you can do with this API. from m in Methods\r\nwhere m.NbLinesOfCode > 30\r\norderby m.ILCyclomaticComplexity descending\r\nselect new { m } Can easily be deported into a C# function of your project: using NDepend.CodeModel;\r\nusing NDepend.CodeQuery;\r\n\r\npublic List GetComplexMethods(ICodeBase codeBase)\r\n{\r\nvar complexMethods = (from m in codeBase.Application.Methods \r\n                      where m.NbLinesOfCode > 30\r\n                      orderby m.ILCyclomaticComplexity descending\r\n                      select m).ToList();\r\n\r\nreturn complexMethods;\r\n} NDepend API documentation is accessible online . Now all what you need is a bit of imagination to build the perfect analysis tool that fits your needs. Built-in NDepend Metrics NDepend comes with many preconfigured queries to match most of the common metrics rules. To explore the predefined rules open the rules explorer view: And then you get this panel open: Actually around 240 rules and more are already defined for you and ready to use. However you should pay attention to what is defined. For instance according to NDepend rules here is the filter that defines a method to refactor and probably you can consider the filters as too restrictive or not enough: from m in  JustMyCode.Methods\r\n           where m.NbLinesOfCode > 30 ||\r\n                 m.CyclomaticComplexity > 20 ||\r\n                 m.ILCyclomaticComplexity > 50 ||\r\n                 m.ILNestingDepth > 5 ||\r\n                 m.NbParameters > 5 ||\r\n                 m.NbVariables > 8 ||\r\n                 m.NbOverloads > 6\r\nselect new { m } Even though NDepend rules are quite close to what we could consider as baseline rules I strongly recommend that you spend a bit of time sharpening the pre-built rules or write your own queries! You just have to create your custom group and then create your queries. Now you should be asking yourself what the heck is JustMyCode? Don’t panic you are asking the good question! JustMyCode is just a filter that eliminates generated code that comes when using frameworks like Entity or any other. Since it makes no real sense to check the quality of the code generated by other tools, JustMyCode should always be used in each of your queries. You can extend the definition of JustMyCode by doing so: notmycode\r\nfrom m in Application.Namespaces.WithNameLike(\"NameSpace.Of.GeneratedCode\").ChildMethods() select m This way I say that all the code within the “NameSpace.Of.GeneratedCode” namespace is to consider as generated code. You can also filter by attributes, file names, naming convention, inheritance or whatever you need, very easy! By default NDepend considers as generated code contained in designer.cs files, in InitializeComponent method and few others like this. Graphical Views You have four graphical views in NDepend. Two of them actually focus on the same aspect of dependency. The third one is the metric code view that I quickly spoke about in the beginning. And the fourth one is simply the feature that helps you export the result of your CQLinq query into a graph. Side by side code dependency matrix view and graph view: If you are not familiar with these graphics don’t be frustrated, usually no body does! So NDepend has tooltips to help you understand the various values. Basically the matrix and the graph both help you to figure out how assemblies, namespaces, classes, methods depend on each other’s. The graph view is more useful to show out a “spaghetti code” and the matrix view is more suited to check whether or not the code follows the principle of low coupling and high cohesion. So most of the time it’s better to analyze both graph and matrix. I did not understand why the lines were so curvy in the dependency graph, even for small amount of nodes you get curvy lines instead of straight lines.  I don’t think that it will make a huge difference in your daily life but I don’t see the point with all these curves, for me it adds unnecessary confusion and I would prefer to see straight lines most of the time. Look by yourself: Here is what NDepend developer team said about it: The graph algorithm comes from an older version of MsAgl. Actually we don’t find the result that over-curly, but anyway, Graph is an important feature that will face major enhancements in the mid-term Next, the metrics view displays proportionate blocs of different size according to the metric you selected. It can be number of lines of code, number of IL instructions or any option in the drop down: The metric view used to give me headaches because when changing to # lines of code I had this message many times: So as you can see even though the message is quite clear, it’s not helpful since it doesn’t tell you more about what is actually missing! In case your are in the same situation, check manually that each  project is configured to generate the pdb files in the assembly directory. Continuous Integration You can easily set up NDepend to be part of your build server in your continuous integration process. For that can either configure the command line as a build task or get the ready to use extension NDepend TFS 2010 integration on Codeplex . NDepend is compatible with CCNet, TFS, Finalbuilder, Team City and probably many more, read about it here . Open Source I introduced NDepend by saying that it is a commercial tool. It’s not false but NDepend API is completely free to use in any of your project. Actually NDepend comes with a command line tool called “NDepend.PowerTools” and its source code: As it is said in the documentation: NDepend.PowerTools are a set of short open-source static analyzers, packed into the Visual Studio solution So you are free to extend the code for your own usage. One very cool feature of the PowerTools is the code duplicate detector. I think that this feature should be fully integrated in NDepend UI and hopefully here is what NDepend said about it: Certainly code cloning detection is a future flagship feature Documentation NDepend documentation is complete and up to date. It covers all the features of the tools and have many screenshot and examples. It also comes with a 3 minutes introduction video. But unfortunately the video is recorded for the version 4 of NDepend and the current release is the version 5. Also I think that Patrick Smacchia could have done better by recording more videos and as the developer team said: More videos is definitively something we need to offer and will offer Other features Beyond all these good features I already introduced, NDepend also offers: Static HTML report Test coverage analyzer Continuous analysis Cirtical rules build failure What is missing in NDepend? For me NDepend is a great tool but still need some improvements. Actually you can’t miss that nowadays apps combine different languages. Most of the time your app will be like a set of C#, or whatever your drug is, and JavaScript lines of code and for this reason your code analysis will for sure have to be polyglot or at least bilingual. And since NDepend does not understand JavaScript, you will have to get other tools to analyze all your code. This can be a killer point for some users who would prefer a tool like Sonar which support different languages. For instance in a C# ASP.NET SPA application, most of the code will probably be written in JavaScript and so I will have to complete NDepend C# report by using another tool. And then the second missing feature would be the debt analyzer report . Patrick Smacchia has chosen to focus NDepend on facts and metrics from source code, which is good and an objective point view. But I think that a smarter interpretation of the analysis through a debt report would be very appreciated by many developers. We can excpect some improvement here according to Patrick Smacchia: Same as code cloning, debt analysis is a future flagship feature Final Word As a .Net user, I found NDepend very powerful and quite easy to use. I really think that the CQLinq language and the NDepend library are actually the best features of the tool since I did not find any equivalent. NDepend team is very active and we can expect many good features in the next releases. Now just try it here Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged c# dead code , code analysis; c# , complex method c# , dependency matrix C# , NDepend analysis , static analysis of C# , static code analysis , static code analysis. NET . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Static Analysis Tooling for C# and .NET, NDepend in Depth” Gagannarang 09/04/2018 à 08:38 Thank you for sharing for such valuable information with us. It is going to help me with some extent.Can you tell me that if i can use NDepend for C language? Waiting for your reply..! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-07-07"},
{"website": "Octo", "title": "\n                Flocker overview            ", "author": ["Arnaud Mazin"], "link": "https://blog.octo.com/en/flocker-overview/", "abstract": "Flocker overview Publication date 03/11/2014 by Arnaud Mazin Tweet Share 0 +1 LinkedIn 0 If you’ve started using Docker for a while you might be asking the same question «how to manage a group of Docker containers into a consistent topology?». On your local workstation, projects such as fig simply help you do the job: local, fast, easy. Flocker gets a step further by adding some major features: Multi-hosts management :  containers can be spread onto several Docker hosts, called nodes , ZFS-based shared filesystem : store your non-volatile storages onto it to deal with high-availability. The magic in Flocker network proxy design allows you to talk to any of the nodes to reach any container. The price is that exposed ports must be unique per container to avoid conflicts. Flocker relies on two YAML files describing The applications involved into the topology (think a DB server, an app server, links and so on): application.yaml A static definition of the containers location (on which node they run): deployment.yaml Flocker YAML files are declarative . Flocker will perform the needed operations to comply with the YAML files, and nothing if it is all set-up. To move a container from a host simply modify the location of the container into deployment.yaml and rerun Flocker. Actually, container motion is not the correct term as containers are destroyed on a node and then re-created on another one. You may really have a look at two Vagrant-based tutorials to understand the basics of Flocker: A simple single MongoDB tuto explaining a container motion and the need for data persistence, An ELK example , more advanced which involves the famous ElasticSearch, LogStash and Kibana stack. Many questions remain about flocker: How hard is it to setup the underlying multi-host ZFS configuration? Do we need Puppet/Chef/SaltStack/Ansible to do so? How to handle container pools (as in fig scale)? How fast? Live Migration? Automatic Container start / restart? However, it’s totally worth it having a look. I bet some of its ideas might probably be reused into other projects (such as Kubernetes ). Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations , News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-11-03"},
{"website": "Octo", "title": "\n                Get an instant access to up-to-date metrics at low cost            ", "author": ["Grégory Bataille"], "link": "https://blog.octo.com/en/get-an-instant-access-to-up-to-date-metrics-at-low-cost/", "abstract": "Get an instant access to up-to-date metrics at low cost Publication date 25/09/2014 by Grégory Bataille Tweet Share 0 +1 LinkedIn 0 Bank’s current systems and issues Today, Retail and Private Banks’ Core Banking System (CBS) and Portfolio Management System (PMS) are strong assets. They have matured over the years and are often a very solid basis for the rest of the satellite IT systems. They are used to efficiently manage the basic core banking data, like clients, portfolios, their security composition, the pending orders, the market transactions and so on. Portfolio-level and bank-level consolidated metrics however are often based on long running algorithms and are therefore executed either during end of day batches or on-demand, meaning users’ have to work with more or less outdated data or data that is long to get. The issue is that in recent years, banking has seen several shifts: An ever faster market (more actors, algorithmic trading, easier access to information) A growing pressure on returns. Clients want reliability, while banks need to lower their margins due to competition A complexification of the products to try and manage risks while meeting clients growing sophistication The increasing regulatory pressure impacting margins, efficiency and attractiveness All of this has underlined an increasing need for up-to-date consolidated information (exposures, performance, variance, volatility, …) at different levels (position, portfolio, desk, region, bank). While some of this complexity has already been addressed in Investment Banks (IB), Retail and Private Banking have lagged behind, mostly because the need was initially not identified as so important but more importantly because they lack the resources. Indeed the IB solutions are generally plagued by a very high TCO (Total Cost of Ownership) that is neither justified nor bearable for other kinds of banks. An example – Portfolio variance One of the core metric used by portfolio risk managers is the portfolio variance. To calculate a portfolio variance over a time period, one needs to compute the variance of each of the securities held on the time period, their correlation with one another ( covariance ) and to aggregate them based on the security relative weights in the portfolio. Because this metric needs to integrate a lot of heterogeneous values (numerous security prices, portfolio compositions, forex rates) it generally takes seconds, if not minutes, to compute on classical systems (in some big institutional portfolio scenarii, it even happens that the CBS or PMS cannot do the computation). This is because A lot of data needs to be retrieved The dataserver, generally a unique central database, is usually suffering from a lot of contention since it supports the whole operational work of all the banks’ users And the computation itself requires a lot of operations, which puts strain on the application server (again, oftentimes, there is only 1 or 2 of those for the entire user population) Thus, running such heavy computations on the online system adds an unmanageable additional level of contention, lags, slownesses for the users (and sometimes directly for the clients who access data through the e-banking portal for example). And those are the main reasons why banks are struggling with portfolio risk management: the system is not helping the risk managers enough by providing them with the right information at the right time. Most of the attempts to solve for this problem in the operational system have failed because it either Tries to keep the database as a central component in the architecture, and it creates an unmanageable amount of contention and transactions (too much volume to process) Or it tries to load every single relevant thing in memory (several GB) at startup on a single node meaning that the server takes hours to start, and is a single point of failure Another often seen approach is to go through Business Intelligence (BI) components. It is however not perfect either, as it is mostly based on overnight computations on frozen data, which does not fit our stated need for up-to-date, near real-time metrics. Most of those issues or limitations come from choices that were made based on the available technology and the available budget at the time. But today, a lot of those parameters have changed. Led by the needs of the internet giants and the increased capacity of commodity hardware, new kinds of technologies and architectures have emerged and it is now possible to build within the operational system very efficient ways to perform heavy computations on massive amount of data. A New Hope Data access has a cost, and as is well known, typical disk solutions (SAN or NAS) are the weakest link when it comes to throughput speed, and nothing beats local RAM. Hardware throughput time The first thing is therefore to realize that the increased capacity of commodity hardware at a low premium gives access to a lot of RAM as well as accompanying local computing power for a very low cost of ownership. AIX vs Dell Cost This comparison is a shortcut that we can’t really make because you don’t get the same support or quality between the 2 products, but it gives a good appreciation of the cost difference between typical servers and commodity hardware Distributing data and computation is a direct consequence of this trend. While massively distributed processing was reserved to giants (Research institutes, major internet companies), cheap hardware now makes it available to everyone. This allows any IT department of any size to consider grid computing solutions nowadays. And in turns, this ended up with the creation of numerous distribution frameworks that give those setups a level of reliability that is expected from classical server/mainframe. Commodity hardware and new distribution frameworks therefore make grid storage and grid computing the key basis for a fast, highly available, highly reliable and cheap system. While those new concepts were key to solve the problem at hand, in our particular case, a paradigm shift in the system architecture was also necessary to achieve the best result possible. The classical paradigm is a pull model (intraday values are computed on-demand) but we are now going to look at some kind of push based one. Indeed, while with a pull model one can consider adding resources to try and bear the load, it is still very difficult to give users fluid and reactive interfaces but more importantly, it is not possible to build proactive systems that will react on thresholds because the consolidated data is just not there! This is actually not a new paradigm but it is definitely novel in the banking industry. A proposed solution for our use case Thinking about what would be an ideal world for our risk manager. We can see 2 main axis: Data visualization . The risk manager first wants to have access to an up to date dashboard containing all the KPIs he needs, as, for instance, the simplified example of the portfolio variance presented here. He also wants this to be a fast dashboard, not a background report that runs for a few minutes. But he also wants to be able to play with it and have instant return . Scenarii include changing the time window used or removing an outlier value from the KPI calculation. Here is an example of the kind of dataviz done nowadays and that users more and more expect from their systems at work. Alerting . Coming up with a system that provides always up-to-date KPIs, it is possible for risk managers to define alerts based on thresholds, or more complex criteria. They will then be able to react quickly to impacting market events. Think about volatility for example. A portfolio manager might want alerting level on volatility. In the case of long term trading, he might want to move away from a security that becomes volatile. In the case of tactical trading, he might rather wait for a high volatility time to try and trade during the peaks. Once a distributed architecture is chosen, the challenge is to make it highly available. Indeed, commodity hardware will fail, and software solutions are therefore used to cope with that. Partionning and replication are the keys here and will be provided by a software solution (in our case we chose Infinispan) that will be configured (typically the replication factor) depending on performance consideration and most importantly on the cluster topology. The second thing is about changing how systems are built in banks. Rather than staying with a single type of paradigm, best of breed architectures must be considered for each use case. Banks classically only build systems based on a pull model . They have a strong data layer, and when some consolidated data is needed, the computation is done on the fly. To improve response time, some consolidations are made and persisted in batches (often overnight) but that means the user accesses outdated information (think back on the volatility use case from before). The shift is therefore to open the information system to other architectures, and in the considered use case, to introduce a push one where consolidated values are constantly recalculated and the user just gets instant access to the latest computed data, that is generally only a few seconds old at most. Proposed Architecture This type of infrastructure combined with the following software architecture design will have several benefits: Fast access to data . Data being in memory (RAM), the latency to access it is extremely low. This allows for quick computation cycles when metrics are constantly recalculated, but it also provides fast response time for request on data that might still need to be computed on demand (it is not possible to precompute and keep up-to-date every single metric and every single scenario) Parallel processing . The grid setup enables the use of numerous CPU that can work in parallel. Moreover, the computing nodes and the data nodes are the same which means that the computation can happen on local data, without incurring any penalty due to data communication over a network (which as shown is an order of magnitude slower than local memory). Portfolio variance for example, which is based on individual instrument variance and covariance, is a good candidate for such a setup. Reactive architecture . Those KPIs are dependent upon market events. For example the portfolio variance will depend on security price, that impacts the security variance and covariance, and transactions, that impact security weights in the portfolio. Those market changes will trigger a recalculation of the metrics for the impacted portfolio (rather than simply recomputing everything on a schedule). Incremental calculations . To compute many of those metrics, a lot of operations are required. Portfolio variance will require to compute the variance of each security, then the covariance of each security with all the others composing the portfolio and so on. This is computationally intensive and would be costly if done on every impacting market change. However, a lot of those metrics can be calculated, or sometimes approximated incrementally. This means that from a KPI and a market indicator change (e.g. a security price), the new KPI value can easily be computed (a few operations only) from its old value and the parameter change. Each of the benefit of our architecture as presented above is key in addressing the needs of our use case, and their combination is what makes the end system achieve the desired efficiency. Here is what such an architecture would look like in the described volatility use case Reference data processing Near-real-time processing of live data A prototype and the learnings we extracted With these considerations of data grid, computing grid and reactive architecture in mind, we chose to try the Infinispan product (aka JBoss Data Grid). It was a trending product and had been chosen by JBoss as their main data caching product. Additionally, some of the latest grid computing features looked promising and we wanted to test it in a real life use case. We therefore implemented a prototype containing: a data-structure, a “market simulator” to fake a reuters pricing feed, and we implemented the calculation of the portfolio performance and the security variance (describing this prototype is not our point here and will be the subject of a future article). Those were interesting proof of concept since while still easy enough to implement, they required a fair amount of computation and could be easily scaled to a large number of portfolios to test our setup. We have learned several thing: Low cost architecture . This is indeed a low cost solution. We used a JBoss application server (free) with the Infinispan cache technology (free). Our example ran on a developer machine emulating a cluster of 2 servers, with great performance. There is no reason to deploy the application on highly resilient machines as Infinispan manages the replication and high availability, making the system fault tolerant. Structure the data . Organizing data on a grid means structuring it very differently to take full advantage of the distributed storage and computing. The fact that the data is serialized a lot between nodes also drives some of the choices. The goal is generally to not stream data, but only results between the nodes. The main teaching is that data organization needs to be balanced in size and in number between the nodes. In size, because with nodes of generally the same size, they should be filled equally; and in number because when distributing a computation, each node should have about the same amount of work to perform. Additionally, like any key/value NoSQL store, Infinispan imposes limits (object hierarchy, data lookup) that requires to be balanced by careful data modelization. Manage the concurrency . Obviously such systems receive a very high volume of events (Reuters can generate upwards of 100’000 notifications per second) and we have discussed that this was the main issue with a centralized database model that can’t cope with the transaction volume generated. This equally means that transactions cannot be used in the cache technology. To preserve the consistency of the data, we must therefore use new techniques. One (over-) simplified solution we could work with in the context of our prototype was the notion of “single updater” . The principle is simply to say that while each event triggers a new thread that stores the change and can realize some data crunching, only a single thread is responsible for collating all those events and intermediary results and then updating the KPI values. It is worth mentioning that in a “real life” implementation, this single threaded update model would need to be given a lot of thoughts to maintain its virtues, typically to prevent it from becoming a bottleneck (for example having an updater thread per data type). Another approach would be to look into the new reactive frameworks (e.g. Akka). Know the product . Infinispan is a complex enough technology that is made to answer several possible use cases. That means that it has a lot of possible configurations and that the defaults are likely not best for a given situation. You need to learn the product to not fall into traps. For example, the constant serialization in the grid setup has made us use an immutable pattern on our business objects to ease development. If you have more questions about this, be sure to leave a comment, and continue following our blog, there should be a technical article on Infinispan coming in a few weeks. To sum up New paradigms that have emerged in the recent years have allowed for a new class of applications/frameworks. Those have for the main part not found their way yet in the Retail and Private Banking sector but they should. We have seen that by using commodity hardware, those solutions are cheap. Through new architecture, they are made resilient and extremely performant. Finally, they are easy to put in place as they are not intrusive. The solution mentioned above is simply reading from the core banking data, never writing. And it exposes data in services that any front end (like an existing PMS) can consume and overlay on top of the core data coming from the Core Banking System (CBS). This simplicity makes this kind of system easy to setup in existing environments, alongside the CBS, that is by the way not meant to be replaced as it holds the master data and is the ultimately trusted source (especially if some of the realtime metrics are only approximated). I’ll conclude by opening a new door. Nowadays, additional focus is put on risk management and in particular on credit management. An architecture as described above can be fully leveraged to realize what-if scenarii, simply by faking the inputs (market events) in the system. While banks are today building complex what-if systems completely separate from their core banking one, the kind of platform we described before enables a smooth integration of the online platform and the what-if simulation, which of course means a lot less development and maintenance work. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged architecture , banking , grid computing , infinispan . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-09-25"},
{"website": "Octo", "title": "\n                The BAF method at OCTO            ", "author": ["Fabien Lamarque"], "link": "https://blog.octo.com/en/the-baf-method-at-octo/", "abstract": "The BAF method at OCTO Publication date 15/09/2014 by Fabien Lamarque Tweet Share 0 +1 LinkedIn 0 The BAF (“slap in the face” in French) is a self-improvement method widely used at OCTO Technology. Whenever a consultant feels like he lacks perfection in any way, he may present his peers what he did, how he did it and why he did it this way. Then coworkers (associates, juniors, seniors and trainees) discuss the consultant’s work and evaluate it. Then they use the Perfection Game method to assess it on a scale from 1 to 10 (1=worthless, 10=perfect) while giving useful tips on how to improve his work quality. The exercise is not only about spotting errors, it can also be about how to formulate an idea, find a way to engage a crowd or how to better manage a team. The main goal is to share ideas and knowledge thus improving everyone through communication. Like a code review, it can be harsh to be subjected to criticism or honest feedback. Everyone should understand that the main objective is to spread knowledge without hard feelings or frustration. BAFs can be used for improving virtually everything (technology choices, interviewing methods, communication, programming dilemmas, roadmap planning, etc.) and OCTO people thrive on improving themselves. OCTO motto being “There is a better way” , that could explain why it is so popular amongst us. Two examples: When writing a blog article: -Hi fellow coworker, I just wrote a blog article about BAFs , do you mind BAFing it? -I give you a score of 8 out of 10. I found your written English somewhat OK, the subject is interesting, the recursion is kinda fun and you gave examples. You would have scored 10 if you: double-checked typos, for example “felllow” line 17. talked more about how companies can use these methods. When doing a consulting mission: As a consulting company, clients often come to us to evaluate their application architectures, methodologies or technical choices. We usually address this through a mission but recently one of our clients specifically asked for a BAF of their .Net application architecture Three OCTO senior architects invited our client’s developers and architects for a day, so that they could present us their company, objectives and architectural choices. Then everyone: discussed the reasoning behind the choices made until now (“You may or may not have made the right choice, but why did you do it?”) talked about alternative solutions (“Did you even consider this framework? If yes why did you refuse to use it?”) drawn several architectural diagrams and discussed their pros and cons read examples of code, considered their maintainability After a full day of work, the ROTI ¹ revealed an overall score of 5 without rounding! This great recognition was then confirmed by feedback received soon after by email: “We were impressed with your performance. The technical expertise, pedagogy and methodology were perfect.” These are two examples of BAFs that you can use everyday in your company or project team and feel free to expand it to whatever fields you need to improve. ¹: ROTI, Return On Time Invested: every participants in the event evaluates on a scale from 1 to 5 the value of the event that brought them in regards to time spent. 1=the event was a total waste of time, 5=couldn’t have spent my time better Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-09-15"},
{"website": "Octo", "title": "\n                Measuring Web Application Performance – 2/3            ", "author": ["Zakaria El Marouri", "Thiago Ramos Santiago"], "link": "https://blog.octo.com/en/measuring-web-application-performance-2-of-3/", "abstract": "Measuring Web Application Performance – 2/3 Publication date 06/09/2014 by Zakaria El Marouri , Thiago Ramos Santiago Tweet Share 0 +1 LinkedIn 0 In the first article, we saw why performance test strategy is so important in order to ensure the proper functioning of a web application and how it can affect your business. No more pain, in this second article I will show you the steps to plan and execute your performance test. Imagine that after a marketing campaign, your e-commerce website is getting a peak of visitors and unfortunately instead of increasing, sales are actually going down drastically. How is this possible?  If I am getting more visitors, why am I making less money? It’s quite easy to understand actually: with peaks of hits the website probably suffers from outages, timeouts and consequently, may be down during visits. London Olympics 2012 is an excellent example of such situation: the official site took almost 1 minute to complete any shipment. Unfortunalty most of customers were not satisfied and preferred to leave the web site. To avoid being trapped in a similar situation you have to know how much your application can take so that you can adopt many solutions to face a peak of visitors. Only performance testing will tell you about the robustness of your web site. Plan a performance test in 8 steps Identify the execution environment The first requirement for a reliable performance test is to have a dedicated  environment: you can either use an isolated environment like an internal network, without external access to the internet, or the own production environment. Yes, it sounds crazy, but it’s common to see performance test in the production environment. You can consider it as the ultimate test. Identify the criteria for acceptable performance A baseline must be defined so that you can distinguish between a slow and a fast response time.  There are three important time limits to know: 0.1 second is about the limit for having the user feel that the system is reacting instantaneously , meaning that no special feedback is necessary except to display the result. 1.0 second is about the limit for the user’s flow of thought to stay uninterrupted, even though the user will notice the delay. Normally, no special feedback is necessary during delays of more than 0.1 but less than 1.0 second, but the user does lose the feeling of operating directly on the data. 10 seconds is about the limit for keeping the user’s attention focused on the dialogue. For longer delays, users will want to perform other tasks while waiting for the computer to finish, so they should be given feedback indicating when the computer expects to be done. Feedback during the delay is especially important if the response time is likely to be highly variable, since users will then not know what to expect. Basically you should never be over 1 second. Identify test scenarios You have to be aware of the current status of the application in term of volume of current users and in a more global way volume of data. If your website is visited by 10 concurrent users there no need to build a scenario with concurrent 1000 users. Also your test scenario must replay the real case scenario from end to end: for example,  if we’re talking about a virtual store, then the scenario should consider the user connection then products selection and the check out. You can also choose to target only the most critical scenarios in a technical or business point of view. The performance test you build must be realistic. Also remmember that only results from identical test executions, with exact same data can be used to identify a change in performance. It will make no sens to compare results from sessions using different databases. Identify the test load levels This just means that you have to define how many concurrent users you want and the ramp up period. It can be 10, 1000 or 1 million users as long as it is representative of the actual traffic of your web site. Choose your performance toolbelt There are dozens of tools that can run load tests. Each of them have its own particularity. For instance one tool can focus on reporting when an other one will be more efficicient for generating very high traffic. Also some tools will offer you a wide range of functionnality – not sure you will always need all of them. Build your tests Usually test scenarios consist in one  XML file that declares the URLs to fire, user credentials and any other needed data. You can automatically generate these files using  the testing tool you choose: for instance JMeter  uses a proxy to watch the URL you are browsing and then copy it into an XML file to be used as input for the test load. However in some cases you will have to work a bit more to make you test work: for instance ASP.NET WebForms applications generate a dynamic value called the VIEWSTATE. This value is set inside each page within an input field. Depending on the tool you choose you have different ways to deal with this situation. With JMeter using regular expressions to parse the HTML output and send back the VIEWSTATE value will do the job. Run the tests Last check before runing your test: make sure your database and application cache are cleaned. Once your environment and configuration are ready just run it ! Report Analysis Well, for me the success of the whole operation depends on this crucial part – what is a report good for if you can not understand it? A misunderstand of the collected statistics will for sure  take you away from the real problem cause. The two main statistics are response time and error rate. You should always consider these rules when it comes to read your report: There is something wrong with your environment if more than 20 percent of the test execution results appear not to be similar to the rest Focus on the 95th percentile not the average Pay attention to error rate, it indicates the maximum users your application can serve correctly Don’t hesitate to use additional tool for a deeper analysis of the report, BlazeMeter is a good one. What are the criteria for choosing the proper tool? You should focus on four criteria to correctly choose the tool that fits your needs: Project requirements is your application a desktop or web one ? What protocols are you using ? Price An expensive tool will not necessarily satisfy your expectation Integration with other tools it can be very interesting for advanced analysis and a wider vision of your system some tools supports plugins input Report’s quality Usually comes in two flavors: text and graphical. The visual representation is a real plus to understand the report think about it. Available Tools There are three type of tools when it comes to load testing: Load testing from Desktop Load testing SaaS Resource monitoring tools Desktop Load SaaS Load Resource monitoring tools (hardware) Apache JMeter Bees with Machine Guns Gatling HttpPerf LoadImpact LoadRunner Loadster Loadstorm LoadUI MultiMechanize NeoLoad OpenSTA QEngine (ManageEngine) Rational Performance Tester Siege Testing Anywhere WAPT WebLOAD Apica Load Test Blazemeter Blitz.io CloudTest Soasta AppDynamics NewRelic Our selection For a complete and efficient test we choose three tools to use together: Test creation and execution In the cloud distributed test loading Performance analysis and counters JMeter is the most widely used tool for creating and executing load testing scripts in the opensource world. Whereas the scenarios will be created in the JMeter, the best choice in terms of integration and execution of these scripts in the cloud is without a doubt the BlazeMeter, which in turn, in addition to rich reports in detail even offers integration with New Relic, which as a “detective” will be responsible for the monitoring of data and application servers, increasing the graphs and reports with data from the server hardware utilization. BlazeMeter is a web application, paid on-demand, which provides a simple integration with JMeter load testing script, simulating thousands of users visiting a Web site using Amazon Elastic Compute Cloud (EC2) as a service for the execution of tests. NewRelic is a SaaS solution that allows you to monitor Web applications. The idea is pretty simple, there is an agent that collects performance data and sends it asynchronously to a server of New Relic, displaying a kind of x-ray its application, with information of CPU, memory, response time, throughtput, number of requests, errors, slow transactions, database usage, JVM in real time. Thus, we have three tools that integrate to provide a complete insight of the load tests results. Explenation: JMeter records the scenario from the user computer. It generates an XML file that contains the URLs, credentials and the test configuration (number of users, ramp up period…). This file is then sent to BlazeMeter, wich then fires the given URLs. While BlazeMeter is attacking the server NewRelic wacthes its performance counters (i.e CPU and memory usage …). Once the test is finished a reported is generated. Isn’t JMeter enough? We have seen that we can use JMeter to create, execute, and analyze the results, then, why use another tool in the cloud to perform and analyze the results, if we could make the tests from a simple desktop? Simple, imagine that you are testing a virtual Grand Lodge, surely it will have defense mechanisms ante-hacker or some kind of defense against DDoS ATTACK “attacks”, which means that when your test via JMeter start, you’ll be using the same IP to perform the requests to the server, and logically will be easily recognized and blocked by the tools of protection However when using a load testing tool in the cloud, it instantiates several machines (that create other virtual machines) which in turn perform http requests to the specified address. Making a simple analogy, using a load test tool desktop, you’ll be a soldier in the middle of a battle, with a machine gun, shooting at your target, if it is located, will be neutralized. While using a load testing tool in the cloud, you command an army of machine guns, all scattered in various places, firing in a same location. Conclusion Despite a focus on speed to be an ancient practice of the giants of the internet, companies begin to notice that their customers want on top of navigation performance, however the relationship between amount of users and response time is exponential, that is sometimes, after a point, the more users its application has, the longer it will take to load, thus creating new challenges for those who want to become big on the web. A good strategy to stay one step ahead, is to use performance testing techniques in order to discover the application behavior during a peak usage, plus discover bottlenecks and, obviously, the access limit. Who wants to be big on the internet, should prioritize the performance testing. After all, time is money, and with so many options available no one else is willing to lose time. Perform load testing often allows you to track the evolution of application every release, and measure whether performance regression between alleged improvements on the site. But that’s a subject for next post =) Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged cloud , jmeter , web application perfomance . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-09-06"},
{"website": "Octo", "title": "\n                The “BAF” improvement technique of the OCTO            ", "author": ["Thiago Ramos Santiago", "Damien Joguet"], "link": "https://blog.octo.com/en/the-baf-improvement-technique-of-the-octo/", "abstract": "The “BAF” improvement technique of the OCTO Publication date 05/08/2014 by Thiago Ramos Santiago , Damien Joguet Tweet Share 0 +1 LinkedIn 0 The “BAF” is a technique widely used in OCTO. In general, this allows the consultant for a continuous and incremental way to propose a delivery, a technical definition, an article, a conference, etc … Always together with a critical review of the counterparty, with a view to continuous improvement. In the OCTO development sessions historically assumed the name of “BAF”. NB: Our consultants equipped with the technique called “PG” (Perfect Game) advocating a meaningful assessment in the form: “I put x / 10 because I liked … To 10/10, you should add / fix …” Still, our clients often ask us about specific interventions to help them in application architectures, and use of technology … We recently innovated to offer a customer the “BAF” technique to propose the architecture of a .Net application in a day, with three architects OCTO. Given the history of “BAF”, this proposal proved challenging … The day came, after a brief presentation of the company, the project objectives and architectural solutions chosen, then the participants: • discussed the reasons for the choice of architecture made • Took the colored pens to discuss a flip about the best solutions available in the world .Net • They decorated the room with architectural diagrams • They discussed examples of code • And of course, ate and had coffee on the terrace OCTO France After a day of work, the “roti” (Return on time invested: Participants in an event evaluates on a scale of 1 to 5, the value of the event that brought them in regards to time spent) revealed … and without rounding the overall score was 5! This exceptional recognition was confirmed by feedback received soon after by mail: “We are impressed with the performance The technical expertise was perfect, pedagogy, methodology … the customer returned some praise with a touch of modesty, for a great recipe.”. This is a success story that we always aims to repeat. And why not, next time, with your company? Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology , News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-08-05"},
{"website": "Octo", "title": "\n                Track your iOS application code quality using Sonar            ", "author": ["Cyril Picat"], "link": "https://blog.octo.com/en/track-your-ios-application-code-quality-using-sonar/", "abstract": "Track your iOS application code quality using Sonar Publication date 07/02/2014 by Cyril Picat Tweet Share 0 +1 LinkedIn 0 This article is the iOS counterpart of Romain’s recent article on tracking code quality with Sonar on Android . The problematic remains the same: quality is often overlooked for mobile Apps and the cost of setting up a proper quality tracking environment is usually deemed too high. This article will focus on showing that, while the iOS platform is not as tooled as Android, it’s now fast and easy to track code quality on iOS projects . Even though this has been possible in the past mostly using Java tools (see a preceding post on tracking iOS code quality with Jenkins ), it has often been limited to big projects where a quality process has to be enforced (and sometimes some metrics were even part of the contract – even if I am not a big fan of this). The main limitation was in that case the time and expertise to set everything up. This article follows a presentation we held at soft-shake last October. Make use of Sonar for your mobile developments – It's easy and useful! from cyrilpicat Meet SonarQube For the uninitiated, SonarQube is a continuous quality analysis platform running as a web server that tracks metrics regarding your code and its structure. It can be extended through plugins, and usually embeds useful tools and checks. Historically SonarQube only dealt with Java code but it has been extended since, and it handles most common languages as of today (available features may vary). Our use case being an iOS application, we are wandering far away from the Java world and there is no official support of this language by SonarSource. Romain Felden and I created a dedicated community plugin – Sonar Objective-C plugin – that enables Objective-C project analysis. System prerequisites Quite a few tools will have to be installed on your computer, this is clearly the most time-consuming part. Hopefully you will have to do that only once on your CI server (or locally on your workstation): SonarQube tooling : you have to install SonarQube , SonarQube Runner and SonarQube Objective-C plugin via the standard plugin install procedure Build tooling : xctool , which is replacement for Apple xcodebuild tool with quite a few enhancements. The best way to install it is via HomeBrew . A side note on that point: xctool is only used internally by the plugin to build, test and compute the metrics, it has no impact whatsoever on your project, it will still be built with Xcode/xcodebuild. Analysis tooling : you have to install OCLint (violations) and gcovr (code coverage). gcovr is a simple binary to put somewhere in your PATH. Tip: You may need to go wild and install a development version of OCLint (0.8/0.9) as in my experience, the stable 0.7 version had blockers on my projects Project prerequisites Situation is easier than for Android because there is actually only one build tool: Xcode. The only things you should do are: copy sonar-project.properties in your Xcode project root folder (along your .xcodeproj/.xcworkspace file) copy run-sonar.sh in your Xcode project root folder and make it executable (via chmod) The run-sonar.sh file will stay untouched. You should then describe your project in the sonar-project.properties file , via the following properties: sonar.projectKey / sonar.projectName sonar.sources sonar.objectivec.project / sonar.objectivec.workspace sonar.objectivec.appScheme and sonar.objectivec.testScheme /li> Tip: Comment sonar.objectivec.testScheme if you have no unit/UI tests in your application Demonstration After some research, I stumbled upon the iOctoCat open-source application. Despite its name, it has nothing to do with OCTO. It’s an old iOS client for GitHub (by GitHub). This application is interesting because it’s non-trivial, it has some tests (you’re not misreading) and clean dependencies (with CocoaPods). To push things even further, here is a fork of the repository cleaned up and fixed for the latest version of Xcode. Stage 1: look ma, no hands Unfortunately, nothing works out-of-the-box like in Android, where you can get first analysis results with no configuration at all! But there is no free lunch, is there? Anyway you first need to configure the demo project. This is done with the following command lines: git clone https://github.com/cyrilpicat/ioctocat ./bootstrap.sh Tip: On a your own project, you don’t have to build the project first, the plugin takes care of that. The bootstrap.sh script is a facility provided by iOctocat developers to download and build all the dependencies of iOctocat Stage 2: the full monty You should configure the sonar-project.properties for this project. Here is what mine looks in the end (you can download the file from here ): ##########################\r\n# Required configuration #\r\n##########################\r\n\r\nsonar.projectKey=iOctocat\r\nsonar.projectName=iOctocat\r\nsonar.projectVersion=1.0\r\nsonar.language=objc\r\n\r\n# Project description\r\nsonar.projectDescription=(Old) GitHub client for iOS\r\n\r\n# Path to source directories (application code, not third-party code)\r\nsonar.sources=Classes\r\n\r\n# Xcode project configuration\r\nsonar.objectivec.workspace=iOctocat.xcworkspace \r\nsonar.objectivec.projects=iOctocat.xcodeproj\r\nsonar.objectivec.appScheme=iOctocat\r\nsonar.objectivec.testScheme=iOctocat Unit Tests\r\n\r\n##########################\r\n# Optional configuration #\r\n##########################\r\n\r\n# Encoding of the source code\r\nsonar.sourceEncoding=UTF-8\r\n\r\n# JUnit report generated by run-sonar.sh is stored in sonar-reports/TEST-report.xml\r\n# Change it only if you generate the file on your own\r\n# The XML files have to be prefixed by TEST- otherwise they are not processed \r\nsonar.junit.reportsPath=sonar-reports/\r\n\r\n# Paths to exclude from coverage report (tests, 3rd party libraries etc.)\r\n# sonar.objectivec.excludedPathsFromCoverage=pattern1,pattern2\r\nsonar.objectivec.excludedPathsFromCoverage=.*Tests.* Then you run the analysis on your project with the following command line (in your Xcode project root folder): ./run-sonar.sh The dashboard you get is illustrated in the following picture: Dashboard: iOctocat analysis If you look at it and compare to its Android counterpart, you will see that quite a few metrics are missing , like: Complexity : cyclomatic complexity by method/class/file Design : dependencies between classes, files Documentation : code documentation (doxygen, appledoc etc.) Size : number of statements, number of classes, methods and properties Tests : no support for tests other than unit tests (integration, acceptance etc.) And then? Your next question should be: fine it looks nice and easy but when are we going to have a full support for the Objective-C in SonarQube ? Well that’s not an easy question, as the plugin is done mostly on our free time. But it’s faster with real users: star the GitHub project and I am sure it will go faster! Aside from this twisted answer, here is the plan as of today: have a fix release very soon for the various issues raised since last October (v0.3.2) add support for the various test frameworks out-of-there (v0.4) add a basic support for the Objective-C language (syntax highlighting, classes, methods etc.) (v0.9) While I expect to have the first two quite soon, the latter is more my 6-12 months plan. Why so long? Because until now, the plugin was more about getting existing tools work smoothly and with no configuration. Now the hardest work begins: have SonarQube understand the Objective-C language. Afterwards the plugin should be really useful, but still not as complete and integrated as a supported SonarQube plugin. I must say that on my side I have no plan to get it further the useful stage. But who knows, plans change! Why did I tell you all this? Even if the analysis is still limited, it’s nice you can get this analysis with no modification of the project at all. Personally I find it more easy to setup than its Android counterpart. Either way, now you know how to get relevant metrics out of your neat iOS application in no time using Sonar. No more excuse for not using it on your own project, even if it is a indie iOS App. Follow @cyrilpicat Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged code quality , development , iOS , iphone , mobile , objective-c , sonar , sonarqube , test . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 6 commentaires sur “Track your iOS application code quality using Sonar” Rishi 07/05/2014 à 00:06 Super Awesome Stuff !!\r\n\r\nThanks a Lot for this Post !! Cyril Marat 26/05/2014 à 13:28 Guys, you may know that all these missing metrics can be acquired by OCLint , which can be extended by plugins. So, if I write these plugins, will I have that additional code metrics in Sonar?\r\nThank you in advance. Arnaud Heritier 27/05/2014 à 11:48 http://repository-rfelden.forge.cloudbees.com is empty. Normal ? Guna 10/08/2015 à 11:22 Isn't it true that SonarQube Obective-C and Swift Plugins are commercial only?? dsc 03/11/2015 à 13:03 Thanks for the detailed doc. \r\nAfter following all the steps, I am stuck with one step while running the demo app. Please see the output I am getting.\r\n(Tried to change the ONLY_ACTIVE_ARCH to YES in the run-sonar.sh as well as in the xcode build settings.)\r\n\r\nOUTPUT:\r\nRunning run-sonar.sh...\r\n.Extracting Xcode project information...\r\n.........ERROR - Command 'xctool -workspace iOctocat.xcworkspace -sdk iphonesimulator ARCHS=i386 VALID_ARCHS=i386 CURRENT_ARCH=i386 ONLY_ACTIVE_ARCH=NO -scheme iOctocat -reporter json-compilation-database:compile_commands.json build' failed with error code: 1\r\nqbadmins-Mac-Pro-78:ioctocat qbuser$ ./run-sonar.sh\r\nRunning run-sonar.sh...\r\nExtracting Xcode project information...\r\n.......ERROR - Command 'xctool -workspace iOctocat.xcworkspace -sdk iphonesimulator ARCHS=i386 VALID_ARCHS=i386 CURRENT_ARCH=i386 ONLY_ACTIVE_ARCH=NO -scheme iOctocat -reporter json-compilation-database:compile_commands.json build' failed with error code: 1 wine 11/12/2015 à 04:58 i have some error msg when execute the sonar-runner command:\r\n11:52:16.717 INFO  - Sensor Lines Sensor (done) | time=70ms\r\n11:52:16.717 INFO  - Sensor QProfileSensor\r\n11:52:16.722 INFO  - Sensor QProfileSensor (done) | time=5ms\r\n11:52:16.723 INFO  - Sensor ObjectiveCSquidSensor\r\n11:52:17.575 INFO  - Sensor ObjectiveCSquidSensor (done) | time=852ms\r\n11:52:17.575 INFO  - Sensor Objective-C SurefireSensor\r\n11:52:17.575 INFO  - parsing sonar-reports\r\nINFO: ------------------------------------------------------------------------\r\nINFO: EXECUTION FAILURE\r\nINFO: ------------------------------------------------------------------------\r\nTotal time: 7.810s\r\nFinal Memory: 10M/337M\r\nINFO: ------------------------------------------------------------------------\r\nERROR: Error during Sonar runner execution\r\nERROR: Unable to execute Sonar\r\nERROR: Caused by: org.sonar.api.resources.Project.getPackaging()Ljava/lang/String;\r\nERROR: \r\nERROR: To see the full stack trace of the errors, re-run SonarQube Runner with the -e switch. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-02-07"},
{"website": "Octo", "title": "\n                Measuring Web Application Performance – 1/3            ", "author": ["Zakaria El Marouri", "Thiago Ramos Santiago"], "link": "https://blog.octo.com/en/measuring-web-application-performance-1-of-3/", "abstract": "Measuring Web Application Performance – 1/3 Publication date 06/08/2014 by Zakaria El Marouri , Thiago Ramos Santiago Tweet Share 0 +1 LinkedIn 0 During a typical lunch talk, a colleague said that we could easily list the most popular stereotypes about a nation just by asking  Google: After some laughs I started wondering about public opinion concerns on the main web sites of the internet. Adapting a little the previous question I got some interesting answers: I was surprised to see that the results indicated that most people were asking about the same topic: web sites performance So it looks like the speed at which the content is delivered is as important as the content it-self. “Subconsciously, you don’t like to wait,” said Arvind Jain, a Google engineer who is the company’s resident Speed maestro. “Every millisecond matters.” According to the New York Times : Google and other tech companies are on a new quest for speed, challenging the likes of Mr. Jain to make fast go faster. The reason is that data-hungry smartphones and tablets are creating frustrating digital traffic jams, as people download maps, video clips of sports highlights, news updates or recommendations for nearby restaurants. The competition to be the quickest is fierce A simple comparison would be a supermarket right next to your house and another on the other side of town, you will of course go to the closer one since it takes you less time to do your shopping. In other words, people tend to visit sites that are proving content faster than competitors. And this raises the question: “How faster than my competitors should I be?”. Quote from Harry Shum, a computer scientist and speed specialist at Microsoft: Two hundred fifty milliseconds, either slower or faster, is close to the magic number now for competitive advantage on the Web 250 milliseconds (1/4 of a second, or literally a wink of an eye) is considered the number to be faster than your competitor. In 2009, a study by Forrester Research found that online buyers have lot of money and little patience. They expect pages to load within  two seconds or less. Between three and four seconds they have a dissatisfied customers (and unhappy customers tend not to do many purchases). After 5 seconds they just leave. 2 seconds 3 to 5 seconds more than 5 seconds Just three years earlier, a similar study from Forrester found that the average expectations for page loading times were of 4 seconds or less,  so it means that increasingly the customer who is spending money on the web wants to be answered quickly. Time is money Surveys conducted by Amazon demonstrated a 100 ms increased loading time results in 1% loss in sales ( source ). In this case, how much cash would lose a virtual store that happens to be 1 second longer to load ? 100 milliseconds = -1% of sales. 1 second = 1000 milliseconds. 1 second = could mean until 10% of sales. Imagine how would react the owner of a virtual store after hearing that he loses 10% of potential sales every second that his Web site slows down? The one million dollar question Can you see the paradox? All companies want their web site to have a growing volume of visitors, however the higher the number of visitors, the higher the server’s response time and a slower loading. We could came to the conclusion that the more traffic your have the slower you pages will load and then the less the money. With this principle in mind, are you aware of the load time threshold that makes a user stay or leave your web site ? What is making your web site so slow? It’s easy just do some performance tests on your application and you will probably get the right answer. Performance testing consists in evaluating the capacity, robustness and availability of an application versus the amount of concurrent connections, evaluating its performance especially under high workload and also considering its behavior under normal circumstances. The main goal of such experiments is to ensure that the software does not present problems or outages in conditions of insufficient computational resources (such as memory, disk space or processing), when working in high concurrency or suffering some denial-of-service attack. Some benefits of performance testing are: Check the quality of the system developed Test the infrastructure capacity hired Be aware of supported simultaneous access Identify the point of bottlenecks of its implementation, the performance SPOF There are some techniques to evaluate the performance of a web application, the top 3 are: Load test Stress testing Non-regression test of Performance . Load test During a load test the number of simultaneous connections is growing over time. The goal is to evaluate the application’s behavior in accordance with the growth of traffic (requests) so that we can identify both application’s capacity limit and bottleneck (coding, hardware, database …) Stress Test In a stress test, a higher load beyond normal usage patterns is fired against the application, some crash scenarios are tested, in order to determine the resilience and stability of the system. For instance this could be made by removing a node from the clusters of application servers or making a database unavailable. Non-regression Performance Test Ensures that a modification of the system has no impact on its performance. How does the load test work? According to Wikipedia : load test is used to check the limit of data processed by the software until it can’t not process it. It is also known as Volume test: Used to validate and assess the acceptability of the operational limits of a system according to varying workloads, whereas the system under test remains constant. In General, measurements are taken on the basis of the data transfer rate of the workload and transaction response time aligned The goal of the load test is to determine the Web application’s behavior through normal conditions and high peak loads. As they gradually increments from normal load to peak,  response time and hardware behavior should be analysed. How to identify application’s limit? To get to know your application’s limit, an evaluation should be made on four aspects: Unavailability High response time Data inconsistency Concurrency issues Looking at the figure below, we have a simple example of run report, where we can see the results of a 18 minutes test , having a limit of 1,300 concurrent users, with a ramp up  of 100 users per minute. In other words, imagine that your virtual store made a huge promotion ant that  each minute 100 new users are connecting. In this scenario, how many users can you application serve without noticeable impact? What is the number of users and the time threshold that turns your application into a slow website that users just want to leave ? Analyzing the graph generated in this test, we can say that till 8 minutes, or approximately 800 concurrent users, the application behaved stably, but after that we had a high latency and a high response time and so we identify our website stress limit point. In the next article we will see how a load test must be planned and executed to help us being aware of  our web application performance Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged performance test , web application . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-08-06"},
{"website": "Octo", "title": "\n                Docker container’s configuration            ", "author": ["Arnaud Mazin"], "link": "https://blog.octo.com/en/docker-containers-configuration/", "abstract": "Docker container’s configuration Publication date 21/02/2014 by Arnaud Mazin Tweet Share 0 +1 LinkedIn 0 As we saw in the previous post , Docker basic usage seems pretty simple. Let’s try to dig a little deeper into the configuration management. From an OPS perspective, we can consider containers as application black boxes, without caring about how they are built, and simply use them. However there are several aspects of a container that must be configured to ensure their correct integration in an information system. Container run-time configuration It is possible, and thus recommended, to limit CPU and memory usage in LXC, and therefore Docker containers. Docker run command line provides both limitations with the -m and -c options. But we might need to configure the container application itself to adapt its memory usage or processes/threads counts to match those settings. We’ll try to illustrate this by improving the previous memcached example. Docker provide a simple way to provide configuration options to containers by setting environment variables: $ docker run -e MY_VAR=666 -t -i 127.0.0.1:5000/ubuntu/memcached:latest /bin/bash\r\ndaemon@078c7cf85f66:/$ echo $MY_VAR        \r\n666\r\ndaemon@078c7cf85f66:/$ exit Now we can adapt our startup script and Dockerfile to use this feature: # Dockerfile\r\nFROM 127.0.0.1:5000/ubuntu:latest\r\n\r\nRUN dpkg-divert --local --rename --add /sbin/initctl\r\nRUN ln -s /bin/true /sbin/initctl\r\n\r\nRUN apt-get install -y memcached\r\n\r\nADD memcached.sh /\r\nEXPOSE 11211\r\nENV MEMCACHED_MEM 512\r\nENV MEMCACHED_CON 1024\r\nCMD    [\"/memcached.sh\"]\r\nUSER daemon And the memcached.sh startup script #!/bin/bash\r\n# Default values if env vars are not set\r\nmem=${MEMCACHED_MEM:-64}\r\nconn=${MEMCACHED_CON:-1024}\r\n\r\nexec memcached -m $mem -c $conn If we rebuild the container and launch it will by default start with a 64MB memory usage and 1024 connections, but we can also force other values: $ docker run -d -m=130m -e MEMCACHED_MEM=128 -e MEMCACHED_CON=512 127.0.0.1:5000/ubuntu/memcached:latest\r\n9215e32d9bee4c632623b9d4cfff226f260069c28f94307989788e48c8fbf9a9 A simple ps command confirm the variables handling. \\_ lxc-start -n 9215e32d9bee4c632623b9d4cfff226f260069c28f94307989788e48c8fbf9a9 -f /var/lib/docker/containers/9215e32d9bee4c632623b9d4cfff226f260069c28f94307989788e48c8fbf9a9\r\n      \\_ memcached -m 128 -c 512 Applications that can natively rely on environment variables get reconfigured easily this way. For others it might be a little trickier: # by default, start 4 workers unless overriden by the environment variable\r\nworkers=${NGINX_WORKERS:-4}\r\nexec /usr/sbin/nginx -g \"worker_processes $workers;\" Inter-container communications Topology-dependent variables must be injected into containers as soon as we try to make them communicate with each-other. Docker links feature simply set environment variables to provide such information. In the following example, we’ll consider two nginx instances, one acting as a static content provider (web1), one acting as a reverse-proxy (rp1) in front of he first one: Consider that we have two Docker images providing both roles: nginx:latest and nginx/rp:latest. We won’t get into much details about how the containers are built, this is not the point. Let’s start the first static Nginx whose configuration is stand-alone, and name the running container web1 : $ docker run -d -name=web1 127.0.0.1:5000/nginx:latest\r\n3932c46c8ad522cae11f7fd942e488379d4f6a04801f51eb9435fc5c56e8272e Now let’s start a simple bash into the reverse-proxy nginx and linking it to web1 : $ docker run -link=web1:web -t -i 127.0.0.1:5000/nginx/rp:latest /bin/bash\r\ndaemon@eefeee437402:/$ export | grep WEB\r\ndeclare -x WEB_ENV_NGINX_WORKERS=\"1\"\r\ndeclare -x WEB_NAME=\"/insane_fermi/web\"\r\ndeclare -x WEB_PORT=\"tcp://172.17.0.28:80\"\r\ndeclare -x WEB_PORT_80_TCP=\"tcp://172.17.0.28:80\"\r\ndeclare -x WEB_PORT_80_TCP_ADDR=\"172.17.0.28\"\r\ndeclare -x WEB_PORT_80_TCP_PORT=\"80\"\r\ndeclare -x WEB_PORT_80_TCP_PROTO=\"tcp\" All WEB_PORT_ variables can be used to configure our upstream to point to web1 nginx instance. Unfortunately, Nginx upstream configuration can’t be easily based on environment variables, this is the worst case scenario, and we will have to play with sed: # Tweak a env variable to help templating a NGinx configuration file by removing the tcp:// prefix\r\nNGINX_UPSTREAM_URL=${WEB_PORT#tcp://}\r\n\r\n# use a sed to create the final site.conf NGinx configuration file\r\nsed -e \"s/##NGINX_UPSTREAM_URL##/$NGINX_UPSTREAM_URL/g\" /site.conf.tpl > /etc/nginx/sites-enabled/site.conf with a template file site.conf.tpl which might look like: upstream upstream_server  {\r\n      server ##NGINX_UPSTREAM_URL##;\r\n}\r\n\r\nserver {\r\n    listen       80;\r\n    server_name  example.com;\r\n    location / {\r\n     proxy_pass  http://upstream_server;\r\n     proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;\r\n     proxy_redirect off;\r\n     proxy_buffering off;\r\n     proxy_set_header        Host            $host;\r\n     proxy_set_header        X-Real-IP       $remote_addr;\r\n     proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;\r\n   }\r\n} We simply have to start the second nginx instance and publicly expose its HTTP port: $ docker run -d -link=web1:web -name=rp1 -p=80:80 127.0.0.1:5000/nginx/rp:latest\r\n50dfbb2e9979eb2a9cf499a22e42172339c5e8f9138cb69f0d97ea4ec31fbd2a Wonderful, we now have a working chain. However two concerns remain: On a multi-host toplogy (container running onto several physical or virtual hosts), the link feature won’t apply. Fortunately, if you can provide those exact same environment variables by other means, you will be able to make things work What happens if you stop, destroy and recreate web1? Docker will allocate a new container which will unlikely have the same IP address: failure! Your environments variables used to start the reverse-proxy point to nothing interesting, you have to restart all the depending containers. Tools like fig or MaestroNG have been written on top of Docker to help you handle such situations, MaestroNG being able to handle multi-hosts design. If you want to add even more control over the configuration and orchestration of your Docker containers, you still can code on top of frameworks like capistrano to do this. Conclusion Pros : Docker is doing well with cross-container communications by using env variables. Some real simple topology management (links) can provide a static way of managing links. Cons : On a larger scale, or in complicated toplogies, with lots of links and containers, it might make sense to use some orchestration tools on top of Docker to provide some high-level deployment management (rolling deployment, zero-downtime deployment…). In very dynamic and elastic architectures, the use of distributed services registry such as Zookeeper , etcd or serf must be considered. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Infrastructure and Operations and tagged DevOps , Docker , LXC . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Docker container’s configuration” Mark Dastmalchi-Round 21/08/2015 à 11:34 Alternatively, you can use Tiller (http://github.com/markround/tiller) which provides a standardized, easy to use way of generating configuration files. Many containers already use it, as it can generate configuration files from a variety of sources: environment variables; a HTTP webservice; zookeeper cluster; or even just plain YAML files on disk. I wrote it specifically so that you can build a single Docker container, but ship configuration files for different environments (or let users specify parameters at run time). \r\n\r\nIn your example above, you'd just use standard ERb templates, so you could do something like: upstream upstream_server  {\r\n      server ;\r\n} Or whatever, and it would get filled in from environment variables. But you could use the defaults plugin to provide default values if the user omits them, and so on. There's some more examples on my blog, or linked from the README.md on the github page. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-02-21"},
{"website": "Octo", "title": "\n                Docker registry first steps            ", "author": ["Arnaud Mazin"], "link": "https://blog.octo.com/en/docker-registry-first-steps/", "abstract": "Docker registry first steps Publication date 11/02/2014 by Arnaud Mazin Tweet Share 0 +1 LinkedIn 0 Here at Octo, we are fond of Docker. Not because we completely master it, but because we don’t (yet). And as DevOps-minded guys, we like new perspectives in Dev / Ops relationship. Docker is mainly about this, shifting each other’s expectation. Now that the 0.7 and 0.8 releases are out, its production readiness has never been closer and it’s getting pretty exciting. In a previous article, we’ve answered a few questions allowing to understand the basic concepts of Dockers, let’s play around with few Docker command lines and manage a local repository (also called registry). Reminder Docker is a new approach in application packaging and deployment in many ways. Some quite disturbing paradigms are therefore considered: Ops don’t have to care about how the containers are built neither do they have to care about what they are made of. They just have to consider them as application-level (almost) black boxes which are simply connected to each other. Docker containers are usually made of everything needed to run over a LXC -aware Linux kernel (libC, middleware, application). They are often based on a regular distribution (CentOS, ubuntu, Debian…). Container images are built once (usually by dev guys) and reused as-is everywhere , from development workstation to production envs, with no modification at all. Prefer the « rebuild/redeploy » pattern rather than « upgrade » when deploying new application version. Upgrades simply consist of shutting down the former container and start a new one in its place. Rely on registries to store and version container images. Registries are a major component in Docker ecosystem. They are used at different stages of the application lifecycle: Dev guys use the docker.io public registry as some kind of github to easily get ready-to-use containers with application stacks (Java, PHP, Rails…) deployed Ops guys might contribute to those middleware-ready containers by applying internal security and deployent patterns , if needed. Dev guys uses their local Docker instance and internal Docker registries as Git with origins: (think commit , tag , pull , push ) to produce ready-to-deploy applications . Ops guys rely on internal Docker registries as a Nexus platform: download a container based on a given version number and deploy it . Container upgrades however only download the missing commits, not all the new container. Some examples by practice All the following commands must be run on a LXC-aware Linux, physical or virtual, with Docker freshly install. You can follow the Ubuntu installation procedure to perform such install, it’s pretty straight forward. Let’s start a local registry. It can itself run within a Docker container of course to ease its deployment. Let’s use the container proposed on docker.io’s registry and expose its TCP/5000 port on the host: $ docker run -d -p=5000:5000 stackbrew/registry:latest Now let’s create a local copy of the standard ubuntu container image: $ docker pull ubuntu:latest\r\n[...]\r\n$ If we have a look at the local images by running: $ docker images\r\nREPOSITORY           TAG                 IMAGE ID            CREATED             SIZE\r\nstackbrew/registry   latest              3321c2caa0cf        3 weeks ago         472.2 MB (virtual 3 GB)\r\nubuntu               latest              8dbd9e392a96        8 months ago        128 MB (virtual 128 MB) We can see that 2 container images are locally known. Let’s tag the ubuntu:latest to prepare it to be pushed on the local registry. Tagging is a way to give a more understandable name to a container image, but also a way to push them on a remote registry, private or public. $ docker tag 8dbd9e392a96 127.0.0.1:5000/ubuntu:latest\r\n$ docker images\r\nREPOSITORY              TAG                 IMAGE ID            CREATED             SIZE\r\nstackbrew/registry      latest              3321c2caa0cf        3 weeks ago         472.2 MB (virtual 3 GB)\r\nubuntu                  latest              8dbd9e392a96        8 months ago        128 MB (virtual 128 MB)\r\n127.0.0.1:5000/ubuntu   latest              8dbd9e392a96        8 months ago        128 MB (virtual 128 MB) ubuntu:latest has the same IMAGE ID as the one freshly tagged onto our new registry, looks good. Let’s push it on our local regsitry, listening on the TCP/5000 port. By uploading this image, we make it available to any other Docker host around. $ docker push 127.0.0.1:5000/ubuntu\r\nThe push refers to a repository [127.0.0.1:5000/ubuntu] (len: 1)\r\nSending image list\r\nPushing repository 127.0.0.1:5000/ubuntu (1 tags)\r\nPushing 8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\r\nPushing tags for rev [8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c] on {http://127.0.0.1:5000/v1/repositories/ubuntu/tags/latest} Now let’s start our own new docker container by creating the following Dockerfile: FROM 127.0.0.1:5000/ubuntu:latest\r\n\r\n# tweak ubuntu's initctl\r\nRUN dpkg-divert --local --rename --add /sbin/initctl\r\nRUN ln -s /bin/true /sbin/initctl\r\n\r\nRUN apt-get install -y memcached\r\n\r\nADD memcached.sh /\r\nEXPOSE 11211\r\nCMD    [\"/memcached.sh\"]\r\nUSER daemon You might have noticed that we added a memcached.sh file which content might look like: #!/bin/bash\r\n\r\nexec memcached It’s very trivial at this stage and could have been totally written into the CMD Dockerfile statement. Let’s keep it that way for further improvements. We can now build our brand new container image: $ docker build -t 127.0.0.1:5000/ubuntu/memcached:latest . Once the container image has been built, we don’t need the Dockerfile anymore, except if we want to re-build the image later on or on another docker host. The local Docker agent keep tracks of this local image. It’s therefore possible to have a look at the full tree of Docker images using a docker images --tree : └─8dbd9e392a96 Size: 128 MB (virtual 128 MB) Tags: ubuntu:latest, 127.0.0.1:5000/ubuntu:latest\r\n  └─b5f46a480005 Size: 155.8 kB (virtual 128.2 MB)\r\n    └─86f4d2bdda8b Size: 16 B (virtual 128.2 MB)\r\n      └─55bc3a7d11cb Size: 50.99 MB (virtual 179.2 MB)\r\n        └─d60079a9c49b Size: 28 B (virtual 179.2 MB)\r\n          └─34d94e18df44 Size: 178.4 MB (virtual 357.6 MB)\r\n            └─43f04dd23753 Size: 178.4 MB (virtual 535.9 MB)\r\n              └─4fd5c706f3a8 Size: 178.4 MB (virtual 714.3 MB) Tags: 127.0.0.1:5000/ubuntu/memcached:latest You can see that several patches (for instance, filesystem content changes due to RUN or ADD statements in the Dockerfile, container configuration change by ENV , CMD or USER ) have been applied on top of the standard ubuntu:latest image to produce the final memcached one. Docker history command details the patches that have been applied. They represent each non-comment line of the Dockerfile. Be aware that the oldest change is at the bottom of the stack, as in some kind of git log output. docker history 127.0.0.1:5000/ubuntu/memcached:latest\r\nIMAGE               CREATED             CREATED BY                                      SIZE\r\n4fd5c706f3a8        6 seconds ago       /bin/sh -c #(nop) USER daemon                   178.4 MB\r\n43f04dd23753        8 seconds ago       /bin/sh -c #(nop) CMD [/memcached.sh]           178.4 MB\r\n34d94e18df44        9 seconds ago       /bin/sh -c #(nop) EXPOSE [11211]                178.4 MB\r\nd60079a9c49b        11 seconds ago      /bin/sh -c #(nop) ADD memcached.sh in /         28 B\r\n55bc3a7d11cb        13 seconds ago      /bin/sh -c apt-get install -y memcached         50.99 MB\r\n86f4d2bdda8b        28 seconds ago      /bin/sh -c ln -s /bin/true /sbin/initctl        16 B\r\nb5f46a480005        29 seconds ago      /bin/sh -c dpkg-divert --local --rename --add   155.8 kB\r\n8dbd9e392a96        9 months ago                                                        128 MB Now, it’s time to push it on the local registry and run it: $ docker push 127.0.0.1:5000/ubuntu/memcached\r\n[...]\r\n$ docker run -d -p 11211:11211 127.0.0.1:5000/ubuntu/memcached:latest A truncated output of a ps afx might give you something like: /usr/bin/docker -d\r\n \\_ lxc-start -n 84273abb1fcf73e6b538893c1b703a9c9e5911d36c1ba876e82db06d30f25259 -f /var/lib/docker/containers/84273abb1fcf73e6b538893c1b703a9c9e5911d36c1ba876e82db06d30f25259\r\n |   \\_ /bin/sh -c cd /docker-registry && ./setup-configs.sh && ./run.sh\r\n |       \\_ /bin/bash ./run.sh\r\n |           \\_ /usr/bin/python /usr/local/bin/gunicorn --access-logfile - --debug --max-requests 100 --graceful-timeout 3600 -t 3600 -k gevent -b 0.0.0.0:5000 -w 4 wsgi:applic\r\n |               \\_ /usr/bin/python /usr/local/bin/gunicorn --access-logfile - --debug --max-requests 100 --graceful-timeout 3600 -t 3600 -k gevent -b 0.0.0.0:5000 -w 4 wsgi:ap\r\n |               \\_ /usr/bin/python /usr/local/bin/gunicorn --access-logfile - --debug --max-requests 100 --graceful-timeout 3600 -t 3600 -k gevent -b 0.0.0.0:5000 -w 4 wsgi:ap\r\n |               \\_ /usr/bin/python /usr/local/bin/gunicorn --access-logfile - --debug --max-requests 100 --graceful-timeout 3600 -t 3600 -k gevent -b 0.0.0.0:5000 -w 4 wsgi:ap\r\n |               \\_ /usr/bin/python /usr/local/bin/gunicorn --access-logfile - --debug --max-requests 100 --graceful-timeout 3600 -t 3600 -k gevent -b 0.0.0.0:5000 -w 4 wsgi:ap\r\n \\_ lxc-start -n 621385033517cdc180d4f66c7f4687216602a070178fee15976caff2c325a924 -f /var/lib/docker/containers/621385033517cdc180d4f66c7f4687216602a070178fee15976caff2c325a924\r\n     \\_ memcached Two Docker containers are running, the first is the registry (which, as you can see, happens to be using gunicorn) and the second one, only runs a single memcached process. It’s now possible to simply use the memcached service: $ telnet 127.0.0.1 11211\r\nTrying 127.0.0.1...\r\nConnected to 127.0.0.1.\r\nEscape character is '^]'.\r\nversion\r\nVERSION 1.4.13\r\nquit\r\nConnection closed by foreign host. Now what? We just saw how to basically play with Docker containers and a local repository. The actions we performed can be summarized like this. Note that we didn’t get into details with the Docker save and load commands which are backup / restore features from / to a local tar archive. We didn’t either explicitely use the commit statement, which is used each time docker build applies a line from a Dockerfile and save it as a new image version. In the next article, we will discuss about the configuration of Docker containers. Stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Infrastructure and Operations and tagged DevOps , Docker , LXC . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Docker registry first steps” David 29/05/2014 à 12:39 Nice Info Shared!I would like to add:\r\nUse of the registry, without the index, which is under the full control of Docker, is best suited for storing images on private networks. The registry spins up in a special mode which restricts communication with the Docker index. All security and authentication needs to be taken care of by the user Kihtrak 03/02/2015 à 07:30 Can we have the steps for the following scenario,\r\n1. Private Docker registry creation on rhel.\r\n2. Storing the docker container image on private registry.\r\n3. Setup Websphere Application Server on the docker container\r\n4. Export the docker container to other environments; Import docker container\r\n\r\nThanks. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-02-11"},
{"website": "Octo", "title": "\n                Impact mapping: business success in software development            ", "author": ["Sergey Larionov"], "link": "https://blog.octo.com/en/impact-mapping-business-success-in-software-development/", "abstract": "Impact mapping: business success in software development Publication date 29/04/2014 by Sergey Larionov Tweet Share 0 +1 LinkedIn 0 We had a chance to meet the author of Impact Mapping Gojko Adzic at OCTO Technology. After the interview and Product Owner Survival Camp workshop attendance in London, where Gojko explained us the advantages of this new method, we would like to share with you some ideas about it. What is Impact Mapping? Impact mapping is a visual strategic planning technique that helps teams to align their activities with overall business objectives and make a better decision. Impact Mapping prevents organisations from getting lost while building products and delivering projects by clearly communicating assumptions. Why do I need it? Impact mapping helps you to: produce more value with less software ( business goal oriented ); improve the collaboration and interaction like post-it or Kanban or Storymap ( visual ); get started easily ( fast ); facilitate strategic planning and thinking for a better decision making ( big picture thinking ); test the hypothesis and fail fast finding the better way, then validate a new hypothesis and go on ( multiple criteria ); align technical experts and managers ( simple ); adapt quickly to changing market needs ( flexible ). How does it work? As I said it’s simple. There are only two steps to make your impact map. First is the meeting with stakeholders. Second, make it visual. Preparation On the first step realize what is the business goal asking the stakeholder why we are going to do this job. Then define the actors suggesting who could help us to achieve this goal, who we want to help, who are our customers. The next question would be how could they help us to reach the goal. And the last thing we will treat together is what could we do to support the required impact. Mapping Then the next time you will come to see your stakeholder, to make sure that you understood the right thing and all together you are going to the right goal, draw the map. Examples Racing competition Imagine that you want to win a race with your team. There are at least two people who can make it possible: pilot and mechanic. They have their own ways to make better their job: for pilot drive faster, for mechanic make a faster pit stop. What would help them to achieve this? Better engine for pilot’s car or state-of-the-art tools for mechanic. This map shows how to extract multiple possible solutions for a goal achievement. Online gaming Here is another example. Why do we meet today? To find the way to enhance the number of players up to 1 million. Who can help us? Players can invite new people, internal team and advertisers could attract new players as well. How could they do this? Inviting friends and sharing the results, organizing new events and promoting the game. Then just chose what you want to start from. When you’ll find the very first step in your new product or feature development, don’t forget about the easiest way to test your hypothesis – the minimum viable product for fast and quantitative market testing. Plan your trip We are talking about impacts. Here is one example of our daily life. To get from point A to point B we can use different type of transport and different roads that will influence travel cost and time. Isn’t it easier to make a decision when it’s visual and we can compare different ways? Some secrets Define SMART goals S pecific – target a specific area for improvement. M easurable – quantify or at least suggest an indicator of progress. A ssignable – specify who will do it. R ealistic – what results can realistically be achieved, given available resources. T ime-related – specify when the result can be achieved. Create options to make a better choice Measure everything More profit… How much more is more? Assign and validate measureable values of your success. Make each step meaningful Rethink your user stories, make them lead you to a real business goal. Interview with Gojko Adzic How to define a good goal? Make it measurable. Make sure that it changes somebody’s behaviour. Use 5 Whys to reach the real client needs and success criteria. Use some key questions to facilitate goal finding. You are asked to make a function/project, climb up on the demand stairs. Usually people tell you the solution, but not their needs. Example: I want to go to a bar and take a beer. But the goal behind is socialization or relaxation. Ask: What you really want to archive? > Why is it important? > Use iterative “Why” (5 whys technique or ask “Why” in different way) until client will tell you “This is a ridiculous question. It lets us to earn more money”. You will find the right goal doing this. You could find that the thing that was asked is not at all the thing the client really need. You can lose the realization of project, but win a lot of money for a client and be asked for another analysis later because of good goal definition. Help client to prove that “There is a better way” ;) For what type of client I can use Impact Mapping? There are no limits . Impact Mapping is the way to visualize the enterprise strategy. Typically Gojko uses it for big financial organisations. Who participate in a workshop? Senior technical and business people . Business people help to define a good goal. Technical people propose a better implementation. Who is Impact Mapping for? For senior technical and business people . It helps them to visualize the strategy and align the vision to reach the goal faster. How to apply Impact Mapping in Product Owner’s life? In prioritization management and User Story definition . Usually this method is used for senior business and technical people. But Product Owner could use it to visualize its product vision with the client. And on the User Story level you could change the standard template: “As… I want… for …” to goal oriented one like “… can help us to achieve… by … faster/better” How to apply Impact Mapping on the Low Level approach for a single project of IT department? You probably don’t need Impact Mapping on the Low level when you don’t have a senior business people and clear strategy . However, think how to minimize output (number of useless functionalities/User Stories) and maximize outcome (business value). What is a timeline for Impact Mapping? Usually it’s from 6 months to 5 years . But it depends whether you can define a clear goal for a shorter period of time than it will work too. Can I change the goal in process? Sure . The most important is to reach the goal as soon as possible, if it’s done, then chose a next one. Links Slides Impact Mapping: http://fr.slideshare.net/SergeyLA/impact-mapping-short-octo Impact Mapping by Gojko Adzic: http://impactmapping.org/ Video presentation by Gojko Adzic: http://www.youtube.com/watch?v=PGW631c4cO0 Slides Web Giants (in Fench): http://www.slideshare.net/OCTOTechnology/minicourse-web-giants/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology and tagged agile , Business Agility , Impact Mapping , Product Vision . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Impact mapping: business success in software development” Johnf233 29/05/2014 à 14:08 I think this is one of the most significant information for me. And i'm glad reading your article. But should remark on some general things, The web site style is wonderful, the articles is really excellent  D. Good job, cheers MERGEL 11/07/2016 à 17:10 there are various methodologies to help think about the business value created behind an IT solution: lean startup, canvas, MVP, impact mapping...\r\n\r\nwould help to know the intersection b/w all these (they all handle the same objects in a way or another: story maps, user stories, EPIC, ...in the end it's about the right product features, the right way for the right targets)\r\nin order to build bridges (when teammates or business sponsors in a project are used to different methods or frameworks)\r\n\r\nthanks Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-04-29"},
{"website": "Octo", "title": "\n                The new Web application architectures and their impacts for enterprises – Part 2            ", "author": ["François Petitit", "Mickaël Tricot"], "link": "https://blog.octo.com/en/new-web-application-architectures-and-impacts-for-enterprises-2/", "abstract": "The new Web application architectures and their impacts for enterprises – Part 2 Publication date 27/03/2014 by François Petitit , Mickaël Tricot Tweet Share 0 +1 LinkedIn 0 In the first part of this article, we talked about new front-end architectures, which consist of JavaScript Web applications using APIs provided by a back-end server: The new Web application architectures and their impacts for enterprises – Part 1 . We saw that they recently emerged thanks to the increasing performance of browsers and the rise of industrialization tools for JavaScript development. In this second part, we will focus on: reasons why you should adopt these new architectures; opportunities they provide; impacts for enterprises. Why adopting these new architectures? Address heterogeneous clients with the same back-end code A major advantage of this architecture is the development of an API: functionally oriented and developed using standard technologies such as JSON over HTTP, it can be used by many other clients than the initial Web application. It is a well-known architectural pattern, widely used in the mobile world. Enterprises usually have experience in implementing Web services linking complex information systems with mobile applications. With a server-side MVC architecture, we would first need to develop a service layer calling the back-office, before using it into the Web application. With a client-side MV* architecture, we no longer need an extra-layer, as APIs can be consumed directly. Besides, when starting from scratch, we can invest right away in a “clean” API, free of any specificity related to the GUI technology. … and open the API? If you already use an API for your applications, making it accessible to the outside should be quite easy. You would thus embrace the concept of Open API . This obviously requires adopting a specific approach and setting many other elements, like an ecosystem for your API customers, but it is a good opportunity to consider when you have these Web services available! Improve development productivity At OCTO, this architecture already allowed us to realize a dozen projects of all kinds: Web, mobile or desktop, business or data visualization applications. Looking back, using technologies like AngularJS or Backbone.js significantly improved the productivity over using server-side MVC frameworks such as JSF or GWT. When working in a Java or .Net environment, if we do not want to overload the infrastructure with a new middleware, the productivity of front-end development with technologies like AngularJS is thus a very strong advantage. This is less true if we compare with more web-friendly technologies such as Ruby on Rails or PHP; in that case client-side MV* architectures still retain the advantage of consuming agnostic APIs, as explained before. Deliver more powerful, rich and ergonomic Web applications Beyond the productivity, it is also necessary to compare the end product. When using a MV* framework, we develop the whole application in the browser, for the browser. This allows developers to directly and easily access all features offered, which are numerous since the advent of HTML5: data storage, offline, file system access, multi-threading, data push… Today, Web applications probably use less than 10% of our browser capacities, not because of a lack of use cases but because of the lack of facilities for JavaScript development. Precisely, with these new technologies, we can experiment quickly and often, because the environment is extremely fast (especially live-reload that reloads code in the browser right after a source file changed and the use of a node.js development server, much faster than a Java or. Net server). The browser no longer is the black box that annoyed us when we constantly had to debug code on it. It is the new platform and it opens up new horizons in creativity. Remember the features coming with HTML5 : How many of them do you currently use? How many will you use in a year from now? Replace a heavy and complex application and deploy it as a SaaS Editors are more and more moving their fat client solutions to Web technologies. We can mention Google that has provided a well-known web-mail and office software suite for a long time. But we were even more surprised with the latest version of Microsoft Office, Office 365, which is entirely available in the cloud, with SaaS subscriptions as sales model. This was made possible by taking advantage of the new browser capacities. Using MV* architectures with mainly JavaScript and good tooling is clearly the solution today to address these needs. Complex business applications were also mostly provided as fat clients. This can be explained mainly by the bad performances of Web technologies back then. Now, CIOs generally want to get rid of fat client technologies like VB or Swing, but no long term and open solutions have emerged. One of the last challenger was Flex, but his destiny left in the hands of Adobe is not reassuring. Another one is .Net combined with WPF, in the Microsoft ecosystem. GWT tried and almost succeeded in making realistic the development of complex Web applications. In the end Google, always at the cutting edge of technology, decided to reduce its involvement in this framework, to hire the AngularJS developers and to invest in the Dart platform. The alternative to the maintenance of old applications is now to look into new Web architectures. We still have to be very careful about the relative immaturity of the technologies before revamping complex applications. The platform itself (the Web and the browsers) are mature enough, but technologies are moving fast and can induce a significant maintenance cost for an enterprise that is not a “pure-play” internet company. What impacts for my enterprise? From a technical perspective, the impacts of these technologies on the enterprise are actually quite limited: we know how to develop and run Web applications in production. Besides, setting up an API is nothing really new and security patterns are already used for mobile applications and RIAs. What is new is the need for development skills, especially in JavaScript. Need for JavaScript skills We need to go 5 or 10 years back to understand the fear of developing in JavaScript. At that time, this language was hard to maintain because it was natively offering a very few safeguards: no types, no compilation, no object-oriented concepts… JavaScript was considered as a hacker language, to avoid in production and whose only merit was to be executed by all browsers. And the Web 2.0 came along. In order to create richer Web applications, we started to use frameworks that could generate JavaScript for us: JSF , GWT , ASP.Net, etc. Limitations caused by these libraries (difficulty to step aside the standard behavior, debugging complexity, strong dependency with framework editors) as well as the illusion of entrusting pure Java or .Net back-end developers for front-end development, have led to disappointments on both the applications delivered and the productivity. With the new MV* frameworks, there is more flexibility in the developments and a better productivity, provided you agree to develop directly in JavaScript. The price to pay is to acquire JavaScript skills and more: typically, it is no more about integrating a jQuery plug-in to bring some dynamism to a static web page; instead an entire application is to be built with a JavaScript appropriate framework. Knowledge of application architecture and development industrialization (tests, etc.) are prerequisites for these new technologies. Fortunately, the latest MV* frameworks offer ease-of-use and guidance, which greatly lower the entry barrier even for a non-expert developer. And this is the major achievement of this evolution: client-side MV* applications development is now as easy as that of classic MVC applications . Temptation to create teams by technology layer: front-end and back-end The growing decoupling between front-end (HTML/CSS/JavaScript) and back-end (except to use node.js on the server!) technologies creates a distinction between developers, depending on which technical layer they work on. Separating developers is indeed an opportunity to grow expertise on a technology and to be able to focus on the functional complexity of the applications, but it is also a risk of losing some versatility. Reorganizing the team must be considered: we could simply dedicate development teams for the front-end and others for the back-end. However Agile methodologies recommend building feature teams instead, covering all the required skills for developing features rather than technology layers. Conclusion: some pitfalls to avoid In conclusion, we must keep in mind that the new Web architectures provide great opportunities for developing applications and offer new possibilities that can benefit to end users and enterprises. Though we must be careful when choosing the right technology: These solutions are not necessarily the best for all use cases: there are for instance better solutions for building static websites, in terms of performance and SEO. Comparing native mobile applications with Web applications is still not in favor of the Web: there are still many possibilities with native mobile technologies with no equivalent in Web technologies. Each context must lead to a different strategy choice. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles and tagged AngularJS , Architecture , development , GWT , HTML5 , JavaScript , Mobile , productivity , REST , RIA , Web . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “The new Web application architectures and their impacts for enterprises – Part 2” Abhishek 25/01/2015 à 11:03 Thank you for writing both the articles. They have been very helpful and informative. We are moving ahead in the similar direction as a consulting company. I just wanted to few things from you.\r\n\r\nHow is it going for you guys? Any successful examples of projects done with this philosophy of development? \r\n\r\nAre you able to reuse some of the business services (APIs) written for one project in another project? Also are they written in a database agnostic way?\r\n\r\nLooking forward to interact more on this topic. Mickaël Tricot 27/04/2015 à 15:27 We are a consulting company too and we have worked on these new web architectures for many customers in the last couple of years. And yes, we have seen successful projects where the different applications (mobile, web) are built on top of a single REST API. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-03-27"},
{"website": "Octo", "title": "\n                Edito March 2014            ", "author": ["Jean-Philippe Briend"], "link": "https://blog.octo.com/en/edito-march-2014/", "abstract": "Edito March 2014 Publication date 12/03/2014 by Jean-Philippe Briend Tweet Share 0 +1 LinkedIn 0 We are initiating a new set of articles in order to give you our opinion concerning what’s happening in the Tech world. Javascript, ma muse ? #WebFront In this beginning of year 2014, Javascript MVC frameworks are everywhere. Gaining momentum in 2012 and 2013, they are now well established. According to our customers’ demands, these technologies must be under control in 2014. If one of your New Year’s resolution is to learn a Javascript MVC framework, the first step is to choose one. Beware : this choice must not be definitive. Go for it but regularly keep in touch with what’s happening, evolution is very fast. This month we have chosen an article from Craig McKeachie providing a map of Javascript MVC frameworks as of September 2013 : Choosing a JavaScript MVC framework . Instead of doing a simple comparison based on features, Craig is opposing different aspects : community, leadership, philosophy, dependencies, … As of date, we, Octo, have chosen AngularJS and Backbone.js . Tip : if you choose AngularJS, here is a link which could help you : Ultimate guide to learning AngularJS in one day . Refactoring, always forever ! #SoftwareCraftmanship 2014 : it’s not only about disruptive technologies. Martin Fowler reminds us what are the different refactoring use cases (or workflows) which we may met one day of our developper’s life : Worflows of Refactoring . If you, readers, are not yet refactorers, take a look at these slides and you will understand how refactoring is a base element in a developper’s life (who said Software Craftmanship ?). Because we are not fundamentalists but pragmatics, let’s remind these 2 sentences from Martin : “ Remember the economic justification ” et “ Balance refactoring with feature delivery “. Fest Assert is dead, long live AssertJ #SoftwareCraftmanship Refactoring means Unit Tests. We would like to use the new design of AssertJ ‘s website to remind you that this project is an active fork of the very popular library Fest Assert. As an old contributor of Fest Assert, Joël Costigliola has wanted to come back to a community driven library, with many assertions. We advise you to try this library if you still have not yet. If you don’t use such a framework to write readable or customized assertions, or if you are still sing the old Fest Assert, go for it : AssertJ ! NoSQL challenging #NoSQL If you are interested in NoSQL databases, you should take a look at these articles from Kyle Kingsbury (named Call me maybe : Riak , NuoDb , Cassandra , Kafka , MongoDB , Redis ). He is shaking them to find their limits in terms of durability, consistency, performance : he is looking for promises and then verifies them (!). It’s a must read for both content and approach, or even if you want to quickly challenge a NoSQL database according to your needs. #NeverEndingDebate #AdaptAndProgress #SoftwareCraftmanship We would like to point an interesting article concerning Software Craftmanship movement : Pairing vs Code Review : Comparing Developer Cultures . You can find dozens of articles following the classical schema “[Advantages|Drawbacks] of [Pair Programming|Code Reviews]”, but Paul Hinze wants to do a Developer’s Culture approach. His conclusion : admit your weaknesses and get better. If you still don’t have one, find a team for which code and best pratices are important and solve problems together. According to us, regardless you are doing Pair Programming, Code Reviews or a mix of them : do your own cooking to improve yourself ( Continuous Improvement ). Advanced Optimizations #Performances A new JDBC pool made the buzz : HikariCP . This JDBC pool has been announced as the fastest of the moment with “zero overhead”. We advice you to read the very instructive explanations of their optimisations : Down the Rabbit Hole . The author did a benchmark which happened to be too optimist. Fortunately, he completely rewrited it and now the results are more realistic but still promising. We recommend you to use Tomcat JDBC Pool or HikariCP (be careful, it is still young), which seems to be the fastest on the market and to avoid using DBCP and C3P0. Tweet Share 0 +1 LinkedIn 0 This entry was posted in News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-03-12"},
{"website": "Octo", "title": "\n                The new Web application architectures and their impacts for enterprises – Part 1            ", "author": ["François Petitit", "Mickaël Tricot"], "link": "https://blog.octo.com/en/new-web-application-architectures-and-impacts-for-enterprises-1/", "abstract": "The new Web application architectures and their impacts for enterprises – Part 1 Publication date 21/03/2014 by François Petitit , Mickaël Tricot Tweet Share 0 +1 LinkedIn 0 Web applications evolve. From static HTML sites first to AJAX applications more recently, through multiple dynamic technologies (PHP, ASP, Java, Ruby on Rails…), Web application architectures and their dedicated tools regularly experience major advancements and breakthroughs. For two years, we have seen a new wave of technologies coming, transforming the landscape of Web applications. Unlike RIA or AJAX before, there is no well defined name yet for this new trend. We will call it “MV* client-side architectures” . Here is the main principle: the server no longer manages the whole page but only sends raw data to the client; all the pages generation and user interactions management is done on the client side, that is to say in the browser. In this post, we will go into details of this architecture and explain why it is emerging. In a second post, we will see why it is relevant to embrace it today, opportunities they offer and what are the likely impacts for enterprises. New Web application architectures: what are we talking about? This diagram illustrates the evolution of Web application architectures: Model 1: classic Web application On the first diagram, the Web application is mainly executed on the server side. It sends directly to the browser HTML pages, CSS and possibly JavaScript enhancing the behavior. Then, for each user action requiring new data, the server is queried and returns a whole new HTML page. Model 2: AJAX Web application The second diagram introduces the AJAX pattern, for Asynchronous JavaScript And XML , which appeared in the mid-2000s (see also this article by Jesse James Garrett: http://www.adaptivepath.com/ideas/ajax-new-approach-web-applications/ ). This architecture principle can make the application more responsive by reducing exchanges between browser and server. When a user action generates a client call to retrieve new data, the server only returns view fragments. Thus, only a small part of the screen is refreshed, rather than the entire page. This requires the development of client-side JavaScript in order to manage partial refreshments, for example by using the jQuery library and its $.Ajax function or others tools more integrated with server platforms (such as Java Server Faces or Google Web Toolkit for Java environments). This architecture brought more reactivity but also more complexity. It has many pitfalls: the extensive use of jQuery can make impossible the application maintenance , without implementing complex technical rules (offered today by MV* frameworks like Backbone.js and AngularJS) despite their aim to ease developments, server-side frameworks like Java Server Faces turned out to be very heavy and complex , leading to many bugs and performance issues. Model 3: client-side MV* Web application The third diagram shows the new MV* client-side architecture , whose principle disrupts with the previous ones: now the server sends only raw unformatted data and the client is responsible for generating the screen out of it. The term MV* refers to the MVC pattern, for Model View Controller , widely used server-side to separate data and views management. More and more we use this term MV* , in order to highlight the little differences from pure MVC implementations. But this is an expert discussion… The important point in this new architecture is the shift of all the UI logic from the server to the client . This separation of concerns between server and client is not a new phenomenon. It has been taken back by native mobile applications, consuming APIs independent from the client. The new Web application architectures bring this possibility to Web applications. Why were these architectures not implemented earlier? Basically, the JavaScript language is there since the Web exists. The principle does not seem so revolutionary, since it is very similar to the classic client-server applications that existed already before the Web. So why did not we think of this new architectures earlier? The answer is simple: it was not possible; except if you are Google! Indeed, two factors were limiting the possibility to develop with JavaScript: browser limitations in terms of capacities and performances the lack of industrialization of JavaScript development The end of browser limitations The first point was obvious until Microsoft launches Internet Explorer 9 and 10. Slowness and bugs in previous versions prevented the deployment of applications massively using JavaScript. Unless having the strike force of a Google engineering team, developing a Gmail in Internet Explorer 6 was simply not realistic. Since Firefox and Chrome have become popular in the browser market, Microsoft has caught up, as shown in this chart: The results of SunSpider JavaScript performance tests in different browsers, illustrate the breakdown which happened around 2010, with the huge improvements in Internet Explorer: between IE6 and IE8, performance tests results have improved by factor 25, from 177000 ms to 7000 ms . Since then, performances keep improving significantly. Together with the new capacities of both desktop and mobile devices, the browser can now do more than just displaying Web pages: it can dynamically generate pages, make 2D/3D drawings, execute complex algorithms, etc. JavaScript development industrialization But having a powerful execution platform is useless if we can not develop effectively. The second technological revolution of Web development simply resides in JavaScript development tooling. If you are following this blog, you may have already heard about AngularJS or Grunt . These are examples illustrating the new JavaScript development ecosystem, which can be summarized in two main categories: Development frameworks: while we had libraries like jQuery before for easing JavaScript development, developers now have proper frameworks for structuring their applications. There are two advantages: accelerating development and ensuring a better maintainability of the code. As of today, the best frameworks are AngularJS, Backbone.js and Ember.js. Industrialization tools: the industrialization of JavaScript development has exploded in the past two years, heavily inspired by what already existed for other platforms such as Java. The same way Java developers use Maven, JavaScript developers can now use Grunt to automate testing and build their application , as well as applying the specific front-end development workflow (files concatenation and minification, CSS sprites generation, etc.). The industrialization is also driven by the fact that JavaScript is spreading over other areas than just Web applications, especially server-side with node.js. This is even more surprising to learn that node.js itself is used as the technical platform for Grunt and its numerous plug-ins. As a conclusion, all the required tools exist today for developing effectively and industrializing JavaScript. Conclusion of Part 1 In this article, we have presented what we mean by “MV* client-side architectures” and why they are now emerging. In the following part, we will study why you should now use these architectures , what are the pitfalls to avoid and what are the impacts for enterprises . Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles and tagged AngularJS , Architecture , development , GWT , HTML5 , JavaScript , Mobile , productivity , REST , RIA , Web . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “The new Web application architectures and their impacts for enterprises – Part 1” inf3rno 21/04/2014 à 20:23 For example: REST with linked data, or websockets with zmq are new techniques in my opinion, these are not... elias 16/12/2015 à 10:16 Nice & interesting expression the concept of client sever web application evolution diagrammatically.Thank u. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-03-21"},
{"website": "Octo", "title": "\n                Octo hosted the first Swiss DevOps Meetup            ", "author": ["Farhdine Boutzakhti"], "link": "https://blog.octo.com/en/octo-swiss-devops-meetup/", "abstract": "Octo hosted the first Swiss DevOps Meetup Publication date 06/03/2014 by Farhdine Boutzakhti Tweet Share 0 +1 LinkedIn 0 Octo Technology hosted last week in Lausanne (Switzerland) the first meetup of the Swiss DevOps community. As guest speaker, @NuttySwiss , Senior Site Reliability Engineer at Twitter gave a talk about Apache Mesos , Aurora and SRE. Mesos is the heart of Twitter infrastructure and allows to run multiple services in a reliable way inside their data centers. Aurora is the scheduler that run on top of Mesos. SRE stands for Site Reliability Engineering. Octo was happy to welcome this meetup and is looking forward to help the Swiss DevOps community to grow. For further details about the meetup: http://www.meetup.com/devopsch/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged DevOps , meetup , switzerland . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-03-06"},
{"website": "Octo", "title": "\n                Web Giants: video and slides of January 24th conference at São Paulo            ", "author": ["Nicolas Landier"], "link": "https://blog.octo.com/en/web-giants-video-and-slides-of-january-24th-conference-at-sao-paulo/", "abstract": "Web Giants: video and slides of January 24th conference at São Paulo Publication date 30/01/2014 by Nicolas Landier Tweet Share 0 +1 LinkedIn 0 On January 24th in São Paulo, Mathieu Despriée – Consultant and Senior Architect at OCTO Technology – gave a talk about the innovative practices the Web Giants use to apply: Amazon, Facebook, Google, Netflix and LinkedIn to name some. Here are the video and the slides, covering the following topics: Reduce the Time to Market; Mitigate issues when dealing with growing traffic and high volumes of data; Excellence and quality for the products. Links Video: http://www.youtube.com/watch?v=PGW631c4cO0 Slides: http://www.slideshare.net/OCTOTechnology/minicourse-web-giants/ Dedicated website to the Web Giants (in French): http://www.geantsduweb.com/ Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged Web Giants . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-01-30"},
{"website": "Octo", "title": "\n                Docker Q&As            ", "author": ["Arnaud Mazin", "Cyril Descossy"], "link": "https://blog.octo.com/en/docker-qas/", "abstract": "Docker Q&As Publication date 20/01/2014 by Arnaud Mazin , Cyril Descossy Tweet Share 0 +1 LinkedIn 0 After talking a while with few co-workers about Docker versus LXC and regular VMs, it appeared that a few questions and answers could help people understand the concepts and their main differences. Q: What are the differences between VMs (hosted on VMWare, KVM…) and LXC containers? VMs are about running several full guest Operating Systems (Linux, Windows, BSD…) on shared hardware resources. Those OS can be different in each VM and can be different from the host OS. Each OS (kernel) thinks that it’s running on its own on regular hardware, given generic or hypervisor-specific drivers. Containers are about resources isolation : processes, file-systems, network interfaces and other kernel resources (shared memory, mutexes, semaphores…) within a single running operating system , given some usage limitation (CPU, memory, iops). Containers are often described as chroot on steroids. FreeBSD jails or Solaris zones/containers address the same goal. Virtualization pattern Container pattern Q: Can LXC containers on the same host run several Linux distributions? Short answer: Yes. Long answer: LXC containers share the kernel of their host: they can gather kernel informations (physical/virtual, kernel version…) of the host but have their own group of processes, network configuration and filesystem. So, you can start a LXC Debian Container on a LXC CentOS Host: the uname command returns the information of the host kernel (except the hostname) whereas the file /etc/*-release contains the information of the running Debian. In this case, package management systems are RPM for the host and DPKG for the container. Q: So if containers are nothing but isolation, why can they be compared to VMs? In Linux world, starting a container with a full-OS-like process tree (init/systemd/upstart, ssh) within a dedicated filesystem, gives you access to a logical isolated system which really behaves like a VM, from a user perspective, you can log by ssh on it on a dedicated IP address, do sudo, ps, top, yum/apt-get install, kill, service apache2 restart. OpenStack, libvirt, vagrant, have now plugged LXC as yet-another-way of providing VMs, even if this term doesn’t really fit. Q: What are the differences between regular LXC and Docker containers? From a technical perspective, there are only a few thing to mention: Docker is simply an encapsulation Layer on top of LXC, iptables, exposing an API and a command line client to provide value-added features. Given the used file system stack (formerly AUFS, now devmapper ), it natively provides versioning features of the containers file systems. A container is described as incremental changes to apply over a previous container version. It implements a LXC use case in which you don’t start a whole full-OS-like process tree, but only a single application process, or a single application-centric process tree. You have to provide some script to launch your application at startup, or ask a supervisord to do it for you, but you won’t be easily able to rely on the distribution native SysVInit / upstart / systemd system startup toolchain. Note: as today (0.7.x Docker release), Docker is exclusively bound to LXC. It might change in the future releases with its refactoring getting it more pluggable and extensible. Q: How fast(er) are containers? At runtime: You are very close to the bare-metal performances. Only a little accountability and quota enforcement overhead apply. At startup: You don’t have to start a whole kernel, only fork a process tree. It’s significantly faster. If you use Docker, it’s even faster because you only have to start the application process(es) you need. Nothing useless! Q: How about memory footprint? Memory footprint is only the memory consumed by the running processes. It makes containers so lightweight that it’s totally possible to launch many containers on the same host. Q: What are Dockerfiles? As Makefile or Vagrantfile, a Dockerfile is text file that describes the way to build Docker image, ready be instantiated as containers. Dockerfile syntax is as of today really simple and contains only few self-explanatory statements (FROM, RUN, CMD, USER, EXPOSE). # from which image we start\r\nFROM ubuntu:latest\r\n\r\n# what commands to run to build the container\r\nRUN apt-get -y install python-software-properties\r\nRUN add-apt-repository -y ppa:nginx/stable\r\nRUN apt-get update\r\nRUN apt-get install -y nginx\r\n\r\n# what local file to copy to the container to build it\r\nADD nginx.conf /etc/nginx/\r\nADD nginx.sh /\r\n\r\n# what network port will be accessible for this container\r\nEXPOSE 80\r\n\r\n# what command to launch at container startup\r\nCMD    [\"/nginx.sh\"] Q: What kind of nestings are possible? VMs in VMs It makes sense on a few use cases (for instance when testing deployment of an OpenStack, on top of an OpenStack :)). Performance degradation is really an issue. Containers in VMs This is probably the most meaningful use case, mostly if it’s a real pain to get a VM from your Ops guys. It allows you to keep the containing VM « clean » and completely de-correlate the life cycle of underlying containers. Container reinstallations can be performed as you like. You might have to play with network tricks like NAT, MASQUERADE, PAT to expose the container services. Containers in containers There are no real issues to perform such trick . Hosting container must have LXC elevated privileges to run. Containing depth can be insane, though we don’t see any real-like use case at more than two or three levels of nesting. VMs in containers Never tried this. Theoretically, only applies for hypervisors that can run on a real Linux Kernel (kvm for instance). We don’t see the benefits of this. Q: What about Puppet / Chef / Salt / Ansible? Does it make sense to use such tools? Inside/Outside a Docker container? On the host side, it’s totally worth it. You can for instance find Puppet modules to manager Docker containers. On the guest side, things are far less clear. There are basically two steps where config management could be used: At build time (to create the container) At startup time to finalize the configuration (apply environment specific parameters) before starting the processes As an example, we could imagine some kind of Dockerfile that uses puppet, in a masterless usage, to do some container setup (build time): # let's pretend there is a Centos Docker image with puppet available\r\nFROM centos:puppet\r\nADD conf /etc/puppet/\r\nRUN puppet apply -v -e 'include tomcat7_rhel' Using such tools at startup time or even during run time might be useful if you have to perform very tricky environment configurations. We’re far from having a real consensus about this. In any case, it implies to split the classes/cookbooks to clearly separate two execution steps, quite a lot of refactoring. Conclusion Lots of questions remain yet to answer and we’ll try to address them in other posts: local registry usage, configuration management, administration/operations, network, storage. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Infrastructure and Operations and tagged DevOps , Docker , kvm , Linux , LXC , vm , VMWare . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-01-20"},
{"website": "Octo", "title": "\n                Track your Android application code quality using Sonar            ", "author": ["Romain Felden"], "link": "https://blog.octo.com/en/track-your-android-application-code-quality-using-sonar-2/", "abstract": "Track your Android application code quality using Sonar Publication date 09/01/2014 by Romain Felden Tweet Share 0 +1 LinkedIn 0 When it comes to mobile applications, quality analysis is often overlooked. At first glance these short development bursts don’t fit the usual canvas that comes with thorough monitoring, alerts and whatnots. What if this was just a misconception? What if the cost of setting up a proper automated quality tracking environment was actually really low? This article follows a presentation we held at soft-shake last October. Soft shake 2013 – make use of sonar on your mobile developments from rfelden Meet SonarQube For the uninitiated, SonarQube is a continuous quality analysis platform running as a web server that tracks metrics regarding your code and its structure. It can be extended through plugins, and usually embeds useful tools and checks. Historically SonarQube only dealt with Java code but it has been extended since, and it handles most common languages as of today (available features may vary). Our use case being an Android application, we are not wandering far away from the Java world. A dedicated plugin created by several Octos – sonar-android-plugin – is going to bridge the gap between pure Java code and Android code. System prerequisites SonarQube has to be installed on your computer to continue, a detailed procedure can be found here . For steps 2 and 3 you will need to have sonar-android-plugin installed, this is done following the standard plugin install procedure. Project prerequisites The usual way to analyse a project is to use either a build tool (e.g. maven) or the SonarQube Runner. Since we are working with Java, using the project build tool is common sense. Android-wise, the promoted tool would be gradle, its predecessor being ant. This is not an ideal situation, since gradle does not support all the required features yet (namely code coverage extraction) and ant, well, you know about ant (or you don’t want to). To cope with all this, we are going to go the maven way. Maven has been used for quite some time on Android, unofficially but rather efficiently. I will be using Quality Tools for Android as a support. This github project provides a well-defined maven structure plus profiles covering different aspects of testing and reporting. Demonstration Let’s use a sample application bundled with the maven-android-plugin: MorseFlash . This project is extremely trivial but suits our purpose just fine. To push things even further, here is a fork of the repository cleaned up and configured to run the full demonstration. Let’s look at the application from two points of view: one would be the regular Java code, which is very well covered by SonarQube out of the box, while the other would be Android-specific code. The latter includes layout files, resource files, everything that is not commonly found in a standard Java application. It also includes integration testing, especially its code coverage analysis. Stage 1: look ma, no hands Let’s start with the regular aspect. Using maven, pushing analysis data into sonar is done this way: mvn sonar:sonar Running this on our small project already gives us detailed informations regarding the code, without even using the Sonar Android plugin. Dashboard: out of the box analysis Only issues and code coverage are not accurate, since we are using a standard java reference that does not match Android specifics. You only get standard Sonar Way issues, and instrumented testing is out of the scope. Stage 2: be specific This is not enough, we obviously lack all the Android stuff. To help us out, let’s introduce sonar-android-plugin. Added to SonarQube, it provides an Android Lint based profile and contains a whole new set of violations and issues regarding API levels, graphic performance (layout optimisations) and other fancy checks. Here is what we get with the plugin added. Running mvn clean install and then mvn -Dsonar.profile=\"Android Lint\" sonar:sonar we can see that the number of code files increased (8 to 12, counting 4 new xml files) and so did the number of issues, with a slightly different distribution (critical and info disappeared). Tip: To use both profiles at once (and possibly keep irrelevant messages) it is possible to have Android Lint inherit from Sonar Way. Tip: we have to run clean install before sonar:sonar because Android Lint requires the code to be compiled. Dashboard: Android Lint analysis Stage 3: the full monty There is still a missing part regarding integration testing. Code coverage is generated on the device using emma (built in the maven-android-plugin) and will be extracted using JaCoCo if we add the following settings (taken from the Quality Tools for Android, with only minor changes). (pom.xml) <profile>\r\n    <id>sonar</id>\r\n    <modules>\r\n        <module>morse-lib</module>\r\n        <module>morseflash-app</module>\r\n        <module>morseflash-instrumentation</module>\r\n    </modules>\r\n\r\n    <properties>\r\n        <sonar.profile>Android Lint</sonar.profile>\r\n        <sonar.jacoco.excludes>*test*</sonar.jacoco.excludes>\r\n        <sonar.jacoco.itReportPath>target/jacoco-it.exec</sonar.jacoco.itReportPath>\r\n        <sonar.jacoco.reportPath>jacoco.exec</sonar.jacoco.reportPath>\r\n        <sonar.core.codeCoveragePlugin>jacoco</sonar.core.codeCoveragePlugin>\r\n        <sonar.skippedModules>morseflash-instrumentation</sonar.skippedModules>\r\n        <sonar.exclusions>**/org/jacoco/**</sonar.exclusions>\r\n        <jacoco.version>0.6.2.201302030002</jacoco.version>\r\n    </properties>\r\n\r\n    <dependencyManagement>\r\n        <dependencies>\r\n            <dependency>\r\n                <groupId>org.jacoco</groupId>\r\n                <artifactId>org.jacoco.agent</artifactId>\r\n                <version>${jacoco.version}</version>\r\n                <classifier>runtime</classifier>\r\n            </dependency>\r\n        </dependencies>\r\n    </dependencyManagement>\r\n    <build>\r\n        <pluginManagement>\r\n            <plugins>\r\n                <plugin>\r\n                    <groupId>org.jacoco</groupId>\r\n                    <artifactId>jacoco-maven-plugin</artifactId>\r\n                    <version>${jacoco.version}</version>\r\n                </plugin>\r\n            </plugins>\r\n        </pluginManagement>\r\n    </build>\r\n</profile> (morseflash-app/pom.xml) <profile>\r\n    <id>sonar</id>\r\n    <dependencies>\r\n        <dependency>\r\n            <groupId>org.jacoco</groupId>\r\n            <artifactId>org.jacoco.agent</artifactId>\r\n            <classifier>runtime</classifier>\r\n            <scope>compile</scope>\r\n        </dependency>\r\n    </dependencies>\r\n    <build>\r\n        <plugins>\r\n            <plugin>\r\n                <groupId>org.jacoco</groupId>\r\n                <artifactId>jacoco-maven-plugin</artifactId>\r\n                <executions>\r\n                    <execution>\r\n                        <id>instrument-classes</id>\r\n                        <goals>\r\n                            <goal>instrument</goal>\r\n                        </goals>\r\n                        <configuration>\r\n                            <excludes>\r\n                                <exclude>*test*</exclude>\r\n                                <exclude>*/test/*</exclude>\r\n                            </excludes>\r\n                        </configuration>\r\n                    </execution>\r\n                    <execution>\r\n                        <id>restore-instrumented-classes</id>\r\n                        <phase>package</phase>\r\n                        <goals>\r\n                            <goal>restore-instrumented-classes</goal>\r\n                        </goals>\r\n                    </execution>\r\n                </executions>\r\n            </plugin>\r\n            <plugin>\r\n                <groupId>com.jayway.maven.plugins.android.generation2</groupId>\r\n                <artifactId>android-maven-plugin</artifactId>\r\n                <configuration>\r\n                    <test>\r\n                        <coverage>true</coverage>\r\n                        <createReport>true</createReport>\r\n                    </test>\r\n                </configuration>\r\n                <extensions>true</extensions>\r\n            </plugin>\r\n        </plugins>\r\n    </build>\r\n</profile> (morseflash-instrumentation/pom.xml) <profiles>\r\n    <profile>\r\n        <id>sonar</id>\r\n        <build>\r\n            <plugins>\r\n                <plugin>\r\n                    <groupId>com.jayway.maven.plugins.android.generation2</groupId>\r\n                    <artifactId>android-maven-plugin</artifactId>\r\n                    <configuration>\r\n                        <dex>\r\n                            <!-- Required for EMMA -->\r\n                            <noLocals>true</noLocals>\r\n                        </dex>\r\n                        <test>\r\n                            <coverage>true</coverage>\r\n                            <createReport>true</createReport>\r\n                        </test>\r\n                    </configuration>\r\n                    <executions>\r\n                        <execution>\r\n                            <id>pull-coverage</id>\r\n                            <phase>post-integration-test</phase>\r\n                            <goals>\r\n                                <goal>pull</goal>\r\n                            </goals>\r\n                            <configuration>\r\n                                <pullSource>/data/data/com.simpligility.android.morseflash/files/coverage.ec</pullSource>\r\n                                <pullDestination>${project.basedir}/../morseflash-app/target/jacoco-it.exec</pullDestination>\r\n                            </configuration>\r\n                        </execution>\r\n                    </executions>\r\n                </plugin>\r\n            </plugins>\r\n        </build>\r\n    </profile>\r\n</profiles> Tip: to extract the code coverage file, the device must be connected with adb in root mode. If you are using GenyMotion don’t forget to list the attached devices first so you know the IP address, because switching adb to root mode disconnects the device and it does not reattach automatically. Here is the complete procedure: adb devices adb root adb connect <ip-address> The data we obtain is indeed more accurate, as you can see on the following screenshot. Dashboard: complete analysis There is a flaw in the code coverage counting though, but it is actually due to the tests in morseflash-app that only target code in morse-lib (duplicate class MorseCodeConverterTest that can be found in both modules). In the end you obtain figures that you can drill down into, giving you a fine grain view on covered code. Drill down Why did I tell you all this? These steps only aim at providing a detailed view of what is needed to obtain relevant metrics on an Android project. One could have boldly skipped to the last step, getting the whole shebang at once and possibly defeating the purpose of this demonstration. Either way, now you know how to get relevant metrics out of your neat Android application in no time using Sonar. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged android , code quality , sonar . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Track your Android application code quality using Sonar” Michenux 01/10/2014 à 16:34 I can't make it work it.\r\nCode coverage is always empty in sonar.\r\nI use Sonar 4.4 Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-01-09"},
{"website": "Octo", "title": "\n                The art of benchmarking            ", "author": ["Henri Tremblay"], "link": "https://blog.octo.com/en/the-art-of-benchmarking/", "abstract": "The art of benchmarking Publication date 08/01/2014 by Henri Tremblay Tweet Share 0 +1 LinkedIn 0 A benchmark comparing JavaEE and NodeJS made the buzz lately on the web. I was surprised by the results and decided to reproduce it to verify an intuition. Also, the article was followed by multiple comments that are themselves worthy to be commented. Which brings us to the current blog post. But what is a benchmark? A benchmark is meant to measure the performances a piece of software. An attempt to reproduce in laboratory what will happen in production. If you ask the domain experts, they will tell you that benchmarks are a really dangerous, even though useful, tool. And that they are pretty much providing false results. From my point of view, it is a bit exaggerated but not that much as we will see below. However, one thing is certain. Benchmarks must not be taken lightly and some precautions are in order. For instance: Have database volumes similar to the target Have a data set sufficiently heterogeneous and meaningful to prevent any cache induced bias Hardware with at least proportional throughput when compared to the target Note that, as opposed to popular belief, benchmarks are generally optimistic. If you have bad response times during your benchmark, you can bet that it won’t improve in production. You should beware the opposite. Good response times during a benchmark do not guarantee good response times in production. Because the benchmark might be flawed. Another interesting topic is the benchmark perimeter. Its goal. The one of our current concern is about NodeJS vs J2EE. To be precise, by reading the code, we notice that on the Java side, it is a servlet retrieving a Json document in CouchDB (and not CouchBase ) through couchdb4j . That’s what we want to test. It’s meaningless to suggest to use Vert.x instead because it has a closer behavior to NodeJS. That’s not what we want to test. We are testing NodeJS against a servlet using couchdb4j to retrieve a json document. That’s it. Knowing if another technology could be more efficient is another benchmark. Then, and that’s why experts are wondering, you need to make sure you are testing what you think you are. For instance, for this benchmark, we might reprove using a database on the same machine as the application server. It prevents you from clearly knowing which server is using the system resources. However, I’ve configured the same environment to stay close to the original. Contrariwise, benchmarks are frequently criticized on the optimisation side. “Have you activated the parameter xxx?”. I disagree with that approach. A benchmark is always done at the best of one’s knowledge . As soon as you made sure you are testing the right thing (that it’s really the application server that is the bottleneck for instance), you’re good. What do I mean by “at the best of one’s knowledge”? If, for example, I’m testing Java against C++. But I’m not aware that I should add compilation flags to optimize the C++ code. And Java wins the contest. My results are not flawed. Because, at the best of my knowledge, if I put the Java program in production, it will be faster. My goal is reached. My benchmark is valid. Of course, if I then publish my result and get recommendations from others, my knowledge has now increased. So I can now redo the benchmark taking advantage of my new wisdom. If I still have time and budget for it obviously. And remember, a benchmark is a scientific experience. Publishing the protocol is as important as publishing the results. But back to our original subject. If we dig a little bit into the benchmark, we notice that the results are suspicious. Indeed, it doesn’t feel right to have a stable requests per second rate and a slower response time when adding virtual users. That’s a sign that we have a bottleneck somewhere. So I’ve reproduced the benchmark. I haven’t used the same framework versions because some where quite old and hard to find. So I’ve bootstraped these ones: Java HotSpot 64 bits 1.7.0_21 Tomcat 7.0.35 CouchDB 1.2.0 couchdb4j 0.3.0-i386-1 All this was run on a Ubuntu 13.04 VM with 4 CPUs but that’s not really important. I’ve considered that the “concurrent requests” mentioned were in fact the number of virtual users. I’ve replaced JMeter by Gatling for a matter of personal taste. I got the following results: Concurrent Requests Average Response time (ms) Requests/second 10 233 43 50 1237 42 100 2347 42 150 3506 42 Good news! My results are coherent with the ones in the article. By that I mean that I also encounter this suspicious stability of the requests per second rate (raw performances are a lot lower for Java and NodeJS on my machine for some reason hard to explain without the original benchmark code). Still feels like a bottleneck. To be convinced, a little vmstat is of purpose. cs   us sy id 1134  3  4 93 1574 13 13 74 1285 13 13 74 1047 12 13 75 It shows us that our system CPUs, on heavy load, are just hanging around doing nothing. So I logically follow this analysis by a thread dump to determine what’s blocking my system. \"http-bio-8080-exec-294\" - Thread t@322 java.lang.Thread.State: WAITING at sun.misc.Unsafe.park(Native Method) - parking to wait for <49921538> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject .await(AbstractQueuedSynchronizer.java:2043) at org.apache.http.impl.conn.tsccm.WaitingThread .await(WaitingThread.java:159) at org.apache.http.impl.conn.tsccm.ConnPoolByRoute .getEntryBlocking(ConnPoolByRoute.java:339) at org.apache.http.impl.conn.tsccm.ConnPoolByRoute$1 .getPoolEntry(ConnPoolByRoute.java:238) at org.apache.http.impl.conn.tsccm.ThreadSafeClientConnManager$1 .getConnection(ThreadSafeClientConnManager.java:175) at org.apache.http.impl.client.DefaultRequestDirector .execute(DefaultRequestDirector.java:324) at org.apache.http.impl.client.AbstractHttpClient .execute(AbstractHttpClient.java:555) at org.apache.http.impl.client.AbstractHttpClient .execute(AbstractHttpClient.java:487) at org.apache.http.impl.client.AbstractHttpClient .execute(AbstractHttpClient.java:465) at com.fourspaces.couchdb.Session.http(Session.java:476) at com.fourspaces.couchdb.Session.get(Session.java:433) at com.fourspaces.couchdb.Database.getDocument(Database.java:361) at com.fourspaces.couchdb.Database.getDocument(Database.java:315) at com.henri.couchbench.GetDocumentServlet.doGet(GetDocumentServlet.java:26) Interesting. The system is waiting to obtain an HTTP connection to CouchDB. The problem is that couchdb4j is using httpcomponents . And httpcomponents only allows, by default, 2 parallel connections. That’s sad for my 150 users. They just have to wait for their turn. I fix that by hacking the driver to hardcode allowing 150 connections. Hop! My response times are cut in half and requests per second rate is now at 100. Concurrent Requests Average Response time (ms) Requests/second 10 100 96 50 416 103 100 1023 95 150 1450 89 But my job is not done. If I look at my new metrics, my CPU is higher. However, I’m seeing a quite high system CPU (sy) and the context switching flying high. cs    us sy id 11979 57 21 22 12538 54 22 24 Tomcat 7 is using a synchronous HTTP connector by default. Let’s switch in NIO to reduce the amount of threads required to handle the load. Concurrent Requests Average Response time (ms) Requests/second 10 54 177 50 135 357 100 278 348 150 1988 73 Yippee! The results are really good. For 50 users, a 8,5 times requests per second increase and response times 10x lower. But with an unexpected surprise. The response times for 150 users are worse. The reason is simple. If you have a look at the GC graphs, because of the higher throughput, the JVM memory amount isn’t sufficient for 150 users. The CPU time dedicated to GC is now at 30%. But I will stop my optimization for now. This post is already pretty long. To conclude, a benchmark is a really useful tool to use with care. Nobody knows all the possible optimizations of a system and everyone does his best. However, you should keep an eye open. Suspicious results are a sign that something is wrong and should be investigated. In case of doubt, call an expert ;-) Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 7 commentaires sur “The art of benchmarking” Praise 08/01/2014 à 12:52 It looks like \"The art of profiling\" would be a better title. Nice post anyway! kirk 08/01/2014 à 16:10 Hi Henri,\r\n\r\nBrilliant post rebutting a bad benchmark. Also this is a great example of how to use my Performance Diagnostic Model as I teach it in my course. ;-) I might William Louth 09/01/2014 à 16:07 You can use adaptive control valves to determine the optimal level of concurrency which is something that is not necessarily constant throughout the day or under different workloads.\r\n\r\nI wrote two articles on applying this control mechanism to Apache Cassandra.\r\n\r\nhttp://www.jinspired.com/site/adaptively-controlling-apache-cassandra-client-request-processing\r\n\r\nhttp://www.jinspired.com/site/canceling-uncontrolled-jitter-with-adaptively-controlled-jitter kirk 11/01/2014 à 22:48 @Praise, no, this is the art of benchmarking.. recognizing when things are wrong and taking appropriate steps to fix them. Since benchmarking is about performance it seems appropriate that to fix you'd engage in performance troubleshooting... which Henri did.\r\n\r\nHenri recognized the results were the result of a artificially constrained workload and proceeded from there. You don't have to read Williams blog postings to know that having properly sized connection/thread pools is an important aspect to a well tuned system. William Louth 29/01/2014 à 10:50 \"You don’t have to read Williams blog postings to know that having properly sized connection/thread pools is an important aspect to a well tuned system\"\r\n\r\nWell, it looks like you yourself did not read the articles...hmmm...what's new.\r\n\r\nThe point of the articles is that no one (man) can from the outset determine the appropriate set point of such a system. It will change with time and workload changes. It is far better for the system to adaptive tune this setting. Our work should be to engineer such adaptive mechanisms and then stand back and let the self regulated mechanism (the machine) do its magic.\r\n\r\nI covered this in a TSS video here (3rd video segment):\r\n\r\nhttp://www.theserverside.com/feature/A-revolutionary-new-approach-to-application-monitoring-with-William-Louth kirk 30/01/2014 à 09:33 Hi William,\r\n\r\nThe maths behind queues is very well known and I would suggest that one man can indeed set a reasonable limit on a thread pool size. It just requires you first understand what your constraining/contended resource is, it's average service time and then you can set a limit that prevents your users from taking over your system. I'll agree that this does get tricker in a multi-modal system but it's still a reasonable approach.\r\n\r\nAs for other thoughts from your blog. I know use it as a perfect example of how *not* to find memory issues. By treating a memory problem as an execution problem you end up saying the world is a single color and everything should be viewed that way. In doing so you end up making it more difficult to see and understand why. If you look at it as a memory problem the different view makes it much much simpler to see and understand why. William Louth 30/01/2014 à 10:09 Kirk it is not a memory problem it is either a (resource) capacity management problem or workflow control problem. You either add more capacity to meet the requirements, assume you could make the world ever so small and static, or you control the cost of execution (and its allocate rate).\r\n\r\n...and queuing laws are still to this day being misrepresented and misinterpreted...even by those claiming to be specialists. But AGAIN that is not the issue here. Code changes. Workload changes. Systems and resources changes (they too have queues). Time changes.\r\n\r\nTHERE IS NO ONE NUMBER THAT CAN BE OPTIMAL AT ALL TIMES. THE SYSTEM AND THE VALUES THAT DRIVE ITS BEHAVIOR MUST ALSO BE ADAPTED WITHIN AN EXPLORATIVE MODE. BUT IF PEOPLE CAN'T ADAPT OR THINK IN TERMS OF ADAPTIVE MECHANISMS THEN YOU MIGHT AS WELL PICK FROM THE FOLLOWING SET \r\n\r\n{10, 25, 50, 100, 1000} Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-01-08"},
{"website": "Octo", "title": "\n                SQLFire from the trenches            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/sqlfire-from-the-trenches/", "abstract": "SQLFire from the trenches Publication date 31/03/2014 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 In a first article , I have explained why I think that NewSQL is a disruptive storage technology designed for traditional Information Systems. NewSQL relies on a scalable architecture and is designed to run on commodity hardware. In order to get actual figures for SQLFire, we have built a Proof of Concept for stress test purposes. The goal of this article is to give you some feedback on these stress tests in the chosen scenario. The first sections provide a description of the test environment and hypotheses, as well as some feedback from our experience on the stress test protocol that we have used. If you trust our hypotheses and are mainly interested in NewSQL figures, go directly to the SQLFire feedback paragraph . POC architecture The applicative stack is Java Spring based, with a very conservative design. The goal is to model applications which are common in current Information Systems. The persistence tier has been implemented both with MySQL and SQLFire. We took advantage of the SQL interface of SQLFire as described in the previous article . The application exposes requests through a HTTP API which returns JSON responses. It does not fully implement REST principles, but from a server point of view it is equivalent to an API consumed by modern single page javascript applications. Stress tests description Data generation Stress tests have been performed with the Gatling tool in order to determine the sustainable performance level of both MySQL and SQLFire. We have used 3 scenarios: Transaction scenario : this one is purely transactional. A first article is bought, then after 1 or 2 seconds, a second article is bought, then after 2 or 3 seconds the receipt is emitted. As the Stock table is updated in a consistent manner, all 3 tables are updated by this scenario. Inventory scenario : The inventory of a given store is queried every 5 to 10 seconds, in order to simulate a back-office activity. Turnover scenario : The SaleOperation table is queried every 5 to 10 seconds in order to compute an overview of the turnover of a specified subset during a given period. These scenarios are run concurrently with 850 virtual users for the transaction scenarios, 200 for the Inventory scenario and 200 for the turnover scenario. For the purpose of these tests, a substantial volume of artificial test data has been generated. The effort required by such data generation is frequently underestimated. Nevertheless, the two key requirements for the dataset are to be big enough and to be representative in term of data distribution. Indeed, sequentially generated numbers can lead to very different behaviors than real life data. We have chosen Databene generator as the dataset generator. Why is it good? The table below give you a subset of the Store table: id apeCode contactReference dealerParametersReference groupId 1500 9305P JOMUNA RAMIRO 29309 1501 5742R RUPO SIPU 54435 1502 6867K BOTILO SAZORU 84726 1503 4139H LOPU ZUBA 89186 1504 2874A BOSUNO MUTA 7736 1505 2544Q LIFUTU ZUTO 26976 1506 3822S MORODA POVUCE 51560 1507 1449S JIJI ROJONU 93844 1508 7867G ZISA FENUFO 36446 1509 2327R RADISA BEBE 24972 You can notice that even if the technical id is simply incremented, the business apeCode and the other data look reasonably random. The EC2 platform Stress tests have been performed on the EC2 platform. The infrastructure is described in the diagram below: We have used several instances of Amazon EC2. Instances have been built from ubuntu images. We have not used Amazon RDS for MySQL in order to have a total control on the type of instances that we use. For equivalent reasons, we have not used Elastic Beanstalk because at the beginning of our tests (January 2012) fine tuning was not available. To ease the tedious task of setting up a stress test platform over and over, we have chosen industrialization. We have used knife to automate the creation of the EC2 instances, chef and chef-server to install and configure the applications. Three recipes have been used: base: which installs apt, git, java, maven app: which installs tomcat over the base role mysql: which installs mysql over the base role sqlfire: which installs sqlfire over the base role Since there was no chef cookbook for SQLFire, we wrote ours. The recipe on itself was relatively easy with 3 files of less than 80 LOC. The trickiest part concerned the affectation of the locator role. (Small digression for clarity purpose: SQLFire uses a locator per cluster that is responsible for maintaining the cluster topology and serving it to clients and joining nodes. The first locator has to be unique and known by all the other nodes.) We used Opscode Chef server for that purpose. The first node to start registers as the default locator by storing its IP address on chef server. Each node that connects subsequently retrieves then the locator IP address from the chef server. A bunch of SSH scripts have been used to deploy the application on the tomcat servers and to launch the different Gatling tools. Stress tests: what rocks and what has been learned Concerning the environment A cloud environment and particularly Amazon EC2 is the best platform to run stress tests. You can start with a lot of tests with very small instances, then launch many very large instances to test the scalability of your platform. From a project manager perspective, it is however necessary to provision about 1 day per developer to clearly understand the concepts, the billing , and install a fully operational workstation. Tooling for deployment is very useful. However, based on my experience, I would recommend to proceed a bit differently. Rebuilding a totally new environment with different software configuration was not needed in our case. A combination of AMI (amazon machine image) and automated configuration would have been sufficient. Our SSH scripts have been far more used than the Chef tooling which was probably overkill. Chef server is oversized. Launching our environments by providing the IP address of the locator as a parameter would have been easier to implement while being as efficient. Data generation: the quality of the generated dataset is good and the tool was simple enough to use (declarative XML syntax, maven integration). The relation between tables can be specified and the dataset takes them into account. Two difficulties should be mentioned: relation is managed in the process memory. The maximum dataset size for a single run is limited by the memory size. In our case, the dataset has been generated with several benerator runs. Time needed is a problem too as we reach a throughput maximum of about 4 millions of lines per hour. An alternative could have been running injection tests and then use the generated data. I recommend mixing both approaches: data generation is required in order to have significant results but can be deferred after the first load tests. The initial data can be completed later in an easier way by simply running some injection tests. SQLFire feedback Data modeling The data modeling approach has been already described in the first article and is one of the best SQLFire aspect. The modeling theory is clearly established and the constraints are clearly compatible with our model. Administration SQLFire provides basic tools for backup, import and export . We started with sqlf write-data-to-db but the tool is not easy to use. Each update of the schema, even in the partitioning strategy, requires regenerating the db-schema.xml file. Moreover, the error message on the client importer is not clear. Our recommendation is using syscs_util.import_table_ex which is clearly described in this article . The error messages are much more understandable and the throughput is higher: we have been able to load 5 million lines in 32 minutes. Memory sizing Memory sizing was clearly the trickiest part of SQLFire. In order to have a fair comparison between MySQL and SQLFire we wanted to run the stress tests with the same amount of RAM for both systems. With SQLFire, for about 4.2 GB of CSV data and 19.5 billion lines, a 15 GB heap was required. With 3 JVM each with a 3 GB heap we were able to load the data but the system was not responsive. 15GB of RAM is a very large quantity for a MySQL server. We investigated in order to reduce this figure. First, in order to be efficient, SQLFire stores its index entirely in memory. With our normalized relational schema, we have 9 foreign key indexes and one standard explicit index on the largest table. By removing this explicit index I was able to load 4.2 GB of CSV in 6.15 GB of occupied heap. But in this case again, the system was not responsive. Next, SQLFire can overflow its data to disk by using such kind of directive create table ...EVICTION BY LRUHEAPPERCENT EVICTACTION OVERFLOW PERSISTENT SYNCHRONOUS; . Using this option was our first choice but it was inefficient in our case. Using this option was our first choice but it was inefficient in our case, as activating of deactivating this option does not lead to any behavior change in the heap footprint. This point is quite simple but not trivial. SQLFire stores the primary key of each table entirely in memory. Thus, the relative size of primary key over the total size of a record is important because our data model has relatively small columns and few columns per table. Even if the exact implementation of a primary key constraint in SQLFire is not described, remember that the size of a map with 100 empty objects indexed by integer is not 400 bytes but 68,168 bytes as described in this article . Future version of SQLFire will probably allow to overflow part of the primary keys to the disk. I’m therefore waiting for the final release note of this version. Performance results Stress tests have been performed both with MySQL and SQLFire on different EC2 instances. First the configuration of Tomcat has been updated in order to remove the bottleneck that appeared with about 150 HTTP req/s. We have used org.apache.coyote.http11.Http11NioProtocol with 8 acceptor threads. We have increased the number of files that can be opened by the tomcat process as each TCP connection is seen as a file in Linux. This was done by adding ulimit -n 8096 in /etc/init.d/tomcat6 . The first test on SQLFire with an empty database on an m2.4xlarge instance demonstrated good response times at 2 000 concurrent users. The CPU usage was low (about 3%). The average throughput was measured at 435 HTTP req/s. Next stress tests have been performed on a database pre-filled with the data. MySQL was filled with 19.6 million of lines. Due to the aforementioned memory footprint limitations, we had to remain below 1 million rows for SQLFire. We have stressed the platform using the Gatling tool, with a ramp up adjusted until we noticed a plateau on the throughput while the number of concurrent users was still increasing. The response time was not limited in our test. The 99th. percentile of the response time was below 5 s. for MySQL and below 15 s. for SQLFire. For SQLFire the 95th. percentile was below 12 s. Here is the Gatling results corresponding to the first tests hereafter named Run A: All the other results are summarized in this table: Stress test id Run A Run B Run C Run D Run E Run F Run H Nb. Rows 1,90E+07 1,90E+07 934000 934000 934000 934000 934000 Nb. Concurrent users 5,25E+03 5,25E+03 5250 5250 1050 900 1050 MySQL 1 m1.large 1 m1.large SQLFire 1 m2.xlarge with a synchronous persistent schema 1 m2.xlarge with a not persistent schema 1 m1.small with a not persistent schema 1 m1.small 3 m1.small Tomcat\t5 5 c1.medium 5 c1.medium 5 c1.medium 5 c1.medium 1 m1.medium 3 c1.medium 3 c1.medium Gatling 5 m1.small 5 m1.small 5 m1.small 5 m1.small 1 m1.small 1 m1.small 3 m1.small Inventory (avg req HTTP/s) 33 33 25 26 6 10 11 Transaction_new (avg req HTTP/s) 241 236 133 138 34 48 43 Turnover (avg req HTTP/s) 33 33 25 26 6 10 11 Transaction_with_txId (avg req HTTP/s) 241 236 133 138 34 48 43 Calculation of Total (avg req HTTP/s) 241 236 133 138 34 48 43 Grand Total HTTP (avg req HTTP/s) 789 774 449 466 114 164 151 Max grand Total HTTP (max req HTTP/s) 1100 1081 550 550 150 200 300 To sum these results up, in our use case, the throughput of MySQL is greater than the throughput of SQLFire even if SQLFire is executed on a bigger instance . After further investigations, the MySQL architecture reaches its plateau when the MySQL server is saturated. The disk IO are quite high but it is not the key of the saturation. The IO level on the network are relatively reasonable too. This screenshot shows a very high level of system CPU. I presume that it is due to a lot of concurrent network connections. As the available memory on the MySQL server was high we have increased the dataSource.pool.maxActive parameter on the JDBC pool and the max_connections in /etc/mysql/my.cnf . Several tests with lower values leaded to errors due to no available connection. This level could be further tuned but it gave us a good reference for MySQL performance. Our main conclusion is that MySQL can do aggressive caching in our use case. The throughput of the data to write is easily sustainable with the disks used on EC2. Our application is transaction intensive which corresponds to the use case that we would like to test on SQLFire. Comparison with SQLFire results shows that the transaction request has half the throughput of MySQL’s. As shown in the following screenshot, the whole CPU is consumed by the Java process of SQLFire. In order to understand the difference we first look at the SQL queries generated by our HTTP requests. For each business transaction we get these SQL queries: HTTP request Insert Update Select Inventory 1 Turnover 1 Transaction_new 2 2 6 Transaction_with_txId 1 1 8 Total for Transaction 1 By combining these figures and the figures of the Gatling reports we got an estimation of 6 000 SQL queries/s. for MySQL and 3 200 SQL queries/s. for SQLFire. This is quite high but it does not explain why SQLFire has a smaller throughput. We then looked at the GC usage. During the run C , our GC analysis shows that total GC time is 425 ms. for 450 s. of execution time. The 1.6GB heap is only occupied at 1GB. Other measures show 50s. of GC for 30 min. of run. The GC impact is far below 3% of the CPU usage. We then checked that the queries were correctly tuned for SQLFire. As the data are stored in memory, we would like to be sure that full scan did not lead to this overconsumption of CPU. First, take into account that MySQL and SQLFire schema are defined with the same index. Analysis of the query plans shows the following results CONSTRAINTSCAN 22 INDEXSCAN 1 ROWIDSCAN 19 TABLESCAN 0 The worst case seems to be a constraint scan which should not be problematic according to the documentation . Gemfire which is the low level layer of SQLFire, provides some interesting metrics. We have activated a metric collection every 500 ms. on a small m1.small machine. We noticed a peak at 14 concurrent connections. The maximum throughput was 178 gets/s. on the Store, Transaction and Country regions. During update, we reached 235 update or create per second. This can be the sign of a throughput limit for our small machine with 1 vCPU. However we reach more than 10 000 create/s. during insertion. Lastly we look at the thread dump of SQLFire. During this period we got the following distribution: Thread.State Nb. of threads startup Nb. of threads with 600 concurrent users RUNNABLE 817 416 TIMED_WAITING (on object monitor) 10 10 TIMED_WAITING (sleeping) 2 WAITING (on object monitor) 14 415 WAITING (parking) 10 10 At startup, the runnable threads are quite uniformly spread in SQLFire. 3 hotspots can be localized with 165 threads processing the network in com.vmware.sqlfire.internal.impl.drda.DDMReader.fill , 133 threads in com.vmware.sqlfire.internal.impl.drda.DRDAConnThread.parseEncodedString and 140 other threads in com.vmware.sqlfire.internal.impl.drda.DRDAConnThread.parseCcsidMBC . When the number of concurrent users increases, the repartition is different. For example with 600 concurrent users, 406 threads are processing the network in com.vmware.sqlfire.internal.impl.drda.DDMReader.fill and 401 threads are waiting in com.vmware.sqlfire.internal.impl.drda.NetworkServerControlImpl.getNextSession . My analysis draws me to conclude that SQLFire is in general CPU consuming but becomes finally becomes limited by the network processing when the number of concurrent users increases. However, we can’t get more details in the documentation nor can we tune parameters in order to go beyond this limitation. Conclusion This Proof of Concept has allowed us to identify SQLFire strengths and weaknesses. The use case and product combination that we have chosen were not the good ones. A relatively small platform is not effective for SQLFire which has a high overhead in memory occupation and CPU consumption. Using SQLFire on one node is clearly not a good choice. The supposed latency benefit of the in-memory storage was totally invisible in our use case thus there is no visible advantage into using SQLFire for this purpose. The overhead becomes larger with a distributed architecture and we could not run this architecture properly on the small platform we had chosen. Finally SQLFire is a relatively small products. Data loading tools are not always effective, architecture limitation leads to a very high memory consumption. Next version will probably to solve some part of these problems. As it is today, SQLFire cannot be a MySQL one-to-one replacement on EC2. The VMWare benchmark described in the sqlfire best practices guide may lead to such conclusion as it seems to be close to our use case. I see three crucial differences. First, it runs on a VMWare platform with relatively more resources than our EC2 platform. Second, the exact scenarios are not described. When I studied the Spring Travel Application that was used, I noticed than none of the SQL queries generated contains joins. On the contrary, the application of our POC implements a lot of joins. And finally the benchmark only mentions a number of concurrent threads without details about think times and scenarios. As with a smaller number of threads (1780 threads for VMWare benchmark and 5250 concurrent threads in our case) their MySQL server is saturated, I assume that their think time is relatively small. Such benchmark is much more IO intensive with more simple queries. To conclude, clearly identifying the performance bottleneck of the application with concrete measure is a key point before choosing a technology. Some real architectural advantage of NewSQL technologies can be completely hindered by a specific use case. Many thanks the NewSQL team which has participated in the implementation of this Proof of Concept (in alphabetical Order): Nicolas Colomer, Nicolas Landier, Borémi Toch and Djamel Zouaoui. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged NewSQL , NoSQL , Performance , RDBMS . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-03-31"},
{"website": "Octo", "title": "\n                Let’s dig into SQLFire            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/lets-dig-into-sqlfire/", "abstract": "Let’s dig into SQLFire Publication date 20/03/2014 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 A year ago, I promised in a previous (French) article to test the ability to migrate a standard Hibernate/SGBRD application to a NewSQL technology. It is now time to give you the results of our investigation. Don’t worry I will first sum up this previous article and explain why I strongly believe that NewSQL is an important subject. Then I will present the hypothesis of our POC. And finally I will give you the results of this POC, our conclusions about what we will do the same way and what we will do differently on a real project. NewSQL: What is it? In what way is it different from NoSQL? Why study NewSQL now? What is it? Many of you will think of NoSQL. Experiences from the web giants have shown that NoSQL proposes a new way to think about storage. NoSQL architectures are new kind of storage architectures that are distributed i.e. data are spread over several computers, without schema so without SQL querying capability, and without transactional guaranty . In return to such limitations NoSQL architectures have shown very high scalability over commodity hardware. NewSQL, as presented by InfoQ is at the intersection of 3 architectures: relational, NoSQL and data grid. In what way is it different from NoSQL? Indeed NewSQL is a distributed storage , with a scalability purpose like NoSQL, potentially stored totally in memory, but queryable through a SQL interface . NewSQL makes several fundamental different choices from NoSQL. The first choice is the choice of the SQL language which is the lingua franca of the Information System. The second one is the choice of a relational schema with some limitations on the possible transactions. E.g. in SQLFire only requests with colocalized execution plans are authorized and transactions are limited to the READ_COMMITTED mode. And finally the choice is made to use the distribution and data replication to carry out the scalability and resiliency of the data. SGBDR have been designed 30 years ago with the technological constraint at that time. Disruptive architectures like NoSQL have taken better advantage of the fundamental researches and technical improvements of the last 30 years than the SGBDR whose core is very similar to the original one. Study of Ben Stopford at QCon 2011 as shown that only 6% of the instructions executed by a RDBMS engine are really useful. Why study NewSQL now? I am convinced that NewSQL can build a synthesis between these two worlds. With its better proximity to existing SGBDR architecture, it could offer an easier solution to take benefit from these novelties. So, is today the end of the relational database ? Probably not as they have proved their efficiency for 30 years and existing application will continue to use them. But in virtualized environments like the cloud , a distributed storage, being able to use commodity hardware and using primarily the RAM can be a concurrency advantage . Hypothesis for our research study Previous architectural considerations let predict a scalability gain and a better behavior on commodity hardware. But we have learned from experience that purely theoretical considerations can be very misleading. So we have decided to compare the behavior of an application on a MySQL database and on a SQLFire database : the NewSQL product by VMWare. The hold use case The hold use case is a sale application. One of our client had a scalability limit on such use case with its SGBDR. This application is multi-country and multi-Point of Sale. Each point of sale owns its stock which is updated during each transaction. Currency conversion, Value Added Taxes are computed by the application according to country specific parameters. Each sale operation is recorded and grouped in a sale transaction at the end of which the customer will pay. The mental image of a supermarket cash desk is the easier way to keep in mind these concepts. Constraints for each implementation We have set the following constraints for each implementation: Operation logging and inventory updates must be transactional All data related to a same shop must be consistent The original applicative code should be changed as little as possible The characteristics of the SQL schema We chose to set our own rather than use an existing benchmark so that we could precisely define the database schema characteristics. In distributed storage systems, data partitioning is crucial. Schemas with simple relations (or schemas foregoing them altogether) are easy to partition and do not model the real complexity of the task. Our schema is described below: Several sale operations are grouped into a sale transaction which is identified by a payment of the customer. Each time a customer buys an article, a line is inserted in the SaleOperation table and the stock of the corresponding article is decremented by 1. If this operation is the first one of the transaction, a line is inserted in the SaleTransaction table which will contain the total for the transaction. This line will be updated by each new operation. This schema is particularly hard to partition due to the numerous relations between tables that are used by these transactions. Indeed, colocation is a key property for a transaction in a distributed storage system. If a transaction requires to read or write data on more than one computer – one node – the performance will suffer. The characteristics of the transactions In order to challenge this we have queried this schema using four different transactions as described in the following diagram: Buy: a new line is inserted in SaleOperation , the corresponding Stock line is decremented and the SaleTransaction total is updated. This query goes along with another one not represented in this schema. This second query involves Country and CategoryFamily and VAT in order to identify the corresponding VAT rate to apply. Total: the corresponding SaleTransaction line is queried to display the grand total to be paid. Inventory: is an operational request to get an overview of Stock for a given Store Turnover: does a group by over SaleOperation in order to display the turnover  for a given store over the last period Different ways to distribute data We will finish this article by summing up our experience migrating these types of schemas to SQLFire. Some of our readers will probably argue that database technology has allowed the partitioning of data since the mid-90’s, so why has it suddenly become so interesting now? Distributed databases are based on either shared-nothing pattern , where two databases are put side by side and the application has to deal with the limitation, or shared disk / shared cache , where an extra layer synchronizes the nodes. From IBM Redbook “Data Sharing in a Nutshell” sg247322 page 16 NewSQL databases try to offer a new approach. SQLFire in particular is based on a shared nothing approach with some constraints given to the queries . Extra SQL language keywords  define how data are spread across the different nodes. By default, lines of a partitioned table are spread across nodes by hashing the primary key. If searching for a line by the primary key, the node that gets the request can immediately identify the node where the data is located. Conversely if you look for a line with another filter, the first node has to send the queries in parallel to all the nodes and then has to merge the results. When joins come into play, searches between tables are frequent so it is far more efficient if data frequently queried together are collocated on a same node. That what’s allowed by SQLFire with such keywords. CREATE TABLE Account (… client_id int constraint FK4A[..] references BankClient …)\r\nPARTITION BY COLUMN (client_id)\r\nCOLOCATE WITH (BankClient); In this example, Account will be located on the same node as the related BankClient . How to distribute data with SQLFire So, back to our example, how do we choose the partition strategy for our schema? French reader can find further details in the original French article by Sebastian de Bellefon. The tables of our schema are all of very different sizes. The following diagram describes the size of each table. Partitioning strategy In order to be representative a scalable architecture, we hypothesize that the bigger table cannot be contained in a single node. Partitioning serves to define how data should be stored. To do so, we segregated the tables in two groups: Large tables with a many modifications and few large queries Small tables with few modifications but many reads The first group had to be partitioned to spread data over several nodes. Updates can be performed in parallel on several nodes. However, large queries which require global searches will be ineffective as they require access to several nodes. The second group requires replication on each nodes. In this case, every read access is very quick but each modification requires synchronization of every nodes. I learned of these concepts during Ben Stopford’s talk at QCon 2011; they are also clearly explained in SQLFire documentation. Partitioning Replication Large volume of data can be stored Data size inflates Write effective Write is not very effective Read can be distributed but with a high latency Read is effective Choice of the pivot In order to apply these concepts, the first task is to identify the main concept that will be used as a pivot to organize other large tables. In our case, SaleTransaction is clearly this pivot table. Clients transactions can be spread over different nodes and transactions can be inserted concurrently in those nodes. We must then identify the auxiliary data (i.e. the numerous data that can be collocated with the transactions). One important point: the partition key of the auxiliary data should be the same as the partition key of the principal table. Each collocated table has to contain the columns of the partition key. Then data of the partition key should be properly distributed. In our case, this means putting the SaleTransaction primary key in the SaleOperation table and studying the distribution of the operations by transaction. SQLFire can specify the distribution policy by hashing or by range but here a hashing strategy is sufficient. Stock , and Store could be seen as other candidates for a second main concept. However, notice the problem on the Product table: if you look back to the database schema you will see that one line in of the Product is related to several inventories ( Stock ) and several transactions. How to partition such a table? It is impossible. Why not replicate it? Another limitation of SQLFire is that for standard SQL queries, execution plans including joins have to be collocated (a stricter definition of these limitation is described in the documentation ). We had to use an alternative syntax for queries between Store and Product . Moreover the transaction design is highly optimized for collocated data as explained here . But the best and most simple argument is that Store and Stock are very small compared to SaleOperation and SaleTransaction tables and they will never grow. We therefore decided to replicate them. The other data are clearly referential data, rather static and with very low volume (e.g. VAT rates). These data will be replicated on each node. Each modification will require to synchronously update the data on the network but this is neither a frequent nor a response time sensitive operation. Impact of the access pattern Lastly we have to check the selected access pattern to validate the choice made on the schema. Here clearly, both the Buy and Calculation of Total transactions can be collocated. The Inventory Transaction will be collocated because all of the data involved are replicated. The Turnover transaction will have to query several nodes and aggregate the results. As there are no joins involved this functionality is provided by SQLFire. select SUM(op.amount), op.currency from SaleOperation op where op.groupId=:groupId and op.date >= :date group by op.currency order by SUM(op.amount) desc This allows to distribute the query over several nodes. As such the final repartition model we have chosen is as follows: Implementation The implementation of this model is quite simple. I will show you the difference that exists between a schema for Derby and a distributed schema for SQLFire. create table CategoryFamily (...) REPLICATE;\r\ncreate table CategoryFamily_VAT (...) REPLICATE;\r\ncreate table Country (...) REPLICATE;\r\ncreate table Product (...) REPLICATE;\r\ncreate table SaleTransaction (... primary key (id)) PARTITION BY PRIMARY KEY;\r\ncreate table SaleOperation (... saleTransaction_id bigint constraint FK4ACCC4C04589BBE4 references SaleTransaction, primary key (id)) PARTITION BY COLUMN (saleTransaction_id) COLOCATE WITH (SaleTransaction);\r\ncreate table Stock (...) REPLICATE;\r\ncreate table Store (...) REPLICATE;\r\ncreate table VAT (...) REPLICATE; Are there limitations? Yes obviously. The most visible limitation that is to not be able perform joins on non-collocated data. But this choice is clearly what allows SQLFire to have joins that are as efficient as a standard SGBDR while having an architecture as scalable as NoSQL – at least in theory. Conclusion and thanks NewSQL brings scalability and resiliency by partitioning and replicating data over several commodity hardware nodes. We have seen in SQLFire, our test product, how we can program it, somewhat restricted. We have also seen that it means we need a better conception on the schema. In the next article of this series I will give you our operational results for this POC. Many thanks to Sebastian de Bellefon  whose work inspired this article. And the entire NewSQL team who participate in the implementation of this Proof of Concept (in alphabetical Order): Nicolas Colomer, Nicolas Landier, Borémi Toch and Djamel Zouaoui. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged NewSQL , NoSQL , Performance , RDBMS . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2014-03-20"},
{"website": "Octo", "title": "\n                Hadoop 2 stable release is coming and why you should care            ", "author": ["Rémy Saissy"], "link": "https://blog.octo.com/en/hadoop-2-stable-release-is-coming-and-why-you-should-care/", "abstract": "Hadoop 2 stable release is coming and why you should care Publication date 05/09/2013 by Rémy Saissy Tweet Share 0 +1 LinkedIn 0 The first beta version of Hadoop 2 has just been released. It is the 2.1.0. More interesting, the stable version is expected to follow by mid-September. Only a few bugs away from Hadoop 2! That’s a good news but you might be wondering why you should care about it? After all, the most important is what this new major version can bring to your next datalab or to your production cluster, isn’t it? In this article, we will cover the differences between Hadoop 1 and Hadoop 2 that you should care about. YARN Probably the first thing that comes to mind when talking about Hadoop 2, YARN is also the most important change. In YARN, both the JobTracker and the TaskTracker have been replaced by the ResourceManager and the NodeManager , two daemons dedicated to cluster resources scheduling. Below some facts implied by YARN: A better scalability The first reason for this architectural evolution is a more efficient resources management which fosters scalability of clusters beyond several thousand nodes. Good to know but these are extreme cases and it is most likely that you won’t have to cope with such giant clusters. New distributed computing frameworks Much more interesting is the side effect of this architectural evolution. Indeed, MapReduce is now a framework like any other. So what? Well, it is similar to migrate from a mass production to tailor made. Now you have the opportunity to use the distributed computing paradigm that fit the best to your needs given your data and the algorithms you want to apply on it. And it has already begun. Let’s looks at these few examples: Tez, a tailor made MapReduce for Hive and Pig Yes, Tez is a reimplementation of the MapReduce paradigm. However, it integrates some specificities that makes it fit much better to the specific needs of SQL like analytics tools . Therefore, a request performed through Tez generates less jobs to run on a cluster. Who says less jobs means less resources needed to perform the actual work, thus leading to faster execution time. Storm-Yarn and Spark Streaming, for real time distributing computing and data flow processing Twitter Storm and Apache Spark are Open Source solutions for real time distributed computing and data flow processing. Yahoo! has recently released a port of Storm for YARN: Storm-Yarn . Therefore, if your problem is to build computing or dataflow processing topologies  (to compute agregates,  predictions using R ou something else, …) on large data flow volumes that feed the cluster in real time, it is now possible to do it without needing to build a separate and Storm dedicated cluster and without generating MapReduce jobs. Giraph, for distributed graph analysis In a previous article , we discussed about graph databases and how to process it. Apache Giraph is an Open Source solution for distributed graph processing. Facebook uses it to analyse its social graph. Therefore, now you can rely on the resources of your Hadoop cluster to perform this kind of analysis. A finer grained control over resources allocation In YARN, the ResourceManager and the NodeManager are dedicated to the cluster resource management and their scheduling. Since MapReduce is no longer tightly coupled to resource manager, a Hadoop 2 cluster does not manage mappers and reducers slots. Instead, we talk about containers which is a more generic abstraction. A container can execute any kind of processing depending on the framework you chose to use for that specific job. Concretely, what is the difference? Unlike Hadoop 1, a container is sized by two parameters: the minimum and maximum amount of memory it can use the minimum and maximum number of vcore it can use Simpler libraries to write your own YARN applications The API used to develop a distributed computing framework on YARN (called an application), is a bit complex. New libraries have been written in order to simplify such development thus fostering the integration of new frameworks on top of YARN. Therefore, if you have a computing need that is currently not properly fulfilled by the existing frameworks or if the cost to port your existing distributed codes is to high, you can write your own YARN application and thus take advantage of Hadoop distributed filesystem and resource allocation in a better way. Hadoop on Windows Hadoop 2 now officially supports Windows Server and Windows Azure. Moreover, Microsoft is increasingly involved in Hadoop development. Therefore, if your IT and your internal skills are mainly on Windows platforms, moving to Hadoop is no longer a synonym of moving to GNU/Linux. And that can cut some superfluous training costs. HDFS HDFS also benefits from some major improvements even thought some of them are already widely spread in the most common Hadoop distributions. NameNode HA One of the first major improvement of HDFS 2 is the High Availability of the NameNode. You already know it since it has already been released in most if not all Hadoop distributions. Snapshots It is now possible to create readonly or readwrite snapshots of HDFS directories. These snapshots have the following important features: the cost of creating a snapshot and the memory space it uses is claimed to be constant (O(1) complexity) quotas on the maximum number of snapshots snapshots metrics are available through the NameNode WebUI NFSv3 The HDFS command line and web client were not very user friendly and convenient to use, especially for non technical people. Now, it is possible to access HDFS though an NFS share. Therefore, HDFS is viewed and manipulated like any other network share on a network. Used in conjonction with Kerberos which is already supported in Hadoop, it is now possible to provide HDFS shares to non tehcnical people in your company, thus facilitating its adoption. However, it is a new feature and like any other new feature, it will be necessary to assess it first in order to understand how HDFS specific features (block sizes, write once files, append only, …) impact the use of an NFS share. Conclusion With this new major release, Hadoop has reached a new maturity level and is now ready for our IT departments and corporations. Moreover, the integration of NFS, snapshots and the possibility to use new distributed computing frameworks clearly make Hadoop the glue , that is facilitating the development of large scale processing on large volumes of data. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , News and tagged Architecture , Hadoop , HDFS . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2013-09-05"},
{"website": "Octo", "title": "\n                Breakfast event ‘the Giants of the Web’ in Brussels            ", "author": ["Hanneke De Visser"], "link": "https://blog.octo.com/en/breakfast-event-the-giants-of-the-web-in-brussels/", "abstract": "Breakfast event ‘the Giants of the Web’ in Brussels Publication date 12/04/2013 by Hanneke De Visser Tweet Share 0 +1 LinkedIn 0 UNLOCK THE SECRETS OF THE GIANTS OF THE WEB 10 tips for improving your business In the US and everywhere else, the Giants of the Web are reinventing the way IT is done. These revolutionaries are Amazon, Facebook, Google, Netflix, and LinkedIn , to name but a few. OCTO is releasing L es Géants du Web , a new book explaining what has changed in the world of IT. Join us at our breakfast event where Ludovic Cinquin, VP of OCTO Technology and co-author of Les Géants du Web , will speak about the innovative practices that make the Giants of the Web so successful. Come and share in our passion! To give you a brief insight, we will talk about how these Giants: Reduce their Time To Market by using practices like Lean Startup and Continuous Deployment , Address ever-growing challenges relating to increases in traffic by adopting NOSQL and Commodity Hardware technologies, and Maintain excellence and high-quality by abiding by Pizza Team principles and by keeping their teams Feature-oriented while never forgetting the importance of measurement. This free event is geared toward anyone interested in drawing inspiration from the Giants’ business culture: marketing teams, project managers, architects, managers, IT directors, and geeks of all creeds. Please note that registration is required! Schedule 8:15AM: Reception 8:45AM – 10:00AM: The Top Ten Business Practices of the Giants of the Web – A presentation 10:00AM – 10:30AM: Wrap-up – Q&A Location: Hotel Le Méridien Carrefour de l’Europe Brussels 1000 Belgium (next to Brussels Central Station) Speaker Ludovic Cinquin – CEO France The seminar will be held in French with support material in English. Click here to register Tweet Share 0 +1 LinkedIn 0 This entry was posted in News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2013-04-12"},
{"website": "Octo", "title": "\n                Graph databases: an overview            ", "author": ["Michel Domenjoud", "Thomas Vial"], "link": "https://blog.octo.com/en/graph-databases-an-overview/", "abstract": "Graph databases: an overview Publication date 12/07/2012 by Michel Domenjoud , Thomas Vial Tweet Share 0 +1 LinkedIn 0 In a previous article , we introduced a few concepts related to graphs, and illustrated them with two examples using the Neo4j graph database. For the previous years, many companies have been developing graph databases — as software vendors like Neo Technology (Neo4j), Objectivity ( InfiniteGraph ), Sparsity ( dex* ), or by building their own custom solution to integrate it into their applications, like LinkedIn or Twitter. Thus it can be hard to grasp a global picture of this rich landscape in continuous evolution. In this new article focused on graph databases, we will give you the elements that are necessary to understand how they fit into the ecosystem, compared to the other kinds of databases and to the other types of graph processing tools. Specifically, we will try to answer an important question — when to use a graph database and when not use one. The answer is not that obvious. What is a graph database? A graph database is a databases whose specific purpose is the storage of graph-oriented data structures . Thus it’s all about storing data as vertices and edges. By definition , a graph database is any storage solution where connected elements are linked together without using an index . The neighbours of an entity are accessible by dereferencing a physical pointer. There are several types of graphs that can be stored: from a “single-type” undirected graph to an hypergraph, including of course property graphs. Hence a graph database meets the following criteria: Storage is optimized for data represented as a graph , with a provision for storing vertices and edges Storage is optimized for the traversal of the graph, without using an index when following edges. A graph database is optimized for queries leveraging the proximity of data , starting from one or several root nodes, rather than global queries Flexible data model for some solutions: no need to declare data types for vertices or edges, as opposed to the more constrained table-oriented model of a relational database Integrated API with entry points for the more classical algorithms of the graph theory (shortest path, Dijsktra, A*, betweenness, …) Positioning The NoSQL movement has been gaining popularity for the past few years, in particular because it can address several issues for which relational databases don’t give a satisfactory answer: availability , for the processing of large datasets, and partitioning (relational databases give too much priority to consistency, the ‘C’ in the CAP theorem) flexibility of the schema difficulty in modelling and processing complex structures like trees, graphs, or too many relationships In the database ecosystem, graph databases mostly address the last two items : process highly connected data easily manage complex and flexible data models offer exceptional performances for local reads, by traversing the graph Positions of NoSQL databases (source: Neo4j) Graph vs relational databases A graph database is a good fit for exploring data that are structured like a graph (or derived such as a tree), in particular when the relationships between items are significant. The ideal use case for a query, is starting from one or several nodes and traversing the graph. It’s always possible to issue more global queries such as myEntity.findAll (“find all entities of a kind”), but in this case an indexation mechanism must be used. Such a mechanism can be crafted into the graph (supernodes added for indexing purposes), or use another solution on top of the database (with Apache Lucene for example). By contrast, relational databases are well fitted to findAll-like queries, thanks to the internal structure of the tables. It’s all the more adequate when aggregations over a complete dataset must be performed. Despite their names though, relational databases are less suited for exploring relationships. Such use cases need indices, in particular foreign keys. As stated before, with graph databases traversals are performed by following physical pointers, whereas foreign keys are logical pointers. To make it clearer, let’s take a naïve example where we try to store companies, people who work for them, and for how long they have been working there. For instance, we are trying to find all people working at Google. With a relational model, we could execute the following query, which would probably need 3 index lookups corresponding to the foreign keys in the model. In the case of a graph, the query will need 1 index lookup, then will traverse relationships by dereferencing physical pointers directly. This is a very simple example, however it shows a situation where the performance of a graph database will be superior than that of a relational database. The difference in performance will be all the more important as the volume of data increases, because except the first index lookup to find the starting node(s) for traversal, the query will more or less run in constant time. With a relational database, in contrast, each index scan dereferencing a foreign key will cost O(log2 N) if using is a B-Tree index (N being the number of records in the table). Comparing performances between graph databases and other types of databases is a challenge, because of their difference in nature and in purpose. For a query, a graph database will necessarily encompass a relational database if the access pattern is what it was designed for: a traversal. The difference will be even greater when the depth of the traversal is important, or when it is not known in advance (the execution plan for an equivalent SQL query could not even be optimized). Some public benchmarks illustrate it well. Graph storage and graph processing Another comparison that can be made concerns graph databases vs large-scale graph processing frameworks, such as Google’s Pregel or BSP, that were recently presented . The first difference is that they are tools with quite different purposes, although they represent data in the same way.  Jim Webber from Neo Technology gives an interesting (though not independant) point of view : graph processing tools , as their names say, only address analytical use cases (OLAP) , whereas graph databases mainly address storage and transactional processing use cases (OLTP) graph processing tools, akin to other tools like Hadoop, incur significant latency costs (several seconds) when performing calculations. In contrast, Neo4j aims at low latency reads (around a millisecond) the two families don’t aim at the same volume of calculations, and don’t have the same requirements in infrastructure: with a graph database such as Neo4j, high volumes are typically handled with vertical scaling on a single server. Tools like Pregel handle them with a scale-out approach, by distributing the processing across several commodity servers graph processing tools are fully distributed in nature, a characteristic that most graph databases don’t have Thus we deal with two different families that address different types and scales of problems. Speaking about use cases, a graph database will be well fitted for navigation inside a graph, or solving a shortest path problem, whereas a graph processing framework will address clustering problems , betweenness computations or a global search over all paths. Graph databases have a structural limitation regarding their size: partitioning a graph is a difficult problem , in particular when it has to consider parameters such as the network latency, the access patterns in the graph (which relationships are followed the most), and the real-time evolution of the graph. Some graph databases like OrientDB and InfiniteGraph advert themselves as easily partitionable, while the Neo4j guys are working on it. But one has to keep in mind that a graph databases is adequate for the volume that most organizations have to deal with, and that it can store several billions of nodes on a single server. One should not consider Pregel at the expense of Neo4j every time — just like relational databases should not be dismissed altogether because they can’t handle high volumes as well as BI tools or a SMAQ stack (Storage, MapReduce And Query) like Hadoop. The presentation of the new challenger Titan by Marko Rodriguez gives more insight on the rich graph ecosystem. The following chart, taken from the presentation, sums up very well the position of several tools, depending on the volume of data to handle and the expected speed of the graph traversal. The bigger the graph (and the portion of the graph being traversed), the higher the latency. Classification of graph processing tools (source: Marko Rodriguez) Data model A last point where graph databases shine, is the ease of data modelling. Oftentimes, when one has to represent a business problem and the associated entities, it looks like this: Business model Then, when one has chosen the storage system, the data model must be fitted into it. If a relational database is chosen, one generally starts by normalizing the model so that it complies with the 3rd normal form, and it could look like this: Relational model But if one chooses to model the data with a graph database, it will probably look like this: Graph model Modelling data as a graph is natural , and has the nice benefit of staying legible even by non-technical people. The previous example is simplistic, and just as a relational model is sometimes denormalized for performance reasons, some less legible optimizations of the graph may be brought into the graph. So, what to choose? There is no single answer to this question, and the take-away of this article is that everything depends on how the data will be used . If your system requires you to query data like a graph, a tree, a hierarchical structure — in short with a complex and highly connected model –, then a graph management systems is probably the best choice. The next step depends on the way the data is to be processed: if you have real-time access needs and the volume is not too high, a graph database or a custom graph representation in memory should be fine. If there is in addition a need to store data as a graph, a graph database will do it. Such a choice will allow you to easily traverse the relationships, with some trade-offs like complementary indices for reporting or CRUD display. And vice-versa if you opt for a relational database. Lastly, if the volume of data to process is huge, a graph processing solution must be considered, at the cost of higher processing delays and a batch approach. The election of the database is a crucial architectural decision that is often made at the beginning of a project, but it may not be carved in stone. By keeping one’s code and architecture clean, complying to the principle of separation of responsibilities, and with a clean view of the data model, new choices can be made and the architecture can evolve. If necessary, two separate persistence solution can coexist for each use case: this is the world of polyglot persistence . In the next article, we will dive deeper into the characteristics of graph databases and development tools that are proeminent today. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 7 commentaires sur “Graph databases: an overview” Ravi Venkatesan 13/10/2012 à 14:50 Superb article.  This is a forerunner to complex network databases. Kapil 02/05/2013 à 09:10 Very good head start on when to use graph databases over relational databases samadhan 27/09/2013 à 10:20 it very good article use in graph database Milap 14/03/2014 à 11:35 Very Good Article.. Keep Posting.... sourvo datta 20/04/2014 à 13:41 very helpful article bro. sowmya sree 30/03/2015 à 17:40 Helped my homework at college! thanks!! Miguel 25/02/2021 à 17:03 Helped my homework at college! thanks!! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-07-12"},
{"website": "Octo", "title": "\n                OCTO will organize a workshop “User eXperience” with Adaptive Path – April  10th, 11th and 12th            ", "author": ["Nelly Grellier"], "link": "https://blog.octo.com/en/octo-will-organize-a-workshop-user-experience-with-adaptive-path-april-10th-11th-and-12th/", "abstract": "OCTO will organize a workshop “User eXperience” with Adaptive Path – April  10th, 11th and 12th Publication date 19/02/2013 by Nelly Grellier Tweet Share 0 +1 LinkedIn 0 Since 2007, OCTO intervenes on problems involving User eXperience as a key element in helping its clients achieve innovative products and applications , useful and usable in tune with the complexity of their business challenges. After our first meeting during USI 2012, we are delighted to welcome the Adaptive Path team for a workshop which will walk you through UX from strategy to practice, promising intesity despite a playful approach. Jesse James Garrett’s team , Adaptive Path , will give for this workshop UX Essentials ( Programm details – pdf version ) entirely dedicated to UX, provided for the first time in France on April the 10th, 11th and 12th ! « After my keynote at USI, I looked forward to working with OCTO again. I am happy we have this opportunity to work together to bring UX thinking to France! » Jesse James Garrett , Chief Creative Officer, Adaptive Path. Each day includes fun, hands-on activities designed to build skills and provide a framework that participants can immediately put into practice. This workshop is led by Adaptive Path practitioners and will examine the key elements that contribute to creating great experiences. Participants will spend a day immersed in strategy , followed by a day focussed on research , then wrapping it all up with a day devoted to putting those insights into action. I want to subscribe to the UX Essentials worskhop Tweet Share 0 +1 LinkedIn 0 This entry was posted in News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2013-02-19"},
{"website": "Octo", "title": "\n                Speaker at USI 2013? Now possible thanks to the call for paper!            ", "author": ["Céline Pageaux"], "link": "https://blog.octo.com/en/speaker-at-usi-2013-now-possible-thanks-to-the-call-for-paper/", "abstract": "Speaker at USI 2013? Now possible thanks to the call for paper! Publication date 01/02/2013 by Céline Pageaux Tweet Share 0 +1 LinkedIn 0 Last year we created a system called « call for paper », which allows anyone to propose their session and be part of the official program! Its success was made apparent through over 50 proposed sessions and nearly 20 callbacks! Once again this year, we invite you to propose your session, with the deadline set on the 4th of March, 2013 : Do you have a biography , a session title and resume , and a high-definition picture ? Are you an opinion leader, a referent on one or more IT subjects, a super geek, a manager, or part of IS department? Then you’re ready! We need your contibutions, your ideas, and your knowledge to make the 5th yearly USI an event as great as the reputation that proceeds it. As many as 700 guests are expected this year! The USI team will determine, after having studied your proposition, the length of your session ( 20 or 40 minutes ). For a callback, the first criteria will be the quality of the proposed session’s resume (depth and form). Thus we count on you to send your best work, one that will astound members of the jury. The USI team will study every proposition and select those that seem the most interesting. You may be invited to come present your session beforehand, and the final list of accepted candidates will be announced at the end of March 2013 . You can also see the list of proposed sessions online, and vote for the ones that you most want to see at USI 2013. The USI site has everything you need to post sessions on popular social networks, allowing you to easily share your session with those around you. So don’t hesitate! Click here to submit a session for USI A glass to your candidacy then, and thank you! The USI team Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles and tagged Call for paper , USI2013 , USIEvents . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2013-02-01"},
{"website": "Octo", "title": "\n                Build your own “cloud machine” at home            ", "author": ["Arnaud-Francois Fausse"], "link": "https://blog.octo.com/en/build-your-own-cloud-machine-at-home/", "abstract": "Build your own “cloud machine” at home Publication date 28/01/2013 by Arnaud-Francois Fausse Tweet Share 0 +1 LinkedIn 0 You are reading this article, so it’s likely that you are a geek who loves to test software products and make complex infrastructure mocks like an OpenStack cloud. Seeing four or five desktops stacked in your living room doesn’t make your wife happy. Right? Your dream is to have a very monolithic machine made of several motherboards, switch, disks, power supply, DVD reader and network cards. It was also my dream, and once I decided to turn it to reality. In fact it was a must to study cloud technologies in good conditions. OK, it took me some time and money but it was a good investment! Let’s have a look at my “cloud machine: Key features of the “cloud machine” This machine is made of: 3 motherboards, 2 with virtualization capability 1 managed switch 1 backplane with 4 disks (3 HD, 1 SSD) 1 DVD reader/burner 1 power supply 1 power supply control board Plenty of cables! The architecture is as follows: Choice of motherboards Since my objective was to study cloud/virtualization and especially KVM, at least 2 CPU needed to support virtualization instructions. I was also looking for a small form factor because my home is rather small! I wanted all hardware Linux compatible and finally, my budget was also limited. I chose to restrict my search to commodity hardware and to miniITX form factor. Most of miniITX available cards are based on Atom; so they don’t support virtualization instructions. I was disappointed by the lack of Atom Z series in commodity boards. But AMD saved me, by releasing the APU series. Finally, I chose the ASROCK E350M1 . This card provides very good value for the price, is simple to install and manage. But the key issue of commodity hardware is the lack of management functions: no IPMI, no ability to reset through the network, no SMB access… Then, I decided to use two ASROCK E350M1 as “compute” nodes, and to find a board with remote management capabilities as a “controller node”. Looking for such motherboard I chose the Supermicro X7SPA-HF . This board is rather expensive but has some interesting capabilities: IPMI onboard, access to SMB, 2 Ethernet ports, passive cooling… Choice of the network cards Transferring gigabytes of disk images on the network requires serious bandwidth. I could not afford 10Gbps cards for obvious price reasons, so I chose the Intel PRO/1000 CT which supports jumbo frames (useful for heavy payload) and some other interesting features. It is affordable and close to what is found in mid range servers environments. Additionally their form factor is compatible with compact integration. Choice of the switch Ability to make VLANs was a primary requirement. But the price was a key issue since most managed switches are made for large installations and not for SoHo applications. I chose the Netgear ProSafe GS108T which offers great value for the price and allows to prototype most of usual network cloud configurations (VLAN, DHCP snooping…). Choice of RAM and disks I chose common low cost hardware that one finds on good shops on the Internet. Compute nodes can bear up to 8 GO. The disk backplane, an ICY Box IB-2222SSK , is a very important device for the mechanical integration. However, I had some compatibility problems with the disks and the ICY Box. First, the mechanical aspect of the device (difficult to insert the disk in one slot) and second, Samsung disks that could not start rotating visibly for obscure power supply reasons. I managed to make it working with Hitatchi disks and I am finally pleased with this configuration. I advice to test some alternative solutions (not easy to find on the market however…). Nothing to say about the DVD reader/burner, except it’s a SATA interfaced device. Choice of power supply and control This is the most custom made device of the Cloud machine, as well as the mechanical structure. In fact, PC power supplies are not designed to drive more than one motherboard. To catch the design stakes, you have to know how an ATX power supply is controlled. Look at ATX article on Wikipedia and ATX12V PSDG2.01.pdf (search on Google). There are two main issues: Make sure that Power on sequence (power and reset) is correct for the 3 motherboards using only one power on button; Shutdown of power supply shall be effective only if the 3 motherboards are in shutdown state. I intended to design a control board to make all these controls. In fact, I only did a first version, very simple (based on TTL gates) which insures the correct power on sequence. But shutdown is done only from the Supermicro board. At the moment this design is sufficient, but I plan to make a control board based on Microchip PIC micro-controller that will implement all necessary logic. The power supply is a 650W EPS, but a low cost 450W ATX would be sufficient. Mechanical design and structure Stacking motherboard is a pattern that makes me dreaming ;-) and I chose to reproduce such arrangement . Overall dimensions of the cloud machine are 23cm x 26cm x 52 cm. I chose to put the power supply on the bottom part of the structure for several reasons: The PSU fan blows up, pushing warm air in its upwards natural flow; The center of gravity is close to the bottom of the structure. The drawback of this design is that dust falls into the PSU when it’s turned off. Electric ground is connected to the whole structure through the PSU screwing. All parts of the “Cloud machine” are in aluminum except steel thread bars that support the motherboards. I have a welding torch and aluminum solder which is fairly easy to use. A simple metal saw, a basic drill and some metal files make you able to do it as I did. Further improvements The next version of the control board will implement : Enhanced power supply control (common synchronized switch off); The ability to reset the ASROCK motherboards from the Supermicro motherboard (using USB port and a USB microcontroller on the control board). Conclusion It took me several week-ends to achieve this machine and I am really pleased to use it for my new software mocks. I spent around 2000 € in total. I had almost a heart attack when I saw that ASROCK motherboards didn’t support virtualization. But after BIOS update, they did as promised!! The next blog article will present how this Cloud machine is installed with Linux and operated using PXE and SSH. Have fun making your custom machines! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Infrastructure and Operations and tagged cloud virtualization appliance , home made , motherboard , PSU , switch . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 4 commentaires sur “Build your own “cloud machine” at home” Aaron 18/08/2016 à 12:56 You mentioned having the Supermicro X7SPA-HF as the control node since it has IPMI support.  Does that mean that if you have boot issues with the other non-IPMI nodes, then you can't remote into them, correct?  Or is there some way you proxy the keyboard/VGA from the other nodes to the IPMI node? marjorie jackson 27/03/2018 à 08:20 Thanks for all steps you mentioned above but I need your help in this I am having an issue with my pc. whenever I am trying to open it, it gets shut down by itself. I have checked its display and electricity connection that all is okay. I think there is an issue in its motherboard. I also took it to shop for repair but still I am having this. do I need to change it?\r\nand also Visit Bitdefender customer service for Bitdefender antivirus issues dell support number 02/05/2018 à 18:58 After all the process to be known for the motherboard there is a different category which will be helpful for the users for the right things which is applicable to the learning or building up the own cloud. 0xc00000e9 02/11/2018 à 06:46 You mentioned having the Supermicro X7SPA-HF as the control node since it has IPMI support. Does that mean that if you have boot issues with the other non-IPMI nodes, then you can't remote into them, correct? Or is there some way you proxy the keyboard/VGA from the other nodes to the IPMI node? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2013-01-28"},
{"website": "Octo", "title": "\n                Add Lint checks to Android Maven builds            ", "author": ["Stéphane Nicolas"], "link": "https://blog.octo.com/en/add-lint-checks-to-android-maven-builds/", "abstract": "Add Lint checks to Android Maven builds Publication date 15/02/2013 by Stéphane Nicolas Tweet Share 0 +1 LinkedIn 0 Android offers a great tool to leverage the quality of apps : Lint . Lint can check missing translations, unused resources and other common mistakes in Android programming. Nevertheless, up to now it was not possible to use Lint inside automated builds (i.e. on a Continuous Integration server like Jenkins). This situation just changed with release 3.5.1 of the Android Maven Plugin . OCTO Contributed to release 3.5.1+ of Android Maven Plugin to offer integration of Lint as a simple maven goal : android:lint . The complete documentation of this goal can be found on the official documentation of the Maven Plugin goals . This articles provides a rapid overview of the android:lint goal’s usage and parameters. Create a new Android project and its maven configuration If you know how to create an Android project using Maven, or already have such a project, you can go directly to next section . Let’s start a new Android project using acquinet maven archetypes . In a terminal, go to your working directory and type the following command : mvn archetype:generate \\\r\n  -DarchetypeArtifactId=android-with-test \\\r\n  -DarchetypeGroupId=de.akquinet.android.archetypes \\\r\n  -DarchetypeVersion=1.0.9 \\\r\n  -DgroupId=com.foo.bar \\\r\n  -DartifactId=my-android-project \\\r\n  -Dpackage=com.foo.bar.android In Eclipse, right-click in you package explorer view and select “Import”  to import the project using the wizard “Import existing maven projects” : Using the latest version of Eclipse (Juno), you will have to hack the imported projects to build them inside Eclipse : Right-click on the first project go to properties >> Java Build Path >> Sources tab Remove the src/test/java folder Apply the same procedure to the test project An alternative, for now, is also to create a src/test/java folder in your project. As you can see, everything got much simpler than 2 years ago . You can now go back to your terminal and build the project (both the app and its tests) using maven : >mvn clean install\r\n...\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary:\r\n[INFO] \r\n[INFO] my-android-project - Parent ....................... SUCCESS [0.143s]\r\n[INFO] my-android-project - Application .................. SUCCESS [3.228s]\r\n[INFO] my-android-project-it - Integration tests ......... SUCCESS [7.966s]\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD SUCCESS\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 12.234s\r\n[INFO] Finished at: Mon Jan 07 14:57:48 CET 2013\r\n[INFO] Final Memory: 22M/206M\r\n[INFO] ------------------------------------------------------------------------ If you reach that point, then everything is setup properly and you now have a working Android Maven project that will also build inside Eclipse. Add Lint checks to your Maven build First, check that you are using the right version of the Android Maven Plugin. This plugin is managed by the parent pom of your project. It should look like : <project>\r\n....\r\n    <pluginRepositories>\r\n        <pluginRepository>\r\n            <id>oss.sonatype.org-jayway-snapshots</id>\r\n            <name>Jayway OpenSource SNAPSHOTs on Sonatype.org</name>\r\n            <url>http://oss.sonatype.org/content/repositories/jayway-snapshots/</url>\r\n            <snapshots>\r\n                <enabled>true</enabled>\r\n            </snapshots>\r\n        </pluginRepository>\r\n    </pluginRepositories>\r\n    <build>\r\n        <pluginManagement>\r\n            <plugins>\r\n                ....\r\n                <plugin>\r\n                    <groupId>com.jayway.maven.plugins.android.generation2</groupId>\r\n                    <artifactId>android-maven-plugin</artifactId>\r\n                    <version>3.5.1-SNAPSHOT</version>\r\n                </plugin>\r\n                ....\r\n            </plugins>\r\n        </pluginManagement>\r\n    </build>\r\n</project> When you are sure to use the right version, you can now configure the Android Maven Plugin in your application’s project pom.xml file : <build>\r\n    <plugins>\r\n        ...\r\n        <plugin>\r\n            <groupId>com.jayway.maven.plugins.android.generation2</groupId>\r\n            <artifactId>android-maven-plugin</artifactId>\r\n            <executions>\r\n                <execution>\r\n                    <id>lint</id>\r\n                    <goals>\r\n                        <goal>lint</goal>\r\n                    </goals>\r\n                    <phase>install</phase>\r\n                </execution>\r\n            </executions>\r\n            <configuration>\r\n                <sdk>\r\n                    <platform>17</platform>\r\n                </sdk>\r\n                <lint>\r\n                    <skip>false</skip>\r\n                    <failOnError>true</failOnError>\r\n                </lint>\r\n                <undeployBeforeDeploy>true</undeployBeforeDeploy>\r\n            </configuration>\r\n            <extensions>true</extensions>\r\n        </plugin>\r\n        ...\r\n    </plugins>\r\n</build> You will now see that Lint is executed during the install phase of a maven for your project. These 3 lines should be logged during your maven build : > mvn clean install | grep -i Lint\r\n[INFO] Performing lint analysis.\r\n[INFO] Writing Lint XML report in <path to your project>/my-android-project/my-android-project/target/lint/lint.xml\r\n[INFO] Lint analysis completed successfully. Customizing Lint usage via the Android Maven plugin The new android:lint goal of the Android Maven Plugin supports quite a lot of features, most of them are directly related to the lint program (use lint --help to display all lint options). You can use either command line parameters in the form of android.lint.* or, inside the <lint></lint> section of your pom.xml file you can use the following options : Controlling Lint invocation skip=true|false whether or not to execute lint. False by default. failOnError=true|false if true, any lint error (not warnings) will stop the build. Defaults to false. This flag is useful for Continuous Integration builds as it allows you to enforce lint usage. Controlling Lint granularity ignoreWarnings=true|false if true, don’t report lint warnings, only errors are reported. False by default. warningsAsErrors=true|false if true, all lint warnings will be treated as errors. Defaults to false. Controlling Lint output enableHtml=true|false if true, lint will write a HTML report in the target/lint folder. False by default. htmlOutputPath=<path> path of the HTML report, only taken into account if previous property is set to true. enableXml=true|false if true, lint will write a XML report in the target/lint folder. True by default. xmlOutputPath=<path> path of the XML report, only taken into account if previous property is set to true. Controlling Lint input As usual : using lint, you can use a file named lint.xml at the root of your project to define the different level of severity of lint issues. This file will also be taken into account by the Android Maven Plugin. enableSources=true|false if true, lint will check the project’s source folder. True by default. sources=<path> path of the folder of the sources folder of your project. Defaults to ${project.build.sources} enableClasspath=true|false if true, lint will check the byte code of your project’s classes. False by default. classpath=<path> path of the folder of the classes of your project. Defaults to ${project.build.outputDirectory} enableLibraries=<path> if true, lint will check the byte code of your project’s dependencies. False by default. libraries=<path> path of the jars of the libraries of your project. Defaults to all jars of dependencies with a scope that is not “provided”. Lint goal and maven phases We must issue a warning here : the last two properties of the lint configuration are disabled by default. Using them is sensitive as they can have side-effects : both classes and libraries of a maven project are only present during certain phases of a maven build. For the classes, your project must have been compiled (during the compile phase). For the libraries, the dependencies’ artifacts must have been resolved by maven (during the generate-sources phase of maven). For this reason, we strongly encourage you to use the android:lint goal during the install phase of your application : <build>\r\n    <plugins>\r\n        ...\r\n        <plugin>\r\n            <groupId>com.jayway.maven.plugins.android.generation2</groupId>\r\n            <artifactId>android-maven-plugin</artifactId>\r\n            <executions>\r\n                <execution>\r\n                    <id>lint</id>\r\n                    <goals>\r\n                        <goal>lint</goal>\r\n                    </goals>\r\n                    <phase>install</phase>\r\n                </execution>\r\n            </executions>\r\n            <configuration>\r\n                <!-- your lint config here-->              \r\n            </configuration>\r\n        </plugin>\r\n        ...\r\n    </plugins>\r\n</build> Conclusion The new android:lint goal of the Android Maven plugin will let you add quality checks to your Android projects. You can either use it on the command line or inside an automated built on a continuous integration server. You can even make your build fail if you have any lint error or any lint warning. We hope Android developers will welcome this tool and that lint checks will be more and more widespread in the Android community. At OCTO, we are proud to have contributed to the Android Maven Plugin. We think that this new maven goal is a good way to enhance the robustness of our applications. However, something is still missing according to us : a lint plugin for Sonar in order to visualize all lint issues directly inside our Sonar dashboards. That’s our next R&D target, anyone interested in coordinating their efforts can just leave a message below this post. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Project support and tagged analysis , Android , lint , Maven , quality , tools . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2013-02-15"},
{"website": "Octo", "title": "\n                Hadoop in my IT department: benchmark your cluster            ", "author": ["Rémy Saissy"], "link": "https://blog.octo.com/en/hadoop-in-my-it-department-benchmark-your-cluster/", "abstract": "Hadoop in my IT department: benchmark your cluster Publication date 07/01/2013 by Rémy Saissy Tweet Share 0 +1 LinkedIn 0 The stress test is a very important step when you go live. Good stress tests help us to: ensure that the software meets its performances requirements ensure that the service will deliver a fast response time even under a heavy load get to now the scalability limits which in turn is useful to plan the next steps of the development Hadoop is not a web application, a database or a webservice. You don’t stress test a Hadoop job with a heavy load. Instead, you need to becnhmark the cluster which means assessing its performances by running a variety of jobs each focused on a specific field (indexing, querying, predictive statistics, machine learning, …). Intel has released HiBench, a tool dedicated to run such benchmarks. In this article, we will talk about this tool. What is HiBench? HiBench is a  est un collection of shell scripts published under the Apache Licence 2 on GitHub : https://github.com/intel- hadoop/HiBench It allows to stress test a Hadoop cluster according to several usage profile. Micro Benchmarks WordCount This test dispatches the counting of the number of words from a data source. The data source is generated by a preparation script of HiBench which relies on the randomtextwriter of Hadoop. This test belongs to a class of jobs which extracts a small amount of information from a large data source. It is a CPU bound test. Sort This test dispatches the sort of a data source. The data source is generated by a preparation script which relies on the randomtextwriter d’Hadoop. This test is the simplest one you can imagine. Indeed, both Map and Reduce stages are identity functions. The sorting is done automatically during the Shuffle & Merge stage of  MapReduce. It is I/O bound . TeraSort This test too dispatches the sort of a data source. The data source is generated by the Teragen jobs which creates by default  1 billion of 100 bytes lines. These lines are then sorted by the Terasort . Unlike Sort , Terasort provides its own input and output format and also its own Partitioner which ensures that the keys are equally distributed among all nodes. Therefore, it is an improved Sort which aims at providing an equal load between all nodes during the test. With this specificity, this test is: CPU bound for the Map stage I/O bound for the Reduce stage Enhanced DFSIO This test is dedicated to HDFS. It aims at measuring the agregated I/O rate and throughput of HDFS during reads and writes. During its preparation stage, a data source is generated and put on HDFS. Then, two tests are run: A read of the generated data source A write of a large amount of data The write test is basically the same thing as the preparation stage. This test is I/O bound . Web Search Nutch indexing This test focuses on the performances of the cluster when it comes to indexing data. In order to do it, the preparation stage generates the data to be indexed. Then, indexing is performed with Apache Nutch . This test is I/O bound with a high CPU utilization during the Map stage . Page Rank This test measures the performances of the cluster for PageRanking jobs. The preparation phase generates the data source in the form of a graph which can be processed using the PageRank algorithm. Then, the actual indexing is performed by a chain of 6 MapReduce jobs. This test is CPU bound . Machine Learning Naive Bayes Classifier This test performs a probabilistic classification on a data source. It is explained in depth on Wikipedia . The preparation stage generates the data source. Then, the test chains two MapReduce jobs with Mahout : seq2sparse transforms a text data source into vectors trainnb computes the final model using vectors This test is I/O bound with a high CPU utilization during the Map stage of the seq2sparse. When using this test, we didn’t observe a real load on the cluster. It looks like it is necessary to either provide its own data source or to greatly increase the size of the generated data during the preparation stage. K-Means clustering This test partitions a data source into several clusters where each element belongs to the cluster with the nearest mean. It is explained in depth on Wikipedia . The preparation stage generates the data source. Then, the algorithm runs on this data source through Mahout . The K-Means clustering algorithm is composed of two stages: iterations clustering Each of these stages runs MapReduce jobs and has a specific usage profile. CPU bound for iterations I/O bound for clustering Analytical Query This class of tests performs queries that correspond to the usage profile of business analysts and other database users. The data source is generated during the preparation stage. Two tables are created: A rankings table A uservisits table This is a common schema that we can meet in many web applications. Once that the data source has been generated, two Hive requests are performed: A joint An agregation These tests are I/O bound . Using HiBench Run a stress test Run HiBench is not very hard: retrieve the sources on GitHub Ensure that nobody is using the cluster Ensure that you have correctly configured your environment variables Then, the file bin/hibench-config.sh contains all the options to tune before startting the stress test. It includes the HDFS directory where you want to write both source and result data, the path of the final report file on the local filesystem, … Once configured, ensures that the HDFS directory where you want to write your data source and your results exists on the cluster and run the command bin/run-all.sh. Now you can take a coffee… or two. Interpretation of the results Results are written into the hibench.report file with the following CSV format: test_name end_date <jobstart_timestamp,jobend_ timestamp> size_in_bytes duration_ms throughput Beware that the actual result file does not contain the column header  above. The DFSIOE test also writes a CSV and an interpretation of its results in its subdirectory dfsioe. Limitations Latest Hadoop support Presently, HiBench runs on Hadoop 1.0. This means that the latest versions of Cloudera or Hortons Works distributions for example won’t be able to run all tests since they rely on Hadoop 2. However, the effort necessary to support Hadoop 2 is not that big for the majority of the tests since it is mainly a matter of updating configuration parameter names. Also, HiBench alone is not enough for a good report of a stress test. It is necessary to also retrieve the informations provided by the JobTracker/ResourceManager like the mean execution time of Maps, Reduces, Shuffle and Merges of every job in order to build an accurate final report. A public benchmark repository, a big lack This is a lack that HiBench tried to address through its wiki page which invites you to post your results but with no success until now. Building a public benchmark repository in order to provide a set of meaningful metrics to compare a cluster is still an uncover issue but  would be interesting and quite useful. What are the alternatives? An alternative exists to HiBench, but it is more focused on a specific usage profile. GridMix GridMix is included in Hadoop besides the example jobs like TeraSort, Sort, … However, it generates MapReduce jobs which are focused on sorting large amount of data and does not cover other profiles like Machine Learning. Conclusion In spite of these drawbacks, HiBench greatly simplifies the benchmarking of a Hadoop cluster. In the future, this domain will certainly see new tools with more functionalities and a better coverage or different usage profiles. It is only the beginning. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Big Data and tagged bigdata , Hadoop . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 4 commentaires sur “Hadoop in my IT department: benchmark your cluster” Vladimir 29/01/2013 à 08:34 Hello!\r\nCan you give some tips about benchmarking Hadoop cluster filesystems? I want to try Giraffa, QFS etc. vs HDFS and make some reasonable conclusion about their relative performance. Rémy SAISSY 31/01/2013 à 15:33 Hi Vladimir,\r\nI haven't digg on the specific issue of benchmarking such filesystems yet.\r\nBut if I had to do it, I would at least follow the following guidelines:\r\n - build several identical clusters: same size, same hardware, same software stack. Only the filesystem changes\r\n - run the EDFSIO test on each cluster\r\n - read/write different volumes of data: small, medium, large, very large. Compressed and not compressed Vladimir 05/02/2013 à 13:44 Thanks for your respond.\r\nThe interpretation of results provided in the post was plenty helpful. I wasn't able to find any interpretation provided in the HiBench repository as well as in the HiBench publication. Thanks again. Maybe you have any tips on how to interpret the CSVs generated by DFSIOE test? Rémy SAISSY 05/02/2013 à 13:50 The best source of information I've found to interpret these data are these links:\r\nhttps://github.com/intel-hadoop/HiBench/raw/master/WISS10_conf_full_011.pdf\r\nand \r\nhttp://software.intel.com/sites/default/files/blog/329037/hibench-wbdb2012-updated.pdf Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2013-01-07"},
{"website": "Octo", "title": "\n                Reinventing Banking & Trading on iPad for Keytrade Bank            ", "author": ["Rémy VIRIN"], "link": "https://blog.octo.com/en/reinventing-banking-banking-on-ipad/", "abstract": "Reinventing Banking & Trading on iPad for Keytrade Bank Publication date 16/11/2012 by Rémy VIRIN Tweet Share 0 +1 LinkedIn 0 This is the END. We created an iPad app for Keytrade Bank , and we just launched it!, You can download it here . Please contact us if you what to talk with us about this reference & our expertise! Mixed feelings: happiness because it’s really a major achievement , nostalgia because we really loved building this app and now we are just slightly nervous as we hope users will love to consult their accounts, transfer money and trade using the Keytrade app as much as we enjoyed creating it. For the banking part, this app allows one to customize accounts (rename accounts, assign pictures to each account), to consult the list of transactions, to transfer money and to visualize the monthly In & Out of an account with clear charts. With respect to trading , our app offers a heatmap updated in real time with all the markets and all companies (when available from provider) and, of course, buying and selling stocks is possible from within the app. One can also consult the personal portfolio containing an overview of all trades. To get a better idea of this project, here are some metrics: 8 months 2 Product Owners, one for Banking, one for Trading (from Keytrade Bank) 1 graphic designer (from Keytrade Bank) 1 UX designer 4,5 iOS developers (among them 1 from Keytrade Bank trained by OCTO) + 1 Scrum master 2 web-services developers (from Keytrade Bank) 21 Sprints 29 storyboards 1 user test session 302 User Stories & 2 300 complexity points 46 Releases of the app 3 languages 45 K lines of code (frameworks excluded) 17 K lines of comment 1 035 Unit Tests Quality First Quality has been the primary focus from the start. For example, when a developer finished implementing a user story, another one had to validate the feature and to review the code. When peers validated the story, the product owner (PO) had to test the feature to ensure it matched what he had asked for. To lower the numbers of bugs, we used the TDD (Test Driven Development) method. We tested the logic and the model of the app. To change the state of a story from “in progress” to “done” we had to respect a checklist: this is called a Definition of Done (DOD). For example, we had to check if the code was clean, well documented and tested, if it didn’t add warnings etc. We also had a continuous integration server (Jenkins). On each commit, an automatic build ran the unit tests and gave us metrics like number of lines of code, code coverage, code duplication. My colleague Cyril Picat described in t his article how to add these metrics Jenkins for iOS . With all these standards we dramatically decreased the number of bugs. However since the project is quite large, one may discover bugs from time to time that will be fixed in the next versions. Each week, we released a new version of the app to Keytrade employees’ iPads. This process can be time consuming, that’s why we decided to automate it. So we just had to press a button and the users received a notification on their iPads to install the new version. So, how did we do it? It’s really simple, we created a new job on Jenkins, which build the app thanks to the Xcode plugin, and then we pushed it to Appaloosa , OCTO deployment solution available as SAAS. UX Rules ! For us, User eXperience (UX) is really important. We believe a good UX changes everything. That’s why we had a UX designer during the whole project to challenge every usage, to create new patterns of banking and trading on tablets. This was the first time that Keytrade worked with a UX designer. At first, people didn’t understand how and why creating user interfaces was complicated. After a few months, we can see how UX improved our app, and how efficient it is to work with a UX designer during a project. At the end of the project, we gave the opportunity to Keytrade’s real customers to test the app during a dedicated users testing session , before launching it on the Appstore. We received a lot of feedback, and of course, we worked one extra month to improve the app taking into account the remarks and recommendations! Now, Keytrade Bank wants to hire a full time UX designer, … another proof that UX is not an option for quality apps ! The following images show us how UX helped us in our process. This is an iteration process (Agile): First a UX designer drew the screen on a sheet of paper and in powerpoint Then the product owner validated the storyboard with the UX designer and added Business Rules on the powerpoint for the development team The graphic designer and the developers could start their work and iterated with the whole team. Be Agile, be able to change everything, anytime. At OCTO, we work using the Agile methodology. We believe Agile helps us to deliver better products that better fit the needs . It was the first project with Agile methodology for Keytrade. At the project start, we had no idea of what we will create. We didn’t know how we should present data, what should be in the app, what shouldn’t. The only thing we knew is that we wanted to change how people interacted in banking and trading on tablets. We wanted to be a reference, to be the #1 of the Appstore! We created a lot of features, and we didn’t keep them all. We threw away a lot of concepts. We developed, designed, integrated the design, and fixed bugs on features that we didn’t ship. Why ? Because we wanted to give users the best we could deliver, a valuable product . We didn’t want to deliver useless features. Sometimes we deleted features; sometimes we rebuilt them from scratch. It can be hard to accept, when you have worked for several hours and spent a lot of time and energy on a feature, but it’s important to go through with it for the quality of the final product. Takeaways To help us in our Agile approach, we had some tools. In particular we used Kanban Tools and Google Docs. These 2 tools were synchronized thanks to custom Google apps scripts. These scripts are really powerful when using Google Docs. Our product backlog was a spreadsheet, which was hosted on Google docs. When we added or modified a task, it was automatically updated on our Kanban. The most important point was the great collaboration with the Keytrade teams : the graphical designer, the back-end developers & the marketing team but also the business experts & the top management. And first, to ease collaboration being in the same room helps a lot. We were in Keytrade premises 3 days a week. We were really close to the back office team, which is a good thing to quickly solve problems, and ease communication. This has been a strategic decision that saved us a lot of time. Once a week, we demoed our progress to the Keytrade management team. This was important for us to have their feedback in order to know whether the product fitted their needs. As a result, Keytrade wants to go further on Agile methodology and considers starting more Agile projects. To conclude, our whole team Keytrade+OCTO is really proud of what we have created! This is probably one of the best apps OCTO have worked on. At this very moment, while writing this article, Keytrade Bank is the first app of the Appstore (in Belgium). And the score of the app is 4+ on the store. You can have a quick overview of this with the following screenshots, but the best way, is to test it ! This is the End of the V1…. But just the beginning for this app! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles , News and tagged Agile , app , bank , banking , belgium , delivery , iOS , iPad , keytrade , trading , UX . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 5 commentaires sur “Reinventing Banking & Trading on iPad for Keytrade Bank” Stroobant Dirk 18/11/2012 à 08:26 Hallo,\r\nAs a former programmer is read this article with great interest.\r\n\r\nHowever , my first experience with this app is not that good. After filling in my credentials , following error message appeared \"Please fill in the appropriate test on the secure site\"\r\n\r\nI found no solution to this problem on the key trade website.\r\n\r\nKind Regards\r\n\r\nDirk Rémy Virin 18/11/2012 à 14:19 Hello Dirk,\r\n\r\nIt will be difficult for us to help you, because we don't work at Keytrade Bank anymore. Have you try to call the help desk ?\r\n\r\nhttps://www.keytradebank.com/fr/support/contact\r\n\r\nI'm sure they will fix your problem very quick. I hope you'll be able to test this app !\r\n\r\nFeel free to contact me if needed.\r\n\r\nRémy. arnaud404 18/11/2012 à 15:35 Hi Dirk,\n\nTo access the application, you need to fill in some informations on the secure website. \nPlease head to the secure website (https://www.keytradebank.com/secure), go to your preferences, and fill in the knowledge and experience test in the personal preference tab. \nThis is what this message means. \n\nFeel free to contact the helpdesk if you need further assistance. \n\nArnaud Harold Tor 10/05/2013 à 17:44 Hi there!\r\n\r\nOnce again, congratulations on the great UX and the fantastic product!\r\n\r\nI wrote a review of it after it was launched: http://haroldtor.tumblr.com/post/38625653331/keytrade-bank-ipad-app-review\r\nLike I mentioned, it takes a lot of effort, humility, testings, neutrality of a UX designer and breakdown of hierarchical internal bureaucracy to achieve what you have achieved. Hats off to you guys! Well done!\r\n\r\nHarold Philippe Guicheney 10/05/2013 à 19:25 The video that presents the app : http://www.youtube.com/watch?v=U75rj7WT-qY Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-11-16"},
{"website": "Octo", "title": "\n                Hadoop in my IT department: How to plan a cluster?            ", "author": ["Rémy Saissy"], "link": "https://blog.octo.com/en/hadoop-in-my-it-department-how-to-plan-a-cluster/", "abstract": "Hadoop in my IT department: How to plan a cluster? Publication date 26/11/2012 by Rémy Saissy Tweet Share 0 +1 LinkedIn 0 Ok, you have decided to setup a Hadoop cluster for your business. Next step now, planning the cluster… But Hadoop is a complex stack and you might have many questions: HDFS deals with replication and Map Reduce create files… How can I plan my storage needs? How to plan my CPU needs? How to plan my memory needs? Should I consider different needs on some nodes of the cluster? I heard that Map Reduce moves its job code where the data to process is located… What does it involve in terms of network bandwidth? At which point and how far should I consider what the final users will actually process on the cluster during my planning? That is what we are trying to make clearer in this article by providing explanations and formulas in order to help you to best estimate your needs. Some important aspects of the Hadoop architecture Planning a Hadoop cluster requires a minimum knowledge the Hadoop architecture. The distributed computation At his heart, Hadoop is a distributed computation platform. This platform’s programming model is Map Reduce. In order to be efficient, Map Reduce has two prerequisites: Datasets must be splitable in smaller and independant blocks Data locality: means that the code must be moved where the data lies, not the opposite. The first prerequisite depends on both the type of input data which feeds the cluster and what we want to do with it. The second prerequisite involves having a distributed storage system which exposes where exactly data is stored and allows the execution of code on any storage node. This is where HDFS is useful. Hadoop is a Master / Slave architecture: The JobTracker ( ResourceManager in Hadoop 2) Monitor jobs that are running on the cluster Needs a lot of memory and CPU ( memory bound and cpu bound ) The TaskTracker ( NodeManager + ApplicationMaster in Hadoop 2) Runs tasks of a jobs on each node of the cluster. Which means Maps and Reduces Its jobs need a lot of memory and CPU ( memory bound and cpu bound ) The critical component in this architecture is the JobTracker/ResourceManager . The distributed storage HDFS is a distributed storage filesystem. It runs on top of another filesystem like ext3 or ext4. In order to be efficient, HDFS must satisfy the following prerequisites: Hard drives with a high throughput An underlying filesystem which supports the HDFS read and write pattern: one big read or write at a time (64MB, 128MB or 256MB) Network fast enough to cope with intermediate data transfer and block replication HDFS is a Master / Slave architecture: The NameNode and the Secondary NameNode Stores the filesystem meta informations (directory structure, names, attributes and file localization) and ensures that blocks are properly replicated in the cluster It needs a lot of memory ( memory bound ) The DataNode Manages the state of an HDFS node and interacts with its blocks Needs a lot of I/O for processing and data transfer ( I/O bound ) The critical components in this architecture are the NameNode and the Secondary NameNode . These are two distinct but complementary architectures. It is possible to not use HDFS with Hadoop. Amazon with their Elastic MapReduce for example rely on their own storage offer, S3 and a desktop tool like KarmaSphere Analyst embeds Hadoop with a local directory instead of HDFS. Some important technicals facts to plan a cluster No need to be an Hadoop expert but the following few facts are good to know when it comes to cluster planning. How HDFS manages its files HDFS is optimized for the storage of large files. You write the file once and access it many times. In HDFS, a file is split into several blocks. Each block is asynchronously replicated in the cluster. Therefore, the client sends its files once and the cluster takes care of replicating its blocks in the background. A block is a contigous area, a blob of data on the underlying filesystem, Its default size is 64MB but it can be extended to 128MB or even 256MB, depending on your needs. The block replication, which has a default factor of 3, is useful for two reasons: Ensure data recovery after the failure of a node. Hard drives used for HDFS must be configured in JBOD, not RAID Increase the number of maps that can work on a bloc during a MapReduce job and therefore speedup processing From a network standpoint, the bandwith is used at two moments: During the replication following a file write During the balancing of the replication factor when a node fails How the NameNode manages the HDFS cluster The NameNode manages the meta informations of the HDFS cluster. This includes meta informations (filenames, directories, …) and the location of the blocks of a file. The filesystem structure is entirely mapped into memory. In order to have persistence over restarts, two files are also used: a fsimage file which contains the filesystem metadata the edits file which contains a list of modifications performed on the content of fsimage . The in memory image is the merge of those two files. When the NameNode starts, it first loads fsimage and then applies the content of edits on it to recover the latest state of the filesystem. An issue would be that over time, the edits file keeps growing undefinitely and ends up by: consuming all disk space slowdown restarts The Secondary NameNode role is to avoir this issue by regularly merging edits with fsimage , thus pushing a new fsimage and resetting the content of edits. The trigger for this compaction process is configurable. It can be: The number of transactions performed on the cluster The size of the edits file The elapsed time since the last compaction The following formula can be applied to know how much memory a NameNode needs: <needed memory> = <total storage size in the cluster in MB> / <Size of a block in MB> / 1000000 In other words, a rule of thumb is to consider that a NameNode needs about 1GB / 1 million blocks. How to determine my sizing needs? Determine storage needs Storage needs are split into three parts: Shared needs NameNode and Secondary NameNode specific needs DataNode specific needs Shared needs Shared needs are already known since it covers: The OS partition The OS logs partition Those two partitions can be setup as usual. NameNode and Secondary NameNode specific needs The Secondary NameNode must be identical to the NameNode . Same hardware, same configuration. A 1TB partition should be dedicated to  files written by both the NameNode and the Secondary NameNode. This is large enough so you won’t have to worry about disk space as the cluster grows. If you want to be closer to the actual occupied size, you need to take into account the parameters of the NameNode we explained above (a combination of the trigger for the compaction, the maximum fsimage size and the edits size ) and to multiply this result by the number of checkpoints you want to be retained. In any case, the NameNode must have an NFS mount point to a secured storage among its fsimage and edits directories. This mount point has the same size than the local partition for fsimage and edits mentionned above. The storage of  the NameNode and the Secondary NameNode is typically performed on RAID configuration. DataNode specific needs Hardware requirements for DataNode s storage is: SAS 6Gb/s controller configured in JBOD (Just a Bunch of Disk) SATA II 7200 rpm hard drives between 1Tb and 3Tb Do not use RAID on a DataNode. HDFS provides its own replication mecanism. The number of hard drive can vary depending on the  total desired storage capacity. A good way to determine the latter is to start from the planned data input of the cluster. It is also important to note that for every disk, 30% of its capacity is reserved to non HDFS use. Let’s consider the following hypothesis: Daily data input: 100Gb HDFS replication factor: 3 Non HDFS reserved space per disk: 30% size of a disk: 3Tb With these hypothesis, we are able to determine the storage needed and the number of DataNodes. Therefore we have: Storage space used by daily data input : <daily data input> * <replication factor> = 300GB Size of a hard drive dedicated to HDFS : <Size of the hard drive > * (1 – <Non HDFS reserved space per disk>) = 2.1TB Number of DataNodes after 1 year (no monthly growth) : <storage space used by daily data input> * 365 / <HDFS reserved space in a disk> = 100TB / 2.1TB = 48 DataNodes Two important elements are not included here: The monthly growth of the data input The ratio of data generated by jobs processing a data input These informations depend on the needs of your business units and it must be taken into account in order to determine storage needs. Determine your CPU needs On both NameNode and Secondary NameNode , 4 physical cores running at 2Ghz will be enough. For DataNodes , two elements help you to determine your CPU needs: The profile of the jobs that are going to run The number of jobs you want to run on each DataNode Job profile Roughly, we consider that a DataNode can perform two kind of jobs: I/O intensive and CPU intensive . I/O intensive jobs These jobs are I/O bound . For example: indexing search clustering decompression data import/export Here, a CPU running between 2Ghz and 2.4Ghz is enough. CPU intensive jobs These jobs are CPU bound . For example: machine learning statistics semantic analysis language analysis Here, a CPU running between 2.6Ghz and 3Ghz is enough. The number of jobs The number of physical cores determine the maximum number of jobs that can run in parallel on a DataNode. It is also important to keep in mind that there is a distribution between Map and Reduce tasks on DataNodes (typically 2/3 Maps and 1/3 Reduces). To determine you needs, you can use the following formula: (<number of physical cores> – 1) * 1.5 = <maximum number of tasks> or, if you prefer to start from the number of tasks and adjust the number of cores according to it: (<maximum number of tasks> / 1.5) + 1 = <number of physical cores> The number 2 keeps 2 cores away for both the TaskTracker (MapReduce) and DataNode (HDFS) processes. the number 1.5 indicates that a physical core, due to hyperthreading, might process more than one job at the same time. Determine your memory needs This is a two step process: Determine the memory of both NameNode and Secondary NameNode Determine the memory of DataNodes In both cases, you should use DDR3 ECC memory. Determine the memory of both NameNode and Secondary NameNode As explained above the NameNode manages the HDFS cluster metadata in memory. The memory needed for the NameNode process and the memory needed for the OS must be added to it. The Secondary NameNode must be identical to the NameNode . Given these informations we have the following formula: <Secondary NameNode memory> = <NameNode memory> = <HDFS cluster management memory> + <2GB for the NameNode process> + <4GB for the OS> Determine the memory of DataNodes The memory needed for a DataNode is determined depending on the profile of jobs which will run on it. For I/O bound jobs, between 2GB and 4GB per physical core. For CPU bound jobs, between 6GB and  8GB per physical core. In both cases, the following must be added: 2GB for the DataNode process which is in charge of managing HDFS blocks 2GB for the TaskTracker process which is in charge of managing running tasks on the node 4GB for the OS Which leads to the following formulas: <DataNode memory for I/O bound profile> = 4GB * <number of physical cores> + <2GB for the DataNode process> + <2GB for the TaskTracker process> + <4GB for the OS> <DataNode memory for CPU bound profile> = 8GB * <number of physical cores> + <2GB for the DataNode process> + <2GB for the TaskTracker process> + <4GB for the OS> Determine your network needs The two reasons for which Hadoop generates the most network traffic are: The shuffle phase during which Map tasks outputs are sent to the Reducer tasks Maintaining the replication factor (when a file is added to the cluster or when a DataNode is lost) In spite of it, network transfers in Hadoop follow an East/West pattern which means that even though orders come from the NameNode , most of the transfers are performed directly between DataNodes . As long as these transfers do not cross the rack boundary, it is not a big issue and Hadoop does its best to perform only such transfers. However, inter-rack transfers are sometimes needed, for example for the second replica of an HDFS block. This is complex subject but as a rule of thumb, you should: Use a Spine Fabric network topology Better throughput Better resiliency to failures Avoid oversubscribtion of switches A 1Gb 48 ports top of rack switch must have 5 ports at 10Gb on distribution switches Avoids network slowdown for a cluster under heavy load (jobs + data input) If the cluster is I/O bound or if you plan to perform data input on recurrent data input which sature the 1GB and which cannot be performed outside of office hours, you should use: 10Gb Ethernet intra rack N x 10Gb Ethernet inter rack If the cluster is CPU bound , you should use: 2 x 1Gb Ethernet intra rack 10Gb Ethernet inter rack Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Big Data , Infrastructure and Operations and tagged bigdata , Hadoop . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Hadoop in my IT department: How to plan a cluster?” GHERMAN 27/06/2015 à 18:44 Hi,\r\n\r\nGreat article but there are some missing information:\r\n- how do we know the \"maximum number of tasks\" ?\r\n- how do we know the \"HDFS cluster management memory\"?\r\n\r\nThank you! Olap on hadoop 19/08/2015 à 13:52 Great post!! These days organization using different technology with Hadoop and to plan cluster of data and performing orations on hug database. nice to see your article. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-11-26"},
{"website": "Octo", "title": "\n                Android Testing :: testing private methods            ", "author": ["Stéphane Nicolas"], "link": "https://blog.octo.com/en/android-testing-testing-private-methods/", "abstract": "Android Testing :: testing private methods Publication date 07/09/2012 by Stéphane Nicolas Tweet Share 0 +1 LinkedIn 0 This article is about testing private methods in android. This is a fairly common problem in android (even in Java at large) and can be solved easily. The technique proposed here provides the additionnal benefit of using a traditional way of solving the problem in the Java world . (suspense :) ) Using the android platform, you are used to divide your application into two projects : one for the main source code of your application, one for the tests This is a mandatory structure on android, stongly emphasized by Google. Even, if you use the android-maven-plugin , you follow this pattern, and it is also the case if you use robolectric . Moreover, it is a common practice to name the packages of your application using the following convention (inside the AndroidManifest.xml files of both projects) : foo.bar foo.bar.tests Where foo.bar follows more or less your web domain name in the reverse order. If you use the same package names to name the packages of your class, and this is a fairly common and intuitive practice, then you will face a big trouble : you will only be able to test your public methods : a test class inside a package named foo.bar.tests won’t be able to access the private and protected methods of a class in the package foo.bar . For instance : In your app under test : package foo.bar;\r\n\r\npublic class Foo {\r\n    protected int m()  { return 2 };\r\n} In your test app : package foo.bar.test;\r\n\r\nimport android.test.AndroidTestCase;\r\n\r\npublic class FooTest extends AndroidTestCase {\r\n    public void testFoo() {\r\n        //m() is not accessible from Foo\r\n        assertEquals( 2, new Foo().m() );   //should not compile\r\n    }\r\n} This code would generate a compile error. The method m() is only accessible to the classes that belong to the package foo.bar and the notion of subpackages doesn’t exist in the Java programming language. Consequently you have several options. 1. Consider testing only public methods. Actually, this makes a lot of sense. Object-Oriented programming strongly emphasize the notion of contract and testing internals of a class can appear as violation of the notion of encapsulation. Nevertheless, there are some cases where you really want to test some internal methods as they are complex, or just as they contain more of the logic of the class than public methods, or because you divided the different problems faced by public methods into some more specialized private methods (and this is a very good practice in IT, the famous divide and conquer principle). 2. Subclassing your class under test is an alternative : you can use protected methods in your class under test. And, in your tests, whenever you need to test a protected method, although you can’t invoke it directly, you can create a smaller inner class of the test class that extends the class under test and overrides every protected methods under test. This overridden method will just just invoke the super method, just to re-give it a package level visibility. The inner class protected methods will then be visible to the test class as they now belong to the same package. Here is an example of this technique : In your test app : package foo.bar.test;\r\n\r\nimport android.test.AndroidTestCase;\r\n\r\npublic class FooTest extends AndroidTestCase {\r\n    public void testFoo() {\r\n        //m() is not accessible from Foo \r\n        //but it is from FooUnderTest\r\n        assertEquals( 2, new FooUnderTest().m() );\r\n    }\r\n\r\n    private class FooUnderTest extends Foo {\r\n        @Override\r\n        protected int m() { super.m() };\r\n     }\r\n} Although it is a common and accepted practice in Java to give a normally private method a protected visibility just to accomodate the case, purists are still under shock (and I belong to that camp), but that’s nevertheless a standard in Java. But this example demonstrates the drawback of this technique as well : it requires a huge, almost useless, amount of code, just a workaround and syntactic solution to a Java problem, but contains no interesting logic and is, indeed, a useless and thus painful programming effort. Nevertheless it could please purists as there is no need to alter the system under test only for testing purposes. 3. Using introspection is also possible. Reflexion allows to bypass the built-in visibility mechanism of Java. It allows to invoke any private / protected method on an object, or to use a private / protected field. Here is an example : In your test app : package foo.bar.test;\r\n\r\nimport android.test.AndroidTestCase;\r\n\r\npublic class FooTest extends AndroidTestCase {\r\n    public void testFoo() {\r\n        Method method = targetClass.getDeclaredMethod(\"m\", new Class[0]);\r\n        method.setAccessible(true);\r\n        int actual = method.invoke( new Foo(), new Object[0]);\r\n        assertEquals( 2, actual );\r\n    }\r\n} Although this leads to some much more cryptic syntax and make you loose the ability to refactor your code easily (as you will have to hard code method under test names in your tests), this technique is only one that can be used to test a really private method. It will please purists as there is no need to alter the system under test only for testing purposes. 4. Avoid naming your Java packages in the same way as you name you Application package : in other words, there is absolutely no obligation to use the same name in your Java package and inside your AndroidManifest.xml. It was a bad idea from Google to call a “package” the app identifier inside an AndroidManifest file . This “package” is indeed an ID for your application inside google market for instance but it imposes no kind of constraints on Java packages inside your app and has indeed nothing to do with them. Here is an example : In your test app AndroidManifest.xml: ...\r\n package=\"foo.bar.test\"\r\n ... In your test app Java classes : package foo.bar; //<---That's the point !!\r\n\r\nimport android.test.AndroidTestCase;\r\n\r\npublic class FooTest extends AndroidTestCase{\r\n    public void testFoo() {\r\n        //m() is now accessible directly from Foo\r\n        assertEquals( 2, new Foo().m() );\r\n    }\r\n} How does this work ? Simply because test classes and classes under test will be loaded inside the same Dalvik VM, all classes loaded from both apps and test-apps will be merged at runtime. Thus there is no difference in placing a class in an app or another on this specific matter (but please note that a class under test can’t depend on any class that is found in the test app dex/apk, this would result in an exception ‘ClassNotFoundException : class loaded from the wrong dex’ ). Conclusion This latter approach is much simpler than anything else and relies on the common Java practice of giving the protected visibility level to private methods for the sole purpose of testing them. By using the same package for your test classes and your classes under test, you can test very simply internal methods and the package structure of your test app is much clearer. Moreover, there is an additional benefit : you can use the eclipse “create junit test” wizard on your class under test (right click on the class under test and find it in the “ New >> JUnit Test Case ” option). Just change the source folder field of the wizard and choose your test app source folder instead of the app folder. This article is the first of a serie of articles that will be dedicated to testability of android applications, one of the axis of research inside the Mobile Development Team @ Octo Technology. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Android , automated tests , Testability , unit test . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 11 commentaires sur “Android Testing :: testing private methods” raultm 29/11/2012 à 10:59 Hi Stephane,\r\n\r\nWaiting your post about Android testing!\r\n\r\nBye Stéphane Nicolas 29/11/2012 à 14:13 Hi Raultm, I didn't catch it. What are you waiting for ? raultm 29/11/2012 à 14:56 At the DroidConNL you told me that you were writing a post about Android Testing, didn't you? Stéphane Nicolas 30/11/2012 à 06:56 Ohhhh Raul, nice to meet you again ! Yeah, but not this one. It will be about testing technologies on Android. Here is the beginning of the whole idea : https://plus.google.com/u/0/117469301043100028709/posts/Tz1n6DmLjaw\r\n\r\nLet's continue our chat on G+ jmini 02/05/2013 à 15:10 To point \"3. Using introspection is also possible\"\r\n\r\nWhat do you think from the Accessive library?\r\nhttp://code.google.com/p/accessive/\r\n\r\nFrom my point of view, it is a nice wrapper library for the Java introspection functionality. Stéphane Nicolas 03/05/2013 à 19:59 Does it work on Android's Dalvik ? Adil Hussain 25/02/2014 à 15:17 Genius! Just when I was losing hope I came across this article. Very elegant solution (option 4). Thanks a million. naxa 02/07/2014 à 16:06 targetClass cannot be resolved\r\n\r\nwhere do you get it from? vince 10/07/2015 à 11:00 Hi! \r\nI've tried to use the option 4, setting the app test package as the one tested, but I can't see the private methods :( . I'm using Eclipse+ADT, testing on Android KK 4.4.4\r\n\r\nThanks in advance for helping me.\r\nVince mochadwi 29/04/2020 à 01:03 Looks like the point no. 4 is now obsolete, and won't works for AS latest version (4.0.0). @VisibleForTest(PACKAGE_PRIVATE) is recommended instead mochadwi 02/05/2020 à 20:15 This doesn't works (anymore/I'm missing something) for AGP 3.5.x & above. \r\n\r\nI still don't know how to change the package name in `AndroidManifest.xml` (section no. 4) because test/androidTest directory is the same package with main. Should we change it just for the purpose of testing? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-09-07"},
{"website": "Octo", "title": "\n                iOS dev: How to get your code coverage right?            ", "author": ["Cyril Picat"], "link": "https://blog.octo.com/en/ios-development-right-code-coverage/", "abstract": "iOS dev: How to get your code coverage right? Publication date 03/09/2012 by Cyril Picat Tweet Share 0 +1 LinkedIn 0 When I decided to tackle my preceding blog article on quality metrics for iOS , I wasn’t prepared to spend that much time to get something robust and correct . The part on which I stumbled most was the code coverage, not because it’s that difficult to make it work (there is plenty of resources on the Web) but because in all articles I have seen the solution was working but was not reporting accurate and useful metrics (I am sure I have missed some, sorry for this if this is the case). Note: the fact that Xcode support for it is very fragile and has changed with almost each version of Xcode did not ease this and explain why people were first focused on making it work. Here are some of the pitfalls I saw in all the articles talking about code coverage: Pitfall #1: only the files under test are reported in the code coverage report. It means you do know the coverage on what you did test but not on what you have not tested. This is the biggest pitfall according to me. Pitfall #2: third-parties libraries and test files are impacting the coverage figures Pitfall #3: the report is not structured so difficult to analyze and make it actionable Pitfall #4: no article makes the difference between GHUnit and OCUnit , even if there are some indeed The setup I proposed in the preceding article is still valid and will avoid you these pitfalls. As the article was already long enough, I decided not to make it longer and keep all the detailed explanations for a new article. Here it is. Let’s see how to tackle these issues one by one. If you tried to setup the code coverage using my last article, it will make all the steps a bit more logic. Pitfall #1: Cover it all! The two most popular blog posts on how to get code coverage for the iPhone can be found here and here . Follow them and you will only get the coverage for the files that are actually tested by unit tests. OK you might know that all other classes are not tested, but how much does it weight? And does other people in your team know? What if your “standard” says that this part of the application should be covered (for example an utility class)? Why does this happens? This is because the recommended approach is to enable the build settings ‘Generate Test Coverage Files’ and ‘Instrument Program Flow’ only on the test target, which means that only the tested files and the tests themselves will have .gcno files (code to cover) and .gcda files (code covered) generated during compilation (for .gcno files) and execution (for .gcda files). This is why in my setup I also enable the two build settings ‘Generate Test Coverage Files’ and ‘Instrument Program Flow’ on the main target . Notice that I only enable them in Debug to avoid having issues with production code. Is that enough? No, the second difference is to make GCOVR operate on the main bundle and not the test bundle. This is because the test bundle only contains the .gcno/.gcda files of the tests while the main bundle contains both. This is what the --object-directory build/YOUR-PROJECT-NAME.build/Debug-iphonesimulator/YOUR-MAIN-TARGET-NAME.build/Objects-normal/i386 option in GCOVR command line is doing. Note: this is only valid for OCUnit. There are other differences for GHUnit but I cover them later ( Pitfall #4 ) You can check that everything is working locally first by going to ~/Library/Developer/Xcode/DerivedData/YOUR-PROJECT-NAME/Build/Intermediates/YOUR-MAIN-TARGET-NAME.build/Objects-normal/i386' . It should contain (once you have run your tests) a lot of .gcno and .gcda files. Check that there is a .gcno but no .gcda file for files you know not being tested. Once done, you will see that your coverage metric in Jenkins will decrease a lot and will show you what your coverage really is! Pitfall #2: It’s not mine! Now that you cover all your code, you will get a new problem: it will also report coverage for code that is not yours! That’s not desirable because this is the kind of noise that make a metric false and quickly abandoned. This is resolved in two steps: isolate your third-parties libraries in a folder (this is common sense even if you do not want to compute code coverage). Notice it should be a real folder and not a Xcode logical one. I have put all mine in an ExternalFrameworks directory: add a --exclude '.*ExternalFrameworks.*' flag to the GCOVR command line Note: you should also exclude your unit tests themselves with --exclude '.*Tests.*' but this part was often already covered in the mentioned articles Once done, your coverage metric in Jenkins should increase significantly as I haven’t seen much tests in the iOS frameworks I am using! Pitfall #3: Make it actionable! The latest pitfall I have seen is that all these reports are flat and give you only a project and a file-by-file view of the coverage. No intermediary view. Very often we structure your application in layers and Apple MVC pattern encourages you to do so. Wouldn’t it be great to know the coverage of the different layers? This is particularly true because you don’t test all the layers with the same kind of tests and you don’t put the same test efforts on the different layers. My best practice is to structure your project in directories that are both meaningful for day-to-day work and for the code coverage analysis. This is an example of a typical structure I use: Notice that you should use real folders (not Xcode logical ones) if you want this to have an impact on Cobertura report. This is how I get the following report already shown in the preceding post: In this way I can quickly check that sensitive application layers like domain (Model directory here), services and utilities (Utils here) are correctly tested. As a rule of thumb, here is what I am targeting today in my applications for unit tests code coverage: 100% code coverage on Model, Manager, Utils . Build should fail under 80%. These layers are normaly 100% testable without much mocking effort. 80% code coverage on Service . Build should fail under 50%. This is because a few services might require much mocking effort. 0% code coverage on Controllers, Views . Build should not fail because of it. Unfortunately, it’s not possible yet to specify specific thresholds by package in Cobertura Jenkins plugin (vote for the open issue in JIRA here !). Someone has posted a solution for this on stackoverflow.com , but I have not tested it yet. Of course once you start adding UI tests, the coverage of the Controllers/Views should increase significantly. I have not tried yet to include these tests in the coverage report, but I hope to be able soon. Once done, it should become clear which part of the application you should test next . Pitfall #4: Of course xxUnit is the best tool! Last, but not least: we all have our preferred tools to work. There are strong advisers of OCUnit or GHUnit, and very often not both are covered when it deals to getting test results or coverage. I won’t go back to the explanations already given in my preceding article, but will focus on what is structurally different between the two: GHUnit is not integrated in Xcode : That’s why a build setting like ‘Test after build’ is not useful and this is also why you have to specify ‘Application does not run in background’ for GHUnit in order to have the application quit at the end of the tests (and write the test reports). GHUnit builds a completely new application : That’s why it has its own bundle and its own AppDelegate. This is also why you have to put fopen$UNIX2003 and fwrite$UNIX2003 in a different place. And most importantly, that’s why you need to copy the .gcno generated for your application code in your test bundle, otherwise you will not get the coverage for files that are not tested (you go back to Pitfall #1 then). This is what the: cp -n build/YOUR-PROJECT-NAME.build/Debug-iphonesimulator/YOUR-MAIN-TARGET-NAME.build/Objects-normal/i386/*.gcno build/YOUR-PROJECT-NAME.build/Debug-iphonesimulator/YOUR-TEST-TARGET-NAME.build/Objects-normal/i386 || true is made for and it is important to do this before the GCOVR command line for it to be useful. Conclusion The unintelligible steps detailed in the preceding article should become logic now, do they? I hope that this will help you make the most of your improved or new coverage metrics ! Follow @cyrilpicat Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged cobertura , Code coverage , development , gcov , gcovr , iOS , iPhone , Jenkins , lcov . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 4 commentaires sur “iOS dev: How to get your code coverage right?” Vijay 26/09/2012 à 09:06 Tremendous effort,. superb article covering everything needed for code coverage. Great work. Thanks a lot for sharing with us. Cyril Picat 26/09/2012 à 10:29 @Vijay thanks! Mike Yost 24/12/2013 à 16:40 Thanks for the effort.\r\nI am trying to implement code coverage on Xcode5, but gcovr cannot seem to find the .gcna/.gcda files.\r\nif pwd = the source directory\r\n   gcovr -r . --object-directory=\r\nit looks for the input files in the sources directory.\r\n\r\nif pwd = the derived build dir,\r\n    gcovr -r . --object-directory=\r\nit says it's looking at / for all the files and tries to search my entire 500 gb disk. \r\nI let it run overnight and it didn't finish, so I think it's just hung.  \r\nHas anyone else had this problem? shmily 29/09/2016 à 12:08 These ariticles are really helpful to me.\r\nBut I did as you wrote in these articles,and it did have the coverage report. But the question is that it included all the source code (*.m 、*.h). And as you know we don't need the *.h file's statics data.\r\nHow can i filter out the *.h file? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-09-03"},
{"website": "Octo", "title": "\n                Batman rises in Monte-Carlo            ", "author": ["Henri Tremblay"], "link": "https://blog.octo.com/en/batman-rises-dans-monte-carlo/", "abstract": "Batman rises in Monte-Carlo Publication date 05/09/2012 by Henri Tremblay Tweet Share 0 +1 LinkedIn 0 I had the chance, with Alexis Flaurimont, to speak about the usefulness of parallel programming at Breizh C@mp this year. One of the goals was to demonstrate that parallel programming is a lot easier to code than a couple of years ago. During the presentation, we used the Monte Carlo method . It is, I must confess, an embarrassingly parallel algorithm. Perfect to demonstrate that parallelization can greatly improve an application performances. The Monte Carlo method is usually explained by calculating Pi. It consists of calculating statistically something that we can’t calculate mathematically. Pi is in fact a bad example since it’s more efficient to calculate it using power series. I’ll let you read the Wikipedia article for a longer explanation but let just say that the idea is to randomly draw values and decide what they worth. The average worth value of all the draws will approximate the real answer. The more we draw, the closer we are from the real answer. From where I stand, Pi just ain’t no fun. So I picked Batman instead. To calculate the area of the Batman logo to be precise. Much more amusing. I’ve based my calculation on the Batman equation” that was circling around Internet last year. With a lot of minutia and helped by Wolfram Alpha , I’ve coded it in Java. Please note that the Wolfram Alpha equation is wrong. I’ll look at how to tell them without success. If someone knows how, please leave a comment. Here’s the fix: The code is on Github . Visually, it looks like that: batman There’s also a Pi calculating version, but, since it’s just a circle, as I said, it ain’t no fun. But useful to test the Monte Carlo implementation. Back to Batman. Quite nice visually, but that was more useful in conference than here. What we want here is to compare the performances. For that, we use a 30 seconds execution of each. The goal is to perform as much draws as we can. I did three implementations: Sequential: A loop does on draw after the other Parallèle: A ForkJoinPool launch parallel loops on each available CPU GPU: Aparapi is used to calculate on the GPU To give you an idea, here’s a snippet of the sequential code: MonteCarloCalculator calculator = instantiateAlgorithm(constructor, 0);\r\nnew MonteCarloCmd(calculator).run(); Quite obvious. We call a calculator that loops. In parallel it’s a bit more complex but not much: ForkJoinPool pool = new ForkJoinPool(); // pool creation\r\nfor (int i = 0; i < pool.getParallelism(); i++) {\r\n   MonteCarloCalculator calculator = instantiateAlgorithm(constructor, i); // one calculator per CPU\r\n   pool.execute(new MonteCarloCmd(calculator)); // we launch the parallel execution on the pool\r\n}\r\ntry {\r\n   latch.await(TIMEOUT, TimeUnit.SECONDS); // and wait for everything to be done\r\n} catch (InterruptedException e) {\r\n   throw new RuntimeException(e);\r\n}\r\npool.shutdown(); // close the pool (that can be reused if needed) The real code on GitHub is a bit more complicated because I’ve put in place a listener pattern to give some feedback while it’s running. This implementation is in fact penalizing the parallel algorithm since it requires data aggregation using atomic references and optimistic updates during the run. Than wouldn’t be necessary in a traditional implementation. For the curious among you, here the Java version of the Batman equation: // Wings bottom\r\nif (pow(x, 2.0) / 49.0 + pow(y, 2.0) / 9.0 - 1.0 = 4.0 && -(3.0 * sqrt(33.0)) / 7.0 = 3.0 && -(3.0 * sqrt(33.0)) / 7.0 = 0) {\r\n\treturn true;\r\n}\r\n// Tail\r\nif (-3.0 = 0 && 3.0 / 4.0 = 0) {\r\n\treturn true;\r\n}\r\n// Ears inside\r\nif (1.0 / 2.0 = 0 && y >= 0) {\r\n\treturn true;\r\n}\r\n// Chest\r\nif (abs(x) = 0 && 9.0 / 4.0 - y >= 0) {\r\n\treturn true;\r\n}\r\n// Shoulders\r\nif (abs(x) >= 1.0\r\n\t\t&& y >= 0\r\n\t\t&& -(abs(x)) / 2.0 - 3.0 / 7.0 * sqrt(10.0) * sqrt(4.0 - pow(abs(x) - 1.0, 2.0)) - y + (6.0 * sqrt(10.0)) / 7.0\r\n\t\t\t\t+ 3.0 / 2.0 >= 0) {\r\n\treturn true;\r\n}\r\nreturn false; To compare the implementations, a benchmark have been done on a Quadruple Extra Large GPU cluster at Amazon. We had 16 CPU cores and 996 GPU cores. Here are the results for the 30 seconds execution: Sequential: 179 786 000 draws Parallel: 709 731 000 draws GPU: 12 582 912 000 draws In parallel, we get let improvement than I was hoping for. We surely can do a better job. As I said, the implementation is penalizing the parallel execution. And it’s not that terrible since it shows a nice demonstration of the Amdahl’s law . One thing to remember: The sequential version is using only one CPU while the parallel one is using them all. While keeping a relatively simple implementation. On the GPU side, the simplicity effect is less obvious. The good news are that Aparapi is quite refreshing. The library is translating the kernel (the part that runs on the GPU) bytecode into an OpenCL implementation which is called through JNI. It’s developed by AMD. You can cook a matrix calculation example in about 10 minutes. However, doing a Monte Carlo is a bit more complicated. Here are some issues you might encounter. You can use only really simple JDK classes. For instance, java.util.Random just doesn’t work GPUs don’t like conditions (“if”). To get good performances, you have to modify your code into a linear computation For some reason, bitwise (&, |, ^) operations are silently not working On top of that, you won’t get with Aparapi the performances as high as you would get with CUDA. But the gap is slowly closing. For instance, there are new annotations helping the memory management. Data localisation in memory is really important on a GPU. Still, you surely noticed that, without any optimisation effort, the GPU version is 17 times faster than the parallel one! I’ll conclude like this: Sequential, you already know about it, it’s reassuring, no need to think too much… and it’s slow Parallel, not that complicated after all. A lot complexity hidden from the developer. Indeed, the GitHub code is in fact much more complicated than required GPU, I’ll say it’s refreshingly easier than 2 years ago, but we are not in the commodity realm yet. It’s not that hard, but you need to make sure you really need it before using it Meanwhile, let Batman rise in parallel. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-09-05"},
{"website": "Octo", "title": "\n                Toward a better software factory            ", "author": ["Mikael Robert", "Vincent Canuel", "Amel Benazza", "Cyril Descossy", "Vincent Canuel"], "link": "https://blog.octo.com/en/toward-a-better-software-factory/", "abstract": "Toward a better software factory Publication date 29/05/2012 by Mikael Robert , Vincent Canuel , Amel Benazza , Cyril Descossy , Vincent Canuel Tweet Share 0 +1 LinkedIn 0 The improvement of software factories is one of the current OCTO’s R&D topics. This article presents our vision regarding their future. Before diving into new features and perspectives, let us first describe the concepts and limits of current software factories. What is a software factory? A software factory is made of several tools for both development and automation of the overall software building process, from development steps to production delivery. A software factory also contains all the tools required for software development, thus including documentation, wiki and source code manager. Software factory is important in development industrialization : it is actually the keystone of a continuous integration. Indeed, state-of-the-art software development follows a process based on the software factory. It provides a better control of development quality and progress through indicators. The following diagram provides an overview of a software factory : The represented workflow is quite complete and answers the needs of many projects. The choice to actually implement each feature depends on the project maturity. What are the benefits of a Software Factory ? The “defect cost increase” states that the later an anomaly is detected, the more expensive it is to fix it, with increase magnitude ranging from 1 to 1000. So, early bugs detection is a real gain, which can be achieved through continuous integration with a software factory. A software factory is a powerful tool for development and allows the automation of the following steps : Checkout of reference code. Moreover, it reinforces the use of a professional source code manager. Source code compilation Run of automated tests Measure of test code coverage and code quality Respect of development standards Check the good use of design patterns Compliance with architecture rules A software factory also induces additional benefits : Provide indicators about quality and progress of the development process to non technical team members. Continuous improvement of the code quality and a higher trust in the produced code Continuous improvement of the development team productivity The most advanced software factories provide automatic deployment and driving of development, through features such as : Generating technical reporting site, such as maven site Driving and reporting development team activity with issue manager such as Jira or Redmine Packaging and deployment on platforms (test, staging, production…) Deployment to different environments (see : https://blog.octo.com/5-bonnes-raisons-de-deployer-en-continu/ ) Who is the target for this factory ? Although a software factory is first and foremost used by development teams, its influence zone extends further. Indeed, 4 main profiles are identified : Business teams : the frequency of deployment is higher so they can test more often. Feedback on the application being developed can also be managed by the software factory. Techlead (or project manager?) : He can follow code quality metrics and shares commons tools with development teams. Production team : they can follow the deployment phases. Development team : they produce the code and can delegate the build steps to the software factory. What are the challenges ? The industrialization of a project raises many challenges at different levels. The first and most important one is to automate as many tasks as possible. It is one of the main goals of the DevOps stream because it minimizes the risk of human errors. In the long run, time is better spend on maximum automation than on correcting oversights and typos… that people generate from time to time! Regarding to the quality of development, its goal is to improve the consistency and to establish some rules and standards that can be quickly applied. At Operations level, delivery success is guaranteed by optimizing the automation of the delivery process. Everyone has been faced with stories similar to “oh ! I forgot to change the properties in the 42rd properties file of prod-18 environment!”. According to this, identified priorities concern both code lifecycle and management tasks such as driving and reporting of activities. Finally, a software factory is also very important to avoid technical and functional regressions but also performance regressions. The later requires the ability to be able to test and to measure the performance of the software being developed. All these improvements lead to a better time to market (TTM). What are the limits ? For a long time, IT teams maintained bigger and bigger projects, which imply higher maintenance costs, longer time passed into the software factory’s workflow and higher bug probability. Time to market could be improved with a better build time (compilation, tests, quality…). However, unless a complete code refactoring to allow parallel test runs with tools such as Maven 3, build time optimization limits are quickly attained. We have identified several other limits on the current model : Continuous deployment (the real one) : many software factories are already used to deploy continuously, but it’s often a custom deployment script that is called by the software factory. Multi-platforms management : iOS, Android, .NET, Java… impossible to build all of them in the same environment. Cloud deployment of all parts of the complete software factory is far from easy and reproducible, even if cloud software factories exist (see this article . Generating a complete software factory per project typology (Java or iOS…) can be imagined. Those software factories need to be adaptable and multi-projects : software factory mutualization is a must. One software factory per project is too expensive to maintain and therefore to be considered as a viable option. Monitoring : for example, following the time to resolve in real time. Unbreakable build : it is already possible in current software factories, it is a requirement for continuous deployment. Build distribution on several nodes : possible on some platforms, setup complexity is high. Nevertheless, it should be a good way to speed up build times. (Time to market focus) Toward a software factory 2.0, DevOps as spearhead DevOps is a set of practices intending to improve collaboration between development and operations teams (See : this article ) and tends toward a responsibility of every project actor. The DevOps stream offers tools dedicated to these tasks, especially for implementing and measuring automation. This brings the idea of a very promising software factory 2.0 in which the DevOps tools are mixed with the actual software factory in order to answer to the following issues : Improvement of build workflow Today, there are still many manual steps that could be automated through DevOps tools (server deployment, configuration management…). In the same way, validating the required but manual steps (UAT, deployment decision…) could be facilitated through the DevOps tools. Environment Management Widely used by DevOps, CMDB tools (Configuration Management DataBase) can centralize the environment configuration, but also integrate it into the deployment automation (we’re talking about deployment to run the various tests but also for production!) Platforms setup These tools are not just a fad because the server management tools such as Chef or Puppets are not only highly productive but also safer. Indeed, they prevent the classic “oversights” when setting up a new machine. They allow the automation of the setup of a server with the packages and the configuration necessary for the wanted services, so why not a setup for a Software Factory or a part of it! Automated test (unit, staging, integration, …) Nothing new for testing purpose, but the integration of modern instrumentation tools for testing and separation of these various tests is an important factor in the new software factory. Software Quality Not always in place in current software factories, this dimension is still very important so it must be part of the new software factory. Performance decrease detection Making performance tests is good, but running them regularly to be able to detect a decrease of performance from day to day is better. Even though, it is often very expensive and time consuming. However, the tools presented above should facilitate their integration into the build. Speed of execution Build time is too long? On demand parallel execution or distribution on multiple nodes can be a solution. The idea is to reuse the patterns found in other applications (clustering, load distribution…) to build them. Continuous deployment to production Once all steps are cleared, let’s deploy to production. The benefits of continuous deployment have been proven many times. Audit With all the features that will be managed by the software factory, traceability is a must-have in order to understand why things have been done but also to be able to set up an archiving system. Another important element is the ability to go back (rollback) if the deployment goes wrong. And finally … Multilanguage aspect Indeed, Multi-technology IS is a real management issue : Java, NET, iOS, Windows Phone… each compiler needs its own platform but this goes against the wish of having a shared software factory… To be continued into the next episode… stay tuned! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , General -- DO NOT USE . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-05-29"},
{"website": "Octo", "title": "\n                Untar on iOS, the pragmatic way            ", "author": ["Mathieu Hausherr"], "link": "https://blog.octo.com/en/untar-on-ios-the-pragmatic-way/", "abstract": "Untar on iOS, the pragmatic way Publication date 16/12/2011 by Mathieu Hausherr Tweet Share 0 +1 LinkedIn 0 The problem Why untar? Network connections cause latencies on your mobile app. Replace 10 downloads of 1Mb by 1 download of 10Mb is a good improvement to your app. There’s a well-known unix tool for that: tar. What is tar? Wikipedia says: “Tar is now commonly used to collect many files into one larger file for distribution or archiving, while preserving file system information such as user and group permissions, dates, and directory structures.” That’s exactly what we need with some extra features. How to untar an archive file on your iOS device? How to untar? A goolge search about untar on iOS provide two solutions: BSD libarchive. This lib is already in your iOS device but without header files. Apple calls that a “private API”. You can’t use it in App Store. If you are doing an app for jailbroken device there isn’t any problem but I need to push my app to the App Store. Davepeck BSD libarchive implementation https://github.com/davepeck/iOS-libarchive . The code of libarchive is open source so davepeck has packaged this code for iOS and you can add it to your project. This libarchive implementation is the same than the tar command on your Mac. You can create or extract tar file and provide gzip or bzip compression. But this lib includes 617 files, 347 606 lines of code (without any Objective-C wrapper that you have to code by yourself) and weights 4.7MB when build. Just remember that you need wifi to download app heavier than 20MB. Why not using libarchive? With libarchive: We add code dependencies to our project. These dependencies need to be updated. More lines of code means more bugs. According to Steve McConnell’s “Code Complete”, there are on average between 15 and 50 bugs per KLOC (Kilo line of code), so if I take a calculator and make rocket science estimation, there are 5000 bugs in libarchive. If Apple changes an API and break libarchive compatibility, your code will not stay reliable. Is there another way? What if I must push my app in the app store but don’t want to link this 4.7MB of hardly-maintainable code? Let’s code my own light and pragmatic untar implementation. What do we really need? Libarchive weights 4.7MB but do a lot of things we don’t really need. We need to untar files but we don’t need: To create tar files. To uncompress files (gzip or bzip). If we want to compress our file we can use zlib and then untar. We don’t have to untar during inflate. To work with old fashion tar (from the 80’s). To work with unix ownership and right: iOS will not let us set this rights even if we wanted. To work with simlinks, hard links, FIFO or other special stuffs. We just want to handle files. The solution What’s a tar file? A tar file is composed of 512 bytes data blocks. The first block is a header for the first file; the next n blocks are the content of this file and then a new file start with a new header block. A tar file looks like that: (H is header block, C content block, / partial content block) At the end of the tar file there is two empty blocks. A new file starts always after an integer number of blocks. Some files haven’t any content block (like folder). Each block has 512 bytes. A header block comports a lot of datas. Few of these data, and none of the new UStar header data are useful for us. We just need: The file name: Encoded in ASCII, it’s a relative path like “mydir/myfile.pdf” The file size: Encoded in ASCII, it’s an octal value in bytes. The file type: we only need two file types, ‘0’ for regular files and ‘5’ for directories With the file size we know when the header block of the next file starts. And where to strip the content of the last block of the file. The result: Light-Untar for iOS Not so complex isn’t it? I just implemented this code in Objective-C and push it to github: https://github.com/mhausherr/Light-Untar-for-iOS This code is under BSD license; you can use it for your own project. Is it better than libarchive? Light-Untar-for-iOS has only 2 files and 168 lines of code, which include the comment lines with the license text. To use it, include the .h file and use this NSFileManager method: [[NSFileManager defaultManager] createFilesAndDirectoriesAtPath:@\"/path/to/your/extracted/files/\" withTarData:tarData error:&error]; tarData is an NSData. You can create the NSData with this code: NSData *tarData = [NSData dataWithContentsOfFile:@\"/path/to/your/tar/file.tar\"]; Limitations about Performances and Security Performances Libarchive  is written directly in C and does the job faster than my Objective-C code. The question is: what’s the most important thing for you? To win 40ms on each untar? To save 4.7Mb of the size of your app? If you choose the second one, Light-Untar-for-iOS is for you. Security What are major security issues of tar? Symbolic links: Someone can add links to give access to folders outside the working directory. Set root ownership or add execution right: Someone can add an executable file with root access and launch it. Neither of these are implemented in my code, so you can use it safely. And you know what? It’s an iOS lib. On iOS , applications are sandboxed. The system himself protects this code. We don’t need any other protection. Conclusion Mobile development is primarily development on low capacity devices. Size of the build app is also important. If you have a simple problem to solve, ask you what is the best: A huge framework can do that for you. Do you need really all this framework? Can’t you just implement the requested feature? To go further http://www.gnu.org/s/tar/manual/html_node/Standard.html http://en.wikipedia.org/wiki/Tar_(file_format ) http://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670 Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged iPhone , untar . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 8 commentaires sur “Untar on iOS, the pragmatic way” Christoph Ketzler 17/02/2012 à 14:17 Hi\r\n\r\nOn question for the Light-Untar-on-iOS. Is it possible to untar very large files? Larger than the available RAM? \r\n\r\nThanks in advance.\r\nChristoph Mathieu Hausherr 17/02/2012 à 14:43 Hi Christoph,\r\n\r\nYes, you can untar very large files. I've just fix a bug about NSAutoreleasePoll, just update your code and it will be OK.\r\nI test this code with 1Go+ tar archives.\r\n\r\nMathieu Christoph Ketzler 27/02/2012 à 08:57 Thank you very much. Inspired by your post, I coded my own ActionScript version of untar.\r\n\r\nhttp://www.ketzler.de/2012/02/as3-simple-untar/ varoon 30/03/2012 à 11:20 Can we unzip \"Tar.gz\" or \".tgz\" file on IOS device using your code ? Phil 19/06/2012 à 01:59 I experienced some crashing at the\r\n\r\n [self writeFileDataForObject:object inRange:NSMakeRange(location+TAR_BLOCK_SIZE, objectSize) atPath:filePath]; \r\n\r\nline of the \r\n\r\n-(BOOL)createFilesAndDirectoriesAtPath:(NSString *)path withTarObject:(id)object size:(int)size error:(NSError **)error \r\n\r\nmethod -- because the inRange: range was beyond the end of the file.\r\n\r\nSo I added this code prior to that line to at least stop the crashing:\r\n\r\n                if (location + TAR_BLOCK_SIZE + objectSize > size) { \r\n                    objectSize = size - location - TAR_BLOCK_SIZE;\r\n                    NSLog(@\"corrected objectsize to %ld\", objectSize);\r\n                }\r\n\r\n(Oh yeah, I also changed the local variable 'size' to the name 'objectSize' to distinguish it from the method argument 'size'.)\r\n\r\nI have no idea what side effects this will have, but it no longer crashes and seems to work.\r\n\r\nIt is possible that my tar file has something wrong with it -- it is coming gzipped from a server that I don't control.  So maybe your code is fine and I am unzipping it wrong, or it was tarred wrong, or whatever, idk.\r\n\r\nAny thoughts?  Anyone?\r\n\r\nThank you for this code though!!  Saved my butt. Martin 18/02/2014 à 12:16 Can we display file in ios. With all file contens like .doc file and image file.\r\n\r\nThanks in advance.\r\nMartin Alexey 03/12/2015 à 11:54 Old-BSD licence with an infamous advertisement clause... No, thank you. Chris Ostmo 06/04/2016 à 09:48 It all works easily and as advertised. Great job!\r\n\r\nOne suggestion: Update the examples here and on github to include the progress block. I'm not a newbie, so it only took about 5 minutes for me to figure out that I needed to add progress:^(float f){} to the end of the parameters, but the examples produce \"No visible @interface...\" errors that prevent compiling.\r\n\r\nThanks! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-12-16"},
{"website": "Octo", "title": "\n                iOS dev: How to setup quality metrics on your Jenkins job?            ", "author": ["Cyril Picat"], "link": "https://blog.octo.com/en/jenkins-quality-dashboard-ios-development/", "abstract": "iOS dev: How to setup quality metrics on your Jenkins job? Publication date 28/08/2012 by Cyril Picat Tweet Share 0 +1 LinkedIn 0 iOS development projects are not first-in-class when it comes to managing the quality of the software produced . Very short projects, very short time-to-market, it is not the kind of projects where you see a lot of attention towards quality, unfortunately. Here at OCTO we try to do it differently, even for this kind of projects. Or above all, for that matter. But here comes another issue: the lack of tooling . Here is one of our latest attempt to setup quality metrics on a short and budget constrained iOS project. And yes, a teaser, look at our dashboard at the end of this six weeks (six Agile iterations) project : The following article will detail how to setup all these quality metrics in a integrated report in Jenkins (Continuous Integration). Here is the breakdown of this article: Tooling overview Step 0: Setting-up Jenkins Step 0bis: Setting-up your Jenkins job Step 1: Setting-up easy metrics Code metrics (LOC) Code duplication Step 2: Setting up harder metrics Test results Alternative: Test results with GHUnit Test coverage Alternative: Test coverage with GHUnit Step 3: Enjoy! Conclusion Tooling overview When it comes to tooling, Objective-C is a mixed case because it is at the same time a proprietary platform and a language derived from C. In practice, here is the current situation on quality tools for Objective-C: there is not much static analysis tools (ok there is Clang Static Analyzer but mostly for memory management, far less useful since the introduction of ARC for iOS). You could use clang AST to write your own checks for your project or company but I haven’t yet heard of anybody having done this. No FindBugs , PMD or Checkstyle for Objective-C yet. In fact by saying this I thought I’d better double check and I found OCLint that looks interesting but is in a very early stage. even basic code metrics are not easy to compute. Most of the C tools for calculating LOCs work (for example SLOCCount or cloc ) but you can’t get interesting metrics like number of classes, class complexity, method complexity etc. one of my preferred quality metric, code duplication, is feasible but not that easy. You can use PMD . for this (with a specific grammar) or use Simian (that supports it out-of-the-box but is not free for commercial use) code coverage is feasible but not easy to get it right. You can do it thanks to a gcc feature called GCOV . there is no way to aggregate, consolidate and track all these quality metrics . No Sonar for iOS, said differently (or not yet, see the end of the article (LINK)). To fill in the gap, we have used directly our build server (Jenkins) to act both as the dashboard and the history of these metrics. Let’s look at how to setup all those metrics. Note: If you get lost on the way, do not hesitate to check the end of the article, I have put links to sample configuration files Step 0: Setting-up Jenkins We will not detail this point as there are a lot of posts around on doing this. For example you can go with this one in case Jenkins is not already installed at your place. I recommend using the same user as your CI user because otherwise you will run into issues with Keychain certificates (the certificates you will install on your server will not be available to the Jenkins user). Step 0bis: Setting-up your Jenkins job As above, I refer you to this article if it is something new for you. I also recommend using the Xcode plugin for Jenkins . Step 1: Setting-up easy metrics Let’s dive in interesting things. Setting up code metrics and code duplication is quite easy, so let’s start with it. Code metrics (LOC) I decided to go for SLOCCount as it already provides a Jenkins plugin. The documentation for it is here . I followed the instructions on both the SLOCCount page and Jenkins plugin page, that is basically: download latest archive of SLOCCount, unzip it somewhere, run ‘make install’. It will install sloccount in /usr/local/bin. install the slocount plugin in your Jenkins (via Jenkins UI) add a Shell Build Step in your Jenkins job to compute the code metrics. This is the step I added in my job: sloccount --duplicates --wide --details YOUR-PROJECT-ROOT-DIRECTORY | grep -v -e '.*ExternalFrameworks.*' | grep -v 'top_dir' > build/sloccount.sc This is how it looks in my Jenkins job configuration: Notice two added commands compared to the standard sloccount command: I excluded third-party source code with “ grep -v -e ‘.*ExternalFrameworks.*’ “ I avoid an issue with SLOCCount plugin (see here ) with “ grep -v ‘top_dir’ “. This issue has been fixed meanwhile so this step might not be necessary any more Tell SLOCCount plugin where to find the report by filling the ‘Publish SLOCCount analysis result’ field with ‘build/sloccount.sc’ (see below) Once this done, relaunch your build. You might have to modify your Jenkins configuration to have /usr/local/bin in Jenkins path (unfortunately Jenkins do not use your system path). I fixed this by setting the PATH variable in Jenkins configuration with the content of my path (‘echo $PATH’). You can do this in the ‘Global properties’ section of the configuration (not in your job configuration but in Jenkins configuration): If you go back on your dashboard, you should see the trend graph on it and have also access to the full report through the left menu. Actually, depending on the plugins, the trend graph does not appear from the start but once it has enough data to display (typically after 3 or 5 builds). The detailed report gives you the details of LOC by file and by folder . You can do a first sanity check here to know if no file has been forgotten or is present by mistake. Here is an example of the report by folder: Code duplication The tool chosen here is PMD because it is free to use. In fact the Objective-C support is more ‘beta’ than it is in Simian, so you might choose the later. Objective-C support in PMD is made possible thanks to Joshua Kennedy. Announcements and details on how to setup it are posted on his blog so I refer you to his post . Basically you have to: download PMD and unzip it somewhere (I went for ~/PMD so the extracted path is ~/PMD/pmd-4.2.5) download the Objective-C grammar JAR and save it at the same place (~/PMD in my case) install Jenkins DRY plugin (via Jenkins UI) Add a Shell Build Step in your job and paste the following command: java -Xmx512m -classpath /Users/udd/PMD/pmd-4.2.5/lib/pmd-4.2.5.jar:/Users/udd/PMD/ObjCLanguage-0.0.6-SNAPSHOT.jar net.sourceforge.pmd.cpd.CPD --minimum-tokens 100 --language ObjectiveC --encoding UTF-8 --format net.sourceforge.pmd.cpd.XMLRenderer --files YOUR-PROJECT-ROOT-DIRECTORY --files YOUR-TEST-ROOT-DIRECTORY > build/cpd-output.xml This is how it looks in my Jenkins job configuration: Configure DRY plugin to use the CPD output. For this you have to fill the field ‘Publish duplicate code analysis results’ with ‘build/cpd-output.xml’ Just one comment: I had trouble to make it work and always get a blank report file first (so as few other if you read the comments). <?xml version=\"1.0\" encoding=\"MacRoman\"?>\r\n<pmd-cpd>\r\n</pmd-cpd> If your are in this case, here is the hint: you have to specify a full path for the JARs in the classpath, for some reason I ignore. Well, if things went right, you should now have on your dashboard a nice trend graph like the following: and you can access from the left menu the detailed report, including the report by file which is quite interesting: and if you drill down to the duplication, the detail of each duplication per file: Nice, isn’t it? Step 2: Setting up harder metrics – that’s where it hurts! Let’s step to the harder part, getting test results and code coverage metrics in Jenkins. Getting test results is pretty easy thanks to the Xcode Jenkins plugin though, because the plugin will take care to generate the test results XML file in the expected JUnit format. Test results To achieve it, you need to, basically: Add a Xcode Build Step to your job. This one should compile your test target. I recommend compiling in Debug for the simulator because this will be useful when we will want to add code coverage results. In our case, see the step configuration below: enable the build setting ‘Test after build’ in your test target of your project in Xcode. Otherwise your tests will not be launched on the CI server. Notice that this will cause an ugly warning on the computer on which you develop when you run the tests (⌘ + u) Configure Jenkins to publish the report. For this you have to fill the field ‘Publish JUnit test result report’ with the value ‘test-reports/*.xml’ , as shown below: Once done, if you run your build again, you will get a new trend graph on your dashboard: And you will be able to drill down to the individual tests or failures , in case there are some. Alternative: Test results with GHUnit Some of you might be using GHUnit instead of OCUnit (Xcode default). Here is a summary of the differences: you do not need to enable the build setting ‘Test after build’ in your test target of your project in Xcode make sure that you have followed GHUnit documentation on command line usage . You should have added a script file RunTests.sh and a Run script Build Phase in your test target to launch the script. Notice that I didn’t use the latest version of the script since I had an issue when running in Jenkins. I used this older one . For reference, see this discussion on Google Groups you should not add a Xcode Build Step to run the tests but rather a Shell Build Step in your Jenkins job configuration. The shell code to run is the following: GHUNIT_CLI=1 WRITE_JUNIT_XML=YES xcodebuild -target YOUR-TEST-TARGET-NAME -configuration Debug -sdk iphonesimulator clean build This is how it looks in my Jenkins job configuration: The right path to the generated XML file for GHUnit is ‘build/test-results/*.xml’ , as shown below: Test coverage Take a breath now (or a coffee), this is the harder part . There is a few good posts around on how to compute code coverage for iOS, but not always up-to-date The way to do it has changed drastically with the different versions of Xcode. The following has been tested with Xcode 4.3.3. To summarize, you should: enable the two build settings ‘Generate Test Coverage Files’ and ‘Instrument Program Flow’ in the test target of your project in Xcode enable the two build settings ‘Generate Test Coverage Files’ and ‘Instrument Program Flow’ in the main target of your project in Xcode, but only for Debug add code for missing fopen$UNIX2003 and fwrite$UNIX2003 functions. This is more an Xcode bug. I added this code in my AppDelegate.h: #ifdef DEBUG\r\n    FILE *fopen$UNIX2003(const char *filename, const char *mode);\r\n    size_t fwrite$UNIX2003(const void *ptr, size_t size, size_t nitems, FILE *stream);\r\n#endif and this one in my AppDelegate.m: #ifdef DEBUG\r\nFILE *fopen$UNIX2003(const char *filename, const char *mode) {\r\n    return fopen(filename, mode);\r\n}\r\n\r\nsize_t fwrite$UNIX2003(const void *ptr, size_t size, size_t nitems, FILE *stream) {\r\n    return fwrite(ptr, size, nitems, stream);\r\n}\r\n#endif Once done, I recommend to check everything is working locally first . For that you should run your tests and go to: ~/Library/Developer/Xcode/DerivedData/YOUR-PROJECT-NAME/Build/Intermediates/YOUR-TEST-TARGET-NAME.build/Objects-normal/i386' and ~/Library/Developer/Xcode/DerivedData/YOUR-PROJECT-NAME/Build/Intermediates/YOUR-MAIN-TARGET-NAME.build/Objects-normal/i386' Both locations should contain several .gcno files if the project is properly setup and some .gcda files if your tests did generate coverage data. You can check that the coverage is correct by opening one of the files with CoverStory , a GUI for analyzing the coverage data. Once your project properly set up, you have to configure Jenkins to generate this report automatically. This is explained in this blog post . Basically you need to: download GCOVR , a Python script that will be used to convert the coverage files to a Corbertura XML format that is understood by Jenkins. Copy it at a safe place in your server (I put it in my repo in scripts/ directory). add a Shell Build Step to your Jenkins job with the following content: scripts/gcovr -r . --object-directory build/YOUR-PROJECT-NAME.build/Debug-iphonesimulator/YOUR-MAIN-TARGET-NAME.build/Objects-normal/i386 --exclude '.*Tests.*' --exclude '.*ExternalFrameworks.*' --xml > build/coverage.xml This is how it looks in my Jenkins job configuration: install Cobertura plugin for Jenkins (using Jenkins UI) Configure Cobertura plugin to use the right output file. For this you have to fill the field ‘Publish Cobertura Coverage Report’ with ‘build/coverage.xml’ Launch your Jenkins job! It works? Congratulations, but don’t worry if it doesn’t, this part is a bit a makeshift job, it might need some trial-and-error . Once done, you will get a new trend graph on your dashboard as below: and you will be able to drill down to see the coverage of all your different files: and event of the specific lines of your source code: Satisfied? If you try this step, you will notice that you don’t really get the same dashboard as the one shown above, right? In fact there is plenty to say about getting an useful code coverage measurement , and as this article was already long enough, I have detailed this in another blog article with all the tips and tricks that lead me here. Alternative: Test coverage with GHUnit For adding code coverage, the main differences with OCUnit (Xcode standard) are: add the code for fopen$UNIX2003 and fwrite$UNIX2003 functions in the main.m file of your test target, not in the AppDelegate. This is how my main.m file looks like: #import <UIKit/UIKit.h>\r\n\r\nint main(int argc, char *argv[])\r\n{\r\n    @autoreleasepool {\r\n        return UIApplicationMain(argc, argv, nil, @\"GHUnitIOSAppDelegate\");\r\n    }\r\n}\r\n\r\nFILE *fopen$UNIX2003(const char *filename, const char *mode) {\r\n    return fopen(filename, mode);\r\n}\r\n\t \r\nsize_t fwrite$UNIX2003(const void *ptr, size_t size, size_t nitems, FILE *stream) {\r\n    return fwrite(ptr, size, nitems, stream);\r\n} edit your test target .plist and add the property ‘Application does not run in background’ with value ‘YES’ . This is depicted below. Notice that this is useful because the coverage files are only written when the application exits. add a Shell Build Step in your Jenkins job configuration between the launch of your unit tests and the launch of the GCOVR script, with the following content: cp -n build/YOUR-PROJECT-NAME.build/Debug-iphonesimulator/YOUR-MAIN-TARGET-NAME.build/Objects-normal/i386/*.gcno build/YOUR-PROJECT-NAME.build/Debug-iphonesimulator/YOUR-TEST-TARGET-NAME.build/Objects-normal/i386 || true This is how it looks in my Jenkins job configuration: Step 3: Enjoy! Congratulations! But don’t forget that all of this is only useful if you analyze it and track it regularly. This is what the trends are made for, and this is how I use these metrics : I manually mark important builds as ‘Keep forever’ in Jenkins and I enabled ‘Discard Old Builds’ in my Jenkins job configuration. In practice I only keep builds delivered at the end of each Agile iteration , to have a good vision of the coarse-grained variations (trends) This is why all the graphs shown in this article looks so nice and clearly outline trends. I review them at the end of each iteration to add actions to the backlog of the next iteration . Example of such actions: ‘the code duplication has gone red, we will fix all red during the next iteration’ ‘The service layer is no more tested at a 80% test coverage, add tests on XXX and YYY in the next iteration’ etc. Conclusion Honestly this is still a bit long/clumsy to setup , I think for a first installation it will take you about 2-4h . A lot more if you also need to install Jenkins and don’t know Jenkins yet. The good news is that once done for a first project, it’s about 30 min of work for all the future mobile projects you will work on. I think this is acceptable even for very short projects. I have an another good news for those that gave up because of time/pitfalls, we are currently working on a Sonar plugin for Objective-C . This will make all this easier and will bring new metrics. Stay tuned, we will announce it here as soon as we release the first useful version. Last thing: I have put here the full configuration of my Jenkins job in case you want to compare with yours. I have also put a sample configuration for test and coverage for OCUnit and the same sample configuration for GHUnit , because this is the hardest part and I thought it deserves it. And yes, I welcome your feedback if I forgot some steps or you think I should clarify a particular step. Follow @cyrilpicat Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged code duplication , coverage , development , ghunit , iOS , iPhone , Jenkins , lines of code , loc , ocunit , pmd , quality , sonar , unit test . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 22 commentaires sur “iOS dev: How to setup quality metrics on your Jenkins job?” Jean-Christophe 28/08/2012 à 12:08 Very nice post and useful too! For the iOS jobs I manage I put all these small scripts (for PMD, uint tests, libs API documentation generation...) into a second repository. This repo is cloned into a \"scripts-ci\" directory of the workspace and therefore these scripts are shared between all iOS applications and library jobs. Josh Kennedy 28/08/2012 à 17:08 Nice write-up.  You should also checkout the Clang scan-build Jenkins plugin. Jon Reid 28/08/2012 à 17:29 This is very helpful. Another item that may be worth adding to the reporting is cyclomatic complexity, using http://code.google.com/p/headerfile-free-cyclomatic-complexity-analyzer/ Cyril Picat 29/08/2012 à 08:18 @Josh, yeah I have used Clang a few times but since I have switched all my projects to ARC, it's only throwing errors (internal), so I disabled it.\r\n\r\nFrom what I understood Clang is only focused on memory issues right? Doesn't it become irrelevant with ARC?\r\n\r\nBy the way thanks for your nice Objective-C grammar for PMD! Cyril Picat 29/08/2012 à 08:19 @Jean-Christophe, you're right it's better to put these scripts in a common place either in a repository or on the server. Cyril Picat 29/08/2012 à 08:20 @Jon, I didn't know about it, thanks for the pointer!\r\n\r\nI will give it a try and update my post if I manage to integrate it nicely Liam 25/09/2012 à 08:43 Fantastic Article - I'm currently struggling with the Code Coverage metrics for GHUnit however... Thing is (as you quite rightly point out above), the test app needs to be launched and shut down before the coverage files are generated. Unfortunately, the process above seems to run the batch script to create coverage.xml before any launch of the application and I'm left unsure as to how I can run the app via script... Any thoughts would be greatly appreciated!\r\n\r\nLiam Mason 25/09/2012 à 11:21 A great post. Thank you.\r\nHave any body tried to get the coverage data on device instead of simulator? The problem I met is:\r\nIf I change the option of build setting from simulator to device, there is no gnco file generated. Cyril Picat 26/09/2012 à 10:24 @Mason No, I did not try to generate coverage on device, sorry.\r\n\r\nInterested in knowing the solution if you find one. Cyril Picat 26/09/2012 à 10:28 @Liam It looks like you are not running your tests. Did you add the command line for it?\r\n\r\nYou can have a look at https://blog.octo.com/wp-content/uploads/2012/08/configuration-jenkins-ghunit.png for a sample GHUnit configuration, it might help to check you get eveything right. LiohAu 28/09/2012 à 10:57 Thanks for this post it helped a lot.\r\n\r\nNevertheless i'm still getting issues with sloccount, the file generated seems \"correct\", but the jenkins plugin has a lot of failures like : \r\n\r\nERROR: Publisher hudson.plugins.sloccount.SloccountPublisher aborted due to exception\r\njava.lang.NullPointerException\r\n\tat hudson.plugins.sloccount.model.SloccountParser.parseLine(SloccountParser.java:92)\r\n\tat hudson.plugins.sloccount.model.SloccountParser.parse(SloccountParser.java:59)\r\n\tat hudson.plugins.sloccount.model.SloccountParser.parse(SloccountParser.java:50)\r\n\tat hudson.plugins.sloccount.model.SloccountParser.invoke(SloccountParser.java:40)\r\n\tat hudson.plugins.sloccount.model.SloccountParser.invoke(SloccountParser.java:17)\r\n\tat hudson.FilePath$FileCallableWrapper.call(FilePath.java:2099)\r\n\tat hudson.remoting.UserRequest.perform(UserRequest.java:118)\r\n\tat hudson.remoting.UserRequest.perform(UserRequest.java:48)\r\n\tat hudson.remoting.Request$2.run(Request.java:287)\r\n\tat hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:72)\r\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n\tat java.lang.Thread.run(Thread.java:680)\r\n\r\nDo you have a sample file which is working ? i tried to replace the grep -v \"top_dir\" with grep -v 'Have a non-directory at the top, so creating directory top_dir', but does not change anything. Do you have any idea ? Andreas 28/09/2012 à 12:16 @LiohAu, I think there is a bug in the Sloccount plugin for Jenkins, version 1.8. Look at this: https://issues.jenkins-ci.org/browse/JENKINS-14135 Rémy Virin 29/11/2012 à 15:42 Yes, there is a bug with the version 1.8 of the SLOCcount plugin.\r\n\r\nIt has been fixed, but the plugin is not update into Jenkins plugin. You can find the code here : https://github.com/jenkinsci/sloccount-plugin\r\n\r\nOnce you have the code, you must build the plugin using maven (mvn package).\r\n\r\nThen drag & drop the sloccount-plugin.hpi into your jenkins/plugins folder.\r\n\r\nDon't forget to delete the sloccount-plugin.jpi and his related folder.\r\n\r\nThis worked for me.\r\n\r\nRémy. cindy 17/01/2013 à 09:00 very great article, it's really helpful.\r\nbut I got a question, I can't get the code coverage file in the directory \"build/YOUR-PROJECT-NAME.build/Debug-iphonesimulator/YOUR-MAIN-TARGET-NAME.build/Objects\" as mentioned, because there's no project produced in the \"DerivedData/..\" directory. My project can't produce ***.app,I don't know why. (XCode 4.5) Cris 31/01/2013 à 03:18 Thank you for the write up.  I'm using XCode 4.6dp3  and the default  OCUnit for testing\r\n\r\nAfter setting:\r\nTest after build to YES\r\n\r\nI get the error:\r\n\r\nRunPlatformUnitTests:81: warning: Skipping tests; the iPhoneSimulator platform does not currently support application-hosted tests (TEST_HOST set)  \r\n\r\nAre your instructions for GHUnit? Is using GHUnit required? rover 05/02/2013 à 01:20 I keep encountering this error . Set the class path but still didn't help .. How did you do it \r\n\r\nJust one comment: I had trouble to make it work and always get a blank report file first (so as few other if you read the comments).\r\n\r\n\r\n\r\n\r\nGreatBlog to ff10 05/02/2013 à 16:43 I have the problem that the my Jenkins PMD plugin fails with the following error message:\r\n\r\norg.xml.sax.SAXException: Input stream is not a PMD file.\r\n\r\nI checked the file to find a perfectly good XML file with some findings.\r\n\r\nAnyone encountering the same problem? ff10 05/02/2013 à 16:47 @Cris: It seems that you try to run Application Tests. It's quite tricky to get those running with an automated test environment. There used to be hacks but as of Xcode 4.6 the hack to modify apples apptest runscript won't work anymore.\r\nCheck this so thread for another approach: http://stackoverflow.com/questions/12557935/xcode-4-5-command-line-unit-testing rover 05/02/2013 à 18:28 For some reason my post didn't make it through . I keep running into the empty xml Reports running PMD for code duplication. Tried setting the class path but that didn't help either. Any clues ? Niels 20/02/2013 à 11:07 Great article!\r\n\r\nI think I'm \"almost there\" with the code coverage. I can find all the gcno and gcda local in the i386 folder just like expected. I have tried to copy the gcovr script to that folder and if I run it like this \"python gcovr -x > coverage.xml\" I will get an xml file which actually contains things.\r\nBut if I try \"python gcovr -r . -x > coverage.xml\" I get an empty file.\r\n\r\nIf I try to add it as a shell script step on Jenkins I always end up with an empty coverage file.\r\nAny ideas what I might be missing to get the coverage into the coverage.xml file? Renzo Crisóstomo 02/11/2013 à 05:00 I have been using sloccount a lot with Objective-C projects on OSX, never have a problem until recently that I upgraded to OSX 10.9 Mavericks. When I’m trying to run this simple script:\r\n\r\n#!/bin/sh\r\nsloccount --duplicates --wide --details WeatherApp > Build/sloccount.sc\r\n\r\nI’m getting this: \r\n\r\n/Applications/sloccount/compute_sloc_lang: line 52: c_count: command not found\r\nWarning! No 'Total' line in Models/ansic_outfile.dat.\r\n\r\nThe output file has this:\r\n\r\nCreating filelist for Application\r\nCreating filelist for Controllers\r\nCreating filelist for Helpers\r\nCreating filelist for Managers\r\nCreating filelist for Models\r\nCreating filelist for Support\r\nCreating filelist for Views\r\nCategorizing files.\r\nComputing results.\r\n\r\n\r\n44\tobjc\tApplication\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Application/AppDelegate.m\r\n11\tobjc\tApplication\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Application/AppDelegate.h\r\n24\tobjc\tControllers\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Controllers/CitiesViewController.m\r\n10\tobjc\tControllers\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Controllers/CitiesViewController.h\r\n74\tobjc\tHelpers\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Helpers/TranslatorHelper.m\r\n47\tobjc\tHelpers\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Helpers/ValidatorHelper.m\r\n18\tobjc\tHelpers\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Helpers/ErrorNotificationHelper.h\r\n21\tobjc\tHelpers\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Helpers/TranslatorHelper.h\r\n14\tobjc\tHelpers\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Helpers/ValidatorHelper.h\r\n85\tobjc\tManagers\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Managers/WeatherAPIManager.m\r\n20\tobjc\tManagers\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Managers/WeatherAPIManager.h\r\n15\tobjc\tSupport\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Support/main.m\r\n13\tobjc\tSupport\t/Users/ruenzuo/Documents/GitHub/north-american-ironman/WeatherApp/Support/Includes.h\r\n\r\nAnd Sloccount Plugin for Jenkins is unable to parse it.\r\n\r\nAny thoughts on that? Praveen Joshi 01/10/2015 à 10:56 I have been trying the pad setup of the metric and while executing the command:\r\njava -Xmx512m -classpath /Users/udd/PMD/pmd-4.2.5/lib/pmd-4.2.5.jar:/Users/udd/PMD/ObjCLanguage-0.0.6-SNAPSHOT.jar net.sourceforge.pmd.cpd.CPD --minimum-tokens 100 --language ObjectiveC --encoding UTF-8 --format net.sourceforge.pmd.cpd.XMLRenderer --files YOUR-PROJECT-ROOT-DIRECTORY --files YOUR-TEST-ROOT-DIRECTORY > build/cpd-output.xml\r\n\r\nI get the error stating that net.sourceforge.pmd.cpd does not exist\r\nPlease help me with this. I have given the absolute path in my class path variable. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-08-28"},
{"website": "Octo", "title": "\n                Improve Agile adoption with “Shu Ha Ri”            ", "author": ["Mohamed Majdi Hizem"], "link": "https://blog.octo.com/en/improve-agile-adoption-with-shu-ha-ri/", "abstract": "Improve Agile adoption with “Shu Ha Ri” Publication date 03/07/2012 by Mohamed Majdi Hizem Tweet Share 0 +1 LinkedIn 0 During my last two missions, I was involved in coaching a project team for Agile adoption. What struck me about these missions is that although both missions were very similar (same objective, same OCTO team, same team characteristics on the client side) we were able to significantly improve Agile adoption by our client by using “Shu Ha Ri”. This article aims to explain “Shu Ha Ri”, to show how we have applied it and the benefits that it can bring. Presentation of “Shu Ha Ri” Originally, “ Shu Ha Ri ” is a concept describing the different stages of learning martial arts. This concept was applied in the Lean approach at Toyota (“ The Toyota Way to Lean Leadership: Achieving and Sustaining Excellence through Leadership Development”, Jeffrey Liker, Gary L. Convis ). “Shu Ha Ri” consists in three steps that a novice has to follow to acquire a skill or master a technique: Shu : the disciple learns the basics by following the rules laid down by the master Ha : having mastered the fundamentals, the disciple applies the rules but begins questioning them, understanding their subtleties and seeking exceptions to rules Ri : the disciple who has mastered the rules can transcend and adapt them We have successfully used “Shu Ha Ri” to structure our mission to support Agile adoption by the project team. We chose this model because it is simple and has proven itself in the Lean approach at Toyota. In the following sections, we will detail how we did it. Applying “Shu Ha Ri” Before getting to what Shu Ha Ri is and how it works, we will give you some context about the two missions that we will use as examples. The first one was conducted without “Shu Ha Ri” and the second with it. Similarities between the two missions Considered missions are very similar: Same client type: medium size insurer Same project typet: redesign in Web technology of a Mainframe application Same OCTO team (same people!) Same team organization on client side Neutrality of the client’s project teams towards Agile: they were not resistant but awaiting to see what Agile could bring Same mission objective: coach a project team to adopt Agile Same mission duration: 6 months Differences between the two missions The major difference between the two projects was the coaching approach that we adopted Approach without “Shu Ha Ri” In the first project, we agreed with the client to position his team members as decision makers. Each one was accompanied by an OCTO consultant to assist and coach him. For instance Project manager : 1 client team member + 1 OCTO consultant assistant Product Owner : 1 client team member + 1 OCTO consultant assistant Tech Lead : 1 client team member + 1 OCTO consultant assistant In this configuration, it is the client’s team which was responsible for the results of the project from the beginning. Approach with “Shu Ha Ri” In the second project, we managed to convince the customer to proceed incrementally by planning three deliveries of the application with intervals of 3 months. So we agreed with him to proceed as follows: For the first delivery (“Shu” phase), Agile experts (OCTO consultants) are the decision makers of the project (project management, functional trade-offs, technical trade-offs) and Agile trainees (client’s team) shall comply with decisions of experts. Therefore, OCTO team bears the responsibility for the results of this first delivery. Example : being in the business analysis team, in this phase I was responsible of organizing the team, of organizing and moderating business analysis rituals and of the  delivery of user stories, tests and screens For the second delivery (“Ha” phase), the Agile experts transfer the decision making role to Agile trainees. Therefore, Agile experts position themselves as assistants to support trainees in their work and to advise them. Hence, the results of the second delivery are the client’s team responsibility. Example : being in the business analysis team, I created a list of tasks to perform business analysis. Those tasks were distributed to the client’s team members. For my part, I wasn’t responsible of any task. However, I continued to participate in specification tasks, I warned the team when necessary and I acted when someone asked for my help. In the third and final phase (“Ri” phase), the consultants leave the project to let the client’s team manage it. Benefits of “Shu Ha Ri” In the mission without “Shu Ha Ri”, a perverse effect made the Agile Adoption particularly difficult. As they were under pressure for results, Agile trainees (client’s team) had a tendency to cling to models and methods they knew best because it feels reassuring (which is quite natural!). Therefore, whenever we tried to introduce an Agile practice, it was followed by endless discussions where Agile experts (OCTO consultants) had to convince the client’s team of the benefit of such a practice for the project. What makes it complicated is that, for the Agile trainees, the benefit is theoretical in the sense that it is a promise when he needs certainty and immediate results. Therefore, in order to advance, the Agile expert had to seek a compromise by twisting the Agile practice (which is far from being beneficial!). Example : we lost a lot of time and energy to convince the client’s Product Owner and the Tech Lead of benefits of the specification and development in incremental mode (start by simplest functionalities and compexify them progressively). Indeed, they were used to waterfall project logic where everything must be specified and predicted before beginning any development. Moreover, they had difficulty accepting bringing to end-users an application without all the features (including vital and not so necessary ones). In fact, they feared the users’ reaction and it was difficult to reassure them on this aspect. Therefore, the Product Owner tended to create User Stories that were too big, hoping to implement the largest amount of features. Developers were regularly complaining about such User Stories. As for the project with “Shu Ha Ri”, the perverse effect mentioned above was neutralized. Indeed, in the “Shu” phase, Agile experts bear the responsibility of the project success. Therefore, Agile trainees have less pressure and they have time to observe and understand the mechanics and the values ​​of Agile. Moreover, they can see concretely how the Agile practices allow the team to deliver the project and achieve users satisfaction. Example : In our project, after the first delivery, the client’s project manager was surprised that users were satisfied with the result even if we were not able to deliver all what was planned. Indeed, users were happy to be able to use certain critical features after only 3 months and were confident in our ability to deliver new features in the next 3 months. As a result, with this first success, Agile trainees were convinced of the effectiveness of Agile and were more confident to abandon their old practices in favor of those of Agile. This has greatly increased the efficiency of the phase of “Ha”. Indeed, OCTO consultants noticed that Agile trainees were scrupulously applying principles and tools used in “Shu” stage. In addition, the client’s team began to want to push the model to see its limitations and propose process improvements. Obviously, the OCTO team remained present to answer questions and to provide support. Example : the development team began to master their development cycles (iterations) and wondered if it wasn’t more effective to go toward a continuous-flow development model. This question interested the project manager and we decided to study it together to measure the benefits and disadvantages compared to the context and constraints of project. Finally, the phase of the “Ri” will soon take place and everyone is confident in its success. Conclusion Through the two experiences I have described, I was totally convinced by “Shu Ha Ri” effectiveness. Personally I’m going to use it for any situation requiring skills acquisition (being the trainee or the expert). I strongly encourage you to experiment it (even in a private context) to see the benefits. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology , Project support and tagged Agile , Lean , management of evolution , project support . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-07-03"},
{"website": "Octo", "title": "\n                Introduction to large-scale graph processing            ", "author": ["Thomas Vial"], "link": "https://blog.octo.com/en/introduction-to-large-scale-graph-processing/", "abstract": "Introduction to large-scale graph processing Publication date 18/06/2012 by Thomas Vial Tweet Share 0 +1 LinkedIn 0 Graphs are very attractive when it comes to modelling real-world data, because they are intuitive, flexible (more than tables and rows in a RDBMS), and because the theory supporting them has been maturing for centuries. As a consequence, there are several graph databases available, Neo4j being one of the most renowned. The same goes for graph processing, algorithms are numerous and well understood and have immediate applications: single-source shortest path, route finding, loop detection, subgraph matching, … to name a few. Neo4j comes with a small collection of such algorithms built in its graphalgo package. Problems arise when processing very large graphs, when visiting billions of highly connected vertices. In such cases a graph can’t fit on a single machine, and the implementation resorts to a big batch distributed over a cluster of machines. Algorithms typically follow the edges of the graph, so a naïve approach will introduce significant overhead due to machine-to-machine communication (partitioning the graph optimally across the cluster is little more than partitioning the graph without heuristics, a hard problem ). We will give an overview of the use of BSP (Bluk-Synchronous Parallel), an algorithm often used for processing of such large graphs. Then we will explore a list of products and frameworks dedicated to graph processing, either using BSP or based on other approaches. NB: to prevent ambiguities, in the following an element of a graph will be coined a “vertex” or an “edge”, while a “node” will refer to a computation unit in a cluster of machines. The BSP algorithm An answer to problems arising with very large graphs is the Bulk-Synchronous-Parallel (BSP) algorithm. It is not an algorithm derived from graph theory; in its most general form it doesn’t even deal with graphs. Rather, BSP is to be compared to MapReduce: it is a way of distributing the batch processing of a big dataset across a cluster of machines. The difference lies in the flow of data that is exchanged as the processing takes place. MapReduce shuffles batches of independent records, according to a partition scheme reflecting the topology of the cluster, to achieve data and computation colocality. In order to enforce this locality, data is merged and shipped as it enters map or reduces phases, which is costly. Instead, BSP partitions the input data once and for all during its initialization phase, and each node processes its subset of the data (partition) independently by consuming messages from a private inbox: the data does not move from node to node, rather nodes exchange instruction messages that are assumed to be small and thus more efficiently transmitted. This is an incarnation of the Message Passing Interface (MPI) pattern. There is no restriction on the outcome of the processing phase: modification of the local partition’s data, output of a new data set, … and in particular production of new messages, which will be shipped to the appropriate nodes and consumed on the next iteration, and so on. A master node ensures that all nodes have finished processing the current iteration before firing the next one – such iterations are called supersteps . Thus after each superstep, all nodes join a common synchronization barrier . The role of this barrier is to decouple the production and consumption of messages: all nodes pause their activity while the BSP plumbing enqueues messages into each node’s private mailbox, ready for consumption when processing resumes at the next superstep. This way a message produced by node A for node B will not interrupt the latter during its processing activity. Resources are mostly dedicated to computing, and each node can run its processing routine without concern about concurrent access to the data it owns (no locks needed, unless of course the routine itself is multithreaded). As a downside, the barrier may hinder the whole batch if the processing is not equally balanced between nodes: faster nodes will sit idle while waiting for the other ones to complete a superstep. The algorithm ends when no messages are produced during an iteration. BSP for graphs, and Google’s implementation In the description of the BSP algorithm, there is no mention of a graph, no restriction on how input and output data are represented. When the input data set is a graph, though, BSP is a sensible approach. This is the direction Google took in the implementation of Pregel, as discussed in their 2010 paper , which we strongly recommend you to read. Pregel is both a BSP implementation and a graph processing library on top of it. The idea behind Pregel is that many massive graph processing algorithm consists in exploring the graph along its edges . Starting from a fixed set of vertices, one can hop from vertex to vertex and “propagate” the execution of the algorithm, so to speak, across the set of vertices. This reminds of BSP’s way of running an algorithm iteratively by passing messages. In BSP messages are exchanged between computation nodes, in Pregel messages are exchanged between vertices of the graph . Knowing the partitioning function of the graph, it’s easy to know what node hosts each vertex so that each node can dispatch incoming messages at the finest level: So, at a logical level vertices send messages to each other; at the physical level the underlying BSP foundation takes care of grouping those messages together and delivering them to the appropriate nodes of the cluster. After each superstep, the processing covers a bigger portion of the graph. Of course messages are not necessarily sent along edges of the graph, but it is the most common scenario. To our understanding, the default partitioning function in Pregel does not take into account the shape of the graph. It doesn’t try to optimize the partitioning to minimize node-to-node communication by keeping strongly connected vertices together. In Pregel the logical level we mentioned earlier takes the form of a vertex-centric C++ API designed around an abstract Vertex class. Once the graph is loaded into the cluster, all processing occurs in memory. Each vertex exposes an ID that is used for both partitioning and message addressing. Concrete classes also define custom attributes so that each vertex instance bears its own properties. They provide an implementation for the Vertex::Compute() function, that implements the intended algorithm – the function can alter attributes of the vertex , address new messages to other vertices (e.g. by enumerating the outgoing edges of the vertex), or even modify the topology by creating and deleting other vertices or edges. The Compute() function receives, as an argument, the list of messages that were sent to the vertex during the previous superstep. Combiners and aggregators can be used to add a level of abstraction on top of the message infrastructure. Combiners… combine messages addressed to a single vertex into one (applying for example an sum() or max() function), thus reducing message traffic, while aggregators maintain a shared state over the cluster (e.g. statistics). Both combiners and aggregators are accessible from a vertex’s Compute() function. We mentioned that the graph itself can be modified during processing. That is, vertices or edges can be added or deleted. The modifications are applied between supersteps, at the same point where messages are exchanged, so that the user code executing a given superstep doesn’t need to protect itself for modifications in the middle of its execution. Automatic and user-provided strategies apply when several pending modifications are in conflict (e.g. adding an outgoing edge to a vertex being deleted). What about fault tolerance? The system needs to be able to recover from a node crashing. In Pregel, according to the paper at least, fault tolerance is not enforced by processing each partition redundantly on several nodes; rather, each node regularly performs a checkpoint by flushing its working set to disk. The coordinator node then has all remaining nodes resume processing at a point in time compatible with the complete set of checkpoints saved by each one. The Pregel inspiration Just as with MapReduce again, several teams took inspiration from the Pregel paper and started to write their own implementations, or at a minimum refer to Pregel as a basis for comparison with their own approach. And, of course, Pregel is not available to the public. Here is a quick survey of the most visible alternatives. Apache HAMA HAMA is one of the most well-known Pregel “clones”. According to Edward J.Yoon, the project leader, HAMA is about to end its incubating period and become a top-level Apache project . Actually it’s not a Pregel clone; HAMA started in 2008, 2 years before the Pregel paper was issued. It is a BSP implementation over Hadoop, with a small graph processing framework atop that is inspired by Pregel. Vertex data comes in and out of Hama through HDFS APIs, in the form of adjacency lists stored in text files: each line begins with a vertex ID, followed by the list of IDs of vertices connected to the former by its outgoing edges. The BSP nodes and jobs management, including fault tolerance, and node-to-node communication (messages) directly leverage the facilities offered by Hadoop. The synchronization barrier is enforced using Zookeeper. The upcoming 0.5.0 release will rely on Hadoop 1.0 , but the set of functionalities that will be added is not detailed. All we can say from the source code for 0.5.0-RC2 , is that there doesn’t seem to be any built-in connector (for HBase or Neo4j for example), and that the Pregel-like API will not be 100% complete (no aggregators or explicit support for topology mutations). Still, this will have to be confirmed when 0.5.0 is released. Apache Giraph Another incubating Apache project, Giraph (started in 2010), directly took the Pregel path. It features a single release, 0.1.0 (Feb 2012), based on Hadoop 0.20. The PageRankBenchmark example included runs fine on Hadoop 1.0.2, provided the project is recompiled against the appropriate version of the libraries. The Giraph API, as far as one can tell, is closer to Pregel’s and more complete in this respect, since the project was explicitly started as a clone. Regardless, Hama and Giraph are quite close to each other in their approach – Hama is more mature with several releases, but Giraph has active contributions from Yahoo!, Facebook, Linked and Twitter employees, so watch out. In terms of technologies, both products rely on the same Hadoop components to manage a cluster of graph processing nodes (Hadoop and Zookeeper for job management and synchronization, RPC for message delivery, HDFS APIs for data input and output). According to this presentation , Giraph mimics Pregel’s strategy of issuing checkpoints for fault tolerance, but how exactly is not clear since the documentation is still a bit scarce… Phoebus This project is mentioned here because it’s also stated as a “Pregel implementation”. Phoebus is written in Erlang. As what seems to be a single developer’s hobby project, its future is uncertain – though commits are still being brought to the code base. It’s certainly not ready today for production use, because the README file has been announcing fixes for fault tolerance since day 1. GoldenOrb GoldenOrb is yet another Pregel clone, or used to be. As a matter of fact, a single version – 0.1.1 – was released in Aug 2011, and the source code has been showing no activity ever since . Besides, it was supported by Ravel Data, a company that is probably out of business (its website is down). Other graph processing platforms The projects listed below are not Pregel clones or BSP implementations, and advertise themselves as graph processing platforms among other things. Still, with the exception of Haloop, they have one thing in common: like BSP, they rely on message passing between vertices in order to propagate the processing along the graph. Often they try to compare themselves to Pregel. Another common feature between these projects is their academic nature. HipG HipG is founded by VU University in Amsterdam. The 1.5 and 2.2 versions, released in early 2011, can be downloaded directly from the website (source code is available but there is no public source control). HipG is a Java-based distributed processing framework, relying on a message-passing framework from Ibis . HipG features a graph API quite close to Pregel’s, but the core algorithm is finer-grained than BSP, as there is no single synchronization barrier and thus no global supersteps. Of course, by defining a single barrier one can simulate BSP, but other algorithms can be made more efficient with a hierarchy of barriers conducting a tree of dependent processing tasks (see the HipG paper , section 2.3). In contrast, with BSP several nodes can sit idle waiting for other nodes to complete the current superstep, even if they are dealing with a completely independent portion of the dataset. Although HipG is distributed in nature, fault tolerance is not implemented yet, as stated in the conclusion of the paper. Signal/Collect Signal/Collect is another academic project, founded by the University of Zurich and Hasler Stiftung (Swiss foundation for the promotion of IT). As BSP and Pregel, it uses the MPI paradigm to process the graph, and supports both synchronous (= BSP) and asynchronous (= actors) models. In contrast with other solutions that are strongly vertex-centric (the Pregel influence, again?), the Signal/Collect API gives vertices and edges the same importance. Signal/Collect is written in Scala. The latest release, 1.1.2 (Mar 2012), doesn’t support distributed processing, but this is announced for the 2.0 version due in May 2012. Distribution will be implemented with Akka, so 2.0 will probably be a major rewrite of the project. Trinity Trinity is a Microsoft Research project, written in .NET of course. As such, it is not publicly available (yet?), but we can infer some characteristics from the Trinity paper published by Microsoft. The first obvious differences with Pregel and other processing frameworks, is that Trinity is a processing engine and a graph database (itself built on top of a key/value “memory cloud”, as they coin it). The approach is not BSP, but relies on vertex-to-vertex message passing without the cost of a global synchronization barrier. Trinity is meant to address both offline analytics and online querying – with the perfomances claimed, such online querying doesn’t limit itself to CRUD access to the database, but also covers heftier processing such as searching 3-hop friends in a social network. In order to achieve this performance, several techniques are applied. First, as a necessary condition, all processing occurs in memory. The set of vertices necessary for a given computation stage is kept compact, to keep the number of computation nodes low and save on bandwidth (the paper claims “dozens” of servers, as opposed to the “hundreds” or “thousands” of Pregel and such, but the discussion lacks argumentation since the actual uses of Pregel at Google are not known). Another interesting factor concerns the partitioning of the graph. In addition to the traditional partitioning across the cluster, Trinity makes the sensible assumption that most messages will be sent along the edges of the graph. This heuristic is used to perform a second, orthogonal partitioning, reflecting the vertices and the nodes that are most likely to communicate together. The combination of the two schemes is called bipartite partitioning . It eliminates the need of BSP’s global synchronization barrier, because Trinity can tell when a vertex has received all its messages in the current iteration, without waiting for all nodes to join a common barrier. The heuristic raises an interesting question, unanswered in the paper: can the processing algorithm modify the graph on the fly? Modification of vertex attributes is permitted, but I doubt the topology can be modified as it directly impacts vertex placement and the optimization of message delivery. Regarding the API, it is quite different than Pregel’s. Vertices do not expose a single Compute() method, rather each vertex go through a pipeline of independent processing stages. “Cell parsers” take care of transforming vertices between stages, eliminating unnecessary local variables to keep the graph compact in memory. Haloop The project advertises itself as a prototype on its front page, so don’t expect production-ready code. It is supported by the National Science Foundation. Haloop takes a different approach than the other frameworks. It is not a BSP implementation, nor is it built on top of an MPI core: Haloop enhances Hadoop by allowing iterative jobs (loops). Of course, BSP can be approximated by writing a big outer loop called superstep, but the flow of data would still be that of Hadoop. Haloop is actually a fork of Hadoop 0.20.x. There is no official release, but the source code is publicly available under an Apache 2.0 license. Product comparison Provided your use case deals with a large graph, or your problem can be modelled as a graph before being stuffed into a processing service, the following table should help you decide which products should be considered as eligible for your needs. Pregel is identified by almost all projects as the reference for distributed processing of large graphs, so we tried to show how much of the Pregel API is proposed by the various solutions. Project Type Algo / Approach Pregel-like API 1 Fault tolerance Stack Recent activity License V A C M Hama 2 Incubation 3 BSP Y N Y N Y Hadoop 1.0 Active Apache Giraph Incubation BSP 4 Y Y Y Y Y Hadoop 0.20 5 Active Apache Pheobus Hobby BSP 4 Y Y Y Y N Erlang Active Apache GoldenOrb Sponsored BSP 4 Y N N N ? Hadoop 0.20 Dead? Apache HipG Academic MPI Y Y 6 N Y 7 N Ibis MPJ None GPL Signal/Collect Academic MPI Y Y N? Y N/A 8 Proprietary Scala 8 Active Apache Trinity Academic MPI Y Y 6 N N? Y Proprietary .NET ? Not publicly available Haloop Prototype MapReduce N N N N Y 9 Hadoop 0.20 (fork) Active Apache 1. V=vertex object model, A=aggregators, C=message combiners, M=explicit support for topology mutations 2. 0.5.0 is not released yet; the information comes from the 0.4.0 documentation and the source code for 0.5.0-RC2 3. Undergoing transition to Apache Top-Level Project 4. inspired by Pregel 5. the project compiles and the example runs with Hadoop 1.0.2 6. Through operations called “reducers” 7. “On the fly” graphs 8. version 2.0 will be based on Akka 2.0, to enable distribution of the computation 9. Assuming that Hadoop’s native HA capabilities were brought forward into Haloop 10. Hadoop fork Stricly speaking, none of those solutions have known references in production , and none of them comes with a significant collection of algorithms from the graph theory. Apache Hama and Giraph look more promising because of their open-source nature, their activity and their proven technical foundation – i.e. Hadoop. Between the two Hama is more mature, and more flexible (not purely graph-oriented), but Giraph has contributions from people working at big Internet companies (Yahoo!, Facebook, Twitter, Linkedin) that have always been strong supporters of the open-source ecosystem. Conclusion Our next step, at OCTO, will be the implementation of a real use case with (probably) one of the solutions that were presented in this article. This will be the opportunity to look deeper into the products, to challenge their maturity, their robustness and HA capabilities! And of course, if other promising graph processing platforms come to our knowledge, we’ll write about them. Further reading In addition to the project home pages linked in the article, here are some reading suggestions: OCTO’s introduction to graph modelling and visualization with Neo4j and Gephi (in French) The Pregel paper , a must read to get a detailed description of Googles’ incarnation of BSP A presentation and an blog article on Pregel Another article about the so-called “ Pregel clones ” (some credits for the present article go to C.Martella, as his article gave me names of solutions I had not heard before) Edward J.Yoon’s blog with frequent posts about Hama (and more) Various discussions about MapReduce and graphs: here , here and here Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 6 commentaires sur “Introduction to large-scale graph processing” Dam 18/06/2012 à 15:08 And what about \"Titan\" just released last week : https://github.com/thinkaurelius/titan as Apache 2.0 licence.\r\n\r\nproviding Rexster interface and using Cassandra or Hbase as backend.\r\n\r\nHere is quite good presentation of Titan and its future echosystem :\r\n http://www.slideshare.net/slidarko/titan-the-rise-of-big-graph-data Thomas Vial 18/06/2012 à 15:34 Hi Dam,\r\n\r\nWe have Titan in our backlog; it came out too recently for us to review it before issuing the article.\r\n\r\nThanks for noticing!\r\nThomas Dam 18/06/2012 à 16:04 Can't wait for your reviewing of Titan :) Thomas Jungblut 19/06/2012 à 05:08 Hi Thomas,\r\n\r\nvery cool summarization.\r\n\r\nI'd like to clarify a bit regarding Hama:\r\nWe have worked very well on the graph module the last weeks, forming a full and complete package now. \r\nWe support aggregators and if you sneak a bit deeper into the BSP core functionality you can also do some topology mutations.\r\n\r\nOur 0.5.0 release features:\r\n-Pluggable RPC managers (you can implement various RPC formats, Hama now ships with Avro using ProtoBuf as default)\r\n-Message compression using Snappy and Hadoop codecs\r\n-New implementation of Bipartite graph matching example\r\n\r\nand several other performance improvements.\r\n\r\nWould be cool if you could extend your post accordingly ;)\r\nIf you have additional questions, feel free to contact me directly or come to our mailing list.\r\n\r\nRegards,\r\nThomas Thomas Vial 19/06/2012 à 08:19 Hi Thomas,\r\n\r\nThanks for the insight! Do you know when Hama 0.5 will be released?\r\nBetween Titan and Hama, we'll have plenty of matter for a follow-up article :)\r\n\r\nThomas Eric 10/07/2012 à 21:36 This is a cool post on graph db methodology.  I was hoping you'd be interested in having it republished on DZone.com's big data portal - I'm sure our readers would be interested. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-06-18"},
{"website": "Octo", "title": "\n                The Esper CEP ecosystem            ", "author": ["Thomas Vial"], "link": "https://blog.octo.com/en/the-esper-cep-ecosystem/", "abstract": "The Esper CEP ecosystem Publication date 04/05/2012 by Thomas Vial Tweet Share 0 +1 LinkedIn 0 Mathieu’s introduction to Complex Event Processing (CEP) has announced a series of articles on various CEP solutions. We begin this series with a post about Esper. Esper, maintained by EsperTech , is a Java platform dedicated to complex event processing and event stream processing (ESP), that is, a collection of frameworks and tools that can be combined to build event-oriented applications and integrate them together. Most of the foundation of such applications is brought by the 3 different Esper packages. These are advertised as “editions”, but are rather complementary building blocks. Building an event-oriented application with Esper involves: coding the main applicative logic with event processing statements, using the core algorithmic engine Esper Event Stream and Complex Event Processing (Esper Engine for short). It is distributed as an open-source project with a GPLv2 license packaging, integrating and deploying the application. A good candidate for these tasks is the dedicated Esper Enterprise Edition (EsperEE) optionally securing the event processing logic with EsperHA, which brings persistence capabilities to Esper, thus enabling high-availability and recovery scenarios This post thus offers a global vision of the Esper ecosystem as a CEP platform. The article concerns version 4.5.0 of the platform, but at the time of this writing the latest released version is 4.6.0. NB : the documentation for EsperEE and EsperHA can be found in the download packages from EsperTech’s website. The Esper engine This is the common ground of the 3 editions and where the core event processing logic is implemented. It comes in two flavours: Esper for Java and NEsper for .NET. We will concentrate on the Java version for the rest of this post, because it is the most mature and because the other editions, EsperEE and EsperHA, are only available in a Java environment. The engine is a JAR, and instantiated by the means of a Java API and an XML configuration file. Thus, used alone, the Esper engine is embedded inside a Java or JEE application, along with business logic, GUI, integration and other frameworks. If we recall Mathieu’s classification of event processing approaches, Esper implements the first paradigm: it processes incoming events by the means of statements executed against event streams . In this case, an event is a plain Java object (POJO), and a stream is a pipe of events of a given type (class). Streams are used to feed events into the engine, and to “connect” the output of a statement to the input of another, thus allowing statement composition (chaining). Here streams have no physical existence but inside the JVM where the engine is running. Statements are a central notion in Esper. They are continuous queries, registered with the engine, which continuously monitor the streams they depend on. They are written in EPL, a SQL-like language designed for querying data streams, with similar constructs like projection, selection, aggregation functions, joins (correlations), named windows (akin to SQL views). It also provides language constructs for more elaborate and event oriented features such as pattern matching and sliding windows, to account for the temporal nature of event streams. -- Detects products pertaining to abandoned carts\r\ninsert into AbandonEvent (productId)\r\nselect cart.productId\r\nfrom pattern [\r\n   every cart=AddToCartEvent\r\n      -> (timer:interval(10 min) and not CheckoutEvent(cartId = cart.cartId))\r\n];\r\n\r\n-- Gives statistics on abandoned products (moving aggregate over a sliding window)\r\ninsert into AbandonStats\r\nselect\r\n   productId,\r\n   count(*) as abandons\r\nfrom AbandonEvent.win:time(10 min)\r\ngroup by productId; Unlike classical SQL queries, statements are triggered by incoming events, evaluated against them, and forward matching events to their associated listeners – or to another statement in the case of chaining. Statements can also correlate event streams with a relational database, by the means of a regular SQL query joined with other streams. This can be used when real-time events are to be correlated with historical data, or enriched with lookups against reference tables stored in a database. So Esper’s main job is matching events against patterns, and emitting high-level (complex) events. Being able to explain how bigger events are produced is essential. This requires keeping a detailed history of each triggering, at every stage, so that output events can be drilled down to the finest ones. Esper has auditing facilities at the statement level, but they are designed for debugging purposes by logging statement activity. There is, as of yet, no such thing as a native event repository and history, but this is planned for the next major release (5.0). Beside these core functionalities, the capabilities of the Esper engine can be extended by means of plugins, which are deployed as separate JARs alongside the main engine, and activated through configuration. Many EsperHA and EsperEE functionalities (DDS, JMX, …) are shipped as plugins. With the community edition of the engine, all processing state (sliding window contents, event flows, current values for aggregated expressions) is stored in memory, as are statement definition and statement-listener bindings. All these are lost in case of a failure. Persisting all these elements is possible with the HA edition, described below. EsperHA The HA plugin is responsible for storing the internal state of the engine, for recovery purposes. This is enabled on a statement-by-statement basis, through annotations appearing before the actual EPL code, and with a bit of XML configuration. When a failed engine is brought up again, or when a backup engine is started, the saved state is restored automatically – though with some limitations regarding event size or statement complexity. The state is not saved continuously during normal operations: checkpoints are issued at regular (configurable) intervals measured in seconds or number of events, so a complete recovery cannot be guaranteed unless the input feed can detect and replay incompletely processed events. More precisely, there are 4 persistence profiles to choose among, to fine tune the persistence needs of a particular statement: Piece of state @Transient @Durable @Resilient @Overflow Events needed by windows & patterns X X EPL definition of the statement X X X Statement state X Statement-to-listener bindings and listener state(*) X (*) this is the serialized contents of the Java listener object at the time of the binding, not at the time of the crash. Generally speaking listeners should be stateless, and delegate the persistence of data to another component (service writing to a database, messaging system for the publication of a derived event, …) @Transient is the default profile, with no persistence whatsoever, and the only one available from the Esper community edition @Durable tells the engine to automatically restart statements, without having to register and bind them again @Resilient is the most complete profile, because almost everything is stored, including statement state (e.g. the current value of an average over a sliding window, whose value would be incorrect after a @Durable or @Overflow statement is restarted) @Overflow is used to flush events that partake in state to disk, so the space they occupy on the heap can be reclaimed when memory is scarce The type of persistence store is configurable, and can be either of BerkeleyDB, Cassandra (beta) or JDBC with support for Oracle, MySQL and Microsoft SQL Server. Note that persisting events adds a serialization overhead and might impact performance. You will notice that this is all about recovery , i.e. being able to catch up after a crash with losing as little data as possible. What about clustering and load balancing? The engine and the APIs give very little support for these needs. The only provision is for switching between a primary and a backup EsperHA engines, in a shared-data situation where the store is a BerkeleyDB database: All other situations (load balancing, share-nothing, global persistence across CEP engines and messaging system) must be handled outside of the engine with message routing patterns, possibly with the help of the monitoring facilities offered by EsperEE: EsperEE EsperEE is not a plugin in itself, like EsperHA. It is rather a collection of plugins and interactive applications, aiming at managing and integrating Esper engines together EsperHQ EsperHQ is a versatile application, combining functionalities of different kinds of tools: A runtime container for Esper engines and for CEP applications packaged as WAR archives. This makes it an alternative to traditional platforms like JEE or servlet containers A management console for hosted or remote Esper engines, using EsperDDS described below A basic IDE for EPL statements and eventlets A frontend to local CEP engines , to issue on the spot EPL queries or launch event-driven continuous displays Regarding statement creation, EsperHQ provides a crude but comprehensive EPL designer. There is no graphical flow language such as can be found in a fully integrated CEP platform like StreamBase, but it eases the task of writing complex EPL code. EsperHQ also has a wizard for creating eventlets , i.e. graphical Flex components subscribing to events produced by a contained engine, and updating their display accordingly. Such eventlets are kept in a local repository, and can be either packaged and exported, or reused in mashups along with static content, for example to create dashboards. The push technology underneath is Adobe’s BlazeDS. Miscellaneous components EsperHQ makes a fairly big step towards an integrated CEP platform. On the other hand, the components described below are lightweight plugins or tools that augment Esper with ancillary functionalities. EsperIO is a collection of adapters (Java classes) that help exchange events between Esper and the outside world, or from an engine to another. They give support for well-known protocols and formats like JMS, CSV, HTTP. More specific needs like social networks, trading platforms or other brokers (e.g. AMQP) must be addressed with custom connectors built on top of the included SDK. EsperJDBC allows one to query Esper statements and named windows from a Java application in the form of traditional JDBC calls. Its primary use is the integration with reporting tools. EsperJMX exposes 6 MBeans that give some control over an engine: mostly configuration, statement management and metrics, on the spot querying of a statement or a named window. EsperDDS does very much the same job as EsperJMX, but uses dedicated APIs that rely on JMS channels to expose the management services and to push the results to the client. The resources to manage are addressed through predicates operating on their metadata (e.g. statement name or annotations). Note that EsperDDS is not (yet?) capable of establishing a subscription to events produced by a remote statement – it deals with engine management. Conclusion The three “editions” described herein, Esper Engine , EsperEE and EsperHA , are building blocks of an almost-complete CEP/ESP platform. In our opinion, to compete with commercial solutions, Esper is still missing: more user-friendly interfaces – that is, a graphical language as an alternative to EPL authoring enhanced simulation and backtesting features – they are expected in the upcoming 5.0 version, with the announced event capture and replay features true HA functionalities – native load balancing and cluster management – which still require custom development and architectural design, including careful integration with an external messaging system between checkpoints EsperHQ mitigates the complexity of EPL by letting one graphically chain smaller statements together inside an engine. Cross-engine statement chaining would be a real plus as it would allow one to graphically compose applications by routing events between them through “gateway” statements. In the meantime, this can be done in CEP applications, either manually or with EsperIO input/output façades. Still, at this time Esper is the leading general-purpose low-cost CEP solution, especially with its open-source engine. But another emerging solution, Microsoft StreamInsight (.NET), is evolving rapidly, and leverages LINQ, a framework now well known to developers, to query event streams. Although the roadmap of StreamInsight is unclear, it may be a matter of time before it is mature enough to compete with Esper on its own grounds. Are there any aspects of the Esper suite that you want covered in greater detail? Let us know by leaving a comment with ideas for a next article! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged CEP , Esper , events . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “The Esper CEP ecosystem” Nirmalya Sengupta (@baatchitweet) 05/05/2012 à 02:11 A very concise collection of features and capabilities of Esper! Kudos!\r\n\r\nA follow-up article on the APIs which one can use to create home-grown, riff-raff replacement of enterprise-grade components, may be useful to many. Thomas 08/05/2012 à 21:37 Hi Nirmalya,\r\n\r\nThanks for your feedback. There are lots and lots of extension points in Esper, different APIs that allow you to integrate the engine with other components or services and build the platform of your dreams. \r\nAn example I have in mind is the concept of \"virtual data window\". It could be a good candidate for building an event store; that's an interesting use case I'd like to explore in greater detail.\r\n\r\nAnother example. I've just noticed that 4.6.0 introduces data flows, which are a \"pipe and filter\" approach for event processing. Interestingly it's a solution to what I coined \"[the lack of] a graphical language as an alternative to EPL authoring\" in the conclusion. Granted, it's still EPL, but it's a new DSL that could be easily generated from a graphical tool à la StreamBase (drag & drop of processing units connected together by small streams). It's a high-level alternative to statement composition through INSERT INTO clauses, more powerful too, with the notion of event sinks and sources.\r\n\r\nQuoting the docs (http://esper.codehaus.org/esper-4.6.0/doc/reference/en-US/html/dataflow.html):\r\n\r\n\"Data flows in Esper EPL have the following purposes:\r\n* Support for data flow programming and flow-based programming.\r\n* Declarative and runtime manageable integration of Esper input and output adapters that may be provided by EsperIO or by an application.\r\n* Remove the need to use an event bus achieving dataflow-only visibility of events and event types for performance gains.\" Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-05-04"},
{"website": "Octo", "title": "\n                HTTP caching with Nginx and Memcached            ", "author": ["Bertrand Paquet"], "link": "https://blog.octo.com/en/http-caching-with-nginx-and-memcached/", "abstract": "HTTP caching with Nginx and Memcached Publication date 04/05/2012 by Bertrand Paquet Tweet Share 0 +1 LinkedIn 0 Deploying an HTTP cache in front of web servers is a good way to improve performances. This post has two goals : present the basics of HTTP Caching present the new features I have implemented in the Memcached Nginx module to simplify HTTP caching Cache? What is it ? Caches are used to improve performances when accessing to a resource in two ways: it reduces the access time to the resource, by copying it closer to the user. Some examples: the L2 cache of a microprocessor, a database cache, a browser cache, a CDN , … it increases the resource building speed, by reducing the number of accesses. For example, instead of building the homepage of your blog at each request, you can store it in a cache. This scenario will be the topic of this post. The HTTP cache Architecture The HTTP cache is usually placed in front of your web server. The architecture look like that : For a given URL, the HTTP cache asks the page to the web server the first time it is queried and store it for the following requests, which allows to reduce the server load and increase the site speed: the page is no longer dynamically rendered, but just taken out of the cache. There are lot of HTTP cache implementations. The best known are : Varnish Squid Nginx with cache on local file system Apache2 with cache on local file system How it’s work? The HTTP cache relies on the HTTP cache headers, used by the client browser, to work out how long the copy of a page can be kept. For example, if your web server asks for a given URL with an HTTP header containing: Expires: Thu, 31 Dec 2037 23:55:55 GMT , it knows this resource will not be modified before the 31 December 2037, which allows this resource to be queried only once, and stored until its expiration date. In addition to Expires , some others HTTP headers are involved in cache mechanisms, like Cache-Control and Vary . Please consult the RFC 2616 for more details. Unfortunately, caching is a bit more complex as different browsers/reverse proxy/HTTP caches do not manage HTTP headers exactly in the same way. The simplified HTTP cache architecture is : Note that the HTTP cache store not only the URL data (ie. the web page content), but also the HTTP headers, expiration times and other meta-data. Some implementations allow us to configure caching without using HTTP headers, if you can not or do not want to modify your application. For example, see the varnish VCL . However, I invite you to use HTTP headers that are standard and more simple to deploy (it is all in the application). In the above implementations, Varnish and Squid are daemons, running independently of the web server. Both tools make caching in memory or disk. Apache2 and Nginx will be able to do HTTP caching, directly in the web server process. The performance difference is not huge (at least for Nginx). However, having separated cache processes have other benefits: it allows sharing the cache across multiple applications, or to use other features like load balancing or routing of some URLs on specific servers. Limitations This cache system works well. But some use cases are difficult to implement. An example: my website displays a ranking computed every 3 minutes by an external daemon. This daemon stored the ranking in a web page, which is loaded in Ajax. To implement that, the simplest solution is to write a file on disk which will be served by the web server, as a static page. One problem is that the file contains only data: no meta data, such as the expiration time of the resource. We would prefer to place directly our ranking in the HTTP cache, and specifying the expiration time. Such a solution is actually quite difficult to implement, as the ranking is updated by an external daemon and the cache is filled by the web server. For this feature, we will use an architecture like : In this architecture, for each incoming HTTP request (or only for a specific URL pattern), the HTTP reverse proxy asks the storage if it has data for this URL. If so, the HTTP reverse proxy sends data back to the client. If not, the HTTP reverse proxy sends the request to the web server backend. In this architecture, the storage is not filled by the HTTP reverse proxy but, for instance, by another server which executes background business process (like the back office server) or by the web server, which fills the storage in addition to render HTML pages, while processing some specific requests. This architecture can been seen as an “exploded” HTTP cache: the reverse proxy part and the storage are split, and the automatic storage filling by HTTP headers analyze is removed. This architecture is only relevant if the cost of resource generation exceeds the cost needed to retrieve it from storage. This is often the case: a query to a storage like Memcached is much faster than any database call. And you can design your website to have only some URLs in cache: /cache for example, to avoid unnecessary calls to the cache. This feature is today very important: your website speed impacts directly your business. “Amazon found every 100ms of latency cost them 1% in sales”. That’s why you have to speed up your website, for instance, by doing some long processing in background like sending an email, calculating a ranking, rendering a PDF invoice … The asynchronous job mechanism is now integrated in some web frameworks, like Rails and its delayed jobs . Nginx / memcached module The Nginx / Memcached module allows you to setup the previous architecture, using Nginx as a HTTP reverse proxy, and Memcached as storage. (Note: memcached is often used as shared HTTP session storage) The module Nginx memcached works very well. But it has a big limitation: it can not store HTTP headers with data. For example, pages served by Nginx via Memcached storage have the default Nginx Content-Type. Moreover, it is quite difficult to store multiple type of data in Memcached : CSS, JS, images, HTML, json … You can add some specifics HTTP headers, but only in Nginx configuration. These headers will be shared by every resources served by Nginx/Memached, unless you put lot of ugly “if” in the configuration. Note that the Nginx / Redis module has the same problem. That is why, I have modified the Nginx / Memcached module to store HTTP headers directly in Memached. To use it, you only have to insert in Memcached something like : EXTRACT_HEADERS\r\nContent-Type: text/xml\r\n\r\n<toto></toto> Nginx will send back a page which contains just <toto></toto> , but with and HTTP header Content-Type: text/xml . While modifying the Nginx / memcached module, I also add some extras features: Honor If-Modified-Since HTTP header, by replying 304 Not modified if resource on Memcached contains an appropriate Last-Modified header. Hash keys used in Memcached, to get around the Memcached hash max key size (250 chars) Memcached insertion via Nginx , on a specific URL. It allows you to use the cache with a simple HTTP client, instead of a Memcached client. which is useful to share data accross multiple servers : the same HTTP load balancers can be used for reading and writing into the cache. Memcached purge via Nginx , on a specific URL, which, as above, allows purging Memcached with a simple HTTP client,  like curl . Partial Memcached purge using Memcached’s namespaces . Useful when you cache data for multiple domains, and when you have to purge only one. Here is an example of Nginx configuration, in which Nginx watches if there is data in Memcached for every URL, before sending requests to backends web servers : location / {\r\n    error_page 404 = @fallback;\r\n\r\n    if ($http_pragma ~* \"no-cache\") {\r\n       return 404;\r\n    }\r\n    if ($http_cache_control ~* \"no-cache\") {\r\n       return 404;\r\n    }\r\n    set $enhanced_memcached_key \"$request_uri\";\r\n    set $enhanced_memcached_key_namespace \"$host\";\r\n    enhanced_memcached_hash_keys_with_md5 on;\r\n    enhanced_memcached_pass memcached_upstream;\r\n  }\r\n\r\n  location @fallback {\r\n    proxy_pass http://backend_upstream;\r\n  } This module is open source and is available on github . It is used in production at fasterize , with all features, and works very well. I hope this module will be usefull for you ! Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Infrastructure and Operations . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 11 commentaires sur “HTTP caching with Nginx and Memcached” Tung GNU 26/07/2012 à 16:11 Hello, your module is what I'm finding (memcached with HTTP header). I will study it to implement on my site in a near future. Thanks. Vid Luther 30/09/2012 à 10:52 Hi,\r\n We use batcache here with WordPress. I wanted to see if I'm understanding the concept correctly.\r\n\r\nAs long as batcache writes to the same memcached server, with the same key, subsequent requests to nginx will not even hit the PHP side of WordPress right? devin 10/12/2012 à 02:26 Hello,\r\n\r\nEverything is installed correctly and I'm not getting any errors. I'm running nginx as a reverse proxy for apache. I'm checking out the live stats of memcached with this admin tool:\r\n\r\nphpMemcachedAdmin\r\nhttp://code.google.com/p/phpmemcacheadmin/\r\n\r\nand the problem is that no keys/data are being written to memcached. Is there another process involved with writing keys or reading keys from memcached that I am missing?\r\n\r\nI will include the server section of my nginx.conf file, in case you see anything wrong?\r\n\r\nThanks,\r\nDevin\r\n\r\n   #unless specified, activate the caching\r\n    set $do_not_cache 0;\r\n\r\n    location @fallback {\r\n    proxy_pass http://wordpressapache;\r\n    }\r\n\r\n    location / {\r\n                    # If logged in, don't cache.\r\n                    if ($http_cookie ~* \"comment_author_|wordpress_(?!test_cookie)|wp-postpass_\" ) {\r\n                            set $do_not_cache 1;\r\n                    }\r\n\r\n                    ### enhanced memcached code begin\r\n                    error_page 404 = @fallback;\r\n                    if ($http_pragma ~* \"no-cache\") {\r\n                    return 404;\r\n                    }\r\n                    if ($http_cache_control ~* \"no-cache\") {\r\n                    return 404;\r\n                    }\r\n                    set $enhanced_memcached_key \"$request_uri\";\r\n                    set $enhanced_memcached_key_namespace \"$host\";\r\n                    enhanced_memcached_allow_put on;\r\n                    # set expires to 120 seconds\r\n                    set $enhanced_memcached_expire 120;\r\n                    enhanced_memcached_hash_keys_with_md5 on;\r\n                    enhanced_memcached_pass 127.0.0.1:11211;\r\n                    ### enhanced memcached code end\r\n\r\n                    proxy_cache_key \"$scheme://$host$request_uri $do_not_cache\";\r\n                    proxy_cache_bypass $do_not_cache;\r\n                    proxy_no_cache $do_not_cache;\r\n                    proxy_cache lasercache1;\r\n                    proxy_pass http://wordpressapache;\r\n    } Kien 12/02/2013 à 17:39 Hi,\r\n\r\nI'm using this modules however i did not get it works correctly.\r\n\r\nI'm using python script to put data in memcache\r\nmc.set(\"my_key\", \"EXTRACT_HEADERS\\nContent-Type: text/html\\r\\nThis is heading page\")\r\n\r\nIn nginx config.\r\n\r\nlocation / {\r\n\tset $enhanced_memcached_key  \"my_key\";\r\n\tenhanced_memcached_pass  127.0.0.1:11211;\t\t\r\n\terror_page         404 @fallback;\r\n}\r\n\r\nWhen I go to any link, the Browser let me downloaing a docu\r\nment with the content:\r\n\r\nEXTRACT_HEADERS\r\nContent-Type: text/html\r\nThis is heading page\r\n\r\nSeems that this module couldn't extract header and data part.\r\n\r\nIs there anything wrong in configuring and setting up data?\r\n\r\nCould you please help me? I really need your module works on our site.\r\n\r\nThanks Bertrand Paquet 13/02/2013 à 12:06 @Kien, you forgot a \\r on the first line : EXTRACT_HEADERS\\r\\n, and a blank line (only \\r\\n) before 'This is heading page'\r\n\r\nI have improved doc on github. Bertrand Paquet 13/02/2013 à 12:11 @Vid Luther\r\n\"As long as batcache writes to the same memcached server, with the same key, subsequent requests to nginx will not even hit the PHP side of WordPress right?\"\r\n\r\nCorrect, if the key used by nginx to request Memcached and the key used by batcache store data on memcached are the same.\r\nLot of caching system are implemented in PHP : caching system will intercept the HTTP request (in PHP) in early stage, and will request memcached before launching wordpress. I don't known how batcache is working. It's better for performance if you can do that at nginx level. Bertrand Paquet 13/02/2013 à 12:17 @devin\r\n\r\nThis module is not designed to fill memcached automatically. If you are searching a module which do automatic caching using HTTP headers, please consider nginx proxy_cache (http://wiki.nginx.org/HttpProxyModule#proxy_cache), or varnish (https://www.varnish-cache.org/).\r\n\r\nWith this module, memcached have to be filled directly by your code (connecting to memcached), or by sending http request to this module (https://github.com/bpaquet/ngx_http_enhanced_memcached_module#store-data-into-memcached) Kien 13/02/2013 à 12:28 Hi Paquet,\r\n\r\nThanks you very much. I'm able to use your module now. Nick 03/02/2014 à 05:33 Hello,\r\n\r\nIs it possible to store a http code in memcache with this module? I would like to store a 403 error in cache.\r\n\r\nThanks Okidoki 10/09/2014 à 23:36 Hi, I just installed php5-fpm, php5-memcached and memcached prior to discovering this Enhanced Memcache module. Do I need to uninstall php5-memcached and memcached before installing the Enhanced module (from github) or can I go ahead and install it anyway and it'll take care itself? I use Ubuntu 14.04 with Nginx php-fpm zendopcache.  Thanks in advance! jack 24/10/2018 à 12:35 Does your module can work in \"memcache.hash_strategy = consistent\" configuration ? Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-05-04"},
{"website": "Octo", "title": "\n                Data Grid or NoSQL ? same, same but different…            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/data-grid-or-nosql-same-same-but-different/", "abstract": "Data Grid or NoSQL ? same, same but different… Publication date 02/02/2012 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 For three years now, NoSQL as a piece of technologies for Big Data has spread over the world and is challenging the centralized world of RDBMS. The space of distributed storages is yet not new and banks, online gaming platforms are using for several years technologies called “data grid” to address latencies and throughput issues. And to be completely franc, “Big Data” is not far from being the “new SOA”: a radical paradigm shift lost in the middle of commercial buzz words but that’s another story… What are the common points? The main differences? In the rest of the article, the term nosql will be used to talk about “transaction oriented” solutions like Cassandra , Riak , Voldemort , DynamoDB . On the other side, “Data Grid” will group solutions like Gigaspaces , Gemfire/SQLFire , Oracle Coherence . Both Data Grids and NoSQL are talking about distributed storage No surprise…this is a distributed vision of the “database” and of the storage which enable to support more throughputs, more volume under, generally, cost constraints. The main idea resides in the fact that instead of using high-end server to store and query data, you use several “low-end” servers to store more data, increase the throughput and being able to scale-out, address elasticity issues. Indeed, this article reminds us the unit cost of a transaction is approx.. 4 times cheaper on a cluster of low-end server than on a high end server, (the main drawback of distributed storage will be the bandwidth consumption that stays expensive). So in both cases, the distributed vision of the storage implies: Partitioning the data based on a key . Then the key is associated to a server (or a specific buckets served by a server). Routing the queries to the server that holds the data …The main idea is to avoid asking to all the servers for a specific piece of data. So the storage system needs to route the query to the right server. There are different implementations of that routing. Solution like Cassandra will implement a server side routing, implying at least and almost systematically one network hop (except if you are lucky and hit the right server). Solution like Voldemort will use a client-side routing. “Data Grid” like Gemfire will also use a client-side routing, will learn from the cluster where the data is located (and thus avoid the network hops) All these distributed systems use replication mechanisms to ensure the availability and, in some cases,  the durability (by mitigating the probability of losing a server before the data be replicated or by limiting the data corruption) Data Grid and NoSQL comes from two distinct worlds: the latency oriented architecture or throughput architecture I was reading again this article of Vogels. As he stated (talking about GPU and CPU): “… the most fundamental abstraction trade-off has always been latency versus throughput. These trade-offs have even impacted the way the lowest level building blocks in our computer architectures have been designed. Modern CPUs strongly favor lower latency of operations with clock cycles in the nanoseconds ” Even if it is quite difficult, should I say dangerous, to compare things, we can see the same trade-offs between NoSQL and data grid solutions. Data Grids come from a world where each milli-seconds (and now nano-seconds) count. These “data grids” have thus rapidly quit the disk and use the memory as main storage (even if it is configurable and we will discuss about that later). On the other side, NoSQL solutions have been developed mainly for web-scale industries where the latency is not less important but let say around the second (because the end user is a human). In that case, you do not need to answer quicker and quicker, but you need to serve more and more requests. NoSQL solutions have been developed to answer specific needs…whereas Data Grids are much more configurable: Bridging the gap If you look at the history of these solutions you will, at 10 000 foot high view, see that NoSQL solutions are mainly clones of the Dynamo model that have been developed at Amazon to store session and virtual caddies . The choices that were made fitted the Amazon.com needs : response time predictability, infrastructure elasticity and scaling out, multi-datacenter resiliency, design for failure… “Data Grids” come from more heterogeneous (and thus richer) environments. You need to use them to relieve the RDBMS, to keep data in your local JVM, to scale…You need to integrate them with the historical part of your Information System, so you need SQL-like integration, you need java, .Net, ruby client APIs… In short, data grids are clearly more configurable and so more adaptable than the NoSQL solutions. Without being exhaustive, we can think to: “Data Grids” enable you controlling the way data is stored . They all have default implementation (Gigaspaces offers RDBMS by default, Gemfire offers file and disk based storage by default….) but in all cases, you can choose the one that fits your needs: do you need to store data, do you need to relieve the existing databases…. In order to minimize the latency, data grids enable you to store data synchronously (write-through) or asynchronously (write-behind) on disk . You can also define overflow strategies. In that case, data is store in memory up to a treshold where data is flushed on disk (following algorithms like LRU …). NoSQL solutions have not been designed to provide these features. Data grids enable you developing Event Driven Architecture . In some cases, an often-seen architecture could be a pure RDBMS plus a Message Oriented Middleware that is responsible to propagate the modifications (via events). Data Grids provide notification mechanisms for all or specified keys. Clients can so be notified on create / update / delete operations (like triggers) Querying is maybe the point on which pure NoSQL solutions and data grids are merging . Basically, both solutions provides pure map API which means you can put, get or delete an object by the key. Data grids already provide SQL queries (with limitations on joins) for a while but looking at the NoSQL market, you will see that these NoSQL solutions start providing secondary indexes that enable SQL-like queries, or MapReduce-like queries (with Brisk or DynamoDB that provides integration with Hadoop or Elastic MapReduce). Data grids enable near-cache topologies . The classical way of scaling readings with a RDBMS solution is to add a cache layer on top of the RDBMS (typically a memcached with difficulties like cache synchronization, cache partitioning to avoid collapsing the RDBMS in case of failure..). We do it so we know it works but data grids facilitates the deployment of “near cache topologies” – which means that data can be cached on the client side – ensuring the auto-eviction when the data is updated in the central storage, accelerating the readings… Both systems have the same constraints especially when it comes to ACID The funny thing is that both systems have the same constraints especially when it comes to ACID. In fact, both solutions enables you to play with consistency and/or availability either using quorum based protocol or synchronous / asynchronous replication mechanisms . There is yet a subtility that can help data grids manage consistency (or at least conflict resolution): “data grid” use a master/slave topology per partition whereas nosql solutions use a master/master topology per partition. Moreover, the “data grids” can offer you Atomicity and Transactions management BUT under certain constraints : typically, you will have to override the partitioning mechanism and ensure data colocation, you will not be able to rebalance the cluster while transactions are pending…These features come at a cost on the operability of the solution. This may not be blocking depending on the use cases but NoSQL solutions like Dynamo have made their choices: operability, elasticity with impacts on the dev side. NoSQL offer a storage vision whereas data grids offer more cases On that side, the match is not that simple. Data Grids offer much more complicated deployment options . You can use them in a classical client / server architecture: the data is stored on dedicated JVM whereas the business processes have their own dedicated JVM. In that case, the data grid is seen as a pure storage layer. That case is finally quite close of the NoSQL solutions. What data grids offer (and not NoSQL solutions) is a peer-to-peer deployment where business processes and data are deployed in the same JVM. What changes is that the unit of deployment is the sum of the business process and you data. From the distributed storage point of view, there are no real advantages if you use your data grid as storage: data will be partitioned across the cluster, based on the key. This is yet quite different if your business cases necessitate writing and reading a lot of local data that is specific to your business case and must not be shared with the other business processes. In that case, you will benefit from this kind of deployment (gains in latency and bandwidth consumption). The main drawback of data grids is that they need to know the java object (typically in the classpath) and in the best case, you can choose between default java serialization (Serializable or Externalizable) or the specific serialization. That can complicate upgrading the model, adding indexes…without downtime… On the opposite (and it solves the previous issues), NoSQL solutions work with a byte array: object versioning, serialization and unserialization must so be “manually” managed (even if these solutions often use protocols like Thrift, Avro…). So what? what to conclude? I am sure I forgot some points but in short that both systems are under the same constraints. The clear points is that “data grids”, due to their own story are certainly more adaptable. That does not mean you must not look at NoSQL solutions because they can fit your requirements and these solutions work (see the works of Netflix with Cassandra ) More generally, sometimes I am asking myself: why will we move to distributed storage (if we move to)? May be the following elements: the variability of workload and the needed elasticity (maybe) the cost of this kind of infrastructure compared to traditional RDBMS infrastructure. I said maybe because I am convinced the economics equation must be explicitly posed. the resiliency of the systems (and the implied cost pressure) : we now need to address higher level of failure for cheaper (because the cost of mitigate the failure must not be higher than the cost of failure itself) the trivialization of infrastructure which will imply having less server diversity to serve more and more different use cases the platform vision that will push forward to develop multi-tenant architecture Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 6 commentaires sur “Data Grid or NoSQL ? same, same but different…” Dominique De Vito 08/02/2012 à 11:05 it just sounds like data grids are mainly for the middle tiers, while NoSQL databases are for the end tiers.\r\n\r\nThis being said,  data grids, as a processing/architectural model, sounds like a great evolution of application servers + not-so-clever distributed cache, when both are being used into the middle tiers. Because the previous not-so-clever distributed cache used into the middle tiers need/use resources, but don't give the expected results, that is, the scalibity needed in case of high workloads.\r\n\r\nGemstone/VMWare wrote about an interesting pattern \"data grid+database (for example, NoSQL)\" when announcing SQLFire 1.0 : http://www.infoq.com/news/2012/01/sqlfire-1-0\r\n\"The last scenario is interesting because it attempts to split the data of an RDBMS into two distinct categories. Data that is historical and does not need quick access since it is used for reference only purposes (this can be accessed by the RDBMS), and data that is transactional or holds current state (e.g. client sessions) which is accessed via SQLFire as an intermediate cache.\"\r\n\r\nWell, I have heard one person (the CTO ?) from GigaSpaces talking about the same pattern too. omallassi 08/02/2012 à 11:54 Yep I agree with you concerning the evolution of \"application servers + not so clever distributed cache\". \r\n\r\nI also agree with you concerning the approach of keeping fresh data in memory and others in the DB. That's a frequently meet pattern that can help the RDBMS Sebastien Lorber 14/02/2012 à 12:53 This pattern seems not so new, it's like using the Hibernate L2 cache (btw you can use a datagrid do to that).\r\n\r\n\r\n\r\nActually datagrids can be used as an end-tier. \r\nBut you need to understand that once you have persistent data inside your distributed system it's not so easy to update the distributed system configuration. For sure you can add map/reduce functions but in some cases you'll have to migrate from one distributed system to another. Even if it's using the same technology, once the distributed system configuration changes, it's a different distributed system that may be incompatible with the previous one.\r\n\r\n\r\nFor exemple the number of buckets of your distributed map can't be increased neither at runtime or by restarting your distributed system. \r\nThat number of buckets have effects on the performances of your system and may become inappropriate according to the number of cache servers you add to your system. For exemple Gemfire default is 113 buckets and VMWare recommends between 20 and 50 buckets / cache server thus if your number of server increase a lot...\r\nPerhaps\r\n\r\n\r\nThis means you should not over-optimize your end-tier distributed system but also configure it according to the load predictions.\r\n\r\n\r\n\r\nActually with Gemfire you can rebalance your system when there are transactions running. Your clients will get a TransactionDataRebalancedException. omallassi 14/02/2012 à 16:23 I know. We are in sync. Eric Bezille 05/04/2012 à 14:49 Good summary on the use cases of Data Grid. By the way we do have the HW + SW to run this Data Grid Cache out of the box in a very efficient way: google \"Exalogic+Coherence\" ;)\r\n\r\nTo add to your use cases, some design patterns / requirements still require in-memory Database solution (like TimesTen) to accelerate legacy applications where you don't want to re-invest in an heavy re-write. And with the big trend on Big Data, and real-time operations on unstructured data, NoSQL approach is also a very important piece beside Hadoop. I let you read further on this here : https://blogs.oracle.com/EricBezille/entry/big_data_opportunit%C3%A9_business_et Amit Thawani 01/05/2014 à 14:06 Salut Olivier, Thanks for the nice article. As you explained in the beginning, banks are late adopters thus I read your post today. Hope you are doing great.\r\n\r\nCheers, Amit Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-02-02"},
{"website": "Octo", "title": "\n                Face recognition in RIA applications            ", "author": ["Jan Fajfr"], "link": "https://blog.octo.com/en/face-recognition-in-web-application/", "abstract": "Face recognition in RIA applications Publication date 20/10/2011 by Jan Fajfr Tweet Share 0 +1 LinkedIn 0 Face recognition is exciting machine learning task which during the last decade has brought some good results, used mainly in security applications to perform person identification. However it used to be reserved only to university research and implemented only by companies specializing in this field. But the time invested into this field by universities, companies and independent developers has brought its result in the means of several open-source libraries which any developer can use to perform image processing tasks including face recognition. Take a look at the complete overview of existing computer vision algorithms and libraries. This article will show you how to incorporate face recognition into your web page using EmguCV image processing library and Silverlight Web Camera features. EmguCV is a .NET wrapper for OpenCV library, written in C++ by Intel and published as open-source. The method which will be used is Eigenfaces algorithm. If you are interested in the details of this algorithm, please refer to the previous article on this topic: The basics of face recognition . This post describes quite simple application which consists of two parts – Silverlight client, which is responsible for the image capture and eventually pre-processing and the server part which performs the actual face detection and face recognition. Capturing the image with Silverlight Since the version 4 Silverlight gives us the possibility to use the camera to capture images or videos using the CaptureSource class. The following snippet shows how to access the camera and show directly the capture picture. CaptureSource captureSource = new CaptureSource();\r\ncaptureSource .VideoCaptureDevice = CaptureDeviceConfiguration.GetDefaultVideoCaptureDevice();\r\ncaptureSource .CaptureImageCompleted += new EventHandler<CaptureImageCompletedEventArgs>(captureCompleted);\r\n\r\nif (captureSource .State != CaptureState.Started)\r\n{\r\n    // Create video brush and fill the WebcamVideo rectangle with it\r\n    var vidBrush = new VideoBrush();\r\n    vidBrush.Stretch = Stretch.Uniform;\r\n    vidBrush.SetSource(captureSource);\r\n    WebcamVideo.Fill = vidBrush;\r\n\r\n    // Ask user for permission and start the capturing\r\n    if (CaptureDeviceConfiguration.RequestDeviceAccess())\r\n        captureSource .Start();\r\n} Notice that I have to set the callback for the CaptureImageCompleted event. That should not surprise you, if you have worked with Silverlight before. Blocking of the user interface is not allowed, and so all operations depending on external sources are asynchronous. Now lets take a look at what happens when the image is actually captured: private void captureCompleted(object sender, CaptureImageCompletedEventArgs e)\r\n{\r\n    var image = e.Result;\r\n    var coll= new ObservableCollection<int>(image.Pixels);\r\n    switch(appMode)\r\n    {\r\n        case AppMode.Recognition:\r\n            client.RecognizeAsync(coll,image.PixelWidth);\r\n            break;\r\n        case AppMode.Training:\r\n            client.TrainFaceAsync(coll, image.PixelWidth,TextBoxLabel.Text);\r\n    }\r\n} As a result, we obtain WritableBitmap object, which has a simple Pixels property of type int[] . This property holds all the pixels of the image in one dimensional array created by aligning all the rows of the image to one array. Notice that each pixel is presented as Int– this means that that we have a classic 32bits representation of color for each pixel. We take this array and we send it to the server. When the application is in training mode, we send also the label, which should be added to the Image. If the application is in recognition mode, we send just the image and we hope to receive the label describing the person on the image. Face recognition using EmguCV on the server As described in the previous article, face recognition has two phases: Face detection, and actual face recognition. We will use EmguCV to perform both of these tasks. EmguCV is a wrapper for OpenCV library. This basically means that we can call all the functions inside OpenCV library without the need of using constructs such as DLLImport directive and without the need of knowing the structure of the OpenCV library. EmguCV is surely more friendly for C# developers than OpenCV (at least we do not have to treat the pointers), on the other hand the documentation is not perfect. Several times you will have to address directly the documentation of OpenCV to understand the structures and methods. For this point of view it is important to understand the algorithms which you want to use from EmguCV/OpenCV, because you will have hard-time finding the documentation to understand what each method does. Detecting the face I have created a function which takes the array of pixels, and the size of the image, converts the picture to gray scale, detects the face, trims the rest of the image and resizes the resulting image to demanded size, just as shown in the following picture: public static Image<Gray, byte> DetectAndTrimFace(int[] pixels, int iSize, int oSize)\r\n{\r\n    var inBitmap = ConvertToBitmap(pixels, iSize);\r\n    var grayframe = new Image<Gray, byte>(inBitmap);\r\n    var haar = new HaarCascade(getHaarCascade());\r\n    var faces = haar.Detect(grayframe,1.2,3, HAAR_DETECTION_TYPE.DO_CANNY_PRUNING, new Size(30,30));\r\n    if (faces.Count() != 1)\r\n        return null;\r\n\r\n    var face = faces[0];\r\n    var returnImage = grayframe.Copy(face.rect).Resize(oSize, oSize, INTER.CV_INTER_CUBIC);\r\n    return returnImage;\r\n} The standard structure to treat images inside EmguCV is Image<ColorType, depth> . So from the previous code snippet you can see, that we are returning gray-scales image, where each pixel has 8 bits (byte structure). We first have to reconstruct an Bitmap image from pixels array and than create Image<Gray, byte> object from resulting Bitmap. In this step EmguCV converts automatically the image to gray-scale. Next the face detection is performed using HaarCascade detection algrorithm. The description of this algorithm is not part this article. Once the face is detected (and if there was only one face), we copy the rectangle wrapping the face and resize it to demanded output size. It is important to use the same size, when adding the image to the training set and recognizing the face from the image. For actual recognition EmguCV contains EigenObjectRecognizer class, which needs several arguments to be created: Array of images and corresponding array of labels. Eigen Distance Threshold – Eigenfaces algorithm measures the distance between images. This thresholds defines the maximal distance needed to classify the image as concrete person. Big values such as 5000 will make the classifer to return the closest match, even if the probability that the person has been recognized is quite small. Set it to 500 to obtain some reasonable results. MCvTermCriteria – is a class which represents OpenCV structure for terminating iterative algorithms. It is composed of two numbers the first being the number of iterations and the second one is demanded accuracy. For some algorithm it makes sense to iterate until the accuracy is not bellow certain threshold. For eigenfaces algorithm it is the number of iterations which is important and it will impact the number of eigenfaces being created. I have obtained some good results having around 40 eigenfaces for 100 images in the database, but this might depend on your scenario, quality and diversity of the images. MCvTermCriteria termCrit = new MCvTermCriteria(40, accuracy);\r\nEigenObjectRecognizer recognizer = new EigenObjectRecognizer(\r\n                 trainedImages.ToArray(),\r\n                 labels.ToArray(),\r\n                 eigenDistanceThreshold,\r\n                 ref termCrit); Face recognition The actual face recognition is a piece of cake if we already know how to create the recognizer. First we have to detact and extract the face from the image with the help of previously mentioned DetectAndTrimFace method. And the second step is to pass this image to the recognize method of the recognizer. public String Recognize(int[] pixels, int size)\r\n{\r\n   var imgFace = ImagesProcessing.DetectAndTrimFace(pixels, size, \r\n   var recognizer = ImagesProcessing.CreateRecognizer(folder, ...);\r\n   String label = recognizer.Recognize(imgFace);\r\n   if(!String.IsNullOrEmpty(label))\r\n       return label;\r\n   return \"Could not recognize\";\r\n} Adding the picture to the training set This again is easy. Once we do the face detection and trimming, we will just see if there is a folder for the given label and if not, we will create one. One option is to use Guid as a name of file to be sure, that we wont have collisions. [OperationContract]\r\npublic void AddToTraining(int[] pixels, int size, String label)\r\n{\r\n    var faceImg = ImagesProcessing.DetectAndTrimFace(pixels, size, fixedFaceSize);\r\n    var directoryNames = GetDirectories();\r\n    //no such label - create directory\r\n    if (!directoryNames.Contains(label))\r\n        Directory.CreateDirectory(DIR + \"\\\\\" + label);\r\n\r\n    faceImg.Save(DIR+ \"\\\\\" + label + \"\\\\\" + Guid.NewGuid().ToString() + \"_\" + fixedFaceSize + \".jpg\");\r\n} Performance The face-detection phase is quite instantaneous. Using the HaarCascade algorithm to detect face in the image, we have a chance, that OpenCV already offers a set of HaarCascade features. The creation of these features is much more time consuming, than their usage. The face-recognition phase is also not that time-consuming. In this example I was working with database of 100 images (10 persons, each having in average 10 images, each image being 80×80 pixels). In this setting the creation of the recognizer is almost instant. Surely the read access of images from the disk, takes more time than, the actual creation of recognizer (the eigenfaces algorithm). If we have a database of pictures which does not change, we create the recognizer only once and than share it every-time there is a request for recognition. This of course is not possible when the image database changes. In that case we have to recreate the recognizer. There is no possibility to add new image to existing recognizer. The bottleneck of this approach is still the network over which we have to pass the image from client to the server. To minimize the data which has to be transfer, take a look at the tip which shows you, how to perform the face detection on the client side at the end of the article. Tricks and tips Equalization of the image Histogram equalization is a technique to improve the contrast and adjust the brightness of the image. EmguCV gives us the possibility to call the equalize function which resides in OpenCV: public static Image<Gray, byte> EqualizeHist(Image<Gray, byte> input)\r\n{\r\n    Image<Gray, byte> output = new Image<Gray, byte>(input.Width, input.Height);\r\n    CvInvoke.cvEqualizeHist(input.Ptr, output.Ptr);\r\n    return output;\r\n} Take a look at the picture before and after equalization: Creating additional images Generally the results are better when the training set is large. If we do not have enough images, we can try to create new ones by slight rotation or horizontal flipping. EmguCV gives us the possibility to call Flip or Rotate method on each picture: var fliped = faceImg.Flip(FLIP.HORIZONTAL);\r\nvar rotated = faceImg.Rotate(angle, new Gray(0.3)); Of course we have to be careful only to use Horizontal flipping (faces upside-down would just confuse the recognizer) and to rotate just to some angle. More image treatment on the client side In the presented code, you can notice, that I am sending the image to the server, right after image capture, without any preprocesing. Because the default size of the image is 640×480 pixels. Knowing that each pixel is represented by 32 bit Integer this means that we are sending roughly 1200 kB to the server for recognition or training. Since images are resized later anyway, we could resize them directly before sending to the server. Resizing the images to 320*240 would reduce the size of send data to 300 kB (but of course we can do more). Unfortunately Silverlight does not have method for resizing images, however there is a great project on Codeplex , which contains several extensions methods for WritableBitmap, one of them being the resize method. Face detection on client side It is not possible to move all the face recognition process to the client side, from one simple reason: the recognizer has to, at some point, load and process all the images in the database. Performing this part on the client side, would demand us either to embark all the images into the client or transfer these images over the wire. Either of these options is not attractive for web developers. However the process of face detection could be moved to the client side, while it is not dependent on any big amount of external data. Great article which explains face detection in Silverlight is available as FaceLight project on Channel 9 Will we log to our bank using only our web camera? Well the answer is no, face recognition is still not completely trust-worthy. Eigenface algorithm presented above is too much sensitive on the brightness changes and face-pose changes. When testing the algorithm, I obtained successful results in 60-70% of the total tries. However new algorithms have been developed combining several image processing and face recognition techniques and of course many companies are specializing in this field and develop software with much better accuracy. Another problem is the fact that it is far too easy to present a taken photograph to the web camera and unless the system is capable of recognizing the real person from taken picture it will be cheated. We will probably see mutual systems where the login and password will still be the main security check and face recognition will be used instead of the third confirmatory information such as RSA generated number or confirmation code. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged .NET , EmguCV , face recognition , Silverlight . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 8 commentaires sur “Face recognition in RIA applications” startrinity 22/02/2012 à 14:26 Jan, great article. Hope that banks and other commercials will use this technology. Please consider a web site (startrinity.com) where we offer a face login service for web applications. We have used much more complicated algorithms rather than facelight@codeplex yetsin 15/07/2012 à 09:16 Thanks for this great article, Jan. I try to do this but I couldn't. I'm new in Silverlight and I couldn't understand that part. Can you share the project of silverlight. clarity of diamonds 17/07/2012 à 03:07 I am genuinely glad to read this weblog posts which consists of \r\ntons of useful facts, thanks for providing such information. Taiwan 20/07/2012 à 07:59 I really like your blog.. very nice colors & theme. Did you design this website yourself or did you hire someone to do it for you?\r\n\r\nPlz reply as I'm looking to construct my own blog and would like to know where u got this from. appreciate it Jan Fajfr 23/07/2012 à 13:43 Yetsin,\r\n\r\nThe code of the demo is now available at Github. Here is the link:\r\nhttps://github.com/hoonzis/Silverlight-Face-Recognition\r\n\r\nHope that you can get it to work. yetsin 30/07/2012 à 22:26 Thanks for sharing your code, Jan. But I couldn't run it. I solved \"errors\". When I run it, it starts webcam, I can see it. And than I wrote a name, click \"Add to training set\" but nothing happens. Any image saved or any face detected. in webconfig \r\nconnectionString=\"Data Source=.\\SQLEXPRESS;Initial Catalog=facerec;Integrated Security=SSPI;\"/>\r\nI dont have this sql file. Is it the reason of not to run? yetsin 14/08/2012 à 00:27 I solved it, thanks again. cams 03/08/2013 à 08:47 Thanks for this great article.\r\nI have downloaded the code from\r\nhttps://github.com/hoonzis/Silverlight-Face-Recognition.\r\n\r\nBut it could not work properly.When I try to run this application,it starts webcam and I added a name to  training set but nothing happens.\r\n\r\nI found a connection string in webconfig \"connectionString=”Data Source=.\\SQLEXPRESS;Initial Catalog=facerec;Integrated Security=SSPI;”/>\".\r\nIs all the images are saved in this database?Please rectify.I dont have this sql file.Please let me know the reason of not working the application. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-10-20"},
{"website": "Octo", "title": "\n                An overview of RestKit, a Core Data enabled iOS and MacOSX framework for Restful applications            ", "author": ["Rémy Saissy"], "link": "https://blog.octo.com/en/overview-of-restkit-a-core-data-enabled-ios-macosx-framework-for-restful-apps/", "abstract": "An overview of RestKit, a Core Data enabled iOS and MacOSX framework for Restful applications Publication date 22/12/2011 by Rémy Saissy Tweet Share 0 +1 LinkedIn 0 ASIHttpRequest, AFNetworking, MKNetworkKit,… The iOS/MacOSX programming landscape is full of helper libraries to deal with asynchronous network communications in your applications. RestKit can be seen as yet another library to do it. However, its approach is radically different since it does not only address asynchronous networking but also object mapping and a seamless persistance of such mappings locally through CoreData. This approach is quite interesting when it comes to interact with Restful web services because of the time (and code!) you can save. In this article, we are going to look at RestKit’s main functionalities through the code to do it. The objective is to give you a precise idea of what can be done and how to do it. Network communications through RKClient In RestKit, network communications are managed by the RKClient class. This class encapsulates details on the HTTP level and provides control to the developer over it by simply editing properties. RKClient is where you can find the functionalities of ASIHttpRequest , AFNetworking or MKNetworkKit. From RestKit standpoint, it is an intermediate layer as we will see later in this article. RKClient *client = [RKClient clientWithBaseURL:@\"http://api.domain.com/v1\"];\r\n[RKClient setSharedClient:client];\r\nNSLog(@\"This is my default client: %@.\", [RKClient sharedClient]);\r\nif ([[RKClient sharedClient] isNetworkReachable]){\r\n    // Do some work...\r\n} This code creates a new RKClient instance and set it as the default one. You can create as many instances as you want. Each instance is tied to a specific base URL which serves as the prefix to all network communications. RKClient exposes several properties to configure HTTP communications. Here is a review of the most common use case you might have. Customize HTTP Headers NSLog(@\"HTTP headers added: %@.\", [RKClient sharedClient].HTTPHeaders);\r\n[[RKClient sharedClient].HTTPHeaders setObject:@\"MyAppSpecialHeader\" forKey:@\"X-MyApp-Special-Header\"];\r\n[[RKClient sharedClient].HTTPHeaders removeObjectForKey:@\"X-Old-Key\"]; This allows you to control custom HTTP headers you want to use with your requests through this client. SSL communications RKClient *sslClient = [RKClient clientWithBaseURL:@\"https://api.domain.com/v1\"];\r\nsslClient.disableCertificateValidation = YES;\r\nSecCertificateRef myCustomRootCertRef = SecCertificateCreateWithData(NULL, certData);\r\n[sslClient addRootCertificate:myCustomRootCertRef];\r\nNSLog(@\"NSSet of my additional root certs: %@.\", sslClient.additionalRootCertificates); As shown in the example, you can disable SSL certificate validation and add your own root certificates for a given RKClient . Request queue and Cache policy control RKClient is asynchronous, its concurrency level is customizable and requests can suspended in which case they are enqueued until the queuing limit is reached. [RKClient sharedClient].requestQueue.concurrentRequestsLimit = 2;\r\n[RKClient sharedClient].requestQueue.requestTimeout = 10;\r\n[RKClient sharedClient].suspended  = YES;\r\nNSLog(@\"Number of requests in the queue: %d. Total number of request currently loading: %d.\",\r\n[RKClient sharedClient].loadingCount, [RKClient sharedClient].count); It is also possible to access and control the cache of the RKClient instance. Several policies such as use the cache only when offline or use the cache when the request timeouts as builtin. The developer can dynamically change the caching policy and also has a direct access to the cache implementation so she can invalidate entries, add others, … [RKClient sharedClient].cachePolicy = RKRequestCachePolicyLoadIfOffline|RKRequestCachePolicyTimeout;\r\n[RKClient sharedClient].requestCache.storagePolicy = RKRequestCacheStoragePolicyPermanently; Builtin support for authentication schemes RestKit natively supports 4 authentication schemes: HTTP Auth HTTP Basic OAuth1 OAuth2 This is configured through properties of RKClient . For example, to configure OAuth2 you would do the following. [RKClient sharedClient].authenticationType = RKRequestAuthenticationTypeOAuth2;\r\n[RKClient sharedClient].OAuth2AccessToken = @\"YourOAuth2AccessTokenHere\";\r\n[RKClient sharedClient].OAuth2RefreshToken = @\"YourOAuth2RefreshTokenHere\"; Once configured this way, all network communications will use the credentials you specified. Other authentication schemes are configured the same way. Each have its own set of properties in RKClient . A real life example with RKClient Now let’s consider a real life example. Let’s say that we want to retrieve an RSS feed of today from a website which requires an HTTP Basic authentication. We don’t want to cache anything and we also don’t want to wait for more than 10 seconds to retrieve the feed. The code to do it would look like this: #import <Foundation/Foundation.h>\r\n#import <RestKit/RestKit.h>\r\n\r\n@interface MyClass : NSObject<RKRequestDelegate>\r\n\r\n- (void)retrieveFeed;\r\n\r\n@end\r\n\r\n@implementation MyClass\r\n\r\n- (void)retrieveFeed\r\n{\r\n  RKClient *client = [RKClient clientWithBaseURL:@\"http://rss.domain.com\"];\r\n  client.requestQueue.requestTimeout = 10;\r\n  client.cachePolicy = RKRequestCachePolicyNone;\r\n  client.authenticationType = RKRequestAuthenticationTypeHTTPBasic;\r\n  client.username = @\"rssuser\";\r\n  client.password = @\"rsspwd\";\r\n  NSDictionary *queryParameters = [NSDictionary dictionaryWithObjectsAndKeys:@\"today\", @\"date\", nil];\r\n  NSString *getResourcePath = RKPathAppendQueryParams(@\"/feed.xml\", queryParameters);\r\n  [client get:getResourcePath delegate:self];\r\n}\r\n\r\n- (void)request:(RKRequest *)request didLoadResponse:(RKResponse *)response\r\n{\r\n  if (request.method == RKRequestMethodGET) {\r\n        id xmlParser = [[RKParserRegistry sharedRegistry] parserForMIMEType:RKMIMETypeXML];\r\n        NSError *error = nil;\r\n        id parsedResponse = [xmlParser objectFromString:[response bodyAsString] error:&error];\r\n        if (error == nil)\r\n            NSLog(@\"GET:/user returned with HTTP Code %d and parsedContent: %@\", [response statusCode], parsedResponse);\r\n  }\r\n}\r\n\r\n- (void)request:(RKRequest *)request didFailLoadWithError:(NSError *)error\r\n{\r\n  if (request.method == RKRequestMethodGET) {\r\n    NSLog(@\"Failure of GET:/user with error %@.\", error);\r\n  }\r\n}\r\n@end As you have probably noticed, a delegate is provided to handle the asynchronous results. RestKit relies a lot on delegate both internally and for its public interfaces. Another interesting point is how we parsed the response of the feed which is expected to be XML content. Indeed, RestKit provides a parser registry that you can use to get an instance of a parser for one of these four mimetypes: RKMIMETypeJSON for application/json RKMIMETypeFormURLEncoded for application/x-www-form-urlencoded RKMIMETypeXML for application/xml RKMIMETypeTextXML for text/xml JSON parsing is based on JSonKit and XML parsing is based on libxml2 . Therefore, most of the time, you don’t need to include another third party library for parsing network responses. Object Mapping through RKObjectManager What makes RestKit quite different from other libraries is the object mapping layer and its seamless support for persistence through CoreData. RKObjectManager is the entry point in RestKit to manage object mappings. Each instance of this class encapsulates an RKClient , an RKObjectRouter , an RKObjectMappingProvider and an RKManagedObjectStore. But let’s see by the examples how to use the Object Manager. Create a mapper for a remote API RKObjectManager *objectManager = [RKObjectManager objectManagerWithBaseURL:@\"http://api.domain.com/v1\"];\r\nRKObjectManager *sslObjectManager = [RKObjectManager objecxtManagerWithBaseURL:@\"https://api.domain.com/v1\"];\r\n\r\n// Initialize the managed object store because both will have Core Data managed objects.\r\nobjectManager.objectStore = [RKManagedObjectStore objectStoreWithStoreFilename:@\"api.sql\"]; \r\nsslObjectManager.objectStore = [RKManagedObjectStore objectStoreWithStoreFilename:@\"apissl.sql\"]; \r\n\r\n// We want the SSL Object Manager to be our sharedManager.\r\n[RKObjectManager setSharedManager:sslObjectManager]; You can have several object managers and choose which one is the sharedManager . Each  Object Manager instance has an RKClient instance. Therefore you can control RKClient parameters on a per remote API mapper basis. Create a mapping and register a route for it RKManagedObjectMapping *myMapping = [RKManagedObjectMapping mappingForClass:[MyClass class]];\r\n\r\n// Map attributes\r\n[myMapping mapAttributes:@\"id\", @\"name\", nil];\r\n\r\n// These attributes have different name in objc and JSON.\r\n[myMapping mapKeyPathsToAttributes:@\"expirationTime\", @\"expiration_time\", nil];\r\n\r\n// Since it is a Core Data managed class, we can indicate to RestKit which property is the primary key (optional).\r\n[myMapping setPrimaryKeyAttribute:@\"id\"];\r\n\r\n// Set relationships. Previously mapped classes are used here.\r\n[myMapping hasOne:@\"user\" withMapping:[[RKObjectManager sharedManager].mappingProvider objectMappingForKeyPath:@\"userMapping\"]];\r\n[myMapping hasMany:@\"sessions\" withMapping:[[RKObjectManager sharedManager].mappingProvider objectMappingForKeyPath:@\"sessionMapping\"]];\r\n\r\n// Set both the mapping and the serialization mapping.\r\n[[RKObjectManager sharedManager].mappingProvider setMapping:myMapping forKeyPath:@\"tokenMapping\"];\r\n[[RKObjectManager sharedManager].mappingProvider setSerializationMapping:[myMapping inverseMapping]  forClass:[MyClass class]];\r\n\r\n// Register two routes for this class, a GET and a POST.\r\n[[RKObjectManager sharedManager].router routeClass:[MyClass class] toResourcePath:@\"/api/foo\" forMethod:RKRequestMethodGET];\r\n\r\n// The POST route has a dynamic parameter which is the objc property name prefixed by a semicolon.\r\n[[RKObjectManager sharedManager].router routeClass:[MyClass class] toResourcePath:@\"/api/foo/:id\" forMethod:RKRequestMethodPOST]; Requesting a resource using its route // Our object is managed through CoreData. RestKit takes care of updating it.\r\n@interface MyClass : NSManagedObject\r\n\r\n@property (retain, nonatomic) NSInteger id;\r\n@property (retain, nonatomic) NSString *name;\r\n\r\n@end\r\n\r\n@implementation MyClass\r\n\r\n@dynamic id, name;\r\n\r\n@end\r\n\r\n@interface MyController : UIViewController<RKObjectLoaderDelegate>\r\n\r\n- (void)loadData;\r\n\r\n@end\r\n\r\n@implementation MyController\r\n\r\n- (void)loadData\r\n{\r\n  // Retrieve an instance stored in CoreData. RestKit provides a category to encapsulate Core Data interactions.\r\n  MyClass *obj = [MyClass findFirstByAttribute:@\"id\" withValue:@\"42\"];\r\n  obj.name = @\"A new name to set remotely\";\r\n\r\n  // Perform the post request.\r\n  [MyClass postObject:obj delegate:self];\r\n}\r\n\r\n- (void)objectLoader:(RKObjectLoader*)objectLoader didLoadObject:(id)object\r\n{\r\n  NSLog(@\"Request succeed and  response loaded into CoreData: %@.\", object);\r\n}\r\n\r\n- (void)objectLoader:(RKObjectLoader*)objectLoader didFailWithError:(NSError*)error\r\n{\r\n  NSLog(@\"An error occurred: %@.\", error);\r\n}\r\n@end Other response delegates are available and let the developer receive its content as a dictionary or an array of objects if the return was expected to be an array. Requesting a resource without a route Loading a request without a route is also possible. The use case is when a request expects an array of a specific object. In order to request without a route, the developer uses the loadObjectsAtResourcePath: method call. RKObjectMapping *userMapping = [[RKObjectManager sharedManager].mappingProvider objectMappingForKeyPath:@\"sessionMapping\"];\r\n\r\n[[RKObjectManager sharedManager] loadObjectsAtResourcePath:@\"/api/foo\" objectMapping:userMapping delegate:self]; Nothing else changes. Configuring a mapping In order to cope with a maximum of situation, RestKit provides a set of properties to configure each mapping to fit to what the remote API returns or expects to receive. Here are some of the most useful of these properties: Handle missing attributes myMapping.setDefaultValueForMissingAttributes = YES;\r\nmyMapping.setNilForMissingRelationships = YES; You can set both missing attributes and relationships to nil. Mapping unpredictable JSON keys Let’s say for example that user id/name is the key in a JSON and that for each “key”, the value is the user object. Let’s consider the following JSON: { \"users\":\r\n  {\r\n    \"foo\": { \"id\": 1234, \"email\": \"foo@domain.com\" },\r\n    \"bar\": { \"id\": 5678\", \"email\": \"bar@domain.com\" }\r\n  }\r\n} To handle this kind of unpredictable mapping in RestKit, we would do: RKObjectMapping *myMapping = [RKObjectMapping mappingForClass:[User class]];\r\n// RestKit cannot infer this is a collection, so we force it\r\nmyMapping.forceCollectionMapping = YES;\r\n// Map our attributes.\r\n[myMapping mapKeyOfNestedDictionaryToAttribute:@\"firstName\"];\r\n[myMapping mapFromKeyPath:@\"(firstName).id\" toAttribute:\"userID\"];\r\n[myMapping mapFromKeyPath:@\"(firstName).email\" toAttribute:\"email\"];\r\n\r\n[[RKObjectManager sharedManager].mappingProvider setObjectMapping:myMapping forKeyPath:@\"users\"]; Dates are formatted in a very specific way RestKit let the developer handle it in different ways: By default, a mapping uses an array of date formatters Each mapping can define its own array of data formatters to use instead of the global one Each mapping can define its preferred date formatter to use in first place myMapping.preferredDateFormatter = [[[NSDateFormatter alloc] init] autorelease];\r\nmyMapping.dateFormatters  = [NSArray arrayWithObjects:[[[NSDateFormatter alloc] init] autorelease], [[[NSDateFormatter alloc] init] autorelease], [[[NSDateFormatter alloc] init] autorelease], nil];\r\n\r\n// Set as the application wide default set of formatters.\r\n[RKObjectMapping setDefaultDateFormatters:myMapping.dateFormatters]; Avoids duplicate entries of an object in the Core Data store RKManagedObjectMapping *myMapping = [RKManagedObjectMapping mappingForClass:[CoreDataManagerObject class]];\r\nmyMapping.primaryKeyAttribute = @\"id\"; Note that this is specific to objects managed with CoreData. Initializing using a seed database You can initialize the Core Data store using a seed database. For example you may want to seed your store if it is empty. RKObjectManager *objectManager = [RKObjectManager objectManagerWithBaseURL:@\"http://api.domain.com/api\"];\r\n\r\n// ... Initializing mappings.\r\nNSArray *objectsInStore = [MyObject allObjects];\r\nif (objectsInStore count] == 0) {\r\n  NSString *storeFilename = objectManager.objectStore.storeFilename;\r\n  NSString *pathToSeedDB = [[NSBundle mainBundle] pathForResource:@\"MySeedDB\" ofType:@\".db\"];\r\n  RKManagedObjectStore *objectStore = objectStoreWithStoreFilename:storeFilename usingSeedDatabaseName:pathToSeedDB managedObjectModel:nil delegate:self];\r\n  objectManager.objectStore = objectStore;\r\n} Handling Core Data errors Sometimes the Core Data store may have failure. For example when your data model is invalid. RestKit provides a way to handle such events with the RKManagedObjectStoreDelegate . @interface MyAppDelegate : UIResponder <RKManagedObjectStoreDelegate>\r\n@end\r\n\r\n@implementation MyAppDelegate\r\n\r\n- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions\r\n{\r\n  RKObjectManager *objectManager = [RKObjectManager   objectManagerWithBaseURL:@\"http://api.domain.com/api\"];\r\n  objectManager.objectStore.delegate = self;\r\n}\r\n\r\n// Delegates\r\n\r\n- (void)managedObjectStore:(RKManagedObjectStore *)objectStore didFailToCreatePersistentStoreCoordinatorWithError:(NSError *)error\r\n{\r\n  NSLog(@\"Fail to create a persistent store. Error: %@\", error);\r\n}\r\n\r\n- (void)managedObjectStore:(RKManagedObjectStore *)objectStore didFailToDeletePersistentStore:(NSString *)pathToStoreFile error:(NSError *)error;\r\n{\r\n  NSLog(@\"Fail to delete a persistent store at path %@. Error: %@\", pathToStoreFile, error);\r\n}\r\n\r\n- (void)managedObjectStore:(RKManagedObjectStore *)objectStore didFailToCopySeedDatabase:(NSString *)seedDatabase error:(NSError *)error\r\n{\r\n  NSLog(@\"Fail to copy seed database: %@. Error: %@\", seedDatabase, error);\r\n}\r\n\r\n- (void)managedObjectStore:(RKManagedObjectStore *)objectStore didFailToSaveContext:(NSManagedObjectContext *)context error:(NSError *)error exception:(NSException *)exception\r\n{\r\n  NSLog(@\"Fail to save context: %@. Error: %@ (%@)\", context, error, exception);\r\n}\r\n@end Unit Testing In RestKit, all network communications are done using two classes: RKRequest and RKResponse . Unit testing thus requires reimplementing the – (void)fireASynchronousRequest in RKRequest to avoid enqueuing the request and mocking your API. In this case, your reimplemented method will build its own RKResponse packet and call the relevant delegates. To know which one to use, it is needed to look at the source code of the version of RestKit compiled with the application. Here is an example of fireAsynchronousRequest modification for unit test. - (void)fireASynchronousRequest\r\n{\r\n// From the original implementation.\r\n\r\n[self prepareURLRequest];\r\nNSString* body = [[NSString alloc] initWithData:[_URLRequest HTTPBody] encoding:NSUTF8StringEncoding];\r\nNSLog(@\"Sending %@ request to URL %@. HTTP Body: %@\", [self HTTPMethod], [[self URL] absoluteString], body);\r\n[body release]; \r\n\r\n_isLoading = YES;\r\nif ([self.delegate respondsToSelector:@selector(requestDidStartLoad:)])\r\n[self.delegate requestDidStartLoad:self]; \r\n\r\nRKResponse* response = [[[RKResponse alloc] initWithRequest:self] autorelease];\r\n[[NSNotificationCenter defaultCenter] postNotificationName:RKRequestSentNotification object:self userInfo:nil];\r\n\r\n// Now execute the mocking code.\r\nNSString *bundlePath = nil;\r\nNSInteger statusCode = 200;\r\n\r\nid requestJsonObject = nil;\r\nif (self.params != nil) {\r\n\r\n  NSString *jsonString = [[[NSString alloc] initWithData:[self.params HTTPBody] encoding:NSUTF8StringEncoding] autorelease];\r\n\r\n  // If you use the latest version of RestKit with Object Mapper 2.0:\r\n  id parser = [[RKParserRegistry sharedRegistry] parserForMIMEType:RKMIMETypeJSON];\r\n  NSAssert1(parser, @\"Cannot perform object load without a parser for MIME Type '%@'\", RKMIMETypeJSON);\r\n  NSError **error = nil;\r\n  requestJsonObject = [parser objectFromString:jsonString error:error];\r\n}\r\n\r\n// Switch to determine which to to use\r\n…\r\n\r\n// Prepare the results.\r\nNSData *responseData = nil;\r\nif (bundlePath == nil)\r\nresponseData = [NSData data];\r\nelse\r\n{\r\n// Load the mock file.\r\nNSString *responseString = [NSString stringWithContentsOfFile:bundlePath encoding:NSUTF8StringEncoding error:nil];\r\nresponseData = [responseString dataUsingEncoding:NSUTF8StringEncoding];\r\n}\r\n\r\n// Send the response. This is a superclass to be able to return a custom statusCode.\r\nMyHTTPURLResponse *urlResponse = [[[MyHTTPURLResponse alloc] initWithURL:_URL MIMEType:RKMIMETypeJSON expectedContentLength:[responseData length] textEncodingName:nil] autorelease];\r\n[urlResponse setStatusCode:statusCode];\r\n\r\n// Delegates in the order RestKit is expecting it.\r\n[response connection:nil didReceiveResponse:urlResponse];\r\n[response connection:nil didReceiveData:responseData];\r\nif ([response isError] == YES)\r\n[response connection:nil didFailWithError:[NSError errorWithDomain:@\"test domain\" code:statusCode userInfo:nil]];\r\nelse\r\n[response connectionDidFinishLoading:nil];\r\n} Conclusion The choice of integrating a third party library is an important one. RestKit is an active project and from my experience, the questions you might ask on their forums will find an answer quickly. Moreover, if you need object mapping or Core Data support, RestKit is definitely a very good choice since its clean architecture and API saves a lot of time to developers. If you don’t need such functionalities, then you should focus on RKClient ‘s functionalities compared to other frameworks and keep in mind that even though it is quickly moving towards blocks, RestKit still heavily relies on delegates. Feel free to let me know your feelings and comments. References RestKit: http://restkit.org/ RestKit Source Code on gitHub: https://github.com/RestKit/RestKit RestKit Google Group: https://groups.google.com/forum/#!forum/restkit Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged iOS , iPad , iPhone , mobility , RestKit . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-12-22"},
{"website": "Octo", "title": "\n                Devoxx 2011, the main trends            ", "author": ["Michel Domenjoud"], "link": "https://blog.octo.com/en/devoxx-2011-the-main-trends/", "abstract": "Devoxx 2011, the main trends Publication date 03/12/2011 by Michel Domenjoud Tweet Share 0 +1 LinkedIn 0 One of the biggest Java conference in the world took place in Antwerpen, Belgium on last November. This year, Devoxx main themes were (in no particular order) The future of Java Alternative languages on the JVM HTML5 JavaFX Android A bit of Cloud, NoSQL and high performance architecture We also had a great announcement about a new conference coming in 2012: Devoxx France ! Obviously, OCTO was there. In this article we won’t cover the sessions in details. A lot of blogs already did it. We’ll however summarize this edition’s main trends and give our impressions. Java Java SE & EE All announcements from Oracle about Java were already made at JavaOne, but the two opening keynotes gave us a good reminder and here are the key points: Java SE 7 is out since last July and brings several new functionalities. Java SE 8 is planned for 2013 and will include mainly Jigsaw (Java modularity) and Lambda (closures in Java) projects. Java EE 7 is planned for Q3 2012 and will be cloud oriented. Standard metadata will be provided for easing provisioning, multi-tenancy support, elasticity, etc. We should see an emerging standard for solutions such as Cloud Foundry from vmWare or OpenShift from RedHat. Other standards should evolve, with by example a 2.0 version for JMS, which had no new version since 2003. Mark Reinhold gave us a talk about Project Jigsaw, which aims to allow Java applications and the JDK itself to be natively modular. For example,  we should be able to deploy Java applications as RPM package. We will also be able to take only small parts of the JDK for specific applications. This project is interesting, but still  frightening on one point : they don’t seem to be really close from projects like Maven which already had to deal with many modularity and dependencies problems for years : can we hope they won’t fall in the same traps as Maven in the beginning? Another important point is the great emphasis made on JavaFX 2, which is now the main GUI framework for thick clients and mobile. Therefore, no more investments will be done on Swing, even if Oracle will continue to support it. A lot of sessions were about Java FX, and it was quite conclusive. Let’s hope it won’t became the same flop as Java FX 1. We also had two great retrospectives about Java by Joshua Bloch, who made a really in depth analysis of pretty much all the original JDK classes! Coming from the guy who gave us the generics, it’s a quite interesting talk. Watch it on Parleys.com when it will become available, you could discover some tricky details about Java. The Oracle guys also tried to give us some messages: They (Oracle, but also the JCP) take care of Java, and developers shouldn’t worry about its future. Oracle’s behavior towards Java is all about transparency. First of all, Open JDK is now the reference implementation of Java. But the most important point is the new JCP 2.8 process, which made all JCP discussions public, and the arrival of new members such as Twitter, and London JUG and SouJUG (Brazil) in executive committees. We can hope that it will allow a lighter mood than last year (with Apache’s resignation). Talking about mood, a good moment to feel opinions differ was the debate we watched on the last day. It involved Mark Reinhold and Brian Goetz from Oracle, Joshua Bloch from Google, Emmanuel Bernard from JBoss, Ben Evans from London JUG and Brian Prince from Microsoft. A lot of questions were really controversial and although most answers were quite evasive, we had a glimpse about the cold war between these companies : First, it’s always interesting to see different companies’ philosophies, from Oracle, very corporate, Google, open minded with a startup spirit, to JBoss, with a full open-source model. It’s also really unfortunate to see how bad the relations between Oracle and Google are, and to see Oracle guys acting as Android simply doesn’t exist even if it is now the most prevalent mobile OS. Apart from Java EE and Java SE, some other hot topics were addressed : Java on the Cloud While Java EE 7 should bring standards for a Java PaaS platform next year, these platforms are already here today, and seem really promising: Cloud Foundry from Spring Source and OpenShift from Redhat/JBoss. Cloud Foundry is available in beta since april 2011. It provides an open PaaS platform under Apache license and offering multi languages support : Java, Ruby, NodeJS … Cloud Foundry can be used either on CloudFoundry.com or any other commercial provider, either on your private cloud, or locally for development purpose with Micro Cloud Foundry. It provides also integration with several third party technologies, and can be fully integrated with Spring Framework. OpenShift is a promising concurrent to Cloud Foundry, using JBoss and Redhat products. We had a great demo, especially on how to manage applications with JBoss Forge and Git. Some differences from Cloud Foundry are that it is for now restricted to Java, not open sourced yet and that you can’t instantiate it in a private cloud. A new generation of application servers They made really great efforts in the new version of JBoss Application Server 7. We tested it, and it’s just amazing: fast start (1,7s), low memory usage, fast deploy and undeploy, easy to configure, and testable. It sounds like other application servers such as Websphere or Weblogic are now far far behind: using this open source server for plain Java EE applications should be considered more than ever. Other languages on the JVM Most of you already heard about Scala or Groovy. We will now have to count with Clojure, Ceylon, Kotlin and Fantom which are all new languages on the JVM. Why in the world is everyone creating a language? It seems that everyone now agrees that the JVM is an incredible piece of software but that Java still has space for improvement. All of them are trying to solve something. Pick you favorite: less line of code, more modularity, having functional programming, make it easy to do parallel programming… My personal guess is that we haven’t found the real “Java killer” yet. But all these great language experiments will certainly give good ideas to its inventor. Meanwhile, these languages are to consider. They might help you on your current project. We had the “silver bullet framework”, now it’s time for the “silver bullet language”. Use them wisely. Java & NoSQL This year, we saw 2 emerging projects with quite the same goal, but different approaches: Spring Data and Hibernate OGM. Both of them try to give an abstraction layer on top of NoSQL databases for Java application, particularly for the really tough task which is data denormalization (Chris Anderson showed used really well how hard this point can be). Where Spring Data started a different project for each type of NoSQL database, Hibernate OGM tries to apply JPA concepts on top of NoSQL databases, starting with Infinispan (an in memory datagrid solution). This approach is quite ambitious but they take the problem step by step: for now, they only handle entity modeling and simple JPQL queries. Web and HTML5 HTML5 is a really hot topic, and we saw some very appealing demonstrations. While HTML5 isn’t fully standardized yet, most web browsers already support it, except from Internet Explorer. With HTML5, we can now easily do web offline, game programming, use a webcam and microphone, do voice recognition, but we also have many simplifications in HTML tags. Google is pushing hard on it. For instance, Paul Kinlan (from Google) gave us a great demo around webkit and Chrome HTML5 features. Linked to that, everyone is also trying the make it easier to code in the browser. It could be the old way, but easier (jQuery) and the new way (Dart… yes, even Javascript just got competition). Mobile & Android As we were five Java-ists we haven’t attended much sessions around mobile, but the Android keynote by Google was clear: the future is mobile. Everything is moving to the mobile so you, developers, should start to move to where the business will be. And you should hire a designer for you GUI… Conclusion We are living in interesting time my friends. Java and its developers took a five years vacation (healing from the generics?) but now they’re back. Jump in the train and keep learning. Because they are moving, they are moving fast. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Java . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Devoxx 2011, the main trends” Svend 04/12/2011 à 02:35 Hi,\r\n\r\nThanks for this post!\r\n\r\nGiven the momentum that Google is bringing to the (mobile) Java (and non-Java) world nowadays, I feel it is really a pity that the current official leader of the language is Oracle. Google appears to me as the equivalent of Sun ten year ago, big and innovative at the same time. The world would be so different if they instead of Oracle had acquired Sun.\r\n\r\nJava and JEE appear to me now as heavy and sluggish boats, capable or carrying huge loads but so slow when it comes to adapting direction. On the other hand it's really great that the new kids on the block like Scala or Clojure and the other languages you mention are available on the JVM now, it compensates a bit for the inertia of Java.\r\n\r\nAt the moment when JEE (7) is only specifying cloud-compatible features, many PAAS great offering are already there, including Google's GAE, Springsource's (sorry, I mean VmWare) CloudFoundry and Heroku. By the time JEE 7 specs will finally be out of the door and implementations will be available, clearly Google and SpringSource (oups, sorry again...) will already have moved on to the next step.\r\n\r\nAnd then there's mobile...\r\n\r\nVery exciting time to be in IT indeed :-)\r\n\r\nPS: IHM is a French-language acronym... Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-12-03"},
{"website": "Octo", "title": "\n                Getting from shell to Puppet            ", "author": ["Gabriel Guillon"], "link": "https://blog.octo.com/en/getting-from-shell-to-puppet/", "abstract": "Getting from shell to Puppet Publication date 20/02/2012 by Gabriel Guillon Tweet Share 0 +1 LinkedIn 0 After this (french) article , dealing with managing servers with shell scripts (what we were doing), and this (also french) one , which dealt with tools for automated deployment (what we planed to do), including Puppet, here is the article about Puppet. By doing. With blood, tears, and victories ;) Because yes, going from servers managed by shell scripts to Puppet, when you don’t know Puppet, it’s not so easy. Where do we come from On the project we have five environments of three computers each: one web server (Apache), one app server (Tomcat), one database server (PostgreSQL). Plus one ftp server, plus two monitoring servers (Zabbix)(which host a web server and a database server) and deployment server (OCS). Plus a demo server. Plus two servers for an other project (which host the same components). To summarize, we have 17 servers almost identical, plus 4 atypical ones. We use shell scripts (bash) to install those servers : Core configuration (packages, users, DNS, environment variables, monitoring daemons, …) Installation of base softwares, without their configurations: they are shipped with the applications. And we use one other shell script to check whether everything goes well, and a little bit of glue (Makefile, home made Perl modules, lib directory for shared shell functions, meta-configuration files, …) To be humble, we kind of master shell. It’s a pleasure to solve problems by some well crafted shell lines. For powerful they are, those install scripts hit limits. They : Are not idempotent: run twice, the same install script can break things. Do not handle software upgrades : getting from Postgresql 8.4.3 to Postresql 8.4.7 is not like installing 8.4.7 from scratch. Are too much project-dependent : they are built for one project. Installing a server for another project requires to modify the scripts. Are not dev-compliant : we want to be devops, we want the developers to install servers/VM. It’s possible with scripts but all the devs do not know shell, right ? But they play with configuration files all the time, so what if installing a server/VM was just a matter of tweaking configuration files ? Dev could then install a full server/VM ;) We have two choices: Continue with scripts, with the risk they might become fat and impossible to maintain. Go on with serious soft : Puppet/Chef/cfEngine We choose Puppet for those reasons: A PoC was made, and it shows that it can fit (and provide us a bootstrap to start with) We ear about Puppet all around us We don’t know Puppet :) So we will puppetize our scripts. Migration Puppetizing our scripts means that we will destroy almost everything we have been working on for more than a year, and start building again. It’s an effort. Agile teaches us not to be attached to the code (if you are, refactoring will be a pain for you), but it pinches the heart. Mourning done, it’s time to learn Puppet. At this very moment, troubles begin… The idea behind Puppet is ‘convention over configuration’. It means that you specify less but you have to learn more things (the “conventions”). And there is a lot of things to learn… Puppet works with contracts: you specify them, in its Ruby-like language, what you want, and it handles. I insist: you specify, it makes. It means that you do not do. Which means that if you were used to code (to act to obtain something), it’s not the case anymore : you specify a target, it reaches it. You don’t really know how, by the way… This way of doing leads to a different way of thinking, and it’s disturbing. It’s not easy to get into it when you didn’t see / learn it before, which was indeed my case: I didn’t find this paradigm in any other language I learned (by myself). We started with simple things, things that Puppet is good at: creating users, playing with system files, installing packages. The web is full of documentation about how to do such things. About the documentation… I rarely saw such a badly organized documentation. I always have to watch at least three web pages before finding what I’m looking for: is it a metaparameter? a function? a type? a fact? something else? With time, the you’ll get into the syntax of Puppet. But more because you remember it than because it’s logical: a bit like memorizing a text without any sense (and for everything else there is this very well done document ) Once simple things are puppetized, we tried less simples tasks: downloading a package from Nexus, running it if it’s a binary, installing it if it’s an rpm, creating a database if it’s doesn’t exists, creating corresponding roles, updating pg_hba.conf and .pgpass. It’s seems obvious that Puppet doesn’t follow the Perl motto (turn complicated things into simple ones, keep simple what is it) and making Puppet do what it’s not good at is a pain: we had to fight with it to make it do what we want. And to my opinion it’s a failure when I have to write shell into a program to make it do what I want… Add to this the obscures (obfuscated ?) error messages that almost never show you the root cause of the problem, and you easily understand that I was angry with Puppet. To summarize, here is what I blame Puppet for : It is slow. I made my tests in a VM, and it’s damn slow. Syntax is obscure : why keywords are sometimes in camelCase, sometimes not? Why sometimes brackets and sometimes curly brackets? Why ‘owner’ and not ‘uid’? Why should I always have to specify the ‘title’? Why do I have to sometimes put colons, sometimes not? Why sometimes ending a line with semi-colon, sometimes with a coma, sometimes with nothing? Why does Puppet sometimes accept coma at the end of the lines, sometimes not? Why ‘title’ and ‘name’ ? Why do facts and user defined variables have the same syntax? Why ‘source’ LOOKS LIKE a path but WITHOUT ‘manifests’? Why doing a syntax that LOOKS LIKE Ruby but is not? Do I have to use classes or nodes? The documentation, badly organized. Error message are far from being clear : instead of saying that the variable is undef can’t you suggest me it could come from an uninitialized plugin? I never learned Ruby or programming by contract, the fee to Puppetland is expensive. Some hints A pyramid of classes Migrating from our shell scripts to Puppet has been done gradually: we have built one module per functional component to replace: Tomcat, Postgresql, Apache, … We made much more functional component than we expected at first: pki, sudo, user (to create users but also to install scripts in their ~/bin), <component>db (for the database of those components), ntp, logrotate… The  documentation of Puppet tells us that the ‘node’ directive has to be used, but we used ‘class’ to pass parameters (such as password) from classes to classes. Thus, we are able to change the password of all classes in one move. So, we have grouped those components in classes, and stacked these classes. To Puppetize a server, we just had to assign the classes to this server, et voilà . class basenode ($user_hash=undef, monitored=true) {\r\nclass { 'basesystem': }\r\nclass { 'puppet': }\r\nclass { 'nexus': }\r\nclass { 'hosts': }\r\nclass { 'user': hash => $user_hash }\r\nclass { 'env': }\r\nclass { 'ntp':        }\r\nclass { 'ssh_keys': }\r\nclass { 'sudo': }\r\nclass { 'baseapp': }\r\nif $monitored { class { 'monitorednode': } }\r\n}\r\nclass dbnode ($min_sec=undef, $i_pass=undef, $mo_pass=undef, $g_pass=undef, $pgversion='x.y.z-1', $backup_retention_days=undef, $monitored=true ) {\r\n  class { 'basenode': monitored => $monitored}\r\n  class { 'pgsql': log_min_duration_statement => $min_sec, pgversion => $pgversion, backup_retention_days => $backup_retention_days }\r\n  if $monitored { class { 'pgsql::zabbix': } }\r\n  class { 'pgsql::tools': }\r\n  class { 'db': i_pass => $i_pass, mo_pass => $mo_pass, g_pass => $gestion_pass}\r\n  if $monitored { class { 'db::zabbix': } }\r\n  class { 'pshops': }\r\n}\r\nnode 'db1'  {\r\n  class { 'dbnode': min_sec => '1000', i_pass=>'hop', mo_pass=>'zou', g_pass=>'bla'}\r\n} In this example, db1 is a dbnode, which itself contains basenode. And some parameters are passed to specify passwords. Whilst we built our classes and put our servers into them, we progressively deleted our installation scripts. Wrapping We dared wrapping some Puppet’s directives to suit our needs : define user::userfile (\r\n  $basedir = \"/home/${user::username}\",\r\n  $source  = '',\r\n  $content = '',\r\n  $mode    = '0644',\r\n  $owner   = $user::username,\r\n  $group   = $user::username,\r\n  $replace = ''\r\n)\r\n{\r\n  file { \"${basedir}/${title}\":\r\n    owner  => $owner,\r\n    group  => $group,\r\n    mode   => $mode,\r\n}\r\n  if $source  { File[\"${basedir}/${title}\"] { source  => $source } }\r\n  if $content { File[\"${basedir}/${title}\"] { content => $content } }\r\n  if $replace { File[\"${basedir}/${title}\"] { replace => $replace } }\r\n} This function puts a script in the ~/bin of the user, with correct rights. It’s avoid us to repeat ourselves (DRY :) ) Nexus Some of our binaries installers are in Nexus. And guess what? Puppet didn’t know how to handle such a thing. We had to teach it : nexus/init.pp : class nexus {\r\n  $repo       = 'thirdparty'\r\n  $target_dir = ''\r\n  $user       = 'user'\r\n  $pass       = 'pass'\r\n  $server     = 'some.server.tld'\r\n  $port       = '7853'\r\n  $url        = \"http://${user}:${nexus::pass}@${server}:${port}/nexus/service/local/artifact/maven/content\"\r\n} nexus/get.pp : define nexus::get (\r\n  $groupeid,\r\n  $artifactid,\r\n  $version,\r\n  $type,\r\n  $classifier = '',\r\n  $mode       = '0644'\r\n)\r\n{\r\n  include nexus\r\n  $full_url = \"${nexus::url}?r=${nexus::repo}&g=${groupeid}&a=${artifactid}&e=${type}&v=${version}&c=$classifier\"\r\n  exec { \"download_via_nexus ${title}\":\r\n    command     => \"mkdir -p \\$(dirname \\\"$title\\\") ; /usr/bin/wget -q \\\"$full_url\\\" -O \\\"${$title}\\\" ; /bin/chmod ${mode} \\\"${$title}\\\"\",\r\n    path        => [ '/usr/bin','/bin'],\r\n    timeout     => 0,    unless      => \"test -e \\\"${title}\\\" &&  test `/usr/bin/wget -q \\\"${full_md5_url}\\\" -O -` = `md5sum \\\"${title}\\\" | /bin/cut -d' ' -f1`\",\r\n  }\r\n} nexus/exec.pp : define nexus::exec (\r\n  $groupeid,\r\n  $artifactid,\r\n  $version,\r\n  $classifier = '',\r\n  $type = '',\r\n  $command = 'echo \"I should do something with \\\"<%= title %>\\\"\"',\r\n  $purge_after = false,\r\n  $mode = '0644',\r\n  $creates = '',  $unless = '',  $onlyif = '',\r\n  $path = ['/bin', '/usr/bin']\r\n)\r\n{\r\n  nexus::get {$title:\r\n    groupeid => $groupeid,\r\n    artifactid => $artifactid,\r\n    version => $version,\r\n    type => $type,\r\n    mode => $mode,\r\n    classifier => $classifier\r\n  }\r\n  $real_command = inline_template($command)\r\n  exec { \"exec_from_nexus ${title}\":\r\n    command   => $real_command,\r\n    path      => $path,\r\n    logoutput => true,\r\n    timeout   => 0,\r\n    require   => Nexus::Get[\"$title\"]\r\n  }\r\n  if $unless  { Exec[\"exec_from_nexus ${title}\"] { unless => $unless } }\r\n  if $onlyif  { Exec[\"exec_from_nexus ${title}\"] { onlyif => $onlyif } }\r\n  if $creates { Exec[\"exec_from_nexus ${title}\"] { creates => $creates } }\r\n} nexus/extract.pp : define nexus::extract (\r\n  $repo = thirdparty,\r\n  $groupeid,\r\n  $artifactid,\r\n  $version,\r\n  $type = '',\r\n  $classifier = '',\r\n  $target_dir = '',\r\n  $owner = $user::username,\r\n  $group = $user::username,\r\n  $extract_cmd = \"tar --owner ${owner} --group ${group} -C '${target_dir}' -xz\",\r\n  $creates,\r\n  $nexus_url = 'http://user:pass@some.serveur.tld:7853/nexus/service/local/artifact/maven/content'\r\n)\r\n{\r\n  $full_url = \"$nexus_url?r=$repo&g=$groupeid&a=$artifactid&e=$type&v=$version&c=$classifier\"\r\n  exec { \"extract_from_nexus ${title}\":\r\n    command => \"/usr/bin/wget -q '$full_url' -O- | ${extract_cmd}\",\r\n    path    => ['/bin', '/usr/bin'],\r\n    unless  => \"test -d '${creates}'\",\r\n    timeout => 0,\r\n    require => File[$target_dir]\r\n  }\r\n\r\n  exec { \"ensure correct u:g to ${creates}\":\r\n    path    => ['/bin', '/usr/bin'],\r\n    command => \"chown -R ${owner}:${group} '${creates}'\",\r\n    unless  => \"test `stat -c '%U:%G' '${creates}'` = '${owner}:${group}'\",\r\n    require => Exec[\"extract_from_nexus ${title}\"],\r\n  }\r\n} Look at the shells commands we had to use here and there to make Puppet act the way we want: do not download things twice, put things in the right directory, put the correct rights, … Shell The most tricky part was to make Puppet install Postgresql. We install Postgresql from binaries (that were downloaded from Internet and put in Nexus), and we wanted Puppet to handle not only installation but also update from minor to minor (x.y.z -> x.y.z1) and from minor to major (x.y.z -> x1.y1.z1), thus with dump and restore. As Puppet is not shipped with this functionality, we had to take a weapon of mass destruction : shell. nexus::exec { \"/var/cache/puppet/pgsql-${pgsql::params::version}\":\r\n  groupeid   => 'org',    artifactid => 'postgresql',\r\n  version     => \"${pgsql::params::version}\",\r\n  classifier => \"linux-x64\",\r\n  type       => 'bin',\r\n  mode       => '0755',\r\n  purge_after => false,\r\n  path        => [ '/bin','/sbin','/usr/bin','/usr/sbin' ],\r\n  command     => $minipgsqlversion ? {  # Depending on the installed version ...\r\n    # Same major as it's asked to me (x.y.z vs x.y.z1) : update without dump\r\n    $pgsql::params::miniPgVersion  => \"bash -c 'if test $pgsqlversion != ${pgsql::params::dirPgVersion} ; then echo -e \\\"\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nn\\\\n\\\" | <%= title %> --prefix ${pgsql::params::prefix} --datadir ${pgsql::params::datadir} --locale ${pgsql::params::loc} --mode text --servicename ${pgsql::params::service} --superpassword ${pgsql::params::pass} ; else /bin/true ; fi'\",\r\n    # Nothing found : installation\r\n    'notfound'   => \"echo -e \\\"\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nn\\\\n\\\" | <%= title %> --prefix ${pgsql::params::prefix} --datadir ${pgsql::params::datadir} --locale ${pgsql::params::loc} --mode text --servicename ${pgsql::params::service} --superpassword ${pgsql::params::pass}\",\r\n    # Default case (x.y.z vs x1.y1.z1) update with dump\r\n    default       => \"bash -c 'name=/opt/pgsql-$pgsqlversion-to-${pgsql::params::dirPgVersion}-\\$(date '+%Y%m%d-%H%M%S').sql.gz ; /usr/local/postgresql-$minipgsqlversion/bin/pg_dumpall -U postgres | gzip > \\$name ; service postgresql-$minipgsqlversion stop ; echo -e \\\"\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nn\\\\n\\\" | <%= title %> --prefix ${pgsql::params::prefix} --datadir ${pgsql::params::datadir} --locale ${pgsql::params::loc} --mode text --servicename ${pgsql::params::service} --superpassword ${pgsql::params::pass} ; service ${pgsql::params::service} start || /bin/true ; gunzip -c \\$name | ${pgsql::params::prefix}/bin/psql -U postgres '\",\r\n  }\r\n} Again, look at the deployed arsenal. Here is the facter that provides the Postgresql version : Facter.add(\"pgsqlversion\") do\r\n  setcode do\r\n    psql=Dir.glob(\"/usr/local/*/bin/psql\").sort[-1]\r\n    if psql\r\n      %x(#{psql} --version).split(\"\\n\")[0].split()[-1]\r\n    else\r\n      'notfound'\r\n    end\r\n  end\r\nend\r\nFacter.add(\"minipgsqlversion\") do\r\n  setcode do\r\n    psql=Dir.glob(\"/usr/local/*/bin/psql\").sort[-1]\r\n    if psql\r\n      %x(#{psql} --version).split(\"\\n\")[0].split()[-1].gsub(/\\.\\d*$/,'')\r\n    else\r\n      'notfound'\r\n    end\r\n  end\r\nend Once built the Puppet’s configuration, we could sleep more peacefully at night. Benefits We didn’t use Puppet from the beginning of the project because we wanted to be fast, and we didn’t know Puppet. Moreover, the team was young and Puppet was not the key tools to beginning with (to be honest: it was shell) Thus, beginning by shell scripts was a good idea, but it hits limits I talked about hereabove. The puppetization of platforms was not painful only for me, but to the others teams too, in the good way : the psychorigidy of Puppet has shown us that some things weren’t clear on our servers (the uid of our main unix user was not the same everywhere, for example). It forced us to make things clearer and more explicit. Where we hesitated to run our installation scripts, we now ask Puppet to do it: happiness of programmation by contract is there: when something is done, it’s not to be done anymore, and you don’t care how it was done (except in some rare cases). When one thing is puppetized (the installation of Tomcat, for example) we have no more doubts: it’s installed in the same way everywhere, and it works. Installing a new server takes 10 minutes: 30 seconds to install the core stuff (which basically renames the server and installs Puppet) and 9min and half to drink tea waiting for it’s over. Followed by the sweet feeling that things went well. Truly, we simplify our install and update process: everything is in one place (on the Puppet master), and one command line to type. Conclusion Installing servers with Puppet is a real plus compared to shell (yeah captain Obvious!). But Puppet is quite hard to learn. Our choice at the beginning of the project was to not start with it, for these reasons : It was important for the team to learn shell first, even if the lifetime of the scripts had to be short. According to us, adding Yet An Other Tool was not a good idea. The environment was small (15 servers at that time) and installed by our hoster. So installing the OSs by shell scripts was not unreasonable. Nowadays we think we have made the good choice : learning Puppet was not the first thing to do. But the second, just after learning shell :), since we achieved all of goals with Puppet. Some useful links : Puppet Cheat Code Sheet , what you always forget about the syntax of Puppet’s most commons resources. Puppet module s. Lots of Puppet modules. In the rare cases they don’t fit you, they will help you a lot. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged DevOps , Linux , Puppet , Shell , unix . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Getting from shell to Puppet” Guyo 23/02/2015 à 16:06 Really! I appriciate what you say. As you say with puppet any one who manage servers can sleep peacefully at night. I want Ocs server to run with puppet; how can I configure all necessary packages? Please inform me with my email \"guyo.mamo@ju.edu.et\" Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2012-02-20"},
{"website": "Octo", "title": "\n                Complex Event Processing            ", "author": ["Mathieu Despriee"], "link": "https://blog.octo.com/en/complex-event-processing/", "abstract": "Complex Event Processing Publication date 24/11/2011 by Mathieu Despriee Tweet Share 0 +1 LinkedIn 0 Today’s information society abounds in a myriad of information flows, computer-based human collaborations, software agent interactions, electronic business transactions, and the explosion of data on the Internet. Understanding what is happening in these environments is becoming increasingly difficult. In other words, we need to find the best ways to make sense of this wealth of data, to improve the quality and availability of information, and to ensure effective responses. Traditional storage and data analysis technologies are not adapted to this exponential growth of volume and event rates. In addition, the value of information may decay rapidly over time. For instance, some events that may help anticipate a production outage have no value once the outage happened. There is a need to process data as soon as events occur, with respect to latency constraints. We need to move away from traditional client-server (query-response) interaction models, to more asynchronous, event-oriented, loosely coupled push model, with applications able to take decisions based on events data. Complex Event Processing (CEP) is a set of technologies that allows exploring temporal, causal, and semantic relationships among events to make sense of them in a timely fashion. This article is the first of a serie exploring these technologies, their capabilities and possible applications. Some business use cases Use cases that may benefit from CEP are varied, but we can identify some patterns in them, showing the decisive contribution of this technology. Manufacturing Execution systems Event processing can be used in plants to detect anomalies or determine if significant changes require re-planning of production. Plant floor systems push events from numerous sensors to a centralized control system that will explore events patterns and emit new, aggregated, rich events to take decisions. Patterns: Active diagnostics of problems, Real-time operational decision Location-based services RFID tags, mobile phones, and Wi-Fi enabled devices feed information about their spatial location into server-side systems. Applications can be tracking goods in the supply-chain, or pushing information to a customer, based on his mobile phone location. Patterns: Information dissemination, Observation systems Algorithmic trading Heterogeneity of information sources and event rates impose an event processing approach on modern financial IT systems, in which quasi real-time market analytics can hardly be implemented in conventional client-server architectures. Patterns: Information dissemination, Real-time operational decision Telecommunications Near real-time data coming from telecom subsystems could be analyzed together with business data from IT systems, or with historical data. With the use of predictive models, fraud detection can be improved. Patterns: Predictive processing, Real-time operational decision eCommerce Clickstream analysis helps in optimizing user experience on commercial web sites, to adapt advertising, or drive page layout. This requires low latency decision, with immediate pattern recognition. Patterns: Real-time operational decision Power Utilities The utility sector requires an efficient infrastructure for managing electric grids and other utilities. This requires immediate response to variations in consumption, using events coming from numerous data sources, aggregated along the grid. Patterns: Real-time operational decision, Active diagnostics, Information dissemination Criteria for adopting a complex event processing approach “It’s all about time” ! The word “ complex ” in CEP refers mainly to the complexity of state management over time while processing the events. Typical examples are: Calculations over sliding windows Correlation of events along time, such as: determination that event X occurs before event Y within a precise timeframe ; or determination of a non-occurrence within a timeframe. Most CEP implementations also provide advanced pattern detection, such as a non-deterministic finite state automaton, similar to a regular expression search over a flow of events, with influence of time in the search. Another key influence of time is timeliness. Timeliness is the ability to handle events and produce output in a constrained time. It can be seen as end-to-end latency, and can reach the milli-second scale with CEP, or below (cf [perf 1, 3]). CEP tools also provide the ability to arbitrate between guaranteed time and correctness of output (eg. waiting or not for late or unordered events). Then come event volumes and rates : CEP tools performance can exceed 10’000 and even reach 100’000 events/s [perf 1, 2]. Other complexity factors can also motivate a move towards CEP technologies as well : number and type of event sources if the application is expected to change over time (new event sources, new interactions and responses) richness of information in output events: such as counts, averages, composition of events from different sources context dependant situations: eg. detection of events occurring within a defined spatial distance, or within a defined group of customers (possibly querying an external systems to determine the context) correlation of real-time data with historical data intelligence in event processing: eg. inference models or machine learning capable models This table sums up the areas in which event processing could particularly fit (from [Chandy et al 2011]) : Event rates Application complexity (time, state, context) Timeliness High High High High High Low High Low High Low High High In other cases, more traditional messaging systems and/or transactional systems may be more suited than CEP. Choosing a CEP technology CEP Market – March 2011 There are various vendors, having different approaches and paradigms in their event processing products. We can identify the following paradigms [Helmer et al 2011] : Paradigm Possible applications Event stream oriented and query based, this can be seen as a continuous query running on an infinite flow of data Well suited for aggregation of event data, with SQL-like join logic (between events within the flow or with external DB) ECA (event/condition/action) rule based, this approach having ancestors in active database paradigm (eg. triggers in database) Well suited in scenarios where business users should be able to define event patterns, by composing simple rules Inference rule based, with similarities to what can been seen in BRMS Well suited when actions have to be taken when certain states are reached ; or in business activity monitoring context with real-time decisions Time-state machine based Well suited in monitoring situtations, but with a well defined finite state space One can see strong similarities with more traditional technologies, ranging from BRMS, to versatile messaging systems (JMS), or EAI. Beside the ability of CEP to handle very high rates of incoming events, CEP above all brings a coherent set of specific features. Here is a list that may help you refining your need around CEP : Functional capabilities: Data Reduction : Filtering, Projection (discarding some attributes), Aggregation over time windows Modeling capabilities for event shape and payload, and more generally for query logic Reasoning: Transformation (eg. enrichment, change of shape…), Pattern detection (including detection of the absence of an event) Time handling : event timestamps, intervals of occurrence, particularly with respect to time windows and pattern detection, sliding time windows (or other kind of moving windows) Context awareness : taking into account the context in which the event occurs, capability to query external systems (eg. database with business data, or historical data) Logging and analysis : for audit purposes, or retrospective event processing (understanding precursor events that led to a particular output event) Prediction, learning and adaptation : pattern discovery, scoring against a data-mining model, or machine-learning capabilities Presence and capabilities of an integrated development environment (IDE) Non-functional capabilities : Input and output connectivity to event sources and event sinks Routing (defined statically or dynamically) and partitioning for workload distribution Performance in response time (end-to-end latency) Predictability : low variance of latency (e.g. 95-percentile with guaranteed latency), acceptable rate of incoming events before latency increases Scalability and elasticity Availability and recoverability : fault tolerance, ability to recover, continuous operation (applying change while the application is running) Consistency and integrity in a distributed system : management of temporal granularity and potential network delays, clock synchronization Security and privacy : segregation of event streams, and user habilitations Usability, Maintainability, Manageability : depending on the kind of users that will be implementing and maintaining the logic (developers and/or business analysts) Conclusion and future work CEP is more an approach than a technology. On the contrary, there are several types of implementations available, and an even greater number of products on the market. Given the increasing importance of real-time information processing, choosing the best solution for your needs is not an easy task. For this purpose, our next series of articles on CEP will explore several CEP products in detail, and expose their key features. Bibliography [Chandy et al 2011] The event processing manifesto, 2011 Authors : Mani K. Chandy ; Opher Etzion ; Rainer von Ammon [Grabs et al 2009] Introducing Microsoft StreamInsight, 2009 Authors : Torsten Grabs, Roman Schindlauer, Ramkumar Krishnan, Jonathan Goldstein [Helmer et al 2011] Reasoning in Event-Based Distributed Systems, 2011 Authors : Sven Helmer, Alexandra Poulovassilis, and Fatos Xhafa [perf 1] Sybase Aleri performance http://m.sybase.com/files/Data_Sheets/SybaseAleri_CEPPlatform_PerfTesting_ds.pdf [perf 2] Esper performance http://esper.codehaus.org/esper/performance/performance.html [perf 3] StreamBase performance at QCon 2011 http://qconlondon.com/dl/qcon-london-2011/slides/RichardTibbetts_ComplexEventProcessingDSLForHighFrequencyTrading.pdf Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged CEP , Complex event processing , Event Sourcing , events . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-11-24"},
{"website": "Octo", "title": "\n                Introduction to Datastax Brisk : an Hadoop and Cassandra distribution            ", "author": ["Jordan Pittier"], "link": "https://blog.octo.com/en/introduction-to-datastax-brisk-an-hadoop-and-cassandra-distribution/", "abstract": "Introduction to Datastax Brisk : an Hadoop and Cassandra distribution Publication date 29/07/2011 by Jordan Pittier Tweet Share 0 +1 LinkedIn 0 As the Apache Hadoop ecosystem grows while its core matures, there are now several companies providing business-class Hadoop distribution and services. While EMC, after it acquires Greenplum, seem the biggest player other companies such as Cloudera or MapR are also competing. This article introduces Datastax Brisk, an innovative Hadoop distribution that leverage Apache Hive data warehouse infrastructure on top of an HDFS-compatible storage layer, based on Cassandra. Brisk try to reconcile real-time applications with low-latency requirement (OLTP) and big data analytics (OLAP) in one system. “Oh really ?” Introduction Usually systems designed for OLAP and OLTP require different data models and OLTP systems need to be regularly offloaded because they don’t cope with huge amount of data. Thus the need for ETL. But ETL processes are usually complex and not flexible, they have to be adapted each time the source or destination system slightly change. Lastly they are not well-suited for small but numerous datastreams, which is where we are heading with real time analytics. If you were to design a unique system that could handle both low-latency queries and big data analytics you would need some kind of resources isolation so that a big batch running on your data doesn’t impact the response time of your application. Let’s see how Brisk handle these challenges. Brisk Architecture In the typical Hadoop Stack (see  our related blog posts ), the primary datastore is Hadoop Distributed FileSystem, aka HDFS. HDFS fits well MapReduce jobs thanks to data locality awareness but this comes at the expense of a major SPOF, the NameNode metadata server. Although the secondary NameNode server mitigates data loss risk, the failover time is in dozens of minutes. With Brisk, the HDFS layer is replaced by a Cassandra-based filesystem. Similar to HDFS, large files are split in chunks (more precisely, in “rows”) but stored in a column family. Data replication and consistency tradeoff are handled as in every column family in Cassandra. Locality awareness comes “for free”, each data chunk has a key mapped in Cassandra’s Distributed Hash Table : every node of the cluster knows where each piece of data is located. What’s great with CassandraFS is it transparently replaces HDFS, whether you access HDFS through the native Java API or through the CLI, you don’t have to change your existing code. A typical Brisk cluster is comprised of two types of nodes. Some nodes are dedicated to do analytics while others only deal with low-latency queries. Both types of nodes run Cassandra but analytics nodes also run an Hadoop TaskTracker to execute long-running MapReduce programs. Cassandra-only nodes operate solely as a NoSQL datastore. Hadoop nodes and Cassandra-only nodes make two different groups inside the same Brisk cluster. If you are not familiar with Cassandra datamodel and its OLTP capabilities, now is the time to have a look at the “Play with Cassandra” blog posts series. Data stored in the CassandraFS is actually stored in a keyspace handled only by Hadoop nodes. There is a JobTracker node, automatically elected among Hadoop nodes, which try to schedule job on TaskTrackers so that they work only with local data. If the JobTracker fails, you can manually force another Hadoop node to be the new JobTracker. Things get a bit more complicated when you want to run MapReduce jobs on data stored in a regular keyspace. Part of the data could be assigned to vanilla Cassandra (remember that cassandra-only nodes and Hadoop nodes belong to the same “ring”/cluster). Possibly, tons of data need to be read (for analytics purpose) from nodes also facing real time requirements. Brisk provides an elegant solution to the resource isolation problem thanks to Cassandra native replication ability. The trick is to : 1) Replicate the keyspace so that the Hadoop nodes group owns a full copy of the data stored in the keyspace. 2) Tell the Hadoop nodes to read data using only the copy and thus avoiding to hit the cassandra-only nodes. In the end, the architecture looks like : Details on write replication A keyspace with these two properties (replication and “in your group”/local queries) can be created through cassandra-cli : create keyspace myKS with placement_strategy='org.apache.cassandra.locator.NetworkTopologyStrategy' and strategy_options=[{Brisk:1, Cassandra:1}]; That’s it. In this example the replication factor has been set to 2 (1+1) : one full copy of the data lays on each nodes group. The placement strategy tells the data routing mechanism to satisfy the consistency level of the request (read or write) favoring nodes “close” to the request issuer. In Brisk, when a node joins the cluster it will automatically declare itself as part of the “Brisk” virtual DC or Cassandra virtual DC depending on its role (OLAP or OLTP, role is configurable in /etc/init.d/cassandra or /etc/default/brisk file). That’s why Cassandra-only nodes are concidered far-away (in another DC) from Hadoop nodes, thus Hadoop nodes avoid reading data from them. Long story short, replication, virtualDC and “network topology aware” strategy achieve the resource isolation property. Likewise, if your MR job writes its results in a column family of the myKS keyspace, these results will eventually (as in eventually consistent, depending of your write consistency level) be copied on the Cassandra-only nodes group. Dont write too much data or the replication process could adversely impact the response time of your Cassandra-only nodes. With Brisk, you program your MapReduce jobs as you would with a typical Hadoop stack. Unfortunately Hadoop streaming is not supported so you are left with no choice but Java. Example of Mapreduce on Cassandra In the following paragraphs I will go through the canonical WordCount example, but the input “text corpus” is stored in a regular Cassandra keyspace (instead of CassandraFS or HDFS) and the result will also be put back in Cassandra. First thing first, we need a new keyspace and column family to store our text corpus. We also need a column family to store the output of the MR job. In the cassandra-cli (be sure Brisk was started with “Hadoop enabled”) : create keyspace wordcount with strategy_options=[{Brisk:1, Cassandra:0}];\r\nuse wordcount;\r\ncreate column family input_words with comparator = AsciiType and default_validation_class = AsciiType;\r\ncreate column family output_words with comparator = AsciiType and default_validation_class = AsciiType; This keyspace will be stored only on the Hadoop nodes of the cluster, this is the meaning of the “strategy_options” parameter. None of the operations to come will involve Cassandra-only nodes. Next, we need some input texts. I chose the Bible (old and new testaments) because it’s quite big and available in text format here. https://sites.google.com/site/ruwach/bibletext Get it and unzip it. The following python code snippet inserts each line of the Bible as a row in the input_words column family. import sys,pycassa\r\nfrom string import strip\r\npool = pycassa.connect('wordcount', ['localhost:9160'])\r\ncf = pycassa.ColumnFamily(pool, 'input_words')\r\nb = cf.batch(queue_size=100)\r\ni = 0\r\nfor line in sys.stdin:\r\ni += 1\r\nb.insert('line'+str(i), {'bible' : strip(line)}) We execute it and insert the data : find . -type f -name \"*.txt\" -print0 | xargs -0 cat | sed \"s/^[0-9 ]*//\" | python myscript.py The MR program is available here : github repo It’s the classical “wordcount” example. The following lines are specific to Brisk and are put in the run method of the job, where the job setup is done : // Tell the Mapper to expect Cassandra columns as input\r\njob.setInputFormatClass(ColumnFamilyInputFormat.class);\r\njob.setOutputFormatClass(ColumnFamilyOutputFormat.class);\r\n// Set the keyspace and column family for the output of this job\r\nConfigHelper.setOutputColumnFamily(job.getConfiguration(), \"wordcount\", \"output_words\");\r\nConfigHelper.setInputColumnFamily(job.getConfiguration(), \"wordcount\", \"input_words\");\r\n\r\n// Set the predicate that determines what columns will be selected from each row\r\nSlicePredicate predicate = new SlicePredicate().setColumn_names(Arrays.asList(ByteBufferUtil.bytes(\"bible\")));\r\n// The \"get_slice\" (see Cassandra's API) operation will be applied on each row of the ColumnFamily.\r\n// Each row will be handled by one Map job.\r\nConfigHelper.setInputSlicePredicate(job.getConfiguration(), predicate); The code is compiled and run : /bin/rm -rf WordCountCassandra_classes ; mkdir WordCountCassandra_classes\r\nclasspath=. && for jar in /usr/share/brisk/{cassandra,hadoop}/lib/*.jar; do classpath=$classpath:$jar done\r\njavac -classpath $classpath -d WordCountCassandra_classes WordCountCassandra.java\r\njar -cvf /root/WordCountCassandra.jar -C WordCountCassandra_classes/ .\r\nbrisk hadoop jar /root/WordCountCassandra.jar WordCountCassandra Hopefully the job ended well and you can now ask “how many times does the word “advantage” appear in the Bible” by retrieving the column named “advantage” from the “bible” row of the “output_words” column family : Welcome to the Cassandra CLI.\r\nType 'help;' or '?' for help.\r\nType 'quit;' or 'exit;' to quit.\r\n\r\n[default@unknown] use wordcount;\r\n[default@wordcount] get output_words[ascii('bible')]['advantage'];\r\n=> (column=advantage, value=15, timestamp=1308734369159) A more realistic example would have been to do analytics on evolving data (such as tweet feeds) stored in a column family updated by a “real time” application (such as a tweeter client) and output the results in another column family (preferably in another “keyspace”, for ressource isolation reason). This way, you could do analytics on the latest data and let the results of the MR job be accessible to your application immediatly after they are available. Apache Hive in Brisk Brisk also includes Apache Hive, sligthly modified in order to work with CassandraFS. Hive provides a SQL-like syntax where queries are “translated” to MapReduce jobs. Hive really ease big data handling. Let’s see how with the continuation of the WordCount example. If the wordcount MR example went well, you now have an “output_words” Column family that looks like this : With this schema it’s easy to get the number of times the noun “aaron” appears in the Bible. But what if you want to know what is the most frequent word ? You would have to fetch all the columns, possibly write another MR job. Instead, we are going to use Hive which can response to this simple query in one line. Hive integration in Brisk is straigthforward. The syntax is the same as Hive on top of HDFS, but in Brisk, the data is actually stored in CassandraFS. It gets (a bit) tricky when you want to map an existing Cassandra column family (“output_words” in our example) into a Hive table. Especially if the name and the number of columns are dynamic. The following Hive-QL statement creates an Hive table whose data is stored in the Cassandra column family “output_words” and where each row is a 3-tuple [row key, column_name, column_value]. So, in this example, you end up with one row in the HiveTable for each distinct word in each text of the text corpus. hive> CREATE DATABASE WordCountDB;\r\n\r\nhive> CREATE EXTERNAL TABLE WordCountDB.output_words\r\n(sourcetext string, word string, count int)\r\nSTORED BY 'org.apache.hadoop.hive.cassandra.CassandraStorageHandler'\r\nWITH SERDEPROPERTIES ( \"cassandra.columns.mapping\" = \":key,:column,:value\" )\r\nTBLPROPERTIES ( \"cassandra.ks.name\" = \"wordcount\" );\r\n\r\nhive> select * FROM WordCountDB.output_words limit 5;\r\nOK\r\nbible a 8638\r\nbible aaron 323\r\nbible aarons 29\r\nbible abaddon 4\r\nbible abagtha 1\r\nTime taken: 0.549 seconds Finally, to get the 5 most frequent words in the bible the Hive query looks like : hive> select word, count FROM WordCountDB.output_words WHERE sourcetext='bible' AND length(word) > 3 ORDER BY count DESC limit 5;\r\nOK\r\nagainst 1591\r\nalso 1513\r\naway 1020\r\nafter 959\r\namong 881\r\nTime taken: 34.671 seconds Not bad hu ? Conclusion Brisk is more than just another Hadoop distribution. It has innovative features such as a distributed filesystem on top of Cassandra which act as a unique datastore for both low-latency queries and offline batch processing. It reduces the time between data creation and analysis thanks to “on the fly” data replication which suppress the need for ETL. It’s a Free OpenSource Software and benefit from professional support by Datastax. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Big Data , Consulting Chronicles and tagged bigdata , brisk , Cassandra , Hadoop , Map/Reduce . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-07-29"},
{"website": "Octo", "title": "\n                DevOps tips and tricks, on the ops side            ", "author": ["Arnaud Mazin"], "link": "https://blog.octo.com/en/devops-tips-and-tricks-on-the-ops-side/", "abstract": "DevOps tips and tricks, on the ops side Publication date 05/12/2011 by Arnaud Mazin Tweet Share 0 +1 LinkedIn 0 After applying as much as possible the DevOps principles for more than a year on the run of a highly business critical project of one of our customers, here are some golden rules we eventually found out and tried to stick with. Items are here presented from an Ops perspective. One deployment tool to rule them all Work hard on getting a nice deployment tool (scripts, capistrano, whatever) and use it to deploy on all your environments from development to UAT, perf and of course production. You’ll make sure you’ll apply the fail fast principle to deployment. Make sure it pulls all the artifacts it needs from your repo, Nexus or such. If deployment is painfull do it more often, if it’s fun, do it more often Deployment must not be considered as a failure nor as an exception. It’s fine, it’s a standard and a frequent part of the job, it shows you have worked to bring out new features, improved performances, bugfixes. Don’t be scared to deploy often, « practice makes perfect ». The more often you deploy, the smaller the changes are , the lesser the risks. Monitoring is for everyone From technical (sysadmins, developers, DBA, HelpDesk) to highly non-technical people (marketing, financial, top management), every one loves to see the heartbeat of the system. Propose several screens / dashboards to present the data to each perspective. Commit developers in every steps of the production life, and give them feedback, loads of it Make sure you can wake them up during a tricky update or even better, grab some extra pizza to make sure they stay during the upgrade process. Give access to all data they need to investigate, either in real-time on the production environment or at least by giving access to data / logs with as few delay as possible Use production-like database content, A.S.A.P. Real-life data often contain  stranger stuff than you could ever imagine, use them. Anonymize them (yes, you really want to avoid emailing real users during your tests), reduce their size if needed, but do keep historical data. They will help with UAT, perf testing, and will secure your production roll-out. Share tools and practices with Devs Kanbans, scrums, retrospectives, Jira, Confluence are probably used into your dev team, ritualize such principles on the ops side, you’ll share the same logic and delivery rhythm. Express needs, act as a P.O. when needed If you need something from the Devs that will help you make the run better / cheaper / faster, such as a better log classification, functionnal metrics to monitor, become a Product  Owner , propose user stories and make sure they take life. Negociate with the business to get a minimal amount of complexity points for ops user stories at each iteration. Automatize everything that can be (especially boring stuff) Script, script, script, keep your mind free from silly or time-consuming stuff, you have better things to do. Manage your systems automatically Use Chef , Puppet , cfEngine + a VCS (git, mercurial, SVN…) to store and historize your configurations. Embrace the Test-Driven Infrastructure Approach Try to make every change on your infrastructure testable, use the monitoring tool as a an automatic tool for it. Elect a goalie to keep other ops from task-switching This will ensure you that small tasks are handled as soon as possible and keep people working on long-term tasks focussed. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles and tagged Dev , DevOps , Ops , Production , run , tips , tricks . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “DevOps tips and tricks, on the ops side” Yves Caseau 17/12/2011 à 08:21 A great article and a useful list !\r\nI am becoming more and more found of devops: http://informationsystemsbiology.blogspot.com/2011/11/lean-it-devops-and-cloud-programming.html Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-12-05"},
{"website": "Octo", "title": "\n                Use Azure Blob Storage in your application            ", "author": ["Jan Fajfr"], "link": "https://blog.octo.com/en/azure-blob-storage/", "abstract": "Use Azure Blob Storage in your application Publication date 28/07/2011 by Jan Fajfr Tweet Share 0 +1 LinkedIn 0 This article will describe how to make use of Azure Blob Storage in order to add an “electronic vault” functionality to your application. This post follows the first one which made the introduction to Azure and described how to deploy existing application to Azure. Let’s recall the architecture of the application and see the changes which will be made while adding the connection to Azure Blob Storage. The application is already deployed to Azure, it is hosted by a Web Role and uses Azure SQL as its main data storage. Azure Blob Storage will be added to the application in order to create an “Electronic vault” for each user. Both the client as well as the server will have the possibility to interact with the Blob Storage. Electronic vault The electronic vault allows the user to perform the following: Upload, download and remove files in the vault Generate files and store them in the electronic vault (such as transactions or account overviews) In the mean time, the Vault should store metadata for each document indicating the time of the upload and the initiatior of the upload (or modification). Azure Blob Storage Before we describe how exactly the electronic vault can be implemented, lets take a look at the architecture and  possible usage of Azure Blob Storage. Each Azure Storage account can have several containers. In each container we can have several blobs and each blob can be composed of several blocks. The following diagram shows the structure of Azure Blob Storage the way it can be used to create a electronic vault for each user. Each user will possess one container, which will allow us to assign concrete rights to each container. Note that there is no notion of folders inside of each container, but there is a special naming convention which allows overcoming this issue. As you can see from the diagram, each file can be separated into several blocks. This is especially useful when treating large files. When file is separated into blocks, than the upload has three phases: first the list of block ids is sent, than each block is uploaded separately and at least a commit is send to the server. When some of the uploads fails, server can easily send the list of missing blocks to the client. The same applies also to download operation. This separation also allows parallel upload  by several clients. Architecture of Azure Storage All blobs are stored in a distributed file system in Azure fabric. However each blob belongs to one exact partition server. Each blob has a unique key which is composed of its container name and blob name. This key is used as partitioning key, which assigns the blob to one partition server. The access to each partition server is load-balanced and all the partition servers use a common distributed file system. Because each blob has a unique partition key, than in fact the access to each blob is load-balanced. In a blog post , which dates May 2010, the Azure Storage Team stated, that the targeted throughput for a single Blob was 60Mb/s. Since the aim here is not to give detail explanation of the architecture, you can refer to the Azure Storage Team blog for more details. Access to Blob Storage – APIs We can interact with Azure Blob storage using two possible API’s. .NET API REST API .NET API is a set of classes which can be used on the server side to access the storage. The following snippet shows, how to upload a file to a Blob container. var storageAccount = CloudStorageAccount.FromConfigurationSetting(\"connStr\");\r\nvar blobClient = storageAccount.CreateCloudBlobClient();\r\n\r\nvar container = blobClient.GetContainerReference(\"container\");\r\ncontainer.CreateIfNotExist();\r\nvar blob = blobContainer.GetBlobReference(\"fileName\");\r\nblob.UploadByteArray(data); You can see, that the code is quite straightforward. Note that the access key is part of the connection string which is passed to FromConfigurationSetting method. This API is not accessible for Silverlight clients. Thus if we want the Silverlight client to have a possibility to access the storage we  have two options: Expose the calls to API by WCF services or use the REST API. REST API is completely platform independent and uses HTTP as its transport protocol to execute actions, upload and download data from the storage. The creation of the HTTP request depends on the platform of the client. In C# we can use the following code: public void UploadFile(Uri url){\r\n  var webRequest = (HttpWebRequest)WebRequestCreator.ClientHttp.Create(url);\r\n  webRequest.Method = \"PUT\";\r\n  webRequest.Headers[\"x-ms-meta-comment\"] = \"my comment\";\r\n  webRequest.Headers[\"x-ms-meta-author\"] = \"current user\";\r\n  webRequest.BeginGetRequestStream(new AsyncCallback(WriteData), webRequest);\r\n}\r\n\r\nprivate void WriteData(IAsyncResult result)\r\n{\r\n  var webRequest = (HttpWebRequest)asynchronousResult.AsyncState;\r\n  var requestStream = webRequest.EndGetRequestStream(asynchronousResult);\r\n       //Write the binary data to the stream\r\n  webRequest.BeginGetResponse(new AsyncCallback(UploadFinish), webRequest);\r\n}\r\n\r\nprivate void UploadFinished(IAsyncResult result)\r\n{\r\n  if(result.IsCompleted){\r\n    //Check the response\r\n  }\r\n} Note that the metadata of the file (comments, authors) are added as headers to the HTTP Request. To get more details on the REST API, you can visit the official MSDN reference page. In our banking application, we use both APIs. To give the Silverlight client direct access to the storage we will use REST API. This will also avoid passing of all the data through WCF Services and take significant data load of our servers (or Web Roles). In case of generation of documents (such as transaction overviews), the documents will be generated and inserted to the vault using the server side API. Security Issues The access to Azure storage is protected by “storage access key” (Composed of 88 ASCII characters). Every storage account is provided with two keys (called simply “Primary” and “Secondary”) in order to enable renewing of the access keys and keeping zero down time (both of the keys allows access to the storage at the same time). The access key allows the access to the Azure Storage. When using the .NET API, the key is just provided to the API which internally uses the key to authenticate the calls to Azure. When using the REST API, each request has to be signed by the key. On the server side we can use the .NET API access the storage – the server has the key stored in it’s configuration files. However if Silverlight wants to talk directly to Azure Storage, it will need the key to sign all REST requests, so we would need to give the key to the Silverlight application. And that is the potential security problem. We cannot give the access key away to all Silverlight clients. Silverlight as a client is just compiled XAP package, which could be reverse engineered to obtain the key in the case of the access key hard coded inside the package. So how to allow the client application to directly access the Blob storage without giving  the key to the Silverlight client application ? One option would be to build WCF service (secured by SSL) which Silverlight could ask to obtain the access key after performing authentication against the server. However this way the key would be handed to all the clients and once the attacker would infiltrate only one client machine he would also obtain non-restricted access to the whole storage account. The second option which resolves this issue is to use Shared Access Signatures. Shared Access Signatures Shared Access Signature (SAS) is a temporal authentication token which enables access to concrete blob (or whole container).This way when the client desires to access the Blob storage, he will first contact the server and ask for Shared Access Signature. Server knowing the right information (the demanded blob or container) and being sure that the client is identified will generate the token. This token is then sent to the client using secured channel (such as WCF service secured by SSL). The client can than add this token to its upload or download request which will permit him to access the demanded files. Of course there is still the possibility for an attacker to obtain the Shared Access Signature, however the potential danger is much lower for two reasons: As said before Shared Access Signature is a temporal token The attacker would obtain the access just to one container or blob, not the access key which can be used to sign all requests. That being said, here is the diagram showing the communication between the client, server and the Blob storage. Generating Shared Access Signature Shared Access Signature is a Message Authentication Code, standard term in cryptography describing short piece of information which authenticates a message. It is generated using standard HMAC (Hash-base MAC) algorithm. HMAC algorithms use one of the existing hash functions in combination with the secret key to generate the MAC value. Azure Blob storage uses the HMACSHA256 function, which as the name says, makes use of 256bit SHA function and has the following definition (where “|” represents concatenation): HMACSHA256 (key, data) = sha256(sha256(data|key)|key) The hash is applied two times during the process in order to disable some simple attacks on previously simpler variants of the function which used just hashed combination of data and key. More information can be found on the wikipedia page. The key is simply the Blob storage access key however the data is the combination of the following: Identification of resource which should be accessed (container or blob) Time stamp – specifying the expiration date Permissions which will be give to access the resource The same data is added to the request, so when the server receives the request, it simply performs the same calculation of the HMAC value to verify the Shared Access Signature. That is possible because the server is in possession of the access key. When we are generating the signature on the server side, we can simply use the .NET API, which provides required methods. The following snippet illustrates the generation of Shared Access Signature, which will give the user read, write and delete rights for the next 10 minutes. var container = blobClient.GetContainerReference(\"container1\");\r\nvar permissions = new BlobContainerPermissions();\r\npermissions.PublicAccess = BlobContainerPublicAccessType.Off;\r\ncontainer.SetPermissions(permissions);\r\n\r\nvar expDate = DateTime.UtcNow + TimeSpan.FromMinutes(10);\r\n\r\nvar sas = container.GetSharedAccessSignature(new SharedAccessPolicy()\r\n{\r\n   Permissions = SharedAccessPermissions.Read | SharedAccessPermissions.Write\r\n   | SharedAccessPermissions.Delete | SharedAccessPermissions.List,\r\n   SharedAccessExpiryTime = expDate\r\n}); Conclusion As you could see in the second part, it is quite easy to add a new functionality using Azure Blob storage, which allows your client to upload the data directly to the server without loosing the security of the application. We can achieve the same functionality storing the files in SQL Server, so the question which comes in mind is what are the advantages which Blob Storage has over SQL Server (or SQL Azure). Here is the list of points which might explain why to use Azure Blob Storage. Blob Storage offers build-in support for the metadata for each file Blob Storage has the ability of separating the files into blocks and thus provides better support for treatment of large files. The architecture of Blob Storage allows the access to each blob to be load-balanced and thus provides high access speed ( See details here ). However due to the dependence on network connection this is difficult to compare with SQL Server on premise or Azure SQL and no metrics have been published by Microsoft so far. When comparing Azure SQL with Azure Blob Storage: Blob storage will be probably much cheaper. Note that this will depend on concrete cases, while the pricing scheme is not the same. You can get more details on the official site . Blob Storage has a limit of 2000GB whereas 1 database of Azure SQL is limited to 100GB Of course the choice will depend on the type and the scope of the application but Blob Storage is definitely one of the options to be considered when designing new .NET application. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-07-28"},
{"website": "Octo", "title": "\n                How to test private methods?            ", "author": ["Jonathan Scher"], "link": "https://blog.octo.com/en/how-to-test-private-methods/", "abstract": "How to test private methods? Publication date 18/07/2011 by Jonathan Scher Tweet Share 0 +1 LinkedIn 0 This question is always flying around when you start with Test Driven Development (TDD). It can come from two different sources : either we didn’t followed it “by the book” – and it punished us, or we are trying to use TDD on an existing code. For both cases, you will have to rethink your design. Emergent Design You are doing TDD by the book. You add a test: it fails. You make a crappy copy/paste, and it passes. Now you refactor : you create a private method, and you make two calls to it. If you are following TDD’s rules, that’s almost the only case when you will have to face private methods. And those methods will be tested from your two public methods. Actually, with TDD, you write tests before any methods. Therefore, you will never have to ask yourself “how will I test this method?” If you are doing TDD by the book, and you still ask this question, you may be over-concepting. You may have start to code the core, without knowing how you will integrate it. And you will have surprises when you’ll do it. Delete your code, write a small test, and make it pass. Or read the next paragraph. I have an untested private method in my code. Now what? Shit happens. And you have to deal with it every day. Depending on your design, there are three ways to deal with it : make it public – or expose it as a service in a different class, test it through another public method, or make it protected (or package only). If your private method is actually a service, that could be reused, why not making it public ? That is always an option. You may want to extract it in a different class, in order to be more compliant with the single responsibility principle . If it does not, why do you want to test it ? Because it’s used by another method of this class? Treat it as if it was part of the code of this other method: you will be able to test it that way. Write a test for your public method: it will test the private method. And coverage tools will understand it. Sometimes, you have a bug located in this private method. And you don’t want to recreate the whole context of the public method. Consider the first option: could it be extracted as a public service? Maybe in another class? If not, you always can change the visibility to package only. Your method is now accessible by the whole package. So you can write a test class that can access it. There are other possibilities. If you’re in Java, you can access the method through reflection. In ruby, you can use ‘send’. But those are more advanced stuff, that should be used with caution. Asking this question should raise an alarm. It shows you that you may have a service instead of a private method. If not, it should be tested as if it was the body of you public method using it. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “How to test private methods?” Deepak 20/05/2013 à 10:06 Crisp article on a very interesting topic that most developers keep hopping on!\r\nI feel when unit test or TDD is introduced, it feels like something new has to be done and some new process has to be followed! While its partially true, what is more important is there is a need to concentrate on the design of our code, which is not part of any new process. \r\n\r\nWhile writing unit test cases, we face problems like \"How to test private method\", instead, if we start with \"why is the need to write a private method?\" many times these are directly related to design rather than adopting new process (technically!) like TDD.... \r\n\r\nJourney back to Basics!! Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-07-18"},
{"website": "Octo", "title": "\n                Taking .NET applications to the Cloud            ", "author": ["Jan Fajfr"], "link": "https://blog.octo.com/en/taking-net-to-the-cloud/", "abstract": "Taking .NET applications to the Cloud Publication date 27/07/2011 by Jan Fajfr Tweet Share 0 +1 LinkedIn 0 One of our latest  research and development projects has been an online banking application, which we use to demonstrate the latest tendencies in .NET development. When it came to deployment, we could not find a suitable machine to publish the application. But wait, these days that’s not a problem any more. When you need to deploy a new application you can just publish it to the nearest cloud! The choice obviously came to Azure platform. The question was just how painful the deployment will be? Will it take days before we adapt the application to run on the platform? Actually the process of change was much more fluid than we thought and within an hour we were able to set up the necessary parts and run the application on the cloud. But Azure offers much more than just a runtime platform for your web applications. It offers also several types of storage. One of these is Azure Blob storage. And so after the painless deployment we thought: And what if we used Azure Blob storage to implement one of the new functionalities which was on our to-do list and which perfectly suites this kind of storage? Hence this blog post which describes our experience with Azure and which will have two parts. The first part describes how to take an existing .NET application and deploy it on the Azure platform. The second part which will be published soon will demonstrate how to make use of Azure Blob storage to develop simple content management functionality for the application (Electronic vault). Before we dive into the technical details let’s just briefly introduce the Azure platform and it’s offerings, in order to better understand which parts of the platform will be described in this blog. Azure platform overview Windows Azure has been commercially accessible since February 2010 and offers a possibility for developers to build scalable applications using several different technologies and languages. Besides a runtime platform for different types of applications written in different languages, Azure offers also several other services. Here is a brief overview of the platform: Runtime platform – there are two types of applications which can be hosted: Web Role – prepared to host web applications Worker Role – prepared to host any application which do not handle HTTP request (eg. regular data treatment tasks) Hosting virtual machines Cloud based storage Azure SQL – relational database based on SQL Server 2008 Azure NoSQL storage BLOB Storage Queue Table (key-value) Content Delivery Network This is just a simple overview of the platform, much more detailed description can be found on the official Windows Azure page . Now when we have seen the services which can be used we will go on with the description of the application. Architecture of the application As it has been said already the application which we decided to deploy is a simple online banking application. It is composed of Silverlight client which consumes WCF services hosted on IIS 7 server and uses SQL Server 2008 as its main data store. Let’s take a look at the architecture and at the changes which will be introduced when moving it to Azure. The red flashes in the diagram represent the migrations which will be done when changing to Azure. To sum up what this diagram wants to say: the application has been so far hosted in IIS server and after the transition will be hosted by Azure Web Role. SQL Server will be changed for Azure SQL storage. In the second part of this article, we will show how to integrate Azure Blob Storage to this architecture. As you can see from the diagram, the application uses NHibernate as ORM(Object Relational Mapping) to access the data. One of the questions we had was: will NHibernate work with Azure SQL? Well the answer is yes . Because Azure SQL is very similar to SQL Server 2008, no configuration changes are needed and the same data provider can be used. Just change the connection string and you are ready to go. Understanding the architecture of Web applications in Azure Let’s take a closer look at the architecture of common Web application deployed in Azure. A typical web application can have one or several Input Endpoints. For a Web Role an Input Endpoint is HTTP port on IP address assigned by Azure. The traffic is always load-balanced even if there is only one instance of a Web Role. Actually the service level agreement of Azure states, that each application has to have minimally two instances – if not, than Azure will not guarantee the on-time stated in the service lever agreement. The number of instances which should be created can be easily changed in the configuration of the “cloud” project in Visual Studio. In the diagram I have visualized several Worker Roles, actually our application does not contain any, but we can imagine simple console application turning regularly, making some background treatment on the data stored in the database. That type of application would be run by instance of Worker Role. You can notice, that I did not specify the type of the storage as you can use any type of the storage proposed by Azure. Both the Web Roles and the Worker Roles are running on some node inside the Azure Fabric. Azure Fabric is the name for the grid network of Azure. This network is composed of interconnected nodes, each running Windows Server 2008, either dedicated or composed to create a virtualised environment. Preparing the application for the Cloud Before you start, make sure you have installed the Visual Studio 2010 Azure Toolkit. This will install the Visual Studio “Cloudy” project templates as well as the Azure Emulator. Azure Emulator allows you to try out at your computer those features which are normally accessible only in cloud as well as to try out load balancing your application in several role instances. To be more specific: Azure Emulator does not contain Azure SQL, because you can “emulate” it by simply using your local version of SQL Server. On the other hand there are no local versions for the other types of storage (Blob, Queue and Table), so they are presented in the emulator. In order to deploy the solution to Azure, Azure deployment project has to be added to the solution. Octo.Bank.Web is the ASP.NET web application which exposes several WCF services and hosts the Silverlight client. Newly created project Octo.Bank.CloudService has one Role – previously mentioned Web project. If the project would have another roles (web or worker), they would be added to this list. There are two configuration files (in XML format) which allow you to define applications endpoints, number and types of role instances which should be created and several other options. This configuration can be altered also by editing the Properties page of each role. Visual Studio allows direct deployment to the Azure platform (by selecting Publish in the context menu of the Cloud project). The following dialog specifies the details of the publish: the credentials of the developer which is making the publish, the hosted service in which the application will be deployed and the storage account which will be used in order to upload the application package. Before this can be done, several parts have to be configured in the Azure Management Portal. Azure Management Portal Azure Management Portal is a web application which allows the configuration of all Azure Services. Thus we can use Azure management portal to create and manage applications, databases and all available types of storage. This portal is available at this address: https://windows.azure.com/. Connecting Visual Studio with Azure In order for the developer to be able to directly Publish to Azure, he will have to generate a certificate on his development machine, which Visual Studio will use when uploading the files to Azure. In the second step, this certificate has to be added to the certificates list in the Azure Management Portal. The following image shows the certificates management part of azure portal. You can see that there are two certificates. Each developer who wants to use Visual Studio to interact directly with Azure needs to create and upload his certificate. Moving the application Before we publish the application from Visual Studio, we have to create a container for the application. This container is called Hosted Service. Each Hosted Service has two “deployments” : production and staging. The following screenshot shows the list of hosted services in management portal. As you can see there is one deployment having two environments (staging and production). Each environment has just one web role and there is only one instance for each web role. Moving the database Before we start the move we have to Create new database in Azure Management Portal. When creating the database, the developer has to define which IP addresses will be able to access the database. Generally the database is accessed from Web or Worker role which are on the same Virtual Network created by Azure. However for the development purposes we have to sometimes access the database directly using SQL Server Management studio or other tool. For this situations Azure offers firewall which can be customized to control the incoming trafic. After the creation of the database a connection string is generated which can be used to access the database. The following screenshot comes from the database management portal, highlighting the firewall rule which allows remote access to the database. If we have already a database in production, we have several possibilities to make the migration: export the database scripts and run them against the Azure SQL database or use on of existing migration tools. The complete list of possibilities can be found at this official page. Conclusion If your solution is well structured, then the deployment of an existing application to the Azure platform can be a question of several minutes. After creating the azure cloud project, preparing the database and the hosted service you can easily just interchange the connections strings and deploy to the cloud. One of the missing pieces could be the possibility of automatic deployment from Team Foundation Server. This feature is not out-of-box ready to be used, however there is an API which allows deployment and configuration from .NET application or PowerShell script. This API can be than used to develop custom activity which can be added to the build process of Team Foundation Server. There is an existing opensource project which aims to deliver this feature, accessible at codeplex. If you are interested in Azure and would if you would like to know more, wait for the second part of this article which will discuss Azure Blob Storage. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-07-27"},
{"website": "Octo", "title": "\n                Live USI Paris now !            ", "author": ["Nelly Grellier"], "link": "https://blog.octo.com/en/live-usi-paris-now/", "abstract": "Live USI Paris now ! Publication date 28/06/2011 by Nelly Grellier Tweet Share 0 +1 LinkedIn 0 This year, all the sessions of the main conference room (and thus all the keynotes) will be broadcasted in live on the website now ! You’ll have the choice to watch the sessions translated or not. Please find below all the USI sessions that you can watch in live or not today and tomorrow: June 28th: Simon Sinek’s Keynote The birth of a word, Deb Roy Military operations of the 21st century. Battle field digitisation, Henri Bentegeat Is our history written in advance? Feedback on the impact of demography on our society? Pierre Sabatier Ray Kurzweil’s Keynote Alexandre Dayon’s Keynote The end of relational data bases? Olivier Mallassi Juridic aspects of the Cloud Computing : learning to browse in safety, Benjamin May What kind of mobile strategy for companies? Jean-François Grang et Olivier Martin LinkedIn, Yassine Hinnach Wolfgang Von Rüden’s Keynote June 29th: Michel Serres’s Keynote Beyond the “generation Y”, cutural diversity and its opportunities for companies, Benjamin Chaminade Is the scientific labour organization of the industrial era, applicable to the digital era? Vincent Lextrait André Compte-Sponville’s Keynote Stephen David’s Keynote Brain-computer interfaces, Dr Geoffrey Mackellar Connecting all the objects? Why? rafi Haladjan Pattie Maes’s Keynote Live USI Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged usi . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-06-28"},
{"website": "Octo", "title": "\n                Functional testing of a MVVM Silverlight application with Fitnesse            ", "author": ["Jean-Yves Rivallan"], "link": "https://blog.octo.com/en/functional-testing-of-a-mvvm-silverlight-application-with-fitnesse/", "abstract": "Functional testing of a MVVM Silverlight application with Fitnesse Publication date 18/10/2011 by Jean-Yves Rivallan Tweet Share 0 +1 LinkedIn 0 MVVM is a great pattern to enable (among other things) unit testing of a XAML based application GUI. If you’re not familiar with it, this article explains it well and describes its use with Greenpepper on a real life project. Functional testing works great with MVVM, but several issues prevent the use of functional testing tools (e.g. Fitnesse) with a Silverlight project out of the box. If the previous article convinced you to implement automated functional testing on your application, but its GUI is in silverlight instead of WPF, let’s see how you can circumvent those issues. The following has been implemented with Fitnesse, but should also work with Greenpepper or any other .Net automated testing framework. How can Fitnesse load Silverlight code ? Loading silverlight code in a standard .Net CLR is a matter of referencing the proper assemblies. Since the assemblies needed to load a ViewModel have the same name than .Net counterparts, Visual Studio does not allow to add them directly (it replaces it with the .Net assembly of the same name found in the GAC). So this requires some .csproj editing, as such : <Reference Include=\"System.ServiceModel\">\r\n  <HintPath>..\\DLLs\\System.ServiceModel.dll</HintPath>\r\n</Reference> Silverlight assemblies must be referenced with their location on the filesystem. The assemblies required to load a View Model are System.ServiceModel, System.Windows, System.Runtime.Serialization, System.Xml, and System.Xml.Linq. Add those assemblies as references in your Visual Studio project holding your fixtures, as well as references to your Silverlight projects in your solution, and you’re good to go. What if my view model calls a WCF service ? Now we can call silverlight code from fitnesse. But if you’re writing a Silverlight application, chances are it is calling WCF services to do some work server-side. You can’t directly inject a WCF service implementation into a view model, since a WCF client proxy is generated to call it. Moreover, the client side interface only exposes asynchronous methods. So the challenge is to be able to transform this : // Server side WCF Service signature\r\nint DoSomeWork(int i); into these : // Client side equivalent generated by Visual Studio\r\nIAsyncResult BeginDoSomeWork(int i, AsyncCallback callback, object asyncState);\r\nint EndDoSomeWork(IAsyncResult result); Those are standard asynchonous methods any delegate can produce. A wrapper class can be created to translate the server-side service into the client-side one this way : public class MyServiceWrapper : ClientSideNamespace.IService // Client-side interface {\r\n private ServerSideNamespace.IService service;\r\n private Func<int, int> SomeWorkDelegate;\r\n\r\n public MyServiceWrapper(ServerSideNamespace.IService service) // Server-side interface {\r\n  this.service = service;\r\n  this.SomeWorkDelegate = i => service.DoSomeWork(i);\r\n }\r\n public IAsyncResult BeginDoSomeWork(int i, AsyncCallback callback, object asyncState) {\r\n  return SomeWorkDelegate.BeginInvoke(i, callback, asyncState);\r\n }\r\n public int EndDoSomeWork(IAsyncResult result) {\r\n  return SomeWorkDelegate.EndInvoke(result);\r\n }\r\n} Use this wrapper as a service implementation in your ViewModel and you’re good to go. What if my WCF service returns more than an integer ? Just like method signatures differ between client and server side, data structures are not the same. Properties have the same name, but types can differ, especially Collections implementations. Automapper is an open source library able to solve this issue in a very convenient way, enabling the wrapper to translate a server-side dto into a client-side dto and vice versa. Assuming the WCF service evolves to : DoSomeWorkDto DoSomeWork(int i); The wrapper becomes : public class MyServiceWrapper : ClientSideNamespace.IService // Client-side interface {\r\n private ServerSideNamespace.IService service;\r\n private Func<int, ClientSideNamespace.DoSomeWorkDto> SomeWorkDelegate;\r\n\r\n public MyServiceWrapper(ServerSideNamespace.IService service) // Server-side interface {\r\n  this.service = service;\r\n  this.SomeWorkDelegate = i => Mapper.Map<ServerSideNamespace.DoSomeWorkDto, ClientSideNamespace.DoSomeWorkDto>(service.DoSomeWork(i));\r\n }\r\n public IAsyncResult BeginDoSomeWork(int i, AsyncCallback callback, object asyncState) {\r\n  return SomeWorkDelegate.BeginInvoke(i, callback, asyncState);\r\n }\r\n public ClientSideNamespace.DoSomeWorkDto EndDoSomeWork(IAsyncResult result) {\r\n  return SomeWorkDelegate.EndInvoke(result);\r\n }\r\n static MyServiceWrapper() {\r\n  Mapper.CreateMap<ServerSideNamespace.DoSomeWorkDto, ClientSideNamespace.DoSomeWorkDto>();\r\n }\r\n} The Mapper class is pretty straightforward, just call the CreateMap method for all dto class couples (both ways if necessary). Then include a Mapper.Map call in your delegates in the wrapper and you’re good to go. Conclusion The practice of testing an application with Fitnesse (or any functional testing tool) through its ViewModels is really valuable. Matthieu’s feedback was already self-explanatory about this. Not being able to achieve it on a Silverlight application would have been a shame. At the price of a little development overhead (the WCF services wrappers), this practice is applicable on Silverlight projects too. A recent experience with a Silverlight application developed by Octo showed that this overhead is negligible compared to the benefits of automated functional testing involving the ViewModels. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged .NET , automapper , FitNesse , Greenpepper , Silverlight , Tests , wcf . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-10-18"},
{"website": "Octo", "title": "\n                Scribe : a way to aggregate data and why not, to directly fill the HDFS?            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/scribe-a-way-to-aggregate-data-and-why-not-to-directly-fill-the-hdfs/", "abstract": "Scribe : a way to aggregate data and why not, to directly fill the HDFS? Publication date 08/01/2011 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 HDFS is a distributed file system and quickly raise an issue : how to fill this file system with all my data? There are several options that go from batch import to Straight Through Processing. Bulk load style . The first one is to keep collecting data on local file system and importing them by vacation. The second one is to use an ETL. Pentaho has announced support of Hadoop for Data Integration product. The first tests we conducted lead us to think this works much better to extract data from Hadoop (using Hive) than to import data. Yet this is just a matter of time before Pentaho fixes the few issues we encountered. The third one is to use solution like Sqoop . Sqoop extracts (or imports) data from your RDBMS using Map/Reduce algorithm. I hope we will be able to talk about that solution very soon. Straight Through Processing style . In that domain, you can look at solutions like Flume , Chukwa (which is  part of Apache Hadoop distribution)  or Scribe . In brief, you collect and agregate data in a more STP style, from different sources, different applications, different machines . They globally work the same way than Scribe but solutions like Flume or Chukwa provide more connectors than Scribe in a sense you can, for instance, “tail” a log file etc etc…Chukwa is also much easily integrated with the “Hadoop stack” than what Scribe could be. Scribe : a kind of distributed collector Scribe has been developed and “open sourced” (as many other projects ) by Facebook. The installation is not that simple (should I say tricky?) but Scribe is massively used at Facebook : a proof it works and it is worth fighting with compilation problems or the miss of RPM packages (in the meantime, maybe there is one but I have not noticed it). Scribe collects messages, not in a reliable way but is designed for failure Scribe has a simple way of logging information. A scribe server collects data represented as a category and a message. As an example. LogEntry entry = new LogEntry(“crash\", “app has a crash|userid|input parameters|....”); Scribe can be viewed as a collection of processes , running on different machines and listening to a specified port, in which you can push data (so a category and a message) using Thrift API . For each category , you then define Stores which are the way the data will be stored or broadcasted to different and remote Scribe processes. There are different available stores but the most common are file – writes to a file – or network – sends messages to another scribe server -. You can thus define different Stores for different categories and then route your messages based on that category. Scribe is resilient in the way you can define primary and secondary stores : the secondary stores will be used to fail-over the first one and retries (ie. resending the messages from the secondary store to the primary store) are automatically managed by Scribe. There is yet a couple of things for which Scribe is not reliable. If the Scribe process on the client machine (in green in the above schema) is down, messages will be lost (and your code should be resilient). Moreover Scribe process can buffer, in memory, messages for a time interval. Thus, in case of failure, all these buffered messages will be lost. So Scribe is maybe not the tool you need  depending on the message reliability you must guarantee. Scribe configuration sample : different (backuped) stores for different categories Here is an example of the Scribe configuration (.conf file) : # This file configures Scribe to listen for messages on port 1464 and\r\n# attempt to forward all messages to another Scribe instance on port 1463.\r\n# If Scribe is unable to forward the messages to port 1463, it will buffer\r\n# them on disk and keep retrying.\r\nport=1464\r\nmax_msg_per_second=2000000\r\ncheck_interval=3\r\n# DEFAULT - forward all messages to Scribe on port 1463\r\n<store>\r\ncategory=default\r\ntype=buffer\r\ntarget_write_size=20480\r\nmax_write_interval=1\r\nbuffer_send_rate=1\r\nretry_interval=30\r\nretry_interval_range=10\r\n\r\n<primary>\r\ntype=network\r\nremote_host=localhost\r\nremote_port=1463\r\n</primary>\r\n\r\n<secondary>\r\ntype=file\r\nfs_type=std\r\nfile_path=/tmp/scribetest2\r\nbase_filename=thisisoverwritten\r\nmax_size=3000000\r\n</secondary>\r\n</store> More details concerning Scribe configuration is available here . Yet, Interesting elements are : – the port the Scribe process is listening to (1464 in our example) – the primary and secondary defined stores. The primary store broadcast messages to another Scribe process (listening on port 1463). The secondary one stores messages on the local file system (in case the former is unavailable). Starting Scribe agents on the client machines Starting a Scribe process is quite simple once Scribe is set up scribed -c path_to_conf_file.conf You must so define a configuration file on the different Scribe agents you will start . The above configuration starts a scribe server on port 1464 and forwards all the messages to the primary store (another Scribe process listening on the same host but on port 1463). In case the primary store is not available, to the secondary defined store (a local file system) will be used. When you launch Scribe (on the client machine), you get the following logs (I know there is only on configured store logged but it works) : Starting Scribe as a central service On the central Scribe Server, you run the same process but with different configuration file. In this sample, you simply write the messages to the local file system in /tmp/scribetest port=1463\r\nmax_msg_per_second=2000000\r\ncheck_interval=3\r\n<store>\r\ncategory=default\r\ntype=buffer\r\ntarget_write_size=20480\r\nmax_write_interval=1\r\nbuffer_send_rate=2\r\nretry_interval=30\r\nretry_interval_range=10\r\n<primary>\r\ntype=file\r\nfs_type=std\r\nfile_path=/tmp/scribetest\r\nbase_filename=thisisoverwritten\r\nmax_size=1000000\r\n</primary>\r\n</store> Play with the beast… Of course the idea is to use Scribe from your application code. In that case, you must generate code based on the thrift interface. Yet, a simplest way to test your configuration is to use the provided python script (in the example folder of your distribution) named scribe_cat (You need to define the constant export PYTHONPATH=/usr/lib/python2.6/site-packages/ ). This script opens a connection to a Scribe server and publishes the message in stdin on the specified category (here it is default) echo “my message” | ./scribe_cat default Go and check your log files to see if it works… Using Scribe to log in HDFS The default Scribe compilation does not enable direct logging to HDFS : the mystery of the compilation…So you need to (re)compile Scribe with the  –enable-hdfs option. Once done, you can use Scribe to directly push data into HDFS : port=1464\r\nmax_msg_per_second=2000000\r\ncheck_interval=1\r\nmax_queue_size=100000000\r\nnum_thrift_server_threads=2\r\n\r\n# DEFAULT - write all messages to hadoop hdfs\r\n<store>\r\ncategory=default\r\ntype=buffer\r\ntarget_write_size=20480\r\nmax_write_interval=1\r\nbuffer_send_rate=1\r\nretry_interval=30\r\nretry_interval_range=10\r\n\r\n<primary>\r\ntype=file\r\nfs_type=hdfs\r\nfile_path=hdfs://localhost:9000/scribedata\r\ncreate_symlink=no\r\nuse_hostname_sub_directory=yes\r\nbase_filename=thisisoverwritten\r\nmax_size=1000000000\r\nrotate_period=daily\r\nrotate_hour=0\r\nrotate_minute=5\r\nadd_newlines=1\r\n</primary>\r\n\r\n<secondary>\r\ntype=file\r\nfs_type=std\r\nfile_path=/tmp/scribe-central-hdfs\r\nbase_filename=thisisoverwritten\r\nmax_size=3000000\r\n</secondary>\r\n</store> The main differences, in the configuration file, are the fs_type (which is not set to std but to hdfs) and so the file_path (which is set to the hdfs file). Before running your Scribed process and logging data to HDFS, you need to set up your CLASSPATH variable with some Hadoop jars (as described here ). Then you can start Scribe and broadcast messages : And if you look at your HDFS, you will see the broadcasted messages : Data seems to be buffered by the Scribe process in a way messages are not displayed in the HDFS web console until either you kill the scribed process or you change the configuration to roll out files more frequently. For instance you can decrease the max_size parameter (take car HDFS is not reliable with small files ) to get the following screen shot : To conclude Thus, Scribe is just a way to aggregate and collect data (as syslogNG could do) from several machines, several sources into a central repository that can be HDFS . This can moreover be interesting if you add, upon HDFS, Hadoop to be able to analyze all these data. Then and with all the previous articles, you have set up the following stack : Moreover, there are still a couple of things we should have the opportunity to talk about. The first one is to integrate Scribe to the Java world (and for instance generate the Thrift client API for Java and, why not use a log4j appender . The second one is to integrate the data you have logged into Hive … Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged HDFS , NoSQL , Scribe . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-01-08"},
{"website": "Octo", "title": "\n                Back from the Mule Summit 2011            ", "author": ["Matthias Feraga"], "link": "https://blog.octo.com/en/back-from-the-mule-summit-2011/", "abstract": "Back from the Mule Summit 2011 Publication date 17/10/2011 by Matthias Feraga Tweet Share 0 +1 LinkedIn 0 We Octo Mule fanboys assisted at Mule Summit 2011 in Paris. The summit was quite confidential. This made the interactions with MuleSoft technical staff even richer. CTO and founder Ross Mason was here one more time. It was a pleasure to have such technical speakers. Last year we focused on Mule 3 new exchange architecture (FR). Meanwhile MuleSoft largely improved its ESB with new features. This year we will focus on what makes Mule ESB a reliable Enterprise solution . Business Events Analyzer With version 3.2, Mule ESB has new business monitoring capabilities. These enhancements fill the lack of previous versions regarding monitoring. The main applications of this new feature are: Root cause analyze : track back the exact source of an issue, without implementing complex logging. This considerably reduce the diagnose time. Regulatory compliance data tracking and storage SLA monitoring Business Activity Monitoring Here is how it works: At design-time, the developer choose which exact events he wants to gather, by adding probes in the Mule configuration code . An event is composed of technical information (processing time, status, etc.) as well as business information (order id, price, customer name, etc.). Mule added partial support for Message Store and Message  History patterns. This permits to store an event at any time in the flow. A tracking context is associated with each instance of a business flow (the entire processing of an incoming message, not just a technical flow). This context is enriched with new events while the flow executes. An agent is running on each Mule instance, and the Management Console collects events from agents in the background. Note that it is not possible to track entire messages automatically. The developer has to explicitly configure what information to track. We can’t interact with the message and resubmit it through the Management Console. As a result, the feature can’t be used as a retry point . Manual retry points still has to be tooled up with technical flows. One interesting thing however is that Mule now supports the Correlation Identifier pattern. this permits end-to-end monitoring of business flows with time breaks (getting out of Mule ESB and back again). Examples : Request-Reply over an asynchronous transport (JMS, flat file, etc.) An invoice is received after an order is sent, and we want to monitor the all as a unique business flow High Availability (HA) Before this new feature, HA had to be tooled up with custom JMS persistence . Different instances of Mule ESB then stored messages in JMS on critical points of the flow, using the Wire Tap pattern. Until JMS became the single point of failure, and so on. Announced in the roadmap last year, Mule HA is finally here. All these improvements have a price: unless you buy an Enterprise Edition of Mule, you will have to build HA the old way. The new solution offer an active/active clustering across several nodes as follows: Here is how it works: At design-time, use queue-based transport to activate save points. A developer should place a save point at each critical point of the flow, using queues such as the internal transport, JMS, SEDA, etc. HA relies on Gigaspaces distributed memory to persist messages and other components states. Gigaspaces is embedded on each node of the cluster using save-points enables internal re-load-balancing . This is particularly useful for SEDA-based flow, where the processing can be automatically distributed on workers within the cluster Polled resources as flat files, FTP and JDBC are shared across the cluster and automatically available to only one node at a time Conclusion During the event, MuleSoft also presented two other new products : Mule Studio is an Eclipse-based environment that accelerates Mule developments Mule iON is a PaaS platform that offers no more no less than a new deployment option for your flows. With new mechanisms as persistence (Message Store/Message History), and Correlation Identifier, Mule ESB is extending its initial role. Even if MuleSoft don’t communicate on it, and rather highlights integration with external BPM engines, it is getting closer to BPM . Mulesoft and the community have done a great job in one year. From my point of view, Mule keep its title of most popular ESB. It is very close to fill the gap with vendor’s ESBs, providing as many features, but in a lightweight fashion and for a lightweight price per CPU. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged ESB , lightweight , Lightweight ESB , mule , mule esb . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-10-17"},
{"website": "Octo", "title": "\n                Scribe installation            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/scribe-installation/", "abstract": "Scribe installation Publication date 08/01/2011 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 Scribe installation is a little bit tricky (I need to precise I am not what we can call a C++ compilation expert and thanks to David for his help…). Here is so how I installed Scribe on my Ubuntu (Ubuntu 10.04 LTS – the Lucid Lynx – released in April 2010) Scribe Compilation : get the basic packages… To compile Scribe, you need a couple of dependencies. As far as I remember, I needed (via apt-get install) the following dependencies (the first ones are certainly already part of your distribution) : libtool\r\nautomake\r\nautoconf\r\ng++\r\nmake\r\nlibboost-dev=1.38.1 and (or?)libboost-all-dev \r\nflex\r\nbison\r\npkg-config\r\nbuild-essential\r\nmono-gmcs\r\nlibevent-dev\r\npython\r\npython-dev …compile the thrift and fb303… You first need to compile scribe dependencies : thrift and fb303 which is part of the thrift distribution. Get the source code from the repository svn co http://svn.apache.org/repos/asf/incubator/thrift/trunk thrift Then in the folder ./thrift ./bootstrap.sh && ./configure && make && sudo make install Then in the folder ./thrift/contrib/fb303/ ./bootstrap.sh && sudo make && sudo make install …and then compile Scribe. You can compile Scribe get the source from git repository http://github.com/facebook/scribe.git or download version 2.2 from here run ./bootstrap, configure, make, sudo make install Scribe should work… Scribe compilation with HDFS support : did you forget the option, didn’t you? If you want to use Scribe to log data into HDFS, you need to compile Scribe with --enable-hdfs option. How will you detect it? try to use HDFS with the default compilation of Scribe and you will get one of the cleanest log message I have ever seen :) Thus, you have to recompile Scribe and the main challenge will be to find the compatible version of Scribe and Hadoop…The following compilation has been made with the Apache distribution of Hadoop-0.21.0 and Scribe 2.2. Both $HADOOP_HOME and $JAVA_HOME must be set (I used jdk1.6.0_16) From your Scribe installation folder : ./bootstrap.sh --enable-hdfs\r\n./configure --with-hadooppath=$HADOOP_HOME --enable-hdfs CPPFLAGS=\"-I$HADOOP_HOME/hdfs/src/c++/libhdfs/ -I$JAVA_HOME/include/ -I$JAVA_HOME/include/linux/\" LDFLAGS=\"-ljvm -lhdfs -L$JAVA_HOME/jre/lib/i386/client -L$HADOOP_HOME/c++/Linux-i386-32/lib/\"\r\nmake The make should fail. So I needed to modify the ./src/HdfsFile.cpp (last time I wrote C++ was a very long time ago…) modify the method deleteFile . It contains a call to hdfsDelete but the third parameter is missing. So you need to change the line to this one : hdfsDelete(fileSys, filename.c_str(), 1); You also need to install the following patch if you want Scribe to be able to write on the HDFS. run make again, then sudo make install Scribe Configuration : the easy part In order to make Scribe work, you need a couple of configuration. First you need to set $JAVA_HOME and $HADOOP_HOME (I tested with jdk 1.6.0_16 and Hadoop in version 0.21.0) Then export LD_LIBRARY_PATH=/$JAVA_HOME/jre/lib/i386/client:/$HADOOP_HOME/c++/Linux-i386-32/lib or you will have this kind of error scribed: error while loading shared libraries: libjvm.so: cannot open shared object file And, in order to write data directly on HDFS, you need to set your CLASSPATH variable export  CLASSPATH=$HADOOP_HOME/hadoop-hdfs-0.21.0.jar:$HADOOP_HOME/hadoop-common-0.21.0.jar:$HADOOP_HOME/lib/commons-logging-1.1.1.jar Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged HDFS , NoSQL , Scribe . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-01-08"},
{"website": "Octo", "title": "\n                Droidcon London 2011            ", "author": ["Jérôme Van Der Linden"], "link": "https://blog.octo.com/en/droidcon-london-2011/", "abstract": "Droidcon London 2011 Publication date 18/10/2011 by Jérôme Van Der Linden Tweet Share 0 +1 LinkedIn 0 This year, we had the opportunity to go to the Droidcon in London, a great conference about mobility and Android. The two-day event was organized like in 2010: a barcamp the first day, and conferences with great speakers the second one. Main topics were user experience and interface, Android development, mobile apps business and distribution. User eXperience The number of Android users has incredibly risen! More than 300 000 applications can be downloaded on the market! What makes your app better and prevent it from user rejection ? Look and feel and responsiveness, of course! Make your application smooth! This year, Cyril Mottier talked about UI and gave pieces of advice to develop pretty and fluid applications. It starts with a simple order: do not block the UI thread because it is the one that displays elements on screen and handle interaction with the user. Using it for non UI process leads to slow animations, laggy interface and if five-second freeze occurs: Application Not Responding messages! Android SDK offers several ways to handle secondary threads for non UI processes : execute it with simple Threads and Handlers (maybe with a Looper to do operation queues ), create an AsyncTask or even launch a dedicated service with the IntentService class. Likewise, access your data asynchronously : AsyncQueryHandler , Filter and SharedPreference.Editor.apply() can help you. Use great components All recent successful applications understood it: nice and customized views are the key! Cyril Mottier’s open-source library GreenDroid is one of the most known. It allows you to ease development by using its theme and numerous features for pre-Honeycomb application (and after with compatibility package ): ActionBar, AsyncImageView, PagedView, QuickActions and more! GDCatalog application is available on the Android market to demonstrate the capabilities of this very useful library. Follow Google best practices to optimize your app for smartphones and tablets ActionBar Nick Butcher introduced some guidelines to design UIs for both kinds of devices. The most noticeable UI Pattern that has become unavoidable is the ActionBar. Its aim is to help the user accessing screen details and contextual actions . This action bar places elements in a way to help user interactions. See GMail example below. On the left side, the application icon informs the user where he is. As on a website, a click on this button leads to the application main screen. A second way to navigate inside the application flow is offered by the in-app navigation button. This action can be different from traditional Android back button. The right side presents traditional application parameters button and contextual actions. More contextual informations can be added to the bar under certain conditions (items selection, etc.). To implement an ActionBar on pre-Honeycomb Android version, you can use Google I/O application implementation, ActionBarSherlock or previously presented GreenDroid libraries. Upper versions (v3+) can use SDK ActionBar . With the upcoming Ice Cream Sandwich version, ActionBar will be able to automatically layout the interface for phone or tablet . Little advice for this: provide text and icons for each Action item and set for each one the flag: showAsAction=“ifRoom|withText” . Multi-pane layouts Since Honeycomb, a single APK can be executed on both smartphones and tablets. On large screens, it is preferred to display more complete content than a simple ListView. With Fragments , it is possible to aggregate your code to use it on 2 smartphone screens and a single tablet screen. With this concept, you can imagine a lot of different screen dispositions depending of screen sizes, orientation, etc. We will talk about this subject in a next article. Do’s and don’ts Simple tips were given to improve development: aim for a single APK for both tablet and handset devices use compatibility library customize every view to represent your brand support both landscape and portrait modes extract dimensions for phones and tablets (e.g. values/dimens.xml and values-large/dimens.xml) use themes and styles to reduce code redundancy enable hardware acceleration And other to avoid in your application: do not use small fonts tablets are not “big phones”: they fulfill a different need and require special layout optimizations do not assume that a device running an application with API level >= 11 is a tablet: Ice Cream Sandwich is coming for both tablet and phones! do not assume that all tablets have an xlarge resolution: 7” tablets have a large resolution! Detect tablets with the minimal width your application needs, generaly 600 dp: <manifest ... >\r\n              <supports-screens android:requiresSmallestWidthDp=\"600\" />\r\n              ...\r\n         </manifest> A bit of technique Testing You may have seen comments such as “Doesn’t work on SGSII” or “Force close on HTC Wildfire” on Android Market. Indeed, with no less than 300 phone models, it is difficult to provide an application that works on each device. The best way to minimize bugs is to test your application. You can do it manually or with low cost outsourced hands but the best way is to automate your tests. Several frameworks exist and cover different kinds of tests: Monkey , which ensures to simulate random actions on the UI (buttons, textedit, touch…). This is a good way to validate the robustness of the application. The Google test api ( instrumentation ) which can be laborious to implement although essential if you want to test the interface behavior. Robotium , a good testing framework, that simplifies the test writing over the Google’s instrumentation tests. We also came across two other frameworks (plateforms): Robolectric comes with a great advantage over robotium and instrumentation: you don’t need an emulator to test your application, and everyone knows how much the emulator is slow! So, you run your tests in a standard JVM, in your favorite IDE and you gain a lot of time. Joe Moore (from Pivotal Labs) made a demo with a set of 20 tests. The standard instrumentation test took 45s on an emulator (already launched, so you can easily add one minute for the launch) while robolectric got the job done in only 12s! Moreover, it is very simple to use, simply add @RunWith(RobolectricTestRunner.class) on your test classes and that’s it! Give it a try, it’s free. You can get the slides here . All the previous solutions are either boring to implement (manual creation of the tests), nor realistic (emulator, JVM). Testdroid (developed by Bitbar) promises to answer to both issues with two services: Testdroid recorder , an eclipse plugin, ensures to capture your test while you execute the application. It generates robotium code, so you can replay the tests after. Testroid server is a cloud platform that allows to automatically execute the recorded tests on multiple Android physical devices. A lightweight DeviceAnywhere? Seems promising to test your applications on real devices, on different Android releases, with different form factors… Do Not Repeat Yourself In our projects, a lot of tasks are repeated. The session “Kick-starting Android Application Development” presented some interesting projects to improve Android development: Android plugin for Gradle project allows to use Gradle to reduce time spent on APK build, packaging, integration, etc. It can be use in integration with Maven or Ant and also allows to create Android applications in Scala. Roboguice , is a Google implementation of Guice to use dependency injection . It uses reflection to inject fields at runtime and allows to get rid of resources initialization, service creation,… A simple demonstrative example to get a TextView instance: @InjectView(R.id.name) TextView nameTextView; Another similar project ( AndroidAnnotations ), based on subclass code generation instead of reflection, allows to inject content with no runtime impact on performances. Application distribution Several sessions were related to application distribution, and how to improve your application and its ranking on the Market. Here are some tips from Caroline Lewko (Wireless Industry Partnership), Thai Tran (LightBox) and Konstantinos Polychronis (Bugsense). Do not forget marketing! Nowadays, publishing an application and keeping it visible on the stores is a challenge. WIP shared several tips to make your application successful through time. First of all, as mobile is growing fast , you have to follow the same rule! A way to move at same speed is to make short Agile development cycles and for each, a marketing review of the product . Marketing’s role is to ensure that the application always fits the market’s needs, try different implementations ( A/B testing ). It keeps the product uptodate and brings opportunities to “fail fast” instead of achieving a useless application. Once you have a product that grows fast, you have to make your customers like it! It starts with a good name. Use Google and social networks to find a name that will be easily adopted by the public . Then use those social networks, especially Twitter and Facebook, to make your name and your product known. The more it is known, the more it will be downloaded and visible on stores. Beta test your application Once you have developed it, and tested it, go to beta test. You can do it yourself, recruit some beta testers, send them the apk and get their feedback. You can go further, put your application on the Android Market and automatize feedback. On the market, make your application the less visible possible : do not use your usual account, do not provide description, screenshots, keywords, use a different package name in the application to reset comments and notes when you will deploy the release, provide an invitation code to your beta testers. Another solution, easier, is to get a service like Appaloosa to distribute your apps to a managed range of users. We got the opportunity to show this product during the barcamp and make a demo, which was well welcomed. We really felt the need of such a solution. Get the maximum of feedback In order to ensure customers engagement: give them a way to provide some feedback and use that data to improve your product’s features. You can use frameworks like ACRA or Bugsense to retrieve crash reports (which phones and characteristics, which os version, what kind of error). Be sure to correct bugs after that. Use Google Analytics or Flurry Analytics to get usage metrics (how many users, how many time, what features usage). Collect the maximum of information to improve your application navigation. Use Android Market tools for developers To conclude the conference, Richard Hyndmann from Google talked about developers facilities on the Android Market and how to get visible on it. To be at the top of rank, your app has to be fit and polished. Of course, avoid straight port from other plateform, it generally causes users rejection. If your application offers unique features for Android, it is more valuable and will be rated as so. Seasonality also assures regular update of your app’s position that can appear in several different categories like Top Free & Paid apps, top grossing apps, and others. Also, do not forget to provide new format images for add banner: it is a great way to get downloaded! The Android market offers a new functionality: distribution of multiple APK . It allows a developer to provide different APK files for different kind of devices. For example, one application for handset devices and another for tablets. It can simplify adoption of new platform features and lighten your apps. However, Google advices to use this feature only in special cases and always prefer a single APK file when possible. It appears that Honeycomb users are ready to pay for applications! It can generate some big revenues. Some attention was given to developers ways to make money. In-App Billing system was presented. It is another way to sell virtual content such as additional levels for games or download files. Fees are the same for in-app purchases as the transaction fee for application purchases on the market: 30%. Another tool that can be useful to prevent piracy for paid applications on market is Lice nsing service . Its License Verification Library (LVL) allows your application to set up licensing status protection. The last feature presented is application Data Backup . Implement it to offer users to save application content and preferences, even private files in the cloud. All the data can then be restored, for example when switching to a new Android device. All these tools can be used to make your applications better! Just think to use them. What’s next? We want to finish with some interesting predictions given by Mark Murphy during the opening keynote: In 3 years, 1 application over 3 will be developed using cross platforms tools such as HTML 5 (with Phonegap or Rhodes ), app generators (Appcelerator) or any other framework that could come to provide multi-platforms apps. Indeed, most companies want to provide their apps to the maximum of platforms with the smallest development cost. We even think it could be more than 1 over 3 applications… In 3 years, Amazon will be the second tablet manufacturer (after Apple). The new Kindle Fire offers today a full multimedia experience on an Android based system. Will Amazon still be using Android plateform in the following years ? In 3 years modded roms will be significantly more popular! Modded roms like CyanogenMod could diversify and meet enterprise needs in terms of security and professional tools… This is, with the rise of private application stores, one of the major points that may make Android become an important actor in enterprise. DroidCon 2011 was a great time for us, we learnt a lot, especially that next year will be tablet year! Ice Cream Sandwich is coming and will give the opportunity to develop applications for both devices kinds. Be ready and always make better products! Thanks to organizers and sponsors for this year. Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged Android , droidcon , mobility . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-10-18"},
{"website": "Octo", "title": "\n                The basics of face recognition            ", "author": ["Jan Fajfr"], "link": "https://blog.octo.com/en/basics-face-recognition/", "abstract": "The basics of face recognition Publication date 20/10/2011 by Jan Fajfr Tweet Share 0 +1 LinkedIn 0 Face Recognition is definitely one of the most popular computer vision problems. Thanks to its popularity it has been well studied over the last 50 years. The first intents to explore face recognition were made in the 60’s however it was until the 90’s when Turk and Pentland implemented the “Eigenfaces” algorithm, that this field showed some really exciting and useful results. Bright future Face recognition is recently getting more and more attention and we can anticipate bright future of this field. Security was historically and will stay in the future the main application of face recognition in practice. Here face recognition can help with both: identification and authentication. Good example is the Frankfurt airport security system which uses face recognition to automatize passenger control. Another application can be the security analysis of videos purchased by external city cameras systems. Potential suspects can get identified before committing crime. Take a look at the integration of face recognition to London Borough of Newham , already in 1998. Face recognition can be also used to speedup the identification of persons. We can image a systems which would recognize the client as soon as he walks into branch store (bank, assurance), and the front-office worker can than welcome the client by has name and prepare his folder before he actually gets to the counter. Advertising companies are working on ad-boards which would adapt their content to the persons passing by . After analyzing the persons face, commercials would adapt to the gender, age, or even personal style. This usage however, might not conform to privacy laws. Private companies do not have rights to film persons in public places (of course, depending on the country. Not to forgot, that Google and Facebook had both implemented algorithms to identify users in the huge database of photos which they maintain as part of their social network services. Third party services, such as Face.com offer Image base searching, which allow you search for example for picture which contain together your best friends. One of the latest usages is coming also from Google and it is the Face Unlock feature, which will as the name says, enable you to unlock your phone after your face has been successfully recognized. Latest news come to face recognition thanks to new hardware equipment, specially 3D cameras. 3D cameras obtain much better results, thanks to their ability to obtain three dimensional image of your face and solve problems which are the main issues in 2D face recognition (illumination, background detection). See the example of Microsoft Kinect , which can recognize you as soon as you walk in front of the camera. We should keep in mind, that face recognition will be used more and more in the future. This applies not only to face recognition, but to the whole field of machine learning. The amount of data generated every second forces us to find ways to analyze the data. Machine learning will help us to find a way to get meaningful information from the data. Face recognition is just one concrete method from this big area. How to get started? Several methods and algorithms were developed since than, which makes the orientation in the field quite difficult for the developers or computer scientists coming to face recognition for the first time. I would like this article to be a nice starter to the subject which will give you three pieces of information: What are the algorithms and methods used to perform face recognition. Fully describe the “Eigenfaces” algorithm. Show a fully functional example of face recognition using EmguCV library and Silverlight Web Camera features. Go directly to the second part of this article, describing the implementation. Face recognition process The process of recognizing a face in an image has two phases: Face detection – detecting the pixels in the image which represent the face. There are several algorithms for performing this task, one of these “Haar Cascade face detection” will be used later in the example, however not explained in the article. Face recognition – the actual task of recognizing the face by analyzing the part of the imaged identified during the face detection phase. Face recognition brings in several problems which are completely unique to this domain and which make it one of the most challenging in the group of machine learning problems. Illumination problem – due to the reflexivity of human skin, even a slight change in the illumination of the image can widely affect the results. Pose changes – any rotation of the had of a person will affect the performance. Time delay – of course that due to the aging of the human individuals, the database have to be regulary updated. Methods and algorithms Apearance based statistical methods are methods which use statistics do define different ways how to measure the distance between two images. In other words they try to find a way to say how similar two faces are to each other. There are several methods which fall into this group. The most significant are: Principal Component Analysis (PCA) – described in this article. Linear Discriminant Analysis ( more information ) Independent Component Analysis ( more information ) PCA is described in this article, others are not. For comparison of these methods refer to this paper Gabor Filters – filters commonly used in image processing, that have a capability to capture important visual features. These filters  are able to locate the important features in the image such eyes, nose or mouth. This method can be combined with the previously mentioned analytical methods to obtain better results. Neural Networks are simulating the behavior of human brain to perform machine learning tasks such as classification or prediction. In our case we need the classification of an image. The explication of Neural Networks would take at least one entire article (if not more). Basically Neural Network is a set of interconnected nodes. The edges which are between the nodes are weighted so the information which travels between two nodes is amplified. The information travels from set of input nodes, across a set of hidden nodes to a set of output nodes. The developer has to invent a way to encode the input (in this case an image) to a set of input nodes and decode the output (in this case a label identifying the person) from the set of output points. Commonly used method is to take one node for each pixel in the image on the input side of the network and one node for each person in the database on the output side as ilustrated on the following image: For more information about face recognition using Neural Networks visit this link Eigenfaces algorithm and PCA The eigenfaces algorithm follows the pattern which is followed by other statistical methods as well (LDA, ICA): Compute the distance between the captured image and each of the images in the database. Select the example from the database, which is closest to the processed image (the one with the smallest distance to captured image). If the distance is not too big – label the image as concrete person. What is the distance between two images? The crucial question is: How to express the distance between two images? One possibility would be to compare the images pixel by pixel. But we can immediately feel, that this would not work. Each picture would contribute the same to the comparison, but not each pixel holds valuable information. For example background and hair pixels would arbitrary make the distance larger or smaller. Also for direct comparison we would need the faces to be perfectly aligned in all pictures and we would hope that the rotation of the head was always the same. To overcome this issue the PCA algorithm creates a set of principal components, which are called eigenfaces. Eigenfaces are images, that represent the main differences between all the images in the database. The recognizer first finds an average face by computing the average for each pixel in the image. Each eigenface represents the differecens from the avarage face. First eigenface will represent the most significant differences between all images and the average image and the last one the least significant differences. Here is the average image created by analyzing the faces of 10 consultants working at OCTO Technology, having 5 image of each consultant. And here are the first 5 Eigenfaces: Note that the consultants at OCTO are generally handsome guys and this ghost-look is characteristic to the eigenfaces of any group of persons. You can also notice that there is one face which is dominant. That is due to the fact that I had just limited number of pictures of my colleges and I could take a lot of myself using web cam. This way my face become a little dominant over others. Now when we have the average image and the eigenfaces, each image in the database can be represented as composition of these. Let’s say: Image1 = Avarage Image + 10% Eigenface 1 + 4% Eigenface 2 + … + 1% Eigenface 5 This basically means that we are able to express each image as a vector of percentages. The previous image becomes to our recognizer just a vector [0.10, 0.4, … , 0.1]. The previous equation is a slight simplification of the subject. You might be asking yourself how are the coefficients of each eigenface computed. If we would enter into the details, Now when we have expressed the image as a simple vector, we are able to say what is the distance between two images. Getting the distance of two vectors is not complicated and most of us remember it from school. If in 2D space we have the vectors [x1,y1] and [x2,y2] we now that the distance between these two can be visualized and computed as shown in the following picture: Now we can perform similar calculation for the vectors which represent our images. Behind the scenes This part will give you some more insights into how exactly are the eigenfaces computed and what is going on behind the scenes. Feel free to skip it if you do not want to enter more into the details. The key point in the PCA is the reduction of dimensional space. The question we could pose us is, how would we compare the images without eigenfaces. We would simply have to compare each pixel. Having and image with resolution 50 x 50 pixels, that would give us 2500 pixels, in other words we would have space of 2500 dimensions. When comparing using the eigenvalues we arrived to reduce the dimensional space – the number of eigenfaces is the number of dimensions in our new space. Remember the equation of the distance of two points – now in this equation each pixel would contribute to the distance between images. But not each pixel holds some meaningful information. The background behind the face, the cheeks, the forehead, hairs – these are the pixels which do not give meaningful information about the face. Instead the eyes, nose, ears are important.  In the terms of Image processing lot of pixels just bring noise to the computation of the distance. One of the main ideas of PCA is the reduction of the noise by reducing the number of the dimensions. Reducing the number of dimensions You may be asking yourself, how exactly can we reduce the number of dimensions? It is not easy to imagine the transition from a space where each pixel is one dimension to a space where each eigenface is one dimension. To understand this transition, take a look at this example: In two-dimensional space, each point is defined by its two coordinates. However if we know, that several points lay on the same line, we could identify the position of each point only by knowing it’s position along the line. To obtain the x and y coordinate of the point we would need to know the slope of the line and the position of the point on the line. We have reduced the dimension by one. In this example the slope of the line becomes the principal component, the same way our eigenfaces are principal components during the face recognition algorithm. Note that we can even estimate the position for the points which are not on the line, by using the projection of the point on the line (the case of the last point). From mathematical point of view eigenfaces are graphical representation of eigenvectors (sometimes called characteristic vectors) of co-variance matrix representing all the images. That it how they got their name. For more details on Principal Component Analysis and computation of eigenvectors and eigenfaces visit the following links: PCA and Eigenvectors Isn’t there an easy way? You might be thinking, that this is too complex as introduction to the subject and you might be asking yourself whether there is an easy way to start with face recognition? Well yes and no. Eigenfaces algorithm is the base of the research done in face recognition. Other analytic methods such as Linear Component Analysis and Independent Component Analysis build on the foundations defined by the eigenfaces algorithm. Gabor filters are used to identify the important features in the face and later eigenfaces algorithm can be used to compare these features. Neural Networks are complex subject, but it has been shown that rarely they have better performance then eigenfaces algorithm. Sometimes the image is first defined as linear combination of eigenfaces and than it’s describing vector is passed to the Neural Network. In other words eigenfaces algorithm builds really the base of face recognition. There are several open-source libraries which have implemented one or more of these methods, however as a developers we will not be able to use them without understanding how these algorithms work. Enough of theory now! Visit the second part of the article to see how to include face recognition into your web application. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged .NET , face recognitiom , machine learning . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “The basics of face recognition” startrinity 22/02/2012 à 14:32 Hello Jan, I see that you're interested in facial login systems. I have developed a face login system (startrinity.com) for web sites, please contact me if you are interested in promotion and profit sharing\r\nThanks Anna 10/12/2019 à 11:00 Interesting post, Thanks for sharing. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-10-20"},
{"website": "Octo", "title": "\n                FitNesse technical setup            ", "author": ["Thomas Queste"], "link": "https://blog.octo.com/en/fitnesse-technical-setup/", "abstract": "FitNesse technical setup Publication date 04/12/2010 by Thomas Queste Tweet Share 0 +1 LinkedIn 0 Getting FitNesse working for a development team is not particularly evident. You have many possibilities and some of them are not so effective. The Setup proposed here has a good combination of pros and cons. It will enable the work of two teams: one team of functional people using a central server and one team of developers writing the fixture straight from their workspace. Advantages and disadvantages Pros: Developer friendly One unique source of test cases: the central server Test pages backup along the source code Cons: Pages cannot be created locally; they have to be created on the central server The backup task seems hacky, but it does the job Team process The functional people use the central server to manage the test pages Each developer has its own FitNesse server, which imports the page from the central server and use the local workspace A job adds, delete and commits the changes made on the central server’s pages Basic rules: Pages must be created on the central server, never locally Pages must be modified remotely (using the “Edit Remotely” button); Local pages may be erased at the next sync, so use them carefully Setup Central Server Launcher The launcher will use a local checkout of the project. This checkout is used for compiling the fixture and project code, and also to be able to commit the changes made to the test pages. Launcher command line: java -jar fitnesse.jar -p 8086 -e 0 -r $dir_with_fitnesse_pages The “-e 0” argument disables the built-in versioning system of FitNesse. The “-r $dir” argument tells FitNesse to use this directory as the repository of pages. The whole command line arguments are described here: http://fitnesse.org/FitNesse.UserGuide.CommandLineArguments Classpath We use the “root” page ( http://localhost:8086/root ) to declare the server’s configuration and classpath. This page won’t be added to source control, because its content will be different on a developer’s workstation. The fitnesse-maven-plugin is used to create the list of jars we depend on. It reads the pom.xml file of our maven project, but it can also read a pom file directly from the central repository. Content of the root page: Use Slim engine instead of Fit\r\n!define TEST_SYSTEM {slim}\r\n\r\nChange the default port to avoid conflicts\r\n!define SLIM_PORT {62123}\r\n\r\nClasspath and pom file\r\n!path /home/hudson/workspace/myproject/fitnesse/target/test-classes\r\n!pomFile /home/hudson/workspace/myproject/fitnesse/pom.xml Backup job The backup job is simple. It is run by our Build server, every minute and launches a basic script, which adds, deletes and commits all changes within a directory. Here is a simple batch file for Subversion. The first argument should be the directory to backup. for /f \"tokens=2*\" %%i in ('svn status %1 ^| find \"?\"') do svn add \"%%i\"\r\nfor /f \"tokens=2*\" %%i in ('svn status %1 ^| find \"!\"') do svn delete \"%%i\"\r\nsvn commit -m \"Automatic commit\" %1 We did try to use a FitNesse plugin for Subversion to backup the pages automatically (and in a less hacky way) but that simply did not work at all. This plugin was: http://code.google.com/p/cm-subversion FitNesse has also a pseudo-native support of Git (see here ) but corporate Git repositories are still not the standard. SVN ignore All files in the working directory of FitNesse can be versioned, except: The “root” page we used earlier, this is the content.txt and properties.xml at the root of the working directory The “ErrorLogs” directory All zip files “*.zip” (there should be none because of the “-e 0” flag Source: http://stackoverflow.com/questions/249580/how-do-i-add-fitnesse-pages-to-version-control Developer server Launcher On a developer server, we don’t need the “-r” flag. The pages will be imported from the central server. Launcher command line: java -jar fitnesse.jar -p 8086 -e 0 Classpath As on the central server, the “root” page is used to configure the classpath. The content is the same, with the exception of the local path: Use Slim engine instead of Fit\r\n!define TEST_SYSTEM {slim}\r\n\r\nChange the default port to avoid conflicts\r\n!define SLIM_PORT {62123}\r\n\r\nClasspath and pom file\r\n!path /home/tom/dev/myproject/fitnesse/target/test-classes\r\n!pomFile /home/tom/dev/myproject/fitnesse/pom.xml Wiki import The Wiki Import feature is used to, well… import the pages from the central server. How-to: From the developer’s server, create a new page From the properties, paste the URL to the parent page you wish to import (something like http://fitnesse:8086/MyProject) Debugging Debugging is easy. Add a breakpoint to a fixture then create a “Remote Debug” configuration (within the Debug menu). Under FitNesse, just append the following at the end of the page you want to debug: ?responder=test&remote_debug=true Random problem (Bind, Socket exception) If you experience strange errors with no output, or better Socket and Bind exceptions (like in the screenshot), you should use the SLIM_PORT option (as used in this article). This basically shift the ports used by Slim to avoid conflicts with, let’s say, Tomcat. Add this: !define SLIM_PORT {62123} Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged FitNesse , Maven , SVN . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-12-04"},
{"website": "Octo", "title": "\n                Gamification: State of art            ", "author": ["Marion Duzac"], "link": "https://blog.octo.com/en/gamification-state-of-art/", "abstract": "Gamification: State of art Publication date 04/10/2011 by Marion Duzac Tweet Share 0 +1 LinkedIn 0 Since 2010, gamification has been a hot topic. Worshiped or hated, everybody talks about it. The subject has been discussed previously on our blog, from both a technical and a marketing point of view (read the articles: I’ll write code that writes code for food and Gamification: let’s play ). Friendly reminder for those who have missed it: gamification is the use of game play mechanics for non-game applications. Its implementation results in increased loyalty and engagement from the users. In France we start talking about it but initiatives are scarce. The first one to cross my mind is MobExplore , which is generally described as “The French SCVNGR ”. They both are mobile applications enabling to create scavengers or other types of games and challenges at specific places. To understand what is really happening in the gamification’s world, we have to take a look on the American side and focus on the speakers, the providers and the skeptics. The speakers In the US, where gamification was born, the theory is mainly supported by Gabe Zichermann , speaker, co-author of the book Game Based Marketing and creator of a famous blog about gamification.  « What gamification does is allow marketers to focus on what they know best — convincing consumers to take loyalty and purchasing actions — using a powerful toolkit of engagement gleaned from games. Armed with a new understanding of what makes people tick, and how to wind them up, marketers can build experiences that are enduring and engaging . » If you are interested by more insights on what gamification really is and on having various examples of it, he develops his point in this video . Jane Mc Gonigal , author of the book Reality is Broken , preaches enthusiastically for the idea that video games – especially serious games – can help people develop incredible skills. During her TED session she cites four positive impacts commonly felt when playing games: urgent optimism, social fabric, blissful productivity and epic meaning. She explains that gamers are better prepared to react fast, solve problems and act as teams when needed. She goes even further and says that playing games could help save the world. At first her point seems extreme but she makes us think about the use of game mechanics – and especially the use of serious games – in education field for instance. She was recently proven right as the players of the game Foldit helped scientists to produce an enzyme’s model that could be of major use in the research for anti-Aids drugs. Seth Priebatsch , Ninja Chief at SCVNGR, is another enthusiastic speaker. He describes SCVNGR’s business during a TED session, where he exposes the following idea: the previous decade was the one of social media; the current decade we are living in represents the game era. To end on a humoristic note, Jesse Schell , founder of Schell Games, has also talked about gamification during a session . He describes a totally gamified world where all our actions would be rewarded by points and where you would start your day earning points for brushing your teeth! Crazy… The providers Beyond theory, Americans are taking actions. GSPs (Gamification Service Providers) like Big Door, Badgeville, Bunchball or IActionable are becoming increasingly famous. Here are some examples of their interventions: BigDoor and DevHub : DevHub is a website builder which proposes a free hosting service. They needed to better monetize user interactions and encourage customers to buy additional features . They were also seeking to create a more engaging experience for their users. The solution proposed by BigDoor consisted in creating virtual currency that could be either earned or bought. User actions were rewarded by virtual money, which helped increase motivation and engagement. When virtual currency earned was not sufficient, customers could always use real dollars to obtain it and therefore the desired features. Another aspect of the gamification process was to create a leaderboard which promoted the site creation winners, giving them recognition among other users and increasing all the competitors motivation. The results were immediate: DevHub noticed an increased engagement rate of 300% and in three months, revenue from virtual goods grew to 30% of overall revenue. Badgeville and Bluefly : «Bluefly will use Badgeville to reward fashion-savvy shoppers for watching videos, creating wish lists, writing reviews, and reading blog posts, as they explore and interact with various elements of the site. The game will offer badges to players that highlight their status and draw attention to their fashion credentials within the Bluefly community. As players earn higher badges, they will receive rewards such as early access to products and special deals and discounts . » Bunchball and Playboy : “Gamification is the DNA woven throughout Playboy’s latest digital ventures to develop and engage our audience,” said Greg Johnson, Vice President, Digital Ventures at Playboy Enterprises, Inc. “Since launching Playboy’s Miss Social Facebook application in December 2010, we have over 80,000 active users interacting with the program. We’re seeing over 85% of the audience re-engage and 60% growth in revenue month over month . Bunchball’s powerful analytics tools provide us critical insights into user behaviors and what drives our audience. The Nitro platform’s flexibility enables us to test, iterate and optimize in real time , which is critical to the success of our business. » Those are only a few examples among the numerous initiatives being undertaken in the gamification world. Gartner has published predictions about the trend toward gamification: “By 2015, more than 50 percent of organizations that manage innovation processes will gamify those processes, according to Gartner, Inc. By 2014, a gamified service for consumer goods marketing and customer retention will become as important as Facebook, eBay or Amazon, and more than 70 percent of Global 2000 organizations will have at least one gamified application.” Skeptics Margareth Robertson has written a well-known article – Can’t play, won’t play – where she mainly criticizes a growing phenomenon: the pointsification . From her point of view, what is called gamification today is simply a system rewarding actions with points. « Gamification is the wrong word for the right idea. The word for what’s happening at the moment is pointsification. There are things that should be pointsified. There are things that should be gamified. There are things that should be both. There are many, many things that should be neither. » Kathy Sierra , the famous game developer, has pointed out the bad use gamification in a comment to Gabe Zichermann’s article.  « If a company uses gamification to increase their customer engagement based purely on deals/pricing/coupons/bargains, etc. then sure, it is a fine and appropriate use. But for those who hope to compete on something of value beyond “lowest price / best deal”, then gamification may do far more harm than good. » I would like to end on a funny criticism of social games by Ian Bogost , creator of the CowClicking : « You get a cow. You can click on it. In six hours, you can click it again. Clicking earns you clicks. You can buy custom “premium” cows through micropayments (the Cow Clicker currency is called “mooney”), and you can buy your way out of the time delay by spending it. You can publish feed stories about clicking your cow, and you can click friends’ cow clicks in their feed stories. Cow Clicker is Facebook games distilled to their essence. » (Any resemblance to a real game is of course purely coincidental!) Most of the skeptics criticize the bad use of gamification and not the intrinsic concept itself. As gamification is becoming trendy some are tempted to benefit from its popularity without understanding the global stake. Adding a points system to a website will never enhance the user experience and yet this is what gamification is all about: providing a meaningful and fun experience to the players. In a next article on this topic we will analyze gamification in HR: why and how is gamification coming to our offices? Can the American models fit the French businesses? Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged Engagement , Game , Gamification , Loyalty , Marketing . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Gamification: State of art” Olivier Fontenelle 04/10/2011 à 23:13 Dear Marion,\r\n\r\nVery interesting article, I'm waiting for the next one!\r\n\r\nOlivier Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-10-04"},
{"website": "Octo", "title": "\n                Automating Over The Air Deployment for iPhone            ", "author": ["Vincent Daubry"], "link": "https://blog.octo.com/en/automating-over-the-air-deployment-for-iphone/", "abstract": "Automating Over The Air Deployment for iPhone Publication date 16/11/2010 by Vincent Daubry Tweet Share 0 +1 LinkedIn 0 Automating Over The Air Deployment for iPhone Since the release of the iOS4 we are able to distribute iPhone applications “Over The Air” (i.e: directly downloading the application from the iPhone without using iTunes). This greatly simplifies the deployment process especially for entreprises where iTunes is rarely a corporate tool. It also allows you to create your own enterprise App Store . But until now it was a fully manual process : not anymore ! In this article we’ll see how to automate the deployment with a software factory, thus making the deployment more reliable and more productive . The missing command line : Over the air deployment is based on a new command in the build menu of Xcode called “build and archive” which packages the application with an embedded provisioning profile. There have been many questions on how to integrate this feature in a continuous integration process. The “xcodebuild” command is well known : it builds an Xcode project from the command line and generates an “.app” file. You can use this command to build your application from a script ran by a software factory . But the generated file cannot be distributed over the air since it misses the embedded provisioning profile. We need to build the project into an “.ipa” file, containing the provisioning profile and signed with your developper identity. To be able to find the command line, the trick was to watch the system console log while running a “build and archive” through Xcode. You will then see something similar to this : Console log extract : 31/10/10 20:57:16 Xcode[16510] Running /usr/bin/xcrun with ( “-sdk”, iphoneos, PackageApplication, “-v”, “/Users/barbu/Library/MobileDevice/Archived Applications/FDA8B2FA-5AE1-43E7-BF9F-CD32FD258907.apparchive/TestTemplate.app”, “-o”, “/Users/barbu/Library/MobileDevice/Archived Applications/FDA8B2FA-5AE1-43E7-BF9F-CD32FD258907.apparchive/TestTemplate.ipa”, “–sign”, “iPhone Developer: M VINCENT DAUBRY (J9TS3TJRYX)”, “–embed”, “/Users/barbu/Library/MobileDevice/Provisioning Profiles/68D899AB-9FEB-4CBB-A080-1078FF2FABCF.mobileprovision” ) Writing the script : You can now run a script which will : Build your application into an .app file xcodebuild -target \"${PROJECT_NAME}\" -sdk \"${TARGET_SDK}\" -configuration Release Package it into an .ipa file /usr/bin/xcrun -sdk iphoneos PackageApplication -v \"${RELEASE_BUILDDIR}/${APPLICATION_NAME}.app\" -o \"${BUILD_HISTORY_DIR}/${APPLICATION_NAME}.ipa\" --sign \"${DEVELOPER_NAME}\" --embed \"${PROVISONING_PROFILE}” Here is a sample build script. EDIT: June 13 th , 2012 Since this article has been published, the industrialization of iOS development has been improved. Specially with the release of the XCode plugin for Jenkins . One year ago, OCTO launched the Appaloosa service, the private store service to help our teams, our customers and anybody who needs to go further with the Over-The-Air distribution of their private apps. Appaloosa provides its own Jenkins plugin to automatically deploy the latest version of an application to the private store. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 40 commentaires sur “Automating Over The Air Deployment for iPhone” Zekel 16/11/2010 à 21:49 Looks like you have two typo `DEVELOPPER_NAME` and `PROVISONNING_PROFILE` in this line: \"${BUILD_HISTORY_DIR}/${APPLICATION_NAME}.ipa\" --sign \"${DEVELOPPER_NAME}\" --embed \"${PROVISONNING_PROFILE}”\r\n\r\nThanks for answering my StackOverflow question! Scott 13/12/2010 à 20:58 I'm not clear on how this works.  Do I save you build script as-is, or make changes?  Where exactly do I invoke it from? Vincent Daubry 14/12/2010 à 20:20 Hi Scott,\r\n\r\nYou have to change the environment variables (project name, IOS version, etc). They are at the top of the script.\r\n\r\nYou can run this script from command line or from a continuous integration tool such as hudson.\r\n\r\nVincent. chief 15/12/2010 à 17:14 Many thanks for this tip, one thing that tripped me up was trying to use relative paths int he script which cause it to succeed but leave my IPA in some weird /var/tmp location. So use absolute paths people! James Jennings 16/12/2010 à 03:06 Nice post, I've taken an interest in automating the build and distribution process lately. I've got a Hudson rig running and packaging my apps for OTA distribution.\r\n\r\nI'm actually using a different technique though, which does not use the xcrun command.  I use the following lines to package up the .ipa file, and coupled with an appropriately configured .plist, this has been sufficient for OTA distribution.\r\n\r\nmkdir Payload\r\ncp -rp build/Release-iphoneos/MyApp.app Payload/\r\nzip -r MyApp.ipa Payload\r\nrm -rf Payload Cédric 11/01/2011 à 18:45 Hello,\r\n\r\nI have similar problems here to run such commands and integrate them with Hudson. In fact, we can only build the app in Debug (and no codesign) config. We don't reach the xcrun stage, but the xcodebuild command fails already. The error is that Xcode is unable to codesign the app. Anyone has an idea? It is not exactly the same problem as solved in this post, but closely related. \r\n\r\nThanks! Vincent daubry 13/01/2011 à 13:12 Hi,\n\nHufson creates a hidden directory \".hudson\" in your user directory. You will find the checked out xcode project there. Try opening it and build the project from xcode, you will see what's going wrong.\n\nHope this helps,\nVincent zulfi shah 21/01/2011 à 04:32 How do we integrate this with hudson builds? I believe hudson generates the app files itself and creates the whole web front-end for you. How would you configure it to link to this ipa file? Vincent Daubry 21/01/2011 à 07:16 Hello,\r\n\r\nThis article was specifically dealing with packaging and signing your application from the command line.\r\nIt assumes that you have already a hudson job building to build an Xcode project (to do so, you have to create a custom job inside Xcode and run a shell script with this custom job.)\r\nHere is an article explaining this process :\r\n\r\nhttp://nachbaur.com/blog/how-to-automate-your-iphone-app-builds-with-hudson\r\n\r\nHope this helps,\r\nVincent. zs 22/01/2011 à 04:10 When I add this to my Hudson build, it makes the .ipa file just fine, but when I point to it from the browser, it just says \"Download Failed. Safari cannot download this file\". But when I use XCode's Build and Archive - (Distribute for Enterprise), I get to enter a URL, a title and all that good stuff, and it generates a plist file for me that I need to point to before it works. Any idea what I'm missing? Vincent Daubry 22/01/2011 à 07:40 Hi,\r\n\r\nYou're just one step from having this working : what you need to do now is to create the PList you're talking about. You can either do it by hand (and use the script only for updating an existing application), or you can even do it within the script (but are you creating you new application every day, isn't it going to take longer to write the script than to generate manually the next 10 project you'll be working on.. ?)\r\n\r\n- If you want to create it by hand : just run the build and archive from Xcode as you describe. Put the ipa and Plist somewhere. And when you want to update your application just have the script generate a new ipa and replace the old one.\r\n\r\n- If you want to generate the Plist within the shell script : i suggest you look towards PList command line tools such as PlistBuddy :\r\nhttp://developer.apple.com/library/mac/#DOCUMENTATION/Darwin/Reference/ManPages/man8/PlistBuddy.8.html\r\n\r\nHope this helps,\r\nVincent zs 24/01/2011 à 23:23 Great! Thanks for all the help. Last question: how do I configure Hudson to convert the download link on the webpage to use instead of the usual vdaubry 26/01/2011 à 08:10 Could you explain what download link you are talking about ?\r\nActualy hudson is just running your script, so whatever you want to perform it wont be by configuring hudson. You should rather be looking at shell scripting tutorials. Alex F 27/01/2011 à 15:58 Hi Vincent, \r\n\r\nI enjoy your post as I am planning to integrate UIAutomation with Hudson and this proves helpful.\r\nHere are the details:\r\n - whenever a dev checks-in his code, hudson would take the lead and create builds of each iOS type.\r\n - then I would like to start the iOS simulator and subsequently run some Instruments templates for UIAutomation those.\r\n\r\nThe problem is that I cannot get the simulator to start through command line. My command would be something like:\r\n\r\nxcrun -sdk iphonesimulator4.2 \"\" -v \"/../debug-iphonesimulator\" \r\n\r\nCan you tell me how to start the iOS simulator?\r\n\r\nMerci Brennan Stehling 23/02/2011 à 06:37 What about generating the OTA manifest file? Is that possible? Vincent Daubry 23/02/2011 à 10:54 Hi,\r\n\r\nThe OTA manifest file can be generated through Xcode using \"build and archive\" -> share -> distribute for enterprise.\r\n\r\nThere might be a way to do it from the command line, but as this file needs to be generated only once per project i don't think there is any point automating this process.\r\n\r\n (if you need to update the content of the manifest, to change the version number for example, you can use the PlistBuddy command)\r\n\r\nHope this helps,\r\nVincent Pronob Ashwin 13/03/2011 à 23:35 Hey Vincent\r\n\r\nI just got XCode 4 with 4.3. Now after a couple of mods, I was able to run the script. Now OTA will work on iPad with 4.2.1 but for 4.3 devices, the OTA ipa file installs but for some reason disappears, right after the install completes. However if I restart my iPad the app magically appears. Do you know whats going on? Also how in the world do I create ipa files using XCode 4. It creates some xarch...file. Its so hard for me to find info on this..\r\nthanks\r\nPronob Vincent Daubry 14/03/2011 à 10:09 Hi,\r\n\r\nI have no idea why you have this problem, maybe you should restore your iPad with iTunes.\r\n\r\nHope this helps,\r\nVincent Aki 17/03/2011 à 14:28 Hi Pronob,\r\n\r\nPlease can you list the mods you made to get the approach to work with XCode 4?\r\n\r\nI get an error after codesign is complete:\r\n\"error: Cannot read format tag from /var/folders/QR/QRgbPnxnFpqs9od6W9sNjL5P7qw/-Tmp-/siDo6YFpA7/entitlements_rawWA7x3EiU'\" The entitlements_rawWA7x3EiU file is empty. \r\n\r\nThanks,\r\n\r\nAki David Dunham 23/03/2011 à 02:33 Merci! The one thing that gave me trouble was that paths (e.g. BUILD_HISTORY_DIR) need to be absolute. John 01/04/2011 à 01:40 Has anyone got this to work for uploading to testflight?\r\n\r\nI'm getting through both steps with no indication of any errors. I can upload to testflight; but the app then doesn't completely download. \r\n\r\nUsing Xcode 4.0.1, which apparently changes the Entitlements file a bit. Michael 02/05/2011 à 12:23 Aki, did you fix your \"error: Cannot read format tag from ....\" error? I get exactly the same, but with XCode 3.2.4, so not just a problem with XCode 4 I guess. Jay Van Vark 27/05/2011 à 17:11 I am having trouble with the xcrun part and trying to embed the profile -- seems to be an issue with the appname having a space... I have tried \\ escaping etc etc -- everything I can think of..\r\nAPPNAME=\"Dr. Csillag\"\r\n\r\nI get\r\nerror: Unable to copy '/Users/jvanvark/Library/MobileDevice/Provisioning\\ Profiles/09314421-9C43-4207-BFB0-B263013361FD.mobileprovision' to '/var/folders/zY/zYMtZNeGGNacUATBxDls7U+++TI/-Tmp-/DalULAfDVc/Payload/Dr. Csillag.app/embedded.mobileprovision'\r\n\r\nIf I do this by hand and put in the \\ for the space - it works fine... Any suggestions!? David Casserly 10/08/2011 à 17:09 Nice work, this was useful and works for me. AmitChabra 12/08/2011 à 01:20 Hi Vencent,\r\n\r\n   I have tried your script. Its working absolutely fine in every fine.Thanks for the nice tutorial and script. But lately I have added a dependency project in my main project and dependency project is basically a ZXing widget and ZXing library for QRCode scanning. Its compiling and working fine in the XCode. But when I try the script you have provided its failing. \r\n\r\nI am posting error \r\n\r\n    cd /Users/Companyname/Development/ProjectName_1.0.0/iphone\r\n    setenv LANG en_US.US-ASCII\r\n    setenv PATH \"/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin:/Developer/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/usr/X11/bin\"\r\n    /Developer/Platforms/iPhoneOS.platform/Developer/usr/bin/llvm-gcc-4.2 -x objective-c++ -arch armv7 -fmessage-length=0 -pipe -Wno-trigraphs -fpascal-strings -Os -Wreturn-type -Wunused-variable -DPHONEGAP_FRAMEWORK=YES -isysroot /Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS4.3.sdk -gdwarf-2 -fvisibility=hidden -fvisibility-inlines-hidden -mthumb -miphoneos-version-min=3.1 -I/Users/Companyname/Development/ProjectName_1.0.0/iphone/build/ProjectName.build/Release-iphoneos/ProjectName.build/ProjectName.hmap -I/Users/Companyname/Development/ProjectName_1.0.0/iphone/build/Release-iphoneos/include -I/Users/Companyname/Development/ProjectName_1.0.0/iphone/build/ProjectName.build/Release-iphoneos/ProjectName.build/DerivedSources/armv7 -I/Users/Companyname/Development/ProjectName_1.0.0/iphone/build/ProjectName.build/Release-iphoneos/ProjectName.build/DerivedSources -F/Users/Companyname/Development/ProjectName_1.0.0/iphone/build/Release-iphoneos -F/Users/Shared/PhoneGap/Frameworks -include /var/folders/mZ/mZhhkYHAEAmvIdK006LaM++++TI/-Caches-/com.apple.Xcode.501/SharedPrecompiledHeaders/ProjectName-Prefix-aicfsokcunwhegduttyycdnurxjq/ProjectName-Prefix.pch -c /Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm -o /Users/Companyname/Development/ProjectName_1.0.0/iphone/build/ProjectName.build/Release-iphoneos/ProjectName.build/Objects-normal/armv7/BarcodeScanner.o\r\nIn file included from /Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm:8:\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.h:10:34: error: ZXingWidgetController.h: No such file or directory\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.h:11:25: error: QRCodeReader.h: No such file or directory\r\nIn file included from /Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm:8:\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.h:15: error: cannot find protocol declaration for 'ZXingDelegate'\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm: In function 'void -[BarcodeScanner scan:withDict:](BarcodeScanner*, objc_selector*, NSMutableArray*, NSMutableDictionary*)':\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm:30: error: 'ZXingWidgetController' was not declared in this scope\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm:30: error: 'widgetController' was not declared in this scope\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm:31: error: 'QRCodeReader' was not declared in this scope\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm:31: error: 'qrcodeReader' was not declared in this scope\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm: At global scope:\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm:40: error: expected type-specifier before 'ZXingWidgetController'\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm:40: error: expected `)' before 'ZXingWidgetController'\r\n/Users/Companyname/Development/ProjectName_1.0.0/iphone/ProjectName/BarcodeScanner.mm:40: internal compiler error: tree check: expected class 'type', have 'exceptional' (error_mark) in objc_push_parm, at objc/objc-act.c:18143\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee  for instructions.\r\n\r\n\r\nAny lead is highly appreciable. \r\n\r\n\r\nThanks and Regards,\r\nAmit Chabra Vincent Daubry 12/08/2011 à 08:14 Hi Amit,\r\n\r\nFor some reason the compiler cannot find the library you've included :\r\n\r\n\"error: ZXingWidgetController.h: No such file or directory\"\r\n\r\n\"error: ‘QRCodeReader’ was not declared in this scope\"\r\n\r\nAnyway this is a problem specific to the library you're using, you should try to submit your problem on the ZXing help group.\r\n\r\nP.S: It seems that some people have similar errors on the ZXing group :\r\n\r\nhttp://groups.google.com/group/zxing/browse_thread/thread/bf1dc07949baef28\r\n\r\nHope this helps,\r\nVincent. Oleg 31/08/2011 à 14:44 Hi Vencent,\r\n\r\nHow to add a program organizer? Vincent Daubry 31/08/2011 à 15:36 Hi Oleg, what do you mean by \"program organizer\".\r\n\r\nI dont see what it has to do with the subject of this article. Oleg 31/08/2011 à 15:51 Hi Vencent, sorry for my English. I mean an Organizer in XCode. When I use XCode’s Build and Archive –  the app then enters in the organizer tang 18/09/2011 à 17:41 Hi Vincent,\r\n\r\nI'm trying to apply the technique you used to figure out what parameters PackageApplication accepts. You mentioned you were observing the system logs while performing the \"build and archive\". Which logs were you looking at? I opened up \"Console\" app in utilities but I wasn't able to observe any Xcode internal logs.\r\n\r\nThanks!\r\nTang Julian 03/10/2011 à 20:26 Hi Tang,\r\n\r\nPackageApplication is a perl script so you can look at it in a regular text editor. It's pretty readable and will show you all the options and steps. The script resides in /Developer/Platforms/iphoneos.Platforms/Developer/usr/bin/PackageApplication\r\n\r\nI'm writing this from memory so the \"iphoneos.Platforms\" maye be wrong but it should be obvious what the name is if you look in the directory. Actually, other parts of the path may wrong, knowing my memory. Just do a \r\n\r\nfind /Developer -name PackageApplication \r\n\r\nand it will show you. :)\r\n\r\nJulian Jacky 09/11/2011 à 10:57 hi, \r\n\r\ndoes anyone know where to find the system console log?\r\n\r\nJacky Vincent Daubry 10/11/2011 à 00:30 Hi,\r\n\r\nYou can access the system log via the console app. Try to type \"console\" in spotlight and you should find it.\r\n\r\nHope this helps,\r\nVincent Jacky 11/11/2011 à 08:14 hi Vincent,\r\n\r\nMany thanks for your replay.\r\n\r\nI use the command line \"tail -f /var/log/system.log\" to follow my console system log. But I haven't find the system console log while running a “build and archive” through Xcode. Satheeshwaran 19/11/2012 à 07:19 I get  this error:\r\n\r\nCodesigning '' with 'iPhone Developer: '\r\n+ /usr/bin/codesign --force --preserve-metadata=identifier,entitlements,resource-rules --sign iPhone Developer:  --resource-rules=/var/folders/nf/bz6q3hqd7n7bm2dt8l_1l2fw0000gn/T/cBrNET5m6D/Payload/abc.app/ResourceRules.plist /var/folders/nf/bz6q3hqd7n7bm2dt8l_1l2fw0000gn/T/cBrNET5m6D/Payload/abc.app\r\nProgram /usr/bin/codesign returned 1 : [/var/folders/nf/bz6q3hqd7n7bm2dt8l_1l2fw0000gn/T/cBrNET5m6D/Payload/abc.app: replacing existing signature\r\n/var/folders/nf/bz6q3hqd7n7bm2dt8l_1l2fw0000gn/T/cBrNET5m6D/Payload/abc.app: object file format unrecognized, invalid, or unsuitable\r\n]\r\nerror: /usr/bin/codesign --force --preserve-metadata=identifier,entitlements,resource-rules --sign iPhone Developer: ) --resource-rules=/var/folders/nf/bz6q3hqd7n7bm2dt8l_1l2fw0000gn/T/cBrNET5m6D/Payload/abc.app/ResourceRules.plist /var/folders/nf/bz6q3hqd7n7bm2dt8l_1l2fw0000gn/T/cBrNET5m6D/Payload/abc.app failed with error 1. Output: /var/folders/nf/bz6q3hqd7n7bm2dt8l_1l2fw0000gn/T/cBrNET5m6D/Payload/abc.app: replacing existing signature\r\n/var/folders/nf/bz6q3hqd7n7bm2dt8l_1l2fw0000gn/T/cBrNET5m6D/Payload/abc.app: object file format unrecognized, invalid, or unsuitable\r\n\r\nPlease help me, i am getting screwed. Raj 26/04/2013 à 12:34 I m new to mac and xcode Pls tell me how to run your script and what change need to be made and how?\r\n\r\nThanks in advance Muhammad Akhtar 29/10/2014 à 08:57 Hi Vincent \r\nThe script is working fine and after \"Build success\" it will return an error \r\n\r\n\"** BUILD SUCCEEDED **\r\n\r\nerror: Specified application doesn't exist or isn't a bundle directory : '/Users/username/Desktop/appname/build/Release-iphoneos/.app'\"\r\n\r\ni need your help to sort out this issue.\r\nI'm using Xcode 6.1\r\nRegards \r\nAkhtar sathish 05/06/2015 à 10:59 HI,\r\n       The script works well. But i want to add appIcon and launchImages through script. How do i achieve. please let me. Eula 02/01/2016 à 03:03 Excellent article comme d'hab Steven 14/09/2017 à 08:49 Still valid and great until 2017. I have a build error due to Apple Mach-O Linker fails. I need to find a command line to try to debug what's wrong. Glad I can find it here. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-16"},
{"website": "Octo", "title": "\n                Continuous Delivery: How do we deliver in 3 clicks to 7000 machines?            ", "author": ["Maxence Modelin"], "link": "https://blog.octo.com/en/continuous-delivery-how-do-we-deliver-in-3-clicks-to-7000-machines/", "abstract": "Continuous Delivery: How do we deliver in 3 clicks to 7000 machines? Publication date 12/09/2011 by Maxence Modelin Tweet Share 0 +1 LinkedIn 0 Through this post I would like to share with you the continuous delivery chain that we’ve successfully set up. My point is to describe the whole chain (from the Svn check in to the feedback loop to get the deployment status) and highlight some tricks that we discovered. In our context, we cannot speak about Continuous Delivery without addressing the DevOps approach that we clearly have in our teams. This approach gives the opportunity to share our needs and exchange points of views between the Developer and Operational teams. Some of the important points are described here. So let’s check out how our DevOps team can build and deploy 7000 clients in 3 clicks. Context The solution we have to deploy is a Windows service developed in C# which communicates via FTPS with a server developed in Java. In this article we will focus exclusively on the .NET client application deployment. The Continuous Integration platform is used by both projects, which is why we do not use TFS but Hudson/Maven for the .NET build. We deploy to an installed base of over 7000 company-provided machines located everywhere in France. Technology used From a subversion check in to the deployment of the binaries on the machine we can find several technologies: Regarding the Continuous Integration system we use Jenkins , Maven and Nexus . This is a typical java CI but it works great for .NET solutions as well and it’s really easy to deal with. The OCS Inventory software has been chosen to administrate the fleet of machines. OCS Inventory is based on a web server centralizing all information sent by the OCS Agent installed on each machine. Through this tool, you can get Registry information, hardware information and deploy packages on machines without any action from the user. Machines can be organized through dynamic or static groups. Application connection monitoring with Zabbix The following schema represents the software’s interactions and actions in our Deployment pipeline. 3 clicks to deploy: Deployment Pipeline description First click: a svn commit launches the Continuous Integration process Like in all automated build chains, everything starts with a commit; the build is done and tested automatically afterwards. Our chain is composed of Jenkins as executor, Maven as builder and Nexus as artifacts repository. This is a very common CI chain so I won’t describe it more here. But I would like to highlight 2 points: About Versioning Once you have to deploy a version of an application to many machines, it is important to properly manage versioning. The reasons range from deployment reactivity to bug cases (fixed with the last version but not the previous one), as well as deployment failures and the controlled deployment of new features. Each DLL of your service has to contain a version number and the setup package has to carry it as well to know which version it contains. In our process, Maven’s Pom.xml file carries the version and serves as a foundation for the versioning system (dll version, setup information, artifact names). Technically, before the build, a Powershell script executed by Maven looks for the assembly info files and replaces the version number inside. About Maven execution steps Each step is managed by an in-house cmd script executed by the Maven Exec plugin following the principle “Create a Script for Each Stage in Your Deployment Pipeline [i] ”. Second click: OCS package creation A Unix shell script (written by Ops) takes a specific version of a release artifact from Nexus and pushes it as a package ready to deploy in the OCS database. DevOps point Ops and Dev had to sit and talk here. Deploying OCS packages is under the Ops’ responsibility but application setup is known and developed by Devs. It could take awhile for both teams to understand each others’ requirements. Instead of exchanging many informal mails (which could be a really cold media) and being exposed to misunderstandings on wiki documentation, 1 member from each team can sit together and work on scripts and the setup process to deliver a main first version. Definitely much better than 25 mails and 10 incomprehensible documentation pages (even if it has to be documented as well – and understandable). Note: this step shouldn’t be manual In fact this click should not exist because it does not have any functional importance. It should be the last automatic step of the Maven pom file. The deployment should be reduced to only 2 manual steps: – a new build version delivery “controlled” by Dev – package deployment “controlled” by Ops. The OCS DB insertion as a technical step does not have to be manual and a next iteration on this process will fix it with an Unix shell script. Third click: assign the package to users In our context, deployment waves are not effective for all users. Therefore this step is a functional step and has no reason to be automatic: Ops decide to whom they will send the package and assign it through the OCS Inventory tool. DevOps point Ops people are busy people. They have a business to run. When setting up the deployment pipeline, we were using the same OCS server to deploy in Production or for testing purposes. As you can guess, we did not succeed in writing the deployment script on our first shot. We iterated on it many times. To give the Dev team more freedom (and so on more responsibility) and get less testing requests, the Ops team granted one of the Dev guys a Power User account on OCS. This means that Devs could deploy packages to customers by themselves. The point, here, is that instead of controlling each of our actions, Ops can decide to entrust a Dev guy with responsibility for the test deployment. This action accelerates development and test cycles, involves theDev team in the deployment’s consequences (and how to control them) and gives the Ops team more time to focus on their real work. Definitely a win-win deal. Is an OCS Inventory test platform needed? At one moment, we thought about having an OCS Inventory platform just for dev and testing purposes (instead of working with the production platform). Due to other priorities we did not implement that idea. I finally think that was a good thing because it forces dev teams to feel more responsible about the run and check all impacts of their actions. How do we check that everything works? Deploying an application to clients is different from deploying to a server, you cannot easily control if it was successfully deployed. Our feedback loop is done by 2 tools (OCS Inventory and Zabbix) and a fake user. Through a fake user For each deployment wave, we prepare a machine updated with the current version. This fake client is made part of the group of users to whom we deploy the package. Once the deployment starts we force an OCS Contact on our fake client to download the package and check that everything proceeded smoothly. Through OCS The delivery status set by OCS just informs us if the whole package was transferred and the install script launched, but if the installation script ultimately fails, OCS will not report it. To get a feedback on the deployment status, we write setup information inside registry keys and force an OCS contact at the end of the deployment script. OCS was configured to upload the registry value of these keys and through this system we could make OCS dynamic groups based on these values to detect deployment failures. DevOps point Without deployment testing handled by Dev team (instead of Ops team), Dev would never have cared about this point. Through Zabbix Our application establishes connections with the server. So we can check user connection server reports as feedback loop as well. To read run reports, Ops gave the developers’ team credentials to use Zabbix and they created some graphics to check what was relevant for us (and not for them). As you can read on the zabbix graphic, the deployment was performed in 5 waves. The first and second waves (10 and 100 users) were beta and then 3 waves covering 1K, 4K and finally 2K users were deployed. Successive deployment waves are spaced by at least 1 week, giving the opportunity to react if an error happens. The server is never deployed the same day as client applications to mitigate the risk of failure (and in case of error being able to pinpoint easily the issue). An important point Deployment and run feedback are important for both Dev and Ops teams. If they aware of errors, Dev guys feel more involved and are more reactive to fix errors. And the Ops team does not have to handle and ask forever to fix them. Zabbix and OCS Inventory provide interfaces to have an overview on run. It’s an Ops responsibility to set them up and help Dev to use it. But once it’s done, both teams save a lot of time. Deployment results So far, we have a 1.5% failure rate during the deployment process. Most failures are caused by user machines rebooting during the deployment, viruses, broken OS or loss of the Internet connection. Very few are caused directly by the OCS Inventory tool. Following a deployment, 80% of the targeted clients are deployed within 2 days and 99% within the week. This delay is caused by the fact that it is the OCS Agent in each machine that decides when to contact the server. The maximum deployment wave was for 4000 users in a row. Finally From my point of view, one of the bases of Continuous Delivery is to consider that you have to “Deploy the Same Way to Every Environment”. So we used the same chain for testing as for production deployment. Because of this ongoing delivery testing, we (Dev and Ops) were more confident about our delivery process. One by one, each technical manual deployment step has been automated. A few operations remain manual as they require a functional validation. The Dev team’s definition-of-done then needs to be switched from “producing a binary files zip“ to “delivering a ready-to-be-installed-or-upgraded-and-monitored package“ so that Ops simply have to select who will get the new package (and not create, configure and deliver the whole package).. So, 2 clicks should definitely be enough to deploy a solution… :) [i] From the book “Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation” written by J. Humble and D. Farley Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Consulting Chronicles and tagged .NET , CI , Continuous integration , DevOps . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 7 commentaires sur “Continuous Delivery: How do we deliver in 3 clicks to 7000 machines?” Gregory Boissinot 12/09/2011 à 10:19 Why do you have chosen Maven?\r\nAccording your architecture, all Maven core features are not used.\r\nYou use only the versioning feature of Maven wich is a very poor approach and a limited approach to manage no Java artefacts.\r\nAnd I am very curious about your .Net build process from Maven. Could you explain your build process and give details? Maxence MODELIN 12/09/2011 à 11:05 Hey Gregory! \r\nAt the beginning, for the .NET build we were only using Hudson (which is quite easy to use for dev). But Ops asked to have artifacts in Nexus and we couldn't push them there from Hudson. So we did like the server Java CI and start to use Maven inside ours. So far, it meets our needs so we did not look to make it better. But I will be interested by the features that you've seen to improve it.\r\nI may write a fast post about our .Net build process because by post comments it's a pain and I need schemes. Andrei 25/10/2011 à 18:04 I apologise for the off-topic (this comment can be safely delete) - what software did you use for the above server diagrams? Exemple: https://blog.octo.com/wp-content/uploads/2011/09/Global-Architecture-Schema-bis.png\r\n\r\nThanks! Maxence MODELIN 26/10/2011 à 16:20 Hey Andrei, I've done my diagrams with PowerPoint :) Andrei 26/10/2011 à 16:47 I definitely have to start looking into PowerPoint :-D\r\nAnd can I find all the clip arts (ops and shell script, for example) in the stock collection that comes with PowerPoint? Which version of PowerPoint do you use?\r\n\r\nThanks,\r\nAndrei Rinku 23/01/2015 à 20:00 Hi ... Nice article but will you able to guide how to do a continuous integration for Hadoop application deployment? devops online training in hyderabad 31/10/2017 à 06:13 Nice Article. In short description good explanation about the DevOps. Thanks For sharing the informative news. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-09-12"},
{"website": "Octo", "title": "\n                Behavior Driven Development using MVVM pattern and GreenPepper tests            ", "author": ["Mathieu Despriee"], "link": "https://blog.octo.com/en/behavior-driven-development-using-mvvm-pattern-and-greenpepper-tests/", "abstract": "Behavior Driven Development using MVVM pattern and GreenPepper tests Publication date 06/01/2011 by Mathieu Despriee Tweet Share 0 +1 LinkedIn 0 MVVM is a quite mature design pattern one can use with the microsoft WPF framework, or with Silverlight. You’ll find a lot of literature and tools on the web to start a new development with this pattern. In this article, we show how MVVM can be implemented for a quite large application, and the value it brings in a Behavior Driven Development approach. Context The project consists in the development of a point-of-sale application and management solution dedicated to corner shops. The sale application is a .NET thick-client with a WPF GUI. 95%  of the user interactions with this application are variants of a “sale a  product” scenario, with various degrees of induced complexity : ranging  from a simple change of sold quantity, to a real-time interaction with a  third-party system for product booking and e-voucher printing. As  a consequence, most business scenarios are entangled in code,  leading to a very high risk of regression during development. The company was already using an automated test tool, simulating clicks on the GUI, but this was a separated development stream, in a dedicated technology, and thus painful to develop and maintain. To mitigate these risks, we decided to combine the following 2 approaches : From  a methodology point of view, we decided to use a Behavior Driven  Development approach using the GreenPepper test framework. All the  specifications are written in GreenPepper wiki pages. From  a software architecture point of view, we decided to implement a MVVM  design pattern, so that the automated tests interact with the software  at the highest possible level (very close to the view, above the business  layer), without manipulating directly the graphical components. Behavior Driven Development ? I will not describe here the BDD approach in details. The main idea is to provide to non-technical people a way to write an acceptance test harness by describing the software behavior through scenarios. In BDD, the requirements are usually described using the given/when/then formalism : Given some initial context When some event occurs Then ensure some outcomes With GreenPepper, we can get very close to this formalism (setup/accept/check) and even more since the version 2.6 with special annotations. Let’s see how a design pattern can help integrating with GreenPepper. MVVM MVVM  is an avatar of the traditional MVC pattern, leveraging on the  capabilities of WPF to have a more dedicated and efficient pattern for  this technology. M-V-VM  stands for Model – View – ViewModel, where ViewModel is a “Model of the  View”, an abstraction of the GUI, exposing the content to be displayed  and implementing all the behavior of the GUI, but deferring all the  graphic stuff to the View layer. In  concrete terms, a ViewModel is a simple POCO class exposing Properties  coming from the Model, and exposing “Command” methods that will be bound  to events occurring on the View. It has (almost) no dependency to  graphical libraries, nor to the View, thus achieving a very loose coupling. The  interaction with the View relies on the very powerful (yet complex)  mechanism of WPF bindings. Bindings allow to map a Property from a  ViewModel in the View in both directions (an update of the property is  automatically reflected on the GUI, with no code, and an input from the  user is forwarded to the underlying property). They also allow to bind  the Command methods to events on the GUI (mouse clicks and so on, are  propagated to the ViewModel). [Note : There are similarities between the usual responsibility of a Controller in MVC and the binding mechanisms of WPF.] One can also see the VM as a proxy on top of the Model : it manipulates the same data, but with concepts closer to the View. What  remains in the View ? Only pure graphical stuff : windows, controls,  styles. Almost no code, except for some very local and very graphical  specific logics. Ok, MVVM looks nice, so what ? So  … with this pattern, should I decide to remove the View layer, the software is …  fully functional. It compiles, and can be run if I provide the glue-code  to trigger the ViewModel layer. The code doing the bridge between a BDD test and the ViewModel is as simple as this : // Open a form to reference a new product ?\r\nNewProductVM  formVM = new NewProductVM();\r\n\r\n// Emulate a user entry in the label and price fields ?\r\n// (params will come from values set in GreenPepper page)\r\nformVM.Label = param1;\r\nformVM.Price = param2;\r\n\r\n// Validate the entered value ?\r\nif( formVM.CanSave() ) ...\r\n\r\n// Emulate a click on the save button ?\r\nformVM.SaveCommand.Execute() Usually, such a SaveCommand implementation in the VM calls some service in the business layer, in turn deciding what to do : trigger another service, do some  modification in Model objects and save them, or even check some  condition and throw a business exception, etc… The global architecture : Let’s have a look at the architecture in place to support this : The  application is classically divided in layers : Data access, Service,  ViewModel, View (not used when running GreenPepper pages). Dependencies are injected at run-time using an IoC container (Spring.NET) Model entities are manipulated by all layers, but not directly by the View. The ViewModel acts as a proxy instead. The GreenPepper block brings the Spring context, some mocks to replace peripherals, third-party components, etc. GreenPepper  usually triggers the business scenarios at the ViewModel level, but can  also manipulate the underlying layers when necessary, for instance to  check that the correct data has been pushed to the DB. Conclusion : Today, our BDD test harness represent 170 GreenPepper pages, for a total of around 7’800 assertions. The  software has 20 KLOC (not counting XAML code), the GreenPepper fixtures  represent 5 KLOC. This is undeniably an important part of the overall  development effort. BUT … the value is huge ! 98% of the total written pages of specification is executable.  It is executed every day. Any regression is immediately detected. A  deprecated specification or documentation is eliminated quickly. There  are no misunderstanding on the specification : a test is either green or  red. This BDD test harness comes in addition to the usual unit tests, and it completes it well. Actually, we notice that writing unit tests on the ViewModel layer is often painful, needs a lot of mocks (doing the same job than underlying services) and boilerplate code. Instead, GreenPepper tests plugged at the ViewModel layer allows a very good test coverage. And it’s very reassuring when time for large refactorings comes. As  said beforehand, the business context also induces a very high risk of  regression… and, indeed, we have regressions. Every week. But it’s  quickly detected, corrected, and the software quality is definitely kept  under control. These 20 KLOC of software are shipped today to users for a Beta phase. And we are confident. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged .NET , acceptance testing , Agility , ATDD , automated tests , BDD , C# , design pattern , Greenpepper , MVVM , Spring , Testability , Tests , WPF . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 6 commentaires sur “Behavior Driven Development using MVVM pattern and GreenPepper tests” Sanlaville 12/01/2011 à 16:58 Thanks for the article.\r\n\r\nIs that possible to know how long takes the build (including the 170 GreenPepper pages and the unit tests)? Mathieu DESPRIEE 12/01/2011 à 17:30 The full build with all the greenpepper pages takes ~30 mn. It's executed nightly.\nThe build with only unit-tests, 5 mn, executed at each commit. Florian 17/01/2011 à 16:05 \"Actually, we notice that writing unit tests on the ViewModel layer is often painful, needs a lot of mocks (doing the same job than underlying services) and boilerplate code. \"\r\n\r\nWhat's painful about writing unit tests for ViewModel? Can you give us some examples of that? Mathieu DESPRIEE 20/01/2011 à 14:26 @Florian : Yes, of course.\r\nActually, ViewModel objects have a lot of dependencies upon objects from the Models,  Services, and sometimes also the Repositories, and other ViewModels. In our project, the ViewModels are definitely the classes having the more dependencies to others.\r\nFor each of these dependencies, we have to write some mocks in unit tests setups, or to prepare objects the way we want before each test.\r\n\r\nExample : User-Story = \"the user wants to open a new session\". The click is handled by a Command in some ViewModel, which calls some Service. The Service creates and opens the session, and returns back the Session object. To unit-test my ViewModel, I have to mock the Service. But the mock will have to prepare and return a Session object the same way the real Service would do it. And the subsequent assertions in the unit-test will just check that the properties from the Session object are well reflected in the ViewModel => lots of code, but not a strong test.\r\n\r\nAnd, for the whole project, we notice that it's the unit-tests on ViewModels that require the most boilerplate code, and bring the less added-value.\r\n\r\nOn the opposite, GreenPepper tests are more end-to-end, ViewModel-to-ViewModel : from a Command to some result on some screen. The added value of the test per line of code in fixture is better.\r\n\r\nSo I prefer to be less demanding on UT with the development team for ViewModels : I do not require the same level of coverage of unit-test, if I know that scenarios are well covered by GreenPepper tests. Florian 25/01/2011 à 11:26 Hi Mathieu,\r\n\r\nThanks for the detailed answer, I see what you meant now.\r\n\r\nActually, I was asking the question because I'm a Flex developer and we use the Presentation Model pattern that is a bit similar of the ViewModel. \r\nHowever, our Presentation Model implementation doesn't have any dependency. It does only send messages intercepted by controllers. That's why I was wondering why it could be difficult to test such a simple thing.\r\n\r\nYour feedback about Greenpepper and unit tests is interesting and looks pragmatic to me. I think I'll look closer to this kind of testing with Flex. Luc Jeanniard 28/01/2011 à 16:06 Thank you for this clear introduction to the MVVM pattern. I'll try soon! \r\n\r\nRegarding unit test coverage, using mock is not that painfull when using tools like Moq (http://code.google.com/p/moq/). Creating a mock and/or stub takes two lines of code. Generally speaking, it is not easy to understand where stops BDD and when starts TDD but having much more unit tests than functional tests remains very important for test suite  duration. Rémy asked for how long it takes to run the tests ... good question : Behavior tests are longer than unit tests so it is important to keep much more unit tests than functional tests.\r\n\r\nWell, now, I'll try to make this a reality with MVVM ;o) Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2011-01-06"},
{"website": "Octo", "title": "\n                My reading of Percolator architecture: a Google search engine component            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/my-reading-of-percolator-architecture-a-google-search-engine-component/", "abstract": "My reading of Percolator architecture: a Google search engine component Publication date 15/11/2010 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 In April 2010, Google updated its indexing system. Caffeine – the name of this project – was pretty transparent for the large public but represents an in depth change for Google. It does not directly improve the search page, like instant search , but the indexing mechanism, the way to provide pertinent search results. For the end user, this change allows reducing the delay between when a page is founded and when it is made available in the Google search. Google has recently published a research paper about Percolator , one of the backend systems that subtend Caffeine. Research papers that described the previous system were written on Map/Reduce and Google File System . These two papers became the foundation for Hadoop on which I have written some articles . Therefore I was excited to discover this new architecture. After reading it, I decided to write out this article to give you, not just a summary in itself, but my understanding of this new architecture. The way to build it Google designs and builds tools for its particular needs. Percolator is used in building the index – which links keywords and URLs – used to answer searches on the Google page. Percolator goals are therefore particularly clearly defined: being able to reduce the delay between page crawling (the first time the page is browsed by Google) and its availability in the repository for query . Technically speaking, the previous system, based on Map/Reduce , used a batch approach. When some new pages are added, the whole repository (or a large part of it) is re-processed in order to change in a coherent way all the pages in the repository impacted by the addition (e.g. a link from a new page points to an existing page increasing its page rank). By contrast, Percolator should run on the fly : newly crawled pages should be available in the index straight away. Technically it involves: To be able to process incrementally the indexing pipeline for each new page individually To maintain invariants (e.g. links pointing on a page) during concurrent processing Relational databases are valid tools for such needs but do not satisfy Google’s size constraint. The important point I want to highlight is that Google did not just tune a relational database; they removed some requirements  like the latency . Percolator is only designed for background processing, so tens of seconds of delay are not worthwhile if the sustained throughput is high. The architecture Percolator has been designed on top of BigTable . BigTable is a multi-dimensional, sparse, sorted map used in conjunction to the Map/Reduce pattern in the preceding indexing system. BigTable is a multi-dimensional table: each cell -each piece of data- is identified by a row key, a column key and a timestamp. BigTable is building itself on top of the GFS (the Google Distributed File System). Finally it provides atomic read/write on a row basis. Please refer to this research paper for further information. Percolator uses an API similar to BigTable. It provides cross-table and cross-row ACID properties , allowing developers to safely reason about the state of the systems by preserving invariants. Moreover, Percolator provides a way to add observers – pieces of code that are triggered by modifications. Hence Percolator is a layered system as described in the first schema. Each Percolator server hosts processes for the Google File System layer, the BigTable layer and finally the Percolator server. Each layer communicates with the underlying one through an RPC call. I will now give you explanations of these views of the two main functionalities of Hadoop. ACID transactions First Percolator uses the atomicity of a row write of BigTable. As BigTable provides a reliable (replicated through GFS) storage with atomic write, it has been chosen as the lock support Then Percolator uses the timestamp dimension of BigTable in order to provide snapshot isolation . Snapshot isolation is a transactional isolation level in which previous versions of data are kept in order to give a consistent snapshot of the database to each transaction. It is used in the most famous database (Oracle, SQL Server, Percolator uses a two phase commit protocol , coordinated by the client , as described in the schema below. Percolator uses metadata columns in BigTable: for transactions c:data , c:lock and c:write . The high level synopsis for writing it is the following: Get a timestamp from the Oracle server (no relation to the RDBMS server, it is a sequence server, like a sequence in a Database). Assume the result is t7 Write a lock in the c:lock column of the first row at the given timestamp t7 and data in the c:data column. This is an atomic write. Write a reference to the first row in the c:lock column of the second row at the given timestamp t7 and data in the c:data column. This is an atomic write. We have now reached the end of the request phase. If both writings succeed (no concurrent locks found, no writes after t7), it is equivalent to the end of the Commit Request Phase . Get a timestamp from the Oracle server. Assume it is t8 Write a reference to t7 in the c:write column of the first row at the t8 timestamp Write a reference to t7 in the c:write column of the second row at the t8 timestamp Correspondingly, a get() request a timestamp, let’s say t10 (not on the schema). If it encounters a lock before t10, get() method waits. In the other case it returns the data at t10. Please refer to the research paper to get all details about conflicts and failover details. I prefer to focus on one point: locks are released in a lazy way . In the case of a client failure some locks can remain. To secure transactions, a lock is designed as primary. This is the transitional lock. If a client fails when this lock exists, the transaction should be rolled back: lock and data removed. If it fails after, the transaction must be rolled forward: the second client row should be committed. This process is handled in a lazy way by another client . When a client encounters a blocking lock It first checks if the transaction is alive (Chubby is used for this performance improvement. Chubby is a lock service written by Google.) Each transaction writes regularly a timestamp in Chubby in order to show it is alive.; Then if the transaction is dead, the previous transaction is rolled back. Whereas if the transaction is still alive the second transaction waits. Such an approach would not have been acceptable for OLTP scenario, but is the best choice for Google use case. Indeed, the probability of concurrency (as the page URL is the key of the row) is smaller than in an OLTP scenario. The latency is not a problem like in an OLTP scenario. Tests have been made by the teams and increasing the number of concurrent threads allows them to keep a good throughput. I have learned from this transactional architecture that a good understanding of the needs, of the constraints that can be relaxed and a clear understanding of the consequences of choices are the best means to identify your architecture. XA transactions (distributed transactions) have been known for a long time. I have already been confronted to concurrency issues with XA transactions without finding a satisfactory issue. What I have learned today is that distributed transactions are complex but can be useful if adapted to the business need. Observers Percolator has been designed as an incremental system. Observers are designed for that goal: Percolator is designed as a series of observers . Notifications can be seen as similar to database triggers but they have a simpler semantic than triggers’ one: they run in a different transaction so there are not intended to preserve invariants. Developers have to care about infinite observer’s cycles too. For an observed column, an entry is written in the c:notify column each time data is written in the c:data column. Observers are implemented in a separate worker process.  This worker process performs a distributed scan to find the row that has an entry in the c:notify column. This column is not a transactional one and many writes to the column may trigger the observer only once. I will not go into details of the observer’s implementation: locality of the observer column, random scanning and locking mechanism to efficiently look for the few modifications in a large repository. Please refer to the whole paper for further information. Such architecture is rather different from actual architectures in business IT: Percolator is based on a data sharing pattern whereas distributed architectures (mainly Service Oriented Architectures) are rather stateless; Percolator observers look like database triggers but they are very different: they run in a separate process that scan the modification and do not modify data invariants. The observers do not enforce data invariants like triggers but only provide a tool for an event driven architecture. Cost of the scalability Percolator system has been built on top of an existing layer and in a distributed way . The provided scalability comes at a cost. First it involves much more RPC requests than a Map/Reduce processing. Last, releasing locks in a lazy way combined with blocking API causes contention. These two issues have been resolved: By batching RPC like Read-Write-Modify instead of doing 3 RPC call for each business write; By pre-fetching data in the same row; By delaying lock’s releasing; By increasing the number of threads in order to preserve enough concurrency; Consequently Percolator has been carefully evaluated by comparing it to Map/Reduce and by running synthetic workload based on TPC-E , the last database benchmark for OLTP workload. To summarize, Percolator reached its goal as it allows reducing the latency (time between page crawling and availability in the index) by a factor of 100. It allows simplifying the algorithm. The previous Map/Reduce algorithm has been tuned to reduce the latency and required 100 different Map/Reduce whereas Caffeine requires only about 10 observers. The big advantage of Percolator is that the indexing time is now proportional to the size of the page to index and no more to the whole existing index size. However, when the update ratio (% of the repository size updated by hour) is increased Percolator advantages disappear. At about a 40% update ratio, Percolator is saturated (notifications starts accumulating and latency explodes) and Map/Reduce becomes more efficient. Map/Reduce reaches a limit too, but this limit is reached with a far more important (and not measured) update rate. So an incremental system is more efficient for continuous small updates, and Map/Reduce batch for big changes. In my past experience I have seen difficulties when some batches, for loading start of day data, were replaced by EAI message driven middleware without changing the timeline. Such results tend to confirm my point of view: batch and on the fly processing require different kind of architecture. Some measures were then performed. The conclusion is that read overhead of Percolator compared to a direct read on BigTable is very light: about 5% but write overhead is much larger: about 400%. Last benchmark is based on TPC-E to be able to compare percolator to databases. Some adaptations have been made: one update has been deactivated due to conflict – another implementation was described but has not been measured – and the latencies are larger than the maximum bearable for an OLTP workload. Despite all those limitations the important results were: Throughput by core was linear up to 500 Transactions Per Second / 650 cores  and linear with a smaller slope up to about 11,000 TPS / 11,000 cores. It was more than 3x more than the TPC-E record with 30x more CPU My conclusion is that despite these relatively bad results on a performance perspective, Percolator’s architecture is very interesting because it provides horizontal scalability and resilience . The gain in scalability and resilience comes at a cost . In particular layering causes the most important overhead. Google concluded that TPC-E results suggest a promising direction for future investigation. . I’m a fervent believer of NoSQL. Scalability has strongly modified application architecture, in particular with the web. Distribution of the data has allowed Google and other big internet actors to reach scalability unbelievable till now. I think NoSQL can bring such gains for business IT (throughput, scalability). However, as a result of the distribution, there is a performance penalty. My assumption is to compare it with the difference between mainframe batches on ISAM/VSAM files: for how long does a relational database perform better on batch than a VSAM file processing (if they process better)? Such NoSQL architectures are inspiring influence for use cases or future evolution in application design (e.g. new way to manage schema, new way to search, etc.) where NoSQL will globally outperform database despite the performance penalty. Relative works and conclusion Percolator research papers ends by re-enforcing the fact that incremental processing avoids latency and wasted work – like re-reading the whole repository again and again instead of seeking – of batch processing systems like Map/Reduce for small batches of updates. By constrast Map/Reduce batches are more efficient and scalable for big updates. Some improvements proposed to Map/Reduce do not completely solve the problem thus leading to a full redesign. Compared to other systems, the main particularities of Percolator’s design are: It is not a fully-fledged database: it does neither provide a query language nor join operations. It emphasizes throughput over latency. But it does provide a better scalability than RDBMS or parallel RDBMS; Organization of data, distributed transactions originates from parallel databases. Some references date as far back as 30 years ago; Compared to distributed storage like HBase or Amazon Dynamo , Percolator should be seen as much more a transformation system. It does not provide some of their functionalities like cross-datacenter replication. Some research works (e.g. but not exhaustively CloudTPS) are trying to build ACID transactions on top of distributed storage Faster CPUs than disks have made multi-CPU shared memory system (aka monolithic systems) simpler competitive to the distribution approach for databases. Today, huge datasets like Google index processed by Percolator changes this situation and a distributed architecture is required. Percolator has been in production since April and achieves the goal. TPC-E benchmark is promising but the linear scalability comes at a 30-fold overhead compared to a traditional database. The next challenge, at the end of this research paper, would be to know if it can be tuned or if it is inherent to distributed systems. Such in-depth panorama of Percolator architecture was for me a good way to clearly understand the way the architecture was built, from constraint analysis to optimizations. Even if it is not directly applicable to day to day work, Google is one of the key innovative leaders for large scale architecture. Analyzing such architecture enabled me to stand back from traditional architectures. Percolator may drive some open-source projects like Hadoop built according to GFS , BigTable and Map/Reduce research papers. If it’s not so, Percolator architecture will surely remain a reference, because it is a pioneer in its domain. Main sources Google Percolator Research paper: “Large-scale Incremental Processing Using Distributed Transactions and Notifications”, Daniel Peng, Frank Dabek, Proceedings of the 9th USENIX Symposium on Operating Systems Design and Implementation, 2010. Google research papers on GFS , BigTable , MapReduce Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Google Percolator , Map/Reduce , NoSQL , transaction . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 5 commentaires sur “My reading of Percolator architecture: a Google search engine component” Yousuf Ahmad 17/11/2010 à 15:39 Thanks. Quite a nice overview. However, it could have been even better after some proofreading and editing. \r\n\r\nI still feel I need to read the paper to get a more complete and accurate understanding of the architecture.\r\n\r\nCheers. Marc Bojoly 18/11/2010 à 14:51 I Yousuf,\r\n\r\nIt was my deliberate choice not to get into details on this topic but to give my global overview of it.\r\nShould you wish to dig further, as stated in my article, feel free to link to the main source.\r\n\r\nRegards. Ron Wolf 14/11/2011 à 20:28 thx, nice summary. i'm off to read the referenced papers for more depth. i will make a snarky observation regarding noSQL and the efforts of Google, FB, Twitter, etc to re-invent the wheel. here we have the brilliant staff at Google, step by little step, re-inventing all that went into making SQL DBs the powerhouses that they are. now they've added almost-stored procedures. now they have added almost-transactions (or maybe they are full transactions, i can't quite tell), now they are pre-fetching rows even in a columnar DB. while the rest of this innovative world lags several years behind Google, they too will add these and other typical SQL DB features. why? because they are needed to build robust systems, that's why Oracle & mySQL etc added them in the first place. my prediction, Google will end up with a SQL look alike, tho possibly with a very different query model and be using it years before the rest of the noSQL world catches up. elad v 23/06/2012 à 22:11 hello\r\nI really enjoyed reading your article, I learned a lot from him\r\nI wanted to know if there are similar ideas from other companies, or other methods to make those goals\r\nThank you Marc Bojoly 25/06/2012 à 21:08 Hello,\r\n\r\nThanks for your comment.\r\nAs far as I know this method is specific for Google. Hadoop framework - so a Map/Reduce technology - is (was?) used by Yahoo for the same use case. I don't have read any publication from other companies. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-15"},
{"website": "Octo", "title": "\n                Droidcon London 2010            ", "author": ["Jérôme Van Der Linden"], "link": "https://blog.octo.com/en/droidcon-london-2010/", "abstract": "Droidcon London 2010 Publication date 05/11/2010 by Jérôme Van Der Linden Tweet Share 0 +1 LinkedIn 0 We were in London last week for the Droidcon 2010 , a conference around Android and mobile development. It was a two-day event with a barcamp the first day and a more traditional conference the second day. Main topics were User Experience, Android development in general and a little about marketing. There were many topics during these two days but we decided to cover the ones that appeared more relevant to us. Pay attention on User Interface and User Experience What stroke us most was how Google insisted on making good user interfaces (UI) and good user experience (UX) in applications. Two presentations made by two Google Android Developer Advocate (Roman Nurik and Reto Meier) dealt on this. User Interface Roman Nurik first gave us many advices in UI to create great applications: First impression is essential and not only concerns the application itself! On the market, the application must have a nice icon, a great description, a simple name and representative screenshots. Don’t think only about the app usage, promote it on a website or a blog with a consistent user interface (same colors, icons and style as the application). Regarding the application, the first launch could be the opportunity to greet the user, eventually educate him with simple visual help or pictorial tutorial like Winamp or Goggles applications do: Design for ease use . Avoid complicated registration process and ensure the user can use the application right away. Having a simple, graceful home screen showing the main features of the application is a must, the user knows at first sight what he can do : Hierarchy the information in the interface and use primary UI navigation patterns (more important information at the top, main navigation at top). He gave us some UI tips: Prefer 3 or 4 tabs at the top of the screen rather than 5+ at the bottom. Avoid navigation in the menu, navigation should be available on the screen. Avoid back button visible on the screen (contrary to iPhone): there’s a hardware button for that. Do not alter back stack and avoid a long back stack. It is awful to tap 50 times on back to return to the previous screen. He concluded his presentation by talking about prototyping . Do not open your eclipse and code. First think about information you want to show and organize it . Use a pencil and some paper or use more sophisticated tools like Pencil (as Firefox plugin or standalone) and Adobe Fireworks ( some stuff to help ). As for the new tools ( http://code.google.com/p/android-ui-utils/ ). It provides some stencils for the Pencil tool. Very useful to create a realistic prototype very fast: To ensure to create consistent icons with Android, he presented the Android Asset Studio and the Photoshop icon template and icon guidelines , useful to have great icons in your application and keep a UI consistent between applications. We often criticize Android applications look and feel but everything is provided to create good UI. User Experience Then we saw Reto Meier, another Android Developer Advocate of Google who talked about user experience. He gave many advices to give a great experience to users: Always inform the user of what’s happening : When loading information, use progress bars and progress dialog. Render view and fill data as it arrives (lists…). Avoid blocking and non-responsive UI : Use Thread and particularly AsyncTask! Your UI should respond in 100-200ms. But the most important: don’t fight against user (intuitive navigation flow) and don’t fight against system (Android native experience, back stack…) You can find all these advices in his presentation available on YouTube (from Google I/O 2010). We strongly recommend every Android developer should watch and apply it… very instructive! User experience should be the first top priority of developers . Think beyond the phone Android devices are not only phones : tablets, TV, embedded hardware (cars…) with different resolutions, screen sizes (from 3” to more than 40”) and capabilities (no photo on some tablets, no accelerometer in TV!). Al Sutton said that developers must think about that when developing applications. For example, tablets and TV are designed for landscape mode. Imagine turning your TV in your living room! Think large (use -large for your layouts), landscape (do not block in portrait mode) and adapt your UI to the device . Do not reproduce Twitter or Facebook apps which have bad UX (portrait only, a lot of unused space, small icons) on tablets as you can see below: Another mistake is to confuse density and resolution . Developers must not assume that low density is for QVGA resolution, medium density is for HVGA resolution and high density is for WVGA resolution. These facts are generally true for phone device but not for tablets or TV. For example, Dell Streak (5″) is a WVGA MDPI tablet device and Samsung Galaxy Tab (7″) is a WSVGA HDPI device. Many details on how to manage different screens are available here . Concerning other capabilities, use the PackageManager (getSystemAvailableFeature) to check the availability of a third party hardware (GPS, Accelerometer, compass, photo…) to avoid problems on runtime and avoid displaying non-available features on screen. You are not alone! When you develop an application, don’t think about your app only, think about the platform, others applications, others devices around you. That was clearly the message of Sean Owen, Mark Murphy and Friedger Müffke. Use Intents, provide Intents,… Intents are wonderful Sean Owen is a Google Developer who co-developed Barcode Scanner (in the top 5 of the Market) and its ZXing API on his 20% free time at Google. Sean explained that open sources of the core library of his application was a priority from the beginning for reusing by other developers . So he transformed his application in a dependency, an API : ZXing API based on the Intent mechanism of Android. For those who don’t know what is Intent, it ensures that you are able to communicate inside or outside your application. Inside your application, you move from one screen to another using it. You can also use an external application able to do what you want to do. Typically, when clicking an url, the browser is launched with an Intent. When you want to share information, all applications available in your device capable of sharing info (Twitter, Facebook, email…) are proposed by the system with this mechanism. You can have details here . Concerning Barcode Scanner, any application can use it by launching the good Intent in its application. So anyone can develop an new application which scans QR code whitout developing the scanner itself. The problem with this is when the user of the application does not have Barcode Scanner installed on his device. He must have an ActivityNotFoundException… And that was the main topic of Sean Owen: you can drive downloads (of your application) with that ! What he means is creating a wonderful application that can be launched via Intents, create an API to launch this Intent and catch the exception, launch Android Market to checkout the application missing. And that’s it, a new user got your app downloaded! He concluded his session with a good sentence: “Think platform, not just app”, think about every application you can use, think about all the applications that could use yours … Android is open and provides mechanisms to see larger than just a small app. Reuse models Mark Murphy then talked about reusing code or features. Why should we reuse? Mainly to improve application capabilities and quality and go faster on the market. According to him, three models of reuse are possible: Use APK and Intents, as we seen before with Barcode Scanner. It’s a simple way to integrate new features in your application. As we saw, there can be errors if the dependency is not installed… Use JAR as we have done so with Java since the mists of time. It is a very simple and well-known mechanism to provide some features. The trouble with that is you cannot provide Android resources (pictures and so on), not available in the static R class… Use Android Library Project , a feature recently added in the SDK. It ensures to share code and resources. The main trouble with this mechanism is that you cannot generate an APK or a JAR because source code and resources need to be “added” to your project and compiled at the same time. So if you want to share a library, you’ll need to share the source too. He concluded by talking on how to share all these libraries and mention OpenIntents (see next part)… His presentation is available on slideshare . OpenIntents Friedger Müffke is one of the lead developers of OpenIntents which provide a lot of Android applications . He also took part in organization of Droidcon 2010 in Berlin and Brussels. As Sean Owen and Mark Murphy, Friedger spoke about the powerfulness of Intents and the necessity to share your Intents. It is important not to think monolith but interconnected components to reuse API and applications made by other developers. With Intents all applications are equal , if some applications can manage an Intent the choice is proposed to the user to select his favorite application. For example, if your application allows the user to share his content on Twitter, it is important that your application does not publish itself on Twitter but leaves the choice to the user to choose the application. On the first screenshot below, the user has the choice to share his video especially with Peep and Twitter which are both Twitter application. On the second one, user can choose his favorite application to select a music track: The problem is that you need some information to use Intents which is problematic when you want to reuse Intents of other applications. No information about Intents are available on the Android Market, the only possibility is that the developer documents it on its website. To avoid this problem, an Intent registry is necessary . All developers can promote theirs application Intents and see the possibility to reuse some applications in one place. When no application is able to manage an Intent call through your application, Sean Owen suggests to catch exception to redirect user on Android Market to an application that you know it can manage it. This method requires to specify a specific application and in that case the user has no choice. Friedger Müffke suggests to have a dependency manager to resolve this problem. When an Intent is not managed by any installed application, the dependency manager suggests some applications to be installed . However, this dependency manager should be part of Android OS to be easy to use by all users without no required preinstallation. AllJoyn Qualcomm presented his ambitious project AllJoyn , connect mobile device with a proximity-based peer-to-peer technology . It uses WiFi or Bluetooth to connect devices. A mobile device can discover other devices around it to exchange data, provide services to other devices, consume services of other devices, advertise your services to all devices around you, etc. Possibilities with this project are enormous: Social applications, interact with people around you Multiplayer games, play on games with people around you Information exchange Proximity services, buy or validate ticket near a terminal SDK is already available for who are interested. It consists in setting up some annotations to put on your services to share it with other. Android, a large and rising community During theses two days, we realized how dynamic was the Android community. Developers, IT consulting, telecommunications companies and manufacturers are very highly interested by this OS, thus participated actively in this event. People came from a lot of countries to share their knowledge and best practices. Having a great User Interface and User Experience are primary to take up the challenges of the diversification of Android not only used by phones. All developers realized Intents is one of the most powerful functionality of Android to make interconnected and not isolated applications. We want to give a special thanks to the organizers and participants for these two rewarding days. Tweet Share 0 +1 LinkedIn 0 This entry was posted in News and tagged Android , droidcon , mobility . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-05"},
{"website": "Octo", "title": "\n                EasyMock: Facts and fallacies            ", "author": ["Henri Tremblay"], "link": "https://blog.octo.com/en/easymock-facts-fallacies/", "abstract": "EasyMock: Facts and fallacies Publication date 01/11/2010 by Henri Tremblay Tweet Share 0 +1 LinkedIn 0 There’s been a lot of talking going on on the web lately. About which mocking framework is the best. EasyMock and Mockito are frequently compared . Especially since Mockito is greatly inspired (and is reusing the same mocking code under the hood) by EasyMock with some tweaks in the syntax. Sadly, while reading comparisons , I’ve noticed things that are just plainly untrue. This article is meant to rectify this and give you my opinion on both frameworks. Being EasyMock lead developer, I’m of course biased. Nevertheless, I believe I did, as best as I could, an honest comparison. EasyMock doesn’t mock classes by itself This is false. Since EasyMock 3, standard EasyMock can now mock both classes and interfaces. It used to be true for historical reasons as there were EasyMock and EasyMock Class Extension. The class extension now only exists for backward compatibility reasons (which is an empty shell delegating to EasyMock). EasyMock code is longer because you need to call replay() Come on… Like if calling one more method would kill your productivity… And I always thought that it was bringing a nice separation between the preparation phase and actual testing phase. I will agree on one thing though. It used to be annoying to have to replay every single mock ( replay(mock1, mock2, mock3) ). That’s why EasyMockSupport was made. Using it will now require you to call replayAll() and that’s it. Beside that, the original statement is a bit fallacious. Mockito requires you to add a bunch of verify clauses that would have already been included in the EasyMock expect calls. Here’s an example*: EasyMock (using EasyMockSupport) List mock = createNiceMock(List.class);\r\n\r\n// My business code needs these\r\nexpect(mock.get(0)).andStubReturn(\"one\");\r\nexpect(mock.get(1)).andStubReturn(\"two\");\r\nmock.clear();\r\n\r\nreplayAll();\r\n\r\n// Actual business code\r\nsomeCodeThatInteractsWithMock(mock);\r\n\r\n// Make sure everything was call as expected\r\nverifyAll(); Mockito List mock = mock(List.class);\r\n\r\n// My business code needs these\r\nwhen(mock.get(0)).thenReturn(\"one\");\r\nwhen(mock.get(1)).thenReturn(\"two\");\r\n\r\n// Actual business code\r\nsomeCodeThatInteractsWithMock(mock);\r\n\r\n// Make sure everything was call as expected\r\nverify(mock).get(0);\r\nverify(mock).get(1);\r\nverify(mock).clear(); You cannot “spy” with EasyMock Mockito has a nice feature called spying. You create a real object (not a mock) and spy it. It basically means that the object will behave as usual but that all the method calls will be recorded allowing to verify them after. Here’s an example: List list = new LinkedList(); // real object\r\n   List spy = spy(list); // spy wrapper\r\n\r\n   //optionally, you can stub out some methods:\r\n   when(spy.size()).thenReturn(100);\r\n\r\n   //this method call is spied\r\n   spy.add(\"one\");\r\n\r\n   // since a real add was done, get(0) will return \"one\"\r\n   assertEquals(\"one\", spy.get(0));\r\n\r\n   //size() method was stubbed and to return 100\r\n   assertEquals(100, spy.size());\r\n\r\n   //optionally, you can verify that add was called as expected\r\n   verify(spy).add(\"one\"); In EasyMock there’s no spying per se. However, I tried to think about the use-cases and it brought me to the conclusion that by using a subset of EasyMock capture, partial mocking and delegation features, you should be able to cover them all. It might be more cumbersome though. But I think the use-cases are quite rare. I’m not against adding a spy feature if proved wrong. About the example above, we need to think about the purpose of this test. Let’s say that in this case we have some old code for which we want to make sure the add method is called with the right parameter but we want to keep its usual behavior. This is quite rare but could happen. We would do: // Real object\r\n    List real = new LinkedList();\r\n    // create the mock\r\n    List list = createMock(List.class);\r\n\r\n    // spy the parameter and perform the normal behavior\r\n    Capture c = new Capture();\r\n    expect(list.add(capture(c))).andDelegateTo(real);\r\n\r\n    replayAll();\r\n\r\n    // the actual test\r\n    list.add(\"one\");\r\n\r\n    // get will return what was expected\r\n    assertEquals(\"one\", real.get(0));\r\n\r\n    // check the capture (will throw an error if nothing was captured)\r\n    assertEquals(\"one\", c.getValue());\r\n\r\n    // this is unnecessary since checking the capture is enough\r\n    verifyAll(); Better void handling in Mockito The problem with void method is that they do not return anything. So you cannot do expect(myVoidMethod()) since it won’t compile. Both EasyMock and Mockito won’t require you to “expect” anything since you most of the time don’t need to return anything anyway. You will just do mock.myVoid(); It’s wrong to think that you need to call expectLastCall() with EasyMock. However, it can be done just to make it clear in your code that you are recording. There’s also no need to call once() since EasyMock expects a single call by default. // this is enough by itself\r\n   mock.myVoid();\r\n  // no need to do any of this\r\n  expectLastCall().once(); The main difference is in how you “return” something from a void method. Like throwing an exception. Both frameworks have a special syntax for that. Mockito will use doThrow(new RuntimeException()).when(mockedList).clear(); which is a kind of backward from the usual syntax. EasyMock will instead do mockedList.clear();\r\nexpectLastCall().andThrow(new RuntimeException()); One line longer but keeps the forward syntax (the return is after the call). Obviously, it amounts to the same thing, as we say in French, “blanc bonnet et bonnet blanc”. And both frameworks are forced to do so for technical reasons. Mockito errors are clearer This used to be true. A lot was done in EasyMock 3 to improve error messages. I hope I’ve now filled the gap. EasyMock breaks tests more often Yes. That’s the way it should be. That’s my point of view at least. It’s really one of the main differences. With EasyMock, you will be required to expect everything. So your tests will break as soon as you change something. With Mockito, you will verify what you want and only these verifications can break. What’s the difference? EasyMock forces you to expect everything. Your tests will then break and you will be forced to go back to each test and ask yourself “Hum, was it suppose to break?”. Mockito won’t force you to expect anything. So the test will verify only what the original developer thought would be relevant. But, then, working on some other part of the code, you won’t know a verification is in fact missing. You won’t be forced to have a look at old tests. All tests will stay green and nasty bugs will sneak in. To me, that’s the main difference. A trade-off between a quicker test to write and a test testing the unexpected. Let’s look at an example demonstrating this. Mockito //mock creation\r\n List mockedList = mock(List.class);\r\n\r\n // call the mock in some business code\r\n mockedList.add(\"one\"); // the return value isn't used nor specified\r\n // mockedList.clear(); // adding this call in the business code WON'T break the test\r\n\r\n //verification that add() was called with the right parameter\r\n verify(mockedList).add(\"one\"); EasyMock //mock creation\r\n List mockedList = createMock(List.class);\r\n // We are forced to return a meaningful value\r\n expect(mockedList.add(\"one\").andReturn(true);\r\n replayAll();\r\n\r\n // call the mock in some business code\r\n mockedList.add(\"one\"); // the return value isn't used by was specified\r\n // mockedList.clear(); // adding this call in the business code will break the test\r\n\r\n //verification that add() was called with the right parameter\r\n // and that no other methods were called\r\n verifyAll(); EasyMock example is testing what was intended but it is also doing two things behind the scene. Making sure the mock is accurately mocking a List by forcing to return a value Preventing the test from working if other calls to the mocks are made by the business code. This will force you to wonder if breaking this test was expected by your refactoring. Still, I’ve give you one trick to prevent your tests to break all over the place. Think about what is really important. Is it really important that this getter is called only once by the tested method? No? Then record it as a stub. Do I care about this passed parameter? No? Just use the anyObject matcher. That seems silly but that’s how you get resilient tests. It’s a common mistake. Done usually while fighting to make the test work with the inner implementation of the method instead of thinking about what the tested method should really do (do I hear TDD?) What do I think of those Mockito guys? Some of you might think I just hate them. They stole my code and my users. Fortunately, while in other spheres people keep suing each other these days, that’s not the way it goes in the open source world. Everybody is using each other’s ideas and tries to improve them for the benefit of all. The ethic however dictates us to notice each other when we do so. Which they did. I was probably one of the first person aware of Mockito existence and they are not hiding that a lot of code and ideas are coming from EasyMock. These guys did some nice things that made me think. And they have a nice logo. Or course, it ain’t a unicorn world. For sure there is competition. But that’s what pushes us to evolve. Last Thoughts I must confess that EasyMock development has been slower than I would have liked these last two years. Still, EasyMock 3 has brought a lot of improvements. EasyMockSupport, new class mocking API, improved capture, delegation, etc. Also, the merge between EasyMock and EasyMock Class Extension should have been done a long time ago. However, lots of improvements are in the pipe. I’m even looking for help ! So feel free to contact me (more on that soon on the EasyMock mailing list . In the future, I would like a better integration with testing frameworks ( JUnit , TestNG ) and with Hamcrest (coming soon in EasyMock 3.1). Also, some power features to allow crossing the last mocking boundaries like final and private methods would be nice too. And still reduce the boilerplate code and so on. I’m also not closed to add a Mockito kinda API. I still don’t think it’s the best approach to get long term quality but heck… we used to think only interfaces should be mocked… *: Part of the examples are modified examples taken from the Mockito website Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 11 commentaires sur “EasyMock: Facts and fallacies” Hamlet D'Arcy 04/11/2010 à 15:40 Hi Henri, My name is Hamlet and I'm the author of said fallacious article about the comparison. First off, I have no experience with EasyMock 3.0. In fact, I had no idea it was even out. Sorry that my article unfairly compared Mockito to an older version of EasyMock. However, in general, I do stand by my main points. Mockito is an incremental improvement in some areas over EasyMock and in some areas they are equals. I'm unaware of any areas where EasyMock is superior, but would love to see your reply on that (I am not claiming there are none). Anyway, my three main points stand: 1) an over-reliance on the verify() method means your code has a lot of side effects. A better design is to have side effects limited to only a few places in your system. 2) a natural ordering of test code is arrange-act-assert, and getting away from this ordering obscures the essence of test methods. Making expectations serve as asserts at the top of a test method means your method becomes arrange-assert-act-assert which is more confusing. 3) The best way to organize/manage mock code and replay state is to define factory objects for types and try to hide the replay state and MockFramework as much as possible from the test methods. \r\n\r\nIt is #3 that I am most interested in a discussion of, the other seem smaller in comparison. Brice 07/11/2010 à 17:50 Hi Hamlet,\r\n\r\nDiscussion is always good, the mockito newsgroup is always open for discussion, so it is for many other places :)\r\n\r\nBy the way I was putting a bit of emphasis on BDD on the french article comment, and I believe that BDD comes pretty well along AAA.\r\n\r\nAnd I must say that I agree with your 3 points, however context is king. So you won't write test the same way for every possible test. Henri Tremblay 07/11/2010 à 20:31 @Hamlet: Thank you for your message. First, I must clarify that I haven't said your article was fallacious. That said, the purpose of my article was mainly to rectify wrong ideas, in your article and in general on the Net. Most of them are indeed coming from a comparison between Mockito and EasyMock 2 since too many people are sadly  unaware of EasyMock 3.\r\n\r\nAbout your two first points. They are true. EasyMock relies on verify a lot and the arrange part includes an assert part. If both are a good or bad thing differs according to who you ask about it.\r\n\r\nAbout the third one, I'm not sure I getting it right but I do agree that the mocking code should have as less boilerplate code as possible. Most of EasyMock developments will now be on this since I will agree Mockito is a bit ahead. Georges 12/11/2010 à 12:37 We are using easymock 2.3 together with Powermock to be able to mock private and static methods. This is sometimes needed when writing tests for legacy code.\r\nAny plans on incorporating features like those found in Powermock into Easymock ? Larry Fang 17/11/2010 à 05:37 verifyAll and replayAll are some really nice features I always desire for but again I just realize developers will be forced to extend EasyMockSupport in order to use them, which is a big turn-off. Matt 08/02/2011 à 00:18 I have used both EasyMock 3.0 and Mockito a great deal.  I do hope that both projects maintain their current direction.  Although I often use Mockito for hand-written mock objects because the default behavior is more permissive, I find that it has a tendency to make me lazy in the sense that I can get a test running and not actually verify a lot of behavior.  I find the default behavior of EasyMock is much more strict, so if I have a unit test running with EasyMock I have a good deal of confidence that I have the behavior locked down with the test.  I wrote a \"mock recorder\" which generates mock objects to reproduce method calls on an object tree at test time using EasyMock.  I could not figure out how to do the same thing with Mockito. Henri Tremblay 15/02/2011 à 19:09 @Georges: It's on the roadmap but I'm not sure it will be done in the end. Powermock did a good job at it and maybe these powerful features should stay separated from EasyMock... or not... Haven't decided yet.\r\n\r\n@Larry Fang: The easiest is to have a base class for your tests.\r\n\r\n@Matt: I totally agree with you Matt 31/03/2011 à 14:24 This post is to follow up... and also a shameless plug.  I recently added documentation and tutorials for the Java Test Object Recorder (please, someone help me come up with a better name!) at http://jtor.sourceforge.net/.  The project uses EasyMock 3.0 to record the behavior of a live object and automatically generate mocks.  I re-read the article above and you could think of this project as spying on an entire tree of objects and recording the results for playback in a unit test.  I use the technique to record database connections, web service calls, etc. and convert them into unit tests. Mahesh 19/12/2013 à 19:18 Here is a users perspective.\r\n\r\nI've used EasyMock and Mockito both and I find Mockito much more intuitive and easy to use.  I find stubbing in Mockito to be easier than with EasyMock and find the replay() idea confusing and fluff that I dont care.  I also dont care about the verify() in either frameworks and would rely on JUnit assertions instead.  When it comes to mocking big data objects EasyMock is way too complicated.\r\n\r\nThe last example you gave is simply wrong IMO and something you made up to make a point.  How I would write this with Mockito is as follows.\r\n\r\n//mock creation\r\nList mockedList = mock(List.class);\r\n \r\n// call the mock in some business code\r\nmockedList.add(\"one\"); // the return value isn't used nor specified\r\n// mockedList.clear(); // adding this call in the business code WON'T break the test\r\n \r\n//verification that add() was called with the right parameter\r\n//verify(mockedList).add(\"one\");\r\n\r\n//Using JUnits\r\nassertEquals(1, list.size());\r\nassertEquals(\"one\", list.get(0));\r\n\r\nMockito should be re-named EasyMock and EasyMock be renamed UnintutiveMock! Henri Tremblay 19/12/2013 à 19:34 @Mahesh: An EasyMock nice mock is basically a Mockito mock. So stubbing is quite the same. Why do you think the last example is wrong? An2d 27/11/2014 à 05:21 I switched from Mockito to Easymock (not by choice) and have to admit I didn't like it. I have been using EasyMock with Powermock. After first few hurdles (which I have managed to document here - http://an2dp.blogspot.com.au/), I am feeling comfortable with it. Although I have to write few extra lines, it serves my purpose. Mockito is definitely more intuitive though. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-01"},
{"website": "Octo", "title": "\n                Using Hadoop for Value At Risk calculation Part 5            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/using-hadoop-for-value-at-risk-calculation-part-5/", "abstract": "Using Hadoop for Value At Risk calculation Part 5 Publication date 08/11/2010 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 In the first part of this series , I have introduced why Hadoop framework could be useful to compute the VAR and analyze intermediate values. In the second part and third part and fourth part I have given two concrete implementations of VAR calculation with Hadoop with optimizations. Another interest of using Hadoop for Value At Risk calculation is the ability to analyse the intermediate values inside Hadoop through Hive. This is the goal of this (smaller) part of this series. Using Hive for Business Intelligence The major goal of that implementation with Hadoop was to keep the parameters and call prices in order to analyze them. For that purpose, I used Hive , a framework on top of Hadoop. I recommend you to read that article in order to be have a full overview. In brief, Hive is a framework that translates SQL expressions into Map/Reduce jobs and executes them in Hadoop cluster. There are two main advantages that motivated me to implement this solution: Hadoop can process terabytes of data. It is a way to overtake some traditional BI architectures for particular use cases; Defining Hive tables on top of files in HDFS (Hadoop Distributed File System) involves only creating metadata. As Hadoop has created the draw parameters and call prices in HDFS they don’t need to be moved for BI analysis thus leading to potential high time reduction. Hive is still a young framework and has some limitations. Output of the Map/Reduce must be text files which thanksfully can be compressed. Consequently, the configuration needs to be changed a bit: jobConf.setOutputFormat(org.apache.hadoop.mapred.TextOutputFormat.class);\r\njobConf.setBoolean(\"mapred.output.compress\", true);\r\njobConf.set(\"mapred.output.compression.type\", CompressionType.BLOCK.toString());\r\njobConf.setClass(\"mapred.output.compression.codec\", GzipCodec.class, CompressionCodec.class); After installing Hive (please refer to that article ), the table can then be defined in the Hive command line by giving it the path of the output files in HDFS: CREATE TABLE call_prices (key STRING, price DOUBLE, k DOUBLE, r DOUBLE, s0 DOUBLE, sigma DOUBLE, t INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\r\nCREATE EXTERNAL TABLE cp like call_prices LOCATION '/user/user/output'; Hive only creates some metadata. I can now query my values. I give you a very simple example enabling me to show you the differences between the call price evaluated through Monte-Carlo simulation and the linear approximation toward the underlying value (it is a line whose slope equals the financial delta and goes through the point corresponding to the actual underlying and call price). By combining different Unix tools I can produce the following graph. hive -e 'INSERT OVERWRITE LOCAL DIRECTORY \"HiveOutput\" select cp.s0, cp.price from cp order by cp.price;'\r\ncat HiveOutput/attempt_201010081410_0003_r_000000_0 | tr '\\001' '\\t' > HiveOutput/output.txt\r\ngnuplot -e 'set terminal png size 764,764; set output \"call.png\"; set xlabel \"Stock price (underlying)\"; set ylabel \"Call price (option)\"; plot \"HiveOutput/output.txt\" using 1:2, 0.56*(x-120)+12.35632307; exit;' Today this solution is not well equipped: Hive can’t change the output field separator, only text files are allowed. However, some initiatives are currently taking place. The association between Teradata and Cloudera on the subject (see that article in french or the initial link ) is a sign of the interest of that approach. Some integrations between Hive and BI tools (e.g. Pentaho ) are currently in development. So, that concludes the detailed implementation of the VAR calculation with Hadoop framework. We have computed the VAR and are capable of analyzing the intermediate results with Hive framework on top of Hadoop. In the sixth and last part, I will give you performance figures to see how Hadoop can compete with GridGain and will conclude on the whole subject. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Hadoop , HDFS , Hive , Java , NoSQL , VAR . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 2 commentaires sur “Using Hadoop for Value At Risk calculation Part 5” Bala 23/05/2012 à 17:38 Hi,\r\n\r\nWhen i tried the INSERT OVERWRITE LOCAL DIRECTORY command given above, i am getting the following error \"FAILED: Parse Error: line 1:46 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED' in select clause\". Can you help me to resolve this error?\r\n\r\nThanks,\r\nBala Marc Bojoly 31/05/2012 à 23:26 Hi,\r\n\r\nEffectively the syntax \"ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\" is not supported for INSERT OVERWRITE which always use Ctrl-A (01) delimiters.\r\nI used finally tr to do the transformation.\r\n\r\nThe correct commands are :\r\n\r\nhive -e 'INSERT OVERWRITE LOCAL DIRECTORY \"HiveOutput\" select cp.s0, cp.price from cp order by cp.price;'\r\n\r\ncat HiveOutput/attempt_201010081410_0003_r_000000_0 | tr '01' '\\t' > HiveOutput/output.txt\r\n\r\ngnuplot -e 'set terminal png size 764,764; set output \"call.png\"; set xlabel \"Stock price (underlying)\"; set ylabel \"Call price (option)\"; plot \"HiveOutput/output.txt\" using 1:2, 0.56*(x-120)+12.35632307; exit;' Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-08"},
{"website": "Octo", "title": "\n                Using Hadoop for Value At Risk calculation Part 4            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/using-hadoop-for-value-at-risk-calculation-part-4/", "abstract": "Using Hadoop for Value At Risk calculation Part 4 Publication date 05/11/2010 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 In the first part of this series , I have introduced why Hadoop framework could be useful to compute the VAR and analyze intermediate values. In the second part and in the third part I have given two concrete implementations of VAR calculation with Hadoop. I will now give you some details about the optimizations used in those implementations. Writable, Comparable, Comparator To have a full overview of the implementation, I will give you the content of DrawKey and CustomResultWritable class that you have seen in the signatures. Hadoop is a framework designed for file processing and sorting . Serialization and comparison are two key points to optimize. For that purpose Hadoop defines a custom serialization mechanism . Writable types like LongWritable provide both a constructor and a set() method. Existing objects can be fed by values with this set() .  This reuse of objects prevents instanciating a large number of new objects (in my previous article I noticed that object instantiation was a time consuming phase).  Serialization format is specialized and optimized for Hadoop requirements. For example it provides “out of the box” variable length values with LongWritable class. In order to preserve the complete structure of my result with combination of double and int fields, I need to define a custom class that implements the Writable interface in order to fit with the map() and reduce() signature. So this CustomResultWritable implementation shows how to use Hadoop serialization mechanism. public class CustomResultWritable implements Writable {\r\n\tprivate static String SEPARATOR = \"\\t\";\r\n\t/**  Time in days before the maturity */\r\n\tprivate int t;\r\n\t/** Spot (actual price) of the underlying */\r\n\tprivate double s0;\r\n\t/** Strike (target price) of the underlying */\r\n\tprivate double k;\r\n\t/** Risk free interest rate */\r\n\tprivate double r;\r\n\t/** Implicit volatility of the Equity\t */\r\n\tprivate double sigma;\r\n\t/** Computed prices with those parameters\t */\r\n\tprivate double price;\r\n        \r\n        //Getter and setters\r\n\t\r\n\t/** Default constructor to allow setting by reference\t */\r\n\tpublic CustomResultWritable() {}\r\n\r\n\tpublic CustomResultWritable(int t, double s0, double k, double r, double sigma,\r\n\t\t\tdouble price) {\r\n\t\tsuper();\r\n\t\tthis.t = t;\r\n\t\tthis.s0 = s0;\r\n\t\tthis.k = k;\r\n\t\tthis.r = r;\r\n\t\tthis.sigma = sigma;\r\n\t\tthis.price = price;\r\n\t}\r\n\r\n\tpublic void set(int t, double s0, double k, double r, double sigma,\r\n\t\t\tdouble price) {\r\n\t\tthis.t = t;\r\n\t\tthis.s0 = s0;\r\n\t\tthis.k = k;\r\n\t\tthis.r = r;\r\n\t\tthis.sigma = sigma;\r\n\t\tthis.price = price;\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void readFields(DataInput input) throws IOException {\r\n\t\tthis.t = input.readInt();\r\n\t\tthis.s0 = input.readDouble();\r\n\t\tthis.k = input.readDouble();\r\n\t\tthis.r = input.readDouble();\r\n\t\tthis.sigma = input.readDouble();\r\n\t\tthis.price = input.readDouble();\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void write(DataOutput output) throws IOException {\r\n\t\toutput.writeInt(this.t);\r\n\t\toutput.writeDouble(this.s0);\r\n\t\toutput.writeDouble(this.k);\r\n\t\toutput.writeDouble(this.r);\r\n\t\toutput.writeDouble(this.sigma);\r\n\t\toutput.writeDouble(this.price);\r\n\t}\r\n\r\n\t@Override\r\n\tpublic String toString() {\r\n\t\tStringBuilder builder = new StringBuilder();\r\n\t\tbuilder.append(t).append(SEPARATOR).append(s0).append(SEPARATOR)\r\n\t\t\t\t.append(k).append(SEPARATOR).append(r).append(SEPARATOR)\r\n\t\t\t\t.append(sigma).append(SEPARATOR).append(price);\r\n\t\treturn builder.toString();\r\n\t}\r\n\r\n} toString() method provides a means to write it in text form which is required for a text output format. You should notice that Hadoop provides predefined types for encapsulating Java primitives and String as well as a more generic mechanism with ObjectWritable that allows to combine them. However, since this mechanism is more generic and quite complex, I wrote directly a custom writable type. For the comparison purpose, Hadoop allows custom developments to optimize it too. The default implementation for my key class is the following. But just before I show you the code, I need to precise that I have a hierarchy of keys ScenarioKey extends DrawKey . I have explained in my second implementation where they are used and why I have put the percentile size in DrawKey . I  give you the ScenarioKey implementation, the DrawKey one is very close to it. public class ScenarioKey implements WritableComparable<scenariokey> {\r\n\r\n\t// For toString() implementation\r\n\tprotected static final String SEPARATOR = \";\";\r\n\r\n\t/**\r\n\t * Identifer the Scenario (ParametersValueGeneratorConfiguration)\r\n\t */\r\n\tprivate VLongWritable id;\r\n\t/**\r\n\t * Size of the percentile = totalNumberOfDraws * varPrecision\r\n\t */\r\n\tprivate VIntWritable percentileSize;\r\n\r\n\tpublic ScenarioKey() {\r\n\t\tset(new VLongWritable(), new VIntWritable());\r\n\t}\r\n\r\n\tpublic ScenarioKey(long id, int percentileSize) {\r\n\t\tsuper();\r\n\t\tset(new VLongWritable(id), new VIntWritable(percentileSize));\r\n\t}\r\n\r\n\tpublic ScenarioKey(VLongWritable id, VIntWritable percentileSize) {\r\n\t\tsuper();\r\n\t\tset(id, percentileSize);\r\n\t}\r\n\r\n\tpublic void set(VLongWritable id, VIntWritable percentileSize) {\r\n\t\tthis.id = id;\r\n\t\tthis.percentileSize = percentileSize;\r\n\t}\r\n\r\n\tpublic VLongWritable getId() {\r\n\t\treturn id;\r\n\t}\r\n\t\r\n\tpublic VIntWritable getPercentileSize() {\r\n\t\treturn percentileSize;\r\n\t}\r\n\r\n\t@Override\r\n\tpublic int hashCode() {\r\n\t\tfinal int prime = 31;\r\n\t\tint result = 1;\r\n\t\tresult = prime * result + ((id == null) ? 0 : id.hashCode());\r\n\t\tresult = prime * result\r\n\t\t\t\t+ ((percentileSize == null) ? 0 : percentileSize.hashCode());\r\n\t\treturn result;\r\n\t}\r\n\r\n\t@Override\r\n\tpublic boolean equals(Object obj) {\r\n\t\tif (this == obj)\r\n\t\t\treturn true;\r\n\t\tif (obj == null)\r\n\t\t\treturn false;\r\n\t\tif (getClass() != obj.getClass())\r\n\t\t\treturn false;\r\n\t\tScenarioKey other = (ScenarioKey) obj;\r\n\t\tif (id == null) {\r\n\t\t\tif (other.id != null)\r\n\t\t\t\treturn false;\r\n\t\t} else if (!id.equals(other.id))\r\n\t\t\treturn false;\r\n\t\tif (percentileSize == null) {\r\n\t\t\tif (other.percentileSize != null)\r\n\t\t\t\treturn false;\r\n\t\t} else if (!percentileSize.equals(other.percentileSize))\r\n\t\t\treturn false;\r\n\t\treturn true;\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void readFields(DataInput in) throws IOException {\r\n\t\tid.readFields(in);\r\n\t\tpercentileSize.readFields(in);\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void write(DataOutput out) throws IOException {\r\n\t\tid.write(out);\r\n\t\tpercentileSize.write(out);\r\n\t}\r\n\r\n\t@Override\r\n\tpublic int compareTo(ScenarioKey o) {\r\n\t\treturn ScenarioKey.compare(this, o);\r\n\t}\r\n\t\r\n\t/**\r\n\t * Compares two scenario keys\r\n\t * This method is required for being able to compare two subclass\r\n\t * of Scenario key because cast don't allow to refer to an overriden method \r\n\t * @param k1\r\n\t * @param k2\r\n\t * @return\r\n\t */\r\n\tpublic static int compare(ScenarioKey k1, ScenarioKey k2)  {\r\n\t\tif (k1 == null || k2 == null) {\r\n\t\t\tthrow new NullPointerException();\r\n\t\t}\r\n\t\tint cmp = k1.id.compareTo(k2.id);\r\n\t\tif (cmp != 0) {\r\n\t\t\treturn cmp;\r\n\t\t} else {\r\n\t\t\treturn  k1.percentileSize.compareTo(k2.percentileSize);\r\n\t\t}\r\n\t}\r\n\r\n\t@Override\r\n\tpublic String toString() {\r\n\t\treturn this.id + SEPARATOR + this.percentileSize;\r\n\t}\r\n\r\n} The writable implementation is close to the CustomResultWritable with one notable difference: fields are not Java primitives but VLongWritable . It allows minimizing the quantity of written data. WritableComparable just requires to implement compareTo() method. The implementation reads and compares only the required fields to do it. Because sorting and comparing date is one of the most important task of Hadoop (see the Map/Reduce pattern description in the first article ), an optimization possibility is provided by defining a RawComparator . This class compares directly bytes representation of the objects preventing to deserialize all the data before sorting them. In practice, I deserialize my scenario id first and return immediately if they are different. I implemented it as an internal class of ScenarioKey . static final class Comparator extends WritableComparator {\r\n\r\n\t\tprotected Comparator() {\r\n\t\t\tsuper(ScenarioKey.class, true);\r\n\t\t}\r\n\t\t\r\n\t\t@SuppressWarnings(\"unchecked\")\r\n\t\t@Override\r\n\t\tpublic final int compare(WritableComparable w1, WritableComparable w2) {\r\n\t\t\tScenarioKey d1 = (ScenarioKey)w1;\r\n\t\t\tScenarioKey d2 = (ScenarioKey)w2;\r\n\t\t\tint cmp = ScenarioKey.compare(d1, d2);\r\n\t\t\treturn cmp;\r\n\t\t}\r\n\r\n\t\t@Override\r\n\t\tpublic final int compare(byte[] b1, int s1, int l1, byte[] b2, int s2,\r\n\t\t\t\tint l2) {\r\n\t\t\t// si = index of first relevant byte\r\n\t\t\t// li = total length of the byte array to compare\r\n\t\t\tlong thisId;\r\n\t\t\tlong thatId;\r\n\t\t\ttry {\r\n\t\t\t\tthisId = readVLong(b1, s1);\r\n\t\t\t}\r\n\t\t\tcatch(IOException ioex) {\r\n\t\t\t\tthrow new IllegalArgumentException(\"First VLong is invalid\", ioex);\r\n\t\t\t}\r\n\t\t\ttry {\r\n\t\t\t\tthatId = readVLong(b2, s2);\r\n\t\t\t}\r\n\t\t\tcatch(IOException ioex) {\r\n\t\t\t\tthrow new IllegalArgumentException(\"Second VLong is invalid\", ioex);\r\n\t\t\t}\r\n\t\t\t\r\n\t\t\tint cmp = (thisId < thatId ? -1\r\n\t\t\t\t\t: (thisId == thatId ? 0 : 1));\r\n\t\t\tif (cmp == 0) {\r\n\t\t\t\tint idL1 = WritableUtils.decodeVIntSize(b1[s1]);\r\n\t\t\t\tint idL2 = WritableUtils.decodeVIntSize(b2[s2]);\r\n\t\t\t\t//PercentileSize\r\n\t\t\t\tint thisPs = 0;\r\n\t\t\t\tint thatPs = 0;\r\n\t\t\t\ttry {\r\n\t\t\t\t\tthisPs = readVInt(b1, s1+idL1);\r\n\t\t\t\t}\r\n\t\t\t\tcatch(IOException ioex) {\r\n\t\t\t\t\tthrow new IllegalArgumentException(\"First VInt is invalid\", ioex);\r\n\t\t\t\t}\r\n\t\t\t\ttry {\r\n\t\t\t\t\tthatPs = readVInt(b2, s2+idL2);\r\n\t\t\t\t}\r\n\t\t\t\tcatch(IOException ioex) {\r\n\t\t\t\t\tthrow new IllegalArgumentException(\"Second VInt is invalid\", ioex);\r\n\t\t\t\t}\r\n\t\t\t\tcmp =  (thisPs < thatPs ? -1\r\n\t\t\t\t\t\t: (thisPs == thatPs ? 0 : 1));\r\n\t\t\t}\r\n\t\t\treturn cmp;\r\n\t\t}//End compare\r\n\t}//End Comparator This comparator is registered through this static block: static {\r\n    WritableComparator.define(ScenarioKey.class, new ScenarioKey.Comparator());\r\n} Hadoop framework provides a helper class: WritableUtils to help decode byte representations but the result still remains a very technical code. That concludes the code of implementations I used with Hadoop. I will give you performance figures showing in the last part of that article. But, before that, I will show how Hadoop can be used for Business Intelligence too in the next part of that series. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Hadoop , HDFS , Java , Map/Reduce , VAR . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-05"},
{"website": "Octo", "title": "\n                Using Hadoop for Value At Risk calculation Part 3            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/using-hadoop-for-value-at-risk-calculation-part-3/", "abstract": "Using Hadoop for Value At Risk calculation Part 3 Publication date 04/11/2010 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 In the first part of this series , I have introduced why Hadoop framework could be useful to compute the VAR and analyze intermediate values. In the second part I have described a first implementation. One drawback of this previous implementation is that it does not take advantage of the reduce pattern. I did it by hand. I will now fully use Hadoop reduce feature. Second implementation: extraction of the VAR by the reducer If I simply code a reduce class with the signature public void reduce(DrawKey key, Iterator values, OutputCollector output, Reporter reporter) , this method will be called one time per key scenario Id;price value so one time per draw, because each draw generates a different price. The successive calls to the reduce() method are totally independant (no shared variable). So it is not possible to know if 1% of the the values has been processed, to extract the percentile and thus the VAR in such a way. However, Hadoop provides a way to sort keys one way and group them another way and this feature will help us to solve the problem. The example given uses keys containing two integers and shows how to group by the first integer and sort them by the second one. I am proposing to use it to extract the percentile. I have created a new program for which I won’t give you all the code but just a synopsis. This VarSimpleOptionReduce program  defines two jobs: a job with the previous mapper class and no reduce class (or an identity mapper). It allows computing and generating the call prices and store them; a job with no map task and the IntermediateResultSortReducer that I will describe hereafter. Hadoop will sort the data and this reducer will extract the VAR. The corresponding configuration is the following: /**\r\n * Configuration adapted for a binary compressed result\r\n * @param args\r\n * @return\r\n */\r\nprivate JobConf buildGenerateMapperConfBinaryResult(String[] args) {\r\n\tJobConf jobConf = new JobConf();\r\n\t//identical...\r\n\tjobConf.setMapperClass(IntermediateResultMapperBinary.class);\r\n\tjobConf.setNumReduceTasks(0);\r\n\t//...\r\n\treturn jobConf;\r\n}\r\n\r\n/**\r\n * Configuration adapted for a binary compressed result\r\n * @param args\r\n * @return\r\n */\r\nprivate JobConf buildGenerateReducerConfBinaryResult(String[] args) {\r\n\tJobConf jobConf = new JobConf();\r\n\t//...\r\n\tjobConf.setNumMapTasks(0);\r\n\tjobConf.setReducerClass(IntermediateResultSortReducer.class);\r\n\tjobConf.setPartitionerClass(FirstPartitionner.class);\r\n        //Inputs of the reduce tasks are sorted by DrawKey\r\n\tjobConf.setOutputKeyComparatorClass(DrawKey.Comparator.class);\r\n        //All keys equals according to the ScenarioKey.Comparator will be send to the same reduce class\r\n\tjobConf.setOutputValueGroupingComparator(ScenarioKey.Comparator.class);\r\n        jobConf.setNumReduceTasks(1);//Use 2 if you have two scenarios\r\n\t//...\r\n} Map output is spread among partitions. The partitionner class runs on the output of the map task before sorting. Then each partition is processed by a different reduce task – a different call to the reduce() function-. All keys for the same scenario should be sorted together and not in two different reduce tasks. Thus the optimized configuration is to have one partition and one reduce task per scenario. jobConf.setPartitionerClass(FirstPartitionner.class); So the FirstPartitionner should only take into account the scenario key (scenario id and percentile size). @SuppressWarnings(\"deprecation\")\r\npublic class FirstPartitionner implements Partitioner<drawkey , CustomResultWritable> {\r\n\t@Override\r\n\tpublic int getPartition(DrawKey key, CustomResultWritable value, int numPartitions) {\r\n\t\treturn Math.abs((((ScenarioKey)key).hashCode()) * 127)%numPartitions; \r\n\t}\r\n\t@Override\r\n\tpublic void configure(JobConf job) {}\r\n} DrawKey.Comparator.class and ScenarioKey.Comparator.class compares instances of DrawKey and ScenarioKey . I will give you more details about that in the following paragraph. In this way, the reduce() method will take this kind of input (syntax with parenthesis is only illustrative): 1;10;0.513775910851316\t( \"252\t84.31301373924966\t120.0\t0.05\t0.2\t0.513775910851316\", \"1;10;0.513775910851316\t252\t103.39569385168355\t120.0\t0.05\t0.2\t4.181165705822988\", \"1;10;0.513775910851316\t252\t123.11293496630553\t120.0\t0.05\t0.2\t14.414516512987014\") So reduce() method will be called one time for the scenario key 1;10 with 3 different price values in the ascending order. One drawback of the reducer signature is that the reducer will be called with a DrawKey : 1;10;0.513775910851316 . The corresponding call price value 0.513775910851316 is the smallest one and should be ignored from a business perspective. The real price values computed after each draw 0.513775910851316, 4.181165705822988, 14.414516512987014 are provided in the values. That’s why the call price value is duplicated both in the key (for sorting) and in value (for collecting in reduce() function). I used a third and last refinement for my key, putting the percentile size in it. It allows each reduce() call to get that value in an easy and unique way. The final shape of my key is then scenario id;percentile size;call price . We can now see the reducer implementation: @SuppressWarnings(\"deprecation\")\r\npublic final class IntermediateResultSortReducer extends MapReduceBase implements Reducer<drawkey , CustomResultWritable, DrawKey, CustomResultWritable> {\r\n\t/**\r\n\t * Prerequisite: Key are grouped by ScenarioKey and sorted by DrawKey\r\n\t * So we have for each ScenarioKey, values sorted by Price \r\n\t */\r\n\t@Override\r\n\tpublic void reduce(DrawKey key, Iterator<CustomResultWritable> values,\r\n\t\t\tOutputCollector<drawkey , CustomResultWritable> output, Reporter reporter)\r\n\t\t\tthrows IOException {\r\n\t\t//Collect only the s\r\n\t\tCustomResultWritable previousResult = new CustomResultWritable();\r\n\t\tCustomResultWritable currentResult = new CustomResultWritable();\r\n\t\tint countDown = key.getPercentileSize().get();\r\n\t\twhile(countDown >= 0 && values.hasNext()) {\r\n\t\t\tif(countDown == 0) {\r\n\t\t\t\toutput.collect(key, values.next());\r\n\t\t\t\treporter.setStatus(\"Reduce ended\");\r\n\t\t\t\treturn;//Exit\r\n\t\t\t}\r\n\t\t\telse {\r\n\t\t\t\tcurrentResult = values.next();\r\n\t\t\t\tSystem.out.println(currentResult);\r\n\t\t\t\tif(currentResult.getPrice() < previousResult.getPrice()) {\r\n\t\t\t\t\treporter.setStatus(\"Data are not sorted\");\r\n\t\t\t\t\tSystem.err.println(String.format(\"Previous price: %s, current price: %d\", previousResult.getPrice(), currentResult.getPrice()));\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t}\r\n\t\t\tcountDown--;\r\n\t\t\tif(!values.hasNext()) { \r\n\t\t\t\tString err = String.format(\"Reducer: Key %s has only %d values with an expected percentile of %d\", key.toString(), key.getPercentileSize().get()-countDown, key.getPercentileSize().get());\r\n\t\t\t\tSystem.err.println(err);\r\n\t\t\t\treporter.setStatus(err);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n} The reduce() function reads the percentile size  – lets name it p – and collects the p th. values. The output file contains the VAR 0.191 and the corresponding parameters. 1;10000;4.1013116251405945E-10\t252\t78.12793687367298\t120.0\t0.05\t0.2\t0.19109501386036332 So, like in the first implementation, I use efficiently benefit of the sort feature of Hadoop. But furthermore: I use a partitionner allowing me to create one partition per scenario. It allows Hadoop to distribute each input file for reduce phase on  a different node – each partition being stored in a different file; I use a SecondarySortPartitionner allowing me to get all the values for one scenario in the same reduce task. That way, Hadoop can parallelize the reduce phase for each scenario by instantiating several reduce tasks. That concludes the second implementation for VAR calculation on Hadoop and that part of this series. I used other optimizations in my code that I will explain you in the next part. And in the last articles of this series, I will give you some performances figures, and I will conclude why it is in the interest of using Hadoop for VAR calculation. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Hadoop , HDFS , Java , Map/Reduce , VAR . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-04"},
{"website": "Octo", "title": "\n                Using Hadoop for Value At Risk calculation Part 2            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/using-hadoop-for-value-at-risk-calculation-part-2/", "abstract": "Using Hadoop for Value At Risk calculation Part 2 Publication date 03/11/2010 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 In the first part of this series , I have introduced why Hadoop framework could be useful to compute the VAR and analyze intermediate values. In that second part I will give you a first concrete implementation of the VAR calculation with Hadoop. Hadoop map implementation The implementation is an adaptation of the code described in my previous article . The classes used to compute the VAR are the same: OptionPricer , Parameters and ParametersValueGenerator . The following classes were added for Hadoop purpose: InputParser (parsing of the input file), ScenarioKey and DrawKey , IntermediateResultMapper , VarSimpleOptionDirectSearch.VarSimpleOptionDriver was added for configuration purpose and VarSimpleOptionDirectSearch as the launcher. IntermediateResultMapper is an implementation of the map requirements as described in the first article and is very close to the GridGain implementation: class IntermediateResultMapper extends MapReduceBase implements\r\n\t\tMapper<longwritable , Text, DrawKey, Text> {\r\n\t\r\n\tprivate static Logger log = LoggerFactory.getLogger(IntermediateResultMapper.class);\r\n\t\r\n\tprivate enum HadoopCounters {\r\n\t\tSKIPPED_LINE,\r\n\t\tCOMPUTING_ERROR\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void map(final LongWritable key, final Text value,\r\n\t\t\tOutputCollector<drawkey , Text> output,\r\n\t\t\tReporter reporter) throws IOException {\r\n\t\tString line = value.toString();\r\n\t\tInputStruct is = InputParser.parse(line);\r\n\t\tif (is != null) {\r\n\t\t\tParameters params = new Parameters(\r\n\t\t\t\t\tis.t, is.s0, is.k, is.r, is.sigma);\r\n\t\t\tParametersValueGenerator valueGenerator = new ParametersValueGenerator(is.historicalVolatility, 0, 0);\r\n\t\t\tOptionPricer optionPricer = new OptionPricer(params);\r\n\t\t\tvalueGenerator.initializeReference(params);\r\n\t\t\tvalueGenerator.intializeRandomGenerator();\r\n\t\t\ttry {\r\n\t\t\t\tint percentileSize = (int)(is.drawsNb*(1-is.varPrecision));\r\n\t\t\t\tDrawKey dk = new DrawKey(is.id, percentileSize, 0);\r\n\t\t\t\tfor(int i = 0 ; i < is.jobDrawsNb ; i++) {\r\n\t\t\t\t\tfinal Result result = computePriceScenario(optionPricer, valueGenerator);\r\n\t\t\t\t\tdk.setValue(result.getPrice());//Optimization: set by reference to avoid object creation\r\n\t\t\t\t\toutput.collect(dk, new Text(result.toString()));\r\n\t\t\t\t}\r\n\t\t\t} catch (Exception e) {\r\n\t\t\t\treporter.incrCounter(HadoopCounters.COMPUTING_ERROR, 1);\r\n\t\t\t\tlog.error(\"Error computing var\", e);\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\treporter.incrCounter(HadoopCounters.SKIPPED_LINE, 1);\r\n\t\t}\r\n\t}\r\n\t\r\n\t/**\r\n\t * Compute the price of the call based on a scenario\r\n\t * generated by a draw\r\n\t */\r\n\tprivate Result computePriceScenario(final OptionPricer optionPricer,\r\n\t\tfinal ParametersValueGenerator valueGenerator) throws MathException {\r\n\t\tvalueGenerator.setNewGeneratedParameters(optionPricer.getParameters());\r\n\t\tfinal double price = optionPricer.computeCall();\r\n\t\treturn new Result(optionPricer.getParameters(),price);\r\n\t}\r\n} One key point is the signature of that method: public void map(final LongWritable key, final Text value, OutputCollector output, Reporter reporter) . The input file is read as a (LongWritable, Text) pairs. With a simple file, the key ( LongWritable ) is the byte offset of the beginning the line and the value the content of the line. For example: 0     1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250\r\n45   1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250\r\n... The second line begins at character 45. This byte offset provides easily a unique key. It is ignored in my implementation. The map produces (DrawKey, Text) pairs as described below. 1;10;0.513775910851316\t252\t84.31301373924966\t120.0\t0.05\t0.2\t0.513775910851316\r\n... The key contains the scenario id, the total number of draws and the price value. The values contain the price and the parameters. I will explain a bit later why this choice was important for a further optimization. Thus, all my draws are computed and generated like in the GridGain implementation but directly stored in a file system. So it allows two things To compute the VAR; To process these intermediate results to extract useful information. I will describe these two points. Reduce implementation Output of the map task could be considered like the job results in GridGain implementation (see my second article ). I could have used exactly the same algorithm with an in memory sort. However it won’t have taken benefit from Hadoop map/reduce implementation. Hadoop map/reduce implementation is based on the sorting keys. Map tasks produce files. key1   valueA  \r\nkey2   valueB\r\nkey1   valueC\r\n... These files are sorted by key. Then all the lines with the same key are combined in a pair (key, list of values) form. key1  (valueA, valueC)\r\nkey2  (valueB) Then reduce tasks are fed with such values. The signature of the corresponding reduce function is reduce(DrawKey k, Iterator values, OutputCollector , Reporter reporter) . So  sorting by key is the heart of Hadoop map/reduce implementation because it allows transforming map phase output into reduce phase input. I will not immediately give you “the” reduce implementation because several choices can be made. Before that, I will discuss the determination of the key which is a fundamental point in the definition of the Hadoop implementation. Determination of the key To perform VAR calculation, I need to have the list of all computed prices, sort them and extract the percentile: the 10% lowest values. Finally I have to identify the highest value of that percentile (see my first article for more details). Processing of a map/reduce job by Hadoop can be applyied simultaneously to several sets of data. In order to provide one result for each set, Hadoop processing follows these steps: Applying the map() function to all the values; Sort data by key. Keys identify the sets of data so grouping prepare splitting of the set; Send all the values corresponding to a given key to a reduce() function For my implementation, I will take benefit from the sort provided by Hadoop process. Indeed, by using a simple Treeset , like in the GridGain implementation, I’m limited by the Java heap size. By contrast, Hadoop provides a finely tuned mixed in memory and on disk sort algorithm. To give a clue of Hadoop performance on sorting dataset, I refer to the Terabyte sort : a benchmark consisting in sorting 1 TB of data and which Hadoop has won. First reduce implementation: Identity reducer and VAR extraction by the main program So, in order to use Hadoop sorting capability, I use DrawKey containing scenario id;call price value . That way, the input of the reduce task contains for each successive scenario the sorted prices of the job. At the end of the Hadoop job, I read the sorted results file through the distributed file system API. I iterate through the 1% lowest value and identify the VAR. It is my first implementation. It is configured in Hadoop through the following code: @SuppressWarnings(\"deprecation\")\r\npublic class VarSimpleOptionDirectSearch {\r\n\r\n\tstatic class VarSimpleOptionDriver extends Configured implements Tool {\r\n\t\tprivate static Log log = LogFactory.getLog(VarSimpleOptionDriver.class);\r\n\r\n\t\t@Override\r\n\t\tpublic int run(String[] args) throws Exception {\r\n\t\t\tif (args.length != 2) {\r\n\t\t\t\tlog.error(String.format(\r\n\t\t\t\t\t\t\"Usage %s [generic options] <input /> <output>\\n\",\r\n\t\t\t\t\t\tgetClass().getSimpleName()));\r\n\t\t\t\tToolRunner.printGenericCommandUsage(System.err);\r\n\t\t\t\treturn -1;\r\n\t\t\t}\r\n\r\n\t\t\t// Generate the prices for each draw and sort them\r\n\t\t\tJobConf generateMapConf = buildGenerateMapperConf(args);\r\n\r\n\t\t\tdouble startTime = System.nanoTime();\r\n\t\t\tJobClient.runJob(generateMapConf);\r\n\r\n\t\t\tPath outputPath = new Path(args[1]);\r\n\t\t\tSequenceFile.Reader[] readers = org.apache.hadoop.mapred.SequenceFileOutputFormat\r\n\t\t\t\t\t.getReaders(generateMapConf, outputPath);\r\n\t\t\tlong id;\r\n\t\t\tint percentileSize;\r\n\t\t\tfor (SequenceFile.Reader reader : readers) {\r\n\t\t\t\ttry {\r\n\t\t\t\t\tDrawKey key = new DrawKey();\r\n\t\t\t\t\tWritable value = new CustomResultWritable();\r\n\t\t\t\t\treader.next(key, value);\r\n\t\t\t\t\tdo {\r\n\t\t\t\t\t\tid = key.getId().get();\r\n\t\t\t\t\t\tpercentileSize = key.getPercentileSize().get();\r\n\t\t\t\t\t\tint cpt = 0; // Count number of draws\r\n\t\t\t\t\t\tlog.info(key.toString() + \" cpt:\" + cpt);\r\n\t\t\t\t\t\tdo { // For each value\r\n\t\t\t\t\t\t\tif (cpt == percentileSize) {\r\n\t\t\t\t\t\t\t\tlog.info(\"VAR: cpt\" + cpt + \"\\t\"\r\n\t\t\t\t\t\t\t\t\t\t+ key.toString() + \"\\t\"\r\n\t\t\t\t\t\t\t\t\t\t+ value.toString());\r\n\t\t\t\t\t\t\t} else {\r\n\t\t\t\t\t\t\t\t//Continue silently\r\n\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t\tcpt++;\r\n\t\t\t\t\t\t} while (reader.next(key, value)\r\n\t\t\t\t\t\t\t\t&& id == key.getId().get()\r\n\t\t\t\t\t\t\t\t&& percentileSize == key\r\n\t\t\t\t\t\t\t\t\t\t.getPercentileSize().get());\r\n\t\t\t\t\t} while(reader.next(key, value)); // End for each (id, percentileSize)\r\n\t\t\t\t} finally {\r\n\t\t\t\t\treader.close();\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tdouble computeTime = System.nanoTime() - startTime;\r\n\t\t\tlog.info(\"ComputeTime \" + computeTime/1000000 + \" (ms.)\");\r\n\r\n\t\t\treturn 0;\r\n\t\t}\r\n\r\n\t\tprivate JobConf buildGenerateMapperConf(String[] args) {\r\n\t\t\tJobConf jobConf = new JobConf();\r\n\t\t\t//org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders()\r\n\t\t\t//tries to read the _logs directory as part of the SequenceFile\r\n\t\t\t//which leads to an error\r\n\t\t\t//Putting the history in another folder bypasses the problem\r\n\t\t\tjobConf.set(\"hadoop.job.history.user.location\", \"job_history\");\r\n\t\t\tjobConf.setJarByClass(VarSimpleOptionDirectSearch.class);\r\n\r\n\t\t\tFileInputFormat.addInputPath(jobConf, new Path(args[0]));\r\n\t\t\tFileOutputFormat.setOutputPath(jobConf, new Path(args[1]));\r\n\r\n\t\t\t// Compute intensive task: each mapper will receive a line\r\n\t\t\tjobConf.setInputFormat(NLineInputFormat.class);\r\n\t\t\tjobConf.setMapperClass(IntermediateResultMapperBinary.class);\r\n\t\t\tjobConf.setReducerClass(IdentityReducer.class);\r\n\t\t\tjobConf.setNumReduceTasks(1);\r\n\r\n\t\t\t// Set no limit to the number of task per JVM in order to take\r\n\t\t\t// advantage of HotSpot runtime optimizations after long runs\r\n\t\t\tjobConf.setNumTasksToExecutePerJvm(-1);\r\n\t\t\tjobConf.setOutputKeyClass(DrawKey.class);\r\n\t\t\tjobConf.setOutputValueClass(CustomResultWritable.class);\r\n\t\t\tjobConf.setOutputFormat(org.apache.hadoop.mapred.SequenceFileOutputFormat.class);\r\n\t\t\treturn jobConf;\r\n\t\t}\r\n\r\n\tpublic static void main(String[] args) throws Exception {\r\n\t\tint exitCode = ToolRunner.run(new VarSimpleOptionDriver(), args);\r\n\t\tSystem.exit(exitCode);\r\n\t}\r\n\r\n}// End VarSimpleOption class Please note that I use Hadoop 0.20.2 with the old style API. Hadoop is launched by the main method. A Configured based class is provided to help launching the job. The run() method configures the job, launches it and iterates the results in order to get the VAR value. I will give some explanations about the jobs configuration. jobConf.setMapperClass(IntermediateResultMapperBinary.class);\r\njobConf.setReducerClass(IdentityReducer.class);\r\njobConf.setOutputKeyClass(DrawKey.class);\r\njobConf.setOutputValueClass(CustomResultWritable.class); Map and reduce tasks are configured that way. I use an IdentityReducer that just copies the map output after sorting and so avoids the reduce phase. Output key and value classes have to be defined too. JVM should be informed that the received values are not the default LongWritable and Text ones. Due to generic type erasure, the type of the map output cannot be inferred at runtime without such configuration. jobConf.setInputFormat(NLineInputFormat.class); configures how to split the input file between tasks. I chose NLineInputFormat in order to launch one map task for each line. By configuring the tasktracker to launch as many tasks as available cores on the node, I optimize the compute intensive map tasks by executing one map task per core. As you might have noticed the input format line 1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250 contains both 1000 : the total number of draws 250 : the number of draws for that task That way, I can define input files so that one draw is split between several map tasks. In the following example one draw is divided into 4 tasks. 1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250\r\n1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250\r\n1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250\r\n1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250 Each map task can generate the corresponding number of draws as provided in the input line. And the total number of draws (here 1000 ) is used to define the size of the percentile at 1% (here 0.01*1000=10 ) jobConf.setNumReduceTasks(1); forces having all results sorted together in one single file and not in several files sorted separately. It is required in order to be able to identify the percentile. As for GridGain implementation, it implies that for one job the reduce phase is not distributed . jobConf.setOutputFormat(org.apache.hadoop.mapred.SequenceFileOutputFormat.class); configures a binary output format to reduce the size. That concludes the most important configuration properties for the job and the first implementation. That first implementation is still improvable and that will be the subject of the next part of this series. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Hadoop , HDFS , Java , Map/Reduce , VAR . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-03"},
{"website": "Octo", "title": "\n                Using Hadoop for Value At Risk calculation Part 1            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/using-hadoop-for-value-at-risk-calculation-part-1/", "abstract": "Using Hadoop for Value At Risk calculation Part 1 Publication date 02/11/2010 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 After introducing the Value at Risk in my first article , I have implemented it using GridGain in my second article . I conclude in this latter that relatively good performances have been reached through some optimizations. One of them was based on the hypothesis that the intermediate results – the prices for each draw – can be discarded. However, it is not always the case. Keeping the generated parameters and the call price for each draw can be very useful for business in order to analyze the influence of the different parameters. Such data are often crunched by Business Intelligence. VAR calculation is maybe not the best business use case in order to illustrate that need but I will reuse it as it has already been defined. The purpose of this new series of articles will be to compute the Value at Risk and keep all results, to be able to analyze them. In this first part, I will describe how to persist that data both with GridGain and with Hadoop; In the three next parts I will go into details of different implementations with Hadoop. These parts provide interesting code examples but can be safely discard in first read; Then in the fifth part, I will show how to use Hadoop to do Business Intelligence on the data; And in the last part, I will give some performance figures and improvement possibilities. In these articles, some portions of the code were removed when nothing was changed since the last article and replaced by comment ( //Unchanged... ). Evolution of the current implementation The easiest way to do it is by modifying the current implementation. I created for that purpose a Result class holding parameters and price for each draw. It implements Serializable to be easily stored on the disk and comparable in order for it to be sorted like the price . public class Result implements Serializable, Comparable<result> {\r\n\tprivate final Parameters parameters;\r\n\tprivate final double price;\r\n\t//Constructor, getters, compareTo(), equals(), hashCode(), toString() implementations...\r\n} computeVar() signature is modified accordingly. public SortedSet<result> computeVar(...) throws MathException {\r\n\t\t//Unchanged...\r\n\t\tfor (int i = 0; i < drawsNb; i++) {\r\n\t\t\t//Unchanged....\r\n\t\t\tfinal Result result = new Result(new Parameters(optionPricer.getParameters()),price);\r\n\t\t\t// For each draw, put the price in the sorted set\r\n\t\t\tsmallerPrices.add(result);\r\n\t\t\tif(configuration.isCombineAllowed()) {\r\n\t\t\t//Unchanged...\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn smallerPrices;\r\n} Finally the results are written to disk by calling FilePersistenceManager.writeToDisk() which basically delegates to an ObjectOutputStream plugged to the disk. Some measures, on the same laptop, show performance drawbacks as expected: Heap memory consumption increased since a collection of Result objects is bigger than a collection of simple double . In fact, on my 32 bits machine, I wasn’t able to generate more than 1,000,000 results. More computing over-head to manage the Result objects Writing data to disk leads to a high I/O rate. Writing a file of 1,000,000 results with Java serialization leads to a 61 MB file. My hard drive was saturated with a sustained load of 7 MB/s. The following graph shows the loss of storing the intermediate results defined like hereunder, thru different scenarios “combine” optimization (reference implementation) : only 10% of the total number or draws are sent back to the client function. See the first article ; Without optimization : all prices resulting for the draws are sent back to the client; Full result : both prices and parameters used to compute these prices are sent back to the client; Write to disk : the results are stored to disk using Java serialization. Writing all the intermediate results to disk for 1,000,000 draws is about 40 times slower than the most optimized scenario . The challenge is now to find solutions. This approach is very naive and some optimizations such as using a distributed cache to store the data would probably help. The third version of GridGain brings, for example, an integrate data grid . However, in order to evaluate new architecture solutions, I have chosen to implement it through Hadoop. Hadoop choice Hadoop presentation Hadoop , an Apache project self-defines itself as an open-source software for reliable, scalable, distributed computing . According to Wikipedia it enables applications to work with thousands of nodes and petabytes of data . In brief, it is a tool employed by Yahoo to process huge volume of data. It’s architecture was inspired by Google MapReduce and Distributed File System . I will not describe it any further. Please refer to and refers to this article for additional information. Why choosing Hadoop? I decided to implement this VAR calculation on Hadoop for 3 reasons: Such architecture has proven itself for some of the largest computing tasks of today such as indexing the web. Hadoop is based on the map/reduce pattern which is the basis of my VAR implementation Emergence of the new paradigm, named “NoSQL” , paves the way for new architecture and that article was an occasion to investigate it today. More specifically, “NoSQL” architectures are well suited for BI analysis. So I wanted to see if it would be possible to combine the two functions in one tool . One of the advantage would be to limit the quantity of data moved back and forth. For that purpose I used both Hadoop for map/reduce and Hive for data analysis. Hive is a kind of simplistic warehousing system built on top of Hadoop. In practice, it provides a DSL very close to SQL, which is translated by Hive into map/reduce actions. I will refer you to that article for Hive installation details. Hadoop & Hive implementation Hadoop and Hive are systems based on file manipulation (keep in mind they are based on a distributed file system). Hadoop consists in a Distributed File System , which enables to share code and data between nodes, and a bunch of jobs , divided into map and reduce tasks, coordinated by Hadoop framework. So Hadoop provides out of the box: Distributed storage and split of data: each file is stored as a bunch of blocks distributed through the physical machine with potential replication. Programmer vision is a filesystem , Hadoop manages distribution. Coordination of job with local affinity . In order to drastically improve performance for high volume of data, tasks are executed on the node where the concerned data lies in Hadoop processes files. I have written the following input file. 1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250\r\n1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250\r\n1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250\r\n1;252;120.0;120.0;0.05;0.2;0.15;1000;0.99;250 Each line means: Compute 250 draws of the call price with the following parameters (t=252 days, s0=120€, k=120€, r=0.05%, sigma=0.2, historicalVolatility=0.15) for scenario 1. The goal is to compute 1% VAR (0.99) on 1000 draws . Please refer to my previous article for an explanation about that parameters. The process of a Hadoop job is as follow. The number corresponds to the labels on the schema and are sorted in chronological order: 1. I choose to split data line by line (use of NLineInputFormat , described in a next part of that serie) meaning that each line will be send to a map slot); 2. 250 draws are processed in the map phase and prices are computed. Results are temporarily stored on the local file system. The output format will be key-value as described hereafter; 3. Results are pre-sorted by key and shared through the file system; 4. Results are sent to the reduce task. For the purpose of that first implementation I didn’t use the reduce side as we will see in the next part; 5. Results are shared in the distributed file system. So, the implementation with GridGain has shown that storing all results of disk have performance impact. Hadoop is a different tool. It is less optimized for computing intensive task but provides an efficient framework for distributing processing of large data. It’s implementation of the map/reduce pattern is quite different from GridGain and needs some adaptation of the way to compute the VAR. We will discuss in the next part the detailed implementation of the VAR calculation with Hadoop. The next three parts will be focused on coding and implementation details. If you are only a bit interested in technical details, you can skip these parts in first read and wait for the 4th part in which I will describe how to analyse the intermediate values. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Hadoop , HDFS , Java , Map/Reduce , VAR . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 4 commentaires sur “Using Hadoop for Value At Risk calculation Part 1” DS 12/04/2011 à 23:40 implementation of historic var simulation using php for sp500 stocks:\r\nhttp://indoorworkbench.com/?financerisk.html Sim Con 23/06/2011 à 00:19 Excel spreadsheet to calculate Value at Risk based upon a normal distribution can be downloaded here: http://optimizeyourportfolio.blogspot.com/2011/06/calculating-value-at-risk-in-excel.html:http://optimizeyourportfolio.blogspot.com/2011/06/calculating-value-at-risk-in-excel.html Sim Con 23/06/2011 à 00:19 Sorry, that link should be: http://optimizeyourportfolio.blogspot.com/2011/06/calculating-value-at-risk-in-excel.html Simon Connel 13/08/2011 à 18:20 Could you update this to implemented the modified value at risk, which models skew and kurtosis in the returns distribution.\r\n\r\nThe math is here: http://investexcel.net/223/modified-value-at-risk/ Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-02"},
{"website": "Octo", "title": "\n                Groovy Minute – inject() and accumulators            ", "author": ["Cyril Picat"], "link": "https://blog.octo.com/en/groovy-minute-inject/", "abstract": "Groovy Minute – inject() and accumulators Publication date 12/11/2010 by Cyril Picat Tweet Share 0 +1 LinkedIn 0 What the f*** is a Groovy Minute? I am thinking about starting a new kind of post about Groovy. Publishing short articles on a regular basis where I will give some Groovy hints and try to exchange on Groovy code with others. That’s what a “Groovy Minute” stands for, some Groovy minutes in your day-to-day work -and mine. Not sure at what pace I will do it it but I am willing to publish something each time I come with some worthy-to-share Groovy code. That’s for the title of this series. Concerning the content of these posts, I will talk about some of the language constructs (what they could be used for) and try to share and improve some code I have come up with. First minute…about the inject() method So today, I will talk about the inject() method. Why? I have been preparing a Coding Dojo recently and tried to come with a solution to the Roman to numeral problem . Wikipedia is a good way to grasp the problem and I reproduced part of it below: # I = 1 # V = 5 # X = 10 etc... # XVI = 1 + 5 + 10 = 16 # XIV = 5 - 1 + 10 = 14 Basically you have to iterate on the string from right to left and sum the number unless it is less than the preceding one (in this case you subtract it). The best solution ( * ) I have come with in Groovy is this one : use(RomanNumberCategory) {\r\n    def reverseString = string.reverse()*.toDecimal()\r\n    def last = 0\r\n    reverseString.inject(0) { sum, current -> \r\n        sum += sign(last, current) * current; last = current; sum \r\n    }\r\n} Why does this work? The closure has access to the local context where it is defined thus is able to read and update the last variable during the inject() recursion. I have also a one-liner solution for the romanToNumeral function which is less readable but interesting in its usage of inject() : string.reverse()*.toDecimal().inject([sum:0, last:0]) { map, c -> \r\n    [sum: map.sum + sign(map.last, c) * c, last: c] \r\n}.sum If you want to try some code, the code is on the Groovy Web Console with the tests, just play with it! And don’t forget to leave a comment here with your own solution (a link to your Groovy Web Console script). I will then update this article with better or interesting solutions to be able to comment and vote for it. Everything is new here (vote widget, web console etc…) so your ideas and remarks are all welcome in the comments too. Wait a minute… (a bit of thinking) In which cases inject() is really useful? Inject is the groovy way to do a left fold . Left fold is basically iterating over a list from the head and applying a function to the current element and the accumulated value. This is very useful to calculate a sum, a factorial, a reverse etc… For a sum, the two following pieces of code are equivalent ( ** ): list.inject(0) { sum, elt ->\r\n    sum + elt \r\n} vs def sum = 0\r\nlist.each { sum += it } \r\nsum Yes, it is less functional but it opens you a bunch of possibilities. For example, what happens if you need more than one accumulator? Of course you could inject a map (see one liner above) but I am not sure how efficient it would be. If you prefer to use the pattern just shown, you could rewrite the romanToNumeral() closure as: def sum, last = 0\r\nstring.reverse()*.toDecimal().each { current -> \r\n    sum += sign(last, current) * current; last = current \r\n}\r\nsum So in my mind inject() is a more elegant solution to simple problems, enabling you to write more concise code (without the need to define a variable and to return a value). But keep in mind its equivalent, in case you need for example more than one accumulator. — (*) it is still an imperfect solution as it does not handle invalid characters or invalid combinations (eg. ‘IC’) (**) you should use Groovy sum() in this particular case, of course Updates: solutions suggested First solution inspired by Tim Yates and its injectWithIndex script : def romanToNumeral = { string ->\r\n\r\n    List.metaClass.injectWithTwoAccu = { accu1, accu2, closure ->\r\n        delegate.each { val ->\r\n            (accu1, accu2) = closure.call(accu1, accu2, val)\r\n        }\r\n        [accu1, accu2]\r\n    }\r\n    \r\n    use(RomanNumberCategory) {\r\n        def reverseString = string.reverse()*.toDecimal()\r\n        reverseString.injectWithTwoAccu(0, 0) { sum, last, current ->\r\n            [sum + sign(last, current) * current, current] }[0]\r\n        }\r\n    }\r\n} I find this one very elegant but it is a shame you have to write your own injectWithTwoAccu() … Isn’t there anything to generalize from injectWithIndex() or injectWithTwoAccu() ? A new task for you, Groovy team… Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged development , Groovy , groovy minute , inject , language . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-12"},
{"website": "Octo", "title": "\n                How to “crunch” your data stored in HDFS?            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/how-to-crunch-your-data-stored-in-hdfs/", "abstract": "How to “crunch” your data stored in HDFS? Publication date 27/10/2010 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 HDFS stores huge amount of data but storing it is worthless if you cannot analyse it and obtain information. Option #1 : Hadoop : the Map/Reduce engine Hadoop Overview Hadoop is a Map/Reduce framework that works on HDFS or on HBase . The main idea is to decompose a job into several and identical tasks that can be executed closer to the data (on the DataNode). In addition, each task is parallelized :  the Map phase . Then all these intermediate results are merged into one result : the Reduce phase . In Hadoop, The JobTracker (a java process) is responsible for monitoring the job, managing the Map/Reduce phase, managing the retries in case of errors. The TaskTrackers (Java process) are running on the different DataNodes. Each TaskTracker executes the tasks of the job on the locally stored data. Hadoop also provides tools to supervise current jobs and hadoop engine (http://localhost:50030/jobtracker.jsp). Data and process co- localization HDFS and Hadoop have been designed to avoid data transit over the network and so, limiting the high-bandwith consumption. This pattern is often called “localization” and is frequently implemented in number of data or calculation grids. The main idea is that instead of moving the data from the storage layer to the application layer (and thus consuming bandwidth), the processes are pushed to the storage layer, closer to where the data is physically stored. In a certain way, this is what SQL does in the relational world. Thus, each process works with the locally stored data. Querying data using Hadoop Let’s say you have the following sample stored in HDFS and you would like to get the total amount (CREDIT – DEBIT) of all these lines aggregated by currency (EUR, USD…). GED\tEQSWAP\tJohn\t13/09/2010\tEUR\t10000\tDebit\tSG\r\nGED\tEQSWAP\tJohn\t15/09/2010\tEUR\t10200\tCredit\tSG\r\nGED\tSWAPTION\tJohn\t14/09/2010\tEUR\t11000\tCredit\tHSBC\r\nGED\tSWAPTION\tJohn\t15/09/2010\tEUR\t5500\tDebit\tHSBC\r\nGED\tSWAPTION\tJohn\t16/09/2010\tEUR\t5500\tDebit\tHSBC\r\nGED\tSWAPTION\tJohn\t15/09/2010\tEUR\t11000\tDebit\tHSBC\r\nGED\tSWAPTION\tJohn\t16/09/2010\tEUR\t5500\tCredit\tHSBC\r\nGED\tSWAPTION\tJohn\t17/09/2010\tEUR\t5500\tCredit\tHSBC\r\nIRD\tIRS\tSimon\t13/09/2010\tUSD\t10000\tDebit\tSG\r\nIRD\tIRS\tSimon\t15/09/2010\tUSD\t10200\tCredit\tSG\r\nIRD\tIRS\tSimon\t14/09/2010\tUSD\t11000\tCredit\tBankofAmerica\r\nIRD\tIRS\tSimon\t15/09/2010\tUSD\t5500\tDebit\tBankofAmerica\r\nIRD\tIRS\tSimon\t16/09/2010\tUSD\t5500\tDebit\tBankofAmerica\r\n... Here is what you will have to write in Hadoop… The Map phase : create maps in which the key will be the currency and the value the signed amount The Mapper will read the specified input file and call the map() method for each line of the file. /**\r\n* Key In, Value In, Key out, Value out\r\n*/\r\n public static class Map extends MapReduceBase implements Mapper {\r\n      public void map(LongWritable key, Text value, OutputCollector output, Reporter reporter) throws IOException {\r\n        String line = value.toString();\r\n        String[] lineAsArray = line.split(\"\\t\");\r\n        String currentCurrency = lineAsArray[4];\r\n        String amountAsString = lineAsArray[5];\r\n        String sens = lineAsArray[6];\r\n        DoubleWritable data = null;\r\n        if(\"Debit\".equals(sens)){\r\n        \tdata = new DoubleWritable(Double.parseDouble(\"-\" + amountAsString));\r\n        }\r\n        else if(\"Credit\".equals(sens)) {\r\n        \tdata = new DoubleWritable(Double.parseDouble(amountAsString));\r\n        }\r\n        output.collect(new Text(currentCurrency), data);\r\n        }\r\n      } The Reduce phase : sum all the operations grouped by currency The Reducer will then collect all the previously created map on the different nodes and do what you define in the reduce() method. The reduce() method is called for each key specified as output of the map. In our case, we have two different keys : the two distinct currencies. So the reduce() method is called twice with this kind of map EUR\t-10000,10200,11000...\r\nUSD\t10000,... The “values” argument (see the code sample beside) of the reduce() method is the list of amount grouped by key. Hadoop has sorted the data this way for you : //The reduce is called once per key in the output map of the map() function\r\n public static class Reduce extends MapReduceBase implements Reducer {\r\n\tpublic void reduce(Text key, Iterator values, OutputCollector output, Reporter reporter) throws IOException {\r\n        double sum = 0;\r\n        while (values.hasNext()) {\r\n        \tdouble amount = values.next().get();\r\n        \tsum += amount;\r\n        }\r\n        output.collect(key, new DoubleWritable(sum));\r\n      }\r\n} Run the job on Hadoop Then of course, you have to code the main class that specifies job configuration, can overwrite default xml configuration… public class CurrencyAggregate extends Configured implements Tool {\r\n@Override\r\npublic int run(String[] args) throws Exception{\r\n\tJobConf conf = new JobConf(CurrencyAggregate.class);\r\n\tconf.setJobName(\"CurrencyAggregate\");\r\n\t//output of the Mapper\r\n\tconf.setOutputKeyClass(Text.class);\r\n\tconf.setOutputValueClass(DoubleWritable.class);\r\n\tconf.setMapperClass(Map.class);\r\n\r\n\tconf.setReducerClass(Reduce.class);\r\n\tconf.setInputFormat(TextInputFormat.class);\r\n\tconf.setOutputFormat(TextOutputFormat.class);\r\n\r\n\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\r\n\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\r\n\tJobClient.runJob(conf);\r\n\r\nreturn 0;\r\n\t}\r\npublic static void main(String[] args) throws Exception {\r\n\tint exitCode = ToolRunner.run(new CurrencyAggregate(), args);\r\n\tSystem.exit(exitCode);\r\n} Note : To develop your job, you can work locally using the Standalone Operations defined here . In that case, the master and slaves are configured as “local” and therefore, Hadoop uses the local file system and not HDFS. You can then launch your job on Hadoop with the following command line (the first argument is the input file, the second one the folder where the results will be stored) : $HADOOP_HOME/bin/hadoop jar ./Currency.jar org.CurrencyAggregate /tmp/cashflow-clean.txt /tmp/output10 and you get the following results in the folder /tmp/output10/part-00000 EUR\t200.0\r\nUSD\t200.0 Hadoop Installation & Configuration To run Hadoop, you need a proper installation of HDFS . You also need to modify the mapred-site.xml file on all machines to define the JobTracker : mapred.job.tracker\r\n  master:9001 Thus, you are able to define other properties in it : – mapred.local.dir , the local directory where temporary MapReduce data is stored. It also mahy be a list of directories. – mapred.map.tasks and mapred.reduce.tasks , the number of map or reduce tasks Once the Hadoop cluster is configured, you can start Hadoop with the following command  ( (HDFS must be running) $HADOOP_HOME/bin/start-mapred.sh Option #2 – Doing Map/Reduce can be complex, so we need a “DSL” : Hive or Pig There are two high-level languages that have been developed to work on top of Hadoop: – Pig which provides a specific script language. Unfortunately, I do not have enough time to investigate this further for the moment. I will try to do so in a near future. – Hive which provides a limited SQL-like language (for instance, Date type is not supported…). Hive Overview and Usage Hive works on top of Hadoop and transforms Hive Query Langage statements into Map/Reduce jobs. source : http://wiki.apache.org/hadoop/Hive/Design The Driver, Compiler and Executor components are responsible for receiving, parsing the query and generating a set of map/reduce job, dfs operations… The metastore stores the information on the different “tables”, “columns” stored in the HDFS warehouse. This link provides a deep overview of Hive . Hive supports different data types : INT, STRING, DOUBLE. So, you can define tables in the same way as you will think of tables in the relational paradigm. Hive does not manage Date type so you will have to use timestamp instead…Thus my previous file… GED\tEQSWAP\tJohn\t13/09/2010\tEUR\t10000\tDebit\tSG\r\nGED\tEQSWAP\tJohn\t15/09/2010\tEUR\t10200\tCredit\tSG\r\n... …has to be converted this way… GED\tEQSWAP\tJohn\t1284354000\tEUR\t10000\tDebit\tSG\r\nGED\tEQSWAP\tJohn\t1284526800\tEUR\t10200\tCredit\tSG\r\n... Then, you can create the table CREATE TABLE cash_flow (BookID STRING, ProductID STRING, TraderID STRING, DueDate BIGINT,\r\nCurrency STRING, Amount DOUBLE, Direction STRING, Counterparty STRING)\r\nROW FORMAT DELIMITED\r\nFIELDS TERMINATED BY '\\t'\r\nLINES TERMINATED BY '\\n' STORED AS TEXTFILE; Load data into these tables from you local file system. The file will then be put into HDFS (typically in the folder /user/hive/warehouse) : LOAD DATA LOCAL INPATH '/home/user/cash-flow/cashflow-clean.txt'\r\nOVERWRITE INTO TABLE cash_flow; from HDFS if your datas are already on HDFS LOAD DATA INPATH '/data/cashflow-clean.txt'\r\nOVERWRITE INTO TABLE cash_flow; or without overwriting the datas LOAD DATA LOCAL INPATH '/home/user/hive/build/dist/examples/files/ml-data/u_2.data'\r\nINTO TABLE u_data; note : I have still not tested it but Cloudera provides Sqoop : a tool designed to import relational database into Hadoop or even Hive…It also seems that Sqoop offers the capability to export Map/Reduce results into a relational database. Select datas into these tables in SQL manner Remember the sample written in option#1. You can then easily get the sum of all the operations ‘Credit’ grouped by currencies select Currency, sum(Amount)\r\nfrom cash_flow where Direction='Credit' group by Currency; or filtering them by due date select Currency, sum(Amount)\r\nfrom cash_flow\r\nwhere Direction='Credit' AND DueDate < = unix_timestamp('2010-09-15 00:00:00')\r\ngroup by Currency; You can then run the same query with the operation in “Debit” and you will get your balance. You can even join tables SELECT cash_flow_simple.*,user_simple.*\r\nFROM cash_flow_simple\r\nJOIN user_simple ON (cash_flow_simple.TraderID=user_simple.UserID); Hive provides other complex functions like sub queries , union , partitions, different relational, arithmetic or operational operators and mathematical or date functions …And in the case you have more specific needs, you can also define your own functions and extend Hive functionalities . Hive clients : Hive-cli, JDBC, ODBC… To connect to Hive, you have several options. The simplest one is certainly the Hive-Cli (or even the web client interface ); a shell that enables you to query your HDFS. But moreover, you can use JDBC (need to start a Hive Server $HIVE_HOME/build/dist/bin/hive --service hiveserver )  to connect to Hive : a way to build great user interface on top of your HDFS. note : when writing these couple of lines, the trunk version of Hive does not compile with the version 0.21.0 of Hadoop. https://issues.apache.org/jira/browse/HIVE-1612. PreparedStatement are not implemented in the 0.7.0 version of the Hive JDBC Driver Hive installation Once again, Hive installation is not a big deal and all is well explained here Of course, you need the Hadoop distribution installed on the machines, define a couple of constants like HADOOP_HOME and HIVE_HOME … By default, Hive will store the data on HDFS in the folder /user/hive/warehouse ./bin/hadoop fs -mkdir /user/hive/warehouse\r\n./bin/hadoop fs -chmod g+w /user/hive/warehouse aka. the hive.metastore.warehouse.dir property that can be overridden in the hive-site.xml configuration file. From an operational point of view, Adobe released a couple of puppet scripts to manage Hadoop cluster or dynamically add slave nodes . Hadoop, Hive and Pig use-cases There are several companies that use Hadoop and Hive or Pig to collect and analyse data. Of course, Yahoo! has one of the biggest data warehouse with more than 12 Petabytes of datas. Twitter collects 300Go per hours Facebook stores 12 To per day and provides an easier access to the data warehouse (200 persons / month use Hive) There are also “smaller” companies that use Hadoop. Last.fm uses Hadoop for site stats, chart, reporting, index search… Orbitz stores 500Go per day in Hadoop and provides lots of statistics by integrating the R-project . Research field is also using Hadoop ( http://www.opensciencegrid.org /….). Software companies are also deeply interested in Hadoop.  IBM has developped a product based on Hadoop  named IBM Infosphere BigInsights …Teradata has announced [https://blog.octo.com/teradata-cloudera-partenariat-autour-de-data-warehousing-et-de-hadoop/] its willingness to use Hadoop. Pentaho begins to offer an integration between Hive and its reporting or BI Suite. Remember, noSQL is also about Business Intelligence… Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Hadoop , HDFS , Hive , Map/Reduce , NoSQL . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 5 commentaires sur “How to “crunch” your data stored in HDFS?” Olivier Mallassi 08/11/2010 à 18:07 Hadoop @ Bank Of America : http://www.cloudera.com/videos/hw10_video_the_business_of_big_data\r\nUnfortunately a high level talks but compare SAS and Hadoop (on unknown use cases and infrastructure...)\r\n15 GB : SAS 9 min | Hadoop 9 min\r\n64 GB : SAS 45 min | Hadoop 9 min\r\n640 GB : SAS N/A | Hadoop 28 min\r\n\r\nCheck also the last 2 minutes videos (around minute 18). Sangita 20/01/2011 à 06:50 Hi,\r\nThank you for explaining hadoop and hive in a nutshell.I am really impressed by your article.I am new to hadoop and just now started with hadoop programming.\r\nI have a working hive/hadoop cluster with 4 nodes. I am using the following versions in hadoop and hive .\r\nhadoop 0.20.0 version and hive 0.4.1 from apache web site.\r\n\r\nI wish to develop a multitenant web application using J2EE.And the tenant information will be stored in HDFS.\r\n\r\nI had worked with simple HQL queries (to access the tables directly)but i dont know how to execute such query via a web application.\r\nCould you tell me the procedure to include a hive query in a typical java program (have worked in sql+ java).Is that to be done using JDBC? \r\n\r\nI have gone through the link \r\n\r\nhttp://wiki.apache.org/hadoop /Hive/HiveClient#JDBC_Client_Sample_Code, but am still not sure about the actual procedure.\r\n\r\nAlso u had mentioned that prepared statement is not supported in hive 0.7.0 version.\r\n\r\nWill Cloudera has resolved all these issues in its hadoop distribution? \r\n\r\nRegarding Hive,\r\n\r\nIs there any other way(other than using sqoop) to have the query results in a (in hdfs)file? Actually i don't want to save the query result in a table manually. \r\n\r\nOr where the results of (hive query results)map/reduce jobs will be stored?\r\n\r\nPlease clarify.Kindly send your suggestions and ideas which will help me to get a better insight in this project.\r\n\r\nThank you for spending your valuable time.\r\n\r\nRegards,\r\nSangita\r\n\r\nKindly clarify my doubts though it seems to be Olivier Mallassi 03/02/2011 à 19:50 Hi sangita, \r\n\r\nwith Hive, you can write to the HDFS using statement like\r\nINSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...\r\n\r\nIf you use LOCAL, the result is written to the local FS and not the HDFS.\r\n\r\nYou can also store result set in another table with a statement that looks like \r\nCREATE TABLE AS SELECT * FROM ...\r\n\r\nYou can call Hive from your java code directly with the JDBC API (and Statement API). this is the same code. check the doc : http://wiki.apache.org/hadoop/Hive/HiveClient\r\n\r\nYou can also check Pig. you will have to learn a new script langage but pig is well to describe flow of operations on a dataset. \r\nFrom the Java side, the API are different. Skandha N Maiya 14/02/2011 à 03:15 So hive works on top hadoop?\r\nAs i understand Hadoop is a batch processing system. So Hive queries get converted into batch files? Foram 25/06/2012 à 23:05 Very nice article and very nice outbound links as well. I love the Orbitz example. Exactly what I was looking for - how to use Hadoop, Hive and R Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-10-27"},
{"website": "Octo", "title": "\n                Hadoop Distributed File System : Overview & Configuration            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/hadoop-distributed-file-system-overview-configuration/", "abstract": "Hadoop Distributed File System : Overview & Configuration Publication date 21/10/2010 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 Hadoop Distributed File System can be considered as a standard file system butt it is distributed. So from the client point of view, he sees a standard file system (the one he can have on your laptop) but behind this, the file system actually runs on several machines. Thus, HDFS  implements fail-over using  data replication and has been designed to manipulate, store large data sets (in large file) in a write-one-read-many access model for files. Overview HDFS is made of several Java process running on separate (even if this is not necessary) physical machines and follow a Master/Slave model : – NameNode . The NameNode is responsible for storing the files’ metadatas (the locations of the files…) and for tracing all the operations realised on the file system. All these datas are stored on the local file system of the NameNode in two files called “Edit Log” and “FSImage” : – Edit Log . Stores all changes that occur into the file system metadatas. For instance, adding  a new file in HDFS is traced in this file. – FSImage . Stores all the file system namespace : the location of files (or blocks) in the cluster – DataNodes . The DataNodes simply store the datas, ie. the files’ blocks without any knowledge of HDFS. The files’ blocks are stored on the local file system of the DataNodes and are replicated among the other datanodes based on  the configured replication factor. Commands on the file system HDFS thus provides a lot of commands to import, copy, delete or whatever you wanna do with your files…The exhaustive list of commands is available here and you can interact using either a shell or the Java API For instance, if you want to delete the folder /chukwa/logs and all its subfolders, you can type (on the client machine which must have Hadoop shells installed) : $HADOOP_HOME/bin/hadoop fs -rm /chukwa/logs/*.* To create a folder, you can use mkdir (almost as usual) : $HADOOP_HOME/bin/hadoop fs -mkdir /tmp/myfolder To copy a file from your local directory to the /data directory : ./bin/hadoop fs -put /home/user/cash-flow/cashflow-clean.txt /data HDFS also provides a console (http://localhost:50070/dfshealth.jsp) which in turn provides a HDFS couple of metrics and a way to navigate in the file system. Handling failures In HDFS, the files are truncated into several blocks (the block size can be configured) and these blocks are replicated (based on the configuration) around several replicas, on different DataNodes. Moreover, each DataNodes broadcast its health to the NameNode using a heartbeat mechanism. If the NameNode does not detect the heartbeat, the node is therefore considered dead. The datas are automatically replicated to other available DataNodes to respect the configured replication factor. Moreover it seems that you can dynamically add a new DataNode to the cluster without restarting it . New files will be stored on the added DataNode but you may have to manually rebalance the HDFS blocks into the cluster. This is the simplest part of the deal as I am more than doubtful concerning NameNode failures. If metadatas are corrupted, HDFS could be non-functional…It seems there are two ways of managing this (I may have to digg further with these “fail-over” mechanisms in the future). In the first one, the NameNode can be configured (via the property dfs.name.dir in hdfs-file.xml configuration file) to maintain multiple copies of the FSImage and EditLog (on different devices). The replicas are synchronously done and so decreasing the throughput. In the second one, a SecondaryNameNode can be defined (in the masters configuration file ) and in that case : The SecondaryNameNode periodically compacts the EditLog into a “checkpoint;” the EditLog is then cleared. A restart of the NameNode then involves loading the most recent checkpoint and a shorter EditLog containing only events since the checkpoint. Without this compaction process, restarting the NameNode can take a very long time…The duties of the SecondaryNameNode end there; it cannot take over the job of serving interactive requests from the NameNode. Although, in the event of the loss of the primary NameNode, an instance of the NameNode daemon could be manually started on a copy of the NameNode metadata retrieved from the SecondaryNameNode. Installing & Configuring a HDFS cluster The official documentation is quite complete for version 0.20.2 and 0.21.0. Once HDFS (and in fact Hadoop) has been installed you need to specify the constant HADOOP_HOME (at least for the version 0.21.0) and  the cluster configuration. In our case, we will use 3 machines : 1 master (aka. the NameNode) and 2 slaves (aka the DataNodes). Master Configuration You will have to define which machine is the master, which ones are the slaves and the ssh connexions between them (aka. NameNode and the slaves aka. DataNodes) – copy the phraseless key (for instance id_ras.pub) on the remote slaves ssh-copy-id -i /home/user/.ssh/id_dsa user@master-ip – In $HADOOP_HOME/conf/masters file, write the master IP – In $HADOOP_HOME/conf/slaves , add the two slaves IP These two configuration files are used by the start-dfs.sh shell to remotely start the DataNodes. Slaves Configuration There are no particular configuration needed except the slaves must have the Hadoop distribution installed and must follow the same folders arborescence. GlobalConfiguration On all machines, you will have to modify the ./conf/*-sites.xml files to specify : – core-site.xml . Specifiy the hdfs url (by replacing ip-master with the real master IP) <property>\r\n        <name>fs.default.name</name>\r\n        <value>hdfs://ip-master:9000</value>\r\n </property> If you do not specify this, you will get the following error : 20:13:18,026 INFO org.apache.hadoop.security.Groups: Group mapping \r\nimpl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000\r\n2010-09-12 20:13:18,058 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: \r\njava.lang.IllegalArgumentException: Invalid URI for NameNode addrgeess (check fs.defaultFS): file:/// has no \r\nauthority. – hdfs-site.xml . Specify the replication factor. <property>\r\n<name>dfs.replication</name>\r\n <value>2</value>\r\n</property> you specify the number of replicas (cannot be greater than the DataNodes number) HDFS launching… Launching HDFS is quite simple. Type the following command from the master : $HADOOP_HOME/bin/start-dfs.sh The DataNodes will be remotely (using ssh) started from the NameNode. If this is the first usage of HDFS, you will need to format the file system using the command line : $HADOOP_HOME/bin/hadoop namenode -format To conclude Here is an overview and a short introduction of how to setup a HDFS mini cluster.Yet, I still have a couple of questions to look into. The first one is about the NameNode fail-over mechanisms in terms of service interruption as well as data corruption. The second one concerns  the HDFS (and Hadoop) infrastructure upgrade with (or without) service  interruption. Last but not least, there are also concerns about how to process this data. Storing vast amount of data is a first step,  the next one will be to see how to “crunch” all these datas using Hadoop. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Hadoop , HDFS , NoSQL . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-10-21"},
{"website": "Octo", "title": "\n                Analyzing Groovy / Grails code            ", "author": ["Cyril Picat"], "link": "https://blog.octo.com/en/analyzing-groovy-grails-code/", "abstract": "Analyzing Groovy / Grails code Publication date 19/10/2010 by Cyril Picat Tweet Share 0 +1 LinkedIn 0 Groovy and Grails are a breeze of fresh air in Java development and Web application development as they enable you to write far more expressive and readable code. They also solve most of the classic Java pitfalls (BigDecimal, equals(), etc…) all new Java programmers meet along their way. Even if you write more readable and thus more maintainable code in Groovy, this is not enough let alone: of course you need some good practices to keep your code under control as your application keeps growing. One of these practices would be to analyze your source code and check how metrics and static checks evolve from release to release, or even on a more sporadic way for a specific release. In this field I must say that the Java programmer is one of the best tooled as there are plenty of choices, free or not, for performing such a task (PMD, FindBugs, CheckStyle, Sonar etc…). So, what about it in Groovy / Grails? Tools available in Groovy and what they do Groovy eco-system is far smaller than in Java when talking about code analysis tools. Basically you will find: GMetrics : available either standalone or as a Grails or Griffon (Grails twin brother for RDA) plugin, it computes very basic metrics on your code. By default those are lines per method, lines per class and cyclomatic complexity. Very basic but very useful. CodeNarc : available standalone, as a Grails/Griffon plugin and also as a Gradle (a groovier build system) plugin. It performs static source code analysis, thanks to a set of rules. It performs checks similar to what CheckStyle and PMD do. It has rules for Groovy code but also a set of specific rules for Grails.  As an example of Groovy rules, it will find all sort of empty blocks, problem with generic throw/catch, etc… It will also enable you to define naming conventions or to define thresholds on code complexity. Concerning Grails rules, it will for example check that your Services are stateless as expected. Sonar : actually we are talking about a plugin for Sonar. This is something very new as the plugin is in version 0.2 and had its first release in August this year… Version 0.2 added support for test reports and test coverage. The plugin relies mainly on CodeNarc and GMetrics as well as on Squid (rewrite of JavaNCSS by SonarSource) for some basic metrics. Although in version 0.2, it seems quite usable and I did not find too much trouble using it. It gives you all the power of Sonar: managing the software quality in an integrated tool and being able to see your quality evolving as time passes. Compared to the tools above, it will also give you the duplicated code, which is something definitively very useful. Groovy is Java, isn’t it? What about the other Java tools? Can they work or not on Groovy code? Of course we haven’t tried all of them but basically you will find two categories of tools: those that work on source code (CheckStyle, PMD etc…): they won’t work at all because their parser will surely fail on Groovy code. Note that, for example, you can still use PMD (CPD) to find duplicated code as it actually works for all languages (C, C++, Python etc…). those that work on bytecode (XDepend, FindBugs etc…): they will be able to work but will generate all kind of noises. For example you will get plenty of warnings due to the Groovy framework if you run FindBugs on a Groovy application. In the same way, if you run XDepend on a Grails application, you will get weird dependencies due to Groovy internals (for example Groovy generates one class per closure using a naming convention) or to Grails internals (all GSPs will appear to be very long Groovy methods). I don’t believe running Findbugs would make any sense but being able to use XDepend would be great of course, no matter what your language is. So all those tools are of little help for what we are looking for! Are we looking for the right tools for Groovy? As Java programmers, we hope to find in Groovy / Grails eco-system the same tools we are used to in Java. What a shame there isn’t a FindBugs for Groovy! But does it make sense? Groovy is by nature a dynamic language and of course you don’t develop code in the same way you develop it with a static language. Groovy is both weakly and strongly typed, as you wish, but is very often used with weak typing (I think I should say duck typing actually, see here ). Of course there is a strong emphasis on tests in dynamic languages if you want your software to be both reliable and maintainable. But as mentalities evolve, I would say this is no more a difference between static and dynamic languages given tests have taken such an importance as well for static languages. We no longer ensure code quality afterwards by running analysis tools but we do ensure it beforehand by testing in TDD. To make a long story short, I would say that the code coverage is thus one of the most important metric to watch, more than all the metrics and potential bugs that these tools can compute. Good news, there is a code coverage plugin for Grails which is called Test Code Coverage Plugin . It uses Cobertura underneath as Cobertura has a support for the Groovy language (Clover too). To make my short story even shorter: we won’t necessarily miss all these analysis tools we have in Java. Of course the usefulness of all these tools is highly context-dependent and you might be in a context where you will truly miss one of these tools. For example I would truly miss Checkstyle, Findbugs and PMD in a project with several team members and different skill levels. Or I would truly miss XDepend in a big project where project architecture has been lost or is undefined. No doubt the support for Groovy in this kind of tools will grow if Groovy / Grails make their way in the enterprise or continue to spread in the small to mid-size projects where they are currently used. As usage will increase, anti-patterns will become clearer and enable this kind of tools to become more useful. The final setup is… Sonar looks like the perfect fit for the job even if the plugin is still in its early stage. Note, if you are already using a Hudson or plan to setup one, you could setup most of the above tools (CodeNarc, CPD, Cobertura etc…) in your Hudson. Actually, even if Sonar is really easy to install, analyzing a Grails project with it is a little bit trickier. Here is how to proceed: Step 1: Install Sonar and Groovy plugin Basically you just have to download Sonar and unzip it wherever you want. Then you download Groovy plugin for Sonar and you copy the downloaded JAR in [sonar]/extensions/plugins where [sonar] is the directory where you extracted Sonar ZIP file. At last you run Sonar server, using sonar.sh or StartSonar.bat depending on you platform. For more info, see their 2-minutes tutorial . Step 2: Configure your Grails project A good starting point is the plugin home page . It is only valid for a Groovy project, setting up a Grails project is slightly different. You have basically two choices: 1. Not-mavenized Grails project This is, interestingly, my recommended approach. Actually this is the one that works best at this time. For this you need; to install Grails Test Code Coverage plugin by running: $ grails install-plugin code-coverage to configure Cobertura to generate the XML report needed by Sonar. For this, add these lines to your grails-app/conf/BuildConfig.groovy file: coverage {\r\n\tenabledByDefault = false\r\n\txml = true\r\n} to add a pom.xml file at the root of your project with the following content: <project xmlns=\"http://maven.apache.org/POM/4.0.0\"\r\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\nxsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\r\n  <modelVersion>4.0.0</modelVersion>\r\n  <groupId>com.yourcompany</groupId>\r\n  <artifactId>yourproject</artifactId>\r\n  <version>1.0</version>\r\n  <packaging>pom</packaging>\r\n  <name>Your Project</name>\r\n  <build>\r\n    <sourceDirectory>grails-app</sourceDirectory>\r\n        <plugins>\r\n           <plugin>\r\n              <groupId>org.apache.maven.plugins</groupId>\r\n              <artifactId>maven-compiler-plugin</artifactId>\r\n              <configuration>\r\n                  <source>1.6</source>\r\n                  <target>1.6</target>\r\n                  <excludes>\r\n                      <exclude>**/*.*</exclude>\r\n                  </excludes>\r\n              </configuration>\r\n           </plugin>\r\n           <plugin>\r\n              <groupId>org.codehaus.mojo</groupId>\r\n              <artifactId>build-helper-maven-plugin</artifactId>\r\n              <version>1.1</version>\r\n              <executions>\r\n                <execution>\r\n                  <id>add-source</id>\r\n                  <phase>generate-sources</phase>\r\n                  <goals>\r\n                      <goal>add-source</goal>\r\n                  </goals>\r\n                  <configuration>\r\n                      <sources>\r\n                          <source>src/groovy</source>\r\n                          <source>src/java</source>\r\n                      </sources>\r\n                  </configuration>\r\n                </execution>\r\n              </executions>\r\n           </plugin>\r\n        </plugins>\r\n  </build>\r\n  <properties>\r\n    <sonar.language>grvy</sonar.language>\r\n    <sonar.dynamicAnalysis>reuseReports</sonar.dynamicAnalysis>\r\n    <sonar.surefire.reportsPath>test/reports</sonar.surefire.reportsPath>\r\n    <sonar.cobertura.reportPath>test/reports/cobertura/coverage.xml</sonar.cobertura.reportPath>\r\n    <sonar.phase>generate-sources</sonar.phase>\r\n  </properties>\r\n</project> to generate the tests report and coverage before running Sonar analysis $ grails test-app -coverage 2. Mavenized Grails project This setup is easier but, at the time of writing it out, you will get a few limitations. This is what you need to add in your POM: tell Sonar your are analyzing Groovy code <properties>\r\n    <sonar.language>grvy</sonar.language>\r\n </properties> setup the path for your source code (more on this later) <build>\r\n    <sourceDirectory>grails-app</sourceDirectory>\r\n</build> change the Surefire report path to match grails’ one <build>\r\n   <plugins>\r\n      <plugin>\r\n        <groupId>org.apache.maven.plugins</groupId>\r\n        <artifactId>maven-surefire-plugin</artifactId>\r\n        <version>2.6</version>\r\n\t<configuration>\r\n\t  <reportsDirectory>${project.build.directory}/test-reports</reportsDirectory>\r\n\t</configuration>\r\n      </plugin>\r\n   </plugins>\r\n</build> This is it! Step 3: Enjoy! You just need to run the following command at the root of your Grails project (make sure Sonar is running, refer to Step 1 ): $ mvn sonar:sonar Then go to Sonar web interface by browsing to http://localhost:9000/ and enjoy it! You will get the standard Sonar dashboard you are certainly used to: Be aware you will have to configure CodeNarc and CPD to only report warnings and errors you really want your team to fix . This is the hardest part in setting up this kind of tools on a project. Step 4: Hold on there, there are still a few limitations… The plugin has a few limitations, the greatest one being the problem in parsing some Groovy files (see SONARPLUGINS-596 ). It has obviously an impact on the report because the figures, violations and duplications will completely ignore the files in error. It will be interesting for you to see which files are impacted by looking at the stack traces when running ‘mvn sonar:sonar’. Otherwise, if you use Maven, you get a few more limitations: – Sonar Groovy plugin only locates your source files by the use of the sourceDirectory parameter therefore with the above setting (set to grails-app/), files in src/groovy and src/java won’t be included in the analysis results (see SONARPLUGINS-737 ) – the analysis won’t work if one of your tests is failing (unfortunately Grails Maven plugin ignores the parameter -Dmaven.test.failure.ignore=true, see GRAILS-6850 ) Nothing else? There is one last point about source code quality we did not mentioned at all in this article, which is the IDE. Of course the IDE is of great help to manage source quality at the root by the use of JavaDoc, code completion and refactoring, but also through the use of all sort of plugins (often the same tools as mentioned above). The same is true in Groovy and using a good IDE is certainly a productivity and quality boost. You will even find in IntelliJ IDEA checks on what is call ‘idiomatic Groovy’ – or Groovy code inspections. This is a broad subject and would be for sure an interesting subject to dig further in another article. But here also, you could ask yourself are you really looking for the right tool? You will see plenty of Grails or Ruby on Rails developers going back to the use of a clever text editor like TextMate instead of the big IDE we are used in Java. In fact I am not sure they are wrong, depending of course on the size of the project they are presently working on… Follow @cyrilpicat Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged codenarc , development , Grails , Groovy , sonar , static analysis . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 8 commentaires sur “Analyzing Groovy / Grails code” Wanderson Santos 26/10/2010 à 20:00 Great article. Great tool. Thanks! Wanderson Santos 26/10/2010 à 20:09 There's another easy way to do Step 2!\r\n\r\nJust install Sonar Hudson Plugin and configure it with  \r\n-Dsonar.language=grvy Cyril Picat 27/10/2010 à 09:33 @Wanderson: Thanks for your feedback\r\n\r\nThanks for your tip too. But isn't it rather a replacement for Step 1? claudio 03/03/2011 à 17:40 Hi Cyril,\r\nI followed your steps for mavenized grails project but the code coverage always shows 0% code coverage. Any suggestions?\r\n\r\nThanks!! Cyril Picat 08/03/2011 à 11:38 @Claudio,\r\n\r\nhum that's strange. I did had a look at my POM and my Sonar instance and coverage was OK. A few ideas: What version of Grails are you using? What Maven command are you using, 'mvn install sonar:sonar'? Did you check that the Cobertura report was generated?\r\n\r\nI have to say my primary Grails project is not mavenized and it looks like the last Grails updates break somehow Maven support.  I tried to mavenized it again but there is an issue with Webflow (see GRAILS-5640 ).\r\n\r\nI will see if I can find a workaround to test this again. Anyway, the poor Maven support in Grails is another reason of using Sonar with a non-mavenized Grails project... lostinberlin 29/04/2011 à 10:56 Hi Cyril, \r\n\r\nthanks for the great article. \r\n\r\nCan you verify that you can see the groovy source-code coverage in the \"Coverage\" tab when you click your way through the code coverage pages? I have followed your instructions but can not get it to show the source.\r\n\r\nWith a java project I can see the Class and the lines which, for example, have been covered (green) and have not been covered (red). With a grails-app however I can not. The metrics (in numbers a la 27% covered) are all in the header, but the actual source is missing, meaning you can see percentage-wise how much code is covered, but not the actual lines. \r\n\r\nThe cobertura html report created by \"grails test-app -coverage\" is correct and has all source code included in the web pages. Akhilesh 26/07/2012 à 09:06 Thanks very much,can you advice how to automate the whole process with source at GitHub for grails project Jeff Winkler 17/10/2012 à 00:47 The most important thing we did is to put CodeNarc in the build loop and fail the build when violations increase. Otherwise all the reports in the world, in my experience, don't do much to effect change. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-10-19"},
{"website": "Octo", "title": "\n                Using Hadoop for Value At Risk calculation Part 6            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/using-hadoop-for-value-at-risk-calculation-part-6/", "abstract": "Using Hadoop for Value At Risk calculation Part 6 Publication date 09/11/2010 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 In the first part , I described the potential interest of using Hadoop for Value At Risk calculation in order to analyze intermediate results. In the three ( 2 , 3 , 4 ) next parts I have detailled how to implement the VAR calculation with Hadoop. Then in the fifth part , I have studied how to analyse the intermediate results with Hive. I will finally give you now some performance figures on Hadoop and compare them with GridGain ones. According to those figures, I will detail some performance key points by using such kind of tools. Finally, I will conclude why Hadoop is an interesting choice for such kind of computation. Some performance figures With that implementation it is now possible to grab some performance measures. I used a virtual machine with 2 GB RAM and 4 cores on my laptop, a DELL Latitude E6510 with a core i7 quad core and 4 GB of RAM. Indeed, Hadoop is easier to install on Linux bearing in mind I’m using Windows for my day to day work. Performance comparison cannot be made directly with previous measures taken on a physical machine. So I replayed the GridGain run in which all results are stored on disk on the virtual machine. I have done the measures with 1 and then 4 CPUs. The following graph clearly shows the gain of using Hadoop in both cases: To be honest, I should have said the loss due to Hadoop is, in most cases, slower than GridGain . Hadoop is only quicker when we use one thread and 100,000 draws. I used Hadoop in that case in local (standalone) mode with one process working on the local file system. Even if I don’t show you the figures, I should mention the fact that order of magnitude for Hive requests are very similar: 30 to 50 s. per request is a minimum. I shall conclude that distribution based on a distributed file system is more costly than the RPC based approach, even if I have to write the data to the disk with GridGain. It seems to me that the main reasons are: All intermediate steps in Hadoop involves disk writes whilst GridGain transmits in a RPC way For those tests I used only my laptop so all writes are done on a single disk. No distribution on I/Os are possible; For large volume of data, I could not do performance measures with GridGain due to my 32 bits architecture which limits my heap size. However, beginning with 100,000 draws we can see that Hadoop is as quick as GridGain. So my next step will be to analyze performance for larger volumes of data with the two implementations I have described and some optimizations. Values are written in a text for or in a binary form. The VAR is extracted by the main() function or by the reduce phase Some configuration parameters have been tuned as described below: #core-site.xml\r\nio.file.buffer.size=131072\r\n#hdfs-site.xml\r\ndfs.block.size=134217728\r\n#mapred-site.xml\r\nmapred.child.java.opts=-Xmx384m\r\nio.sort.mb=250\r\nio.sort.factor=100\r\nmapred.inmem.merge.threshold=0.1\r\nmapred.job.reduce.input.buffer=0.9 In brief, it allows having a bigger heap ( mapred.child.java.opts ), processing larger batch in memory before going to disk ( io.file.buffer.size , io.sort.mb , io.sort.factor , mapred.inmem.merge.threshold , mapred.job.reduce.input.buffer ) and reading/writing larger blocks on HDFS ( dfs.block.size ). These experiments show that the response time is quite linear for very high number of draws. So Hadoop bad performance is mainly due to a higher overhead compared to GridGain. In order to be able to clearly see the gain between GridGain and these different tests I constructed an indicator with a “Throughput  dimension” in the sense of dimensional analysis: It allows us to compare the relative performance of GridGain and of the different implementations on Hadoop in a first order of magnitude. The represented runs are the same as above but the scale of the vertical axis is linear in order to see the gain more clearly. This graph shows that on one laptop, Hadoop has a too high overhead in order to be competitive for less than 1,000,000 draws. Yet, for higher number of draws the relative performance is better than GridGain writing intermediate data to disk. A peak – a maximum throughput – appears on the lines. I guess it is due to I/O limitations as tuning that point moves it to the right (The peak is reached for a higher number of draws for Hadoop with reduce optimized). Finally a fully distributed test has been run. The results were correct up to 10,000,000 draws but the performance was bad, at maximum 1.3 better than with one PC. The experiment conditions were not very good, in particular disk size constraint on the other PC leads to very bad blocks repartition. Yet, it remains that the distribution for the VAR calculation of one scenario does not scale well to multiple PCs . The number of writes is very important leading to high number of blocks transfers. And the implementation with only one reduce phase needs to process all the data for reduce on a single machine. Distribution of independent scenarios should be much more efficient because the reduce phase can be distributed. But I didn’t verify it with a performance run. Multiple other optimizations have been tested during investigations but with few or no gain at all. The less bad one was the binary comparator described in the 4th. article . It was tested with 100,000 scenarios of 1,000 draws which is the most favorable case – reading only the scenario key compares 99.9% of the results. Binary comparator allows computing 1.19x quicker in that particular example. Conclusion Hadoop is able to perform distributed Value At Risk calculation. It cannot compete directly with tools like GridGain as it has been designed to process large volumes of data on disk . However, in the case of VAR calculation with further analysis of all intermediate results, it does indeed provide a better framework . First, optimized file system distribution and job distribution in a colocalized way provides out of the box a very good scalability (up to 1,000,000,000 draws, 15.4 GB of compressed binary data on a single laptop). Then, being able to do BI directly on the file system removes a lot of transfer costs for large volume of data . From a developer point of view, Hadoop implements the same map/reduce pattern as GridGain. However, code has to be designed with Hadoop distribution mechanism to be really efficient. Performing financial computing on Hadoop can still be considered today as an R&D subject as the tools are still very young. Distribution for computer intensive tasks is used for about 10 years in Corporate and Investment banks. Benefiting from distribution on storages has allowed big internet actors to process huge volumes of data due to reasonable storage and processing costs that were not affordable with big and monolithic systems. Different initiatives around distributed storage and data processing – with the NoSQL movement – show that more and more integrated tools are currently in development. Such architectures can both help solving particular problems that do not fit very well with traditional approach or allow new data analysis in use cases where processing TB of data was a limitation with traditional architectures. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Hadoop , HDFS , Hive , Java , Map/Reduce , NoSQL , VAR . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-09"},
{"website": "Octo", "title": "\n                Event Sourcing & noSQL            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/event-sourcing-nosql/", "abstract": "Event Sourcing & noSQL Publication date 02/10/2010 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 I saw the talks of Greg Young about CQRS & especially “Event Sourcing” a couple of times and each time, I really really tell myself this pattern is just “génial” (the way we say it in french) even if Martin Fowler wrote about it in 2005 and deals in details with implementation concerns and issues (especially in the cases of integration with external systems). Event Sourcing : stop thinking of your datas as a stock but rather as a list of events… When you look at your database, you look at a stock. For instance, you have X millions of clients, each of them have personal datas like maiden name or address. When you look at these clients, you cannot understand the history of each client. You can just have the current state. The main idea (or mindset shift) behind Event Sourcing is to look at your database not as a stock but as a series of events that can be used to build the current stock. This is more obvious if you think of your bank account. The stock is your balance : how much money you have. The events are all the operations (credit or debit) that occured and “created” the balance. So with this perspective, some of our systems can be viewed like this. Instead of having one stock, you have one event log (typically for write queries) and one or several stocks (typically optimized for read queries or providing different views of your datas ). There are also a couple of important things in “Event Sourcing” : – the messages that are broadcasted into the “events dispatcher” (and so are used to build the stock) must be exactly the same than the events stored in the Event Log. If not, you will loose the event replay capabilities of the system. A tricky point would certainly be to detect errors and know you need to replay the logs… – the longer your system will live, the more events you will have and a complete rebuild will certainly become impossible or at least too long. You will so have to find a way to build trusted snapshot of your stocks and thus clean, periodically, the event log. Another tricky point. Event Sourcing tells nothing about the storage layer of the stock(s) . You can of course use a relational database and inherit the power of SQL language. You could also use a simple cache if it fills your needs. Event Sourcing tells nothing about the Events Dispatcher . Maybe you just want it to be a coding pattern and your Events log and stock will be different tables from the same database. Maybe you will have to introduce a kind of MOM (typically like OpenAMQ ) and have different databases. Maybe you will want to add CEP to your Events Dispatcher and so being able to generate more complex events… At last, Event Sourcing tells nothing about the “Event Log” storage and you can of course use a relational database. But imagine a system where you need,  for write queries, a high available, fault-tolerant system with a high throughput. In that case, noSQL could be a great solutions. You just store the events as key/value pairs… And “Event Sourcing” could resolve the limited querying capabilities of most of the noSQL solutions by querying the stock. Then of course this is not for free and the systems will be more complex : more code to write, more storage to manage, and event mechanism to monitor…But you can have, if you need it, the best of the two worlds : high throughput and availability for write queries, (powerful) SQL language for read queries. In brief, I guess we are entering the BASE World … Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Event Sourcing , NoSQL . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Event Sourcing & noSQL” Olivier Mallassi 24/02/2011 à 14:25 ce papier va plus loin \r\n\"Building on Quicksand\" http://arxiv.org/ftp/arxiv/papers/0909/0909.1788.pdf\r\n\r\npar Pat Helland et Dave Campbell Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-10-02"},
{"website": "Octo", "title": "\n                Kanban Class with David Anderson October 27-28            ", "author": ["Benoit Guillou"], "link": "https://blog.octo.com/en/kanban-class-with-david-anderson-october-27-28/", "abstract": "Kanban Class with David Anderson October 27-28 Publication date 03/09/2010 by Benoit Guillou Tweet Share 0 +1 LinkedIn 0 OCTO and David Anderson are pleased to announce a Kanban workshop on October 27th and 28th . This intensive 2-day Kanban workshop with the Kanban pioneer provides an introduction to Lean, Pull Systems and Kanban and will explain how established industrial engineering theory can apply to software development process. Description Participants in the workshop will learn how to use the simple process of limiting work-in-progress as a driver of change. Kanban is a change management method and a different approach to striking agreements between IT and the business. You’ll learn how to define the policies that constrain the collaborative game of software development. You’ll learn how to use those policies to manage risk and to reset negotiations and recast them as collaborative problem solving. Used effectively, Kanban will change you and your organization. If your workplace has been stagnating and you are looking for new ideas to unleash productivity, innovation, collaboration and creativity, take 2 days and come along. What you will learn ? An introduction to Lean, Pull Systems and Kanban 4 areas of focus to deliver success Value Stream Mapping Process Flow Tracking Implementing different classes of service Implementing a culture of continuous improvement (Kaizen) Applying established industrial engineering theory to software development process Controlling Work In Progress Identifying, classifying and managing bottlenecks Defining release and input cadence for a kanban system Using Metrics and Reporting to drive continuous improvement Establishing policies to prevent abuse and gaming of the kanban system How you will learn ? Case studies Work group exercices on real examples Sharing experiments with one of the Kanban community leaders Is this for you ? If you are a software development executive, project manager, development manager, project lead or developer and you would like to learn how Lean, Pull Systems and Kanban can provide a useful perspective to consider the entire value chain beyond the pure software development, this Kanban workshop is for you! About the presenter David has been part of the agile and lean methodology movement since 1997 when he participated in the team that developed Feature Driven Development at United Overseas Bank in Singapore. He has 26 years experience in the software development starting in the computer games business in the early 1980’s. As a pioneer in the agile software movement David has managed teams at Sprint, Motorola and Corbis delivering superior productivity and quality. At Microsoft, in 2005, he developed the MSF for CMMI Process Improvement methodology – the first agile method to provide a comprehensive mapping to the Capability and Maturity Model Integration (CMMI) from the Software Engineering Institute (SEI). His first book, Agile Management for Software Engineering – Applying the Theory of Constraints for Business Results, published in 2003 by Prentice Hall, introduced many ideas from Lean and Theory of Constraints in to software engineering. His new book, KANBAN, Successful Evolutionary Change For Your Technology Business, explains why and shows you how to get started using it right now. David was a founder of the APLN, a not for profit dedicated to promoting better standards of leadership and management in the technology sector. David is a popular and entertaining conference speaker. He has written or co-authored many articles and papers and is best known for his Agile Management blog. Most recently, David co-authored a Technical Note from the Software Engineering Institute titled, CMMI and Agile: Why not embrace both! Schedule Registration: 8:30-9am Workshop: 9am – 5pm Location 50, Avenue des Champs-Elysées, Paris, France Cost 1500€ for the 2 days workshop. Registration Please download one of those forms : French form – Formulaire Français Kanban Class Details Contact, Information formation@octo.com Other Kanban Resources Scrum sans itérations MMF ou incrément Université du Système d’information : Kanban in operations Kanban external resources David Anderson’s web site Kanban vs Scrum by Henrik Kniberg Kanban Software Engineering website Tweet Share 0 +1 LinkedIn 0 This entry was posted in General -- DO NOT USE . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-09-03"},
{"website": "Octo", "title": "\n                Using GridGain for Value at Risk calculation            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/using-gridgain-for-value-at-risk-calculation/", "abstract": "Using GridGain for Value at Risk calculation Publication date 06/10/2010 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 After a first published article introducing the Value at Risk interest and calculation on a Grid, we will now explore the practical implementation by using a grid computing middleware. I have chosen GridGain , an open source grid middleware which implements the map/reduce pattern (see previous article ).  Firstly, I will give an overview of the Value at Risk implementation independently of the grid computing architecture. Then I will describe the GridGain middleware and the classes that have to be implemented to take advantage of the grid. Finally, I will provide some performance figures from my laptop and analyze how we can improve them. Some portions of the code were removed for readability reason and replaced by comments ( //... ). The following class diagram gives an overview of the different classes: OptionPricer , Parameters and ParametersValueGenerator and VarComputerMaanger are dedicated to the Value at Risk calculation. VarComputerGridRunner and VarComputerGridTask are the classes implemented to take advantage of the grid. Configuration is shared between all layers. Value at Risk implementation This simple class computes the option prices according to the given parameters. It is strictly the implementation of the Black and Scholes expression . Mathematical functions like NormalDistribution are provided by Apache commons Math . public class OptionPricer implements Serializable {\r\n\tprivate static final long serialVersionUID = 5294557359966059686L;\r\n\tprivate static int NUMBER_OF_TRADE_DAY = 252;\r\n\t\r\n\tpublic static class Parameters implements Serializable {\r\n\t\t/**  For Serialization\t */\r\n\t\tprivate static final long serialVersionUID = -3967053376944712365L;\r\n\t\t/** Time in days before the maturity\t */\r\n\t\tprivate int t;\r\n\t\t/** Time in year before the maturity  */\r\n\t\tprivate double tInYear;\r\n\t\t/** Spot (actual price) of the underlying  */\r\n\t\tprivate double s0;\r\n\t\t/**  Strike (target price) of the underlying  */\r\n\t\tprivate double k;\r\n\t\t/**  Risk free interest rate  */\r\n\t\tprivate double r;\r\n\t\t/**  Implicit volatility of the Equity\t */\r\n\t\tprivate double sigma;\r\n\t\t\r\n\t\tpublic Parameters(int t, double s0, double k, double r, double sigma) {//...\t}\r\n\t\tpublic Parameters(final Parameters parameters) { //...\t}\r\n\t\t\r\n\t\t//Getters and Setters...\r\n\t}//End Parameters class\r\n\t\r\n\t /** Initialize only one time and reuse the distribution\t */\r\n\tprivate NormalDistribution normalDistribution = new NormalDistributionImpl(0, 1);\r\n\tprivate final Parameters initialParameters;\r\n\tprivate Parameters parameters;\r\n\t\r\n\tpublic OptionPricer(final Parameters intialParameters) {\r\n\t\tthis.initialParameters = intialParameters;\r\n\t\tthis.parameters = new Parameters(this.initialParameters);\r\n\t}\r\n\t//Getter and Setter both for initialParameters and parameters...\r\n\r\n\t/**\r\n\t * d_1 = \\frac{1}{\\sigma\\sqrt{t}} \\left[ \\ln \\left( \\frac{S_0}{K} \\right) + \\left( r + \\frac{1}{2}\\sigma^2 \\right)t \\right]\r\n\t * \r\n\t * @param spot actualized by a the yield for this simulation\r\n\t */\r\n\tprivate double computeD1(final double s0) {\r\n\t\t//denominator\r\n\t\tdouble denom = parameters.sigma * Math.sqrt(parameters.getTInYear());\r\n\t\t//numerator\r\n\t\t//I don't explain - 0.5 * sigma^2 but in the Excel sheet it is like that\r\n\t\tdouble num = Math.log(s0 / parameters.k) + (parameters.r - 0.5 * parameters.sigma * parameters.sigma) * parameters.getTInYear(); \r\n\t\treturn num / denom;\r\n\t}\r\n\t\r\n\t/**\r\n\t * d_2 = d_1 - \\sigma \\sqrt{t}\r\n\t * @param d1\r\n\t * @return\r\n\t */\r\n\tprivate double computeD2(final double d1) {\r\n\t\tdouble result = d1 - parameters.sigma * Math.sqrt(parameters.getTInYear()); \r\n\t\treturn result;\r\n\t}\r\n\r\n\t/**\r\n\t * Compute the price of the call\r\n\t * C(S_0,K,r,t,\\sigma) = S_0 \\mathcal{N}(d_1) - K e^{-rt}\\mathcal{N}(d_2)\r\n\t * \\mathcal{N}(x) = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}u^2} du\r\n\t * \r\n\t * @return\r\n\t * @throws MathException \r\n\t * \r\n\t * @param spot yield Allows to build a actual yield for each simulation\r\n\t */\r\n\tpublic double computeCall() throws MathException {\r\n\t\tfinal double s0 = parameters.s0;\r\n\t\tfinal double d1 = computeD1(s0);\r\n\t\tfinal double d2 = computeD2(d1);\r\n\t\tdouble result = s0 * normalDistribution.cumulativeProbability(d1) - parameters.k * Math.exp(-1 * parameters.r * parameters.getTInYear()) * normalDistribution.cumulativeProbability(d2);\r\n\t\treturn result;\r\n\t}\r\n} Two interesting points can be highlighted. First, for performance reason, I chose to use float point numbers. In order to prevent falling into some pitfalls , some precautions should be taken. No rounding or other non linear function is performed during the calculation. Then all divisions are delayed till the end of the calculation so as to not lower the precision. Then I define both an initialParameters field and a parameter reference for a performance reason which I will explain with the help of the following class. public class ParametersValueGenerator implements Serializable {\r\n\tprivate static final long serialVersionUID = 2923455335542820656L;\r\n\tprivate final double underlyingYieldHistoricalVolatility;\r\n\tprivate final double interestRateVariationVolatility;\r\n\tprivate final double implicitVolatilityVolatility;\r\n\t\r\n\t/** Used to generate the raws */\r\n\tprivate RandomData randomGenerator = new RandomDataImpl();\r\n\t/** Reference to OptionPricer initialParameters */\r\n\tprivate OptionPricer.Parameters initialParameters;\r\n\r\n\t/**\r\n\t * @param underlyingYieldHistoricalVolatility\r\n\t *            Volatity of the yield of the undelying price (it is the\r\n\t *            standard deviation for the normal distribution)\r\n\t * @param interestRateVariationVolatility\r\n\t *            Volatility of the risk free interest rate (it is the\r\n\t *            standard deviation for the normal distribution)\r\n\t * @param implicitVolatilityVolatility\r\n\t *            Volatility of the underlying volatility (we probably take\r\n\t *            the standard deviation of an index like VIX\r\n\t * @see http://en.wikipedia.org/wiki/VIX\r\n\t */\r\n\tpublic ParametersValueGenerator(\r\n\t\t\tdouble underlyingYieldHistoricalVolatility,\r\n\t\t\tdouble interestRateVariationVolatility,\r\n\t\t\tdouble implicitVolatilityVolatility) {\r\n\t\tthis.underlyingYieldHistoricalVolatility = underlyingYieldHistoricalVolatility;\r\n\t\tthis.interestRateVariationVolatility = interestRateVariationVolatility;\r\n\t\tthis.implicitVolatilityVolatility = implicitVolatilityVolatility;\r\n\t}\r\n\r\n        /**\r\n\t    Reseed with a value that is more likely to be unique\r\n\t\tThis is mandatory to avoid generating two identical random number series\r\n\t\tTo be really sure to not have the same value in two different thread or computer, \r\n\t\tI use a secure random generator instead of the current millisecond\r\n\t\t\r\n\t\tThis method is time consumming, use it a minimum number of times\r\n\t */\r\n\tpublic void intializeRandomGenerator() {\r\n\t\trandomGenerator.reSeed(securedRandomGenerator.nextLong());\r\n\t}\r\n\t\r\n\t/**\r\n\t * Set the reference\r\n\t * @param initialParameters\r\n\t * @param parameters\r\n\t */\r\n\tpublic void initializeReference(final OptionPricer.Parameters initialParameters) {\r\n\t\tthis.initialParameters = initialParameters;\r\n\t}\r\n\r\n\t/**\r\n\t * Put random generated values into parameters instance by modifying it\r\n\t * by reference\r\n\t * \r\n\t * @param parameters\r\n\t */\r\n\tpublic void setNewGeneratedParameters(OptionPricer.Parameters parameters) {\r\n\t\tif(parameters == null) { throw new IllegalArgumentException(\"parameters should not be null\"); }\r\n\t\tif(this.initialParameters == null) { throw new IllegalStateException(\"this.initialParameters should not be null\"); }\r\n\t\t\r\n\t\tif(this.underlyingYieldHistoricalVolatility != 0) {\r\n\t\t\tparameters.setS0(this.initialParameters.getS0() * (1+randomGenerator\r\n\t\t\t\t.nextGaussian(0, this.underlyingYieldHistoricalVolatility)));\r\n\t\t}\r\n\t\tif(this.interestRateVariationVolatility != 0) {\r\n\t\t\tparameters.setR(this.initialParameters.getR() + randomGenerator\r\n\t\t\t\t\t.nextGaussian(0, this.interestRateVariationVolatility));\r\n\t\t}\r\n\t\tif(this.implicitVolatilityVolatility != 0) {\r\n\t\t\tparameters.setSigma(randomGenerator\r\n\t\t\t\t\t.nextGaussian(this.initialParameters.getSigma(),\r\n\t\t\t\t\t\t\tthis.implicitVolatilityVolatility));\r\n\t\t}\r\n\t}\r\n\t//toString() method...\r\n} This class generates random values for the Monte-Carlo method . This simple implementation generates data according to a normal distribution. One interesting thing to notice, from a financial point of view, is the choice of a normal distribution for modeling different values: the yield of the strike (leading to value=initial*(1+nextGaussian()) ) the delta of the interest rate variation ( value=initial+nextGaussian() ) the implicit volatility value ( value=nextGaussian() ) The setNewGeneratedParameters() method modifies a reference to the parameters reference of the OptionPricer by using a reference to the initialParameters . The initializeReference() method allows to make that reference point to the initialParameters of the OptionPricer . It would have been much easier to return a new Parameters instance for each simulation, but some measures with JVisualVM shows that the Parameters constructor was one of the most consuming method. Indeed, allocating memory on the heap requires looking for the next slot with sufficient memory. This is a costly operation. Passing each generated value to the OptionPricer would probably have been more efficient. But this handling of references seems to be a good compromise between performance and maintainability. Another interesting point to mention is the intializeRandomGenerator () method. If you do not have it, you will have a strange behavior which could lead to hours of headache. I will describe you the problem and the solution afterwards but before doing so I need to give you some implementation details. The GridGain tool and the Map/Reduce implementation This tool brings some functionality allowing executing the code on a grid without coding low level logic. Among the “out of the box” provided functionalities, we can mention: Automatic discovery of available nodes Deployment of code and data on each node Load balancing Failover if a node fails or is no more reachable Concretely, the default implementation of GridGain is very easy to start in a development environment. The only prerequisite is that GridGain is deployed and executed on each node. The written application starts an embedded node. After the auto-discovery, the grid is built as shown in the console (in that case 4 processes on a quad core processor): >>> -------------------\r\n>>> Discovery Snapshot.\r\n>>> -------------------\r\n>>> Number of nodes: 4\r\n>>> Topology hash: 0xAEFB9485\r\n>>> Local: CD766E36-2A09-45B5-895F-860F5DFBA386, 10.1.106.135, Windows 7 x86 6.1\r\n, mbo, Java(TM) SE Runtime Environment 1.6.0_20-b02\r\n>>> Remote: B5278A02-CD63-4E7E-89FF-BCA791E9045E, 10.1.106.135, Windows 7 x86 6.\r\n1, mbo, Java(TM) SE Runtime Environment 1.6.0_20-b02\r\n>>> Remote: A4201237-6838-44A3-8B32-3E8B5B1F2FFF, 10.1.106.135, Windows 7 x86 6.\r\n1, mbo, Java(TM) SE Runtime Environment 1.6.0_20-b02\r\n>>> Remote: C9BCB5C8-3C87-4106-9E89-8ABF11FEC959, 10.1.106.135, Windows 7 x86 6.\r\n1, mbo, Java(TM) SE Runtime Environment 1.6.0_20-b02\r\n>>> Total number of CPUs: 4 The code is deployed automatically on each node and run. The map/reduce pattern GridGain implements the Map/Reduce pattern. This pattern is required to be able to distribute the calculation. In brief, this pattern comes from functional programming. The calculation is broken down into the repetition of the same small calculation on a big number of data (map function). Each small calculation can be distributed. Then the results are merged (reduce function). An in depth description with a sample has been provided in my previous article . The map/reduce pattern implementation In this particular case, the code is organized that way. – The VarComputerManager class implements the VAR calculation logic in the computeVar() method. It is not tied to the grid and implements the business logic. To be exact, this method does not return one value but all the values lower than the VAR. See my previous article for more information of the business meaning of the VAR and the calculation algorithm. public class VarComputerManager {\r\n\t/**\r\n\t * Compute the VAR of the option defined in the option pricer\r\n\t * @param optionPricer Instance should be instanciated with the correct parameters\r\n\t * @param valueGenerator Instance should be instanciated with the volatilities, and the reference to the default parameters should have been set \r\n\t * @param drawsNb Number of draws produced on that node\r\n\t * @param totalDrawsNb Total number of draws made on the whole grid\r\n\t * @param varPrecision Between 0 and 1. 0.99 gives the VAR (the percentile) to 1%\r\n\t * @return The n lowest prices generated on that node where n = (1-varPrecision)*totalDrawsNb\r\n\t * @throws MathException\r\n\t */\r\n\tpublic SortedSet<double> computeVar(final OptionPricer optionPricer,\r\n\t\t\tfinal ParametersValueGenerator valueGenerator, final int drawsNb, final int totalDrawsNb,\r\n\t\t\tfinal double varPrecision, final Configuration configuration) throws MathException {\r\n\t\tfinal double nbValInPercentile = (totalDrawsNb * (1 - varPrecision));\r\n\t\tfinal SortedSet</double><double> smallerPrices = new TreeSet</double><double>();\r\n\t\t\r\n\t\tvalueGenerator.intializeRandomGenerator();\r\n\r\n\t\tfor (int i = 0; i < drawsNb; i++) {\r\n\t\t\tvalueGenerator.setNewGeneratedParameters(optionPricer.getParameters());\r\n\t\t\tfinal double price = optionPricer.computeCall();\r\n\r\n\t\t\t// For each draw, put the price in the sorted set\r\n\t\t\tsmallerPrices.add(price);\r\n\t\t\tif(configuration.isCombineAllowed()) {\r\n\t\t\t\t// If the set exceeds nbValInPercentile, remove the highest value\r\n\t\t\t\tif (smallerPrices.size() > nbValInPercentile) {\r\n\t\t\t\t\tsmallerPrices.remove(smallerPrices.last());\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\treturn smallerPrices;\r\n\t}\r\n\t\r\n\t/**\r\n\t * \r\n\t * @param smallerPrices Modified by reference\r\n\t * @param drawsNb\r\n\t * @param varPrecision\r\n\t */\r\n\tpublic void extractPercentile(SortedSet</double><double> smallerPrices, final int drawsNb, final double varPrecision) {\r\n\t\tfinal double nbValInPercentile = (drawsNb * (1 - varPrecision));\r\n\t\t// If the set exceeds nbValInPercentile, remove the highest value\r\n\t\twhile (smallerPrices.size() > nbValInPercentile) {\r\n\t\t\tsmallerPrices.remove(smallerPrices.last());\r\n\t\t}\r\n\t}\r\n}\r\n</double> A defined number of draws of random data is executed by the valueGenerator.setNewGeneratedParameters(optionPricer.getParameters()); and the call price is computed for each draw. The lowest values are identified. This example code remains simple by using a SortedSet . One drawback is that if the same VAR is generated by two combination of parameters, the second value will be discarded ( add() is an optional operation). In practice, I got this behavior in 0.02% of the draws leading to a potential lower VAR value than expected. For the purpose of that example, I chose to keep that imprecision. – Then the VarComputerGridTask class implements the technical logic to deploy the code on the grid. The split() function is a synonym for map() of the map/reduce pattern. It defines how to break down computeVar() into smaller functions. public class VarComputerGridTask extends GridTaskSplitAdapter<gridifyargument , SortedSet<Double>> {\r\n\tprivate static final long serialVersionUID = 2999339579067614574L;\r\n\tprivate static String SESSION_CONF_KEY = \"SESSION_CONF_KEY\";\r\n\tprivate static Logger log = LoggerFactory.getLogger(VarComputerGridTask.class);\r\n\t@GridTaskSessionResource\r\n    private GridTaskSession session = null;\r\n\r\n\t@Override\r\n\tprotected Collection< ? extends GridJob> split(int gridSize, final GridifyArgument arg)\r\n\t\t\tthrows GridException {\r\n\t\t// Split number of iterations.\r\n        Integer iterations = ((Integer)arg.getMethodParameters()[2]);\r\n       \r\n        // Note that for the purpose of this example we perform a simple homogeneous\r\n        // (non weighted) split on each CPU/Computer assuming that all computing resources \r\n        // in this split will be identical. \r\n        int slotsNb = 0;\r\n        //Retrieve the session parameter\r\n        Object[] params = arg.getMethodParameters();\r\n        Configuration conf = (Configuration)params[5];\r\n        //Put it into session to retrieve it in reduce task\r\n        session.setAttribute(SESSION_CONF_KEY, conf);\r\n        if(conf.isThereSeveralSlotsByProcess()) {\r\n        \tGrid grid = GridFactory.getGrid();\r\n            for(GridNode node : grid.getAllNodes()) {\r\n            \tlog.debug(\"Node {} detected with {} cores\", node.getPhysicalAddress(), node.getMetrics().getAvailableProcessors());\r\n            \t//1 process per node is presumed. If several processes are running on the machine, the number of cores will be over estimated\r\n            \tslotsNb += node.getMetrics().getAvailableProcessors();\r\n            }\r\n            log.debug(\"{} slots available\", slotsNb);\r\n        }\r\n        else {\r\n        \tslotsNb = gridSize;\r\n        \tlog.debug(\"1 slot by process so {} slots\", gridSize);\r\n        }\r\n\r\n        // Number of iterations should be done by each slot.\r\n        int iterPerNode = Math.round(iterations / (float)slotsNb);\r\n\r\n        // Number of iterations for the last/the only slot.\r\n        int lastNodeIter = iterations - (slotsNb - 1) * iterPerNode;\r\n\r\n        List<gridjobadapter <Integer>> jobs = new ArrayList</gridjobadapter><gridjobadapter <Integer>>(gridSize);\r\n        \r\n        for (int i = 0; i < slotsNb; i++) {\r\n            // Add new job reference to the split.\r\n            jobs.add(new GridJobAdapter<Integer>(i == slotsNb - 1 ? lastNodeIter : iterPerNode) {\r\n\t\t\t\tprivate static final long serialVersionUID = 1L;\r\n\r\n\t\t\t\t/*\r\n                 * Executes grid-enabled method with passed in argument.\r\n                 */\r\n                public TreeSet<double> execute() throws GridException {\r\n                    Object[] params = arg.getMethodParameters();\r\n                    TreeSet</double><double> result = null;                    \r\n\r\n                    VarComputerManager varComputerManager = new VarComputerManager();\r\n                    try {\r\n                    \tresult = (TreeSet</double><double>)varComputerManager.computeVar((OptionPricer)params[0],\r\n\t\t\t\t\t\t\t\t(ParametersValueGenerator)params[1], \r\n\t\t\t\t\t\t\t\tgetArgument(), //Give the split size on that node\r\n\t\t\t\t\t\t\t\t(Integer)params[3],//Give the total size on the grid\r\n\t\t\t\t\t\t\t\t(Double)params[4],\r\n\t\t\t\t\t\t\t\t(Configuration)params[5]);\r\n\t\t\t\t\t} catch (MathException e) {\r\n\t\t\t\t\t\tlog.error(\"Error computing option pricing\", e);\r\n\t\t\t\t\t\tresult = new TreeSet</double><double>();\r\n\t\t\t\t\t}\r\n\t\t\t\t\t\r\n\t\t\t\t\tlog.debug(\"{}\", String.format(\"SlotId %d: Map %d elements on %d and produce %d results\", this.hashCode(), getArgument(), (Integer)params[2], result.size()));\r\n\t\t\t\t\treturn result;\r\n                }\r\n            });//End GridJobAdapter\r\n        }//End for\r\n        \r\n\t\treturn jobs;\r\n\t}\r\n</double></gridjobadapter></gridifyargument> The split() method receives as arguments the size of the grid and a GridArgument object containing the arguments of the VarComputerManager.computeVar() method seen above. The function determines the number of available slots according to the number of processors. As our tasks are computer-intensive, the most efficient repartition is one slot per core. Lets say 1,000 draws can be broken down into 10 tasks applying computeVar() of 100 draws each. For GridGain, a task is represented by a GridJobAdapter instance. In order to be able to measure the influence of different strategies (e.g. one thread but multiple processes or one process and multiple threads per machine), some points have been specified in a configuration object. The GridTaskSession makes it available to each map() and reduce() method. One other tip is to notice that computeVar() has two close but different arguments. The third one – drawsNb – should be equal to 100 on each node but the fourth one – totalDrawsNb – should be equal to 1,000. totalDrawsNb with varPrecision to define the number of values to return: nbValInPercentile = (totalDrawsNb * (1 - varPrecision)); . As described in my previous article , it allows each node to return only the 10 lowest prices resulting from their 100 draws. This performance optimization reduces the values sent through the network guarantying as well that these 10×100 values contain the 10 lowest values of the total 1,000 draws. – The reduce() function merges all the results. @SuppressWarnings(\"unchecked\")\r\n@Override\r\npublic SortedSet<double> reduce(List<gridjobresult> results) throws GridException {\r\n\t\tint nbValInPercentile = 0;\r\n\t\tfinal SortedSet<double> smallerPrices = new TreeSet</double><double>();\r\n\r\n\t\t// Reduce results by summing them.\r\n\t\tfor (GridJobResult res : results) {\r\n\t\t\tSortedSet</double><double> taskResult = (SortedSet</double><double>)res.getData();\r\n\t\t\tlog.debug(\"Add {} results\", taskResult.size());\r\n\t\t\tsmallerPrices.addAll(taskResult);\r\n\t\t\tConfiguration conf = (Configuration)session.getAttribute(SESSION_CONF_KEY);\r\n\t\t\tif(conf.isCombineAllowed()) {\r\n\t\t\t//Hypothesis is made that the map task already send only the required values for percentile\r\n\t\t\t\tif(nbValInPercentile == 0) {\r\n\t\t\t\t\tnbValInPercentile = taskResult.size();\r\n\t\t\t\t}\r\n\t\t\t\telse if(nbValInPercentile != taskResult.size()) {\r\n\t\t\t\t\tthrow new IllegalStateException(\"All set should have the same number of elements. Expected \" + nbValInPercentile + \", got \" + taskResult.size());\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t\twhile (smallerPrices.size() > nbValInPercentile) {\r\n\t\t\t\t\tsmallerPrices.remove(smallerPrices.last());\r\n\t\t\t\t}\r\n\t\t\t}//End if\r\n\t\t}\t\t\t\r\n\t\tlog.debug(\"Reduce {} jobs and produce {} results\", results.size(), smallerPrices.size());\r\n\t\treturn smallerPrices;\t        \t        \r\n}\r\n</double></gridjobresult></double> In our case, the reduce task looks for the 10 smallest values in the 100×10 smallest values sent by each node. Now lets go back to the parametersValueGenerator.intializeRandomGenerator() method. This paragraph is a bit technical an can be safely ignored by readers who want quickly know how to start the grid Random number generation issue When you increase the number of draws, the precision should increase.  During my tests, with only one thread, the value was converging normally to 0.19 . But with several nodes, the convergence was slower or I got some surprising values even with a high number of draws. Finally, after padding logs everywhere with different parameters I got log containing such lines: 14:21:53.635 [gridgain-#17%null%] DEBUG c.o.r.v.gridgain.VarComputerManager - SO list 125.53733975440497;122.81494382362781;91.27631322995818;108.45186092917872;126.38622145310778;\r\n14:21:53.635 [gridgain-#4%null%] DEBUG c.o.r.v.gridgain.VarComputerManager - SO list 130.49461547615613;113.37411633772678;84.84851329812945;97.68916453583668;126.54608432355151;\r\n14:21:53.635 [gridgain-#6%null%] DEBUG c.o.r.v.gridgain.VarComputerManager - SO list 130.49461547615613;113.37411633772678;84.84851329812945;97.68916453583668;126.54608432355151;\r\n14:21:53.636 [gridgain-#7%null%] DEBUG c.o.r.v.gridgain.VarComputerManager - SO list 130.49461547615613;113.37411633772678;84.84851329812945;97.68916453583668;126.54608432355151; SO list contains my underlying strike generated values. You can notice that some series are totally identical. The union of these different sets is certainly not a normal distribution as initially expected. Due to the algorithm used and consequently to this, several nodes were sending identical results. And as you may have noticed, these results are aggregated by the SortedSet.addAll() method. Look if required at the JavaDoc of the method and you will discover that elements are optional if they are not already present. So, to summarize, N nodes with total identical values are equivalent to one unique node. No doubt possible, the results are totally wrong. But why? To understand it, we need to know how commons math RandomDataImpl works. The javadoc gives us a clue by saying “so that reseeding with a specific value always results in the same subsequent random sequence” . The default implementation I used is based on the Random class of the JDK. The algorithm used to generate random values is called Linear congruential generator .  To summarize, it is a mathematical series that, given an initial value called seed , builds a list of numbers that have the properties of a normal distribution. To give a practical image, it is close to – even though very different in mathematical properties – to the Mandelbrot set where a mathematical formula can lead to something random, chaotic. (Source http://en.wikipedia.org/wiki/Mandelbrot_set#Image_gallery_of_a_zoom_sequence ) However, if you give twice the same seed to a generator, it will generate the same list of values. Writing the following initializeRandomGenerator function public void intializeRandomGenerator() {\r\n\trandomGenerator.reSeed(2923455335542820656L);\r\n} leads to that kind of log: all lists of generated values are identical. 14:51:28.781 [gridgain-#10%null%] DEBUG c.o.r.v.gridgain.VarComputerManager - SO list 105.61456308819832;105.61456308819832;105.61456308819832;105.61456308819832;105.61456308819832;\r\n14:51:28.781 [gridgain-#14%null%] DEBUG c.o.r.v.gridgain.VarComputerManager - SO list 105.61456308819832;105.61456308819832;105.61456308819832;105.61456308819832;105.61456308819832;\r\n14:51:28.781 [gridgain-#12%null%] DEBUG c.o.r.v.gridgain.VarComputerManager - SO list 105.61456308819832;105.61456308819832;105.61456308819832;105.61456308819832;105.61456308819832;\r\n14:51:28.781 [gridgain-#13%null%] DEBUG c.o.r.v.gridgain.VarComputerManager - SO list 105.61456308819832;105.61456308819832;105.61456308819832;105.61456308819832;105.61456308819832; By default, the seed used by Random class is the current timestamp. I have tried to use a mix of the thread identifier and System.nanoTime() but I still got some strange behavior. The easiest fix was to define this initializedRandomGenerator . private SecureRandom securedRandomGenerator = new SecureRandom();\r\n//...\r\npublic void intializeRandomGenerator() {\r\n\t\trandomGenerator.reSeed(securedRandomGenerator.nextLong());\r\n\t} SecureRandom is designed for cryptographic scenario and uses a different implementation that guarantees better randomness. But in counterparty, it is much slower than the standard Random class. So this method initializeRandomGenerator() is called once at the beginning of the computeVar() method to provide the seed, then the standard class is used. This solution fixes the problem. However, it is only a quick answer for the purpose of this article. I have not checked that the union of the generated random lists corresponds to a normal distribution. Further analysis, comparison with other random generators (see this Wikipedia article ) should be useful for a real world use case. Grid instanciation Finally, after resolving that problem, we can look at the VarComputerGridRunner that instantiates the master node of the grid. public final class VarComputerGridRunner {\r\n\t//...\r\n\t/**\r\n\t * Method create VarEngine and calculate VAR for a single Call\r\n\t * \r\n\t * @param args\r\n\t *            Command line arguments, none required but if provided first\r\n\t *            one should point to the Spring XML configuration file. See\r\n\t *            <tt>\"examples/config/\"</tt> for configuration file examples.\r\n\t * @throws Exception\r\n\t *             If example execution failed.\r\n\t */\r\n\tpublic static void main(String[] args) throws Exception {\r\n\t\tconf = Configuration.parseConfiguration(args);\r\n\t\t// Start GridGain instance with default configuration.\r\n\t\tif (conf.getGridGAinSpringConfigPath() != null) {\r\n\t\t\tGridFactory.start(conf.getGridGAinSpringConfigPath());\r\n\t\t} else {\r\n\t\t\tGridFactory.start();\r\n\t\t}\r\n\r\n\t\tresultsLog.info(\"VAR\\tDrawsNb\\tComputeTime (ms.)\\tParameters\");\r\n\t\t\r\n\t\ttry {\r\n\t\t\t//Compute multiple times, until 2 min. or 10^6 (32 bits heap size limit)\r\n\t\t\tdouble maxTime = System.currentTimeMillis() + 2*60*1000;\r\n\t\t\tfor (int i = 1000; i < 100000001 && System.currentTimeMillis() < maxTime ; i *= 10) {\r\n\t\t\t\tVarComputerManager varComputer = new VarComputerManager();\r\n\t\t\t\tOptionPricer.Parameters params = new OptionPricer.Parameters(\r\n\t\t\t\t\t\t252, 120.0, 120.0, 0.05, 0.2);\r\n\t\t\t\tOptionPricer op = new OptionPricer(params);\r\n\t\t\t\tParametersValueGenerator valueGenerator = new ParametersValueGenerator(\r\n\t\t\t\t\t\t0.15, 0, 0);\r\n\t\t\t\tprintTimeToCompute(varComputer, valueGenerator, op, i);\r\n\t\t\t\tvalueGenerator = new ParametersValueGenerator(0.15, 0.20, 0);\r\n\t\t\t\tprintTimeToCompute(varComputer, valueGenerator, op, i);\r\n\t\t\t\tvalueGenerator = new ParametersValueGenerator(0.15, 0.20, 0.05);\r\n\t\t\t\tprintTimeToCompute(varComputer, valueGenerator, op, i);\r\n\t\t\t}\r\n\r\n\t\t}\r\n\t\t// We specifically don't do any error handling here to\r\n\t\t// simplify the example. Real application may want to\r\n\t\t// add error handling and application specific recovery.\r\n\t\tfinally {\r\n\t\t\t// Stop GridGain instance.\r\n\t\t\tGridFactory.stop(true);\r\n\t\t\tlog.info(\"Test finished\");\r\n\t\t}\r\n\t}\r\n\r\n\tprivate static void printTimeToCompute(\r\n\t\t\tfinal VarComputerManager varComputer,\r\n\t\t\tfinal ParametersValueGenerator parametersValueGenerator,\r\n\t\t\tfinal OptionPricer optionPricer, final int drawsNb)\r\n\t\t\tthrows MathException {\r\n\t\tdouble startTime = System.nanoTime();\r\n\t\tparametersValueGenerator.initializeReference(optionPricer\r\n\t\t\t\t.getInitialParameters());\r\n\r\n\t\tfinal double varPrecision = 0.99;\r\n\t\tSortedSet<Double> smallerPrices = varComputer.computeVar(optionPricer,\r\n\t\t\t\tparametersValueGenerator, drawsNb, drawsNb, varPrecision, conf);\r\n\t\t//If combine is not allowed in Map/Reduce, all results are brought back\r\n\t\t//Extraction of percentile should be done here\r\n\t\tif(!conf.isCombineAllowed()) {\r\n\t\t\tvarComputer.extractPercentile(smallerPrices, drawsNb, varPrecision);\r\n\t\t}\r\n\t\tdouble var = smallerPrices.last();\r\n\t\tdouble endTime = System.nanoTime();\r\n\t\tresultsLog.info(\"{}\", String.format(\"%f\\t%d\\t%f\\t%s\", var,\r\n\t\t\t\tdrawsNb, (endTime - startTime) / 1000000, parametersValueGenerator.toString()));\r\n\t\tlog.info(\"{}\", String.format(\"%f (computed with %d draws in %f ms.) with generators %s\", var,\r\n\t\t\t\tdrawsNb, (endTime - startTime) / 1000000, parametersValueGenerator.toString()));\r\n\t}\r\n} When GridFactory.start() is called, the GridGain middleware starts the master node which looks for other nodes and builds the grid. GridGain default configuration uses multicast for nodes discovery but other implementations are available. Then, when varComputer.computeVar() is called, the GridGain middleware calls transparently the VarComputerGridTask class through Aspect Oriented Programming . The split method breaks down the calculation, produces several calls to VarComputer.computeVar() with different parameters. The middleware sends code and data to the nodes. Each node executes VarComputer.computeVar() . Then the results are sent back to the master node. The master node executes the reduce() function before returning to main() . In my example, the main() function has to First extract the percentile if not done by map/reduce; Then identify the real VAR value: the highest value of the computed set. Some performance figures With that implementation it is now possible to experiment some performance measures. I used my laptop, a DELL Latitude E6510 with a core i7 quad core and 4 GB of RAM. The first graph presents the computation time for one thread. It shows that the compute time is proportional to the number of draws (both axis scale are logarithmic) for more than 100,000 draws. First run with 1,000 draws is a particular case with a huge overhead. It is the result of the discovery mechanism. Then, generating 1,2 or 3 parameters has no visible impact in that simple case. The second graph – for 4 simultaneous threads – confirms the previous points. Gain between 1 thread and 4 threads is not visible on such graphs. The following graph draws that gain in two cases: 4 jobs launched on 4 threads in a single process ( Configuration.isThereSeveralSlotsByProcess=true , VarComputerManager is launched alone) 4 jobs launched on 4 processes ( Configuration.isThereSeveralSlotsByProcess=false , VarComputerManager is launched with 3 other GridGain.bat processes) First, as expected, the gain is higher for a higher number of draws. So distribution should only be used for more than 10,000 draws in our example. Secondly, we can notice that the gain is less than 4: between 2 and 2.5. Several main reasons can be invoked: My Core i7 has two physical cores and Hyper-Threading technology allowing 4 threads. So even though the OS sees 4 processors, only 2 are physically available. Performance gain made available by Hyper-Threading is according to that Wikipedia article 30% and not 100% as we presumed by expecting a total gain of 4. This Intel article goes much more into details but quotes gains up to 70%. So 4 threads should lead to a maximum expected gain between 2×1.3=2.6 and 2×1.7=3.4 . The Intel article specifies that extremely computing efficient applications are more likely to have smaller gains with Hyper -Threading. It is globally the case of that sample application which is computing intensive and with very little memory sharing due to the “combine” optimization – i.e. sending only 1% of the total number of draws to the reduce() function.- Only the map task can be distributed. In that particular algorithm, reduce is time consuming Distribution comes at a cost Third, the gain is slightly higher for threads with few draws and higher for processes with many draws. As the differences are not very marked, it could be due to measure imprecision. If I had to formulate an hypothesis, I would guess that JVM and OS best pin processes to core leading to better cache utilization. Then I used a second laptop from a colleague: a Dell Latitude 34800 with a Core 2 Duo and 2GB RAM. Both were connected on our internal (shared) wire network through a switch. Because of the different number of cores, the default homogeneous repartition of jobs by GridGain leads to 3 jobs on each PC. This behavior can be changed programmatically as shown in that example . To bypass that problem, I launched 6 jobs in 6 processes ( Configuration.isThereSeveralSlotsByProcess=false ). VarComputerManager is launched with 3 other GridGain.bat processes on my laptop 2 GridGain.bat processes are launched on the other laptop The impact on the first launch (1,000 draws) is much more important. The compute time is almost linear for high number of draws. On that graph, logarithmic scales prevent us to see clearly the gains. The graphic below makes them visible: The Core 2 duo has two physical cores so the maximum expected gain is 2×1.3+2=4.6 . The maximum measured gain is about 3 . Several main reasons can be invoked: Distribution on different machines comes at a greater cost than local distribution; reduce() function – not distributed – is higher because more results are built simultaneously by 6 map tasks; Gain has been computed with the 1 thread reference on the Core i7. Comparisons by Intel made on two different versions of Core i7 and Core 2 Duo shows that ratio is about 1.5. So, a rough estimation of the maximum expected gain could be 2×1.3+2×0.66=3.9 which is rather close to 3 . In conclusion, distribution comes at a cost . So distribution should be limited to computing with a high number of draws. Optimizations of that code leads globally to correct performance gain. All previous measures have been made with “combine” option (i.e. sending less data to reduce() ) activated. In order to estimate the benefit of that optimization, take a look at the 2 following graphs. They show the relative gain after activating the “combine” optimization both on 4 threads on local PC and on 6 processes on two PCs. In both cases, using the “combine” option – i.e. sending only 1% of the total number of draws to the reduce() function – brings a gain of 7 . This value is the same in both cases, so we can assume that network bandwidth saving is not the key point here. Measures on my laptop with no “combine” option shows a maximal network throughput of 11.6 MB/s on my 100MB/s network interface. The network interface output queue length remains to 0 confirming that point. So I conclude that reducing the workload of the reduce() function leads to that gain. Conclusion Grid middlewares like GridGain provide valuable functionalities in order to implement VAR computation using Monte-Carlo method through the Map/Reduce pattern. However, despite their efforts, code should be written with a distributed deployment in mind. The random generators should be managed to remain accurate. Transferring configuration or other information requires adapting both the technical code and the interface of the business code. Moreover, optimizations are required to provide good performances. Lowering the work of the Reduce() function leads in my case in a x7 gain. Finally, distribution comes at a cost . This cost is higher between machines than between local processes. The distribution over-head is made profitable for a high number of draws, so the granularity of the distribution should be tuned . This simple example provides relatively good gain due to the simplification made and consequent optimizations. In a real world scenario, intermediate results are frequently stored in order to be further analyzed. Implementing such use case will be the next challenge and the purpose of my next article. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Finance , grid computing , Gridgain , VAR . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 6 commentaires sur “Using GridGain for Value at Risk calculation” Ashish Garg 14/05/2012 à 18:32 ParametersValueGenerator(0.15, 0, 0);  \r\nParametersValueGenerator(0.15, 0.20, 0);  \r\nParametersValueGenerator(0.15, 0.20, 0.05);  \r\nThere is three time initialization for compute with diffrent values of nextGuassian. Why we need it three times. Is there a correlation of what values are passed to nextGuassian is it based on the other parameter of StockValue/Strike Price and maturity? Marc Bojoly 14/05/2012 à 23:33 There is no buiness logic in this three time initialization.  It was just a way to check if there is a difference of compute time if we generate one, two or three random parameters - look at l. 68 in the listing of ParametersValueGenerator class -.\r\nThe differences were minimal so I don't emphasize it in my article. Ashish Garg 17/05/2012 à 20:04 Thanks Marc. Really very good article and it helped in to understand the implemenation details.\r\n\r\nParametersValueGenerator(0.15, 0.20, 0.05);\r\nWhat is the value we should pass for random number generator. Does it depend on StockValue/Strike Price and maturity or other parameter .\r\nNextGaussian takes mean and variance what should be assume these values. Does it depend on StockValue/Strike Price and maturity or other parameter. Marc Bojoly 20/05/2012 à 23:19 ParametersValueGenerator(0.15, 0.20, 0.05);\r\nThis generator of parameters takes volatities as parameters : volatility of price yield, interest rate and implied volatility. Initial values of each of these parameters are provided, then a Gaussian generator with the specified volatility is used to generate different values of these parameters.\r\nThe definition of these parameters are detailed in the first article of the series. Links in that article can be used to further understand the mathematical model.\r\nThe way the Gaussian generator are used, the choice of the initial parameters and volatility should be defined by doing a statistical analysis of the historical values of the price yield, interest rate and implied volatility. Here choice was arbitrary as my only goal was to study performance problems. Akhil S 06/06/2012 à 07:41 Hi Marc,\r\n\r\nIn some charts like MultithreadsGain.png and other after it, you have used the term GAIN. Can you please share the calculation logic for computing it?\r\n\r\nAlso can you tell me what was the bandwidth and hierarchy of the network for the multiple machine testbed?\r\n\r\nThanks.\r\nAkhil Srinivasan Marc Bojoly 09/06/2012 à 15:46 Hi Akhil,\r\n\r\nGAIN is a ratio between compute time for 1 job and compute time for N jobs (more details in the some performance figures paragraph).\r\n\r\nI used on or two laptops connected through a shared line for these measures as described in the same paragraph.\r\n\r\n\r\nRegards,\r\n\r\nMarc Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-10-06"},
{"website": "Octo", "title": "\n                Introduction to Grid Computing for Value At Risk calculation            ", "author": ["Marc Bojoly"], "link": "https://blog.octo.com/en/introduction-to-grid-computing-for-value-at-risk-calculatio/", "abstract": "Introduction to Grid Computing for Value At Risk calculation Publication date 15/09/2010 by Marc Bojoly Tweet Share 0 +1 LinkedIn 0 Risk management is today a strategic objective for financial institutions such as, but not limited to, investment banking and insurance companies. Such risk evaluation uses advanced mathematical theory and requires a lot of data computing. In this article, we will introduce basic concepts of Value At Risk estimation in order to show which kind of calculation is required. We will introduce grid computing in order to show how such architecture can help financial companies producing quickly these very important VAR figures. In later articles, we will describe in detail implementations of that algorithm on several grid middlewares. Value at Risk The Value at Risk is one of the major risk indicators for financial portfolio or product. It was introduced in the late 1980’s by American banks (first Banker’s Trust then J.P. Morgan). It gives a measure of the maximal potential loss, during a defined period, under a given probability for a financial product or portfolio. If the “1 day VAR at 90% for your portfolio” is -5%, it means you got less than 10% chance of loosing more than 5% of the value tomorrow. More mathematically, the value of a financial product or portfolio can be represented by a probability distribution. The graph above shows a hypothetic probability curve for the value of a product or portfolio for the next business day. The average closing price is 100€ and the vertical axis shows the probability to occur. The red area represents a cumulated probability of 10%. So a price inferior to 75€ in the red area has less than 10% of chance to occur. In conclusion the 1 day Value at Risk at 90% is 25€=-25% . For a sample of data, it is related to the percentile at 10% . For listed products like stocks and bonds, we have enough historical data to evaluate the percentile based on that history. For more exotic products, like structure products, built on demand, prices are not available. In that case, prices are simulated and the same formula is then used like for historical values. Monte-Carlo simulation The method used is a computational algorithm based on random sampling. This technique is known as the Monte-Carlo method , by reference of the gambling games to the famous casinos in Monte-Carlo . To quickly describe how such method can lead to precise result with random values; let’s see how to compute the number. Take a square with an edge length of 1.0 and a quarter of circle with radius length of 1.0 as in the schema thereafter. If you generate two independent random numbers x and y uniformly distributed between 0 and 1.0 , it’s like putting random dots on your square. With infinity of dots, the whole area is uniformly covered. The ratio between the number of dots in the circle ( ) and the total number of dots equals the ratio between the areas of the quarter of the circle and the square. So . The algorithm consists in generating random couples and counting the proportion of ranks in the circle. Multiplying it by 4 gives an approximation of the number. For financial purpose, we will replace random dots by random simulations of market parameters (such as underlying price, interest rate) and the circle area by a predictable model allowing computing the portfolio value with these parameters. A simple example: portfolio with one call For the purpose of this article, I chose a very simple portfolio with only one product: a long position on a call (i.e. the owner has bought a call). A call is an option of buying, a financial contract giving the right but not the obligation to the buyer to buy a certain quantity of a financial instrument (the underlying) at a given date (the maturity date) at a given price (the strike). The seller sells this contract for a given fee (the premium). We can then build a first model of the payoff of this call according to the price that the underlying will have at  maturity date (lets say in a year). We now want to be able to build a model for the price of a call with a maturity in one year. But today we ignore the price of the underlying that might be in a year. I won’t go deeper into details and will just say that  a more complicated model exists. It is based on probability theory: the Black and Scholes model. It can determine the price of the call by knowing only: characteristics of the call : maturity date and strike price the price of the underlying as of today (and not anymore in the future) the risk free interest rate a parameter of the derivative market known as implied volatility . The risk free interest rate can be estimated on Libor today’s value (close to 5%). The underlying volatility can be deducted from listed options or from specific index like VIX and VAEX (see sample market values ). In the example, we will take a value of 20. The Black and Scholes model for a single call can be expressed as a quite complex but closed formula (i.e. is predictable and can be easily computed). It is an arithmetic expression involving exp , sqrt and N(.) (cumulative distribution of normal distribution functions named NORMSDIST in Excel). See this Wikipedia article for more details and pay close attention to the available graphic which shows how the price of the option is modified when the price of the underlying is modified. Estimation of the required computing power By mixing these two methods, we can estimate the VAR for our sample portfolio. Required steps involved : Generating sample data for the three parameters that we have chosen: the underlying stock price , the interest rate and the underlying volatility . To keep it simple, we assume that these three parameters follow independent normal distributions. Calculating for each set of generated parameters, the corresponding option price according to the Black and Sholes expression Calculating the percentile at 1% for the distribution of option prices Such individual computing is quite short but a large number of samples are required to get a correct precision of the end result. To give an order of magnitude, about 100,000 samples are required in our case. Using Excel 2007, on my laptop (a Dell Latitude with a core i7 with 2 multi-threaded cores and 4 GB RAM) 100,000 samples took approximately 2 s. for one option and one sample parameter simulated (underlying price). On the same laptop a mono-threaded Java program (using only one core) took about 160 ms. It seems to be quick but in real life, each exotic product in a portfolio is a combination of multiple options. A portfolio probably contains tens or hundreds of exotic products. Moreover, more complicated modeling is today used instead of Black and Scholes model requiring more complex calculation or more samples. So, the required computing power can easily get really huge. The grid computing architecture In order to deal with so much computing power at the best cost, the grid computing architecture is the best solution. A grid is a computing architecture which allows coordinating a big number of distributed resources toward a common objective. In our case these resources are computers whose processors are used to do that calculation. These resources can be distributed, i.e. physically distant and linked by a computer network. Each of these N nodes (i.e. computers) will do a small part of the calculation on its processors, allowing in giving out the result (in an ideal world) N times faster than with one single computer. We will now describe how to write the algorithm for that grid. How to compute the VAR In order to be able to split, to distribute the VAR calculation, the algorithm should be adapted . The easiest way to get an approximation of the VAR of a portfolio could be by summing up the VAR of each product. The VAR of each product can be computed independently on each node. The algorithm can in that case remain unmodified. In the above schema, Compute part x corresponds to the calculation of an individual VAR. However, it will not allow speeding up the VAR calculation of our single call portfolio. And in a real life example, it would not allow us scaling up the number of samples so the precision. Despite these flaws, lets see how we would implement it. We can use the map/reduce pattern . This pattern comes from functional programming where map m() list function applies the function m() to each element of the  list, and reduce r() b list aggregates b parameter and all values of list into one single value. Lets take a practical example. The map (\\x -> 2*x) [1,2,3] returns [1*2,2*2,3*2]=[2,4,6] . The reduce (\\x y -> x+y) 0 [2,4,6] returns 0+2+4+6=12 . With such a pattern, the list can be split in N sub lists, sent to the N nodes of the grid. In that example, 1 is sent to the first node, 2 to the second, etc. The \\x -> 2*x function code, sent with the values, is executed by each node: 2*1 by the first, 2*2 by the second, etc. You need to imagine that \\x->2*x is replaced by a computer intensive function. Executing it in parallel will therefore divide the total required computing time. All results 2, 4, etc. are sent back to the sender node. The results are concatenated in the list [2,4,6] and the reduce (\\x y -> x+y) is applied. The map function is split across nodes but not the reduce function which is executed by the sender node. In our case, each sample generation will represent an element of the list . The map() function will, on each node, generate the sample data, compute the corresponding call price and return it. Then all call prices are sent to the reduce node. The reduce() function computes the percentile at 1% of the resulting data. Consequently it will give us the VAR at 99% of our portfolio. This first version works, but can still be a bit improved. First, the reduce() function is not distributed, so it is better to minimize the amount of data it has to process. Next, moving 100,000 prices over the network is resource consuming. In reality, network bandwidth is a scarce resource in a grid Also we might notice that computing the percentile at 1% comes to sorting the data, and taking the highest value of the smallest 1% values. I presume that for the particular business case of this example the intermediate results can be discarded. In that case, this elimination algorithm can be distributed too. The idea is that the 10 smallest values of a set are included in the 10 smallest values of each of its subsets. Lets imagine that I’m looking up the 4 highest value cards of a poker deck. The deck has been split by suit into 4 sets. I can look in each set for the 4 highest cards. I will get an ace, a king, a queen and a jack for each suit. Then, I can merge these 4*4=16 cards and look again up the 4 highest ones. I will get the 4 aces. That can be generalized for any subsets. So now, lets say we are computing 100,000 samples on two nodes. The percentile at 1% corresponds to the highest value of the 1,000 lowest values. On each node 50,000 values have been computed. On each node, we can take the 1,000 lowest values and send them as results instead of the 50,000. Then the reduce() function will only merge the two lists of the 1,000 lowest values, take the 1,000 lowest ones and identify the highest value. Indeed in the worst case where generated prices are totally sorted across nodes, the node with all the lowest values will give us the 1,000 lowest ones, guaranteeing that the percentile is still transmitted. Such optimization allows dividing by 50 the data that are transmitted thru the network. The benefit goes down when the number of nodes increases. In the worst case scenario, with 100 nodes there is no more benefit: each node has computed 1% of the data. But in practice, distributing small list of samples is counter-productive as distribution overhead is higher than the parallel computing gain. As you can see, these kind of business specific optimization ca really help minimizing the required resources. The final algorithm is summarized in the following schema: Conclusion From a business point of view, risk estimation is becoming more and more important for financial institutions to prevent financial losses on fluctuant markets. Moreover, new regulation rules such as Basel III and Solvency II take into account the VAR in their requirements. Computing efficiently such figures is a key requirement for Information System of financial institutions. Through a very simple example, this article has shown that such calculation consists in generating sample data , doing some mathematical processing to compute a value for the portfolio for each sample data. The amount of sample data and sometimes the complexity of the processing require specific architectures to finish such calculations in a reasonable time. Grid computing is the most up-to-date architecture to solve these problems. Using map reduce pattern is one of the easiest way to write an algorithm that benefitiates of grid computing parallel capabilities. Also, business requirements understanding is a key point in optimizing the code. In this example, dropping the intermediate results allows to speed up the calculation. In another article, I will describe an implementation of that example using a grid computing middleware and will give some performance figures. Main references Wikipedia VAR Wikipedia Black and Sholes Wikipedia Monte Carlo in finance Article on grid computing in french Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Finance , Grid , grid computing , VAR . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 4 commentaires sur “Introduction to Grid Computing for Value At Risk calculation” Nicolas Martignole 30/09/2010 à 21:53 Great article, if you could also cover financial library such as JQuantlib, it would be great. Nicolas Marc Bojoly 01/10/2010 à 20:33 I'm currently working on articles about implementation of VaR calculation, but later JQantlib could be an interesting subject. Shalom Carmel 25/11/2010 à 12:00 I have a problem with your methodology of computing the VaR independently for each portfolio product. \r\nThe VaR should reflect the value distribution of the entire portfolio, including correlations between different assets. When you add up individual VaRs you ignore the correlations and end up with an estimate that can be significantly different than the real VaR, especially when you have a mixed portfolio with different types of assets, like stocks, bonds and options. Marc Bojoly 26/11/2010 à 00:56 I totally agree with you. Due to the subadditivity of the VaR, individual VaRs of different products should not be added. \r\nThat's why I have described a very simple example -a \"portfolio\" with only one call- in order to avoid this subadditivity problem. All my samples are for one product: a call. It is not a realistic example from a financial point of view but my goal was rather to introduce grid computing engineering (see my later articles). So introducing a realistic financial example with several products and correlated variables to simulate would have been too complex for the size of this article. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-09-15"},
{"website": "Octo", "title": "\n                How to rescue your data, 3/3            ", "author": ["Gabriel Guillon"], "link": "https://blog.octo.com/en/how-to-rescue-your-data-33/", "abstract": "How to rescue your data, 3/3 Publication date 10/12/2010 by Gabriel Guillon Tweet Share 0 +1 LinkedIn 0 Last time we have seen how to rescue your FAT. In this article we’ll see a third, and last, way of losing data. Physically crash a hard disk Hard disk are made of  mechanical pieces, so they are subject to  ageing. The S.M.A.R.T.  technology, shipped in hard disk for years, can  monitor a bunch of  indicators helping you foresee your hard disk’s end  of life. Under  GNU/Linux, smartd is widely used. As  all the indicators, if they are not seen by a human, they are  useless.  You have here a perfect way to physically crash a hard disk :  wait,  and don’t look at indicators. But facts are stubborn : a dying hard disk complains a lot in syslogs : Jun 28 08:17:09 rhynn kernel: ata5.00: exception Emask 0x0 SAct  0x0 SErr 0x0 action 0x0 Jun 28 08:17:09 rhynn kernel: ata5.00: BMDMA2 stat 0x686c0009 Jun 28 08:17:09 rhynn kernel: ata5.00: failed command: READ DMA EXT Jun 28 08:17:09 rhynn kernel: ata5.00: cmd  25/00:01:32:03:4a/00:00:12:00:00/e0 tag 0 dma 512 in Jun 28 08:17:09 rhynn kernel: res 51/40:00:32:03:4a/00:00:12:00:00/e0  Emask 0x9 (media error) Jun 28 08:17:09 rhynn kernel: ata5.00: status: { DRDY ERR } Jun 28 08:17:09 rhynn kernel: ata5.00: error: { UNC } Jun 28 08:17:09 rhynn kernel: ata5.00: configured for UDMA/100 Jun 28 08:17:09 rhynn kernel: ata5: EH complete You can ignore them too if you want to go into troubles. More  of that, a dying hard disk can completely freeze your computer.  At  this point, you can still ignore your hard disk, but you are really   looking for waste of time and data. That what I’ve done : ignoring, ignoring, ignoring. And badblocks confirm me that they were bad blocks on my disk. Good news : letting this hard disk die made me write this article :) Once you are fed up with weird messages in syslogs and computer freezing, you can Rescue your disk First step : buy a brand new one. Not-so-good-news : you’ll be able to save your data, but It depends on the state of your hard disk (old, dying, dead) It can take a lot of time (from minutes to days, or weeks) You will certainly loose some files But you played with fire, don’t complain :) Now, let me introduce a friend I hope you’ll not need too often : ddrescue . ddrecue  is like dd (it copies data from a file or block device to an  other)  but “try hard to rescue data in case of read errors”. Please note  : two  ddrescue are existing : dd_rescue and ddrescue. I used the  latter, the  package on Debian/Ubuntu is gddrescue. The documentation of ddrescue is pretty well done, you should read it. In a few words, ddrescue can : rescue data from a sick hard disk to a good one or from 2 sick hard disk (in RAID1 , for example) or from a hard disk to an other, and if the other fail, from this other to a third. Here, you “just” crashed a disk, so you “just” need ddrescue in its common need : rescuing a partition (or a disk) Make  a big partition in your brand new disk. The size must be equal  or (a  bit) greater than the size of your sick partition. It will contain  the  (hopefully) rescued data of your sick partition. The ‘logfile’ mentioned is a file containing informations for ddrescue  to know what job  has been done by him, thus allowing you to interrupt  it and restart it  later without redoing everything. First, copy everything that can be copied without retrying bad sectors. Look twice to the source and destination partition before hitting enter ! [root@home]# ddrescue -n /dev/sdb1 /dev/sdc1 /root/logfile.sdb1 Press Ctrl-C to interrupt Initial status (read from logfile) rescued:         0 B,  errsize:       0 B,  errors:       0 Current status rescued:    98671 kB,  errsize:       0 B,  current rate:   72483 kB/s ipos:    98632 kB,   errors:       0,    average rate:   72483 kB/s opos:    98632 kB,     time from last successful read:       0 s Finished Then, retry but this time try to rescue. This step can be very long. I stopped after 2 weeks : my guess was that bad sectors contains no more data. [root@home]# ddrescue -d -r3 /dev/sdb1 /dev/sdc1 /root/logfile.sdb1 Press Ctrl-C to interrupt Initial status (read from logfile) rescued:   148571 MB,  errsize:   2065 kB,  errors:    3774 Current status rescued:   148571 MB,  errsize:   2065 kB,  current rate:        0 B/s ipos:    54681 MB,   errors:    3774,    average rate:        0 B/s opos:    54681 MB,     time from last successful read:      43 s Retrying bad sectors... Retry 1 Checking the filesystem ensure that your data are coherent : [root@home]# e2fsck -v -f /dev/sdc1 Finally, mount your rescued partition read only : [root@home]# mount -t ext2 -o ro /dev/sdc1 /mnt/sdc And copy your data to a safe place. Conclusion When a hard disk is ready to die, it complains. You’d better not ignore those complains… If you do so, you are done for wasting time and data, and using ddrescue. Well, well, as a big conclusion to these three articles : in trying to save a hard disk killed by my laziness, I used different methods to rescue data : To rescue partition table : testdisk To blindly rescue files (either deleted or held in a deleted partition) : photorec To rescue a file allocation table destroyed by pvcreate : dd , mkfs.* To rescue a dying hard disk : ddrescue You should not use those program too often :) But when you are in trouble, it worth remembering they are existing. You should practice them to know how they react, in order to not make more mistakes when something goes wrong. Tracklist of those articles : “How to save a life” by The Fray Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Linux , rescue , security , tips&tricks . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 5 commentaires sur “How to rescue your data, 3/3” PAUL 10/12/2010 à 11:52 Please also note that Spinrite, a Steve's Gibson tool, may be useful to recover your data if valuable.\r\n\r\nIt may recover many data by accessing directly to hard drive (on very low level) and try many different approach to read files.\r\n\r\nIt get me data that ddrescue didn't recover.\r\n\r\nSee http://www.grc.com/sr/spinrite.htm\r\n\r\n\r\nGrégory Gabriel Guillon 10/12/2010 à 12:32 Thanks for the link, I'll check this out! cdg 10/12/2010 à 14:27 an other small trick if your hard disk is dead. But it in the fridge (with the ice)!\r\nSound silly but when it is very cold it could work for a while. So when you take it out and plug it again to the PC you could be able to use it for a while (until it reaches again room temperature)\r\n\r\nPS:\r\n- I do not remember which physics law is involved\r\n- Of course, the hard disk must be kept in a dry place. not in direct contact with ice ;-) Gabriel Guillon 13/12/2010 à 10:32 I also read somewhere that putting your HD in the freezer could help. I forgot that tips, so thank you for mentionning it :) Rich Hartley 12/03/2018 à 02:08 If the issue is loss of lubricant viscosity, or a heat related alignment problem then yes the old fridge trick still works but you MUST be exceedingly careful about moisture/condensation. Remember too that as storage capacity becomes more dense it is also more vulnerable to failure. At some point in the future the 3.5\" form factor must die. I'm not sure what will replace it, though. SSD's are great for speed but they lack long term durability. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-12-10"},
{"website": "Octo", "title": "\n                How to rescue your data, 2/3            ", "author": ["Gabriel Guillon"], "link": "https://blog.octo.com/en/how-to-rescue-your-data-23/", "abstract": "How to rescue your data, 2/3 Publication date 03/12/2010 by Gabriel Guillon Tweet Share 0 +1 LinkedIn 0 Last time , we have seen how to rescue a partition. Let’s go further and make an other mistake… Crash your partition’s FAT The FAT (file allocation table) is a map : it tells the OS where all the  files of your partition are. Thus, there is one FAT by partition, and the FAT is depending of the filesystem of your partition. It’s the  same analogy as the partition table : if you loose this map, the cities  are still there but you don’t know where. There is several ways of crashing a file allocation table. Bad use of dd is one of them, and probably the most common, playing with Partition Magic is an other. But once again I’ll give you the one I used. You are rescuing /dev/sdb. To do that, you bought an other hard disk.  This hard disk is seen by the OS as /dev/sdc (you see my point ? ). You  plan to play with LVM with this new hard disk. Full of confidence, and at an advanced night time, you think “ pvcreate /dev/sdc2″ and you type : [root@home]# pvcreate /dev/sdb2 Just when pressing Enter, you realize that you should have slept before  doing hard disk rescue, but it’s too late : something wrong has been  done. You try to mount(8) /dev/sdb2, and you get that message : mount: unknown filesystem type 'LVM2_member' Well, bad news : you are in trouble. At this point, you know that your FAT has been altered, and just it,  because the OS can see the partition, but can’t read the files in it. It  is the same under Windows (although I don’t know how to only crash a partition table under Windows :) ) : if you can see the  partitions but Windows says that it is not formatted, your are almost  sure that your FAT is gone. Maybe you noticed that pvcreate didn’t take so much time that it  destroyed all the datas (i.e. : all the allocated inodes/sectors  of the partition). I  mean : the guess is that only the file allocation  table has been  destroyed, not the allocated blocks (remember : you loose the map but the cities are still there) If this supposition is true (spoiler : it is), there are two good news : Good news #1 : it’s possible to rescue your datas. Good news #2 : there is (at least) two possibilities. Rescue your files The first possibility that comes into mind is photorec (1).  Photorec comes with testdisk. Its purpose is to find deleted files by  searching beyond the filesystem. I.e. : it can find deleted files even  if your partitions and/or file allocation table is destroyed. In other words : If your partition is deleted, photorec can help you rescuing your files (not the partition table) If your file allocation table is destroyed, photorec can help you in  the same way (i.e.: rescuing the files, not the file allocation table) If you want to rescue a deleted file, photorec can help you again photorec Go on with photorec, which is the ‘canonical’ way of rescuing files. [root@home]# photorec /dev/sdb2 Select a media (use Arrow keys, then press Enter): Disk /dev/sdb2 - 98 MB / 94 MiB (RO) - VBOX HARDDISK [Proceed ]  [  Quit  ] Press ‘Proceed’ Please select the partition table type, press Enter when done. [Intel  ]  Intel/PC partition [EFI GPT]  EFI GPT partition map (Mac i386, some x86_64...) [Mac    ]  Apple partition map [None   ]  Non partitioned media [Sun    ]  Sun Solaris partition [XBox   ]  XBox partition [Return ]  Return to disk selection You are rescuing a partition, not a whole hard disk. So validate on ‘None’ Partition                  Start        End    Size in sectors Unknown                  0   0  1    11 253 63     192717 [Whole disk] P Linux LVM2               0   0  1    11 253 63     192717 [ Search ]  [Options ]  [File Opt]  [  Quit  ] Validate on ‘Linux LVM2’ P Linux LVM2               0   0  1    11 253 63     192717 To recover lost files, PhotoRec need to know the filesystem type where the file were stored: [ ext2/ext3 ]  ext2/ext3/ext4 filesystem [ Other     ]  FAT/NTFS/HFS+/ReiserFS/... Make the good choice. For my case it was ‘Other’ (ntfs) Do you want to save recovered files in /root ? [Y/N] Do not choose to write the files to the same partition they were stored on. To select another directory, use the arrow keys. drwxr-xr-x     0     0      4096  7-Jul-2010 17:30 . drwxr-xr-x     0     0      4096  3-Mar-2010 17:01 .. drwxr-xr-x     0     0      4096 25-Sep-2009 15:50 Desktop drwxr-xr-x     0     0      4096 25-Sep-2009 15:50 Documents drwx------     0     0      4096 20-Apr-2010 16:05 Mail drwxr-xr-x     0     0      4096 25-Sep-2009 15:50 Music drwxr-xr-x     0     0      4096 25-Sep-2009 15:50 Pictures drwxr-xr-x     0     0      4096 25-Sep-2009 15:50 Public drwxr-xr-x     0     0      4096 25-Sep-2009 15:50 Templates drwxr-xr-x     0     0      4096 25-Sep-2009 15:50 Videos Here, select a directory to restore your files to. Photorec will create a  directory named ‘recup_dir.1’ within the one you chose. Please do not select the directory where the partition your are rescuing is mounted (if so) or you’ll probably lose all the datas you are trying to rescue . Once you have chosen the directory, press ‘Y’. Disk /dev/sdb1 - 98 MB / 94 MiB (RO) - VBOX HARDDISK Partition                  Start        End    Size in sectors P Linux LVM2               0   0  1    11 253 63     192717 1 files saved in /root/recup_dir directory. Recovery completed. txt: 1 recovered After a (eventually awfully long) time, your files will be in (maybe a  bunch of) ‘recup_dir.*’. Press ‘q’ as often as necessary to quit  photorec. Let’s see what has been done : [root@home]# ls recup_dir.1 f0012290.f Oumpf… the file is ill named (photorec can’t guess what was the  name of the file, as the partition table was probably destroyed), but it  is there. If you have only a few files, it’s ok, but if you have  plenty, you are done for a Sherlock Homes’ work… This is an understandable limitation of photorec. As I’m lazy, I tried to find an other way of rescuing my files, a way that restores the file allocation table. Other way It’s a bit tricky, so please put yourself in a quiet and gentle place. First, I looked at pvcreate’s man page. I saw this : -Z, --zero y|n Whether  or  not  the  first  4 sectors (2048 bytes) of the device  should be wiped.  If this option is not given, the default is to wipe  these sectors unless either or both of the --restorefile or --uuid  options were specified. This means that only the first 2k of /dev/sdb2 are modified by  pvcreate (and this means the assertion that pvcreate didn’t take so long  that it wiped all the data is true). And this means that, maybe , the file allocation table of the partition has not been touched. So maybe it’s possible to rescue your datas by only restoring the fist 2k of the partition. It’s worth a try and, spoiler again, it works. How to recreate those 2k ? By taking them from an other partition of the same type. But, bad news, it’s not so easy as it seems : if you wan to succeed,  you have to take care of the filesystem you are playing with. In other words : with ext2 you can fetch those 2k in a particular way, and this particular way doesn’t work with ntfs… I’ll explain both : the way that work with ext2, and the way that work with ntfs. Your filesystem is ext2 (or ext3, ext4, xfs) Easy easy easy : You will create a file which will contain a partition of the given type And copy the first 2k of this file to the sick partition. If you have enough free space, it’s highly advisable to make a copy of your sick partition with ‘dd if=/dev/sdb1 of=/some/where’ and play with the copy. In that case do not forget to append ‘conv=notrunc’ to all the dd commands , or you’ll lose time, and maybe data. And if you still have free space, you should create a file of the  same size of the sick partition. For fun, let’s consider you don’t enough space to create a file of the sick partition’s size. So you’ll create a file of 10MB [root@home]# dd if=/dev/zero of=/tmp/rescue bs=1k count=10k 10240+0 records in 10240+0 records out 10485760 bytes (10 MB) copied, 0.0297014 s, 353 MB/s [root@home]# mke2fs /tmp/rescue mke2fs 1.41.11 (14-Mar-2010) /tmp/rescue is not a block special device. Proceed anyway? (y,n) Say yes . [root@home]# dd if=/tmp/rescue of=/dev/sdb1 bs=512 count=4 4+0 records in 4+0 records out 2048 bytes (2.0 kB) copied, 0.00195328 s, 1.0 MB/s At this point, you can now mount /dev/sdb1 and you’ll see you files , yeah :) Try to copy them in a safe place before continuing. If you type ‘df’, you’ll maybe see weird things : Filesystem           1K-blocks      Used Available Use% Mounted on /dev/sdb1                 9911     -5437     14836   -  /mnt/sdb It’s because you have take the 2k from a filesystem with a different  size, thus mke2fs has made different choices concerning the parameter  with whom /dev/sdb1 and /tmp/rescue were made. fsck -y will help us : [root@home]# umount /dev/sdb1 ; e2fsck -y /dev/sdb1 e2fsck 1.41.11 (14-Mar-2010) /dev/sdb1 contains a file system with errors, check forced. Resize inode not valid.  Recreate? yes Pass 1: Checking inodes, blocks, and sizes Pass 2: Checking directory structure Pass 3: Checking directory connectivity Pass 4: Checking reference counts Pass 5: Checking group summary information Block bitmap differences:  -(42--202) -(204--258) -(421--511) -525  -(8234--8450) -(8613--8703) Fix? yes Free blocks count wrong for group #0 (7161, counted=7470). Fix? yes Free blocks count wrong for group #1 (7681, counted=1844). Fix? yes Free blocks count wrong (14842, counted=9314). Fix? yes Free inodes count wrong for group #0 (1896, counted=1168). Fix? yes Free inodes count wrong for group #1 (2008, counted=1280). Fix? yes Free inodes count wrong (3904, counted=2448). Fix? yes /dev/sdb1: ***** FILE SYSTEM WAS MODIFIED ***** /dev/sdb1: 112/2560 files (0.9% non-contiguous), 926/10240 blocks If your filesystem is xfs, the command is not ‘fsck -y’ but ‘xfs_repair-o force_geometry’ Your filesystem is ntfs It’s more difficult : ntfs behaves bad when mixing files seen as  filesystem and filesystem on hard disk. I mean that you can test with 2  files : one that you crash with pvcreate, and the other one used to fix  the first, and it will works. But you can’t (at least, I didn’t succeed)  rescue a physical ntfs FAT with a file, as done with ext2. So, your only hope, Obiwan, is to : Save your partition in a file. It is not an option ! Make a ntfs filesystem on the partition, From here, you can either : Put the first 2k of that partition in a safe place, Put your saved partition in its original place, Put the first 2k you have saved at the beginning of the partition. Or : Reput your partition to its original place skipping the first 2k , leaving untouched the brand new first 2k of the hard disk. As the second method is more elegant, I’ll describe it : Here, /dev/sdb2 contain a ntfs partition salvaged with pvcreate. [root@home]# cat /dev/sdb2 > /safe/place/sdb2.sick [root@home]# mkfs.ntfs /dev/sdb2 #if your partition is huge : mkfs.ntfs -f /dev/sdb2 [root@home]# dd if=/safe/place/sdb2.sick bs=512 seek=4 skip=4 Please note the two options of ‘dd’ : seek and skip . They tell dd to skip the first 4 (512b) blocks of the originating file, and to seek 4 (512b) blocks after the beginning of the destination file. You can now mount your partition. Et voilà :) If I summarize : when you crash your file allocation table, photorec will help you find your lost files. It’s the common way of doing it, but photorec can’t guess the name of your lost files. If you crash the file allocation table with a tool such as pvcreate,  which erases the first 2k of a partition, you can recover your files by  rebuilding those 2k, either by making an other filesystem aside, or by  recreating the partition in place, it depends of the filesystem. Conclusion Crashing a FAT should not happen too often. Anyway, photorec can help you in much more occasions than a FAT crash : accidentally erasing data, partition corruption, … But photorec is slow. In some cases, you’d better use your brain and read man page to save time :) Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Linux , rescue , security , tips&tricks . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-12-03"},
{"website": "Octo", "title": "\n                Let’s play with Cassandra…(Part 2/3)            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-23/", "abstract": "Let’s play with Cassandra…(Part 2/3) Publication date 12/06/2010 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 In this part, we will work in more details and closer to the code with Cassandra. The idea is to provide a kind of simplified current account system where a user has an account and the account has a balance… This system will so manipulate the following concepts: – A client has different kind of properties defining his identity – A client has one account – The account has a list of operations (withdrawal, transfer are all kind of operations) Here is the way it would have been modelized in the relational world (or at least UML world) The Cassandra set up I will not drive deep into the details of a Cassandra set up. This article explains it in details but here are the main points. Define your cluster Each nodes of the cluster has a configuration file called storage-conf.xml where are defined the following main sections – Cluster and Keyspace definition . The cluster is made of several nodes (the Seed) which store all the Keyspaces you will define (and of course data). As we talked about in the previous part [add a link], you define the Keyspace that will contain all the ColumnFamily. <storage>\r\n  <clustername>Test Cluster</clustername>\r\n  <autobootstrap>false</autobootstrap>\r\n<keyspaces>\r\n    <keyspace Name=\"myBank\">\r\n...\r\n</keyspace></keyspaces>\r\n...\r\n<partitioner>org.apache.cassandra.dht.RandomPartitioner</partitioner>\r\n<initialtoken></initialtoken>\r\n <seeds>\r\n    <seed>192.168.216.129</seed>\r\n    <seed>192.168.216.130</seed>\r\n  </seeds>\r\n</storage> Then, you define the IP address of all the nodes (ie. The seeds) that will compose your cluster. During the startup phase, all the nodes will communicate to each other (using Gossip protocol), thus detecting starting or node failures.  You can go further in the definition of your cluster topology and group (as far as I know in another conf file) IPs by datacenters. The InitialToken, if not defined, will automatically be set by Cassandra (based on the cluster topology and following the Consistent Hashing algorithm). The documentation gives more details about ring management The partitioner is a much more tricky and Cassandra provides, by default, two partitioners : the RandomPartitioner and the OrderPreservingPartitioner . In the first case, the data will be partitioned using a row key hash (typically md5). In the second case, the data will be partitioned in their natural order and thus facilitates the range queries. So once again, the choice you made (and you cannot change it during your cluster life) is depending on the way your data is manipulated. – Node access <listenaddress>192.168.216.128</listenaddress>\r\n  <storageport>7000</storageport>\r\n\r\n  <thriftaddress>192.168.216.128</thriftaddress>\r\n  <!-- Thrift RPC port (the port clients connect to). -->\r\n  <thriftport>9160</thriftport>\r\n... The ListenAddress and ThriftAddress enable to define the current IP and listening port for the current node. The first IP is used by all the nodes to gossip each others. The second address is the one used by thrift clients to connect to the node and insert, delete or update data. Define your data models In our example, we will define two ColumnFamily. The first one will store all the customers. The second one all the operations. <keyspace Name=\"myBank\">\r\n<columnfamily CompareWith=\"UTF8Type\" Name=\"customers\"/>\r\n<columnfamily CompareWith=\"TimeUUIDType\" Name=\"operations\" ColumnType=\"Super\" CompareSubcolumnsWith=\"UTF8Type\"/>\r\n\r\n<!-- Number of replicas of the data -->\r\n<replicationfactor>2</replicationfactor>\r\n       <replicaplacementstrategy>org.apache.cassandra.locator.RackUnawareStrategy</replicaplacementstrategy>\r\n<keyscachedfraction>0.01</keyscachedfraction>\r\n      <endpointsnitch>org.apache.cassandra.locator.EndPointSnitch</endpointsnitch>\r\n</keyspace> To begin with the simplest things, the replicationFactor defines the number of nodes the data will be replicated. Then let’s talk about the ColumnFamily. First, you will notice that the schema for each ColumnFamily is not defined (whereas the actual 0.6 version of Cassandra does not allow dynamically adding or removing ColumnFamily, the 0.7 should provide this feature) and you only know that customers and operations will be stored. Data modeling If you look at the UML diagram representing the different concepts, you will notice that there is a “one-to-many” relationship between an account and the operations on this account. An easy way to model this in Cassandra is by using the SuperColumn . Thus the operation has the following structure: Thus: –\tThe key is the account Id –\tThe “Value” is a SuperColumn which stores all the operations for this account (the limitation of this model is the number of operations you could have…). Thus the operation is a list of columns (type, amount, date…) ordered by a time-based UUID inside the SuperColumn. The CompareWith tells Cassandra how to sort the columns (remember the column are sorted, within a row, by their name. In our examples, I want my operations (whose name is a time-based UUID) to be chronologically sorted. That’s what I specify to Cassandra with the CompareWith attribute. The CompareSubcolumnsWith attribute will be responsible for sorting the Column included in the SuperColumn… Here is what you get using the Cassandra-cli tools Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Cassandra , NoSQL . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-06-12"},
{"website": "Octo", "title": "\n                Classical issues: Imprecise computing (part 1)            ", "author": ["Henri Tremblay"], "link": "https://blog.octo.com/en/classical-issues-imprecise-computing-part-1/", "abstract": "Classical issues: Imprecise computing (part 1) Publication date 19/07/2010 by Henri Tremblay Tweet Share 0 +1 LinkedIn 0 I’m starting today a new series of articles called Classical Issues . In it, I’ll address, one after the other, classical issues encountered through my software engineering years. This first article is targeted to demystify computing and give some best practices for an enterprise application. By enterprise application, we mean an application working on things like money, prices and quantities. It will be in two parts. This one is about explaining the root of the problem. The second one will show how to handle it in Java and .Net. Bill is developing a software doing commission payments. He needs to add 1.2$ to each transaction. He codes a method doing just that and the unit test coming along. @Test\r\n public void testAddCommission() {\r\n  double actual = addCommission(1000000.1);\r\n  assertEquals(1000001.3, actual, 0);\r\n }\r\n\r\n public static double addCommission(double nominal) {\r\n  return nominal + 1.2f;\r\n } java.lang.AssertionError: expected:<1000001.3> but was:<1000001.3000000477> “Darn! It’s not working!”. What’s going on? Floating points vs Decimals Floating point numbers were introduced in computers for performance reasons (and only for that). They became ubiquitous since the Intel 80486 arrival and its floating point unit (FPU). They allow to perform a multiplication or division on a decimal number using only one CPU cycle. They are the float , double and quad we find in our code (their respective precision depend on the language). With decimals (which are not floating points), it’s slower. More or less as much operations as what you used to do at school on paper. Luckily, computers are much faster than you at doing this and they make less mistakes. By decimals we mean the decimal (in .Net) and the BigDecimal (in Java). If they are slower, why use them you’ll say? Because they are not using the same internal representation. In both cases, there are a significand (also called mantissa) and an exponent (also called scale). However, the significand of a decimal is an integer. The one of a floating point is a number between 0 and 1 in base 2. It is really important to understand that it changes everything. For a floating point, the numbers on the right of the point will be a power of fractions of 2 since we are in base 2. For instance, 0.1 en base 2 is equal to 0.5 in base 10. Another example: 7.5, written 75E-1 in decimal, will have a significand of 75 and an exponent of -1. For a floating point, we will have 0.75E1 in base 10, which is 0.11 in base 2 ((1/2)^1 + (1/2)^2) for the significand and 1 for the exponent. So, it’s a power series of 1/2 instead of the power series of 1/10. It’s this representation that allows faster computing. As we know, computers love binary numbers. It’s a base problem Problems occur when a number can be represented perfectly in base 10 but can’t in base 2. The usual example is 0.1. It’s value in base 2 is periodic (0.000110011001100…). We can’t represent it precisely. The IEEE 754 standard managing floating point computing is then trying to give the best representation possible and to handle roundings. But the precision lost stays. It is really important to understand we are facing a representation issue, not a precision one. We know how to handle precision issues. You just need to increase the precision and your good to go. A double is too short, just switch to a quad . But our 0.1 will still can’t be represented precisely. Also note that this is language independent. They all handle floating points the same way and use the floating point unit for computing. The 0.1 example in more details: float f = 0.1f;\r\nSystem.out.println(f); // prints 0.1\r\nBigDecimal d = new BigDecimal(f);\r\nSystem.out.println(d); // prints 0.100000001490116119384765625 We could think the conversion to BigDecimal destroyed the number. No. To make things a bit more complicated, it’s the float print out that is misleading.  The real float value is the one shown by the BigDecimal. But the printing algorithm of a floating point is to print enough bits so that two adjacent values will be printed differently. So, in fact, the printing algorithm is trimming the real value. Conclusion As we have seen briefly above, the decimal is different. You still have a significand and an exponent but the significand is an integer.  Because all integers can be represented in base 2, the significand can be perfectly represented and does not need any rounding. This is essential in an application working with amounts for instance. And by the way, our elders knew that. No Cobol developer would dare use a floating point. Sadly, this knowledge was lost along the way. One striking example is Java that didn’t even have the BigDecimal at first. This mistake was repaired and as a bonus, the BigDecimal is not restricted in length (so no precision issue is possible). On the other side, the decimal of .Net is on 128 bits. But it’s large enough to prevent precision issues for the usual amounts manipulated in enterprise applications. First rule: Never use a floating point in an enterprise application. Ever! Even for literals. Add a Checkstyle rule making sure of that. There are obviously exceptions to this rule (performance), but they are extremely rare. As mentioned earlier, the second part of the article will detail the specificities of the BigDecimal and decimal. Indeed, even if they prevent representation issues, some other errors are waiting for you in the shadow. See you there. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Consulting Chronicles and tagged .NET , Java . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-07-19"},
{"website": "Octo", "title": "\n                Xcode 3.2.3 , IOS4 and OCMock            ", "author": ["Vincent Daubry"], "link": "https://blog.octo.com/en/xcode-3-2-3-ios4-and-ocmock/", "abstract": "Xcode 3.2.3 , IOS4 and OCMock Publication date 28/07/2010 by Vincent Daubry Tweet Share 0 +1 LinkedIn 0 To unit test our iPhone developments we rely heavily on Google-Toolbox-for-Mac and OCMock. After updating to the iphone SDK4 (IOS4) and Xcode 3.2.3 we had an unpleasant surprise : our testing frameworks refused to compile and as of today we have no information about how to solve this situation. Nevetheless we managed to resolve this problem : The build error we had was : Undefined symbols: \"_OBJC_CLASS_$_OCMockObject\", referenced from: objc-class-ref-to-OCMockObject in WebServiceTest.o ld: symbol(s) not found We used to simply add the OCMock.framework to the “Link Binary With Libraries” build phase. But it seems you can no longer use the pre-compiled library from the download section of the OCMock website To make OCMock work with Xcode 3.2.3 you have to  : Delete ocmock.framework, the copy file phase, and any search path linking to ocmock header. check out the latest version from ocmock svn repo : http://svn.mulle-kybernetik.com/OCMock/trunk Build the OCMockPhoneSim target Copy the libOCMock.a and Headers folder in your project folder Add the libOCMock.a to your project framework and add a link to the headers folder in your target library search path. You build again but…. nothing happens. After OCMock, now it’s the runscript from Google-Toolbox-for-Mac that freezes on this line : SBSetAccelerometerClientEventsEnabled failed: (ipc/ send) invalid destination port Once again check out the top of the trunk of the GTM svn repository and replace all the GTM files : http://code.google.com/p/google-toolbox-for-mac/wiki/iPhoneUnitTesting Now you can build your test target again. Tweet Share 0 +1 LinkedIn 0 This entry was posted in General -- DO NOT USE and tagged iPhone , OCMock , SDK4 , Testability , Unit Testing . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Xcode 3.2.3 , IOS4 and OCMock” Martin Gladdish 01/02/2011 à 14:30 Thanks for this - this error was making me tear my hair out! Annoyingly this problem still requires checking out and building OCMock from source even though there has been a release of OCMock since this blog post. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-07-28"},
{"website": "Octo", "title": "\n                How to rescue your data, 1/3            ", "author": ["Gabriel Guillon"], "link": "https://blog.octo.com/en/how-to-rescue-your-data/", "abstract": "How to rescue your data, 1/3 Publication date 26/11/2010 by Gabriel Guillon Tweet Share 0 +1 LinkedIn 0 I could show you the recipe on how to rescue your data and disks, but as it mainly depends on the way they are erased / crashed, I will show you in which particular case the rescue I expose can be made. Thus, in those 3 articles I’ll expose you three different ways of crashing data and disks, and the ways to rescue them. It serves another purpose : to show you how easy it is to lose data and disk. I’ve tested for you the three crashes. … Okay… Actually I successively crash my own personal data in those three ways… and finally rescued them three times. I live under GNU/Linux, so I crashed / erased under GNU/Linux (in a way, it’s easier to make mistake with GNU/Linux ;) ). You may have crashed / erased stuff with whichever OS you want, it’s not important, it’s for the example. I rescued everything using GNU/Linux, and this is important (in the same way, it’s easier to fix those mistakes with GNU/Linux) Please note that for the “screenshots” of this article I used a Linux VM (Ubuntu Server) in VirtualBox. So now let’s… Crash your partition table The partition table is located at the end of the first 512 bytes of the hard disk. It tells the system where the four primary partitions are located. The beginning of those 512 bytes is the boot loader. It’s used by the BIOS to boot the system, if told to do so with this disk. If the partition table is erased, the OS doesn’t know anymore where the partitions are. Thus you can’t access them anymore. Please note that the system can’t access them, but they are physically still present . If you want an analogy, it’s like losing the map of an unknown country : cities are still there but you don’t know where. Let say, for example, that you plan to rescue partition /dev/sdb1 (that have bad sectors). For safety reason, you want to save the partition table of /dev/sdb with sfidsk (8) (which have the ability to dump and later restore a partition table) You boot the computer with something like BackTrack , or System Rescue CD to perform your rescue. [root@home]# cd /dev/ # You forgot the option to use, so : [root@home]# man sfdisk # You read the man page, and find that this is -d. Let's try : [root@home]# sfdisk -d /dev/sdb # partition table of /dev/sdb unit: sectors /dev/sdb1 : start=       63, size=   192717, Id=83 /dev/sdb2 : start=   192780, size=   240975, Id= 7 /dev/sdb3 : start=        0, size=        0, Id= 0 /dev/sdb4 : start=        0, size=        0, Id= 0 # Let's go : [root@home]# sfdisk -d /deb/sdb > sdb Just when pressing Enter, you realize that you are in /dev and you just filled the first bytes of /dev/sdb with something that does not really look like a MBR or a computer readable partition table. Two possibilities : the output of sfdisk -d is small : your partition table has not been altered (only the bytes corresponding to the boot loader) and you can mount your partitions (actually, with this example above, it is the case :) ) as long as you don’t reboot , as Linux has loaded the partition table into memory. the output of sfdisk -d is large enough, and fdisk -l output something like : Disk /dev/sdb: 222 MB, 222298112 bytes 255 heads, 63 sectors/track, 27 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x7a697320 This doesn't look like a partition table Probably you selected the wrong device. Device Boot      Start         End      Blocks   Id  System /dev/sdb1   ?       76368      126628   403709618   20  Unknown Partition 1 has different physical/logical beginnings (non-Linux?): phys=(32, 32, 32) logical=(76367, 148, 54) Partition 1 has different physical/logical endings: phys=(32, 32, 32) logical=(126627, 27, 12) Partition 1 does not end on cylinder boundary. /dev/sdb2   ?          13          27      120487+   7  HPFS/NTFS Partition table entries are not in disk order In the latter case, your partition table is unreadable. Once rebooted, the OS won’t be able to access your partitions anymore. Another mean to crash the partition table is to play with tools like Partition Magic. How do you know you have crashed your partition table ? Just when your OS can’t see them anymore :) Rescue your partition table It’s time to introduce teskdisk (1). Testdisk is intended to fix exactly what you have broken : the partition table. To perform that, it scans the disk in search of special bytes and partition boundaries. In my case, I “just” crashed the primary partition table, but testdisk can be used if you have (un)intentionally erased a partition when playing with Partition Magic. Rescuing a partition table with testdisk is rather straightforward. Let’s go : [root@home]# testdisk /dev/sdb TestDisk 6.11, Data Recovery Utility, April 2009 Christophe GRENIER <grenier@cgsecurity.org> http://www.cgsecurity.org TestDisk is free software, and comes with ABSOLUTELY NO WARRANTY. Select a media (use Arrow keys, then press Enter): Disk /dev/sdb - 222 MB / 212 MiB - ATA VBOX HARDDISK [Proceed ]  [  Quit  ] Go on with ‘Proceed’ Please select the partition table type, press Enter when done. [Intel  ]  Intel/PC partition [EFI GPT]  EFI GPT partition map (Mac i386, some x86_64...) [Mac    ]  Apple partition map [None   ]  Non partitioned media [Sun    ]  Sun Solaris partition [XBox   ]  XBox partition [Return ]  Return to disk selection I suppose you use an Intel partition. So go on with it. [ Analyse  ]  Analyse current partition structure and search for lost partitions [ Advanced ]  Filesystem Utils [ Geometry ]  Change disk geometry [ Options  ]  Modify options [ MBR Code ]  Write TestDisk MBR code to first sector [ Delete   ]  Delete all data in the partition table [ Quit     ]  Return to disk selection You have lost partitions, so ‘Analyse’ is your choice. Current partition structure: Partition                  Start        End    Size in sectors 1 * Willowsoft OFS1      76367 148 54 126627  27 12  807419236 Warning: Bad starting cylinder (CHS and LBA don't match) 2 * HPFS - NTFS             12   0  1    26 254 63     240975 Only one partition must be bootable *=Primary bootable  P=Primary  L=Logical  E=Extended  D=Deleted [Quick Search]  [ Backup ] Validate on ‘Quick Search’ Should TestDisk search for partition created under Vista ? [Y/N] (answer Yes if unsure) Give the appropriate answer. Disk /dev/sdb - 222 MB / 212 MiB - CHS 27 255 63 Partition               Start        End    Size in sectors * Linux                    0   1  1    11 254 63     192717 P HPFS - NTFS             12   0  1    26 254 63     240975 Hey, testdisk has found your lost partitions :) Just press Enter. Partition                  Start        End    Size in sectors 1 * Linux                    0   1  1    11 254 63     192717 2 P HPFS - NTFS             12   0  1    26 254 63     240975 [  Quit  ]  [Deeper Search]  [ Write  ] Take a deep breath and press ‘Write’ Write partition table, confirm ? (Y/N) Light a candle and press ‘Write’ You will have to reboot for the change to take effect. Say yes , and quit testdisk (you’ll have to validate on ‘Quit’ several times) Actually, you don’t have to reboot if partprobe is installed. If it is, just type ‘partprobe’ Ask fdisk -l /dev/sdb what it thinks about your rescue : Device Boot      Start         End      Blocks   Id  System /dev/sdb1   *           1          12       96358+  83  Linux /dev/sdb2              13          27      120487+   7  HPFS/NTFS Congratulate yourself, and eventually give some money to the coders of testdisk . Conclusion Be careful when you are root and play with your disks and partition table. It’s very easy to make a mistake that will destroy a partition. Since your are making backups (isn’t it ?), this lost is not very harmful. But testdisk , which can find and restore lost partitions, can be a great help in some cases, and make you save time. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Linux , rescue , security , tips&tricks . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-11-26"},
{"website": "Octo", "title": "\n                Presentation of KNAPP’s KiSoft VISION            ", "author": ["Damien Joguet"], "link": "https://blog.octo.com/en/presentation-of-knapps-kisoft-vision/", "abstract": "Presentation of KNAPP’s KiSoft VISION Publication date 22/06/2010 by Damien Joguet Tweet Share 0 +1 LinkedIn 0 OCTO has been designing state-of-the-art IT architecture for more than 12 years. Recently we realized that user interfaces needed to be improved in order to bring more value to users. That’s why we now work on usability of IT systems. Add to that an interest for innovation processes and the will to partner with our clients  “from concept to cash”, and you will have a pretty good picture of the OCTO DNA. Those traits explain why we were particularly interested by this presentation of KiSoft VISION from KNAPP for order picking assisted by augmented reality. We’ve decided to invite KNAPP at OCTO to present their product and the process that led to it during one of our “Supply Chain Management School” session. We’ve also invited some of our clients to join us and discover what we view as part of a larger trend in the apparition of new types of user interfaces. This proposal was met with great enthusiasm both from Knapp and our clients. As a result, Birgit Huber and Peter Stelzer flew from Austria to Paris on June 17 to make a presentation in front of more than 25 people in OCTO premises. There are many different solutions to setup an order picking system from totally manual to fully automated (for example ASRS : Automatic Storage and Retrieval Systems). For manual picking, paper picking is still prevalent but error-prone, therefore there are different ways to assist the pickers. Voice directed picking is a pretty mature technology but it has some drawbacks : the need to translate the instructions, the impossibility to check that the picked item is the correct one, … In comparison, an augmented reality system should bring a “universal” interface and the camera attached to the system could help validate the picked items. However, that must not come with an increased cost for additional hardware in warehouses. Those are some of the constraints the KNAPP team had to address for the KiSoft VISION project. We discovered the predictions that Peter made when he launched the project 3 years ago when the technology was not ready yet and how those predictions became true (or not), how the system is designed (hardware and software), how the challenges were overcome, … He presented the architecture of the system, which is based on 2D-markers that a camera detects to guide the picker in the warehouse, highlight the position in the shelf where the items are to be picked and display the number of items to pick. Thanks to the camera, the system can validate that the barcode of the picked item is the correct one. Peter then demonstrated the product with a prototype, and presented the next steps for the technology. The session was followed by a buffet during which the participants could try the glasses and test the system with markers and tagged items. Two pictures will help you to better understand the system. Note that it’s still a prototype: in this version, the user sees the  environment thanks to screens inside the glasses. In the next version,  the user will see through the glasses and the information will be  inserted on the glasses (just like with Head Up Displays that can be  found in some cars). When “seeing” markers the system inserts virtual images to guide the    picking (during the demonstration the image was displayed on the  screen   for everyone to see). The item the camera is showing is the  small box   with a marker on the left hand side of the table, an arrow  hovers over   it to indicate that this is the item to pick. Many thanks to Birgit and Peter for the great presentation and their willingness to explain the details behind this great system ! When “seeing” markers the system inserts virtual images to guide the  picking (during the demonstration the image was displayed on the screen  for everyone to see). The item the camera is showing is the small box  with a marker on the left hand side of the table, an arrow hovers over  it to indicate this is the item to pick. Tweet Share 0 +1 LinkedIn 0 This entry was posted in News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-06-22"},
{"website": "Octo", "title": "\n                A maintainable Flex architecture            ", "author": ["David Rousselie"], "link": "https://blog.octo.com/en/a-maintainable-flex-architecture/", "abstract": "A maintainable Flex architecture Publication date 06/07/2010 by David Rousselie Tweet Share 0 +1 LinkedIn 0 With the Flex framework, we are able to quickly develop a GUI that works, especially through the MXML language. Indeed, this language is an effective way to describe the interface with a few lines of code. The problem occurs after the POC step is done, MXML code complexity increases, ActionScript code which implements event handlers, services calls or business logic creeps slowly into the MXML code. After some time, it becomes harder to figure out where data being displayed come from (ie. which code updated it ?), which code calls a service and from where the service arguments come from . In this post, we will see some good practices which help in keeping a high level of maintainability for Flex applications. Separation of concerns The first step is to separate concerns of the application code . Why ? Because by using a unique method to collect data (eg. from a form), to prepare them for a remote service call, to call that service and to update the view, it’s hardly testable thus finding a bug becomes very tricky. We will start by the data model which can be isolated in separate classes . With this, data to be updated will be easier to identify . For example, when the result of a service call arrives, received data will be placed in this data model. Then, you no longer need to worry about data updates, they are unique and are gathered in a single place : the model. We can create, for example, a model per screen of the application . Then, services calls can also be encapsulated in dedicated classes to avoid writing the same technical code 3 times in 3 different ways. It will allow to easily add common features to those services calls like authentication and/or exception handling. It will also be easier to replace these classes by mocks in our tests . Separation of concerns is not a new practice and is part of many GUI design pattern like MVC , MVVM or MVP , but it never hurts to remind ourselves of it from time to time. With Flex, we can find many frameworks like Cairngorm , PureMVC or Mate (and many others) which will help you to implement separation of concerns and thus improve the maintainability and testability of your application. Just keep in mind that these frameworks are not mandatory to get a good maintainability. Code organization rules are enough if they are defined and shared among the developers of the project. They are only a mean to apply those rules . Data propagation The second step is to define how data spreads across the application to ease its debugging (ie. to better understand where data comes from and where it goes) and to ease reuse and testability of written Flex components. The view of a Flex application is defined as a tree of components (MXML and ActionScript) like this : The tree nodes are containers components (*Box, Canvas, List, …) containing other simple components (Label, Button, …) or other containers. Even with a separation of concerns between models, services calls and the view, it’s still possible for any components to be binded directly to the data model and to call services. Thus, these components cannot be reused in any other context : they depend on data with which they are binded to and cannot be used to display any other data. To remove this limitation, you should inject data which these components are working with . It is the same for services calls, if a component is to be reused in an other context (which needs to call another service or not call any service at all), we need to remove the dependency to any service call (ie. the component won’t be aware a service call is done when some event occurs). A solution to these 2 problems of dependency is to push them to the parent component :). This parent component can then, either throw it back again to its parent (and add some information if needed), or be dependent to the data model and any services. Actually, at a certain level in the components hierarchy (by ascending it), it’s not possible to talk about reusability (this could be at the root component level) and having dependencies there is not a problem anymore . We then get a data propagation which looks like this : Data goes down the hierarchy via data binding (i.e. a component binds its attributes to its child components attributes) : In View1.mxml : <Componentx title=\"{modelView1.viewTitle}\" /> In ComponentX.mxml : <mx:Label text=\"{title}\" /> And data goes up through events dispatched by the lowest components in the hierarchy. These events are redispatched (with code or by using the bubbling mechanism which make them automatically ascend the hierarchy of components) or transformed into an event of higher level by parent components. In ComponentX.mxml : <mx:Button click=\"dispatchEvent(new AddItemEvent(this.item))\" /> In View1.mxml : <Componentx addItem=\"serviceProxy.addItem(event.item)\" /> This operation allows you to define a clear API of developed components : Exposed attributes receive data as an input Data goes out via events dispatched by the component It then eases the testability of developed components. Conclusion With these 2 principles applied to a Flex application, it should resist better to code size growth and be longer maintainable. However, we must keep in mind that these principles might not be sufficient and new ones should be elaborated with the context of the project in mind to ease code readability in its whole. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Architecture , Flex , RIA , Testability . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-07-06"},
{"website": "Octo", "title": "\n                “OCTO Talks!” now in english            ", "author": ["Olivier Mallassi", "Nelly Grellier", "Julien Jakubowski"], "link": "https://blog.octo.com/en/octo-talks-now-in-english/", "abstract": "“OCTO Talks!” now in english Publication date 06/04/2010 by Olivier Mallassi , Nelly Grellier , Julien Jakubowski Tweet Share 0 +1 LinkedIn 0 A couple of years after we started out this blog entitled “OCTO Talks!”, we are pleased to confirm the English version is now available. All the articles are not translated yet and as for the other published contents, the translation is dependent on the willingness of its authors, their wish to write them or not in English. We look forward to reading your comments soon! Tweet Share 0 +1 LinkedIn 0 This entry was posted in News . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-04-06"},
{"website": "Octo", "title": "\n                Let’s play with Cassandra… (Part 3/3)            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-33/", "abstract": "Let’s play with Cassandra… (Part 3/3) Publication date 15/06/2010 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 In this part, we will see a lot of Java code (the API exists in several other languages) and look at the client part of Cassandra. Use Case #0: Open and close a connection to any node of your Cluster Cassandra is now accessed using Thrift. The following code opens a connection to the specified node. TTransport tr = new TSocket(\"192.168.216.128\", 9160);\r\nTProtocol proto = new TBinaryProtocol(tr);\r\ntr.open();\r\nCassandra.Client cassandraClient = new Cassandra.Client(proto);\r\n...\r\ntr.close(); As I told previously , the default API does not provide any pool connections mechanisms that would have (1) the capacity to close and reopen connections in case a node has failed, (2) the capacity to load-balance requests among all the nodes of the cluster and (3) the capacity to automatically requesting another node in case the first attempt fails. Use Case #1: Insert a customer The following code insert a customer in the storage space (note that the object aCustomer  is the object you want to persist) Map<String , List< ColumnOrSuperColumn > > insertClientDataMap = new HashMap< string ,List<ColumnOrSuperColumn > >();\r\nList< ColumnOrSuperColumn > clientRowData = new ArrayList< ColumnOrSuperColumn >();\r\n\r\nColumnOrSuperColumn columnOrSuperColumn = new ColumnOrSuperColumn();\r\ncolumnOrSuperColumn.setColumn(new Column(\"fullName\".getBytes(UTF8),  \r\naCustomer.getName().getBytes(UTF8), timestamp));\r\nclientRowData.add(columnOrSuperColumn);\r\n\r\ncolumnOrSuperColumn = new ColumnOrSuperColumn();\r\ncolumnOrSuperColumn.setColumn(new Column(\"age\".getBytes(UTF8),  \r\naCustomer.getAge().toString().getBytes(UTF8), timestamp));\r\nclientRowData.add(columnOrSuperColumn);\r\n\r\ncolumnOrSuperColumn = new ColumnOrSuperColumn();\r\ncolumnOrSuperColumn.setColumn(new Column(\"accountIds\".getBytes(UTF8),  \r\naCustomer.getAccountIds().getBytes(UTF8), timestamp));\r\nclientRowData.add(columnOrSuperColumn); As you can read, the first line is in fact a Java representation of the structure: a map in which a row is identified by its key, and the value is a list of columns.  The rest of the code only create and append ColumnOrSuperColumn objects. Here, the columns have the following names: fullName, age, accountIds. You will also notice that when you create the column, you specify the timestamp the column is created. Remember that this timestamp will be used for “read-repair” and so that all your clients must be synchronized (using a NTP for instance) insertClientDataMap.put(\"customers\", clientRowData); The above lines put the list of Columns into the ColumnFamily named customers (so you can add several ColumnFamily in one time with the batch_insert method). Then, the following line inserts the customer into the Cassandra Storage. You need so to specify the keyspace, the row key (here the customer name), the Column family you want to insert and the Consistency Level you have chosen for this data. cassandraClient.batch_insert(\"myBank\", aCustomer.getName(), insertClientDataMap,  ConsistencyLevel.DCQUORUM); Use Case #2: Insert operations for an account Inserting an operation is almost the same code instead we are using SuperColumn. Map< string , List< ColumnOrSuperColumn > > insertOperationDataMap = new HashMap< string , List< ColumnOrSuperColumn > >();\r\nList< ColumnOrSuperColumn> operationRowData = new ArrayList< ColumnOrSuperColumn >();\r\nList< Column > columns = new ArrayList< Column >();\r\n\r\n// THESE ARE THE SUPERCOLUMN COLUMNS\r\ncolumns.add(new Column(\"amount\".getBytes(UTF8),  \r\naBankOperation.getAmount().getBytes(UTF8), timestamp));\r\ncolumns.add(new Column(\"label\".getBytes(UTF8),  \r\naBankOperation.getLabel().getBytes(UTF8), timestamp));\r\nif (aBankOperation.getType() != null) {\r\n\tcolumns.add(new Column(\"type\".getBytes(UTF8),  \r\naBankOperation.getType().getBytes(UTF8), timestamp));\r\n}\r\nFor now, there is nothing new. A list of Columns is created with three columns: amount, label and type (withdrawal, transfer...). \r\n// here is a superColumn\r\nSuperColumn superColumn = new  \r\nSuperColumn(CassandraUUIDHelper.asByteArray(CassandraUUIDHelper.getTimeUUID()),  \r\ncolumns);\r\nColumnOrSuperColumn columnOrSuperColumn = new ColumnOrSuperColumn();\r\ncolumnOrSuperColumn.setSuper_column(superColumn);\r\noperationRowData.add(columnOrSuperColumn); This case is different from the previous one. Instead of adding the previously defined Columns to the row, we create a SuperColumn with a dynamic (and time-based UUID) name…Quite dynamic isn’t it? Then, the three columns are added to the super column itself. The end of the code is similar to the previous one. The row is added to the ColumnFamily named operations and then associated to the current customer account id. // put row data dans la columnFamily operations\r\ninsertOperationDataMap.put(\"operations\", operationRowData);\r\ncassandraClient.batch_insert(\"myBank\", aCustomer.getAccountIds(),  \r\ninsertOperationDataMap, ConsistencyLevel.ONE); Here is what you get when reading the operations for the accounId Use Case #3 : Removing an item Removing a complete row is – in terms of API – as simple as the rest of the API. cassandraClient.remove(\"myBank\", myAccountId, new ColumnPath(\"operations\"),  timestamp, ConsistencyLevel.ONE); It is yet a little more complex when you are looking inside . In brief, in a distributed system where node failure will occur, you can’t simply physically delete the record. So you replace it by a tombstone and the “mark as deleted” record will be effectively deleted once the tombstone will be considered enough old. At least, you can still use “logical deletion” and write a code that do not use these flagged records. To (quickly) conclude this series of articles I really like Cassandra which looks like a ready to use tools (even if NoSQL is plenty of great tools) and a way to achieve high performance system at “low” (at least lower) cost than with commercial tools. There are still concerns I hope I will be able to discuss like security (Cassandra provides authentication mechanisms…), searching (or at least getting ranges of datas), monitoring (and how to monitor all the nodes of your cluster into a unique tools like Ganglia , Nagios or Graphite or even how to use Hadoop above Cassandra. To be continued… Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Cassandra , NoSQL . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 7 commentaires sur “Let’s play with Cassandra… (Part 3/3)” Rickou 28/09/2010 à 17:01 Thanks for this series of articles even if I have difficulties to write a Java program to get the operations inserted:\r\nwith this storage-conf.xml, how will look like the program ?\r\n\r\n\t\tSlicePredicate predicate = new SlicePredicate();\r\n\t\tpredicate.addToColumn_names(\"amount\".getBytes(UTF8));\r\n\r\n\t\tSystem.out.println(\"\\nrow:\");\r\n\t\tColumnParent parent = new ColumnParent(columnFamilyOperations);\r\n\t\tList results = client.get_slice(keyspace, accoundId, parent, predicate,\r\n\t\t\t\tConsistencyLevel.ONE);\r\n\r\n\r\nWrong ! this leads to an error: UUIDs must be exactly 16 bytes\r\n\r\nCan you publish a reader sample (different from cassandra-cli) ?\r\n\r\nThanks Olivier Mallassi 28/09/2010 à 22:45 Hi Rickou, \r\n\r\nI will be honnest, I wrote this code a long time ago :o)\r\nAs far I remember, I used time-based UUID (and there are 4 types of UUID).\r\nby default, Java do not support time based UUID http://download.oracle.com/javase/1.5.0/docs/api/\r\n\r\nSo I used to lib to manage my UUID : http://johannburkard.de/software/uuid/\r\n\r\nhope this helps! Olivier Mallassi 12/01/2011 à 10:51 Couple of articles talking about enhancement in version 0.7\r\n- Column expiration : http://www.riptano.com/blog/whats-new-cassandra-07-expiring-columns\r\nYou can add a TTL on your column (like HBase)\r\n\r\n- Secondary indexes : http://www.riptano.com/blog/whats-new-cassandra-07-secondary-indexes\r\nYou can so have query like : \r\nget users where state = 'UT' and birth_date > 1970;\r\n\r\nIf you have well defined your columnFamily (with the columns state and birth_date)\r\n\r\n- Live schéma update : \r\nhttp://www.riptano.com/blog/whats-new-cassandra-07-live-schema-updates\r\n\r\n\"Cassandra has always been schemaless within a ColumnFamily, in the sense that columns may be created at will simply by using them in a row. ColumnFamilies themselves, however, and Keyspaces, have to be explicitly defined before use (so Cassandra knows how to index the columns within their rows). \"Live schema updates\" refer to this ability to create, rename, and remove both Keyspaces and ColumnFamilies in a live cluster, and were added early in the 0.7 development cycle in CASSANDRA-44. Prior to 0.7, changing the schema meant editing the configuration file for every node and then manually executing a rolling restart of the cluster, which afforded the opportunity for humans to make mistakes.\" Ekrem SABAN 23/03/2011 à 16:54 A short question concerning the code. Can you elaborate a little the strange syntax in\r\n\r\nHashMap<string ,List>();\r\nList clientRowData = new ArrayList();\r\n \r\nabove? It looks like a \"mirrored\" XML tag.\r\n\r\nThank you! Olivier Mallassi 24/03/2011 à 09:17 @Ekrem, the text editor has mirrored the \"generics\"...\r\nanyway, I tried to fix it in the article but needed to use extra white space inside the generics.... William Sharp 19/04/2011 à 22:00 Great post, great series.  I loved it! Lee 22/05/2011 à 20:48 Excellent article.  Thanks. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-06-15"},
{"website": "Octo", "title": "\n                Let’s play with Cassandra… (Part 1/3)            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13/", "abstract": "Let’s play with Cassandra… (Part 1/3) Publication date 09/06/2010 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 I have already talked about it but NoSQL is about diversity and includes various different tools and even kind of tools. Cassandra is one of these tools and is certainly and currently one of the most popular in the NoSQL ecosystem. Built by Facebook and currently in production at web giants like Digg, Twitter, Cassandra is a hybrid solution between Dynamo and BigTable . Hybrid firstly because Cassandra uses a column-oriented way of modeling data (inspired by the BigTable) and permit to use Hadoop Map/Reduce jobs and secondly because it uses patterns inspired by Dynamo like Eventually Consistent, Gossip protocols, a master-master way of serving both read and write requests … Another DNA of Cassandra (and in fact a lot of NoSQL solutions) is that Cassandra has been built to be fully decentralized, designed for failure and Datacenter aware (in a sense you can configure Cassandra to ensure data replication between several Datacenter…). Hence, Cassandra is currently used between the Facebook US west and east coast datacenters and stored (around two years ago) 50+ TB of data on a 150 node cluster . Data Modeling The column oriented model is quite more complex to understand than the Key/Value model . In a Key/value model, you have a key that uniquely identify a value and this value can be structured (based on a JSON format for instance) or completely unstructured (typically a BLOB). Therefore and from the basis, the simplest way to understand the column-oriented model is to begin with a Key/Value model and imagine that the Value is a collection of other Key/Value elements. In brief, this is a kind of structure where Hashmaps are included in another Hashmap… Completely lost? Here are a the main elements Cassandra defined (this article or the Cassandra documentation provide a more in depth view of the different types of structures you can have in Cassandra) – Column . The basic element which is a tuple composed of a timestamp, a column name and a column value. The timestamp is set by the client and this has an architectural impact on clients’ clocks synchronization. – SuperColumn . This structure is a little more complex. You can imagine it as a column in which can store a dynamic list of Columns. – ColumnFamily : A set of columns. You can compare a ColumnFamily to a table in the relational world except the number and even (I am not sure this is the best idea but anyway) the names of columns can vary from a row to another. More important, the number of columns may vary during time (in the case for instance your schemas need to be upgraded etc…). Cassandra will not force any limitation in that case but your code will have to deal with these different schemas. – KeySpaces : A set of ColumnFamily. Hence, the Keyspace configuration only defines (from the data modeling concern) the included ColumnFamily. The notion of “Row” does not exist by itself: this is a list of Columns or SuperColumns identified by a row key. Data Partitioning and replication Data Partitioning is one of tricky point. Depending on the studied tools, partitioning can be done either by the client library or by any node of the cluster and can be calculated using different algorithms (one of the most popular and reliable being the Consistent Hashing . Cassandra lets the nodes of the cluster (and not the client) partitioning the data based on the row key. Out of the box, Cassandra can nevertheless use two different algorithms to distribute data over the nodes . The first one is the RandomPartitionner and it gives you an equally and hash-based distribution. The second one is the OrderPreservingPartitioner and guarantees the Key are organized in a natural way. Thus, the latter facilitates the range queries since you need to hit fewer nodes to get all your ranges of data whereas the former has a better load balancing since the keys are more equally partitioned across the different nodes. Then, each row (so all the Columns) is stored on the same physical node and columns are sorted based on their name. Moreover, and this is more linked to data replication, Cassandra natively enables replication across multiple datacenters and you can – by configuration – specify which nodes are in which Data Center. Hence, Cassandra would take care of replicating the data on at least one node in the distant Data Center (the partition-tolerant property of the CAP Theorem is so meaning full and I guess simpler to understand…). Consistency management, conflict resolution and atomicity NoSQL solutions like Voldemort or Cassandra have chosen Availability and Partition-tolerance over Consistency . Thus, different strategy have been setting up: “Eventually Consistent”. Dealing with consistency is so a matter of dealing with data criticism and being able to define –for each data – the consistency level you need (based on the trade-offs the CAP Theorem imply). Cassandra defines different levels of consistency and I will not go into further details but here are a couple of them: – ONE . Cassandra ensures the data is written to at least one node’s commit log and memory table before responding to the client. During read, the data will be returned from the first node where it is found. In that case, you must accept stale state because you have no warranty the node you hit to read the data has the last version of the data. – QUORUM . In that case, Cassandra will write the data on < replicationfactor >/2 + 1 nodes before responding to the client (the Replication factor is the number of nodes the data will be replicated and is defined for a Keyspace). For the read, the data will be read on < replicationfactor >/2 + 1 nodes before returning the data. In that case, you are sure to get a consistent data (because N is smaller than R+W where N is the total number of nodes where the data is replicated, R the number of nodes where this data is being read and W the number of nodes the data is being written) – ALL . In that case, Cassandra will write and read the data from all the nodes. Of course, at a given time, chances are high that each node has its own version of the data. Conflict resolution is made during the read requests (called read-repair ) and the current version of Cassandra does not provide a Vector Clock conflict resolution mechanisms (should be available in the version 0.7). Conflict resolution is so based on timestamp (the one set when you insert the row or the column): the higher timestamp win and the node you are reading the data is responsible for that. This is an important point because the timestamp is specified by the client, at the moment the column is inserted. Thus, all Cassandra clients’ need to be synchronized (based on an NTP for instance) in order to ensure the resolution conflict be reliable. Atomicity is also weaker than what we are used to in the relational world. Cassandra guarantees atomicity within a ColumnFamily s o for all the columns of a row . Elasticity “Cassandra is liquid” would have written any marketer and to be honest, a lot of NoSQL solutions have been built upon this DNA. First of all, elasticity is at the data modeling level. Your data will live longer than your business rules and softness in the way your data schemas can evolve across the time is an interesting point . But elasticity is also about infrastructure and cluster sizing . Adding a new node to Cassandra is simple. Just turn on the AutoBootstrap property and specify at least one Seed of the current cluster. The node will hence be detected, added to the cluster and the data will be relocated (the needed time depends on the amount of data to transfer). Decommissioning a node is almost as simple as adding a node except you need to use the nodetool utility (which provides more options to visualize the streams between the nodes…) or a JMX command. Cassandra monitoring Cassandra runs on a JVM and exposes JMX properties. You can thus collecting monitoring information using jConsole or any JMX compliant tool. For instance, you can monitor – Your nodes (which are part of the cluster, which are dead…) – the streams between your nodes (especially in the case you added or removed nodes) – Or stats. For instance, per-Column Family basic stats would be: Read Count, Read Latency, Write Count and Write Latency etc… More JMX plugins are available here (provides graphs…) and some guys are developing web console . Access protocol Cassandra is called using Thrift (even if the 0.6 version introduced Avro protocol). As we told previously, Cassandra is responsible for routing the request to the proper node so you can reach any node of your cluster to serve your request. Nonetheless, the default thrift client API does not provide any load-balancing or connection pool mechanisms . Concerning the connection pool, the main lacks, in my opinion, are (1) the capacity to close and reopen connections in case a node has failed, (2) the capacity to load-balance requests among all the nodes of the cluster and (3) the capacity to automatically request another node when the first attempt fails. A higher level of load-balancing could be setup on the service layer Certain library (for instance Hector in the Java World) provides connections pooling mechanisms and even kind of round-robin load balancing… In the next part, we will play in more details with Cassandra… Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Cassandra , NoSQL . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “Let’s play with Cassandra… (Part 1/3)” Olivier Mallassi 09/10/2010 à 09:40 Cassandra & ec2 perf. \r\nhttp://www.coreyhulen.org/?p=326|performance\r\n\r\nCassandra & munin\r\nhttp://www.coreyhulen.org/?p=277|monitoring] Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-06-09"},
{"website": "Octo", "title": "\n                Unit of work, Transactions and Grails            ", "author": ["Cyril Picat"], "link": "https://blog.octo.com/en/transactions-in-grails/", "abstract": "Unit of work, Transactions and Grails Publication date 22/06/2010 by Cyril Picat Tweet Share 0 +1 LinkedIn 0 Working with Groovy and Grails often gives you the feeling that things are magic and when you dive in, you realize that things are more complex than expected. At the same time, you often realize that a reasonable default behavior has been chosen by Groovy/Grails framework: What about transactions’ magic in Grails? For me it was hard to believe so let’s try to understand a little more how things work. For this, Grails documentation is a bit sparse but you had better go and read it to know how and where to declare transactions. We won’t talk about these details here, you can find them in the documentation . A last remark if you want to play with the different examples, you can find the code in our SVN repository here . The project also includes tests to illustrate the different issues illustrated below. It’s not that magic Imagine a classical example, a banking application. You have built your application incrementally and at a few places you have put some business code in your controllers’ actions because the code was too simple to refactor it in a service. You might come with some code similar to this one: AccountController.groovy class AccountController {\r\n\r\n\tdef transfer = {\r\n\r\n\t\tdef fromAccount = params?.fromAccount\r\n\t\tdef toAccount = params?.toAccount\r\n\t\tdef amountToTransfer = params?.amount\r\n\r\n\t\t// some validity checks, could be performed through a Command Object class\r\n\t\t...\r\n\r\n\t\t// withdraw from the source account\r\n\t\tfromAccount.balance -= amountToTransfer\r\n\t\tfromAccount.save(flush:true)\r\n\r\n\t\t// some logic that raises an error or throws an exception\r\n\t\t...\r\n\r\n\t\t// transfer to the target account\r\n\t\ttoAccount.balance += amountToTransfer\r\n\t\ttoAccount.save(flush:true)\r\n\t}\r\n} In this case, what happens if any error or exception occurs in the ‘some logic’ block? The transfer has been partially performed and committed in the database and a few hundred dollars is lost in the wilderness. If you know Grails well you could answer that the ‘flush:true’ is, in this particular case, useless and that without it, everything would have worked, and you would be right (see section ‘Hibernate Session in the equation…’ below ). But let’s stick to this example for its simplicity, because there are also more complex cases where the flush of the session is automatically performed by Hibernate. To remember The important thing to remember is that Grails does no automatically wrap each ‘request’ (treatment associated with a HTTP request) in a transaction, so you are not as safe as expected. At this point you should ask yourself: what is the correct way to handle transactions in Grails? Grails Model – Business Logic in Services And you are right. Grails paradigm is to avoid putting business code in the Controller layer but rather in the model or service layer. That’s why all Grails magic for making transactions transparent only happens if you are sticking to this paradigm. Imagine you refactor the code below in a service (grails-app/services directory): TransferService.groovy class TransferService {\r\n\r\n\tdef transfer(fromAccount, toAccount, amount) {\r\n\r\n\t\t// withdraw from the source account\r\n\t\tfromAccount.balance -= amountToTransfer\r\n\t\tfromAccount.save(flush:true)\r\n\r\n\t\t// some logic that raises an error or throws an exception\r\n\t\t...\r\n\r\n\t\t// transfer to the target account\r\n\t\ttoAccount.balance += amountToTransfer\r\n\t\ttoAccount.save(flush:true)\r\n\t}\r\n} and that you use the standard Grails mechanism to inject this service in AccountController: AccountController.groovy class AccountController {\r\n\r\n\tdef transferService\r\n\r\n\tdef transfer = {\r\n\r\n\t\t...\r\n\r\n\t\t// call service to make the transfer\r\n\t\ttransferService.transfer(fromAccount, toAccount, amountToTransfer)\r\n\t}\r\n} This time you will get the expected behavior which is, the balance of the first account won’t be modified and committed if an error happens after the first save(flush:true). Why? Here comes Grails magic. by default every call to a Service’s method is made transactional. How does this work? Using a combination of Spring IoC for injecting an instance of a service in the controller and using the Proxy pattern to make sure the call is enrolled in a transaction. This behavior is controlled by the Service property ‘transactional’ which is set to true by default. So if you modify you service code to : class TransferService {\r\n\tstatic transactional = false\r\n\r\n\t...\r\n} you will go back to the phony behavior. There are a few pitfalls you should avoid when using services, which are: do no create instances of the service, always use injection. If you do: class AccountController {\r\n\r\n\tdef transferService = new TransferService()\r\n\t...\r\n} In this case the calls to TransferService won’t be proxied so you won’t get any transactional behavior. do not use closures in services, always use methods If you were declaring transfer() as a closure like: class TransferService {\r\n\r\n\tdef transfer = { fromAccount, toAccount, amount ->\r\n            ...\r\n\t}\r\n} In this case the call won’t be caught by Spring AOP so it won’t be enrolled in a transaction. Lastly, take great care of what exception you throw because checked exception do not cause the transaction to roll back by default. In Groovy this could be pretty confusing because you are not forced to declare or catch checked exception. To avoid this, you can configure which exceptions cause a roll back at Spring level (using rollbackFor in the @Transactional annotation for example, see this part of Spring documentation). To remember use services to factor common business logic AND handle correct transaction behavior always use injection to call a service do not use closures to define a service method use runtime exceptions if you want to roll back a transaction There are other mechanisms to make use of transactions in Grails, mainly through the withTransaction() method but it is not the recommended approach. You could also have a more fine-grained transaction management if you use Spring @Transaction annotations, but you would leave the magical world in this case. I am putting all my business code in services, am I safe? Actually you might. But you better check and understand better the transaction model. Let me show you an example which surprisingly doesn’t work: class AccountController {\r\n\r\n\tdef transferService\r\n\tdef auditService\r\n\r\n\tdef transfer = {\r\n\r\n\t\tdef fromAccount = params?.fromAccount\r\n\t\tdef toAccount = params?.toAccount\r\n\t\tdef amountToTransfer = params?.amount\r\n\r\n\t\t// some validity checks, could be performed through a Command Object class\r\n\t\t// ...\r\n\r\n\t\t// call service to make the transfer\r\n\t\ttransferService.transfer(fromAccount, toAccount, amountToTransfer)\r\n\r\n\t\t// notify the auditing plateform of the transfer\r\n\t\tauditService.notifyTransfer(fromAccount, toAccount, amountToTransfer)\r\n\t}\r\n} So what? Imagine the notifyTransfer() call fails (audit server might be down), an exception is then thrown and the transaction is rolled back. But the question is: which transaction? Because actually there are two different transactions in this transfer() action. One around the transferService() and one around the notifyTransfer() call. Oops! Which takes us to our initial finding: it’s not that magic… Why are there two transactions? We won’t talk long about this but to make the story short, the default transaction propagation on services’ methods is REQUIRED (see this doc ) so a new transaction is created when entering transfer() and committed when exiting transfer(). The same applies to the notifyTransfer() call. See picture below for a better understanding of what is going on: Figure 1 Doc’, what should I do? The obvious answer is to stick to Grails paradigm and make sure your unit of work is always encapsulated in the same transaction. In the case above, the natural way of solving it would be to put the notifyTransfer() call in the transferService.transfer() service call. If this breaks your object encapsulation, you will have to create a new service that will take care of calling both transfer() and notifyTransfer(). To remember Stick to Grails paradigm of putting transactional code in the services Always put your unit of work in a top-level service, aka don’t call two services in a controller action Hibernate Session in the equation… When Hibernate session comes into the game, everything mentioned above becomes true or false. As a quick reminder, the session management in Grails is based on the Open Session In View pattern (OSIV, see this doc ). The thing to remember is that Hibernate Session is not flushed if an exception is thrown which is the reason why we put save(flush:true) in the first example. This enables you to put read/write code in your controller action without fearing of having an inconsistent state in the database but it is actually a false sense of security. Indeed, this feature falls apart when your are calling services because committing a transaction flushes your Hibernate session. Or when you really have to flush the session because you need to. Another pitfall occurs if you read a state in your controller action (thus outside of any transaction) and you then call a transactional service. You then loose the isolation level defined at your transaction level, which is for sure not what you wanted at first. To remember It is OK to rest on Hibernate session and to put simple logic in your controller, as long as you don’t call any service in your controller. You should not load objects outside of a transactional service and use it as an argument provided to the service Quest for a better world… Isn’t there any easier way, any more magic, that could turn transaction management in a really reckless job, at least in Grails? One solution could be to use the ‘one request one transaction’ pattern, in the same way Grails use the OSIV pattern. The trick would be to open the transaction before the controller action is called, and to commit it after the action returns (but before the view is rendered). One way to do this could be to add a filter taking care of starting and committing the transaction. For example, you could add a TransactionFilters in grails-app/conf directory: TransactionFilters.groovy class TransactionFilters {\r\n\r\n\tdef sessionFactory\r\n\r\n\tdef filters = {\r\n\r\n\t\tstartTransaction(controller:'*', action:'*') {\r\n\t\t\tbefore = {\r\n\t\t\t    sessionFactory.getCurrentSession().beginTransaction()\r\n\t\t\t}\r\n\t\t}\r\n\r\n        // no need for an 'after' filter, Spring take care of committing or rolling back\r\n        // the transaction\r\n\r\n\t}\r\n\r\n} With this filter in place, the schema shown in Figure 1. will become: Figure 2 What impact does it have? This would definitively have a very small impact on performance as the ability to make use of read-only transactions or non-transactional requests would be lost. But in my opinion the performance gain is so minor that you would definitively stop using Groovy and Grails if you really had such a performance critical application in production. Of course all this is not applicable to the batch processing part of your application. In that case, the transaction management will be completely different and not tied to any HTTP request, but this is out of the scope of this article. One last point: even if this filter does simplify transaction management, it’s always better to have a good understanding of how transactions work and not just to rely on too much magic. That’s it, I hope I did not lose you on the way. And feel free to leave a comment and ask for clarification. Follow @cyrilpicat Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged development , Grails , transaction . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 10 commentaires sur “Unit of work, Transactions and Grails” Shafiul 23/06/2010 à 07:59 Thanks for this great article. I found no good guide about transaction in grails and was confused. After reading your article, now I have clear idea about transaction management in grails. faenvie 29/06/2010 à 08:47 clear and very helpful summary ... \r\nthis info should make it into the \r\ngrails-documentation.\r\n\r\nand i'll welcome your article \r\nabout batch-processing with grails\r\n;-) \r\n\r\nthanx Diego Bendlin 10/08/2010 à 21:46 Great Article,\r\n\r\nI was wondering if there's a way to change the default transaction propagation on service methods, on a per method basis, or if not possible at least on a service basis.\r\n\r\nFor example, ServiceA could have 2 methods method1 and method2, a would like to tell grails (or hibernate, or both) to use Propagation.REQUIRES_NEW for method1 and stick with Propagation.REQUIRED for method2.\r\n\r\nIs this possible?\r\n\r\nThanks in advance,\r\n\r\nRegards,\r\n\r\nDiego Bendlin Cyril Picat 11/08/2010 à 10:42 @Diego, thanks for your feedback.\r\n\r\nYou can use Spring @Transactional annotation to do what you want. Basically, you can use it on a per-method basis or on a per-class basis:\r\n\r\n\r\nimport org.springframework.transaction.annotation.*\r\n\r\nclass ServiceA {\r\n   @Transactional(propagation = PROPAGATION_REQUIRES_NEW)\r\n   def method1() {\r\n      [...]\r\n   }\r\n   // this method is PROPAGATION_REQUIRED by default\r\n   def method2() {\r\n      [...]\r\n   }\r\n}\r\n\r\nor on a per-class basis:\r\n\r\n@Transactional(propagation = PROPAGATION_REQUIRES_NEW)\r\nclass ServiceA {\r\n\r\n}\r\n\r\nSee Grails doc §8.1 'Custom Transaction Configuration' in \r\nhttp://grails.org/doc/latest/ or related section in Spring documentation http://static.springsource.org/spring/docs/3.0.x/spring-framework-reference/html/transaction.html#transaction-declarative-annotations. Nirav Assar 20/09/2010 à 15:10 Fantastic arcticle and vert clearly explained. You should make this a series and explain how best to use transaction = false and withTransaction in certain situations. Tarun Arora 04/11/2010 à 22:20 Thanks for the Great Article.\r\n\r\nI was just thinking to get rid of the Transactions magic in Grails and manage the transactions in service layer as this will give more control over the service layer. This article pushes me towards the said approach.\r\n\r\nCan we also configure whether the Hibernate Session in GORM is Thread Scoped or JTA scoped? Predrag Knezevic 16/02/2011 à 13:53 I wanted to apply the proposed approach in one of my projects, but I have realized that transactions are not rolled back if RuntimeException (and perhaps some others too) occurs somewhere in code. Hence, I have applied another idea - I wrote a plugin that wraps controller's actions in transactions, just like  withTransaction dynamic method does it.\r\n\r\nThe further instructions and plugin can be found here:\r\nhttp://grails.org/plugin/transactional-controller Cyril Picat 04/03/2011 à 15:04 thanks for the pointer, Predrag.\r\n\r\nOn what version of Grails did you try this? I can confirm that a RuntimeException was rolling back transactions in Grails 1.1.2. Maybe there is an issue with more recent versions, I haven't had the opportunity to test this against a more recent version.\r\n\r\nDid you try to see if the filter was correctly called? Cyril Picat 04/03/2011 à 15:32 @Tarun, I must say I did not completely get your last question.\r\n\r\nMy understanding is that Grails uses the Open-Session-In-View pattern so the session is request-scoped rather than transaction-scoped. Out of the box in Grails, you can have multiple transactions in one request. Of course, the session is flushed before a transaction is committed.\r\n\r\nBut what do you mean by 'is the Hibernate Session Thread-Scoped'? If you think in terms that a thread handles a request, I guess we could say yes. But be aware that the thread is not stopped but re-used in the application server, while the session is not. David 27/07/2012 à 14:05 Useful, and well explained.\r\n\r\nThanks Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-06-22"},
{"website": "Octo", "title": "\n                no:sql(eu) and NoSQL: What’s the deal?            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/nosqleu-and-nosql-what%e2%80%99s-the-deal/", "abstract": "no:sql(eu) and NoSQL: What’s the deal? Publication date 22/04/2010 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 When you took a look at the scheduled speakers, no:sql(eu) in London itpromised to be a fantastic event but this was not taking into account the fact that a volcano deep down in Iceland… NoSQL being about (among other things) availability even in the case of disaster, NoSQL.eu degraded gracefully and eventually took place in a pretty efficient way, with speakers giving their talks remotely, mainly from USA (sometimes very early in the morning due to time differences). This was also an opportunity to meet and discuss with Werner Vogels. Anyway, here is what I will keep in mind after these 2 days in London: – NoSQL is about data modeling . The presentation of the Guardian introduced this concept perfectly: Oracle is still used for the core part because it fits our current needs, Redis is used for statistics computation, Google Spreadsheets are used to be more easily mashed-up with Google Maps. One dedicated storage solution for each type of data. Column Oriented, Document, Key/Value and Graph databases provides many ways of modeling and storing data. – NoSQL is about data usages and reminds us that the most important – and I guess the forgotten part in our traditional systems – is to understand how data is used. A system can live with inconsistent data (in fact this is already the case if you are using database Master/Slave replication, caching or asynchronous approaches). This is not a big deal in most of the cases but we need to keep that in mind (I must admit that we, as architects, often forget about this concern since we have been using RDBMS and ACID transactions for a long time now). What happens for the business if an operation (let’s say a withdrawal) is consistent but my balance stays inconsistent a couple of seconds or even a couple of minutes? Will my data be accessed in a highly concurrent manner and so do I have a high probability to get conflicts on that data? In case of conflicts, do I need complex conflict resolutions strategies for read-repair? more specifically, do I need a Vector Clock model or a much simpler but also efficient mechanism based on timestamps like in Cassandra? How many revisions of my objects do I need to keep? For instance, Riak stores revisions of  objects that can be used for conflicts resolutions but the number of revisions you need to keep have to be correlated to the probability of having a conflict on that data. How much time do I have between the moment my data is written and the moment that same data is read? And what is the probability of being in an inconsistent state? If there is one hour between the write and the read, the probability is quite low. Is my data mainly used for reads or writes ? Some of these systems have been optimized for writes and less for reads. For instance Cassandra does many more disk accesses during a read than during a write – NoSQL is about softness and elasticity : Elasticity  meaning “being able to add or remove more and more machines as you go”, Softness meaning “having a loose data schema that will be able to evolve”. With neo4j , relationships between nodes can evolve, properties of the nodes or relationships can change. To put it simple, your graph changes the way you use it… In Key/Value stores like Voldemort or Column Oriented solutions like Cassandra, your model can evolve (add an attribute…) and storage will deal with it transparently. The next version  (0.7) of Cassandra should implement a feature allowing dynamic and rolling deployment of new ColumnFamily around all the nodes of the cluster. – NoSQL is about diversity . The NoSQL ecosystem is rich and composed of many different tools. Some of these tools are centered on simplicity and ease of use. MongoDB for instance provides a simple way for modeling data (in document and JSON style) with rich query mechanisms. Other solutions are focused on performance and scalability. Riak, Cassandra and Voldemort (which unfortunately was not present and I regret it because this solution is also used on e-commerce sites ) are the best examples of these kind of solutions. Some are focused on alternative modeling or even modeling that are impossible to achieve in a relational model – Graph Databases and Neo4j are the best examples. Neo4j can traverse a graph of 1 million nodes within less than 2ms on commodity hardware… – NoSQL is about collaboration . This conference was offering the opportunity to see different approaches and how these approaches can be mixed together. For instance, you can perfectly have a system where a part of the data is stored and queried through Neo4j (imagine the value of adding relationships between metadatas…) with the objects themselves being stored in a Cassandra data store – NoSQL is about Datawarehousing . Kevin Weil from Twitter talked about Cassandra of course but also talked a lot about how they use the 300Go of logs the site generates every hour. Scribe is used to collect data. Hadoop is used to analyze all that data through Map/Reduce (the number of tweets – 12 billions – are counted in around 5 minutes). Pig (a kind of high level language that democratizes data querying) is used to harvest that data. That kind of logging base is not only used for marketing purposes but also to understand technical problems… – NoSQL is more about availability than necessarily about performance or scalability at the level Amazon or Twitter have reached. Jon Moore tells us that NoSQL is about availability and concurrency access of the data especially in multi-data center contexts. His talk reminded us about the CAP theorem and what partition-tolerant really means in the case of a complete datacenter failure and how to deal with CP or AP in that case. – NoSQL can be about query . Of course Key/Value or Columns-oriented approaches have limited query capacities but graph databases like neo4j provide a rich way of looking for data (on nodes or relationship properties). Document databases like mongoDB or couchDB provide a way of finding, filtering data based on its attributes. Of course they are some common drawbacks and limitations for these NoSQL solutions. – Key management is a critical aspect, since it is the entry point (at least for the key/value and Column oriented systems) to access your data. You can either use large random generation key or add more semantics to the key and try to look for natural uniqueness of the key. Functional keys approaches are back… – The human and expertise aspects again….These systems bring new paradigms. I am sure we will discuss about that another time but we, as developers, have to learn how to develop with these new paradigms. And we, as operations, have to learn how to operate and supervise these systems. Relational Databases are well known and mature mainly due to their age: over 40 years of improvements bring you the guarantee that these systems are stable and reliable! – Supervision is certainly the weakest part of these solutions which provide only few “out of the box” things. We must nonetheless notice that Riak (which is sponsored by Basho) started developing statistics tools using SNMP standards (the future will tell us if these tools will be available in the open source version). Cassandra is currently providing some statistics using JMX, which allows us to use jConsole. Moreover, JMX is supported by a lot of other tools and you can already aggregate information coming from several Cassandra nodes in tools like graphite . To put it in a nutshell, NoSQL is about alternatives : using the most appropriate tool for each specific task. I can hear you thinking “Nothing new!” and you are right except that we have never done this in the database world which has been dominated by 40 years of Relational Databases supremacy …And moreover, NoSQL is also about Cloud computing (SimpleDB, S3…) but that’s another story. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged Cassandra , Column-oriented datastore , Graph Databases , Key/Value Datastore , MongoDB , Neo4j , NoSQL , nosql europe , Riak , Voldemort . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Un commentaire sur “no:sql(eu) and NoSQL: What’s the deal?” Olivier Mallassi 08/10/2010 à 09:49 CAP Theorem http://codahale.com/you-cant-sacrifice-partition-tolerance/\r\n\r\nOne of the most clearest article about CAP Theorem. In short : \"Despite your best efforts, your system will experience enough faults that it will have to make a choice between reducing yield (i.e., stop answering requests) and reducing harvest (i.e., giving answers based on incomplete data). This decision should be based on business requirements.\" Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-04-22"},
{"website": "Octo", "title": "\n                Does Alfresco fit your needs?            ", "author": ["Rémy Saissy"], "link": "https://blog.octo.com/en/does-alfresco-fit-your-needs/", "abstract": "Does Alfresco fit your needs? Publication date 30/04/2010 by Rémy Saissy Tweet Share 0 +1 LinkedIn 0 This article is the English translation (human made, not automatic) of what I published on the French version of this blog few a months ago. It talks about Alfresco 3.1. These days, we hear a lot about collaboration, 2.0 company, wiki, … and also of Alfresco. Alfresco is an Enterprise Content Management system (ECM). It is a free software, it has a big community and its software architecture is close to Documentum but with more recent technologies. At first glance, it is a very attractive and flexible solution but as you know, the perfect software doesn’t exist and that’s why I propose you a review of Alfresco according to some common expectations you would have. The objective is to provide you a first glance of what Alfresco really is. I wrote article not with the intention of selling you or preventing you from using Alfresco, but in giving you  a pragmatic overview of the product. Feel free to contact me and/or to leave a comment. My data need to be stored in a well structured manner If you only need it, use an NFS or a CIFS share. If you also need some smart processing on your data, Alfresco could answer to your need because it is possible  to configure a sequence of actions triggered by adding, deleting and/org modifying data on the repository. Some of the actions can be but not limited to  coping a file, sending an email to the owner of the file or even converting it to another format. Of course you can also extend the mechanism with your own actions. A knowledge base must be built upon my data Sounds good, Alfresco is, after all, an Enterprise Content Management system! All the data stored in Alfresco is uniquely identified in the physical storage. Deleting something in a repository doesn’t physically destroy the content of the data, the system moves it in quarantine… You can also parameter the number of days that data is kept which is useful in preventing people from accidentally deleting important informations. My data are handled by complex workflows Alfresco uses JBPM , a mature workflow engine. This enables it to support workflows from simple review-and-validate to complex ones designed with jBPM Process Designer. Last but not least, the trigger mechanism on the repository allows you to transform, move, copy, send by email, … any data stored on Alfresco. My data have a lot of metadata Data stored in Alfresco are all handled by a data model. Any stored data, users, permissions, documents, directories, … are all bound to a data model. I wrote ‘a’ and not ‘the’ because data models can be extended in order to fit your needs. Therefore it is possible and simple to add custom metadata to let’s say, pictures or office documents. Also, out of the box, Alfresco provides several data models like: data types for directories, pictures, documents; data types for tags; a data model for permissions; a data model for workflows; … Adding metadata is at the root of Alfresco’s content management architecture. My access rights are handled in a fine grained way If default access rights doesn’t fit you, you can create your own permission model or extend the existing one. You may also want to know that permissions are handled per file, per directory and that quotas can be applied. Also, Alfresco uses roles. But what is a role? It is basically a set of permissions. You bind role to a user or to a group of users per file or per directory. By default, the following roles are provided, feel free to extend it: administrator; coordinator; collaborator; contributor; editor; reader. We could talk more about it in another article. My data must be accessed remotely There are several possibilities. The web-client UI First web UI of the product, it doesn’t evolve a lot today. Its characteristics are: Access to almost everything in the system which makes this UI not usable for everybody; The administration panel has many buttons, you can heandle groups, users, import/export your repository, browse your system with a low level tool, the node browser, …; Can be customized though XML configuration files. To extend it, you are more likely to have some Java code to write; Runs on the same instance as the Alfresco core server. In other words, this UI runs on the backend, it can’t be used remotely. Personally, I don’t recommend to use this UI but only for administration purposes. Indeed, there is far more interesting things to do if you want a great experience for your users. The Share UI Share is the web 2.0 UI of Alfresco. It is a user-centric collaborative website, However, in the recent 3.2 version of Alfresco, an administration panel has appeared. It aims at providing collaborative sites to people where they can share their work through wiki pages, forums, document library, links, their calendar, … It is based on the Surf platform, a framework built by Alfresco to easily create 2.0 websites and which communicates with the Alfresco core through REST. Surf has been recently integrated in Spring . REST API and SOAP Alfresco provides a REST API also called Webscripts and a SOAP API. Those two API allow developers to create custom remote frontend which is good since it is a great way to let the core server focus on its main task: content management. The CMIS API A new API available in Alfresco, CMIS has been implemented in Alfresco 3 in order to facilitate inter-operability among several ECM systems. This means that a frontend which uses the CMIS API should be able to switch from Alfresco to another backend implementing CMIS without any customization. JCR and JCR over RMI If you need it (Liferay integration for example), the Alfresco repository implements JCR (JSR-170). You can use it locally or remotely over RMI. However, this method is not recommended since the emphasis is on REST and CMIS. Presently, we are on a collaborative platform project based on Alfresco and our client uses Alfresco as the fundations for its content management. The expectations of our client can be classified in three main categories: administration of the platform; access to data with custom metadata through a user friendly UI; portal like application in order to favor collaborative work. These three kind of expectations are covered by Alfresco. The administration is achieved through the web-client UI, the user friendly UI is a flex based website communicating with Alfresco through REST and the portal like expectations are covered by Share, the collaborative website solution provided by Alfresco on which we can add new components to better fit its needs. At last, we have a consistent, performant and scalable solution and we have saved time compared to a development from scratch. Alfresco must be integrated with Liferay Portal We had this kind of expectation, Alfresco as a backend and Liferay for the portal frontend. Even though Alfresco claims that the integration with Liferay is very good, our experience showed that it is inter-operability rather than real integration. The portlet standard allows agregation of contents retrieved as HTML code. This is exactly what Alfresco provides with few portlets written with its Webscript API but they are more a proof of concept than a real integration. Therefore it is up to your development team to write their own portlets and their REST services on the Alfresco side in order to achieve the integration. If you want Liferay to use Alfresco as its repository backend, it is possible since both implements the JSR-170 but your development team will also have to write down some glue code. I have an AD and I want Alfresco to use it No problem! Alfresco uses the LDAPFactory of the JVM. Therefore, it integrates perfectly with an LDAP directory whether it is AD, OpenLDAP or something else. Some directories have non standard fields? No problem, everything is easily customizable through configuration files. By default, new synchronized accounts are created in the root folder of the repository, but it can easily be moved to a specific /users subdirectory. You can also customize the template of a user account directory so every new user directory can be created with some specific directories and documents in it. By default, removing a user from the Alfresco users database doesn’t remove its home folder. This behaviour can be easily modified so the home folder is removed along with the account. Synchronization through LDAP like any other background task performed by Alfresco is scheduled by Quartz. The latter let you configure when you want a synchronization to happen just like in a standard crontab. In spite of it, Alfresco works with several directories at the same time and distinguish users and groups. In this way, it is easy to adapt the configuration to your context. Lastly, if your LDAP directories limit the number of possible answers in one request, Alfresco, since the 3.2 version, supports LDAP paged requests. Quartz ? But I don’t want internal schedulers in the applications of my IT No problem but a bit of development. Quartz is provided by default by Alfresco and is configured through Spring beans. If you want to remove it from Alfresco, you can but you will have to modify some configuration beans and maybe have to write Webscript code to allow some jobs to be executed remotely, from your central scheduler for example. I assure you it is not impossible. As an example, we were about to do it for a client who wanted LDAP synchronization to be done remotely, by its central scheduler. In Alfresco, the synchronization is a two steps process: extracting the content to synchronize from the LDAP directory and create an XML that can be read by Alfresco for users, groups, …; Inject this content in Alfresco. So we had two tasks, two java classes and one XML with a well known structure. Therefore, to achieve the expectation we just had to: write a small script for the central scheduler to request the LDAP directory and generate an XML for Alfresco; insert it into Alfresco by executing the import tool provided by Alfresco. But this solution has a small issue, the java program requires the Alfresco server to be shutdown during the update… No problem, we just have to write a small REST service in order to execute the import thus there is no longer a need to shutdown the server. This REST service is pretty simple since the only thing we have to do is already provided in Alfresco’s internal API. Security wants secured channels everywhere No problem, LDAPS and HTTPS are perfectly supported. SSO is the standard inside my company and LDAP connection for the outside Kerberos, NTLM, CAS, JAAS, you have the choice. Chained authentication is also supported in the case where, for example, an extranet is login-form based and the intranet is SSO based. Cluster must be supported for high loads Alfresco supports this kind of setup. The configuration effort is minimal here. However, you may want to know that if the cluster mode is activated, every Alfresco instance must use the same database instance and the same data repository. Database? But I thought that everything was in a physical storage! Indeed, it is true for every data that have a content. The users, directories (but not their content), metadata and workflows are stored in a database which is hardly not loaded. A simple MySQL is enough, no need for aggressive optimization on this side. Which databases are supported? Oracle, MSSQL, MySql, PostreSQL, … The choice is wide enough. In spite of it, the load on the database is really small, Alfresco uses neither complex schema nor high frequency requests since it indexes most of its content and heavily uses Hibernate and EHCache. And what about storage? Any prerequisite? The data repository needs a storage capacity according to your expected amount of data. The index adds about 30% of the data repository size. In spite of it, the index, implemented with Lucene must be located on a very fast physical drive which supports file locking. Data must be segmented on several repositories. Alfresco also works with this kind of configuration even though it is a bit complex to configure. In spite of it, you must keep only one database and your backup procedure must still backup everything at the same time. In the case you would like to setup an internal repository with all your data and an external one with a readonly subset of data for internet, you may want to know that it is supported by Alfresco. It is a kind of cluster configuration. In this case, the internet instance of Alfresco is readonly and shares the same repository and database as the internal instance which has full access to the repository and each instance has its own index. We have recently run into the case at Octo and our solution was to avoid a new instance of Alfresco. We chose to create a REST Webscript on Alfresco in order to let the “readonly” side retrieve what it needs through it. Data to be exported were marked by a specific tag. The solution must be simple to deploy and to update Alfresco works with Tomcat and JBoss, it is a WAR or an EAR you have to deploy and that’s all. I need a fine grained events tracking mechanism Alfresco provides an audit mechanism which keeps track of everything that happens on every data in the repository. Configure it cautiously in order to prevent a big increase of the space occupied by your database and a big slow down of your server. The solution must be scalable Alfresco is spring based and its architecture is good if you plan to use it as a foundation on which  other specific projects will occur. I need a wiki Alfresco works with mediaWiki. However, if you only need a wiki, it may be more interesting to use only mediaWiki and to eventually link it to Alfresco or another ECM the day you will need an ECM. I need collaborative softwares with a web based UI Alfresco provides Share. However, it doesn’t cover all the needs and customizing it might be fastidious. The best is to use an Alfresco facade API (REST, SOAP,…) to build custom user friendly UI. My teams must be quickly operational for custom developments The scalable architecture of Alfresco fits very well and you can quickly hand over it. Furthermore, there is an active community which can help you thru forums and blogs. Be advised that it is important that your developers know already about Spring, Java 6 and tomcat in order to be rapidly operational. I develop in Agile It is possible and that’s what Octo does. However, full test driven development (TDD) with Alfresco may sometimes be fastidious, since all of the code is not covered by tests and executing a unit test take about 40 seconds because a mini instance of Alfresco must be initialized. Having an experienced person on Alfresco is definitely a good idea to save time. We have encountered many issues in the setup of REST unit tests with Alfresco because of not so well done things like the time taken to run a single test (a mini instance of Alfresco is launched) or the amount of configuration and files to copy in order to start a single unit test. However, in 80% of the cases, unit tests worked well and since the product is a free software, it made our life easier and we saved time when bugs happened. And…I think that’s an important point. Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged collaborative , Java , REST . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse 4 commentaires sur “Does Alfresco fit your needs?” John Anson 09/06/2010 à 03:27 Thanks, Rémy for this excellent summary.\r\n\r\nHave you integrated CIFS or WebDav with Share for companies that are moving from a CIFS-only structure to ECM?\r\nIt looks like Alfresco creates a share-site/documentLibrary directory structure for each site, which is clumsy to access from a CIFS/WebDav interface.\r\n\r\nWhat would you suggest was the best way integrate CIFS/WebDav with the Share web client? Rémy SAISSY 09/06/2010 à 16:52 Hi John,\r\nyou are welcome, thanks for your feedback.\r\nIf you don't really need to use a network drive access to your data, I would recommend to drop this part and to only use the share web client along with the sharepoint connector.\r\n\r\nIf you really need the network drive access, another solution would be to customize the file-server subsystem as explained here: \r\nhttp://wiki.alfresco.com/wiki/File_Server_Subsystem#Home_Folder_Filesystem\r\nBasically the idea is to tell the subsystem to dynamically map the rootpath to the user's home.\r\nAnd if you need more complex mapping like something based on both the user/group and share sites permissions to define a rootpath, you can also implement your own flavor of org.alfresco.filesys.alfresco.HomeShareMapper in order to take your business needs into account.\r\n\r\nLet me know if you need further informations.\r\nRegards, John Anson 10/06/2010 à 07:41 Thanks Rémy.\r\n\r\nDoes Alfresco support creating the equivalent of Linux hard/symbolic links to folder nodes?\r\nI wondered about creating a structure of links under Company home that access the documentLibrary folders under Sites (but hiding the site/DocumentLibrary structure)\r\nIdeally this would be seamless under CIFS, WebDAV, Alfresco Explorer.\r\n\r\nIs this simple to do in Alfresco?\r\n\r\nRegards,\r\nJohn Rémy SAISSY 10/06/2010 à 10:24 Hi John,\r\nwithout customization, you can use tags (also called categories). Otherwise you can either use node associations or the reference aspect.\r\n\r\nI don't think it is that simple to do, it requires a bit of development anyway.\r\n\r\nRegards,\r\nRemy. Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-04-30"},
{"website": "Octo", "title": "\n                This is the story of a project…            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/this-is-the-story-of-a-project%e2%80%a6/", "abstract": "This is the story of a project… Publication date 14/06/2010 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 This is the story of a project, neither more complex nor simpler than others: an application that communicates with a database and two other systems. Something quite mainstream from a technical and architectural side, something standard from the management side: all must be done for yesterday and there is a lot to do…In short, “it’s gonna be hard” as often say the developers but nobody screams it out too loud. So we build the team. 40 persons are staffed, people are specialized. The teams are organized in pools, so that a kind of contract is setting up between the different pools. Each pool is responsible for treating certain kind of demands. A flow of demands appears. Certain pools are under pressure and become the bottleneck: a stock of demands is created upstream whereas the downstream pools are waiting…Therefore and for these under pressure pools, important things are becoming urgent things. Choices must be made among urgent things to treat the immediate ones. Task switching is becoming the way of working and in the end, the flow slows down. Then the deadline of the “go live” comes: it is in two months. The user acceptance tests are just starting but have been delayed by the tedious and painful integration between the different components. Maybe the built contracts between the teams have complicated the integration: some mandatory parameters are missing, the dates do not respect the proper format, the error codes are partially interpreted… In any case, the user acceptance tests detect more bugs than what the development team can resolve and all is not still tested. So we add more manpower. A team to resolve the bugs, a team, in another place, to finish the development, a third one to integrate the different components. But all these teams share the same strain of code and the changes of some will impact the corrections made by others. In short, you know about the following (the developers have worked all night long and during week-ends) and the end (the deadline has been postponed and the initial scope of work tasks modified). And thanks to the miracle of computer science: the application was finally delivered and running! This was a couple of years ago. I thought this was “the way to go” and that all projects were managed the same way. Since I read and talked with a lot of people. I now see this story with a different state of mind. No priorities or the famous « we need all » Concerning this, I quote Tom De Marco’s citation : You say to your team leads, for example,“I have a finish date in mind, and I’m not even going to share it with you. When I come in one day and tell you the project will end in one week, you have to be ready to package up and deliver what you’ve got as the final product. Your job is to go about the project incrementally, adding pieces to the whole in the order of their relative value, and doing integration and documentation and acceptance testing incrementally as you go. In short: working to always be able to go live, tomorrow Beyond the real organizational and technical issues, you will need to be willing to incrementally build the software. Contractual agreement responsibilities between the teams would be limited so as not to organize themselves on technologies or tasks but on business features. From the technical point of view, each team will so be responsible for the good running of the complete feature. From the management point of view, the managers and business guys will have to make choices: what is THE absolutely needed feature. From my own experience, the more you work in environments where you have to meet the deadline, the more this kind of feature teams and organization will help you. Management pressure and fear culture In Slack , Tom De Marco exposes some characteristics of fear culture: …among the characteristics of the culture of fear organisation are these: – it is not safe to say certain things (e.g. I have serious doubts that this quota can ne met)… – goals are set so aggressively that there is virtually no chance of achieving them – Power is allowed to trump common sense … When I am thinking of fear management, I imagine a kind of despot physically impressive who shouts at his collaborators from his desktop, striking with his fist on his desk…a beautiful cartoon in brief. It seems to be a little more insidious but we have to admit there are contexts where people are under such pressure, where it is difficult to raise an alert, where deadlines are fixed without any considerations of the teams capacity to do and where, in the end, the latter are under the pressure of commitments taken by their managers regarding their own hierarchy. Understanding the problem is certainly the first step. But how can we solve it? What can we do when a manager does not understand the risks and refuse to accept what is unavoidable: you need to choose, prioritize and negotiate the scope to keep the deadline or move it forwards? This task is far from being an easy one and the best answer I got now is the “backlog” coupled with a “burndown chart ”. There are, in my opinion, several benefits in these kinds of situations: 1/ Bring together all the tasks (technical, functional tasks…). These tasks can, of course, be organized or consolidated by use-cases or features. 2/ Share all the tasks with all the project participants. To say it differently, rendering the immensity of what must be done. 3/ Show a confident and realistic deadline and thus, enabling the managers to prioritize efficiently between the tasks. 4/ Show all the added tasks that will necessarily postpone the initial deadline. A few weeks before the « go live » and while the current organization is under pressure, decision is made to add more manpower The Brooks law has been established in 1975 (I was still not born) and states: Adding manpower to a late software project makes it later We have all experienced it. But we have to admit we still all tend to add more manpower to meet the deadline instead of changing the initially defined scope and keep an optimal and adapted team size. Brooks explains his law with two major points. The first one concerns new team members who have to be trained thus consuming productive time of people already in place. The second one is a myth making us believe that development tasks can be segmented “as you go”, not taking into account the intellectual part of the work and the inter-personal communication needed between all developers. We can moreover add difficulties linked to the organization of the developments and the needs to share between all developers the same code. So many details that will make the teams’ productivity decrease. Concerning this and always in Slack , Tom de Marco tells about what he calls “overstaffing” and states: Meeting the deadline is not what this is all about. What this is about is looking like you are trying your damnedest to meet the deadline. In this age of “lean and mean” it is positively unsafe for you to run the project with a lean (optimal) staff. An interesting point of view… “Everything fails all the time”. Why don’t you play with it instead of ignoring it? So as usual, the critics are easy and the art is difficult. We will thus notice that the same errors occur again and again whereas alternatives (which, be sure about that, will have other limitations) exist but rarely tried. “ Risk Management is a discipline of planning for failure ” (Slack, Tom de Marco) and this is maybe where we are not good at. “Everything fails all the time” states Werner Vogels during the last 3 minutes of this video . Yes there are advertisements for EC2 behind that statement. Yes the architect is, by nature, pessimistic. But too often we do not think about the potential failures. Tom De Marco teaches us that managing risks will first demand to identify them, to monitor them, to set indicators that alert us when the failure is upcoming. Sometimes, alternatives will have to be found. Some people will have to be trained.  Parallel version of software will be developed in order to choose, at the very last moment for decision, the most adapted solution (Lean Management calls this “set based design” principle). From the architectural point of view, this approach of risk management will imply to architecturize our systems – in strong collaboration with people from the business – to manage and embrace all these errors. In other words, to forecast in our architecture the maximum of all likely error cases: –\tHow to manage a degraded mode in case the system turns unavailable? –\tWhat are the procedures (if needed manual procedures) to proceed to finalize a business process in case of error? –\tWhat are the mandatory informations needed to finish the business process? –\tWhat are the alarming mechanisms in order to be pro-active regarding the end-user, informing him an error occurs and helping him properly finalize his in progress work? On an existing system, evolution will have to be done in order to make sure these already detected error cases are definitively fixed. But let’s be honest. If you are cost-driven, you will find all the non-business requirements useless. But finally, our faith in an application isn’t it more based on its ability to manage the error (resiliency and reliability) than any other criteria? Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno , Methodology . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-06-14"},
{"website": "Octo", "title": "\n                AOP and Swing : a smart association            ", "author": ["Olivier Mallassi"], "link": "https://blog.octo.com/en/aop-and-swing-a-smart-association/", "abstract": "AOP and Swing : a smart association Publication date 14/04/2010 by Olivier Mallassi Tweet Share 0 +1 LinkedIn 0 This is not a scoop : Swing – even if this technology is widely used in companies – is not evolving a lot. The developer kit still provides today components which are neither complex nor rich as a couple of years ago so you have to buy it elsewhere. The Swing development is still very verbose and finally not really productive, and to be honest, it is not the few JSR in stand-by that will change anything. Beans Binding is in an inactive status. Beans Validation – the draft dates from 2008 – is always not included into the JDK (maybe the version 7). The JSR 296 (which defined a standard application lifecycle) will not represent the most important improvement a framework has known (I swear, I really enjoy Swing…) So we have to deal with all these misses. Moreover, I am fond of AOP since the first version of AspectJ came out. I really think this is an elegant way of developing which is today well equipped (IDE integration, Maven integration…) to be safely used. So here are a few recurrent Swing concerns that AOP can help to resolve. Service injection The idea is quite simple : implementing the IOC pattern by injecting a service implementation (for instance a Spring bean) into all properties annotated with the custom @Inject annotation. For instance, injecting the service myService in the following action (the code can be improved but in this example, the bean is injected using the property name and not the property type) : public class MyActionListener implements ActionListener{\r\n@Inject\r\nprivate MyService myService\r\n\r\n...\r\n} The following aspect enables injection of a service called myService public aspect SpringInjectAspect {\r\n    pointcut fieldsInjection() : within(com.mypackage..*) \r\n     && execution(public void java.awt.event.ActionListener+.actionPerformed(..));\r\n\r\n    before() : fieldsInjection() {\r\n        Class currentClass = thisJoinPoint.getThis().getClass();\r\n        Object currentObject = thisJoinPoint.getThis();\r\n        //to inject bean spring even in super class property\r\n        while (!currentClass.equals(ActionListener.class)) {\r\n             injectBean(currentClass, currentObject);\r\n             //for te next step\r\n             currentClass = currentClass.getSuperclass();\r\n        }\r\n    }\r\n\r\n    private void injectBean(Class currentClass, Object currentObject) {\r\n        Field[] fields = currentClass.getDeclaredFields();\r\n\r\n        for (int i = 0; i < fields.length; i++) {\r\n            Annotation associatedAnnotation = fields[i].getAnnotation(Inject.class);\r\n            if (associatedAnnotation != null) {\r\n                try {\r\n                    //get the bean by the field name\r\n                    Object implementationObject = SpringHelper.getApplicationContext().getBean(\r\n                            fields[i].getName());\r\n                    try {\r\n                        fields[i].setAccessible(true);\r\n                        fields[i].set(currentObject, implementationObject);\r\n                    }\r\n                    catch (IllegalArgumentException e) {\r\n                       ... do what you need to do\r\n                }\r\n                catch (NoSuchBeanDefinitionException e) {\r\n                //log the fact the asked bean do not exist\r\n                    throw e;\r\n                }\r\n            }\r\n        }\r\n    }\r\n} Tooling of the binding mechanism Using a binding mechanism like jGoodies can be sometimes interesting but often implies to write additional and worthless code like : public void setBooleanValue(boolean newValue) {\r\n  boolean oldValue = booleanValue;\r\n  booleanValue = newValue;\r\n  changeSupport.firePropertyChange(\"booleanValue\", oldValue, newValue); \r\n } In this example, each setter must be enhanced with a call to the firePropertyChange method which precises the new and old values and the property name (which will be error prone since the code will be copied and pasted in each setter…) Here again, AOP can help by providing us a way of enhancing a “classical” setter (ie. that only set the  p property). What I call a classical setter is : public void setBooleanValue(boolean newValue) {\r\n  booleanValue = newValue;\r\n } and the following aspect do the rest of the job public aspect jGoodiesBindingAspect {\r\n    pointcut propertyEnhancement() : within(com.mypackage..*) \r\n         && execution (public void com.jgoodies.binding.beans.Model+.set*(..))\r\n\r\n    void around() : propertyEnhancement() {\r\n        Object[] args = thisJoinPoint.getArgs();\r\n        String propertyName = thisJoinPoint.getSignature().getName().substring(3);\r\n\r\n        Object oldValue = null;\r\n        try {\r\n            Method propertyGetter = thisJoinPoint.getThis().getClass()\r\n                    .getMethod(\"get\" + propertyName);\r\n            oldValue = propertyGetter.invoke(thisJoinPoint.getThis());\r\n            Object newValue = args[0];\r\n            \r\n            proceed();\r\n            // if no error occurs\r\n            Method firePropertyChangeMethod = com.jgoodies.binding.beans.Model.class.getDeclaredMethod(\r\n                    \"firePropertyChange\", String.class, Object.class, Object.class);\r\n            firePropertyChangeMethod.setAccessible(true);\r\n            propertyName = org.apache.commons.lang.StringUtils.uncapitalize(propertyName);\r\n            firePropertyChangeMethod.invoke(thisJoinPoint.getThis(), propertyName, oldValue, newValue);\r\n        }\r\n        catch (NoSuchMethodException \r\n            ...do what you need to do\r\n    }\r\n} In that case, the pointcut weaves all the methods, from any classes inheriting com.jgoodies....Model , that start with set. GUI management authorizations Here again this is a really standard concern: being sure that the functions (I mean buttons, menus…) and datas are not accessible, alterable in the Swing GUI. So we are simply talking about enabling or disabling the widgets that need to be (ie. myComponent.setEnabled(...) ). The first step is to define an annotation that will permit to define the roles on a JComponent . In the following sample, the widget myComboBox is enabled only for the users that have the roles “consultation-level-1” and “consultation-level-2”. @Authorize(roles=\"consultation-level-1, consultation-level-2\"\r\nprivate JComponent myComboBox; So, the following aspect will enable or disable the widgets based on the previously defined roles public aspect AuthorizationAspect {\r\n    public pointcut authorized_gui() :  within(com.mypackage..*) \r\n        && set(@Authorize * *..*.*);\r\n\r\n    after() : authorized_gui() {\r\n        String pointCutKind = thisJoinPoint.getKind();\r\n        if (JoinPoint.FIELD_SET.equals(pointCutKind)) {\r\n            java.lang.reflect.Field field = ((FieldSignature) thisJoinPoint.getSignature()).getField();\r\n            try {\r\n                field.setAccessible(true);\r\n                Object theField = field.get(thisJoinPoint.getThis());\r\n                if (theField instanceof JComponent) {\r\n                    Authorize authorization = field.getAnnotation(Authorize.class);\r\n                    String[] roles = authorization.roles();\r\n                    //ask your security context holder the role the user have\r\n                    ((JComponent) theField).setEnabled(SecurityContextHolder.getInstance()\r\n                            .isUserInRole(roles));\r\n                }\r\n            }\r\n            catch (IllegalAccessException e) {\r\n                throw new TechnicalException(e);\r\n            }\r\n        }\r\n    }\r\n} The tricky point in the aspect is the set(@Authorize * *..*.*) that weaves into the component instantiation (and not the call to the constructor) of all the properties that have the specified annotation. So here are a couple of concerns AOP helped us to solve in an elegant, simple and non intrusive way. Other AOP usages exist. The most obvious one is to automatically manage the runtime exceptions. In a GUI context, these exceptions will usually be displayed – sometimes in a popup – with a message like “an error occurs, please contact your administrator”. Other usage can be in validating some specific coding rules (using “declare warning” and “declare error”). Do you have  other GUI concerns you have been able to manage this way? Tweet Share 0 +1 LinkedIn 0 This entry was posted in Archi & Techno and tagged AOP , Java , Swing . Recent Posts How to deal with an Inverse Conway Maneuver? – A talk by Romain Vailleux at Duck Conf 2021 A Journey To build a Business-Driven Data Science Capability New Year’s Resolutions: Shed those excess pounds (from my Google inbox)! Technical Due Diligence–Safeguarding your IT Startup Investment Data+AI Summit 2020 – be Zen in your lakehouse Leave a Reply Cancel reply Your email address will not be published. Required fields are marked * Comment Me notifier par mail en cas de nouveaux commentaires Name * Email * Website Leave a comment This form is protected by Google Recaptcha", "date": "2010-04-14"}
]