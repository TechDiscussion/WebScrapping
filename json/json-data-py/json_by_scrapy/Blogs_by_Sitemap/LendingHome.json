[
{"website": "LendingHome", "title": "lock upstream docker images to specific revisions", "author": ["Sean Huber"], "link": "https://tech.lendinghome.com/lock-upstream-docker-images-to-specific-revisions-6ed4d5e9a2fd", "abstract": "tldr: lock images to sha digests instead e.g. ubuntu@sha256:{{SHA…}} It’s pretty common to start off a Dockerfile by inheriting an official image e.g. ubuntu , debian , alpine , etc. The problem with referring to Docker images like ubuntu:latest is that these image tags can be overwritten which sometimes results in pulling unexpected and potentially insecure or breaking changes from upstream! One common attempt to solve this problem is to refer to more specific tags like ubuntu:20.04 but unfortunately all Docker image tags can be changed and overwritten just like the latest tag. This is pretty similar to how a Git branch can be force pushed and replaced with a completely different commit. Git allows us to solve this kind of problem by explicitly referring to a commit sha instead of the tip of a branch. Luckily this problem can be solved the same way in a Dockerfile by locking images to a specific Docker image layer using its sha256 digest: Now the upstream ubuntu dependency is locked to a specific image layer and will not change until we intentionally change the sha. These sha digests can be committed and pushed thru the same review process as everything else! The sha256 digest for a Docker image can be found by running (with jq ): For example: Referencing upstream Docker images by their sha256 digests may improve the reliability and consistency of containers across environments but remember to routinely update for important changes like security patches or bug fixes ! Our LendingHome team is still growing! Check out our careers page to learn more. We look forward to hearing from you! LendingHome Technology Docker Sysadmin Security System Administration Dockerfiles Written by Staff DevOps Engineer @LendingHome LendingHome Technology Written by Staff DevOps Engineer @LendingHome LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2020-12-28"},
{"website": "LendingHome", "title": "creating a scalable customer experience", "author": ["Baishi Wu"], "link": "https://tech.lendinghome.com/creating-a-scalable-customer-experience-779445125f8d", "abstract": "Customer Relationship Management (CRM) has become a loaded term that can often be all-encompassing. As a result, there are many different interpretations of what “CRM” is and what it includes. But no matter the interpretation, the importance of knowing your customer remains the focus of any CRM. This is simple when companies are small and customers are few; it’s easier to build a few strong relationships. At LendingHome, for example, we never forget our first customers: we’ve named our conference rooms after the street names of houses we’ve helped to finance. As a company scales, not only does its number of customers grow, but so does the number of interactions with every customer. Not to mention the growing number of employees communicating with customers. At that point, it becomes incredibly important to have the right tools in place, so that the company can continue to be customer-first and deliver their product effectively. In LendingHome’s case, as we were building up a large customer experience organization, we had to scale our information and processes to the point where now, every time a person gets on the phone with a borrower or investor, we know who they are and what they need from us. If you do not live in San Francisco and haven’t seen the Salesforce tower overpowering the city’s skyline, or still think that “inbound” just refers to a basketball play after a timeout, you may not be familiar with the proliferation of the various software tools aimed at helping companies improve their customer relationships. Each of these products is meant to create a process around a specific function, and integrating with these products is a build vs. buy decision that needs to be made. Here are a few questions to consider when integrating with a CRM: Do they specialize in something that you shouldn’t? We’ve all been in situations where we’d prefer to write whatever we need ourselves, as it can be easier to write a custom solution than integrate with someone else’s generic one. But the question is whether or not you should do this when it’s not, and won’t be, the core competency of the your company. Salesforce specializes in storing customer information in an actionable way for sales teams; Marketo specializes in leveraging that information and automating customer interactions. Can it accelerate the velocity of your organization beyond your engineering team? The primary benefit we’ve discovered through our integration with vendors like Salesforce or Marketo is that we’ve increased the number of channels where we can accomplish things. Our sales and marketing teams are now empowered to create new dashboards, prioritize workflows, assign cases, and create outreach campaigns without relying on engineering. So now our engineering team is solely responsible for ensuring data is consistent and synced properly. It can be hard to measure “pure pain,” but LendingHome’s first integration with Salesforce may have caused more problems than it solved. Much of this was due to design choices that, while pragmatic at the time, led to problems down the line. For us, the important questions were the following: What should we be syncing? Initially, we allowed information to sync bi-directionally, so updates in our platform would update values in Salesforce and the same would occur the other way around. This proved to be quite expensive since we had to know information before we could do the sync, resulting in multiple queries (and multiple potential failure points). This was compounded by the fact that we made the choice of syncing on email addresses instead of properly using our platform data’s primary keys as lookup fields. How often will we be syncing? In addition to accounting for instances when syncing was occurring correctly, we also had to consider the error cases and the poor-quality data that accumulated as a result. As such, one aspect that we didn’t account for was how often we would need to backfill data, resulting in each record syncing again and again. Initially, we were not using a bulk API to do this work, which often resulted in getting on the phone with our Salesforce account managers, begging them not to rate limit us. What was the result of our syncing? As many know, the debugging process can be the most difficult part of any new integration. When we first began to do this, we would have Sentry errors routed to Slack for errors. Slowly but surely, those would pile up as we had more failures, types of errors, and more vendors to the point that they would all become noise with nothing actionable. As more integrations were coming down the pipeline, we did what would make any manager (or mother) proud: we added rules and boundaries to approach the issues head-on: Limit what you sync! In order to reduce the complexity and keep the information more consistent, we decided to limit the information that we sync, so that it would always go from platform to third party services. This simplified our code on the platform side and removed the need to do any complex querying we needed for updates. As a result, data from platform was always the source of truth. Queue and try, try again! To handle our syncing frequency problem, we also added transaction queues to help batch requests. A pending queue allowed us to let records pile up and leverage bulk APIs to make our requests. Next, we made moving a record from pending into the processing queue an atomic operation that prevented duplicates. Because of the aforementioned one-way syncing, we didn’t care about the order of operations either which meant that we also benefited from debouncing and limited the number of transactions we would sync at once. Finally, a failure queue would let us do retries after the fact. Why don’t you tell me what transaction you are trying to complete! To understand the result of our syncing, we exposed all of the details in every transaction to be visible in our loan origination platform (Figure 1). From this, we then allowed any user to search for a specific transaction, view all transactions in the pending or processing category, and manually retry any failing transactions that have occurred. This created an environment where all parties could help diagnose problems and debug issues. To do the above, we created a separate gem, called LhCrm, that contains all of the commands needed here. Encapsulated are various services that read from LendingHome data and write to third parties, like Salesforce and Marketo, and incorporate the commands to ensure quality and seamless processing. Below is a diagram (Figure 2) that shows the relationship between the various parts of LendingHome and the commands, services, and models that exist inside LhCrm. We designed this as a gem to create clear boundaries in our code and provide a clean interface that would be easy to maintain. In addition, LhCrm functions as a Rails Engine and allows us to incorporate unit tests, run them quickly, and write transactions to independent databases. Through this process, we have learned that one of the most important aspects of creating a great CRM system is ensuring that there is a consistent infrastructure that syncs data accurately and reliably. By doing that, we can create leverage for all employees in our organization! LendingHome Technology 9 Ruby on Rails Customer Experience Salesforce Technology Sales 9 claps 9 Written by LendingHome Technology Written by LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-01-07"},
{"website": "LendingHome", "title": "pete hunt the past present and future of react", "author": ["Li Ouyang"], "link": "https://tech.lendinghome.com/pete-hunt-the-past-present-and-future-of-react-5b47acbf42b", "abstract": "We are incredibly lucky that Pete Hunt came to speak at LendingHome. Pete is a co-founder of Smyte , a YC-backed startup fighting against online fraud. Previously at Facebook and Instagram, Pete is one of the early people that worked on React. Our engineering team is growing and we want to meet you. We’re hiring engineers in our San Francisco and Columbus, OH offices. See our careers page to learn more. We look forward to hearing from you! LendingHome Technology 17 React Reactjs Tech Technology 17 claps 17 Written by LendingHome Technology Written by LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-21"},
{"website": "LendingHome", "title": "speed up ci builds with multi stage dockerfiles", "author": ["Sean Huber"], "link": "https://tech.lendinghome.com/speed-up-ci-builds-with-multi-stage-dockerfiles-8c295301ac06", "abstract": "Leveraging the Docker build cache has been a great way to speed up Docker builds for years now: When building an image, Docker steps through the instructions in your Dockerfile , executing each in the order specified. As each instruction is examined, Docker looks for an existing image in its cache that it can reuse, rather than creating a new (duplicate) image. For the ADD and COPY instructions, the contents of the file(s) in the image are examined and a checksum is calculated for each file. For the RUN instructions, the command string itself is used to find a match. Once the cache is invalidated, all subsequent Dockerfile commands generate new images and the cache is not used. The Dockerfile for a simple Ruby service may look something like: This works great since the RUN bundle install step is only executed if: new alpine:latest changes are pulled in apk system dependencies change (not very often) the contents of Gemfile or Gemfile.lock change Otherwise the build cache is used and the step completes immediately: Nowadays it’s common for Ruby services to use other dependencies as well e.g. Node for handling misc tasks like asset compilation via webpack : But here’s where things get a little tricky: Now whenever the Gemfile or Gemfile.lock changes it invalidates all of the cached steps below it That means all NPM packages must be reinstalled and all webpack assets must be recompiled even though gems have nothing to do with those steps The steps could be reordered so that gems are installed after the Node related steps but that results in the same problem where changes to package.json or webpack assets requires a full reinstallation of gems It’s pretty common for these Node dependencies to only be used for asset compilation at build time. In these cases Node, NPM , and the entire node_modules directory isn’t actually needed when the service runs For smaller services this usually isn’t a big deal but for services that rely on a large number of gem or NPM dependencies this results in much slower build times Now Docker 17.05+ multi-stage builds can be used to solve this problem! This feature allows a Dockerfile to contain multiple FROM steps which can generate intermediate images and utilize the build cache more efficiently. These intermediate images are eventually combined into one final image: Now gem changes no longer impact NPM or webpack steps and vice-versa! This was a simple example but these multi-stage builds are very useful for: Services that have many dependencies so reinstallation is slow Services that have many independent build steps for misc tasks like generating static content e.g. sitemaps, marketing pages, assets, etc. Note that even though the Ruby and Node related steps no longer invalidate each other’s build cache, all of the intermediate Docker images generated within the Dockerfile are still built one at a time, step by step! On a fresh build, the Ruby related steps won’t be evaluated until the Node related steps complete, even though they do not depend on each other. Keep an eye on Buildkit - the concurrent, cache-efficient, and Dockerfile-agnostic builder toolkit! One of its upcoming experimental features allows the intermediate images in multi-stage builds to be compiled in parallel! Our engineering team is still growing! We’re hiring engineers in our San Francisco and Pittsburgh offices. Check out our careers page to learn more. We look forward to hearing from you! LendingHome Technology 4 Docker DevOps Ruby Nodejs Sysadmin 4 claps 4 Written by Staff DevOps Engineer @LendingHome LendingHome Technology Written by Staff DevOps Engineer @LendingHome LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2018-12-03"},
{"website": "LendingHome", "title": "capybara better practices", "author": ["chandrahas raj"], "link": "https://tech.lendinghome.com/capybara-better-practices-5598edb0b961", "abstract": "Never re-invent the wheel Capybara feature specs can be frustrating because it’s difficult to get them to pass 100% of the time. This post will teach you how to avoid a certain race condition, using only built-in Capybara methods. Often times, it’s easy to wonder why your specs run so slow or why your spec is blinky . Most of the times it is because we’ve used something that just works( or maybe not) . You could, of course, take a few minutes to read about it before using it, but since you’re probably as lazy as me, here’s a blog post explaining it. For example, let’s imagine you are writing a spec that clicks on something, waits for a background task to be completed which updates a certain field, and then finally validates that field. (Oofff…lots of steps.) There are many ways to write this spec: The above spec works just fine, but it can be blinky. How? Let's set some assumptions early before I go ahead: Capybara.default_max_wait_time = 5. I have the set the default_max_wait_time of capybara to 5 seconds. Now let’s go through each step. First, you click on validate on the page and the background task gets triggered. Now it’s going to try to find the text on the class: sample_form_text and validate it with validated text. In most cases, this works fine. But this call find('.sample_form_text').text is almost instantaneous. So if the background task takes some time to fill the page with the text, the spec is going to fail. How can we fix this? One method I like is from this post, https://robots.thoughtbot.com/automatically-wait-for-ajax-with-capybara So you can use the background task as shown below: Another option is to use something capybara already provides. Shown below: By default, the find with text option waits for the text to be available and that fixes the blinkiness. Hope this helped! We’ll be back soon with another edition of Today I Learned References: www.rubydoc.info robots.thoughtbot.com LendingHome Technology 4 Ruby on Rails Capybara Today I Learned Ruby Blinky 4 claps 4 Written by LendingHome Technology Written by LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2019-02-04"},
{"website": "LendingHome", "title": "keeping data in sync with activerecord updatedat", "author": ["Sean Huber"], "link": "https://tech.lendinghome.com/keeping-data-in-sync-with-activerecord-updatedat-95bd76955a01", "abstract": "ActiveRecord does not touch updated_at by default when calling: ActiveRecord::Base#update_column ActiveRecord::Base#update_columns ActiveRecord::Relation#update_all This was causing some unexpected issues with caching and incremental ETL to external systems since our usage of updated_at to signify data modifications was inconsistent. We realized that we rarely ever have a reason to modify data WITHOUT touching updated_at so we’ve enabled the touching behavior by default using the active_record-updated_at gem. This gem simply patches ActiveRecord::Relation#update_all to automatically specify updated_at as Time.current when: An updated_at column exists on the table An updated_at value is not explicitly specified in the query The update_column and update_columns methods actually call update_all under the hood so these methods are patched as well. For those rare occasions that we don’t want updated_at to be automatically touched we can explicitly wrap these method calls in a disable block : We’ve found this to be a simpler solution than manually updating all call sites to these methods with explicit updated_at timestamps and requiring all of our engineers to remember this step going forward. Our engineering team is still growing! We’re hiring engineers in our San Francisco and Columbus, OH offices. Check out our careers page to learn more. We look forward to hearing from you! LendingHome Technology 4 Ruby on Rails Ruby Activerecord Etl Database 4 claps 4 Written by Staff DevOps Engineer @LendingHome LendingHome Technology Written by Staff DevOps Engineer @LendingHome LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-13"},
{"website": "LendingHome", "title": "a healthy foundation", "author": ["Matt Marcus"], "link": "https://tech.lendinghome.com/a-healthy-foundation-d0ba70bd8a16", "abstract": "In our busy lives as engineers, product managers, and designers, it is difficult to dedicate time to keep our bodies healthy. This is a shame because you reap substantial mind-body benefits when your body is strong and healthy — you’ll be more confident and engaged while striking a beneficial balance between work and your health. As our team at LendingHome has grown, we’ve done more to foster health and fitness, from organizing daily plank circuits to using standing desks. This has been especially important for me as a recent student-athlete who constantly fought off injuries through preventative techniques and strength training. Here are four simple techniques that promote good posture, core stability, and flexibility. Hips are the most important part of your body that no one ever talks about. As a refresher, here’s a diagram of your hip region. Sitting at a desk all day compresses and tightens your hip flexors. This will inhibit your legs’ range of motion while causing your quad and hamstring muscle groups to feel tighter and your lower back to hurt. Left unaddressed, you’ll be creaky, inflexible, and weaker throughout your whole body. There is a simple solution though! Rolling out your hips with a foam roller will do wonders to loosen your hip flexor and glute muscles. Using a foam roller to stretch and strengthen these overlooked hip muscle groups will reward you with improvements in posture and flexibility. It only requires a few minutes, so squeeze in a quick roll-out during your morning routine or just before bed. Here are videos demonstrating how to use a foam-roller effectively on your hip flexors , IT bands , and glutes . After rolling out, it’s important to hone in on tight muscle areas with some deliberate stretching, which will help you increase your flexibility. Here is my favorite stretch: This stretch will focus on your lower back and outer hip regions. My rowing teammates and I love this stretch because back issues plague rowers. Slouching at your desk is easy but dangerous. There is probably nothing worse for your core stabilization muscles than sitting bent over all day. Slouching while sitting is just as bad as slouching while standing — it leads to poor posture and imbalances throughout your back and shoulders. When you don’t sit up, you compress your spinal region, causing back pain. Good posture while seated will prevent these muscle imbalances. Good posture entails not leaning on your desk while you work, keeping your back straight, and engaging your core as you sit up straight. You adopt good posture habits while standing as well and reap the same benefits. Many engineers at LendingHome have moved to standing desks and seen improvements in their posture and core strength. Keeping good posture for an entire day is tiring! And you may find it difficult to sustain. However, a few minutes of focused core exercises a day will give you the stamina to maintain solid posture for an entire day. For example, one of our engineers leads an optional 10 minute plank circuit at 5pm every day, which appeals to the dancers, athletes, and other teammates who just want a break in their evening. Other exercises like sit-ups, crunches, and leg raises will also help you build core strength. A strong core is just the first requirement for maintaining good posture for the whole day. You’ll also need a strong posterior chain . These muscles run from your upper back all the way to your upper hamstrings. By strengthening this region, you’ll notice improvements in your balance and posture while reducing any back pain you might experience. My favorite exercise to do here is the glute bridge, which looks like this: I particularly enjoy this exercise because it engages the essential muscles that power good posture. It is also customizable and can be done with any combination of the following twists: Holding the pose for a set period of time Some number of up-and-down repetitions Putting a weight on your pelvis that you have to lift as well Raising one leg while grounding with the other. The most stereotypical test of flexibility is to bend forward to touch your toes. Most of us can’t without a serious amount of strain and discomfort. It shouldn’t be this way though. Flexibility through your largest muscle groups, your hamstrings and quadriceps, is an essential foundation for body wide flexibility. Tight hamstrings are also the single largest contributor to lower back tightness and can affect even those who use standing desks. A great way to stretch your hamstrings is with a band, which you can see here . You should ground your hips and back, and then slowly start pulling your leg towards your chest. It’s important to refrain from extending to the point of discomfort or pain. Every 10–20 seconds, give yourself a break of a few seconds before re-engaging that leg. As you progress through the stretch, you should find your flexibility improving as the muscle loosens. A few minutes of this stretch with both legs will do wonders for your hamstring flexibility. When stretching your hamstrings, it is essential that you don’t go too far, too fast. Hamstrings are delicate muscles that are easily strained. I recommend supplementing static (stretch and hold) stretching with dynamic (active) stretches where you progressively increase the extent of the stretch throughout the exercise. Just ten minutes a day of these exercises can make a world of difference in how your body feels. Consistently working these techniques into your daily routine will yield compounding returns for your health and happiness. Interested in working a company that values and understands the importance of a healthy lifestyle? Our engineering team is growing! We’re hiring engineers in our San Francisco and Columbus, OH offices. See our careers page to learn more. We look forward to hearing from you! Thanks to Matt McFarland for reviewing this post! LendingHome Technology 8 Productivity Health Fitness Tips Stretching 8 claps 8 Written by Co-Founder @ModernTreasury. Previously Engineering @LendingHome. Bleed Green @Dartmouth LendingHome Technology Written by Co-Founder @ModernTreasury. Previously Engineering @LendingHome. Bleed Green @Dartmouth LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-11"},
{"website": "LendingHome", "title": "zero downtime migrations with activerecord and postgresql", "author": ["Sean Huber"], "link": "https://tech.lendinghome.com/zero-downtime-migrations-with-activerecord-and-postgresql-b3aca53ffbe6", "abstract": "As our database and traffic continued to grow, we discovered that certain migrations which appeared harmless actually caused table locking and, in some cases, downtime ! Instead of requiring all of our engineers to remember each one of these special cases, we’ve decided to automatically audit migrations at development and test time using the zero_downtime_migrations gem . This gem detects problematic migrations and raises an exception with instructions on how to perform the same operation the “zero downtime way” before it ever has a chance to reach production. It catches things like: Adding a column with a default Adding a non-concurrent index Performing schema changes with the DDL transaction disabled Changing the schema with data or index changes in the same transaction Using each instead of find_each to loop thru ActiveRecord objects Let’s take a look at what happens when we add a column with a default . This migration looks pretty simple! Unfortunately, if the posts table is large or receives significant traffic, then this migration will end up locking the table and blocking database connections until the service goes down! When we run rake db:migrate with this gem installed, we’ll receive an exception that displays instructions for the “zero downtime way” to perform the changes in this migration. Let’s take a closer look at this exception. Check out the zero_downtime_migrations README for more info about the issues this gem addresses as well as a list of all the error messages and instructions it provides to help prevent downtime! This gem doesn’t catch all of the special cases that we know of yet. Since we’re using Heroku Preboot for zero downtime deployments we also need to watch out for problematic migrations like: Renaming columns or tables Dropping columns or tables Changing column types We’ll continue to enhance and add validations to this gem over time as we encounter and learn from new problematic migrations. Try it out in your own projects and let us know what you think! Pull requests are welcome! Our engineering team is still growing! We’re hiring engineers in our San Francisco and Columbus, OH offices. Check out our careers page to learn more. We look forward to hearing from you! LendingHome Technology 77 1 Thanks to Rebecca Green and Matt Wean . Rails Ruby Postgres Activerecord Database 77 claps 77 1 Written by Staff DevOps Engineer @LendingHome LendingHome Technology Written by Staff DevOps Engineer @LendingHome LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-12-06"},
{"website": "LendingHome", "title": "lendinghome featured in the 2017 wealthfront career launching companies list", "author": ["Omar Bohsali"], "link": "https://tech.lendinghome.com/lendinghome-featured-in-the-2017-wealthfront-career-launching-companies-list-2a05ed158729", "abstract": "LendingHome was recently included in Wealthfront’s 2017 “Career-Launching Companies List.” In their own words: We believe the companies we list each year are the ideal places for young people to start their careers because they are all highly likely to turn into large businesses, and nothing early in your career is more important than achieving success — and nothing signals success more than working for a successful company. See the announcement , as well as the full list . Also, we’re hiring ! LendingHome Technology 3 3 claps 3 Written by Explorer, engineer, entrepreneur. Previously: @LendingHome, @Priceonomics. @ycombinator / University of Virginia alum. LendingHome Technology Written by Explorer, engineer, entrepreneur. Previously: @LendingHome, @Priceonomics. @ycombinator / University of Virginia alum. LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-11-29"},
{"website": "LendingHome", "title": "auto labeling pull requests", "author": ["Sean Huber"], "link": "https://tech.lendinghome.com/auto-labeling-pull-requests-118c84f188d3", "abstract": "As our engineering team continues to grow it’s becoming difficult to keep track of all the changes that we’re introducing to our codebase every day. We’d like to find an automated way to categorize the changes in each pull request based on the files that they touch. Let’s try auto-labeling pull requests that match the following criteria: Dependencies have changed e.g. gems or node modules Database migrations have changed Styleguides for Ruby, JavaScript, or SCSS have changed We’ll use dependency , migration , and styleguide as our pull request labels. We just need to define a list of file patterns that match them. We’ll need something that accepts a list of the files changed in a pull request and returns an array of all matching labels. Let’s use Ruby’s built-in File#fnmatch to perform the glob pattern matching. We’re using GitHub to manage our pull requests so let’s use the octokit gem to communicate with their API. We’ll only need to use these two methods: compare to fetch a diff of all the files changed in a pull request add_labels_to_an_issue to actually update pull request labels Let’s create a sinatra app with an endpoint to receive webhook events from GitHub. First let’s generate a Gemfile and define our dependencies. Now let’s create a config.ru file and configure our webserver. This server process can be started by simply calling bundle exec rackup. GitHub allows us to configure webhooks organization wide so that all of our repositories get this auto-labeling behavior. Let’s configure a new webhook that only listens for pull request events. Success! Now all of our pull requests get auto-labeled anytime we push changes to files that match our label definitions. As we continued to enhance and iterate on this concept we’ve found additional uses for labeling files in general. We’ve wrapped these enhancements in the owners gem which includes features like: A command line interface so we can match labels against files locally OWNERS files so that our label patterns can live within our repositories Matching files with regular expressions instead of simple glob patterns Debugging info so we can inspect which patterns matched specific files Finding files that don’t match any of our defined label patterns Let’s update our pull request auto-labeler to use the owners gem instead! Knowing who owns a project or section of a code base is very helpful when asking questions or requesting feedback. The owners gem allows developers to define OWNERS files throughout their repository to provide a human and machine readable way to determine who the maintainers are for specific files of code. These OWNERS files can be used to: Define file patterns for auto-labeling pull requests Find the right people to ask when you have questions Request approval from the appropriate people in pull requests Notify maintainers when changes occur to the files that they care about Try it out in your own projects! What other uses can you find for OWNERS? Our engineering team is still growing! We’re hiring engineers in our San Francisco and Columbus, OH offices. See our careers page to learn more. We look forward to hearing from you! LendingHome Technology 21 Github Git Ruby 21 claps 21 Written by Staff DevOps Engineer @LendingHome LendingHome Technology Written by Staff DevOps Engineer @LendingHome LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-20"},
{"website": "LendingHome", "title": "meet the team eduardo del balso", "author": ["Li Ouyang"], "link": "https://tech.lendinghome.com/meet-the-team-eduardo-del-balso-dfad9adb6002", "abstract": "Welcome to our new series, Meet the Team , where we dive deep with a member of the tech team. We kick off with Eduardo, one of the early engineers, to hear what he’s made of and his life at LendingHome. Give us the rundown of your background. Who are you? What do you do at LendingHome? My name is Eduardo, I come from a sordid past of entrepreneurial endeavors: I was born in Montreal, Quebec but lived in Argentina until I was 5, then moved back to Montreal and grew up there. I received a computer engineering degree (Concordia Co-op! Shoutout!) in Montreal, Québec. Since graduating, I’ve been busy: In 2005, I bootstrapped a web hosting company and sold my way out of it in 3 years Around 2008, I took over my father’s daycares when he got sick for approximately 1.5 years, expanded them from 130 places to 150, including building/leasing a commercial building, and negotiated and executed the sale of one of them (80 places). I then realized that I was leveraging a lot software to run my dad’s business, and also realized that that’s what I enjoyed, so I started working on some app ideas around 2010 and hustling some independent contracts wherever I could find them. In late 2010 I decided that I should get some professional engineering experience and I went job hunting in NYC and SFO, but fell in love with SFO after doing a few trips and crashing at a friend’s place who had decided to move here. On those trips I fell in love with SFO as a city and In 2011 I took my first professional engineering gig at ModCloth. That team was awesome! In 2012 I left ModCloth to join a small t-shirt startup called Thread Council with the man who created Where In The World is Carmen San Diego? (True story!), and while I did a lot of cool things there, including a Kickstarter campaign that raised 50k, the company ultimately did not work out. Matt was an investor in that company, and I had booked some meetings with him to get some advice on the situation at Thread Council and get myself on his radar. Eventually I found out that Keith and him were starting a new company, and I had been keeping an eye out to work with those two people specifically since I had moved here, and so when things went south at Thread Council, the timing ended up being perfect and I sent Matt an email asking if he needed help. The rest, as they say, is history. :) As an early engineer, what made you take the leap to join LendingHome? I really have a lot of respect and admiration for the work of Matt & Keith. A lot of my close network worked with both of them at HomeRun, and through their lenses I got a pretty good view of the type of work they did. Even before HomeRun got acquired, I had always said to myself, “I need to keep an eye on these guys’ next move.” and the moment it became a possibility both in my circumstance and as a decision, I pounced. :) How would you describe a typical day for you? Good question… it’s hard to think of a typical day. punt What are you most proud of? I think what I’m most proud of is how much we’ve accomplished at this company in so little time. Thinking back on what I thought LendingHome would look like 2 years after I joined, I could never in my wildest dreams have imaged the scope of the undertaking that we took on and the fact that we actually pulled this off. In hindsight, I think back and realize that every day at work early on we were making astronomically important decisions on an almost daily basis. I actually open my on-boarding “Welcome to LendingHome” presentation with a slide that says, “LendingHome: A tale of naive ambition.” and I think that’s because I had no idea that we were unknowingly starting 9 companies all at once with a group of 15 people. I always knew it was big, but if I had truly known the immensity of this company’s mandate up front, I would have probably have walked away and said, “That’s a fool’s errand. Have fun morons.” but I’m most proud that we stayed, and we worked together and we never ever once questioned whether it was possible, intelligent, wise or even a good idea. We simply had a tacit agreement that this was going to work and we were all there to collectively figure out the smartest path to success. And now we have a huge-ass company that’s way bigger than I thought it would be. That’s dope. :) What’s your favorite programming pro tip? Oh man, so many… Umm, I think learning about inner-loop/outer-loop and red/green/refactor at ModCloth really changed me in a huge way. I’m very ADHD and am certainly of the refactor-cat lineage, but I think when you BEGIN with a good integration test and graduate to a good unit test, you become hyper-focused on doing the minimum to get that use case green, and not only do you have a good test to keep your code running, but you also have inadvertently found the shortest path to done. Oh, that and those vim shortcut modifier like t and i , for example, dti delete everything until the letter i , or my favorite: di’ deletes everything between the ’ tokens, so you can easily, for example, re-write strings, or rewrite the contents of a ruby block. I don’t know the technical term for that middle modifier in those vim shortcuts, but they’re pretty sweet. What gets you excited about LendingHome? This has changed a lot over time. I think early on it was certainly the team, and I still retain the same amount of love and admiration for all those people and new people too, but things aren’t quite as tightly knit as they used to be as we scale this company up. I think these days, I am excited about all the new challenges around scaling the team, and I like that our environment is very supportive of personal growth. I am unhealthily obsessive about everything I take on, and as we scale this company and I’m finding myself more and more blocked by people challenges and not technical challenges, the wealth of new and old faces that are supportive and offering me advice here is something I’m really appreciative of. What do you do outside of work? It’s been a good 10 years that I put my head down and started to hyper-focus on successfully executing my journey through technology and entrepreneurship. I’ve been thinking about code, system design, LTV analysis, segmentation/conversion funnels, UX optimizations, leveraging software systems for business outcomes, leveraging human/organizational systems for productivity and happiness. I used to once be a well balanced individual, but moving to San Francisco has easily quintupled my focus on these things. These days I spend a lot of time trying to shed myself of technology and take on things that I can’t google. I’ve been a musician my whole life, and recently have found that the more things I can google, the more creativity I don’t engage in, if that makes any sense. So these days I’m spending a lot of time reconnecting with my love for music, mainly by studying playing the piano. I’m actually studying jazz piano with a private teacher and loving it. That and video games… sigh. I blame Sean Murray. For everything. :P Hope you enjoyed the interview! Our engineering team is growing and we want to meet you. We’re hiring engineers in our San Francisco and Columbus, OH offices. See our careers page to learn more. We look forward to hearing from you! LendingHome Technology 12 Startups Company Culture 12 claps 12 Written by LendingHome Technology Written by LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-10-18"},
{"website": "LendingHome", "title": "a case for composition", "author": ["Li Ouyang"], "link": "https://tech.lendinghome.com/a-case-for-composition-ed4a0daf79be", "abstract": "I’ve read way too many blog posts about inheritance versus composition. All of them revolving around the is-a versus has-a relationship. If that subtlety has left you scratching your head, here’s a real life example. One of my first problems to solve at LendingHome was how to programmatically create reports for our investors. These reports would be used by investors to track a loan’s payment history, view loan information, and project loan performance across their portfolio. In total, I had three reports to create. All of them needed to be in CSV file format because, you know … finance folks love their excel. Sounds like I need three different type of report generators, right? Since these are types, inheritance was calling my name. I could have a parent class ReportBuilder containing the bulk of the CSV creation. Then 3 sub-classes determining the format of the report, like the columns and the data to display in the column. Now, like I said, finance people love their excel spreadsheets. It’s not just the investors who are clamoring for reports. Our internal operations need CSV reports. Our third party sub-servicer communicate via CSV reports. Even our scoring engine spits out a CSV report. Lots of CSV reports, and if I wanted a true hierarchal report system, I would need intimate knowledge of each report to properly understand what functionality is common among all the types, and therefore what the sub-classes should be inheriting. Many CSV reports in the system with a high probability of new reports being added. It would, at best, lead to a shallow, wide hierarchal structure, but a real possibility of a deep, wide hierarchal structure (the worst kind). Switching gears to composition , consider a generic ReportBuilder class, composed of a CsvWriter , an Adapter and the collection of objects. In our example, a collection of objects is an array of ActiveRecord objects of class Loan . Each component of my composed classes has a single responsibility. The ReportBuilder is building the report for a given set of objects. The CsvWriter is only involved with the logistics of creating a CSV file. The Adapter is only concerned with producing the required data, and could be reused for another report format, think XML or JSON. Easier to test because the functionality of each part is encapsulated. With inheritance, you might be tempted to skip testing functionality in a subclass because it is tested somewhere else on the hierarchal chain. Do not be lulled into this false sense of security. Each subclass is technically different and all of its functionality should be tested. Flexible for different use cases because the design is focused on what and how it’s creating the report versus how it relates to each other The ability to pass additional context to a specific adapter, without changing the others. It would look something like this: After more than a year of usage at LendingHome , the ReportBuilder has been a success. The CSVWriter and ReportBuilder hasn’t changed much, as expected. The adapters have been changed several times as business needs changed. Updating the adapters has been a breeze with little trepidation. Want another column? No problem. Want to display this other data? I got you. Our engineering team is also growing! We’re hiring engineers in our San Francisco and Columbus, OH offices. See our careers page to learn more. We look forward to hearing from you! LendingHome Technology 32 1 Programming Ruby 32 claps 32 1 Written by LendingHome Technology Written by LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-13"},
{"website": "LendingHome", "title": "cultivating friendgineers", "author": ["Rebecca Green"], "link": "https://tech.lendinghome.com/cultivating-friendgineers-267b1ddba97", "abstract": "our engineers are too humble I yell about how great they are everyone wins Every other friday, someone sends out a glowing email all about the things we accomplished that sprint. (Sometimes it is literally glowing.) Big feature releases are part of it, but it’s mostly people saying nice things about how much we enjoy working together. Here’s what an average email looks like. At LendingHome, we work in two week sprints, culminating in a meeting called “the retro” where we talk about how things went. It’s my favorite meeting. Everyone brings something to talk about and at the end, we have a list of “action items” to make the next sprint better. One day, a few months after I joined, I was “in charge” of the retro. I went down the list of topics people had written down and added action items for the important ones. Sometimes I told people to move it along when they got wordy. For that particular sprint, there were a bunch of items like “April is amazing and I love working with her” and “Alex made some awesome optimizations to our credit-policy workers” and “Renga says really smart things I’m glad he joined”. At the end, I had written “Tell X that they’re awesome” for six or seven people. I didn’t know what to do for them, so I wrote an email just before I left for the weekend saying how great they were. I sent it to the engineering team and figured I was finished. Slightly more than two weeks later, I’m in a meeting with our CTO, and he brings up the email and how last Friday, he hoped that there would be another one. At that moment, I realized that I had started something. Shit. I can’t run retro every time, what am I gonna do. It turns out that when you work with helpful people, it’s easy to crowdsource your job. We created an online sticky board and every other Friday, I tell people to add the cool stuff they did that sprint. Then I copy-paste it into an email, add some emoji and caps lock and press send. It takes barely any time, and it makes us look great . Lately, we’ve been playing with a voting bot in slack that keeps track of scores for everybody. Burrito emoji won the first week so I haven’t been taking it seriously, but sometimes it works. Greg won last sprint because he organized a talk with one of the React founders and that’s pretty awesome. Low Effort, High Impact If it takes you more than 30 minutes, congrats, you’re like me and you rewrite everything five times. But for the rest of you, it’s fast and easy. FUN But we don’t have any fun we’re serious engineers you say. Well ok. Don’t have fun. Send out a serious email, call it “BORING SPRINT RETROSPECTIVE” and make sure the font is set to something like Times New Roman. You can get the rest of the benefits without any fun. Your Boss will like it Your boss is probably exhausted, and they’re in meetings all the time, and they never smile. Silly emails are a great way to remind them that you appreciate the heck out of what they do. Your Coworkers will Like It This one is much more important. Your coworkers are great people who work hard and build cool things! Make them feel loved and appreciated without being a weirdo. Or, be a weirdo with plants all over your desk and send out ridiculous emails talking about how much you love your coworkers. For some reason, they will find it charming. You will Like It I look forward to Fridays for A LOT of reasons, but retro is way up near the top of the list! I smile a lot writing the email, because it’s great to read about everything we accomplished. It’s especially great when it’s about something that you didn’t have to put the long hours in on. The guys that sit next to me just released the thing they’ve been working on for the last six months and I am going to hype the shit out of it. Weekly, or semi-weekly, or bi-weekly, or however you organize chunks of work, in whatever way feels best, let people know that you think they’re the bees knees. They’ll be happy to know that they’re appreciated, and you’ll have made someone’s day. Go spread the love! Feel like being the subject of a glowing email written by yours truly? We’re hiring engineers in our San Francisco and Columbus, OH offices. See our careers page to learn more. We can’t wait to hear from you! LendingHome Technology 82 Thanks to Matt Wean . Agile Retrospectives Culture Engineering 82 claps 82 Written by LendingHome Technology Written by LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-27"},
{"website": "LendingHome", "title": "being lazy with activerecord", "author": ["John Andrews"], "link": "https://tech.lendinghome.com/being-lazy-with-activerecord-77c470917160", "abstract": "In programming language terms, “laziness” refers to code whose evaluation is delayed until the last possible moment. This strategy can have several benefits. For example: In this (contrived) example, since we're using a lazy enumerator, we never actually calculate any squares after the ninth. We're using take_while to limit our iteration to just the numbers which we care about, saving our cpu from making about a million extra calculations. That's super cool but I never had a good use case in ruby, outside of the realm of example problems, until the other day when I was tasked with troubleshooting why one of our reports was failing to generate. The report looked something like this: My hypothesis was that the data set was so large that the process was being killed for using too much memory. Looking at the search terms it was clear that the report was pulling a lot of data. Now Servicing::RemitQuery has a bunch of complicated logic that I didn't really want to dig into, but what I noticed is that we were pulling the whole query result set into memory at the same time with that flat_map. Normally when dealing with a large ActiveRecord result set, I reach for the find_each method. This method executes your query and yields each result to the block just like normal each, except behind the scenes it's fetching your records in batches to keep the memory overhead low. As soon as you're done using the record in the each block, ruby can run GC and reclaim the memory. That is, as long as you're not keeping a reference to it outside of the block. But that is exactly the problem with our method. It is iterating over the query results and flat_mapping them into one of the associations. Normal find_each wouldn't help at all in this situation. Now one might argue that I should just query for payout objects instead of whatever is associated to them. One would not be wrong, however remember that this is a complicated query we're dealing with which is used in other contexts. Not a small task to refactor. Compromise is required to get our report back up and running. Luckily I remembered I have another trick up my sleeve. Laziness! If you call find_each without a block it returns an enumerator, which can be converted to an Enumerator::Lazy with a simple call to the lazy method. Next, compact is eager, meaning that it would force the entire sequence to come into existence so it could remove the nils. Instead we switch to reject(&:nil?) which has an identical effect and is also lazy. Because of laziness, no data is actually loaded until it is accessed inside the generate_csv method, which uses a normal each block to iterate over payouts. No changes to generate_csv were necessary. Execution time improved vastly because now we’re not using swap space. Memory utilization went from “way too much” to “barely noticeable”. Such a huge win with so little code change! Our engineering team is also growing! We’re hiring engineers in our San Francisco and Columbus, OH offices. See our careers page to learn more. We look forward to hearing from you! LendingHome Technology 60 4 Ruby Ruby on Rails Functional Programming 60 claps 60 4 Written by Software Engineer at LendingHome LendingHome Technology Written by Software Engineer at LendingHome LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-06"},
{"website": "LendingHome", "title": "practical guide to postgresql optimizations", "author": ["Omar Bohsali"], "link": "https://tech.lendinghome.com/practical-guide-to-postgresql-optimizations-d7b9c2ad6a22", "abstract": "Basic proficiency with PostgreSQL is one of the highest-leverage skills an engineer can have. Even a cursory understanding of its internals and configuration saves significant time: hours that would have otherwise been spent writing complex one-off scripts to restore data and optimize query patterns can often be trumped with a simple configuration change. In this post, we’ll first go over some of the important configuration options and explain what they do, walk through some common configuration setups, and finally, we’ll link to some helpful resources to guide you further. PostgreSQL is one of the most popular database servers today. It’s inured from 20-plus years of real-world production deployments, so it’s naturally very performant and reliable. Unfortunately, this ubiquity also means that the default configuration settings are a compromise designed to work on a variety of machines. This standard configuration leaves room for many simple changes that can translate into great performance gains. The default PostgreSQL config file lives in postgresql.conf and is pretty long (around 483 lines on a fresh install). Here’s where you can find it on Homebrew and Ubuntu: For many use cases, there are only a few key settings that you need to know about to make a difference: shared_buffers is the amount of dedicated memory that Postgres can use to cache data before relying on the operating system’s shared kernel buffer. Think of it as Postgres’ dedicated short term memory. It’s important to keep things cached because reading data off of disk is much slower than fetching it from RAM; the higher the value for this setting, the more likely it is the data will be accessed from RAM. The higher you set this value, the more memory Postgres can use to cache results. While it’s tempting to set this number very high, a rule of thumb is to keep it around 25–40% of your total available RAM. So for example, on a machine with 16GB RAM, a good value is below 6GB. [1] Keep in mind, if you have a very small dataset, increasing shared_buffers might not make a difference. If you set shared_buffers to a high value, you might see an error about kernel memory settings, shmmax to be specific. Fear not. Shmmax is the maximum amount of memory that your operating system will allow a process to use. See the section below on how to fix this. The total amount of memory you think Postgres will be able to use, including both shared_buffers and the amount of memory available by the operating system caches. The query planner uses this number to figure out how much memory will be available for caching results while running queries. Set this too high and the query planner gets too aggressive and memory usage balloons; set it too low, and Postgres might not use the right indexes because they won’t fit into memory. The maximum number of concurrent connections that Postgres should allow. If your server accepts a lot of traffic with relatively simple queries and you need more throughput, try having a higher number here (keeping in mind constraints from work_mem). On the other hand, if you’re doing complex queries that require a lot of memory, set it lower. work_mem is the amount of memory allocated to each Postgres operation (see max_connections) This determines how much memory a single Postgres operation can use, and is especially helpful for complex sorts and joins. More memory is better, because that makes it less likely that the OS starts swapping to disk, which is slower than keeping the query in memory. Be careful — this is the amount of memory allocated *per* operation. If you have 50 operations and a complex sort going on that requires 100MB per query, you’re suddenly going to be using over 5GB of memory. This is similar to work_mem, but for maintenance operations like CREATE INDEX. If you’re doing a lot of index creation or other types of maintenance tasks, set this high and you’ll save a lot of time. When restoring database dumps, it helps to set this number high to speed up index creation and other maintenance tasks like vacuuming. First, it’s important to figure out what kind of workload you are targeting. There’s no one-size-fits-all approach, but here are a couple of general guiding principles based on your type of workload: If you’re restoring data (maybe through pg_restore from a backup), you’ll want to increase the amount of worker memory (maintenance_work_mem, used to create indexes) and reduce the number of checkpoint segments (see section below on checkpoint_segments), assuming you’re confident that the data you’re loading is intact. Also, if you are restoring data using pg_restore, leverage its multi-core options through the — jobs option so you can restore with many threads in parallel. This almost divides the amount of time required to restore data by the number of processors you use. Caveat — you have to tell pg_dump to export into the directory format for this to work. Also, see the section below on general write-heavy configurations for more ways to reduce restore time. If your deployment is heavily focused on reads, you have a few decisions to make. The most important is: does the data you need to access follow the Pareto Principle where 20% of the data is read 80% of the time, or is it a more evenly distributed read distribution? If your reads are Pareto distributed, you can probably increase RAM with hopes of keeping the necessary data in cache for as long as possible. On the other hand, if you’re in a situation where you have a lot of data and your reads are near-uniformly distributed, it’s good to 1) try and increase shared_buffers so that you can hopefully fit all the data in caches, and if not, 2) upgrade to high-performance storage (like iSCSI 15k drives) or SSDs. If you’re on Heroku, just pick a more performant database plan. Many performance bottlenecks happen at the database layer, so investing in better disks and adding more data caching can really increase performance in read-heavy environments. Postgres writes new entries to a system called the Write Ahead Log (WAL) in 16MB segments. In the default database configuration, the WAL “flushes” to disk every time 3 of these segments fill up (checkpoint_segments is set to 3 by default), and this process is called a WAL checkpoint. [2] Checkpoints are expensive operations, and if you’re in a write-heavy environment, it’s advisable to increase checkpoint_segments to a higher number so that you reduce the frequency at which the WAL flushes to disk. For example, you can set checkpoint_segments to 12 and you will reduce the overall number of checkpoints by a factor of 4. If you are lucky enough to be in a situation where minor data loss is acceptable, you can disable synchronous commits by setting synchronous_commit to off. Bear in mind that this is a risky approach — you’re putting yourself at risk to losing a small amount of data, but disabling synchronous commits in the right environments can significantly boost write volume. If you try restarting your server and get an error like this: That means you have to tweak your kernel memory settings. See the Postgres explanation on kernel resources for a quick walkthrough of how to increase allowable memory allocation on your machine. More info on kernel memory settings here . PostgreSQL is a great database server, and some basic configuration tweaks can help you unlock its performance. It’s very actively maintained with a huge development following, so it’s constantly evolving and keeping up-to-date with user needs. For more information on PostgreSQL configurations, see the online manual (make sure you pick the right version), and also the excellent wiki . Our engineering team is also growing! We’re hiring engineers in our San Francisco and Columbus, OH offices. See our careers page to learn more. We look forward to hearing from you! _______ This post was written by Omar Bohsali . He’s an engineer at LendingHome and works on the retail investing team. Thanks to Donovan Bray, Sean Huber, Aman Gupta, and Joe Damato for reading drafts of this. [1] http://www.postgresql.org/docs/current/static/runtime-config-resource.html#GUC-SHARED-BUFFERS [2] https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server LendingHome Technology 102 1 Database Postgres Performance 102 claps 102 1 Written by Explorer, engineer, entrepreneur. Previously: @LendingHome, @Priceonomics. @ycombinator / University of Virginia alum. LendingHome Technology Written by Explorer, engineer, entrepreneur. Previously: @LendingHome, @Priceonomics. @ycombinator / University of Virginia alum. LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-24"},
{"website": "LendingHome", "title": "productivity", "author": ["Matt Wean"], "link": "https://tech.lendinghome.com/productivity-78da40e42d45", "abstract": "Bob spends his time in and out of meetings and jumping between tasks. At the end of most days, he feels exhausted and frazzled, despite hardly accomplishing anything. Sally has hours each day of uninterrupted time focused on her most important tasks. She goes home relaxed and happy with her progress. This is the difference between managing your time efficiently and being constantly interrupted by meetings, Slack, or Twitter. An hour of flow is vastly more productive than an hour peppered with distractions, which is why Sally is so much more effective than Bob. Here are some techniques you can use to be more like Sally. Before you can improve your productivity, you have to track where your time is going. You can’t improve what you don’t measure. Use an app that records the applications, web pages, and documents you use throughout the day. A great, free option is RescueTime . After a few days, you’ll have a baseline for how you spend your time and what detracts from work on important tasks. The first step to increase your productive time is to reduce the number of meetings you attend. Look back at your calendar and rate how valuable each one was, then cut out any that weren’t a good use of your time. Now that you’ve eliminated extra meetings, you can make the necessary ones more efficient. Pack your meetings. Nothing kills your momentum like having to stop for a meeting. Prevent this by scheduling them back-to-back. It’s worth the extra time to coordinate. Set up meeting-free days. A couple teams at LendingHome recently started to schedule no-meeting days. This opens up huge swaths of time for tackling big problems. If you can’t get other teams to do this, you can schedule an all-day event to reduce the chances of someone putting time on your calendar. Once you have big, open blocks of time, the next step is to put them to good use. According to the author of a study on the effects of interruptions on work , “it takes an average of 23 minutes and 15 seconds to get back to” what you were working on. This is just wasted time, so you should set up tools and systems to avoid it at all costs: Turn off email notifications You can keep the icon badge, but hearing a ding or seeing the notification flash on your screen will pull you out of flow. Mute Slack channels Permanently mute any channels you can, then snooze all notifications when you’re really trying to focus. Hide your apps Seeing Slack or Twitter updates, even out of the corner of your eye, can distract you. Avoid this by only keeping apps relevant to your work visible. Block distractions For a little extra support, try an app like SelfControl to block distracting websites for a set time period. Meditation (specifically Mindfulness ) has been shown to not only reduce stress, but improve concentration and resist distractions . Just 10 minutes a day with a guided meditation app like Headspace will make it much easier to focus. It may seem like magical thinking, but try it one week to see if it works for you. The Pomodoro technique is a simple system where you focus completely on a task for short bursts: Pick a task and work on it for 25 minutes. Take a five-minute break. Repeat. Every fourth Pomodoro, take a 15-minute break instead of five. It doesn’t seem like much, but this is a powerful tool for achieving flow. The key is you only have to resist distractions for a short period, which makes it much easier. Also, by picking a specific task to work on, you avoid costly context switches. You also get improved visibility into how long tasks take, which helps with your estimation ability. There are many apps to manage Pomodoros; the one I’m using is tomatoes . Try implementing a few of these techniques into your process and see how they affect your productivity. Remember that your goal is to have long, uninterrupted blocks of time where you focus on one task at a time and stay in flow as long as possible. You’ll be amazed at how much more you accomplish. Our engineering team is also growing! We’re hiring engineers in our San Francisco and Columbus, OH offices. See our careers page to learn more. We look forward to hearing from you! LendingHome Technology 11 Productivity Time Management Pomodoro Technique Meetings 11 claps 11 Written by Senior Software Engineer at LendingHome LendingHome Technology Written by Senior Software Engineer at LendingHome LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-09-06"},
{"website": "LendingHome", "title": "modest beginnings", "author": ["Keith Tom"], "link": "https://tech.lendinghome.com/modest-beginnings-565204204f51", "abstract": "This is the story of a fintech startup innovating in the trillion-dollar mortgage industry. We started with six people in a tiny room and one desk. Within 18 months, we raised $100 million in funding. The company has grown to over 200 people while keeping the engineering team lean at 35. To date, we originated more than $750 million in loans. And we’re not stopping. This domain is broad and complex, like launching five startups at once. Somehow, we pulled it off: our engineering team built the “full stack” of the mortgage industry. First, we created a consumer site to simplify the borrower experience. Just by logging in, borrowers can see their loan’s status, progress and any new requirements. But, for that to work… We needed an integrated loan origination system to update our borrowers whenever our operations team made progress on their loan. So we built one. Then, we discovered… Before approving and funding a loan, we needed a rules engine to manage and communicate our investor’s varying requirements — including exception scenarios — to our operations team and borrowers. So we built one. Then, we realized… Once we originated and sold loans, we needed a transparent investor experience, both for investing and checking loan repayment status. Borrowers simply log in to repay and investors immediately know when they’ve been repaid. We built it that way. And for all other loan servicing needs, like late payments, early payments, no payments, we built a system for that too. Looking back, this was likely crazy. No one in the mortgage industry does this — some might take on bits and pieces, but no one does it all. At each step, we indeed considered whether to build or buy. For a time, we bootstrapped with third party help, but these systems are particularly interconnected: For the best customer experience , our operations team and borrowers need to be connected from origination to servicing to payoff. To truly innovate and become the world’s best mortgage lending marketplace, our loan origination system needs to balance the needs of both borrowers and investors continuously. To attract more investors , our investor portal needs to be sync with our servicing and our borrower experience. This is all connected. We couldn’t buy — we had to build. All this doesn’t come without challenges. Along the way, we made measured tradeoffs in growing the business and managing tech debt. And though today our maturing company is becoming robust and scalable, at the start, we optimized for speed to prove the business. We learned a lot about managing our monolith, while keeping a path to service oriented architecture when we need. We scaled our teams as well as our code. This is no simple feat. We’re rebuilding the engine while flying the plane. Without a solid engineering team, this would not have been possible. We hired the right blend of entrepreneurs, YC founders, former CTOs, software veterans and artisans with 10–15 years of consulting experience. This isn’t just about smarts, but sweat and hard work. This is our tech team’s origin story. And we’ve only just begun. There’s still much more territory to conquer, much to learn, and many challenges to blog along the way. Join our adventure. Apply now . LendingHome Technology 19 Finance Fintech Lendinghome Marketplace Lending 19 claps 19 Written by LendingHome Technology Written by LendingHome Technology Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app", "date": "2016-08-18"}
]